hardware,3e3bi3,Exist50,27,Tue Jul 21 17:09:58 2015 UTC,Gimme dat delicious FPS per inch...????
hardware,3e3bi3,aigoo,16,Tue Jul 21 18:05:28 2015 UTC,"Better summarized here, but I wanted to give the original source credit.  Edit: http://i.imgur.com/16lMPiE.gif"
hardware,3e3bi3,SirCrest_YT,12,Tue Jul 21 17:10:48 2015 UTC,It's so cute. I want one for a HTPC.
hardware,3e3bi3,Flukemaster,8,Tue Jul 21 17:30:05 2015 UTC,That'd be a beastly HTPC
hardware,3e3bi3,AdmiralShark,3,Tue Jul 21 19:01:43 2015 UTC,I didn't see any mention of shader count or other such numbers... Am I just being blind?
hardware,3e3bi3,logged_n_2_say,5,Tue Jul 21 20:06:27 2015 UTC,"everything is the same as the fury x, aside from the clocks and the consumption.  http://www.anandtech.com/show/9390/the-amd-radeon-r9-fury-x-review/9"
hardware,3e3bi3,TaintedSquirrel,1 point,Tue Jul 21 20:30:53 2015 UTC,I'm pretty sure that GIF is from Blunty's video.
hardware,3e3bi3,rndnum123,5,Tue Jul 21 21:12:08 2015 UTC,Looks like Nano will deliver >40 GFLOPS/watt.
hardware,3e3bi3,ryno9o,2,Tue Jul 21 18:21:05 2015 UTC,Hopefully we'll see some pretty interesting variations of the Nano. They could really make some niche products with a board that small.
hardware,3e3bi3,Alarchy,4,Tue Jul 21 18:57:58 2015 UTC,"Pretty much as expected: 390x performance-ish, with better 4k performance, for 2/3 the power. Since this is a better binned Fiji XT, and aimed at the mITX crowd, I'd expect this would sell for around $550-600."
hardware,3e3bi3,logged_n_2_say,2,Tue Jul 21 18:41:25 2015 UTC,my guess would be these are lower binned fiji xt since these are likely running slower.
hardware,3e3bi3,Alarchy,5,Tue Jul 21 19:32:38 2015 UTC,"Anandtech was speculating (and I trust them) that Nano would be using the highest-binned Fiji XT - specifically to get the better voltage/clock out of it, for the high efficiency boost. Higher binned GPU = less voltage required for same clocks = less power required = more efficient.  Lower-binned GPUs can get more frequency benefit from higher voltages when overclocking on water/LN2, like the Fury X.   http://www.anandtech.com/show/9390/the-amd-radeon-r9-fury-x-review/9"
hardware,3e3bi3,logged_n_2_say,3,Tue Jul 21 19:36:27 2015 UTC,"if true, it seems to totally go against their previous MO.  can you see amd NOT using their most expensive gpu's  in their best performing product, all in the name of efficiency?  my understanding is the differences in efficiency aren't large, unless the bins are way way different.  it will be interesting to see what people can overclock with the nano to.  are we getting non-reference nanos?"
hardware,3e3bi3,Alarchy,2,Tue Jul 21 20:16:00 2015 UTC,"It's really not too much of a stretch - if you need lower power use, you need a higher-bin (low voltage) part. The non-custom 980 TIs have some pretty low bin parts, but they only need to hit a certain spec so it works fine. Similarly with the FuryX, it's water cooled so it doesn't need a high bin part to perform to spec."
hardware,3e3bi3,logged_n_2_say,1 point,Tue Jul 21 20:37:30 2015 UTC,"i'm just trying to wrap my head around it from a business standpoint.  those chips can command the highest profit, and would have the best chance of ""winning"" the top gpu spot if they were allowed to run at the highest clock, which helps the entire line's perception.  it also means the nano will have to be higher priced than projected/hoped in order for it to make sense.  if that's true they are making a large bet on an assumed shifting market.    that lowers the performance ceiling of the fury x, and makes the lesser performing nano more expensive."
hardware,3e3bi3,Alarchy,0,Tue Jul 21 20:46:19 2015 UTC,"I'd imagine there's a bigger market for high-end, full-length GPUs than mITX builds. Sure, if they used only the highest bin Fiji they maybe could have had default clocks of 1125mhz Fury X, but they would have had a much smaller supply of those chips. Instead, they could choose to go with their larger quantity of ""good enough"" chips, spec it at around-980TI/Titan levels, and use their smaller quantity of high-quality chips for a niche part where they know people will pay extra (everyone pays a premium for mITX builds)."
hardware,3e3bi3,buildzoid,0,Tue Jul 21 21:05:19 2015 UTC,Water does not benefit from bad bins. LN2 benefits from bad bins because bad bins have low cold bugs.
hardware,3e3bi3,Alarchy,1 point,Tue Jul 21 19:48:17 2015 UTC,"Lower-binned parts have more voltage leakage at the same voltage/frequency/temperature than higher-binned parts. Higher-binned parts require less voltage at the same frequency to get the same stability at the same temperature, hence why they're good for air overclocking.    Lower-binned parts can have more overclocking headroom when using water/LN2 because at lower temperatures (than air) their voltage leakage is reduced and you can pump more juice into it before it loses stability.  I'm not sure about the ""cold bugs"" thing, unless you mean that as a function of the extra voltage leakage."
hardware,3e3bi3,hojnikb,2,Tue Jul 21 20:14:37 2015 UTC,"cold bug means the minimum temparature, where cpu still operates and/or boots."
hardware,3e3bi3,buildzoid,-4,Tue Jul 21 20:20:14 2015 UTC,I don't have time to explain why you're wrong but you are.
hardware,3e3bi3,Alarchy,1 point,Tue Jul 21 20:51:19 2015 UTC,"http://www.overclock.net/t/1391537/max-safe-7950-voltage-while-ocing/20#post_19981589  There's lots of other examples out there too, but I don't expect everyone agrees."
hardware,3e3bi3,Uproarlol,2,Tue Jul 21 21:07:31 2015 UTC,I just hope its priced right
hardware,3e3bi3,cantbebothered67835,4,Tue Jul 21 17:56:31 2015 UTC,I'm expecting no less than $500
hardware,3e3bi3,dylan522p,-10,Tue Jul 21 18:02:46 2015 UTC,"This has to be a die shrink. No way they can get that kind of performance and efficiency just by adding HBM (yeah I'm pulling this out of my ass, but c'mon)"
hardware,3e3bi3,cantbebothered67835,3,Tue Jul 21 18:53:15 2015 UTC,Running at lower clocks solves that. You can dramaticlyndrop voltage and clocks a decent amount and save a lot of Power
hardware,3e3bi3,dylan522p,2,Tue Jul 21 18:56:07 2015 UTC,"I'm reading that full fiji has a ROP bottleneck and so dialing shader performance down won't impact frame rates that much but, still, <160 watts... (Don't get me wrong, I want this to be true, I'm almost at the point of drooling)"
hardware,3e3bi3,bphase,2,Tue Jul 21 19:08:57 2015 UTC,You can do much the same with Nvidia products aswell. People have clocked 980ti and titans x to really low clocks and voltages it's just the board size isn't that small sadly.
hardware,3e3bi3,cantbebothered67835,2,Tue Jul 21 19:15:40 2015 UTC,"Doesn't help even if it has (probably does), since everything but memory bandwidth is tied to core clock. So if you decrease core clocks by 20%, you lose performance by about that much unless there's a memory bottleneck, which is unlikely in the case of HBM."
hardware,3e3bi3,screwyou00,2,Tue Jul 21 19:32:52 2015 UTC,I'm suspicious of this ROP bottleneck. The comparison to Hawaii's ROPs is a poor one
hardware,3e3bi3,masturbateAndSwitch,3,Tue Jul 21 21:49:00 2015 UTC,"Maybe, but it would explain why the fury x is only 7% faster than the non x despite having 15% more alus and 5% higher clocks.   http://tpucdn.com/reviews/ASUS/R9_Fury_Strix/images/perfrel_2560.gif"
hardware,3e3bi3,masturbateAndSwitch,2,Tue Jul 21 22:26:58 2015 UTC,"I've severely downclocked my r9 290 to have 600Mhz on the core and 800Mhz on memory, and I've undervolted my R9 290 by -25mV. I can tell you it uses noticeably less power and outputs way less heat (it's an average of about -10C, but I've seen it also go down -15C). I also get 60FPS High in Battlefield 4 (I would normally get 60FPS Ultra, or if I increase the core and memory clock a bit more), and I can still get around 30FPS (High settings, not Ultra) in more taxing games like Witcher 2 or a heavily moded Skyrim. You can drop voltages and clock speed and still get good fps, and with the Nano it would seem AMD found a ""sweet"" spot to downclock and undervolt the full Fiji chip without compromising too much performance."
hardware,3e290t,tedlasman,35,Tue Jul 21 11:59:32 2015 UTC,"This should have been a simple series of screenshots on imgur, not a shaky phone camera video. This dude needs to learn to record his screen if he wants to take a video."
hardware,3e290t,SeaJayCJ,10,Tue Jul 21 13:32:14 2015 UTC,It was like watching an episode of cops.
hardware,3e290t,i_mormon_stuff,3,Tue Jul 21 15:23:22 2015 UTC,Filmed by Michael J. Fox.
hardware,3e290t,kennai,1 point,Tue Jul 21 17:18:11 2015 UTC,but money on clicks doe
hardware,3e290t,iPlayRealDotA,8,Tue Jul 21 18:49:36 2015 UTC,Original /r/AdvancedMicroDevices post
hardware,3e290t,Exist50,10,Tue Jul 21 12:13:44 2015 UTC,"A ~20% overclock isn't bad. Assuming stability and reasonable operating temps, that at least puts the Fury X in a better position that it was. And by extension, the Fury as well."
hardware,3e290t,namae_nanka,6,Tue Jul 21 15:37:03 2015 UTC,"At 1453mv, it's bad. Hopefully he was just showing off the voltage that can be attained and not the maximum he could push the card to. Though I have my doubts with that scenario."
hardware,3e290t,Exist50,4,Tue Jul 21 17:09:03 2015 UTC,"Oh yeah, the power consumption would likely reach around 500w at that level. Still, if it's stable, it's mildly promising. Many Hawaii/Grenada chips can't reach 1.25-1.3Ghz without artifacting."
hardware,3e290t,CykaLogic,1 point,Tue Jul 21 17:13:10 2015 UTC,"Way more than that. 1.3v-1.35v is considered the maximum safe voltage before breakdown starts occurring on 28nm. Voltage also scales exponentially, so you're looking at ~40% increase from voltage alone, let alone power usage increase from clock speed increase.  It also looks like he only achieved 1242 MHz with that voltage, seems really fucking bad considering if you pump 1.4v through Hawaii it will hit 1300+ easily.  Finally, some overclocking experiments done with the HBM seem to show that overclocking memory that increases performance by a LOT."
hardware,3e290t,Exist50,1 point,Tue Jul 21 17:45:11 2015 UTC,"IIRC, overclocking either the core or the memory doesn't do too much, but both combined produce noteworthy results"
hardware,3e290t,jinxnotit,1 point,Tue Jul 21 17:50:04 2015 UTC,I think that is largely to do with the interposer.
hardware,3e290t,Exist50,1 point,Tue Jul 21 18:16:52 2015 UTC,How would that matter?
hardware,3e290t,TehRoot,1 point,Tue Jul 21 21:51:20 2015 UTC,1.4v through Hawaii it will hit 1300+ easily.   If you use LN2 yeah.
hardware,3e290t,CykaLogic,1 point,Tue Jul 21 19:25:30 2015 UTC,"Yeah, the point is that 1.4v is neither sustainable nor is 1242 a good clock for 1.4v.  Also, I think 1300/1.4 on Hawaii is just on the edge of what a custom water loop can sustain. 2 360mm rads should be able to sustain it."
hardware,3e290t,hdshatter,13,Tue Jul 21 19:56:46 2015 UTC,"That was 1 site, have not seen anyone else able to get anything decent for fps gains just from overclocking the memory."
hardware,3e290t,KamikazeRusher,4,Tue Jul 21 20:49:01 2015 UTC,"Sunnuva –  FYI:  If you're going to record your screen, stop zooming/closing in if it doesn't focus. No matter how much you physically or mechanically move closer the sensor isn't going to recognize the depth/distance and adjust fast enough. Nobody can see a damn thing when you do this. Also, don't move around so damn much. It's distracting and hard to understand what all is going on, much like working on a 5000-piece puzzle without a photo of the final image and not having the edge pieces to start off with.  EDIT  I'm not talking directly to you, OP."
hardware,3e290t,logged_n_2_say,3,Tue Jul 21 12:51:41 2015 UTC,"couple of things    in the video it says ""modified version"" in the video it still says 4.1.1. on ab, the newest release from may. as of 7/17 unwinder says it's not even on the way   so i guess this is a unauthorized modified version?  wouldn't the beta show a different number?    the ab board is quiet so far, but i don't have access to the beta forum."
hardware,3e290t,Seclorum,2,Tue Jul 21 14:53:56 2015 UTC,"People have been manually going in and modifying afterburner to properly communicate with the voltage controller, because MSI didn't update to accommodate the new voltage controller AMD used.   Why either they didn't update or AMD didn't release the information in the first place, I'll never understand. Especially after they made it a big selling point of the cards overclockability but neglected to have 3rd or 1st party support for that feature."
hardware,3e290t,TehRoot,1 point,Tue Jul 21 15:56:17 2015 UTC,"MSI doesn't control AB development, Unwinder does. They pay him to do it.   AMD doesn't have a unified driver level interface for voltage/powertune control, each board is specific to its implementation."
hardware,3e290t,Seclorum,1 point,Tue Jul 21 19:27:24 2015 UTC,"AMD doesn't have a unified driver level interface for voltage/powertune control, each board is specific to its implementation.   Which just makes the whole situation worse, because why wasn't the information for how to access and control it released?  Especially with all their promotions for the cards being 'good overclockers!' when in fact none of the OC software available supports them or was anywhere close to supporting them properly?"
hardware,3e290t,TehRoot,7,Tue Jul 21 19:35:22 2015 UTC,"AMD doesn't give out that information. Never has. It's the same general principle from board to board, but developers need physical access to probe the I2C interface."
hardware,3e290t,TurboGLH,4,Tue Jul 21 19:36:47 2015 UTC,Here's to hoping for some decent quality videos or even an afterburner download in the next few days.
hardware,3e290t,namae_nanka,2,Tue Jul 21 12:10:32 2015 UTC,I don't think he's affiliated with ab. But you never know.
hardware,3e1zsb,Skymirrh,28,Tue Jul 21 09:57:41 2015 UTC,"I would expect the answer to be something along the lines of ""there isn't enough demand.""  You're right that there is a small market for 16:10 monitors... however, I think most people who want a 16:10 monitor (mainly, professionals) won't need 120Hz+. So the market for a 120Hz+ 16:10 monitor is much smaller than the original 16:10 market, so much so that it probably wouldn't be in their interest to develop/manufacture the panels for such a small group of people."
hardware,3e1zsb,0x60,6,Tue Jul 21 10:42:39 2015 UTC,"I would think that video editors that work with 24/25 fps content would really appreciate the need for no pull down for 24fps content and less noticeable pull down for 25fps content. I also think that video game developers they regularly run their games would appreciate 16:10 and 120Hz, without needing to have extra monitors just for higher framerate. There are probably many places where 120hz would be beneficial. But I understand that the market wouldn't probably be that large.  I too would like to see a 16:10 (2560×1600) 120Hz display."
hardware,3e1zsb,Muorman,10,Tue Jul 21 11:55:18 2015 UTC,Video Editors simply run their dedicated preview monitors at 24/48hz. I know I do occasionally if I care enough about smoothness.
hardware,3e1zsb,SirCrest_YT,2,Tue Jul 21 13:35:08 2015 UTC,"I was thinking of situation where mixed framerate is edited, but realize that those cases are probably really rare, limiting to tv-news content editing or such and they probably have their own solutions/problems."
hardware,3e1zsb,Muorman,5,Tue Jul 21 15:18:42 2015 UTC,"Even in those cases, you're editing to a single framerate. Best to preview at the refreshrate of your audience and target format."
hardware,3e1zsb,SirCrest_YT,4,Tue Jul 21 16:10:47 2015 UTC,"That is exactly my case! Having developed in Unity both on a 16:9 at work and 16:10 at home, the difference is HUGE.  That said I'm not even sure the things I've done in Unity would run properly at 120Hz :P"
hardware,3e1zsb,ancientworldnow,2,Tue Jul 21 14:12:50 2015 UTC,"I work in film/tv post production and we all use ridiculously expensive displays just for preview with the GUI on something more mainstream (a sort of color accurate Ultrasharp or similar). Hell, some suites still have CRT's in them for their preview display."
hardware,3e1zsb,shdjjj8,1 point,Tue Jul 21 18:26:09 2015 UTC,The video editors I know all still use CRT displays.
hardware,3e1zsb,amozetryn,39,Tue Jul 21 17:18:17 2015 UTC,Because apparently no one appreciates the extra vertical pixels for productivity or FoV. I also don't think a basic 16:10 should cost twice the amount of a 16:9 monitor either.
hardware,3e1zsb,Lee1138,18,Tue Jul 21 10:24:29 2015 UTC,"All about production volume really. Marketers pulled the wool over people's eyes and convinced them that ""FullHD"" was the thing to have. Production switched to 1080p because that was what the ""FullHD"", ""HD tv compatible"" marketing slogans were pushing and now the people who appreciate the added working space of a 16:10 monitor are screwed. As if the only thing computer monitors are good for is watching movies and TV content."
hardware,3e1zsb,irwiss,5,Tue Jul 21 11:42:53 2015 UTC,"There are monitor stands which can rotate 90 degrees and any recent GPU can rotate by 90 degrees, if you're interested in vertical, just saying"
hardware,3e1zsb,Lee1138,17,Tue Jul 21 13:07:41 2015 UTC,Yeah but then you have to sacrifice horizontal.
hardware,3e1zsb,irwiss,7,Tue Jul 21 13:15:35 2015 UTC,"If you're coding or doing any other engineering job you(or your employer) can use a multi monitor setup if display space is an issue, I find it more appealing than paying premium for a tiny slice of vertical space.  You could even argue that 16:10 is just as bad as 16:9, considering it superseeded 4:3 which had even more vertical.   There's even bullshit about golden ratio in discussions on aspect ratio so you can figure out it's more of a movie/display industry format issue than any kind of use case consideration"
hardware,3e1zsb,Lee1138,8,Tue Jul 21 13:28:08 2015 UTC,"True, I'm mostly just annoyed that what was essentially just fucking marketing buzzwords made them stop making 1920x1200 for mainstream."
hardware,3e1zsb,FairyFuckFluff,2,Tue Jul 21 13:42:47 2015 UTC,"I currently have an old Samsung 1920x1200 and would love to update to something newer, but all of the options are either 1080p or 1440p. No in-between.   I can't really afford a 1440p and can't bare to look at a 1080p screen. I guess I need to keep saving. Fml."
hardware,3e1zsb,Guanlong,5,Tue Jul 21 18:24:19 2015 UTC,"In most games, the extra vertical pixels don't increase the FoV. In fact, the vertical FoV stays the same and the horizontal FoV decreases."
hardware,3e1zsb,amozetryn,3,Tue Jul 21 14:59:00 2015 UTC,"But you're able to increase the horizontal FoV to compensate for that, which increases what you're able to see vertically."
hardware,3e1zsb,Have_you_tried_this,3,Tue Jul 21 15:19:55 2015 UTC,"Only on some games. What the original poster said is right. On most MOBAs for example, you have a smaller view at 16:10"
hardware,3e1zsb,Dark_Crystal,3,Tue Jul 21 15:54:03 2015 UTC,16:10 is perfect for portrait as a 2nd or 3rd display for doing lots of text editing/reading/programing. 16:9 is too narrow for that.
hardware,3e1zsb,kmartburrito,13,Tue Jul 21 17:32:21 2015 UTC,"There is, actually.  I have a ViewSonic vx2265wm 120hz monitor,  one of the first 120hz panels on the market.  Near 0ms input lag and phenomenal for my purposes.  It's 1680x1050 and 22"", but was all that was available at that time."
hardware,3e1zsb,Dresdof,3,Tue Jul 21 11:41:55 2015 UTC,"I had a Samsung 2233RZ with almost the same specs until very recently, one of the very firsts monitor for 3D Vision.   I wonder how much your ViewSonic cost you.  Edit: spelling."
hardware,3e1zsb,kmartburrito,2,Tue Jul 21 13:50:12 2015 UTC,"I want to say it was nearly 400, wasn't cheap, but have had it since battlefield 3 came out, which is quite a few years. Definitely got my money's worth I feel.  It also supports Nvidia 3d vision, although I've only had AMD cards,  go figure."
hardware,3e1zsb,Dresdof,2,Tue Jul 21 14:58:58 2015 UTC,"3D Vision hasn't been that awesome. It's quite resource hungry and the only games I really enjoyed with it were the Arkham series. Since Arkham Knight didn't bother with it, I'm not thrilled anymore.   120Hz still worth it."
hardware,3e1zsb,Logical1ty,1 point,Tue Jul 21 17:17:56 2015 UTC,I have that one. It's superior in every way except color quality  to my 1080p 120Hz Hans-G monitor (3ms input lag on the latter.. very noticeable).
hardware,3e1zsb,Edmundoh,11,Tue Jul 21 21:29:35 2015 UTC,"MFW going back to 16:9  I feel you man, just not popular enough, someday hopefully though."
hardware,3e1zsb,SSChicken,5,Tue Jul 21 13:56:26 2015 UTC,"Oh you guys and your 16:10. We need more width, not more height! Though more pixels in both is preferable."
hardware,3e1zsb,psycho202,1 point,Tue Jul 21 18:35:12 2015 UTC,How about we put two of those screens in a single enclosure? That would most likely satisfy the guys.
hardware,3e1zsb,psycho202,5,Tue Jul 21 18:47:50 2015 UTC,"Goddamn, the only reason I sometimes like 16:9 being the norm is when I'm on Twitch or YouTube and put the video in theater mode, only to find it can take the whole width while still keeping the top and bottom bars!  I call it the letterbox with benefits!"
hardware,3e1zsb,mmencius,2,Tue Jul 21 14:18:23 2015 UTC,"Yeah, I've got the video in pop-out on a 1680x1050 monitor, it's pretty much a perfect fit :)"
hardware,3e1zsb,Darkstryke,0,Tue Jul 21 18:47:06 2015 UTC,Dude go back to 4:3 and be happy.
hardware,3e1zsb,elessarjd,9,Tue Jul 21 18:24:29 2015 UTC,Thank people that settled for 16:9 TN garbage for the last 5 years.
hardware,3e1zsb,psycho202,4,Tue Jul 21 14:31:14 2015 UTC,"I think this is a case of the suppliers not offering many alternatives.  All the newer monitors in stores went to 16:9 and general consumers outweigh power consumers.  People just buy what's on the shelf or what comes with their PC and suppliers/manufacturers went the easier, more profitable route with 16:9.  Bastards."
hardware,3e1zsb,logged_n_2_say,2,Tue Jul 21 15:04:00 2015 UTC,"To be honest, when I can buy 3x 1080p TN garbage for the same price as a single good 1440p or 120Hz monitor, I know which one I'd chose."
hardware,3e1zsb,Oafah,5,Tue Jul 21 18:48:41 2015 UTC,"Two niches don't always make a product.   The panels are mostly seen as productivity boosts for certain tasks, and one of them isn't gaming.  With that said, 16:10 4lyfe."
hardware,3e1zsb,1kca,3,Tue Jul 21 11:37:46 2015 UTC,"The answer to questions like this is always the same: there isn't enough demand.  Bottom line is, you need more than one loud, dissatisfied voice to drive market change."
hardware,3e1zsb,Oafah,3,Tue Jul 21 12:46:23 2015 UTC,"I know you're right but that reasoning really annoys me.. If people aren't buying something like that, it's because they aren't there to be bought. I'd buy a 16:10 120Hz monitor before a 16:9 144Hz and I'd be happier. Just like local split screen on PC. Few people use it because few games bother to include it."
hardware,3e1zsb,1kca,2,Tue Jul 21 16:38:28 2015 UTC,"16:10 monitors were a thing but a few years ago, and people vastly preferred 16:9. This is exactly why there are so few 16:10 monitors now.  As for local split-screen co-op, I have well over 100 Steam games that support it. It might not be a priority for some developers, but with couch gaming becoming more and more popular, it's on the rise."
hardware,3e1zsb,Hariooo,2,Tue Jul 21 16:41:19 2015 UTC,"I hope so. I wish it was more of a priority for the larger devs tho. The fact that you'll find split screen in console versions of games that are also on PC, but with the feature straight up removed, doesn't make sense to me.   As for the monitors, I know it's unlikely but hopefully there's a resurgence. A 30"" 16:10, 1440p 120Hz IPS monitor.. I'd buy 3 (somehow)"
hardware,3e1zsb,elessarjd,2,Tue Jul 21 16:45:48 2015 UTC,"Seeing there is still a strong cult around 16:10 even today, though arguably getting smaller as time goes, I was under the impression that by 2015 something would have shown up.   So demand is shrinking but you expect there to be an entirely new model designed for this dwindling market?  Like read what you write dude"
hardware,3e1zsb,logicow,4,Tue Jul 21 13:54:35 2015 UTC,"""Arguably"" is the key word. I don't have sources proving that demand for 16:10 is shrinking. This is part of the reason I made this post, I'd love to know if people truly got bored of 16:10 or if its just due to manufacturers pushing 16:9 so hard that 16:10 is fading away.  As stated in the OP, 16:10 displays like the Dell U2415 are still coming out, they just target other consumers (in this case: people seeking IPS panels)."
hardware,3e1zsb,generalgranko,3,Tue Jul 21 14:06:10 2015 UTC,"Don't mind them, he was being a jerk.  Anyway, I totally feel your pain man.  It's like I'm taking crazy pills that wanting more vertical space on my monitor puts me in the minority.  I assume I'll have to bite the bullet eventually and get a 16:9 if I want FreeSync/GSync/120hz.  But I'm not happy about it damnit!"
hardware,3e1zsb,Dark_Crystal,1 point,Tue Jul 21 14:55:23 2015 UTC,some CRT monitors can get to pretty high refresh rates at lower resolutions ;)
hardware,3e1zsb,mmencius,1 point,Tue Jul 21 18:54:57 2015 UTC,"Perhaps because it's not worth it? There is almost literally no 16:10 market. Same as 4:3, I'd take 3x 4:3 144hz monitors in a heartbeat, but nothing like that will ever happen."
hardware,3e1zsb,Stingray88,2,Tue Jul 21 11:40:58 2015 UTC,16:10 masterrace.
hardware,3e1zsb,mmencius,1 point,Tue Jul 21 17:31:26 2015 UTC,Why are there no 120 Hz 4:3 displays?
hardware,3e1zsb,Stingray88,1 point,Tue Jul 21 18:22:18 2015 UTC,I think you mean 5:4...  5:4 was the popular square-ish monitor aspect ratio. 4:3 was the aspect ratio used by NTSC televisions.
hardware,3e1zsb,Pesceman3,1 point,Tue Jul 21 20:21:10 2015 UTC,"... are you sure? I thought for most of the 90s the common computer resolution was 640x480 and around the turn of the century it was 800x600, and this lasted until 1680x1050 took over. Then 1920x1200 and 1920x1080."
hardware,3e1zsb,ICreatedSomeClones,1 point,Tue Jul 21 21:01:46 2015 UTC,"For LCD monitors, yes. The most common computer resolution I've seen before 16:9 became popular was 1280x1024.  I worked IT for a big university and two larger companies, and pretty much all of the non-widescreen LCD monitors I came across were 1280x1024. Same way for the older monitors at the production company I work for now. I don't know if I ever seen a 4:3 LCD monitor to be honest.  For CRT monitors, you're probably right. I was thinking of LCD when I made my comment. Don't think of CRT much anymore hah."
hardware,3e1zsb,Aerundel,-9,Tue Jul 21 21:19:44 2015 UTC,16:10 displays are not generally intended for gaming while 120+Hz displays are usually for gaming.  16:9 is superior for gaming because a 16:10 display will cut off sections on each side of the screen.
hardware,3e1zsb,Aerundel,8,Tue Jul 21 14:32:55 2015 UTC,Uh I game on a 1920x1200 display every day and it's awesome. Every modern game runs fine on 16:10. I don't see how 16:9 is superior seeing that it has less vertical resolution.
hardware,3e1zsb,Pesceman3,6,Tue Jul 21 15:13:47 2015 UTC,"Technically you may lose some peripheral vision depending on the game's implementation. You might see more left/right and less up/down with 16:9, compared to 16:10. But then if you can adjust FOV you can mitigate this.  Some games infamously chopped the top and bottom off a 4:3 scene to make the 16:9 view. So you wouldn't get any added peripheral vision, AND you'd miss out on what's going on above and below. I think Bioshock initially did that before a patch, or Dead Space. Can't remember. When I was playing Company of Heroes heavily I switched to a 4:3 display because widescreen just didn't show as much of the map in a top-down view. Actually, 5:4 afforded the best view in that case."
hardware,3e1zsb,Aerundel,3,Tue Jul 21 15:53:23 2015 UTC,"16:9 is superior for gaming because a 16:10 display will cut off sections on each side of the screen.   That's...very inaccurate. There are plenty of games that support 4:3, 16:9, 16:10, 5:4, 21:9, and a variety of multi-monitor ratios without black bars. It's entirely up to the developer, and even then there are utilities available to bypass locks put on aspect ratio."
hardware,3e1zsb,Seclorum,2,Tue Jul 21 15:22:17 2015 UTC,"I'm not talking about black bars.  Play a game in 16:9 and then in 16:10.  You will not get increased vertical FOV with 16:10, instead you will get decreased horizontal FOV because the height is simply blown up to fill the screen.  As a result the sides are cut off.  http://i.imgur.com/RGVYtok.png"
hardware,3e1zsb,Pesceman3,2,Tue Jul 21 15:56:41 2015 UTC,"Depends on the implementation. Some games would cut the top and bottom of the 4:3 scene to create the 16:9 or 16:10 (not saying that's a good thing, but it has happened). You can't definitively say one aspect ratio is better than the other until you see how the game handles it."
hardware,3e1zsb,Seclorum,0,Tue Jul 21 16:05:30 2015 UTC,"That's entirely because of Lazy developers, not a physical limitation of the hardware.   They do it because just cutting off the sides is 'easier' than forcing the engine to render a different ratio properly."
hardware,3e1zsb,Pesceman3,0,Tue Jul 21 16:04:11 2015 UTC,Unfortunately it's the case with most AAA games that I've played.
hardware,3e1zsb,Seclorum,1 point,Tue Jul 21 16:06:08 2015 UTC,Most likely all Console Ports.   Most games I play all have native support and render it properly without having to fuck with FOV shit.   There is a reason why having a changable FOV is important.
hardware,3e1zsb,elessarjd,1 point,Tue Jul 21 16:34:49 2015 UTC,Yea that console port CSGO
hardware,3e1zsb,Pesceman3,-1,Tue Jul 21 19:10:03 2015 UTC,Which is funny because CS Source didn't have an issue.
hardware,3e1zsb,elessarjd,1 point,Tue Jul 21 19:12:41 2015 UTC,"16:9 and 16:10 are the same width, so the sides can't be cut off."
hardware,3e48nx,twizzlebizzle21,0,Tue Jul 21 20:56:12 2015 UTC,Pc perspective is great
hardware,3e48nx,signet111,6,Tue Jul 21 21:39:47 2015 UTC,Anandtech just seems whenever they feel like it.   More often than not they do articles when NDAs have dropped.
hardware,3e0ak1,jimmyslaysdragons,123,Mon Jul 20 23:38:29 2015 UTC,"one of the engineering samples of the product has been overclocked to 5.20GHz  it is unclear whether the chip was stable enough to run benchmarks   A sample size of one, where they didn't even do stability testing, simply doesn't tell us much of anything about the overclocking potential of the entire line.  I would love if this chip were representative, but we can't know that yet."
hardware,3e0ak1,dhds83,39,Mon Jul 20 23:48:06 2015 UTC,"Seems like someone pushed an engineering sample 4790k (with 2 cores disabled) to 7Ghz. So unless we can get more info on the cooling and other factors, this is more or less meaningless."
hardware,3e0ak1,Exist50,33,Mon Jul 20 23:55:45 2015 UTC,i see your 7 ghz and raise you 7.1+ ghz  my computer is funky.... it shouldn't be able to do that not even for a second.   edit: if any of you have an explanation for this please let me know.... not one of my group of techs  know how this happens. and id like to know.
hardware,3e0ak1,TopAce6,30,Tue Jul 21 00:11:51 2015 UTC,How come Heather's so shy?
hardware,3e0ak1,CJKay93,18,Tue Jul 21 02:07:24 2015 UTC,my wife just wont talk online... its irrational and silly but shes real shy lile that. i cant give a good reason though... cause imo there isn't one.
hardware,3e0ak1,TopAce6,6,Tue Jul 21 02:34:50 2015 UTC,"She's not alone. I got my GF into WoW briefly, and she was creeped out any time someone else in game started talking to her. She didn't want to communicate with anyone online she didn't know."
hardware,3e0ak1,Stingray88,6,Tue Jul 21 08:08:27 2015 UTC,What cooling do you have wtf
hardware,3e0ak1,dylan522p,11,Tue Jul 21 03:51:30 2015 UTC,"generic air cooler that can spin up to jet engine speeds whenever my processor decides to do that.  nobody knows how...becuase its a stock HP Motherboard with a locked down bios, no overclocking options even exsist, but multiple programs confirm that for a short time it gets to insane OC, i see 5ghz regularly.... which is still technically Impossible. due to shitty potato PC and locked bios."
hardware,3e0ak1,TopAce6,23,Tue Jul 21 03:59:18 2015 UTC,"It's almost certainly just a software bug. I think I've seen that myself in that program, actually."
hardware,3e0ak1,deadhand-,7,Tue Jul 21 04:07:09 2015 UTC,"we've seen it in a few other programs besides that one..., my best guess a faulty sensor?"
hardware,3e0ak1,TopAce6,2,Tue Jul 21 04:15:55 2015 UTC,"Could be a few things, hard to say."
hardware,3e0ak1,deadhand-,1 point,Tue Jul 21 05:01:56 2015 UTC,The current version of hwinfo is 5.02 and they probably fixed that
hardware,3e0ak1,thock2,1 point,Tue Jul 21 15:48:30 2015 UTC,Benchmark it and compare it
hardware,3e0ak1,dylan522p,2,Tue Jul 21 04:02:45 2015 UTC,"I would, but can you really Benchmark a 1 second burst? it only seems to happen when running a few select programs that push the pc absurdly hard. basically if I try to run gta v or ark on epic (1fps on my pc) it happens for a very brief time im assuming as it would have to burn up if it stayed that high."
hardware,3e0ak1,TopAce6,-2,Tue Jul 21 04:18:53 2015 UTC,Show your Temps
hardware,3e0ak1,dylan522p,2,Tue Jul 21 05:17:02 2015 UTC,I've seen the same thing with my 4690K running at 4.5ghz. Multiple programs report it running at 5.71ghz but I don't think it actially ever runs that fast.
hardware,3e0ak1,TopAce6,1 point,Tue Jul 21 15:18:50 2015 UTC,"my best guess, is that it hits that speed for 1cycle? I dont know what the refresh rate on the hardware monitors is but thats all I can think of."
hardware,3e0ak1,chandleya,1 point,Tue Jul 21 15:26:11 2015 UTC,"Hello, FloridaMan"
hardware,3e0ak1,TopAce6,1 point,Tue Jul 21 13:17:03 2015 UTC,"yes and say hello to florida computer,  I think we figured it out, my PC is on PCP."
hardware,3e0ak1,americosg,4,Tue Jul 21 13:39:26 2015 UTC,"The article makes it look it was done on all 4 cores, wich is way more dificult than doing the same thing in only 2. But yeah this is meaningless until we can se some benchmarks and stability tests."
hardware,3e0ak1,Exist50,4,Tue Jul 21 00:14:14 2015 UTC,1.35v seems like a very high voltage for 14nm. Not something you'd want to do on air cooling or for anything long term.
hardware,3e0ak1,CRTsdidnothingwrong,-5,Tue Jul 21 01:13:03 2015 UTC,"Don't be such a pussy.  How often do you hear about burnt out processors? Oh that's right, it never happens."
hardware,3e0ak1,CJKay93,21,Tue Jul 21 01:58:25 2015 UTC,That's because 99% of people don't set very high voltages on air cooling.
hardware,3e0ak1,CRTsdidnothingwrong,11,Tue Jul 21 02:08:47 2015 UTC,That's because they're pussies.
hardware,3e0ak1,PhilipK_Dick,32,Tue Jul 21 02:09:51 2015 UTC,I like your style - but I wouldn't let you near my BIOS...
hardware,3e0ak1,DoctorWorm_,10,Tue Jul 21 03:03:52 2015 UTC,"People killed the first 32nm chips, clarkdale, all the time on air. It only took about 1.5v."
hardware,3e0ak1,mikemol,4,Tue Jul 21 03:10:13 2015 UTC,"Don't be such a pussy.  How often do you hear about burnt out processors? Oh that's right, it never happens.   So, years ago for the college computer club, I disassembled an old 486 DX4 66MHz. (Pentium 4s were the common processor by this point.)  When I reassembled it and turned it on, nothing happened. Turned it off, checked all the connections, turned it back on. Still nothing.  We all stood around chatting for a few minutes, until I smelled smoke. Then I saw ir, coming out of the computer.  Disconnect power, open 'er up, and--whew, did that ever stink! But we couldn't see what caused it. So we turned the computer back on. After a while, a thin tendril of smoke rose from the CPU.  See, once upon a time, processors weren't keyed, and it was possible to insert them 90, or even 180 degrees from what was correct. And that's what happened when I reassembled that system..."
hardware,3e0ak1,PubliusPontifex,5,Tue Jul 21 03:05:01 2015 UTC,insulator depletion and carrier migration.  Don't be such an ignorant moron.
hardware,3e0ak1,CRTsdidnothingwrong,0,Tue Jul 21 05:11:09 2015 UTC,Find a single case of those demonstrated in a modern Intel processor from non-ln2 overclocking.
hardware,3e0ak1,Exist50,4,Tue Jul 21 14:51:31 2015 UTC,Of course it happens. Most people just top out on cooling performance before they can reasonably feed the chip enough voltage to kill it.
hardware,3e0ak1,CRTsdidnothingwrong,-7,Tue Jul 21 02:08:39 2015 UTC,"It does not though, there a few select examples of it happening when people have literally tried to see if they It was possible but there's no known examples of it happening in general. It's the overclockers boogeyman; Everyone fears it but nobody's actually seen it."
hardware,3e0ak1,Exist50,2,Tue Jul 21 02:11:26 2015 UTC,"You've said this many times before, but it just isn't true. Just to start with an extreme example, professional overclockers don't try to kill their chips, but that's exactly what they end up doing."
hardware,3e0ak1,CRTsdidnothingwrong,-3,Tue Jul 21 02:13:52 2015 UTC,"As I said, it's been observed in very extreme conditions but it just doesn't happen in regular overclocking yet people still act like 1.4v is the danger zone.  I just hate bitch ass misinformation, nobody's ever killed a cpu with 1.4v"
hardware,3e0ak1,barthw,6,Tue Jul 21 02:18:55 2015 UTC,"I just hate bitch ass misinformation, nobody's ever killed a cpu with 1.4v   Oh the broscience. Pretty sure there are dozens of examples to prove you wrong, besides 1.4v is a big a difference on 14nm vs 22nm or even 32nm."
hardware,3e0ak1,dylan522p,3,Tue Jul 21 06:16:15 2015 UTC,My sandy i7 isn't as stable on 1.45 volts and I had to tone it down to lower clocks twice for a total of 300 mhz then what it used to run. 22nm takes a lot less voltage and 14 even more so.
hardware,3e0ak1,Exist50,9,Tue Jul 21 03:55:06 2015 UTC,"You do realize that node shrinks decrease the voltage you need, right? Sure, 1.4 won't kill a 32nm CPU, and won't kill a 22nm CPU quickly (long term damage is likely, though), but you absolutely cannot say the same of a 14nm chip, especially considering the results for overclocking Broadwell."
hardware,3e0ak1,zeropluszero,1 point,Tue Jul 21 02:23:02 2015 UTC,"I burned out my old Core 2 Dup E5200 in an attempt to hit 4.1ghz before I swapped it for a new Q8300. I don't remember the exact voltages but it was way over the recommended maximum. I got into boot and it ran Tripcode Explorer (I've yet to find a bench that stresses a CPU as hard as this program) for ~15 seconds until it crashed. I got it to post long enough for the BIOs to auto-reset to default settings but it was never able to stay running for more than ~15-20 seconds at stock clocks and voltages after my extreme OC attempt.  Have I ever heard of someone burning out a CPU so bad that it was completely inoperable? No, but I know of many examples where extreme voltages caused permanent damage aside from my own anecdote."
hardware,3e0ak1,refto,-3,Tue Jul 21 15:27:33 2015 UTC,Have all of my upvotes.
hardware,3e0ak1,tepkel,2,Tue Jul 21 08:45:36 2015 UTC,"I suppose this is an improvement since 5775c was shown to be overclocked to 5Ghz in May: http://www.kitguru.net/components/cpu/anton-shilov/intel-core-i7-5775c-broadwell-overclocked-to-5ghz-with-air-cooling/  Everyone got excited until the letdown of actual real life, where most that you can reliably squeeze stable from 5775c is 4.2Ghz.  So, 6700k will be about as fast as Haswell, if we are lucky."
hardware,3e0ak1,Seclorum,1 point,Tue Jul 21 17:06:44 2015 UTC,I hear they got one up to 1.21 gigawatts
hardware,3e0ak1,SecretDragoon,26,Tue Jul 21 15:49:15 2015 UTC,Pretty thin rumor here.    No Data on stability. No Data on cooling used.  Nothing but a CPUZ screenshot for verification...
hardware,3e0ak1,na1nsxr,4,Tue Jul 21 00:07:10 2015 UTC,I mean there should be a validation report. This is pretty silly.
hardware,3e0ak1,Seclorum,7,Tue Jul 21 00:30:55 2015 UTC,"Actually it's all there if you bother to look.  1.35v was good enough to boot 5.2GHz but not much else.  It took 1.285v to boot 5GHz, 1.312v to complete a SuperPi 32M run.  Cooler was a Gelid The Black Edition mounted using the Gelid thermal compound.  Voltages were taken with a multimeter as CPU-Z's current version doesn't report voltage correctly yet, according to the HK site."
hardware,3e0ak1,unsaltedbutter,2,Tue Jul 21 08:09:26 2015 UTC,None of that was in the article. At all. Got any verification?
hardware,3e0ak1,Oversidee,8,Tue Jul 21 15:59:22 2015 UTC,"What are they blurring in the CPU-Z window titlebar? My CPU-Z 1.72 x64 doesn't have anything in the title other than ""CPU-Z""."
hardware,3e0ak1,unsaltedbutter,3,Mon Jul 20 23:56:05 2015 UTC,"You can choose w/e info you want displayed on the title bar like freq, voltage, ram speed, etc."
hardware,3e0ak1,sk9592,3,Tue Jul 21 04:16:56 2015 UTC,aha! Thanks.
hardware,3e0ak1,bphase,3,Tue Jul 21 04:25:45 2015 UTC,"So basically, they blurred out anything that allows us to call bullshit on this overclock?"
hardware,3e0ak1,roro_mush,7,Tue Jul 21 13:35:18 2015 UTC,Don't forget the 5GHz Broadwell  Then retail samples ended stopping at 4.2-4.4 or so. I wouldn't get my hopes up.
hardware,3e0ak1,PhilipK_Dick,18,Tue Jul 21 10:30:15 2015 UTC,"Likely a 'Golden Chip' and most retail SKU won't get anywhere close to that, Intel pulled this same bait and switch when Haswell was released."
hardware,3e0ak1,refto,8,Tue Jul 21 01:33:53 2015 UTC,The fact that this same trick was used last generation makes me think either their marketing department lacks imagination or truly underestimates overclockers.
hardware,3e0ak1,BlayneTX,1 point,Tue Jul 21 03:00:16 2015 UTC,"Not only Haswell, but also Broadwell desktop too. I am really really getting pissed about this(not only the deception but also the lack of progress , ie smaller node -> better IPC -> smaller clock speed -> slightly faster overall performance)"
hardware,3e0ak1,zeropluszero,13,Tue Jul 21 17:09:07 2015 UTC,I want to get hype but remember that Devil's Canyon was rumored to be 5.5ghz on air. http://www.extremetech.com/computing/183708-overclockers-push-new-devils-canyon-haswell-to-5-5ghz-on-air-6-4ghz-with-ln2
hardware,3e0ak1,RadioactivePandaBear,6,Tue Jul 21 03:02:10 2015 UTC,HKEPC. Team of known overclocking cheaters.
hardware,3e0ak1,PhilipK_Dick,4,Tue Jul 21 08:45:06 2015 UTC,"I remember when 5ghz was only attainable using liquid nitrogen, what a world we live in now."
hardware,3e0ak1,risingyeast,11,Tue Jul 21 02:45:25 2015 UTC,5GHz is still a tough nut to crack for everyday use.
hardware,3e0ak1,Teethpasta,11,Tue Jul 21 03:02:38 2015 UTC,Except my old 2500k.  I regret selling it everyday. My 3770k shits the bed above 4.4Ghz. That i5 gave zero fucks at 5Ghz and the voltage and temps were low under a corsair a70. I sold it not knowing what I had.  Still crying to anyone that will listen.
hardware,3e0ak1,zeropluszero,3,Tue Jul 21 03:59:14 2015 UTC,Why would you sell a golden chip?
hardware,3e0ak1,bphase,5,Tue Jul 21 05:08:28 2015 UTC,5ghz golden sandy bridge? Lol.
hardware,3e0ak1,risingyeast,3,Tue Jul 21 08:43:50 2015 UTC,"5GHz was relatively common with Sandy. Seen them go above that often. I wouldn't call it average as I don't have the info, but it wasn't really golden either."
hardware,3e0ak1,atakagi,3,Tue Jul 21 10:28:17 2015 UTC,"It was back when bestbuy price matched anyone, anywhere if you were nice to the person on the phone.  Got the i7 for $220 a few months after ivybridge launched and sold the i5 for $160 to a friend for his first real gaming rig.  Thought it was a good upgrade at the time but I'm sad that chip is just sitting in his rig at stock clocks."
hardware,3e0ak1,SocksForBreakfast,1 point,Tue Jul 21 05:22:05 2015 UTC,I feel you. My 3570k barely manages to take 4.6@1.4v.
hardware,3e0ak1,lancypancy,2,Tue Jul 21 10:35:41 2015 UTC,I still have my 2500k and was thinking of finally upgrading when Skylake comes out.  Are these old 2500k's still a solid contender to Intel's newer i5s?  Is it only with certain batches?  This is the first I've heard of this.
hardware,3e0ak1,LegatusIgnatius,2,Tue Jul 21 07:27:00 2015 UTC,It sure is. Get an all in one liquid cooler and crank that bitch to 4.9ghz all day every day.
hardware,3e0ak1,Teethpasta,1 point,Tue Jul 21 10:32:00 2015 UTC,"My 2500k does 4.3 without voltage tampering. Never found out what to do, but if I try 4.4 my system crashes. Any tips on what I should do to push it further? It's my current bottleneck."
hardware,3e0ak1,Crossbeau,4,Tue Jul 21 04:50:27 2015 UTC,Raise the voltage by a small amount
hardware,3e0ak1,CRTsdidnothingwrong,1 point,Tue Jul 21 05:10:21 2015 UTC,Your making me want to get a beast water cooling setup and move off of mITX and crank my 2600k :'(
hardware,3e0ak1,PhilipK_Dick,3,Tue Jul 21 14:55:14 2015 UTC,"This would be Skylake's redeeming factor, we haven't had a beast overclocker since Sandy Bridge and they really are the funnest generations, so much room to play around in.  I hope we get a cheap Pentium model we can use as throwaways and just dump voltage into until they lose the will to live."
hardware,3e0ak1,LiberDeOpp,5,Tue Jul 21 01:58:07 2015 UTC,I can't see how this will be physically possible on a 14nm process unless there is another next gen thermal material.  It becomes harder to cool the smaller and tighter packed everything is.
hardware,3e0ak1,PhilipK_Dick,3,Tue Jul 21 03:02:02 2015 UTC,The smaller chips also use less power and moving the FIVR off die might help a little with OC. 5.2 on air isn't that hard of an OC if it's a cool room with test bench.
hardware,3e0ak1,Fastfingers_McGee,1 point,Tue Jul 21 05:05:52 2015 UTC,I'm not an extreme overclocker - more of always trying to tweek the best 24/7 I can without feeling like I'm about to melt the chip or degrade it.  I heard that 5.2 was virtually impossible on air with Haswell or ivy.  Not true?
hardware,3e0ak1,BlayneTX,2,Tue Jul 21 12:32:39 2015 UTC,"Wait a minute, I might be reading that wrong cause I use real temp but, 5.2 ghz @ .835v and 95w TDP? that's a bit too good to be true."
hardware,3e0ak1,Fastfingers_McGee,1 point,Tue Jul 21 02:38:02 2015 UTC,That's at idle. The article says 1.35v.
hardware,3e0ak1,charles2531,1 point,Tue Jul 21 02:56:08 2015 UTC,"oh okay, maybe I should read next time."
hardware,3e0ak1,Exist50,2,Tue Jul 21 03:30:25 2015 UTC,Do they have some cores disabled? I thought Intel was going to add more cores to their next gen CPUs.
hardware,3e0ak1,Seclorum,3,Tue Jul 21 03:51:34 2015 UTC,Not on the 115x platform.
hardware,3e0ak1,Exist50,0,Tue Jul 21 03:53:33 2015 UTC,"The chip in question is functionally identical to the 4790k, but with a smaller process node used and the saved space from that used to include a much bigger IGPU implementation. Also they removed the FIVR.   It appears in the 'mainstream' chip lineup, there will be no more than 4 cores with hyperthreading. You need to look to the -E series chips for more cores.  This is probably because very little that normal consumers are doing benefits at all from more cores. And on the -E chips, you get more cores and they are not that much more expensive than the regular consumer chips."
hardware,3e0ak1,Seclorum,7,Tue Jul 21 03:57:23 2015 UTC,There's more architectural changes than just the FIVR.
hardware,3e0ak1,Begoru,0,Tue Jul 21 04:11:59 2015 UTC,"Some yes, but it remains to be seen what performance impact those have if any yet."
hardware,3e0ak1,MilkyTones,1 point,Tue Jul 21 04:37:36 2015 UTC,Some?  They're made by two different teams.  The 6700k is made by the Sandy Bridge guys so I'm very optimistic.
hardware,3e0ak1,BlayneTX,2,Tue Jul 21 20:06:03 2015 UTC,And a while ago people said Broadwell went up to 5.0 GHz.
hardware,3e0ak1,Exist50,1 point,Tue Jul 21 06:10:56 2015 UTC,I'm assuming that these aren't going to be soldered IHS and will need to be delidded again?
hardware,3e0ak1,Seclorum,3,Tue Jul 21 02:30:13 2015 UTC,"No solder, but likely the Devil's Canyon TIM."
hardware,3e0ak1,tbonanno,2,Tue Jul 21 03:41:59 2015 UTC,I believe they are only going to solder the -E chips from now on.
hardware,3e0ak1,alpacIT,2,Tue Jul 21 03:58:29 2015 UTC,Does soldering give better cooling? And which ones have been soldered before?
hardware,3e0ak1,Seclorum,3,Tue Jul 21 07:12:44 2015 UTC,Sandy Bridge was the last line to be completely soldered from what I recall. And yes a solder between the lid and die increases heat transfer which yields cooler temps.
hardware,3e0ak1,lucafusi,1 point,Tue Jul 21 13:18:35 2015 UTC,Yes soldering gives better cooling. It basically mechanically connects the top of the core with the heat spreader in much the same way as TIM does but it's metal so it has much higher conductivity.   Currently only -E chips are soldered but I believe all chips before Ivy Bridge series were also soldered.
hardware,3e0ak1,Le_rebbit_account,1 point,Tue Jul 21 16:01:19 2015 UTC,http://upload.vstanced.com/images/2014/05/27/post-30824-Jack-Nicholson-Creepy-Nodding-SRXv.gif
hardware,3e1kab,CEKPETHO,7,Tue Jul 21 06:31:29 2015 UTC,"I think this is pretty cool. I don't think this will cool all that much better than air because it's only a 2x120 rad for 2 blocks, but I'm glad someone has made it. Plus, it's only a bit more money than other AIO coolers."
hardware,3e1kab,amozetryn,3,Tue Jul 21 10:05:32 2015 UTC,"Yeah, I'm really a fan of the new AIO technology and how it's being embraced. I've wanted something like this for a while. Now we just need one with better rads."
hardware,3e1kab,foxtrot1_1,1 point,Tue Jul 21 17:22:56 2015 UTC,"the price is really cheap, makes me wonder about the quality a bit. its a good looking setup. i too am a bit suspicious of how well this would cool. i could see it working well with like a gtx970 and nonclocked cpu, but beyond that? at least with a 280mm rad could be a big improvement? def cool tho the advances in aio cooling.."
hardware,3e1kab,ddeuced,2,Tue Jul 21 19:22:40 2015 UTC,Took someone long enough.  Going to suck for overclocking though.  Would that radiator even offer thermal advantages over stock air coolers?
hardware,3e1kab,innerknowing,2,Tue Jul 21 13:18:34 2015 UTC,"We need a kickstarter for an expandable AIO that has some sort of seal technology you use then you can remove the tubing with no leaks and add new things to the loop, then it would have an easy to use fill/bleed port."
hardware,3e1kab,hdshatter,2,Tue Jul 21 21:06:16 2015 UTC,the idea seems good but I think the thin rad is not enough  they could've created a bigger octopus by adding another 120mm rad for rear exhaust
hardware,3e1kab,zmeul,1 point,Tue Jul 21 10:22:00 2015 UTC,"The tubing lines between it and the top rad could pose spacing issues. It'd be better to target the common audience of 2x120, even though a 120+240 or a 360 would cool better."
hardware,3e1kab,amozetryn,-2,Tue Jul 21 10:26:11 2015 UTC,it could go: GPU -> 120rad -> CPU -> 240rad -> GPU
hardware,3e1kab,zmeul,0,Tue Jul 21 10:37:05 2015 UTC,Very good idea but they missed the mark.  It should have one pump only and the option to add another radiator. Combine that with relatively low temps and noise and they'd have a bestseller on their hands.
hardware,3e1kab,JustaPassanger,-23,Tue Jul 21 17:12:56 2015 UTC,"Having owned some twenty PC's, about 3/4 of which I've built myself, since 2000 I can tell you with quiet confidence that water cooling has zero advantage over air cooling.  And that in fact it's worse because it's problematic. There isn't a cooling pump on the planet that doesn't start grinding after a few weeks.  It gets louder. And louder. AND LOUDER.  And then the fucking pump fails and you're awakened in the middle of the night to the death beeps of your PC thermal faulting."
hardware,3e1kab,negative_commentary,14,Tue Jul 21 07:21:35 2015 UTC,"Seems like you've had some really bad issues with water-cooled PCs. Regarding your point about ""zero advantage over air cooling"", water cooling isn't for everyone. For most people, it has no tangible benefits."
hardware,3e1kab,0x60,7,Tue Jul 21 09:25:08 2015 UTC,"I have built many computers myself, and many of them have been liquid cooled and I can say with confidence that you are incorrect. I have never had a pump begin grinding, or becoming loud at all, even after years of use. So to say that there isn't a ""single pump on the planet"" is quite ignorant. If this were the case, no one would be doing it.   While I understand that for some it doesn't have its advantages, for some it does. It all depends on the user."
hardware,3e1kab,Dartulius,6,Tue Jul 21 10:23:31 2015 UTC,What water coolers have you used? Most shouldn't be anything like what you say. There's not a massive difference between AIO and air coolers but for a proper custom loop it can cool much better.
hardware,3e1kab,bentan77,6,Tue Jul 21 08:57:31 2015 UTC,"Sorry buddy, you seem like a nice guy.  But you flat out suck at watercooling.  I built watercooled systems in 2002 using pond pumps that were still so quiet a year after install that I didn't know if the computer was powered on except for the HDD light flickering.  I also can overclock both the GPU and CPU by 20% and have their load temps sit in the low 50s.   Is that not an advantage?"
hardware,3e1kab,innerknowing,4,Tue Jul 21 13:20:48 2015 UTC,that weird because my 10 year old universal pump Eheim 1048 is still as loud as the day I bought it. 120mm Noctua at 1000RPM is louder.
hardware,3e1kab,battery_collector,3,Tue Jul 21 10:44:42 2015 UTC,"Those damn Eheims were beasts.  Huge compared to today's pumps but quiet, high flow beasts."
hardware,3e2joe,truHistoryMF,4,Tue Jul 21 13:42:33 2015 UTC,"AMD and NVIDIA release standard designs for the PCBs, many manufacturers modify this. Look at a review using the standard NVIDIA GTX 960 and look at the PCB, techpowerup usually has good shots"
hardware,3e2joe,xtothemess,1 point,Tue Jul 21 14:51:29 2015 UTC,I checked there but they reviewed the full size evga card. Thanks for the tip though.
hardware,3e2joe,xtothemess,2,Tue Jul 21 15:08:49 2015 UTC,"What size did you want? They reviews both ASUS and EVGA models, maybe not the exact model you were looking for tho.."
hardware,3e2joe,plank_,1 point,Tue Jul 21 15:48:12 2015 UTC,Trying to see if the lay out of this card http://m.newegg.com/Product/index?itemnumber=N82E16814487093 is the same as this onehttp://www.techpowerup.com/reviews/ASUS/GTX_960_STRIX_OC/4.html
hardware,3e2joe,xtothemess,2,Tue Jul 21 16:21:01 2015 UTC,Maybe not. If you zoom in on the angled shots there's a capacitor to the left of the PCIe connector on the Asus board that doesn't appear to be there on the EVGA one.
hardware,3e2joe,zoson,1 point,Tue Jul 21 16:28:39 2015 UTC,"Yes I saw that too, but the memory chips seem to be in the same place, if you look at this image: http://www.evga.com/products/images/gallery/02G-P4-2962-KR_XL_4.jpg there is a memory chip where the bottom one is here: http://www.techpowerup.com/reviews/ASUS/GTX_960_STRIX_OC/images/front.jpg  I would say that the layout in terms of the memory is the same, but that the VRM components are different, but in the same place."
hardware,3e49ty,phi0x,1 point,Tue Jul 21 21:04:42 2015 UTC,"Words present in your post indicate that this might be a question or a request for help. If you review our rules you will notice that most questions don't belong here. This includes PC building questions and requests for tech support. If this is the case, please delete this post and resubmit it to either /r/buildapc or /r/techsupport. If this isn't the case, feel free to ignore this post.  I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns."
hardware,3e1efa,CEKPETHO,8,Tue Jul 21 05:25:02 2015 UTC,"That's a bit much.  Edit: Why wouldn't NVidia let their AIB partners modify the Titan X in such a way? I think this is cool and all (minus the gigantic price for ASIC binning which is dumb to me) but investing so much into a card that's not even 'the best' boggles my mind. I'd love to see some insane Titan X cards like this but I digress.  I just think that if this card is going to be listed for $1,050 there's no point in buying it vs the Titan X but the Titan X will never be as premium as the nicest 980 Ti's."
hardware,3e1efa,handlewithnocare,2,Tue Jul 21 14:47:26 2015 UTC,"It's marketing. They release the titan X and don't allow custom designs. This forces the titan to be thermally limited, but still provide a large increase in performance. This way, when the 980 ti releases, it outperforms the Titan X (980ti is not thermally limited meaning higher clocks than the Titan X) and at ""only $650"". This makes the customer think they're getting a fucking bargain when in fact, its price to perf ratio is in line with the 980's. It's smart marketing, if a bit manipulative."
hardware,3e1efa,aranurea,1 point,Tue Jul 21 19:26:20 2015 UTC,"It will still perform much better than a titan x for sure. If you are looking to buy one of these vs a titan, pick this one. You can overclock it to be way better(not because of the ASIC or anything, just the fact that it has a better cooler).  With that said, it is pointless to buy either this or the titan x. EVGA even says that asic is an estimate on OCability. You aren't even guarenteed to get a good OC. Just a card that shows a certain number in gpu z. If you are paying for this card, you are paying for the cooler.  Oh and I'm pretty sure my 980 ti(evga also) is above the lowest tier of these anyways. Yay for a free $100."
hardware,3e1efa,brookllyn,2,Tue Jul 21 18:48:54 2015 UTC,"Where the GPU ASIC quality comes into play, is that consumers can opt for four different models. The first, is the $850 model with 72%+ ASIC quality, $900 for 74%+ ASIC quality, $1000 for 76%+ ASIC quality, or a huge $1050 for 80%+ ASIC quality. These cards aren't for the faint-hearted, but for the true enthusiasts."
hardware,3e1efa,jinxnotit,2,Tue Jul 21 05:26:37 2015 UTC,Haaaa...  Crack pipes and drug dens.
hardware,3e1efa,hdshatter,2,Tue Jul 21 19:42:33 2015 UTC,Mysteriously all lower 980tis by EVGA suddenly became shitty overclockers.
hardware,3e1efa,unph4zed,1 point,Tue Jul 21 21:33:13 2015 UTC,Looking forward to the benchmarks comparing overclocks on lower ASIC cards to the higher ones. I had 4 780s and my lowest ASIC card was the best overclocker on air/H2O. I think these cards are a waste of money but time will tell.
hardware,3e1efa,Joshposh70,1 point,Tue Jul 21 20:29:59 2015 UTC,"From what I've read over dozens of posts and hundreds of comments, the asic has to do with power leakage and is only an indicator of how much voltage and energy the chip uses. It may be an indirect indicator of how well it can overclock, but it's not conclusive."
hardware,3e1efa,jinatsuko,-1,Tue Jul 21 20:35:01 2015 UTC,"Basically, don't buy EVGA unless your buying this $1049 GPU, otherwise you'll have a shitty ASIC card."
hardware,3e1efa,Joshposh70,1 point,Tue Jul 21 16:22:20 2015 UTC,"EVGA 980Ti Superclocked+ ACX 2.0+ 76%~ ASIC rating, seems like an unsubstantiated statement."
hardware,3e1efa,brookllyn,-3,Tue Jul 21 17:25:30 2015 UTC,"This is a new thing, you probably got it a while ago."
hardware,3dxugt,mirh,127,Mon Jul 20 12:42:51 2015 UTC,I thought you meant it was as powerful as an i7 5770 and couldn't believe it!
hardware,3dxugt,nazzo,75,Mon Jul 20 14:32:44 2015 UTC,This sub would have blown up were that true.
hardware,3dxugt,frosty122,27,Mon Jul 20 16:04:20 2015 UTC,Their stock ticker would have blown up even faster.
hardware,3dxugt,hak8or,1 point,Mon Jul 20 16:09:15 2015 UTC,I was already starting my countdown sequence
hardware,3dxugt,Ellimis,11,Mon Jul 20 20:59:00 2015 UTC,"There is no i7-5770, only the 5775C."
hardware,3dxugt,kkjdroid,4,Mon Jul 20 21:30:47 2015 UTC,It was a joke.
hardware,3dxugt,mastermikeee,4,Tue Jul 21 03:52:32 2015 UTC,"Are you sure? There's a 4770 and a 6770, so it would be an easy mistake to make."
hardware,3dxugt,kkjdroid,0,Tue Jul 21 03:55:18 2015 UTC,Pretty sure - he didn't edit his post.
hardware,3dxugt,mastermikeee,5,Tue Jul 21 03:59:41 2015 UTC,:\ I didn't know intel CPUs had been pushed this far
hardware,3dxugt,jinxnotit,55,Mon Jul 20 23:36:49 2015 UTC,"A middle of the road gaming desktop from 2009, condensed to a single piece of silicon consuming 35W and no one is impressed?  Maybe if they made an Ouya..."
hardware,3dxugt,sob3k,15,Mon Jul 20 19:28:04 2015 UTC,15W*
hardware,3dxugt,jinxnotit,2,Mon Jul 20 20:12:32 2015 UTC,Good luck getting that performance out of the 15W from the cTDP.
hardware,3dxugt,jorgp2,13,Mon Jul 20 20:26:34 2015 UTC,"All the available laptops with this chip, are configured to the 15w TDP.  Only the tech demos were 35w"
hardware,3dxugt,jinxnotit,1 point,Mon Jul 20 21:09:44 2015 UTC,"Yes, and when you drop TDP on APU's, you drop performance."
hardware,3dxugt,jorgp2,5,Tue Jul 21 00:13:10 2015 UTC,"This is the retail 15w setup, it actually performs better than this in 35w mlde."
hardware,3dxugt,naanplussed,1 point,Tue Jul 21 00:14:58 2015 UTC,"Time flies. I remember excitement over the 4770 or 4890, then of course 5870."
hardware,3dxugt,CykaLogic,2,Tue Jul 21 14:51:23 2015 UTC,"Not quite middle of the road. CPU still barely compares to Core2Quad and loses straight up to i5 750. GPU is comparable to GTX 260 from 2008.  35w isn't really impressive at all, you could say that NVIDIA managed to cram a 350w console from 2006 into 5w tablet but after this many years it isn't impressive."
hardware,3dxugt,animeman59,19,Mon Jul 20 22:41:46 2015 UTC,"Everything is amazing, and nobody is happy."
hardware,3dxugt,webu,8,Mon Jul 20 23:33:04 2015 UTC,For those who don't get the reference
hardware,3dxugt,Idkidks,1 point,Tue Jul 21 02:15:04 2015 UTC,https://i.imgur.com/l2023Qg.png
hardware,3dxugt,mokahless,8,Tue Jul 21 02:27:31 2015 UTC,My little brother is using my old Q8300/HD5770 build and it still delivers serviceable performance in most games. The fact that ~213W of power consumption has been reduced to 35W in 5 years is impressive IMO. Moore's law has been slowly dying for a decade now so any improvement of this magnitude should be considered impressive.
hardware,3dxugt,ClockworkOnion,5,Mon Jul 20 23:21:50 2015 UTC,"Moore's law has been slowly dying for a decade now   Been seeing people claiming this for 5+ years now.  First, let's properly define Moore's law: From wikipedia, ""Moore's law is the observation that the number of transistors in a dense integrated circuit has doubled approximately every two years.""  Let us also dispel the 18 month myth, which is actually a claim by an Intel executive regarding performance increases, not transistor count and has nothing to do with Moore.  I started some analysis and was going to post charts but I got bored and tired so I stopped properly aggregating the data and just analyzed some more. Here's what I learned along the way:   The biggest jumps are always due to node size decreases AMD has been making steady transistor count/ mm2 until 2013. After 2013 it has been slow seemingly because AMD has yet to change process nodes to something better than 28nm, which they normally would have done last year-ish based on my 15 year graph I didn't chart Intel but a quick look shows they follow the same pattern except without a decline. However, they do have an upcoming delay   So what's this mean? Well, it means the decline is untrue and the people I have been contradicting for the past 5 years without making these charts are all wrong. But it also means there has been a recent decline in the past couple years and upcoming due to delays with upcoming die shrinks.  My charts are unfortunately a huge mess and I don't want to clean it up or fix them but all the information is available to everyone on wikipedia (AMD GPUs, Intel CPUs), techpowerup (GPU transistor count and die sizes) and anandtech/ eteknix reviews (Intel die sizes and transistor counts)  Some things I had an issue with that I was unable to solve the mystery of before getting tired of aggregating data:   Intel Processors seemed to have a smaller number of transistors/ mm2 than AMD's GPUs in 2012 despite the fact that Intel was on a lower process node. Intel die size and transistor count seems damn difficult to find online however reviewers seem to have the data. Why hasn't anyone input this stuff in wikipedia? Between 2000 and 2005, ATI/ AMD video chip transistors/ mm2 seem to be all over the place, sometimes actually declining Google sheets removed the log scale option their charts!!!"
hardware,3dxugt,jinxnotit,1 point,Tue Jul 21 04:14:07 2015 UTC,"But it also means there has been a recent decline in the past couple years and upcoming due to delays with upcoming die shrinks.   That's what I was getting at, I understand and agree with everything else you've stated. I stated ""Moore's law is dying"" because we've only seen one new process (28nm) introduced since the launch of the 40nm HD5770. Intel has kept the dream alive by throwing gigabucks at R&D but even they've failed at keeping pace based on their press releases from last week. I might be placing too much weight on the process node itself IMO but it's the common denominator (size and wafer diameter) that sets the floor for Moore's per-transistor benchmark."
hardware,3dxugt,Teethpasta,-4,Tue Jul 21 12:40:58 2015 UTC,"Then it should be equally amazing that Nvidia has a better performing TABLET part in the K1, which uses less than 5 watts of power."
hardware,3dxugt,jinxnotit,0,Tue Jul 21 00:09:56 2015 UTC,Why are you talking about Nvidia in an AMD post?
hardware,3dxugt,Teethpasta,-1,Tue Jul 21 00:23:02 2015 UTC,It's a hardware post and any and all hardware is up for discussion
hardware,3dxugt,jinxnotit,-1,Tue Jul 21 05:38:20 2015 UTC,"So have you seen those old Atari's?  Wood grained consoles, amiright? And the processors they used...! So crazy to think about how much it cost now a days and the performance we got out of at the time."
hardware,3dxugt,Teethpasta,-2,Tue Jul 21 08:30:06 2015 UTC,Please stop trying to derail the conversation because AMD isn't shown in a good light. Your bias is very apparent.
hardware,3dxugt,ClockworkOnion,1 point,Tue Jul 21 08:50:57 2015 UTC,"Um, all hardware is no longer up for discussion?  And how is AMD being shown in a bad light exactly?"
hardware,3dxugt,jinxnotit,-1,Tue Jul 21 09:02:07 2015 UTC,That the k1 and especially the x1 are better performing in a tablet. And let's not pretend that's not what you were doing. Your bias is glowing as bright as the sun dude. You're just as bad as taintedsquirrel
hardware,3dxugt,rockstarfish,-5,Tue Jul 21 09:57:15 2015 UTC,"Because the subject is about power savings and performance. Sure AMD did something good here, but to pretend they are the only people making amazing performance advances in the area is disingenuous at best and fanboyism at worst."
hardware,3dxugt,CykaLogic,2,Tue Jul 21 00:56:30 2015 UTC,"Who is claiming they are the only company making advances here? Other then your raging insecurity, and entitlement complex.  The entire silicon industry is trying to do this, this is just one example of an AMD chip.  Please, make a top post about how amazing the X1 is, until then you're just thread shitting and crying about Nvidia being ignored.  That tends to happen with out the X86 license."
hardware,3dxugt,LeMAD,1 point,Tue Jul 21 01:36:05 2015 UTC,I don't think you need anything more powerful to play modern games.
hardware,3dxugt,elementalist467,-2,Mon Jul 20 23:39:33 2015 UTC,Tegra is way slower then ps3.  You have been brainwashed well
hardware,3dxugt,playingwithfire,4,Tue Jul 21 01:42:12 2015 UTC,"K1 is half the power of a GT740m, a chip which is roughly 1.6x the speed of the 8800gtx. The 8800 destroyed consoles when it came out, so 80% of its performance is still faster than PS3.  http://www.notebookcheck.net/NVIDIA-GeForce-ULP-K1-Tegra-K1.108453.0.html"
hardware,3dxugt,billyalt,57,Tue Jul 21 02:12:36 2015 UTC,"I own a 6770 and ""powerful"" is not the word you are looking for. I can play TW3 and GTAV at 768p and 30 fps if I use the minimum graphics settings, but it looks on par with a PS3."
hardware,3dxugt,playingwithfire,41,Mon Jul 20 14:19:59 2015 UTC,It is pretty good for an integrated low power graphics device.  As the integrated graphics devices improve the graphics floor of the market rises.  This means that developers can use that as a minimum configuration and get a broad swath of potential users.  Any significant advancement in integrated graphics is good for the industry.
hardware,3dxugt,TylerDurdenisreal,22,Mon Jul 20 15:41:21 2015 UTC,Modern games on minimum setting look quite good actually.
hardware,3dxugt,playingwithfire,9,Mon Jul 20 17:27:43 2015 UTC,"I remember when BF3 came out, I thought it still looked quite good even on the lowest settings."
hardware,3dxugt,anon1821,13,Mon Jul 20 19:40:16 2015 UTC,I genuinely don't know what the difference between Medium and Ultra for half of the settings in most games I play.
hardware,3dxugt,playingwithfire,14,Mon Jul 20 19:43:55 2015 UTC,like 90 percent of it is how few or how many jaggies you get on object edges
hardware,3dxugt,I-never-joke,10,Mon Jul 20 19:59:29 2015 UTC,"Yeah texture, Ambient Occlusion and tessellation matters. A lot of the other settings are pretty trivial."
hardware,3dxugt,anon1821,5,Mon Jul 20 20:00:32 2015 UTC,i love that in witcher 3 you get no performance penalty for setting ultra texture quality...assuming you have a 2gb vram card
hardware,3dxugt,billyalt,6,Mon Jul 20 21:13:33 2015 UTC,But Ultra shadow. Fucking Ultra shadow that does nothing. Cost 5-7fps.
hardware,3dxugt,TylerDurdenisreal,2,Mon Jul 20 21:16:50 2015 UTC,"The only difference between ultra and high textures on witcher 3 is the amount of VRAM it will use, they look identical.  High is for 2gb of Vram Ultra is for more."
hardware,3dxugt,Carr0t,2,Mon Jul 20 22:00:21 2015 UTC,I set it on ultra and it never allocates more like than 1600MB or around that range so it is all good
hardware,3dxugt,playingwithfire,2,Mon Jul 20 22:03:02 2015 UTC,A lot of modern games like Dying Light are very VRAM hungry. Was pleasantly surprised at how light Witcher 3.
hardware,3dxugt,LazyGit,1 point,Mon Jul 20 22:37:47 2015 UTC,good thing I have 6gb total lol
hardware,3dxugt,sluflyer06,4,Tue Jul 21 00:48:36 2015 UTC,"In most games I can see a clear difference between med and high for texture resolution, but beyond that to very high/ultra I can't tell unless I'm shown a side by side comparison. For a lot of other graphical shinies I honestly can't tell the difference between low and > low (or off and on, for stuff like AO which doesn't have levels) unless I'm shown them side by side.  I generally aim for 1920x1200 (native red for my main monitor), 60fps, high textures, and turn stuff other than textures down until I can hit that. It was a sad day when I realised my 2GB 670 gave me unplayable stuttering in GTA V if I put the textures above medium. That was when I started planning my next build.  Then I bought a house. No new build for me. Goodbye savings ;)"
hardware,3dxugt,Konstantine133,3,Mon Jul 20 20:26:18 2015 UTC,AO actually does a lot in partially shadowed areas. Such as under a bridge or dome. Depends on the game and its execution though. Honestly I'm just turning all the settings on because I can.
hardware,3dxugt,sluflyer06,1 point,Mon Jul 20 20:30:15 2015 UTC,"Yeah, IQ comparisons have turned into spot the difference these days."
hardware,3dxugt,RaulNorry,-7,Tue Jul 21 12:56:23 2015 UTC,I hope that is a joke.
hardware,3dxugt,sluflyer06,10,Mon Jul 20 18:06:45 2015 UTC,"I don't see how you think he is joking? The Witcher 3 on Minimum graphics looks just fine. Shadows of Mordor in minimum graphics looks fine. GTA 5 on minimum settings looks just fine.     Tens of Millions of people that play games on a PS4 would agree that games on their console look quite good. Obviously it is nowhere near as clean and beautiful as a maxed out 1080p The Witcher 3, but the game looks just fine. To argue that is just absurd.    If your comparing a GTX 980Ti to a GTX 750Ti obviously the 980Ti is better, but saying the 750Ti is garbage is just stupidity.    You're doing exactly that, but with graphical fidelity rather than GPU's."
hardware,3dxugt,RaulNorry,-8,Mon Jul 20 19:04:00 2015 UTC,The definition of PC gaming for me is and always has been to be able to play the highest settings with 60fps as a bare minimum...I apparently assume too much.
hardware,3dxugt,sluflyer06,8,Mon Jul 20 20:05:29 2015 UTC,"You definitely assume too much. The enthusiast hardware you see plastered all over these subs are a very, very small subset of PC gamers. The benefits of having a PC over a console do not magically dissappear if you can't afford the greatest hardware. Hell, look at how many people are still holding on to their first and second gen Core i* processors!"
hardware,3dxugt,RaulNorry,0,Mon Jul 20 20:18:57 2015 UTC,"In my circle I don't know a single person with anything less than a 970 on a OC'd sandy bridge and thats 1 just person. Everyone else has top end stuff.  Maybe it's our age? We're all married, 30-31 yo, with expendable income"
hardware,3dxugt,playingwithfire,1 point,Tue Jul 21 14:06:19 2015 UTC,"Then your ""circle""  is either very lucky and have well paying jobs, or are placing themselves in debt to maintain their hobby. I'd also guess that you live in America, where the same pieces of tech can be much, much cheaper than everywhere else in the world.   Elsewhere, tech can range from being 50% to 2-300% more expensive, if it is available legally at all. Some amazing games (Metro series comes to mind first) are built and designed on computer with specs far below your supposed ""norm""  of i5s and 970s, or are built using illegally smuggled parts.   Tech companies always advertise the latest, greatest, and most powerful, but you are a fool if you think that's where the majority of their revenue comes from.   Sorry if I'm coming off a bit harsh, but the viewpoint that the average computer gamer will have a computer costing more than 1000-1500 dollars is just plain wrong."
hardware,3dxugt,playingwithfire,1 point,Tue Jul 21 15:46:14 2015 UTC,everyone in my friend group indeed has very well paying jobs in engineering for a large company.
hardware,3dxugt,namae_nanka,1 point,Tue Jul 21 17:10:22 2015 UTC,So why would you assume that people with six figure salaries would have the same computers as the majority of the rest of the world working for far less?
hardware,3dxugt,ClockworkOnion,1 point,Tue Jul 21 18:12:43 2015 UTC,I'm still offering the bet :)  We can do charity donation instead of reddit gold if you actually want to do some good. $5 to a charity of the winner's choice.
hardware,3dxugt,namae_nanka,2,Mon Jul 20 20:31:08 2015 UTC,"Not really. At a glance the biggest difference between low and Ultra in TW3 is worse aliasing and worse texture. Dare I say most of the settings isn't that noticeable between Low and Ultra, and definitely not between Medium and Ultra.  Not at home now. But I will bet you reddit gold that you can't tell the difference between medium shadow and ultra shadow in TW3 screenshots."
hardware,3dxugt,sickofallofyou,33,Mon Jul 20 19:14:26 2015 UTC,"The 5770 was slightly faster than 4850 which in turn was slightly faster than the 8800gtx, widely considered as nvidia's most successful gpu launch to date. The PS3's rsx isn't even close.  I expect the time period for comparable desktop flagship performance to decline rapidly with the use of HBM."
hardware,3dxugt,tman2damax11,6,Mon Jul 20 15:02:05 2015 UTC,I think the 970 takes the cake for most successful launch. Shit has been out for a short time and is 3%+ in the Steam Hardware survey.
hardware,3dxugt,gmarcon83,1 point,Tue Jul 21 00:02:57 2015 UTC,"The HD5770 was more of an in-between of the 4870 1gb and 4890. Factory overcooked cards matched stock 4890s and those chips pushed ~10% OCs so easily that it was hard to find one with stock clockspeeds.  It's still a decent enough GPU, my little brother has been getting by with mine for ~3 years now."
hardware,3dxugt,nut_fungi,1 point,Mon Jul 20 23:16:04 2015 UTC,"I was going by the launch reviews. Considering it had the same specs as 48xx except for the bus width but higher core clock, I'd expect it to be ahead of 4870 if the memory bandwidth wasn't a bottleneck. It surely became a problem on the 4850s volt modded to ~900Mhz with their GDDR3."
hardware,3dxugt,dekomote,1 point,Mon Jul 20 23:20:32 2015 UTC,I'm playing GTA V on a GTS450 @ 1080 @ 30fps.  I <3 i5 3570k.
hardware,3dxugt,skilliard4,1 point,Mon Jul 20 20:17:18 2015 UTC,"I used to have a 6770 and was able to play BF3 at 1080p 60fps (even on large maps with 64 players), most of the settings were on the lowest possible but I was able to get higher texture and shadow quality while still maintaining 60fps most of the time.   I was even able to play the BF4 beta at 1080p 45fps at the lowest possible settings, and I head the beta was really wonky in terms of optimization and the base game ran a lot better on a range of hardware, so I probably could have gotten 60fps on that too, if I hadn't sold the card before retail release.  Overall solid card, played less intensive games like simcity, minecraft+mods, and dirt 3 like a champ. And all these games look well above ""PS3 quality""."
hardware,3dxugt,Liam2349,1 point,Tue Jul 21 02:07:33 2015 UTC,"I think that the 6770 was/is a pretty good GPU, I used to play Sleeping Dogs with pretty good settings on mine running stable at 60 fps."
hardware,3dxugt,skilliard4,-2,Tue Jul 21 02:37:53 2015 UTC,5770 performance is between the 6770 and 6850.
hardware,3dxugt,Liam2349,8,Mon Jul 20 16:43:20 2015 UTC,6770 is 5770...
hardware,3dxugt,skilliard4,0,Mon Jul 20 19:27:30 2015 UTC,On par with PS3? You crazy?  If GTA V and TW3 run with those details the problem must be the CPU
hardware,3dxugt,Liam2349,0,Mon Jul 20 23:38:04 2015 UTC,GTAV at 768p and 30 fps if I use the minimum graphics settings   You sure something isn't wrong with your computer? I have a Radeon 7870 and GTA V runs at 60 fps at very high settings
hardware,3dxugt,LongBowNL,1 point,Tue Jul 21 02:00:25 2015 UTC,No it doesn't. It doesn't have nearly enough VRAM to run very high settings and you won't get anywhere near stable 60.
hardware,3dxugt,clickwir,1 point,Tue Jul 21 02:33:00 2015 UTC,I guess the steam fps counter is malfunctioning then?
hardware,3dxugt,IC_Pandemonium,1 point,Tue Jul 21 03:46:42 2015 UTC,"Yes, because you don't have enough VRAM to run those settings. I don't even have them maxed and I use 4GB when the in-game settings say 2.5GB, and your card is 2GB.  There's that and the fact that on my heavily overclocked 290 and 4670k, I get 60 to 90FPS not even on maxed settings, mostly around 70FPS.  I'd say your results make no sense next to mine."
hardware,3dxugt,CykaLogic,1 point,Tue Jul 21 04:00:45 2015 UTC,"I said very high, not highest. I don't turn on stuff like long shadows/extended shadow distance because they hardly make a difference in visual performance."
hardware,3dxugt,Porcupanda,1 point,Tue Jul 21 04:05:37 2015 UTC,Very high is the highest.. There is no ultra.
hardware,3dxugt,clickwir,14,Tue Jul 21 12:47:06 2015 UTC,"Are they any Carrizo laptops ""in the wild"" now? I know Toshiba will release some on 29 July."
hardware,3dxugt,Elranzer,5,Mon Jul 20 14:32:11 2015 UTC,"Supposedly HP has one. But I've only been able to find references to it, not actually any place to buy."
hardware,3dxugt,clickwir,2,Mon Jul 20 16:19:56 2015 UTC,"Yeah, saw it retail for 500AUD for A6 and 900AUD for A8. Pretty decent performance IMO and beats the heating issues you always get with discrete graphics."
hardware,3dxugt,alainmagnan,4,Tue Jul 21 04:37:45 2015 UTC,"For whatever reason, AMD laptops are always worse versions of their intel counterparts. Worse screen, SSD vs HDD, etc."
hardware,3dxugt,jinxnotit,2,Mon Jul 20 22:35:38 2015 UTC,It's because they are a hell rarer.   That time I found a fresh Z50-75 with a FX-7500 in stock (which is not the usual netbook) I was amazed by what it could produce with only 530€
hardware,3dxugt,TonyCubed,2,Mon Jul 20 23:41:16 2015 UTC,Someone in the AMD sub said that HP Envy 15z has the FX8800P upgrade option. Costs less than $600 for it with the upgrade.. But.. No 1080p screen. :l  http://store.hp.com/us/en/pdp/Laptops/hp-envy---15z-l3k97av-1#!&TabName=specs
hardware,3dxugt,Exist50,6,Tue Jul 21 19:25:34 2015 UTC,Now if I can only find some of these new APU's in stock somewhere. Seems there's tons of news about something I can't actually buy.
hardware,3dxugt,StellaTerra,3,Mon Jul 20 16:18:45 2015 UTC,They're not selling this gen of APUs are stand-alone parts. They're only meant for OEM laptops.
hardware,3dxugt,MuzzyIsMe,2,Tue Jul 21 12:37:57 2015 UTC,And OEM's still seem to be well under the control of Intel and Microsoft. Soooooo we'll never see them. :-(
hardware,3dxugt,JonWood007,4,Tue Jul 21 16:39:06 2015 UTC,Not too bad but pitting it against Terascale and wolfdale seems kind of strange. Would like to see it vs i5 mobile integrated or low end discrete
hardware,3dxugt,StellaTerra,2,Mon Jul 20 16:26:51 2015 UTC,"Why strange? Those are two completely separate desktop components, while older and outdated, sharing the space of a single die for a MOBILE part.  That's pretty damn cool to me."
hardware,3dxugt,namae_nanka,12,Mon Jul 20 19:20:07 2015 UTC,"I'm not seeing it, the compared CPU is an Core Duo 2 E8400?"
hardware,3dxugt,MuzzyIsMe,14,Mon Jul 20 14:22:36 2015 UTC,Just focus on the graphics scores.
hardware,3dxugt,namae_nanka,8,Mon Jul 20 15:46:27 2015 UTC,"To me, this is exciting. I'm gonna keep watching these comparisons as the high end integrateds start gobbling up the low end discretes things will get cheaper, more power efficient, and laptop and tablet gaming will become more and more competitive. Can't wait!"
hardware,3dxugt,boanerges57,6,Mon Jul 20 15:12:32 2015 UTC,"Even low end discrete cards still pummel APUs.  I just can't see that changing, for the simple reality of physics.  A discrete card allows more space for heat distribution and cooling, therefore any half decent discrete will beat an APU.  I think it's great that laptops and low end PCs now have more options for casual gaming, though."
hardware,3dxugt,boanerges57,9,Mon Jul 20 15:50:44 2015 UTC,Yeah that's kind of the point of APUs. Use them in laptops to get an affordable gaming option on the go.
hardware,3dxugt,hojnikb,8,Mon Jul 20 16:08:10 2015 UTC,"I dunno.  I don't really know how the physics plays out, but the historical evidence seems to suggest that a catching up is occurring, that the lowest level of discretes are already being made obsolete.  For evidence:  http://www.tomshardware.com/reviews/intel-core-i7-5775c-i5-5675c-broadwell,4169-6.html  The iris 6200 is beating out the HD 6670, GTS 250, R7 240 & 250 and is hot on the heels of the GTX 750.  Historically there is no way this would have been the case, and we know that Intel is dedicated more and more of its die space to GPUs.  I also think there are some more hidden force multipliers in being on the same die as the CPU, such that data doesn't have to move across a bus, and caches are multi-purpose.  I mean, I'm not a chip designer, but I'll bet making this more viable is a big goal of Intel's.    Also, remember that with a lot of this stuff, when it comes to power, if you want to compare apples to apples, look at APUs/CPUs with a decent aftermarket cooler if you're going to compare to an aftermarket manufactured video card.  Even just with an Evo 212, heat dissipation aint no thang.  Generally these will be used in laptops, so compare crappy laptop CPU cooling with crappy laptop GPU cooling.  I had a dual GTX 880m laptop for a while and that thing thermal throttled like a mother!  I personally would rather have a laptop with one heatsource that has as much power dissipation as it can manage, rather than having two (or more, in my case).  I just mostly think the cooling question is more complicated than ""more separate components = better""."
hardware,3dxugt,Idkidks,5,Mon Jul 20 16:17:30 2015 UTC,You should remember iris 6200 costs a hell more than this AMD solution.   eDRAM is really not cheap
hardware,3dxugt,hojnikb,2,Mon Jul 20 23:42:24 2015 UTC,I just can't see that changing   That's a lack of imagination on your part then.   for the simple reality of physics   Pshaw!
hardware,3dxugt,jorgp2,1 point,Tue Jul 21 00:26:15 2015 UTC,"Perhaps you are right. Once we have mastered quantum tunnelling or something of that sort, we can just warp the heat away to some cold point in the universe. Like my kitchen floors in the middle of the winter."
hardware,3dxugt,hojnikb,1 point,Tue Jul 21 03:10:20 2015 UTC,"Well we already have 200W coolers on CPUs. An APU with decent enough CPU performance and dx12 to make it not matter can easily set aside 150-175W for the GPU. Slap on HBM to remove the last straw and you'll be closer to a 200W GPU in performance. That's reaching high end territory.  Of course if the programming paradigm changes with APUs then all these considerations are pointless, an APU will smoke a discrete GPU regardless."
hardware,3dxugt,LulusPanties,4,Tue Jul 21 14:21:39 2015 UTC,It isn't even using the right graphics driver.
hardware,3dxugt,FallenAgist,2,Mon Jul 20 15:44:24 2015 UTC,Soorry if these tests were done with unfinished drivers
hardware,3dxugt,LulusPanties,1 point,Mon Jul 20 23:42:46 2015 UTC,It's hard to say a test is even relevant if you aren't even certain the hardware was being addressed correctly...  I can test a titan using the generic vga driver and its not gonna do all that well.
hardware,3dxugt,FallenAgist,1 point,Tue Jul 21 06:17:50 2015 UTC,If you look at the driver version (15.100.0.0) you see that's an AMD driver
hardware,3dxugt,IC_Pandemonium,4,Tue Jul 21 11:31:00 2015 UTC,"i would like to see some real world gaming benchmarks, not 3dmark.  Because 7750gddr5 used to be on the same level as 5770. But while this APU has the same shader config, it has much much lower memory bandwidth."
hardware,3dxugt,jorgp2,10,Mon Jul 20 17:25:57 2015 UTC,"If it's on par with a 7750, and only uses 15 watts, including a CPU that's just... wow. The 7750 was a beast too (I had one) it played most of my games on minimum @ 60fps (BF4, CS:GO)."
hardware,3dxugt,Elranzer,1 point,Mon Jul 20 19:13:26 2015 UTC,"7750 with GDDR5 memory. Not the one with ddr3 memory, which is quite a bit slower.  And power usage ranges from 35W all the way down to 12W. But running it at 15W will see a reduction in performance.  So, while this is an impressive chip from efficiency standpoint, its still quite a bit from midrange gpus from a few years back."
hardware,3dxugt,cvance10,1 point,Tue Jul 21 06:20:39 2015 UTC,"But this one has lossless color compression, support for DDR3 2133, and full DirectX 12 support."
hardware,3dxugt,TehRoot,1 point,Mon Jul 20 21:13:25 2015 UTC,"lossless compression and ddr32133MT cant make up the difference in bandwidth. It helps, but those are still bandwidth starved.  DX12 is obviously a huge plus, but then again, no games support it right now."
hardware,3dxugt,IndigoMoss,4,Tue Jul 21 06:08:30 2015 UTC,How does that compare to Intel iris pro 5200?
hardware,3dxugt,crshbndct,7,Mon Jul 20 19:29:55 2015 UTC,I believe it beats it. Only the newest 6000 iris seem to be beating out the apus. But those cost a hell of a lot more and if your spending that much money why are you not getting a dedicated gpu. But that's just my opinion
hardware,3dxugt,Charwinger21,2,Mon Jul 20 21:16:29 2015 UTC,Which one?  The performance gap between 6200 and 6100 is huge
hardware,3dxugt,Nixflyn,3,Mon Jul 20 21:22:04 2015 UTC,It would be the 6200.
hardware,3dxugt,SeaJayCJ,1 point,Mon Jul 20 21:42:44 2015 UTC,Probably corporate. Need to decode 1080p or work or larger res screens. Most calculation rigs I worked on have similar specs.
hardware,3dxugt,crshbndct,1 point,Tue Jul 21 04:40:31 2015 UTC,"Should be about the same, except it'll be available in a mainstream product."
hardware,3dxugt,SeaJayCJ,1 point,Mon Jul 20 21:11:38 2015 UTC,"Where can I buy this APU?   Meanwhile, I can already buy an Intel NUC with the Iris Pro 6200."
hardware,3dxugt,crshbndct,10,Tue Jul 21 12:39:16 2015 UTC,So it's as powerful as a 6 year old mid-level GPU?  That seems about right.  What do other non-APU integrated graphics from 2015 score for comparison?
hardware,3dxugt,Maysock,6,Mon Jul 20 16:20:41 2015 UTC,What do other non-APU integrated graphics from 2015 score for comparison?   less
hardware,3dxugt,frosty122,4,Mon Jul 20 21:40:08 2015 UTC,"You can still play most games with that, remember."
hardware,3dxugt,Maysock,2,Mon Jul 20 23:43:56 2015 UTC,"Isn't the 5770 as powerful as the 4870? So not that great for the demanding games, but can still do some older titles really well, especially with an improved feature set."
hardware,3dxugt,jorgp2,2,Mon Jul 20 21:38:34 2015 UTC,I have a 4790K and a 5770.   Are the graphics in the newest i7 close to this yet?
hardware,3dxugt,Maysock,2,Mon Jul 20 21:41:40 2015 UTC,"I have a 4790K and a 5770.   If you have a 4790k and a 5770, then you're really due for a GPU upgrade.  You can grab an R9 285 for $150, which will be about 3 times faster than your current GPU (albeit using more power), and can actually perform double precision calculations, as well as adding features like FreeSync.   Are the graphics in the newest i7 close to this yet?   The Iris Pro 5200 (Haswell) is about a GT 650M (about a 5670 equivalent).  The Iris Pro 6200 is between an R7 250 and a GTX 750 (about a 5750 equivalent).  The next Iris Pro GPU (coming out next month) is supposed to be ~50% faster, so probably somewhere in between an 5770 and a 5830.     In real world performance though, DDR4 might have a bigger impact."
hardware,3dxugt,ClockworkOnion,1 point,Mon Jul 20 23:52:15 2015 UTC,"http://www.tomshardware.com/reviews/intel-core-i7-5775c-i5-5675c-broadwell,4169.html"
hardware,3dxugt,Esyir,1 point,Mon Jul 20 21:55:34 2015 UTC,I have a 4790K and a 5770.    Biggest CPU/GPU imbalance I've seen in a while. I'm guessing you don't game much?
hardware,3dxugt,Elranzer,1 point,Tue Jul 21 04:43:17 2015 UTC,"No, not at all. Its just an HTPC. I mostly use my phone for stiff these days. I also have a gaming machine with a 3570K and R9 290, but it hasn't been turned on in months."
hardware,3dxugt,I_Will_be_Nice,1 point,Tue Jul 21 05:48:04 2015 UTC,That is one baller HTPC. Any particular reason for the 4790K..?
hardware,3dxugt,sterffff,1 point,Tue Jul 21 05:49:26 2015 UTC,Got it cheap from a PC shop that was going broke.
hardware,3dzk3o,zmeul,5,Mon Jul 20 20:23:08 2015 UTC,Likely they are making more smaller nod shifts. 20nm to 16FF to 16FF+. They are all using some similar things and some different. Same back end but from 20m. Being planar to FF being a FinFet to FF+ likely being a taller gate much like Intels 2nd Gen FinFet.
hardware,3dzk3o,dylan522p,3,Tue Jul 21 03:57:56 2015 UTC,"Come on, Pascal..."
hardware,3dzk3o,arikv2,4,Tue Jul 21 01:49:17 2015 UTC,wont we also be waiting on HBM2 for pascal? any news from sk hynix?
hardware,3dzk3o,dudemanguy301,4,Tue Jul 21 02:29:55 2015 UTC,Q4 2015 sampling probably Q1 2016 shipping and production for pascal halfway through the year.
hardware,3dzk3o,dylan522p,7,Tue Jul 21 04:02:00 2015 UTC,"I think Nvidia might have to ask AMD nicely for a reference this time.   No, but in all seriousness I have no idea but since this HBM been around 7 years in the making chances are Nvidia will have to wait on this one a little longer. Although I don't like the idea of hiding behind as big of a proprietary wall as possible, it's ironic Karma for Nvidia at this point that they aren't the first in on this one."
hardware,3dzk3o,WillWorkForLTC,3,Tue Jul 21 04:03:12 2015 UTC,"Historically, moving to new process nodes and memory standards as well as sizes hasn't been Nvidia's strength or strategy and HBM is a JDEC standard so... No surprises here."
hardware,3dzk3o,andromeduck,1 point,Tue Jul 21 06:49:22 2015 UTC,Only problem is AMD's losing money and Nvidia has quite a bit. If they do have exclusivity it won't be for too long.
hardware,3dzk3o,Blubbey,1 point,Tue Jul 21 12:42:05 2015 UTC,Nvidia would easily sell twice as many cards with Hbm on them so it's not like Hynix would want to limit them. If anything they are the bigger better customer.
hardware,3dy7fm,ParkMyWRX,84,Mon Jul 20 14:36:31 2015 UTC,Speaker technology has barely changed so companies continue to sell older models.
hardware,3dy7fm,Anaron,12,Mon Jul 20 14:50:11 2015 UTC,"There have been some really nice leaps in the past decade, especially with the rise of cheap DSPs for active XOs + correction. Small drivers get more low-distortion bass than ever before, too.  But there's no incentive for PC speaker makers to invest in better sound quality, since anyone who cares buys from a company that markets to audio engineers or audiophiles."
hardware,3dy7fm,euvie,7,Tue Jul 21 00:07:56 2015 UTC,It's turning into a bit of a niche because people either get headphones or proper sound systems. Advance sound systems have always been around so there's less room for PC Speakers to advance.
hardware,3dy7fm,godsayshi,4,Tue Jul 21 01:53:49 2015 UTC,"Exactly, I just bought a receiver and some pioneer bookshelf speakers and couldn't be happier. Runs off optical out on my mobo and sounds much better than the Klipsch promedia 2.1 set it replaced."
hardware,3dy7fm,realthedeal,3,Tue Jul 21 02:20:57 2015 UTC,"Really? I have the Klipsch 2.1 and I love them. Compared to what most people have, they're the shit."
hardware,3dy7fm,lokeenz,2,Tue Jul 21 02:26:39 2015 UTC,PC speakers never hold a candle to some real bookshelfs or other audiophile speakers. Audiophiles are too picky for them to get away with anything less
hardware,3dy7fm,familyknewmyusername,2,Tue Jul 21 07:45:09 2015 UTC,"Or to put it another way: Anyone who buys ""computer speakers"" wants ""computer speakers.""  They don't give a shit about audio quality."
hardware,3dy7fm,negative_commentary,37,Tue Jul 21 07:12:40 2015 UTC,"Just something i've noticed looking at /r/battlestations. Many people have started to use studio monitors instead, which makes a lot of sense, the sound quality is really on the next level, and i feel the same can be said for build quality."
hardware,3dy7fm,Slazher,5,Mon Jul 20 16:27:39 2015 UTC,"You know what, I was actually looking into the M-Audio AVs as an option as well."
hardware,3dy7fm,Dr_Midnight,13,Mon Jul 20 18:38:01 2015 UTC,"M-Audio AV   DON'T.  If you want to go with M-AUDIO, and are restricted by space to 5"" speakers, go with the BX-5's instead. The AV series is horrendous in terms of audio quality, and sounds worse than the speakers in an old Civic Hatchback.  Also, do not get KRK Rokit's. They're overpriced, and overrated.  The Mackie MR5 is a solid unit as well.  For my desktop space, I use an old Logitech Z-5500 set (and have had no reason to upgrade it, honestly), but I've been known to bring up my monitors and use those connected to a USB 4x4 ASIO Audio Adapter. Previously, I had 4x TANNOY 5"" Monitors (Passive). I sold those quite some time ago.  For audio recording purposes these days, I work with a set of M-AUDIO BX8 D2's (8-inch active reference monitors) paired with a Yamaha HS-10W (8-inch active reference sub-woofer) routed through an Outboard Mixer via XLR cables (which is connected to USB Audio via XLR as well)."
hardware,3dy7fm,IsaacJDean,2,Mon Jul 20 20:27:49 2015 UTC,+1 for Mackie MR5. I have the MKIIs. Awesome speakers.
hardware,3dy7fm,sterob,1 point,Tue Jul 21 12:14:18 2015 UTC,"The Mackie MR5 is a solid unit as well.   i would get mackie mr5 if if they sold them here but since it is not, i settle with the krk vxt4."
hardware,3dy7fm,shapul,4,Tue Jul 21 04:30:53 2015 UTC,"That's what I did in the end: about two years ago I got tired of my headphones. I looked at the PC speakers and none of them were attractive enough for me. Then I discovered the ""monitors"" and very soon bought a pair of M-Audio. I am very happy with them as I found much better price/performance ratio in these speakers compared to the PC market.  So, if you are happy with a Stereo setup as opposed to a 5.1 system I strongly suggest to take a serious look at the monitors."
hardware,3dy7fm,Hegulator,2,Mon Jul 20 19:15:57 2015 UTC,I'm using the AV-40's paired with a Dayton Sub-80.  Probably the best 2.1 setup I could have mustered for the money.  I've had them for a few years now and have no complaints.
hardware,3dy7fm,gmarcon83,1 point,Tue Jul 21 02:00:49 2015 UTC,"I would recommend going this way too. I don't know how much you are planning to spend, but give a look at the Presonus Eris 4.5. They are really good sounding, not too expensive and have a great form factor to double as desktop speakers (Front facing Power switch, headphone outputs and volume button)."
hardware,3dy7fm,ElvarP,4,Tue Jul 21 03:10:01 2015 UTC,plus they look cool as fuck
hardware,3dy7fm,glr123,5,Mon Jul 20 17:23:56 2015 UTC,Or spend the money and get amazing headphones. Not as much soundstage but the quality will be fantastic.
hardware,3dy7fm,parasemic,21,Mon Jul 20 17:34:45 2015 UTC,"Ehm, great open back headphones have way better soundstage than mediocre (<1000 euro) 2.1 setup"
hardware,3dy7fm,freaky88,2,Mon Jul 20 18:56:35 2015 UTC,Any you would recommend?
hardware,3dy7fm,parasemic,5,Mon Jul 20 18:22:10 2015 UTC,Can't go wrong with sennheisers.  Also AKG K7XX on Massdrop right now are a goddamn steal. You need an amp though
hardware,3dy7fm,Jack_BE,1 point,Mon Jul 20 18:56:45 2015 UTC,"can confirm,  have a Sennheiser PC 350, it's so awesome Sennheiser actually re-released it after it went EoL.  A good analog headphone needs to be paired with a good soundcard though, since total sound quality is dependent on the end to end chain."
hardware,3dy7fm,Drudicta,1 point,Tue Jul 21 06:20:22 2015 UTC,Any sound cards you can recommend that come with equalization software? I'd like for ALL my windows sounds to be equalized.  Either that or just Equalization software that's embedded into Windows in general once it's installed.
hardware,3dy7fm,SCREAMING_FLESHLIGHT,4,Tue Jul 21 20:32:51 2015 UTC,"Fidelio X2- open, comfortable, nice sound signature, don't require much juice to run so don't require a dedicated amp."
hardware,3dy7fm,benowillock,3,Mon Jul 20 21:10:56 2015 UTC,HD 600's are classics at a reasonable price. You'll need an amp&dac or a sound card that can support 350 ohms though
hardware,3dy7fm,TetsuoS2,2,Mon Jul 20 22:11:05 2015 UTC,Audio Technica AD500X/AD700X/AD900X all have great soundstages.
hardware,3dy7fm,UrbanToiletShrimp,3,Tue Jul 21 01:12:18 2015 UTC,Why not have both? Awesome speakers and headphones.
hardware,3dy7fm,snowlovesnow,1 point,Mon Jul 20 21:05:40 2015 UTC,"Eh I like the epic bass of my subwoofer, thanks though."
hardware,3dy7fm,hayuata,1 point,Tue Jul 21 01:56:00 2015 UTC,"Ditto, I just like the vibrations my subwoofers give off."
hardware,3dy7fm,TexasJefferson,1 point,Tue Jul 21 07:03:11 2015 UTC,"Sitting here with my Z-5500, I don't understand the turn away from 5.1 setups for games & movies.  (Sure the lil' satellites don't compare to actually good speakers, but better reproduction seems like a bad trade-off for surround sound for non-music stuffs.)"
hardware,3dy7fm,SenpaiPleaseNoticeMe,1 point,Mon Jul 20 20:33:10 2015 UTC,Just upgraded from my Klipsch 2.1's to a pair of M-Audio BX8 D2's and I'm blown away. My only issue is that they don't go below around 35 hz but that can be fixed by adding a subwoofer in the future if I care enough.
hardware,3dy7fm,ScepticMatt,1 point,Tue Jul 21 06:09:22 2015 UTC,Dynaudio bm12a represent
hardware,3dy7fm,fRl1jOE_,41,Tue Jul 21 10:29:03 2015 UTC,Mackie masterrace reporting in.
hardware,3dy7fm,nawariata,19,Tue Jul 21 22:10:13 2015 UTC,"Sound technology doesn't progress that fast, so 10 year old speakers sound as good as this years new releases. In fact I'm using 30yo amp and speakers (with replaced drivers though) and not looking for upgrade anytime soon."
hardware,3dy7fm,MyDreamName,4,Mon Jul 20 14:56:35 2015 UTC,"Yup, human hearing is not all that good.  Studios still regularly use microphones from the 40s and 50s because the technology hasn't  had to improve.  As for good 2.1 systems check out the Swan M50w"
hardware,3dy7fm,thepoener,5,Mon Jul 20 15:16:31 2015 UTC,Swan M50w   Are these available anywhere? I can't find them on Amazon or newegg...
hardware,3dy7fm,MyDreamName,-4,Mon Jul 20 19:31:24 2015 UTC,"Massdrop has them occasionally, that's where I got mine"
hardware,3dy7fm,mtocrat,11,Mon Jul 20 20:47:07 2015 UTC,"That would imply that the sound out of those logitech things is subjectively perfect. I assure you, it is not."
hardware,3dy7fm,innerknowing,2,Mon Jul 20 17:02:28 2015 UTC,"For the price, can we expect much better? If you want quality you look elsewhere. We both know that.  They likely choose to spend their R&D on mice and keyboards where they can still add extra LEDs and throw around words like Lasers and DPI to describe their new product.  Marketing speakers is tough work."
hardware,3dy7fm,mtocrat,3,Mon Jul 20 17:26:20 2015 UTC,"I agree, but my point is that R&D hasn't hit a hard barrier of human hearing in this price range and for this size"
hardware,3dy7fm,innerknowing,1 point,Mon Jul 20 17:50:48 2015 UTC,I wonder I'd they have ever really performed R&D on their desktop speakers or if they just purchase some mediocre drivers from someone else and stick them in an aesthetically appealing enclosure for display on our desk.    There is certainly a lot of room to improve if they bothered to.  Even those older electrostatic? Ones that were paper thin were a nice novelty to see since they were actually different and new in this market.  I'm still pissed my old Klipsch 4.1s had such a shitty control pod that rendered the entire unit unusable when it went out.  If it wasn't for that I wouldn't have bought any since....2002?
hardware,3dy7fm,sifnt,1 point,Mon Jul 20 17:57:01 2015 UTC,"If you're budget conscious a used set of bookshelf speakers and a receiver, or a decently reviewed active studio monitor like the behringer TRUTH B2031A will do you much better than a new logitech system. Doubly so if you add a subwoofer and have a measurement microphone to properly equalise them.  Its all about effort/risk/rewards and your preferences."
hardware,3dy7fm,Drudicta,1 point,Tue Jul 21 03:01:11 2015 UTC,To be fair I do like adjustable DPI. but it's about as sensitive as it can get. 10k DPI is plenty and rather insane.  Needs moar lazers though. And flashy lights. Maybe more than 18 buttons?
hardware,3dy7fm,HavocInferno,5,Tue Jul 21 20:35:16 2015 UTC,"He didnt say so. But for that low a price, your options arent many. And Logitechs are good in their price ranges."
hardware,3dy7fm,Dark_Crystal,1 point,Tue Jul 21 08:16:44 2015 UTC,"New D class amps sound great and don't have nearly the wasted power of ""analog"" amps."
hardware,3dy7fm,tbonanno,1 point,Mon Jul 20 18:47:24 2015 UTC,"When you replace drivers, do you have to match them to what speakers you have? Or can you just put any in if they fit and are rated for the right power?"
hardware,3dy7fm,nawariata,18,Mon Jul 20 21:56:53 2015 UTC,"You have to match impedance (Ohm) of the driver (4, 8 or 16), power (Watt) is less important but ideally the sum of your drivers power per channel should be in the same range, or greater, as power output of your amp."
hardware,3dy7fm,nexusheli,6,Tue Jul 21 14:10:57 2015 UTC,"Go check out /r/BudgetAudiophile and take a look at their speaker and amp picks - find a little 2-channel amp and some nice, inexpensive speakers, hook it up to the computer and enjoy sound 10x better than what you'll get out of ""computer speakers"""
hardware,3dy7fm,pithed,3,Mon Jul 20 16:45:02 2015 UTC,This is what I did and really am enjoying the sound compared to a relatively expensive computer speaker system I used to have.  My setup now is micca mb42x speakers and a cheap amp and the sound is surprisingly good.
hardware,3dy7fm,bunchajibbajabba,2,Mon Jul 20 18:21:42 2015 UTC,mb42x speakers and a cheap amp   Same with a Lepai amp. Seems a pretty popular combo among the starving audiophiles. Though the real audiophiles on the cheap would probably go for something used on Craigslist.
hardware,3dy7fm,bigtallsob,2,Mon Jul 20 18:58:40 2015 UTC,"I went from a cheap logitech 2.1 setup (forget which one) to the Miccas with an SMSL-50 amp, and honestly, I was completely underwhelmed. Until I added a Dayton sub. Now I absolutely love the setup."
hardware,3dy7fm,ultracrepidarianist,22,Mon Jul 20 20:37:19 2015 UTC,"I did exactly the same thing - Logitech Z-4s bought on clearance, traded for Miccas/SMSL-50 - but was extremely happy from the word go. I don't really care about bass, but I really dig the fact that a person's voice now basically sounds like an actual person's voice. It just really depends on what you're looking for.   (If I still listened to a lot of music, I'd have to spring for the sub. I think these speakers alone would only work for people who love acoustic/classical.)"
hardware,3dy7fm,YennoX,14,Mon Jul 20 22:47:49 2015 UTC,Anyone who cares about sound quality will have a premium set of speakers hooked up. That leaves the casual users who just want something that works.
hardware,3dy7fm,rndnum123,4,Mon Jul 20 15:36:26 2015 UTC,"Every loudspeaker is more or less just a magnet, a wire coil and a ""sheet of paper"" (diaphragm).  Ten years ago we already had relatively cheap strong magnets, copper wire and good materials for diaphragmas.  There likely won't be that much advances on these materials anytime soon, because most of these materials are cheap and do the job well enough. Surely you can use some advanced exotic materials that cost 20x more are harder to manufacturer and might increase the sound efficiency/watt and quality, but the improvements are seldom worth the 20x cost increase.  There has been an impressive improvement in high efficient-low power speakers for phones, monitors which have rendered many separate computer speakers useless, and when you already have a >100 watt sound system for your computer you likely won't need a new high efficient 5 watt speaker (for these purposes, when you don't want to use the big +100watt sound system, the speakers integrated in most of the current monitors likely do the job well enough).  I don't intended to say to build proper high quality loudspeaker is easy, but the tricks for those (physical arrangement, size/ form of box, proper handling/mixing of low pass/ high pass ...) have been already know 10 years ago."
hardware,3dy7fm,sifnt,2,Mon Jul 20 15:18:24 2015 UTC,There is certainly innovation in digitally equalised active systems and plenty of room for improvement in terms of design in how the speaker moves the air around it. E.g. box speakers sound 'boxy' after you've heard dipoles or similarly open designs.  Personally I couldn't recommend the LXMini highly enough (see: http://www.linkwitzlab.com/LXmini/Introduction.htm )
hardware,3dy7fm,tarheel91,1 point,Mon Jul 20 15:34:09 2015 UTC,"I really like the Vanatoo Transparent One for desk speakers.  DSP correction from the factory with digital parametric equalizing and a direct digital amp.  While hardware may not have changed much over the last few decades, signal processing definitely has as you point out.  http://noaudiophile.com/Vanatoo_Transparent_One/"
hardware,3dy7fm,sifnt,9,Mon Jul 20 17:33:40 2015 UTC,"They look like they'd make really good desktop speakers.  I'm still in love with the LXminis and the beautiful, accurate, open sound field they produce. For desktops / gaming speakers a set of Yamaha bookshelf speakers and a used pioneer receiver from ebay with some equalisation serves me really well."
hardware,3dy7fm,PigSlam,4,Tue Jul 21 02:54:14 2015 UTC,"I'm still using these 2.1 Harmon Kardon computer speakers.  They came with a Dell system I bought in 2001.  I've been through 3 computers since then, but the speakers are still as good as anything else I've found.  Last year, I was looking for some speakers for another system, and I found that people were paying $100 for a used set of these things, which is about what they cost me in 2001!  If a design from 2004 is still offered for sale as a higher end product, it's probably because it's a good design, and it just doesn't need updating."
hardware,3dy7fm,feanor512,3,Mon Jul 20 14:47:25 2015 UTC,I'm still using my Logitech Z-2300s. They've been going strong for ten years now.
hardware,3dy7fm,Dr_Midnight,2,Mon Jul 20 15:23:39 2015 UTC,Logitech Z-2300s   Most definitely a solid 2.1 unit.  Logitech Z-5500s here.
hardware,3dy7fm,Evidence_Of_Absence,4,Mon Jul 20 20:30:37 2015 UTC,I'm jealous of all you guys haha. Aside from the sound quality the Z series is one of the nicest looking desktop speaker sets I've ever seen.
hardware,3dy7fm,warepwn3,3,Mon Jul 20 20:41:14 2015 UTC,Speaker technology hasn't changed much and most people don't know what good sound quality sounds like.
hardware,3dy7fm,mtocrat,2,Mon Jul 20 15:20:03 2015 UTC,Thanks guys for all the replies! TIL the age of speakers does not matter as much as that of other electronics or peripherals!
hardware,3dy7fm,november7286,1 point,Mon Jul 20 15:27:45 2015 UTC,You can check out Corsair's SP2500 as it is relatively new. Its the same case as to when Creative used to actively promoted their 'newer and better' sound card till they reach a wall where the next stage of it would be on the audiophile territory.
hardware,3dy7fm,zaures,2,Mon Jul 20 15:50:23 2015 UTC,"The price is a bit prohibitive, although they do seem great :/ the other options I'm looking into right now are all at least a full $100 cheaper than the Corsairs. Thanks though!"
hardware,3dy7fm,sbuck333,1 point,Mon Jul 20 16:17:35 2015 UTC,"Well, here's your explanation. Speakers do continue to come out but you're not really looking in a price category where people care a lot about sound quality."
hardware,3dy7fm,KillKennyG,2,Mon Jul 20 17:06:08 2015 UTC,I upgraded from Z2300s a couple years ago...and haven't looked back. I love those SP2500s.
hardware,3dy7fm,Mildcorma,2,Tue Jul 21 08:21:42 2015 UTC,"I've had the same Logitech X 230 setup for years now. There really isn't really any technological hill to climb to improve upon speaker technology. Our range of hearing is known and we have been able to cover it with established designs. There is no current gen sound, the technology is the same. There is no new technological leap in speaker design when it comes to fidelity. The only real improvement most people can benefit from is getting  a solid external DAC vs an internal sound card to minimize noise from your motherboard, etc. For most people its big time overkill and not worth the investment as most on board sound is pretty decent these days."
hardware,3dy7fm,iPlayRealDotA,3,Mon Jul 20 16:39:16 2015 UTC,"As others are saying, the tech hasn't changed much so if you want a real upgrade you want some studio monitors. Alternatively, you could just get a really beast pair of headphones and get to enjoy the sound 24/7 without neighbors complaining or ruining your Skype call/ Mumble Channel with constant echo."
hardware,3dy7fm,bitingaddict,2,Mon Jul 20 17:47:28 2015 UTC,"at the price point of desktop speakers, there's not much innovation to be had. if they made radically better speakers, then they'd be expensive to the point that you'd be better off buying some studio monitors and building your own audio system. Which is what happens when people want to get better sound, they switch from dinky desktop speakers to studio monitors and the like."
hardware,3dy7fm,mb9023,2,Mon Jul 20 20:26:51 2015 UTC,It's not like other PC parts where tech is always being upgraded; we have the tech and it's not going anywhere fast really so they don't bother.  I bought a set of Altec Lansings for about £100 10 years ago and they're still absolutely spot on sound wise.
hardware,3dy7fm,Dr_Midnight,2,Mon Jul 20 22:00:42 2015 UTC,All about getting 5.1+ speakers and having them all sit in front of you.
hardware,3dy7fm,bomberman447,2,Tue Jul 21 01:39:36 2015 UTC,"I have a Z2300, and it seems like nothing comes close to the quality of the sub. I just ordered a Z623 last night for my TV, and I really hope that it's at least similar to my Z2300's.   eBay has the Z623's for $85"
hardware,3dy7fm,djsd,1 point,Mon Jul 20 15:07:27 2015 UTC,"Yeah the Z-2300 seems perfect for me, I wish they still sold them!! I'm trying to find a used set for a decent price but it seems like demand has only gone up for these and the Z-5500s since Logitech discontinued them."
hardware,3dy7fm,bomberman447,1 point,Mon Jul 20 15:24:28 2015 UTC,"My suggestion is just get the Z623, or build your own 2.1 setup. I got some bookshelf speakers and a pre-amp that I've been super happy with. I'll probably add a sub at some point but I haven't gotten around to it yet."
hardware,3dy7fm,Soytaco,2,Mon Jul 20 15:55:37 2015 UTC,"it seems like demand has only gone up for these and the Z-5500s since Logitech discontinued them.   It really has. I got lucky several years ago when I found a guy willing to part with his on Craigslist. Search through Craigslist and eBay and you'll likely find some sellers through there. You may need to keep an eye out for deals, however."
hardware,3dy7fm,LuckyTtam,0,Mon Jul 20 20:32:45 2015 UTC,"I have the Z623s and am, very happy with them. Very good sound and lots of bass if you want to dial it up"
hardware,3dy7fm,Tchaikovsky1812,1 point,Mon Jul 20 16:54:22 2015 UTC,I find too muchbass or no bass.
hardware,3dy7fm,LuckyTtam,1 point,Tue Jul 21 03:27:12 2015 UTC,Maybe depends on placement of sub? I only run half bass and it is enough for me (I have a shared wall)
hardware,3dy7fm,mtocrat,1 point,Tue Jul 21 16:27:20 2015 UTC,"You should try to think about why you actually want to upgrade them, because, as everyone else has said, there's no reason to suspect a more recently released speaker system will sound better than your old one.   If they're beat up and covered in mountain due, that's a pretty good reason. If you like they way they sound, just pick up another set of the ones you already have or go for something similar. If you think they can't produce as much of some frequency range, or they don't create an immersive soundstage, these are things you can specifically target when buying a new set.  Now, if you really just want better speakers and are happy with stereo, you should consider moving away from the ""PC Audio"" section. Grab yourself a real receiver/amp and a pair of bookshelf speakers and it will blow the pants off anything you'll find marketed toward PC users. For years, I used a Kenwood KR-9400 with a pair of Polk TSi 200s. That setup was amazing, but at some point i realized i wanted desk space, so I moved them out to my TV and got myself a mediocre pair of Logitech Z523s. What do I think of them? Basically the same as any other PC combo.. they sound OK and I have room for my textbooks."
hardware,3dy7fm,LuckyTtam,1 point,Mon Jul 20 16:38:32 2015 UTC,"I went looking for speakers powered from the electric outlet on the wall, but using a USB for digital sound.  Never found any like that.    I very much like my headset that's totally usb, the sound is very crisp and doesn't have any static or noise.  When it's silent, it's silent, unlike how I recall analog being."
hardware,3dy7fm,mtocrat,1 point,Mon Jul 20 17:05:20 2015 UTC,"The closest you'll get is probably an external DAC with active speakers wired to it. Analog from your motherboard can be noisy because the inside of a computer is a noisy place. Of course, passive speakers with a cheap amp might be a better option. All this is assuming you care about quality sound"
hardware,3dy7fm,LuckyTtam,1 point,Mon Jul 20 17:32:45 2015 UTC,"I think that audio companies are missing the boat, everything should be usb!"
hardware,3dy7fm,jankenpwn,1 point,Mon Jul 20 17:51:37 2015 UTC,The kef x300a do that. May be overpriced but at least it shows that it exists.
hardware,3dy7fm,Splutch,1 point,Mon Jul 20 17:55:37 2015 UTC,"Wow that's expensive, but that's the idea!  The only other thing I can think of would be going optical.  I highly doubt optical would become a standard on motherboards before just using USB."
hardware,3dy7fm,Siegfried262,1 point,Mon Jul 20 19:05:01 2015 UTC,"I just checked and the audioengine a2+ are also like that and a lot more affordable. However, I don't think that you should experience static or noise with quality speakers in that price range anyway. It can also be caused by the way you connect things, you should google some fixes as well"
hardware,3dy7fm,Siegfried262,1 point,Mon Jul 20 20:29:49 2015 UTC,"I don't have any issues as I only play with a USB headset and in the unlikely event I didn't use them there's usually background noise in the house that would alleviate all, if any, small amount of speaker noise.  My wife is probably the only person that hears when the speakers are on but the computer is off.  Those speakers are definitely much cheaper!  Maybe some day when I get my own man cave i'll go all out and get something like that.  Edit: Just saw these for much less."
hardware,3dy7fm,dethfrog,2,Mon Jul 20 20:41:56 2015 UTC,"Your typical PC user or even gamer isn't part of the market that does critical listening.  They don't have to innovate, because people will just buy whatever speaker system.  This is totally fine because what they already have works for most people, quality-wise and cost-wise.  Come see us on /r/audiophile if you're interested in moving into the realm of hifi / mid-fi."
hardware,3dy7fm,arikv2,1 point,Mon Jul 20 17:24:23 2015 UTC,I'm using Boston BA745 from the late 90's and they're amazing.
hardware,3dy7fm,anon1821,1 point,Mon Jul 20 17:33:03 2015 UTC,You should consider starting with a quality 2.0-2.1 setup.  I use a Lepai 2020 and a pair of Micca MB42Xs and the sound quality is leagues above the basic computer speakers I used before. Even without a subwoofer they have a decent low end. Pretty hard to beat for the $90-100ish range.   Plus you can always upgrade to better bookshelf speakers down the road.
hardware,3dy7fm,jedimstr,1 point,Mon Jul 20 18:40:31 2015 UTC,"How exactly is that setup done? Computer --> Lepai amp --> speakers? And with what cables? Sorry for all the questions, but I was on /r/Zeos and couldn't really figure it out"
hardware,3dy7fm,WillWorkForLTC,1 point,Mon Jul 20 20:42:42 2015 UTC,No problem.  The speakers are connected to the amp via standard speaker wire and from there the amp is connected to the PC either via a 3.5mm cable or RCA cables.   I believe the Lepai amp comes with a cable but it's not very long.
hardware,3dy7fm,mduell,1 point,Mon Jul 20 20:49:15 2015 UTC,"Pretty much -- for that amplifier, you can use a cable that has a minijack on each end, or one thats a minijack to RCA (for this, just match the colors - white is left channel, red is right channel). From the amp to the speakers, there is aptly named Speaker wire - Nothing special here, its just insulated copper wire. The amp has spring clips (push in 'lever', insert wire, release 'lever') and the speakers you just screw the nut down to clamp in the wire.  edit: for speaker to amp, make sure you connect red to red and black to black; it won't explode, but it can sound funny due to phase issues if you are not consistent."
hardware,3dy7fm,satsun_,1 point,Tue Jul 21 00:46:34 2015 UTC,"I wouldn't suggest Z623.  I went through many speakers before settling.  Z623 are very expensive but just okay sound.  I settled on Edifier Exclaim speakers which sound great with a bit weak bass but strong mid and highs.  I tried Bose but they had awful highs.  Z623 had overly punchy bass and hollow highs.  I would say if you really care about audio then get a good set.  If I were to get another set they would probably be the HARMON KARDON SOUNDSTICKS, but I don't like subwoofers personally."
hardware,3dy7fm,Seclorum,1 point,Mon Jul 20 18:47:17 2015 UTC,I have connected my PC to my expensive (old kenwood from early 90's) stereo amplifier and handmade speakers (Focal brand)  Never cared about pc speakers although when i was a uni student i had a logitech 5.1 set but it was mediocre at best
hardware,3dy7fm,negative_commentary,1 point,Mon Jul 20 18:53:26 2015 UTC,Any love for Magnepan's Mini Maggie 2.1 desktop setup hooked up to a decent Amp?
hardware,3dy7fm,lawrencep93,1 point,Mon Jul 20 20:53:45 2015 UTC,There also might be plenty of old stock. I don't imagine speakers are an item being purchased as frequently as a motherboard or processor or even headphones which get more wear and tear. Computer speakers often survive multiple builds and multiple owners before they are garbage.
hardware,3dy7fm,Elessun,1 point,Tue Jul 21 02:44:08 2015 UTC,I'm still using the Boston Acoustics 2.1 that came with my Gateway Pentium II 400.
hardware,3dy7fm,november7286,1 point,Tue Jul 21 02:58:25 2015 UTC,"Yep, many people are over ""computer speakers"". I'm more into larger sound and many PC speaker packages were weak in that regard so for the past eight years I've been using a full-on 7.1 home theater setup with big speakers; you can use anything with a computer!  If you're slightly more than just casual in regards to audio use at your computer then definitely explore your options outside of traditional computer speakers but just do a bit of research so you're getting the most for your money."
hardware,3dy7fm,JustaPassanger,1 point,Tue Jul 21 03:31:46 2015 UTC,"Desktop speaker dont change much.   Most of the change I've seen in the past 5-10 years is getting more connectivity to allow your speakers to connect to different devices, and a growing adoption of things like Bluetooth and USB capability.   You also have wireless, which dovetails in with bluetooth, but these kinds of things aren't exclusively sold as ""PC"" Speakers anymore. That's because the PC has adopted standardized connections that everything else audio related already uses. So you dont need to limit yourself for ""PC"" speakers, but just look for Speakers.   I think some of the neatest stuff thats come out lately are those flat magnetic plane speakers, they dont need large cones to produce sound. This allows them to be very flat in design. I believe there are some headphones that adopted the same technology as well.   Overall I would say there is less reliance on ""Speakers"" for PC use, and a surge of users with Headphones. You can easily get something like an ODAC and some good Headphones and get great sound.   Check out the guys at https://www.mayflowerelectronics.com/ , I hear they are pretty good."
hardware,3dzpbi,TheTritium,8,Mon Jul 20 20:59:33 2015 UTC,The design is going to break a lot of USB ports. It's placing way too much pressure on the connector (just look at how much difficulty they have plugging it in).
hardware,3dzpbi,Charwinger21,2,Mon Jul 20 23:43:02 2015 UTC,This. This is why they didn't just amend the USB spec to create reversible and backward-compatible cables and had to create a new port instead.
hardware,3dzpbi,FredFS456,-6,Mon Jul 20 23:49:51 2015 UTC,"No, actually, they didn't have to create a new port.  There are lots of reversible cables for purchase on Amazon.  I've personally tested and reviewed many free samples of them (I am a professional product tester).  Some of them are shit.  Lots of them work very well.  The USB standard keeps getting changed because once everyone has a USB cable for every device, they stop buying USB cables.  And people who manufacture USB cables need to have jobs so they can continue the never-ending cycle of working and consuming."
hardware,3dzpbi,negative_commentary,3,Tue Jul 21 07:16:04 2015 UTC,"No, actually, they didn't have to create a new port.  There are lots of reversible cables for purchase on Amazon.  I've personally tested and reviewed many free samples of them (I am a professional product tester).  Some of them are shit.  Lots of them work very well.   It's not the first 100 plug/unplug cycles that they're worried about. It's the next 10,000 (which USB Type-C is rated for at minimum, up from just 1,500 for Type-A).  If you apply pressure onto the central connector the way most of these cables do, that connector will break earlier, and it is much harder to replace a device than it is to replace a cable.  That's not even getting into the size difference that Type-C brings (which allows for unification of the desktop and mobile cables), or the other related features that it enables (which would require a substantial re-design of Type-A to get working, one which would likely result in incompatibility with older ports and cables).   The USB standard keeps getting changed because once everyone has a USB cable for every device, they stop buying USB cables.  And people who manufacture USB cables need to have jobs so they can continue the never-ending cycle of working and consuming.   Type-A has been the main standard for the last 2 decades.  Type-C is going to be the standard for the next two, maybe even longer (as it is more expandable than Type-A was). It unifies both the desktop cables and the mobile cables, and adds quite a few features.     But maybe you're thinking of mobile cables.  MiniUSB was released in October 2000, and was the USB Forum's small-formfactor design until phones started shrinking to a point where they were too small to use it (and were being used more frequently, requiring more durability), at which point they unveiled the MicroUSB connector in April 2007, 7 years later.  That remained the standard until USB Type-C, which was published in April 2015, 8 years later.  They attempted to create a MicroUSB derivative capable of carrying a USB 3.0 signal, which was backwards compatible with MicroUSB 2.0 cables, however they dropped it in favour of developing USB Type-C (and unifying the desktop cables with the mobile cables) shortly after publishing the spec.     Due to the design, USB Type-C is cheaper to make, more durable, smaller, etc.     This is a very welcome change."
hardware,3dzpbi,Charwinger21,2,Tue Jul 21 10:29:46 2015 UTC,I'm not really seeing where they actually plugged it in.
hardware,3dzpbi,Exist50,1 point,Tue Jul 21 01:19:36 2015 UTC,"I'm not really seeing where they actually plugged it in.   Sorry, was thinking of the other crowdfunding campaign going on for a reversible microUSB cable.  Their design appears to still place substantial pressure on the connector though, just like the other one.  Here is a .WebM of the other cable requiring more pressure to plug in than a normal cable would, and they have multiple examples of it requiring a lot of pressure in the video on their kickstarter page (which is supposed to show how ""easy"" it is to plug in)."
hardware,3dzpbi,Charwinger21,5,Tue Jul 21 01:30:14 2015 UTC,Makes some sense with the huge number of micro usb devices in existence. That connector will take a long time to die out.
hardware,3dww3o,aatk,20,Mon Jul 20 05:13:32 2015 UTC,"With data on it, over a year from what I've heard, maybe more.   If you dont give a shit about any data on the drive, you can probably store it as long as you want."
hardware,3dww3o,Seclorum,21,Mon Jul 20 05:17:47 2015 UTC,"There have been some articles indicating absolute worst-case, an unpowered ssd could lose data in just a few days if it's stored somewhere with fluctuating temps.  http://www.zdnet.com/article/solid-state-disks-lose-data-if-left-without-power-for-just-a-few-days/#!  Anand's take on this: http://anandtech.com/show/9248/the-truth-about-ssd-data-retention  edit: added worst-case verbage and Anand link.  edit2: make sure to read the reply from /u/arthurfm."
hardware,3dww3o,unsaltedbutter,38,Mon Jul 20 05:21:39 2015 UTC,"From the AnandTech article above:   All in all, there is absolutely zero reason to worry about SSD data retention in typical client environment. Remember that the figures presented here are for a drive that has already passed its endurance rating, so for new drives the data retention is considerably higher, typically over ten years for MLC NAND based SSDs. If you buy a drive today and stash it away, the drive itself will become totally obsolete quicker than it will lose its data. Besides, given the cost of SSDs, it's not cost efficient to use them for cold storage anyway, so if you're looking to archive data I would recommend going with hard drives for cost reasons alone."
hardware,3dww3o,arthurfm,9,Mon Jul 20 09:35:33 2015 UTC,"Yeah, Generally you dont want to store them in wildly fluctuating temperature zones anyway."
hardware,3dww3o,Seclorum,0,Mon Jul 20 05:35:56 2015 UTC,Like... the inside of a computer?
hardware,3dww3o,DemanRisu,9,Mon Jul 20 10:44:00 2015 UTC,"Your case ranges from ambient to 45C. Not exactly ""wildly fluctuating""."
hardware,3dww3o,Oafah,7,Mon Jul 20 11:31:09 2015 UTC,Unless your computer regularly is fluctuating between over a hundred degrees and then dropping below freezing...
hardware,3dww3o,Seclorum,1 point,Mon Jul 20 15:53:06 2015 UTC,Or it's on the international space station
hardware,3dww3o,fieldofsnowkiller,3,Tue Jul 21 12:28:41 2015 UTC,"duct tapped to the outside, the inside does not fluctuate like that.   And if your device is in space, you have a whole host of other problems to deal with."
hardware,3dww3o,Seclorum,-6,Tue Jul 21 15:57:53 2015 UTC,It's not like the temps outside right now are fluctuating from 60 degrees at night and 90 degrees in the afternoon causing the house to follow.
hardware,3dww3o,MINIMAN10000,15,Mon Jul 20 05:44:27 2015 UTC,"hey man, maybe he lives on Mercury. -173C at night, to 427C daytime.  ...probably still gets better Internet service than I do in Canada :/"
hardware,3dww3o,JaketheAlmighty,-7,Mon Jul 20 06:39:18 2015 UTC,Did a speed test 3 hours ago. Running Comcast at 93 mbps down 12 mbps up.
hardware,3dww3o,MINIMAN10000,3,Mon Jul 20 07:31:09 2015 UTC,"Mate you're lucky, I pay 80bucks a month and get 1mb/s. Straya's fucked."
hardware,3dww3o,AIternate,6,Mon Jul 20 07:37:32 2015 UTC,Do you run AC at all?
hardware,3dww3o,Seclorum,1 point,Mon Jul 20 10:47:15 2015 UTC,Yeah the time when i describe the fluctating temperatures was Pre AC. We have a 8000 btu AC... I don't know about the rest of the house since i close my room door because the animals get annoying. So my room fluctuates like 68 to 78 now. Rest of the house is probably 66 to 72.
hardware,3dww3o,MINIMAN10000,26,Mon Jul 20 06:02:42 2015 UTC,"What a FUD article. I read the source of this data awhile back. The temperature delta that it'd take to do this in real life would never happen unless you were purposefully attempting it. Not only that, but the data retention rates are for absolute worst case scenarios, not the kind of thing that's going to be happening to 99.9% of people. You have a much better chance of large blocks of your HDD randomly corrupting than this. If you're going to be keeping a SSD unplugged for years, then yeah, transfer it to something else. Don't be afraid of keeping one around for months unless you shut the computer down while the drive was located in an icebox and plan to store it on the pavement during a California summer.   Edit: thank you for the anandtech link, it's a much better and more level headed take on SSD data retention."
hardware,3dww3o,Nixflyn,6,Mon Jul 20 06:06:15 2015 UTC,"thank you(and others commenting on this) -the initial hyperbolic article titles FUDed the shit outta me. and i failed to look into it further, yet was considering this was something i needed to practically take into consideration when planning my storage set up. i cant believe i am alone in this.."
hardware,3dww3o,ddeuced,4,Mon Jul 20 05:51:33 2015 UTC,"I've kept a Crucial M4 with data on it in a storage container for over a year before, summertime temps in there probably flluctuating from 45-120f, everything was fine as far as I could tell."
hardware,3dww3o,CRTsdidnothingwrong,4,Mon Jul 20 11:01:12 2015 UTC,"As Anand says, no serious risk at all unless you live on the moon or somthing equally silly"
hardware,3dww3o,hey_aaapple,2,Mon Jul 20 07:04:12 2015 UTC,"But if I never touch it at all? Never install it onto my pc nor use it for anything, just leave it in the box it's delivered in?"
hardware,3dww3o,headband,12,Mon Jul 20 08:35:33 2015 UTC,so then you probably don't really care about the data on it...since its just completely erased.
hardware,3dww3o,ReallyObvious,10,Mon Jul 20 05:29:52 2015 UTC,"the people in this thread are saying over a year IF you want to keep the data on it.   But you if you have nothing on it (brand new drive), then it would last... forever I guess. You could probably keep it until it becomes old and outdated."
hardware,3dww3o,unsaltedbutter,7,Mon Jul 20 05:34:26 2015 UTC,"If you're going to keep it and not give it to your cousin, at least power it up to ensure it's not DOA. But then yeah, it will be fine to store away since you have no real data on it."
hardware,3dww3o,idoithere,4,Mon Jul 20 07:22:31 2015 UTC,client - 1 yr @ 30 C  enterprise - 3 months @ 40 C  standard target  http://www.flashmemorysummit.com/English/Collaterals/Proceedings/2011/20110810_T1B_Cox.pdf  https://blog.korelogic.com/blog/2015/03/24  edit: i forgot to mention this is for data retention number
hardware,3dww3o,A_Light_Spark,8,Mon Jul 20 06:09:08 2015 UTC,"Store on HDD, run on SSD."
hardware,3dww3o,Liam2349,3,Mon Jul 20 14:30:59 2015 UTC,"Other people have already answered your question - but if you're going to keep it, then just use it. It will just become outdated, so make use of it and enjoy it before that happens.  If you don't have use for the SSD space, then just sell your old SSD if it's inferior."
hardware,3dww3o,imanuglyone,-2,Mon Jul 20 13:02:04 2015 UTC,you dont really cause any wear on it until you use it. So you should be fine even with a hypothetical situation of years in storage. But it won't be very good compared to other options that far down the road
hardware,3dww3o,hdshatter,-1,Mon Jul 20 09:21:49 2015 UTC,"Should be fine in storage for a very long time, even if it did break its a Pro series drive so it has a 10 year warranty."
hardware,3dww3o,fRl1jOE_,-7,Mon Jul 20 05:18:03 2015 UTC,You're right but what if the data is irreplaceable?
hardware,3dww3o,pm_me_your_rockets,8,Mon Jul 20 05:50:23 2015 UTC,"Flash memory should be fine in storage. Nothing really moves or will get worn by the air.  If you want to be safe, keep it somewhere cool and dry but it shouldn't really matter.  Modern SSDs will pretty much last forever with consumer use, especially so the nice Samsung you have."
hardware,3dww3o,THS89,5,Tue Jul 21 22:17:19 2015 UTC,Not true. Data on it doesn't last forever. It actually lasts less than regular HDDs. Look it up.
hardware,3dww3o,Born_To_Roam,4,Mon Jul 20 05:59:12 2015 UTC,Is that if you store it away? Or do I have to worry about my data disappearing after a year of use?
hardware,3dww3o,Kaghuros,1 point,Mon Jul 20 09:21:17 2015 UTC,It'll be fine.
hardware,3dww3o,Freeky,1 point,Mon Jul 20 10:17:56 2015 UTC,"Flash memory works by trapping electrons in a floating gate transistor - a blob of conductive material surrounded by an insulator.  Over time the electrons leak out, kind of like a battery self-discharging, making each cell more difficult to read.  The effect worsens based on the size of each cell, and how much data is stored in them.  e.g. the now-infamous Samsung 840 EVO uses 19nm TLC, where each cell must store one of eight different charge levels (corresponding to 000, 001, 010, 011, 100, 101, 110, 111), people report performance degradation after just 3 months.  Data loss will happen eventually.  After a year?  Unlikely, but possible.  You should be prepared either way, make backups :P"
hardware,3dww3o,Born_To_Roam,1 point,Mon Jul 20 10:40:33 2015 UTC,What media would you recommend making backups on?
hardware,3dww3o,Freeky,1 point,Mon Jul 20 16:50:46 2015 UTC,"Hard disks, tapes, archive-grade optical media, cloud storage.  Ideally more than one, all have failure modes to defend against."
hardware,3dww3o,Seclorum,-6,Mon Jul 20 18:30:37 2015 UTC,"Depends on how much you want to spend really.  Optical media do degrade as well, but it's something that you can generally see happen and so long as you watch the disks and replace as necessary you can use them just fine.   Another option is to use a regular spinning rust drive that you hook up as needed to backup, then store in a cool dry place when not needing it.   Another option is to get a Tape system and by a fuckload of magnetic tape and storing everything on that, but really it's not that good an idea.   Backing up and maintaining backed up data is not a 'one system' fixes all solution.   You need to have a process and be on the look out for bad backups so that you can fix/replace media as they die or break down.   My personal solution is to use an external drive, in conjuction with cloud storage for the bulk of my data. I also dont back up things that can easily be replaced, like steam files.   Optical media might work for some people but it gets annoying when you have more data than a single disk to store, and not enough data for multiple full disks."
hardware,3dxrmo,speckz,11,Mon Jul 20 12:15:21 2015 UTC,"Here lies the problem. You have people who don't normally wear watches doing these smartwatch reviews so there's a real disconnect going on here.  There's quite a difference between how a non-watch-user thinks you should use a watch, and how watch users actually use their watches."
hardware,3dxrmo,YennoX,15,Mon Jul 20 14:54:47 2015 UTC,"As a watch user I use my watch as a fashion accessory first, and a time piece second. The Apple Watch is ugly compared to nice mechanical watches and that is essentially a non-starter.  Apples intended audience is everyone as the other poster said. So it makes sense to review the watch as a normal person would (since that's the intended audience). Most normal people (myself included) have yet to find the value of a smart watch or an apple watch. What's the big benefit over pulling a phone out of my pocket? Nobody has adequately answered that question for me personally."
hardware,3dxrmo,pal25,1 point,Mon Jul 20 16:32:38 2015 UTC,"As a watch user I use my watch as a fashion accessory first, and a time piece second. The Apple Watch is ugly compared to nice mechanical watches and that is essentially a non-starter.   Personally, I find the Apple Watch much more attractive than pretty much all the mechanical watches out there. I find it to be the most attractive smartwatch too. I do agree with you though that I would care more about the fashion element of my watch more than anything else. Watches are fashion pieces first.  Problem is, based purely on fashion, the version of the Apple watch I like the most is the 42mm black stainless steel with the black stainless steel band. That'll set me back $1100... and I could never legitimize paying that much for a watch that I'd want to replace so soon because of the software/hardware becoming obsolete.  I don't think I ever see myself buying a smartwatch because of this."
hardware,3dxrmo,Stingray88,3,Mon Jul 20 17:20:20 2015 UTC,"So? I highly doubt AW was made to just appeal to watch users. In fact, I wager to say that most actual watch users aren't even considering a smart watch. Apple is trying to CREATE a demand, thus, someone who doesn't wear a watch is the perfect person to review it."
hardware,3dxrmo,Banelingz,1 point,Mon Jul 20 16:34:27 2015 UTC,"I think youre also somewhat off the mark, apple wants to convince non-watch-wearers to wear watches as well as watch wearers, so they ""want"" to meet the needs of both.  This is why there are multiple review sources, find the one that works for you."
hardware,3dxrmo,Abipolarbears,1 point,Tue Jul 21 18:42:14 2015 UTC,"Is it really a problem though? It's not like Apple is targeting current watch users specifically; if you look at their marketing, it's apparent that they're aiming this at everyone. So what's the problem if reviewers don't typically wear watches? Because most people don't either anymore."
hardware,3dxrmo,icystorm,2,Mon Jul 20 14:59:09 2015 UTC,"If they're targeting everyone, that would include watch users. Not quite getting your logic.  Anyhow, having actual watch users review smartwatches would be a lot more credible and helpful for that demographic. What do you think is more helpful?   A person who hasn't used a watch in their life commenting on ergonomics/usability/build quality   vs   A person who wears watches on a daily basis commenting on the exact same things?   EDIT: If you're counting children and people in their twenties as 'most people', then yes, most people don't wear watches. Once you go up the age or salary ladder, the statistics change."
hardware,3dxrmo,YennoX,0,Mon Jul 20 15:12:59 2015 UTC,I want someone who is a noob to review it since a that is precisely how I would initially use it.
hardware,3dxrmo,pal25,16,Mon Jul 20 16:34:24 2015 UTC,"Love this comment on the site...  jjj - Monday, July 20, 2015 - link Wow Anandtech selling out in a huge way again. Ugly, beyond stupid navigation and software, a lack of a basic understanding of what the product is, faked screen resolution , huge bezels to accommodate an insufficient battery and an oversized vibration ""motor"" . And then there is the price, 400$ for the kids version and 600 for the adult version- if you had any trace of decency you would ""murder"" them in this article just for the price. Or maybe you guys just have the vision of a potato and can't quite grasp the concept of money either. Or maybe you are just cowards and , like any other American publication, you can't even imagine saying anything negative about Apple."
hardware,3dxrmo,ygolonhceT,7,Mon Jul 20 12:33:56 2015 UTC,"Why faked screen resolution? They aren't using some pentile stuff, they are using 3 stripes RGB arranged like |=, don't they? How can you fake with this subpixel layout?"
hardware,3dxrmo,rndnum123,13,Mon Jul 20 13:35:07 2015 UTC,Stop looking for logic in a rant.
hardware,3dxrmo,DanielPhermous,1 point,Mon Jul 20 13:37:07 2015 UTC,"Well said, thx  :)"
hardware,3dxrmo,rndnum123,4,Mon Jul 20 13:39:30 2015 UTC,Maybe they meant in the marketing on TV or online?
hardware,3dxrmo,twatsmaketwitts,2,Mon Jul 20 14:15:07 2015 UTC,http://arstechnica.com/business/2014/12/anandtech-snapped-up-by-parent-company-of-toms-hardware-and-laptopmag/ The original guy sold the site and moved to apple
hardware,3dxrmo,xtothemess,1 point,Tue Jul 21 14:54:38 2015 UTC,"Or maybe you are just cowards and , like any other American publication, you can't even imagine saying anything negative about Apple.   Lol what?  That's the dumbest part of his rant. Every publication and their mother can't wait to pick up on anything bad about Apple to talk about. Even the most minor thing gets reported to death because it's so damn hip to hate on Apple. And it's not just because it's Apple either, it was the same way for Microsoft when they were blowing up in the late 90s and early 2000s."
hardware,3dxrmo,Stingray88,6,Mon Jul 20 17:14:05 2015 UTC,"it was the same way for Microsoft when they were blowing up in the late 90s and early 2000s.   There's still the odd ""M$"" floating around."
hardware,3dxrmo,Blubbey,0,Mon Jul 20 17:32:54 2015 UTC,"Haha of course. The hate runs deep for some, no matter how irrational it may be."
hardware,3dxrmo,Stingray88,1 point,Mon Jul 20 18:03:16 2015 UTC,its funny because Apple easily gets the most media and comment section hate out of any company on earth.
hardware,3dxrmo,poematik,9,Mon Jul 20 19:44:17 2015 UTC,"This is hardly a review and more of a marketing campaign. I've used the apple watch a fair amount, and it is just as worthless as any other wearable, perhaps moreso. Another sad article from anandtech. They used to be so reliable."
hardware,3dxrmo,umop3p1sdn,21,Mon Jul 20 14:24:35 2015 UTC,"I have no reservations in saying that it’s the best wearable I’ve ever used. However, at the same time I find it hard to recommend this first-generation Apple Watch. It’s clear that there are far too many obvious areas to improve upon,"
hardware,3dxrmo,logged_n_2_say,1 point,Mon Jul 20 15:15:35 2015 UTC,"worthless as any other wearable   personally, i find the activity monitoring to be indispensable. there isn't much else to like about the watch though so i can sort of see where you're coming from."
hardware,3dxrmo,sadwoman,1 point,Mon Jul 20 18:30:43 2015 UTC,Maybe worthless for you. The beauty of this device is that its different for everybody. For me its really helped with my excercise routines and its also helped improve my attention span (which is shot from using smartphones for the past decade) because I'm not checking my phone every 10 minutes
hardware,3dxyrc,anon1821,2,Mon Jul 20 13:24:01 2015 UTC,I have the rts one in my new build very nice build quality and price for a gold rated psu.
hardware,3dxyrc,FallenAgist,1 point,Mon Jul 20 21:18:07 2015 UTC,EVGA just launched 550W and 650W versions of their G2 units.  Should give PSUs like these a run for their money.  I only mention this because the G2 750 and 850 have been insanely popular.
hardware,3dw1iz,Exist50,2,Mon Jul 20 00:26:51 2015 UTC,Zen APU will probably be in niche consoles.
hardware,3dw1iz,alainmagnan,1 point,Mon Jul 20 06:39:18 2015 UTC,"I'm skeptical of it based on cost alone. The only reason I think K12 might have a role is the ""embedded"" applications AMD mentions. Not always, but often, embedded processors are small and cheap."
hardware,3dw1iz,TehRoot,1 point,Mon Jul 20 06:45:55 2015 UTC,"It's a chip aimed at the semi-custom server chip market. Many people from medium to large to huge are now starting to see that big fancy x86 chips don't always need to be there and they might be better off using semi-custom ARM chips to cut down on power usage, data center footprint and cooling costs and focus chip performance on certain applications.   K12 was always aimed at the server market, not for phones or tablets.  There will definitely be a Zen APU, but it's not going to be K12."
hardware,3dw1iz,Teethpasta,1 point,Mon Jul 20 21:37:30 2015 UTC,The k12 is certainly an apu. Like all arm chips. You aren't going to have a pcie graphics card in an embedded device like a phone or tablet.
hardware,3dw1iz,HavocInferno,3,Mon Jul 20 01:39:48 2015 UTC,"Ah, but that's the thing. AMD specifically said at the financial analyst day that it won't be investing in phones and low end tablets. Too little money. Now, the datacenter and embedded, where AMD has said K12 will be be targeted, doesn't necessarily need graphics. It's supposed to be far superior to the current ARM server offerings: http://www.anandtech.com/show/8776/arm-challinging-intel-in-the-server-market-an-overview  Edit: And as you can see, not all ARM chips are APUs."
hardware,3dw7y6,Chewie316,51,Mon Jul 20 01:24:51 2015 UTC,"YOU DO NOT NEED TO WORRY ABOUT WRITE COUNTS  Seriously, just stop thinking about them, your SSD is much more likely to die because of controller or firmware or other hardware issues anyways. Backup important data and just accept that like any other hard drive your SSD could possibly fail but probably won't as long as you've bought a quality model."
hardware,3dw7y6,CRTsdidnothingwrong,15,Mon Jul 20 03:26:15 2015 UTC,"This is something I've never understood about SSDs. People rave on about them, constantly suggest to everyone else to get one, and then go on and do everything possible to not use it to it's full potential.  I've seen forum threads about not putting Firefox on an SSD since it has a lot of I/O operations, or suggesting various tweaks to disable disk cache and whatnot, and I'm like why would you get an SSD and then go through all that trouble to not actually use it. I bought one to use it, not worry about it and constantly baby sit it."
hardware,3dw7y6,jazavchar,14,Mon Jul 20 08:09:51 2015 UTC,"Some people have SSD's from 5ish+ years ago, when they had a really bad TBW. Like, really, really bad.  Nowadays it's a completely different story. I've been using my 850 Evo for almost a year now and in total I've written 3TB on it, and the 'rated' TBW is 150TB, while actual tests show something like 600TB+."
hardware,3dw7y6,darkdex52,5,Mon Jul 20 10:12:56 2015 UTC,"I'm actually trying to remember an endemic of ""oh no, my SSD died because it ran out of writes"" from any generation of SSD and coming up blank. There's been various models that have been notoriously unreliable, firmware problems and various growing pains, but I don't recall any huge problems with writes.  I think geeks just have a tendency to be too close to the topic, to pay too much attention to theoretical problems and miss the forest for the trees. And then there's the old wives' tales that just don't die, yet are never backed up by data. If it's not SSDs it's forced automatic windows updates for home users (on balance a good thing for everyone), screwing with your pagefile, unparking cores, and so on.  /rant"
hardware,3dw7y6,plank_,4,Mon Jul 20 12:23:36 2015 UTC,"Editors at big sites talk shit and stir up hysteria. NAND is more robust than you think, and editors need something to write about, I mean SSD reviews could be really short, here is the read and write scores and here is the controller and here is the NAND and these are its features. You don't get paid a lot for writing one page of non hysterical crap, so you have fear mongering going on just like the mainstream media does."
hardware,3dw7y6,xtothemess,1 point,Mon Jul 20 11:06:18 2015 UTC,"This gives me confidence. I have a 5yo SSD that's only 120GB's and have anything not an OS file off the SSD. I'm planning on buying a 1TB Samsung drive soon, and I think I'll just put everything that isn't a video game on it. Except maybe FF14. That's going on it. Though I might get frustrated waiting for others to load in."
hardware,3dw7y6,Drudicta,8,Tue Jul 21 20:53:30 2015 UTC,I tried to tell a few people this regarding ssd reliability and got downvoted to like -3 for it ;_;
hardware,3dw7y6,TheImmortalLS,21,Mon Jul 20 04:04:30 2015 UTC,YOU GOTTA YELL IT  EXUDE CONFIDENCE
hardware,3dw7y6,CRTsdidnothingwrong,11,Mon Jul 20 04:10:27 2015 UTC,Pretty much how it works. If you say something that sounds right people upvote. Ex  I cant believe all the incorrect info in this thread. The ssd ram transfers caused by the antivirus searches corrode the solid state drive by eating all the electrons up. ALWAYS KEEP YOUR ANTIVIRUSES OFF WHEN USING AN SSD.
hardware,3dw7y6,theasocialmatzah,0,Mon Jul 20 04:29:35 2015 UTC,Wow -3!
hardware,3dw7y6,iPlayRealDotA,2,Mon Jul 20 09:43:17 2015 UTC,In a thread with only a few views. Downvotes are relative.
hardware,3dw7y6,TheImmortalLS,3,Tue Jul 21 05:33:59 2015 UTC,"Neither defrag nor scans cause continuous write.  You literally do not need to think about it, write limit is a virtually unheard of failure mode in consumer devices.  If you think this is some delicate matter that requires attention paid to little software intracacies then you don't know what you're talking about. It's not a concern outside of enterprise workloads, and don't even fucking start if you're going to tell me your dicking around in vmware fusion or some shit is equivelant to such a workload.  YOU DO NOT NEED TO EVEN THINK FOR A FUCKING SECOND ABOUT ANYTHING TO DO WITH WRITE COUNTS, IT'S NOT A KNOWN FAILURE MODE IN CONSUMER APPLICATIONS."
hardware,3dw7y6,CRTsdidnothingwrong,3,Mon Jul 20 04:48:17 2015 UTC,"Source please.  The only case of actual continuous writes I know are two SSD stress tests designed to find out how much data an SSD can actually write until it dies, and tested models survived at least 700TB, some are currently at over 2PB I think.  But that's not in any way going to happen in real world scenario, even if you're a power/heavy user.  With average use, a 700TB+ write volume means like 5-10 years of use, more the latter, and likely you'll upgrade or the controller will fail before that.  So yeah, kills in a month, if you take the most extreme and unrealistic stress test you can find."
hardware,3dw7y6,HavocInferno,1 point,Mon Jul 20 05:07:37 2015 UTC,"average use  700 TB in 5-10 years   Wat. That is at least 70 TB per year, who moves that much data."
hardware,3dw7y6,hey_aaapple,1 point,Mon Jul 20 05:04:39 2015 UTC,"Well if you use it as an OS drive for several hours a day, the GBs stack up.  It's just the prediction I took from one of those tests, where they also factor in usual wear and tear, perhaps some spontaneous cell failures (which can happen, more so with smaller process nodes)."
hardware,3dw7y6,HavocInferno,2,Mon Jul 20 08:37:03 2015 UTC,"My Os drive does not have more than 3 GB of writes per day excluding stuff I download/install myself, and that adds up more or less 1 TB/year, 70 times more is just ludicrous IMO.  You would have to do 210 GB of writes every day on average."
hardware,3dw7y6,hey_aaapple,2,Mon Jul 20 09:29:48 2015 UTC,"I can see some rarer cases with this happening. Some people use SSDs for servers, in RAID etc.  But yeah, for the average user actually it's even more than they'll ever use before they'll have replaced the drive anyway. At least I dont know anyone to keep a drive more than 10 years. Or even 5."
hardware,3dw7y6,HavocInferno,1 point,Mon Jul 20 10:16:04 2015 UTC,There are signs(not always of course) before an HDD dies for the inevitable event ie soudns like clicking etc  What are the signs of the oncoming death of an SSD i am very curious since i will be getting an SSD for my OS soon ?
hardware,3dw7y6,anon1821,2,Mon Jul 20 13:22:19 2015 UTC,If your important data is backed up then why does it matter?
hardware,3dw7y6,CRTsdidnothingwrong,1 point,Mon Jul 20 13:18:31 2015 UTC,"I agree just thar is boring to install windows with all your programs..,,i need to use some program that ghosts my os in a single file image"
hardware,3dw7y6,anon1821,1 point,Mon Jul 20 17:41:31 2015 UTC,It's not like it happens often. As long as you get a good ssd it really probably isn't going to fail.
hardware,3dw7y6,CRTsdidnothingwrong,6,Mon Jul 20 18:41:18 2015 UTC,"First off, the write limits on SSDs are extraordinarily high (high TBs or even PBs).  Maybe you know this, and maybe it poses a problem for you (say if you're using a SSD as a scratch disk for manipulating 8K RAW footage for 80 hours a week).  But I'm going to assume you're just an average user.  Virus scans are looking at data--aka reading it.  Reading data on an SSD causes no problems.    TL;DR don't worry about it, run virus scans on your SSD."
hardware,3dw7y6,Stone_The_Rock,14,Mon Jul 20 20:23:08 2015 UTC,"Just make sure you have a decent quality ssd and you can do whatever you want, even Defrag it.  The only reason you wouldn't defrag it is because it has no benefit."
hardware,3dw7y6,homingconcretedonkey,-3,Mon Jul 20 15:16:40 2015 UTC,defraging is detrimental to the life of an ssd although assuming you have a quality ssd you will probably never notice owing to the fact that many ssds can have petabytes worth of data moved through them before they die.
hardware,3dw7y6,dragonbud20,13,Mon Jul 20 01:40:15 2015 UTC,That was my point. Thanks for eloborating I guess.
hardware,3dw7y6,homingconcretedonkey,1 point,Mon Jul 20 02:04:04 2015 UTC,The reason I mentioned it is because there is a negative impact to defraging although just a minimal one.
hardware,3dw7y6,dragonbud20,3,Mon Jul 20 02:35:42 2015 UTC,There are plenty of file tasks you can do that are just as bad as defragging. As i said its no big deal on a decent ssd.  Simply copying a movie to your ssd has a negative impact on it.
hardware,3dw7y6,homingconcretedonkey,-4,Mon Jul 20 02:55:18 2015 UTC,defraging and scanning are completely different. defraging moves files around the drive with the intention of making them sequential on a HDD but on an SSD there is no advantage to this and it only degrades the drive. A virus scan will to some extent degrade the drive but as long as you have a quality SSD this shouldn't pose a problem as many SSDs can have multiple petabytes worth of data saved and over-written before they die and the wear due to the file accession during a scan is minimal.   EDIT: so I thought about it a bit and realized this EVERYTHING you do to a HDD or SSD will degrade it a little bit BUT it really doesn't matter for the most part because they are literally designed to do those exact things as many times as possible.
hardware,3dw7y6,dragonbud20,2,Mon Jul 20 03:00:10 2015 UTC,Scanning is reading not writing so there is not a problem with SSD i guess
hardware,3dqyes,anon1821,60,Sat Jul 18 16:36:13 2015 UTC,"Nintendo has already made the jump from IBM’s PowerPC to x86-based (AMD and Intel) architecture.    What? I'm pretty sure nintendo has used an IBM power PC processor sense the gamecube, hence the perfect compatibility. I'm pretty sure the wii u doses not use x86."
hardware,3dqyes,Illford,23,Sat Jul 18 19:42:41 2015 UTC,This article is poorly written.
hardware,3dqyes,random_digital,45,Sun Jul 19 00:37:51 2015 UTC,You are correct Nintendo has never used an x86 processor.
hardware,3dqyes,reallynotnick,1 point,Sat Jul 18 20:30:12 2015 UTC,I thought the WiiU has an AMD APU in it?  Either way it runs fine and games look rather pretty on it. Especially Splatoon and Smash.
hardware,3dqyes,Drudicta,1 point,Tue Jul 21 20:55:49 2015 UTC,"It uses a PowerPC IBM CPU paired with ATI AMD graphics. PowerPC is pretty much dead, everyone has moved to ARM or x86. All the other consoles are x86 so having architecture and power to match means more, better ports."
hardware,3dqyes,Illford,58,Tue Jul 21 21:22:46 2015 UTC,"If it doesn't want to be a repeat of WiiU they better be ordering something that's a console successor to the PS4/X1. If they just make it somewhat better than the current gen, they'll be left behind again.  Hopefully it's something with our close to Zen as well as uses the FinFet process."
hardware,3dqyes,LuringTJHooker,30,Sat Jul 18 16:42:29 2015 UTC,"It probably just needs to use a similar architecture (x86, GCN, at least some fast memory to give the GPU some bandwidth) and be in the same ballpark of capability so that games can be easily ported to it. I don't think anyone cares about their console's specs.    It might even be a system that can be carried around and used as a portable handheld. It would have to be a low power chip if they're planning anything like that."
hardware,3dqyes,masturbateAndSwitch,17,Sat Jul 18 19:05:27 2015 UTC,"From what they are talking about it will probably be a handleheld, or an ""hybrid"". Since they say they don't see it as a Wii U sucessor, and 3ds is actually older thus beeing a better candidate to be replaced."
hardware,3dqyes,americosg,12,Sat Jul 18 19:37:58 2015 UTC,"This is interesting, especially since Nintendo's modern handhelds have all been ARM based, and they've been wanting to go the mobile route for some time now. I am not aware of any AMD mobile platforms, as the APUs are generally targeted at the very-low end PC space."
hardware,3dqyes,SecretDragoon,10,Sat Jul 18 21:10:34 2015 UTC,"they've been wanting to go the mobile route for some time now   Nintendo has wanted to stay away from mobile. Iwata made comments numerous times about how it's a cesspool of a market that's fad driven and doesn't have staying power.   If the idea that NX is a handheld and new console rather than referring to merging of the OS and platform like we have with PSN has any validity (which I doubt), most likely we'll see something like a next generation 3DS powered by two Puma cores and a home console with eight Puma cores."
hardware,3dqyes,JQuilty,8,Sat Jul 18 22:39:00 2015 UTC,Nintendo has wanted to stay away from mobile. Iwata made comments numerous times about how it's a cesspool of a market that's fad driven and doesn't have staying power.   I was referring to what the investors wanted. The rumor has been that Iwata was the only one standing in the way of Nintendo going full EA with its platform.
hardware,3dqyes,SecretDragoon,14,Sat Jul 18 22:42:49 2015 UTC,"There are gambling addicted stock traders that want it, but that sentiment has been echoed by Miyamoto and others. You don't get $40 ASP's on mobile. The entire market is a farce built on fads and chasing whales. Look at how quickly Zynga has declined when they were on top not even three years ago."
hardware,3dqyes,JQuilty,1 point,Sat Jul 18 22:45:50 2015 UTC,"Don't think 3DS will use AMD cores, they're not very power efficient vs. ARM/Intel. ARM is also more price competitive, and A72 should deliver enough performance to match AMD's current cores."
hardware,3dqyes,CykaLogic,1 point,Sun Jul 19 17:44:45 2015 UTC,ARM AMD cores?
hardware,3dqyes,brianboiler,1 point,Sun Jul 19 23:37:14 2015 UTC,"Long way off, not even specifics announced yet like TDP, core count, and performance estimates."
hardware,3dqyes,CykaLogic,3,Sun Jul 19 23:52:58 2015 UTC,"I am not aware of any AMD mobile platforms   Carrizo is mobile only, and Beema and Mullins are both mobile-oriented."
hardware,3dqyes,Exist50,1 point,Sun Jul 19 01:48:16 2015 UTC,Haven't they stated that it will replace neither?
hardware,3dqyes,LimEJET,1 point,Sat Jul 18 22:17:44 2015 UTC,Supposed it's a replacement for both almost.
hardware,3dqyes,dylan522p,1 point,Sat Jul 18 22:39:11 2015 UTC,"If it's a semi-portable or a dockable, definitely."
hardware,3dqyes,LimEJET,1 point,Sat Jul 18 22:59:57 2015 UTC,"As far as I know they only confirmed Wii U suport after NX, but I could be wrong."
hardware,3dqyes,americosg,1 point,Sat Jul 18 22:58:55 2015 UTC,"They just barely released the ""New"" 3DS that would be required to play some games....   I better not have to buy another handheld any time soon."
hardware,3dqyes,Drudicta,1 point,Tue Jul 21 20:57:49 2015 UTC,They did launch the 3ds 3 years after the Dsi wich did the same thing. Wouldn't doubt they would do it again.
hardware,3dqyes,americosg,5,Tue Jul 21 22:05:10 2015 UTC,I don't think anyone cares about their console's specs.   they actually do care; PS4 outsold and still does outsell XB1 especially because of hardware specs
hardware,3dqyes,zmeul,-5,Sat Jul 18 23:36:51 2015 UTC,"It sold better because the One was shit at launch and they lost a lot of goodwill with fans because of their claims of DRM and always online. Also, the PS4 is friendlier for people outside of the US to use while the big gimmicks of the One rely on American support because of how it's put together. Also, the One was $100 more because of the Kinect.  It had nothing to do with the specs. If specs sold gaming devices, nobody would have a console."
hardware,3dqyes,Westboro_Fag_Tits,2,Sun Jul 19 02:08:24 2015 UTC,riight .. so the fact that PS4 has quite a couple of games that run at 1080p and same games only run on 920p or lower on XB1 has anything to with it .. wouldn't it!?
hardware,3dqyes,zmeul,-3,Sun Jul 19 02:47:44 2015 UTC,"It wouldn't at all. Nobody bought a PS4 because the majority of console players likely can't distinguish between those two different resolutions. I can hardly tell and I would really have to look close to even tell the difference between the two because it's that negligible for the most part.  Like I said... if they cared about specs, they wouldn't be on console to begin with."
hardware,3dqyes,Westboro_Fag_Tits,0,Sun Jul 19 03:22:21 2015 UTC,"yes they did, go ask them ;)  Digital Foundry has a whole section dedicated to this; here's a very recent one : https://www.youtube.com/watch?v=tkB8gpPzMkw - close to 400k views"
hardware,3dqyes,zmeul,-1,Sun Jul 19 03:25:55 2015 UTC,"The PS4 won this gen because it was cheaper at launch (399 vs 499), has better graphics on multiplats (although thats not so much the case anymore) due to its specs, and its more of a global console versus the XB1 which has most of its services/features reliant on american ISP's"
hardware,3dqyes,poematik,1 point,Mon Jul 20 18:22:37 2015 UTC,"and its more of a global console versus the XB1 which has most of its services/features reliant on american ISP's   and X360 didn't!? for a very long period of time X360 kept outselling the PS3 despite having free multiplayer services (the PS3)  so, yeah ... your explanation falls flat"
hardware,3dqyes,zmeul,1 point,Mon Jul 20 19:21:17 2015 UTC,"No personal attacks, you've been warned."
hardware,3dqyes,OSUfan88,1 point,Mon Jul 20 19:42:35 2015 UTC,"Fair enough. I never do that, but the guy was non-stop flaming everyones comments.   I won't do it again."
hardware,3dqyes,masturbateAndSwitch,7,Mon Jul 20 20:25:08 2015 UTC,"The PS4 is $100 less    Edit: was, ignore me"
hardware,3dqyes,cupofjoe1357,1 point,Tue Jul 21 15:43:38 2015 UTC,It was at launch. Now the Xbox one (minus Kinect) is $50 cheaper than the PS4.
hardware,3dqyes,zmeul,1 point,Tue Jul 21 20:06:48 2015 UTC,"was, now XB1 is ~50$ cheaper depending where you shop and what bundle you want with it"
hardware,3dqyes,Exist50,5,Tue Jul 21 20:26:44 2015 UTC,Why not ARM?
hardware,3dqyes,MINIMAN10000,6,Sat Jul 18 23:57:43 2015 UTC,"Ugh this is my thought anytime with handhelds and then my next thoughts is why would I buy a arm processor based handheld when I'm already holding one of the industry's best arm processors aka my phone. Of course the answer is as it always is. Handhelds have unique interfaces like thumbsticks and first party games. But at this point in time I'd say phones have more first party games and I'm not likely to base my entire decision buying a handheld just on first party games.  So I guess after typing this out the most convincing reasons would be a game based store instead of a generic ""google apps"" that is curated to a higher standard and a better interface.   Phones really should come up with something like the nvidia sheild... although I really wish they put the display in the middle of the controllers like the wiiu controller. Then after that they have a proper input it would be beneficial for them to have a library app that could be downloaded for games that are meant to be played with buttons and joysticks."
hardware,3dqyes,Exist50,1 point,Sun Jul 19 00:09:49 2015 UTC,"ARM can be much stronger than even current flagship phones. I expect K12 to be a strong ARM CPU, but Apple and Samsung are doing a good job. Still, think of ARM with a ten-fold increase in tdp. That frees up plenty of development room."
hardware,3dqyes,Exist50,2,Sun Jul 19 00:18:32 2015 UTC,"ARM can be much stronger than even current flagship phones. I expect K12 to be a strong ARM CPU, but Apple and Samsung are doing a good job. Still, think of ARM with a ten-fold increase in tdp. That frees up plenty of development room.   When x86 and ARM are put on a level TDP playing field, x86 absolutely demolishes ARM. ARM makes zero sense in a home environment, although maybe AMD acquisition of an ARM-based company now makes sense - x86 for home, ARM for mobile.  That's why, since Intel's Medfield, you haven't really seen anything in the mobile phone space from Intel, they are having a hard time getting x86 down to the power levels required."
hardware,3dqyes,Exist50,-3,Sat Jul 18 21:13:29 2015 UTC,"AMD's very specifically said that they don't want to produce low end ultra mobile hardware. The margins are too small.    When x86 and ARM are put on a level TDP playing field, x86 absolutely demolishes ARM.   I'd like to know what specific ARM and x86 processors you are comparing. I may be leaping to conclusions, but from this interview with Jim Keller, the ISA really doesn't matter all that much. It's how you use it. If K12 truly does compare to Zen (sister cores), it'll likely be the most powerful ARM CPU available. AMD's ARM ambitions are mostly server-based, which needs different performance metrics than a phone or other mobile device.   That's why, since Intel's Medfield, you haven't really seen anything in the mobile phone space from Intel, they are having a hard time getting x86 down to the power levels required.   Intel's still trying to compete there, but I don't see Sofia as a viable product (really just leans on Intel's networking tech) and Atom has rightfully not gotten far."
hardware,3dqyes,CykaLogic,2,Sat Jul 18 21:23:11 2015 UTC,I may be leaping to conclusions   You are.
hardware,3dqyes,Exist50,-1,Sat Jul 18 21:48:35 2015 UTC,"So what processors are you comparing? Come on, give some evidence for x86 ""demolishing"" ARM."
hardware,3dqyes,CykaLogic,0,Sun Jul 19 02:51:27 2015 UTC,"So what processors are you comparing? Come on, give some evidence for x86 ""demolishing"" ARM.   Cortex A15 - demolished by an Intel afterthought (Z2760).  That was 2 years ago - Intel has released nothing since."
hardware,3dqyes,Exist50,1 point,Sun Jul 19 07:20:35 2015 UTC,"Uh... Silvermont(Z35xx) is destroying 800/801 and that's on 22nm with a 3 year old core. I'm sure the next generation will beat the 820, especially with the overheating 810 fiasco helping Intel gain marketshare."
hardware,3dqyes,dylan522p,1 point,Sun Jul 19 08:12:30 2015 UTC,"Intel's losing billions to gain marketshare. And mind you, that's not in phones. Intel's efforts there have been abysmal. Intel can't compete at the same tdp as these ARM chips, and even if it could, not for the same price."
hardware,3dqyes,Exist50,2,Sun Jul 19 08:16:12 2015 UTC,"2w tdp, comparable to Snapdragon 805  And Intel can absolutely compete at the same price if they gut their margins like they are doing now."
hardware,3dqyes,dylan522p,1 point,Sun Jul 19 08:25:59 2015 UTC,"Thanks for the link. Forgot about that phone. Still as you can see, it can't beat current gen chips on the CPU side and is slaughtered in battery life and graphics. Cherry Trail seems to offer little over Bay Trail. I think K12 and Zen, having a closer relationship than any other pair of ARM and x86 chips, will show the true strengths of the two ISAs. Otherwise it is essentially impossible to separate the ISA from the implementation. It's clear that x86 is at a disadvantage here, though."
hardware,3dqyes,Exist50,1 point,Sun Jul 19 17:46:18 2015 UTC,Apple has amazing opportunity it's just they can't scale the clocks up. If they could run their cores at 4ghz they would be right up there with intel and IBM in performance per core
hardware,3dqyes,dylan522p,1 point,Sun Jul 19 18:37:09 2015 UTC,"I think that's by design, though. It's far easier to make something low clocked than high clocked, and 4GHz will basically be a tripling of the current clock speed. I personally expect K12 to be the strongest (or at least among the strongest) per-core we'll see from ARM unless Apple decides to expand their custom CPUs beyond the iphone and ipad. Which is possible."
hardware,3dqyes,thinkythought,1 point,Sun Jul 19 18:53:24 2015 UTC,I doubt it. The new MacBook seems perfect proof. They would have scaled up their own architecture to a quad core with a even. More powerful gpu but what Intel can do in 4.5W is way better than what apple could do
hardware,3dqyes,daubertMotion,1 point,Sun Jul 19 19:33:55 2015 UTC,"Well there's obvious issues transitioning from x86 to ARM. Not to mention, a company as big as Apple would need a lot of volume from the fabs, likely on a leading finfet process, which really wasn't available for the Macbook launch. I'm definitely not saying I see it happening for sure, but a computer like the Macbook (low power, highly content consumption oriented) would be the perfect fit."
hardware,3dqyes,Exist50,1 point,Mon Jul 20 01:26:38 2015 UTC,Core m is so good though.
hardware,3dqyes,JaketheAlmighty,1 point,Mon Jul 20 01:32:03 2015 UTC,"Phones really should come up with something like the nvidia sheild... although I really wish they put the display in the middle of the controllers like the wiiu controller. Then after that they have a proper input it would be beneficial for them to have a library app that could be downloaded for games that are meant to be played with buttons and joysticks.   100% of this exists on stuff like the JXD phone, and a few others. The main issue is junky software, meh CPUs, and sketchy build quality.   Sony needs to make another Xperia play, ugh."
hardware,3dqyes,MINIMAN10000,1 point,Mon Jul 20 02:12:41 2015 UTC,"Yeah, the NVIDIA shield console is already more powerful than the wiiu and that thing sips electricity.  They could totally go to NVIDIA for an SoC if they wanted.  While the tegra x1 is powerful for what it is, its still not at the level of the xbone or Ps4, but they could easily make a custom solution for Nintendo that could beat the current co soles on gpu power."
hardware,3dqyes,CykaLogic,-4,Mon Jul 20 02:33:32 2015 UTC,"Eh, Nvidia's been weak on the CPU side. And the company doesn't seem to like the margins the console market offers."
hardware,3dqyes,Klorel,10,Mon Jul 20 03:33:54 2015 UTC,"I don't think anyone cares about their console's specs.   Has to be true. If a console player cared about specs, well they wouldn't have a console for very long would they?"
hardware,3dqyes,cupofjoe1357,2,Mon Jul 20 00:59:42 2015 UTC,"The console specs are compared with other console specs. Consoles provide an environment where your performance in a game is consistent with everyone else's. Usually leaving crippling framerates a thing of the past.  People are also suckered into buying overpriced PCs or not knowing enough about PC specs to know what is good and paying $400 and expecting it to play ps4 games.  They need to find information sources like a local computer store that could tell them ""what could play battlefield 4 and last for the next 4 years"" or so.  I can't help but feel at this point in time with the information people have they would go to spend $1,700 best buy tells them is a top of the line computer and it's a $600 computer with 64 gb of ram."
hardware,3dqyes,sifnt,4,Mon Jul 20 20:53:58 2015 UTC,Players don't. Devs and publishers do. If it takes a large effort to port a game to your system because it has low specs publishers won't even bother.
hardware,3dqyes,Exist50,4,Mon Jul 20 20:57:08 2015 UTC,"nintendo never focused upon hardware or graphical fidelity. i bet the next console won't change this. and that's totally ok to me. don't get your zen hopes too high, you could easily be disappointed."
hardware,3dqyes,OSUfan88,1 point,Sat Jul 18 19:37:16 2015 UTC,"Except at this point it's hardly a matter of what Nintendo wants to focus on. If they're going to try and get 3rd party publishers on board again, they need a system that can at least compete with the other consoles on the market. Developers aren't going to develop for a machine that's prohibitively weak, as the Wii U has demonstrated."
hardware,3dqyes,HeliconPath,1 point,Sat Jul 18 21:28:44 2015 UTC,"The PS4 / XB1 are also much lower specs relative to the PC's of the time than the previous generation, Nintendo could match them this time without taking a loss...   Ideal for them would be making games on the system have to run at 1080p 60fps to be approved, and giving it the 50%-100% stronger GPU than the PS4 that coming 2-3 years late to the party allows them. Would be a strong PR argument, on-top of their already awesome first party games."
hardware,3dqyes,ddrt,1 point,Sat Jul 18 21:19:50 2015 UTC,"The PS4 and XB1 are probably sold on very thin margins. Nintendo's historically had the cheaper console, so they'd have to cut somewhere."
hardware,3dqyes,BrainSlurper,1 point,Sat Jul 18 20:53:02 2015 UTC,"I really doubt a Nintendo console released in 2016/2017 will be more powerful that Xbone or PS4. The WiiU came out 7 YEARS after them, and is barely more powerful.  If it comes out next year, I bet it is less powerful than both, but close. It will have some feature which neither of them have. If it comes out in 2017, there is a chance it's more powerful.  This is just my guess. I firmly believe they need to be more powerful, but I don't know if they'll break their standard."
hardware,3dqyes,ddrt,5,Sun Jul 19 00:13:33 2015 UTC,"nintendo never focused upon hardware or graphical fidelity.   Well, at least not from the Wii onwards."
hardware,3dqyes,poematik,-2,Sun Jul 19 10:58:21 2015 UTC,"Being ""left behind again"" is defined, in this case, as being obsolete."
hardware,3dqyes,BrainSlurper,3,Sun Jul 19 14:59:11 2015 UTC,"Being ""left behind again"" is defined, in this case, as being a nintendo console"
hardware,3dqyes,poematik,1 point,Tue Jul 21 15:47:10 2015 UTC,Fervent console elitism is not only detrimental to conversation but also damning to ones' character.
hardware,3dqyes,Thotaz,0,Mon Jul 20 00:25:51 2015 UTC,"cant believe people say this, I remember when nintendo consoles were considered to have high end hardware inside. times have changed."
hardware,3dqyes,dylan522p,1 point,Sat Jul 18 20:22:21 2015 UTC,Can't believe that I say this or can't believe that it is true? I don't think it's debatable that the wii and wii u were laughably underpowered.
hardware,3dqyes,Hagtzel,1 point,Sun Jul 19 09:24:43 2015 UTC,"They were, but the original wii was sold out for almost 2-3 years straight. Nintendo hit gold with the casual market. I just wish they'd use that money to make a proper ""hardcore"" console"
hardware,3dqyes,Exist50,1 point,Sun Jul 19 19:37:19 2015 UTC,"At this point I think even if Nintendo makes a stronger system they still won't get a ton of players that weren't going to buy a weaker Nintendo console anyway, and they won't be getting any better third party support because it's still the same type of players on that platform (Nintendo fans almost exclusively)."
hardware,3dqyes,Exist50,1 point,Mon Jul 20 18:24:43 2015 UTC,Think if it's an amd apu. With newer tech so it's better design in cpu and gpu. They can match the performance at half or forth the power with a smaller die in 2-3 years. They can cut in price and easily have all the ps4 and xbone games avaliable
hardware,3dqyes,Exist50,1 point,Mon Jul 20 18:36:06 2015 UTC,It just needs to be a bit better than PS4/One to crush the market.
hardware,3dqyes,Exist50,1 point,Mon Jul 20 18:51:00 2015 UTC,Nintendo still has a ton of corporate and image problems to overcome. It's certainly not just the hardware.
hardware,3dqyes,jorgp2,1 point,Sat Jul 18 21:49:57 2015 UTC,"If it doesn't want to be a repeat of WiiU they better be ordering something that's a console successor to the PS4/X1.   They can easily do that and hit their price target. The PS4/XB1 were relatively under-powered when they arrived and AMD has had almost 2 years to improve upon themselves.  If I had to bet though, I'd say Nintendo is using a custom version of the Carrizo mobile APU with a custom AMD graphics chip.  http://www.anandtech.com/show/9319/amd-launches-carrizo-the-laptop-leap-of-efficiency-and-architecture-updates"
hardware,3dqyes,Exist50,0,Mon Jul 20 03:19:42 2015 UTC,"IMO, unless this console is a ways out, Carrizo is out of the question. It's just too cutting edge for Nintendo, given its history."
hardware,3dqyes,jorgp2,3,Sun Jul 19 01:04:56 2015 UTC,"IMO, unless this console is a ways out, Carrizo is out of the question. It's just too cutting edge for Nintendo, given its history.   AMD doesn't release technology to retail first. It's likely Carrizo was funded by Nintendo.  Kabini and the PS4/XB1's custom APU (both Jaguar products) were released about the same time in 2013."
hardware,3dqyes,CykaLogic,0,Sun Jul 19 01:20:04 2015 UTC,"It's likely Carrizo was funded by Nintendo.   That's quite a statement there. And there's a reason I said Nintendo specifically. Sony and Microsoft have the money and ambition to go for the higher end tech, but Nintendo has lagged behind. The GPU in the Wii U is based on a 2008 design. Carrizo definitely has some cost advantages (SoC, smaller die with HDL), but it is still more expensive than Kabini. Also, a lot of Carrizo's advantages stem from lower idle power and power use for video streaming and playback. Neither of which matter much for a console."
hardware,3dqyes,skilliard4,2,Sun Jul 19 01:18:04 2015 UTC,"That's quite a statement there.   It is, it's a guess. Just like yours."
hardware,3dqyes,Exist50,1 point,Sun Jul 19 01:46:56 2015 UTC,"Well, not really. I gave solid reasoning for my statements. There is literally no evidence to suggest Nintendo funded Carrizo. That is not how AMD does semi-custom work, even for Sony/Microsoft. You do realize Excavator has been on the roadmap for a while, right?"
hardware,3dqyes,Schmutzli,1 point,Sun Jul 19 02:04:37 2015 UTC,"But Sony and Microsoft did not find a new core.  On the contrary they held up their low power team for a few years, causing features to be cut from puma+, kaveri, and having Excavator to be delayed a year."
hardware,3dqyes,Exist50,1 point,Sun Jul 19 07:26:14 2015 UTC,You're going to have to elaborate on this a bit.
hardware,3dqyes,Le_rebbit_account,-1,Sun Jul 19 08:11:36 2015 UTC,"Remember the ARM coprocessor that was supposed to be added in 2012, well they had to buy and ARM license, and in the end it was in the silicon but it was not activated.  They had to delay their first ARM server Cores, and cancel their ARM mobile Cores.  They could not build a successor to puma, leaving them to just release a slightly refreshed part. Puma+.  And Carrizo was slated to launch late 2014, with desktop parts coming in this summer."
hardware,3dqyes,zmeul,1 point,Sun Jul 19 08:15:14 2015 UTC,"Well, not really. I gave solid reasoning for my statements.   No, you really didn't.  I don't really want to argue with you, but this is playing out exactly like the PS4/XB1 release.  I have knowledge about things that you don't."
hardware,3dqyes,Tuna-Fish2,1 point,Sun Jul 19 13:34:39 2015 UTC,Doubt it will use Zen because consoles need at least a year's time for verification and stocking. Maybe 16nm considering TSMC is already starting up volume production and this console is targeted for 2017.
hardware,3dqyes,zmeul,11,Sun Jul 19 14:29:12 2015 UTC,"Nintendo fans aren't really swayed by graphics. Nintendo's sales in recent years have been driven by creating niche features. The Wii was a huge success due to motion controls, the DS succeeded due to touch screens, but devices like the Wiiu flopped because it didn't do anything unique.  3DS tried to do something unique, but 3d proved to be too much of a niche as it caused headaches to many, requires a very specific viewing angle, and drove up costs of the device.  PS4 and Xbox fans will fight to the death over which has better graphics, but Nintendo tends to be more about delivering niche exciting features. Perhaps Nintendo could pick up on Virtual Reality with their next console, and that will bring them back into the market."
hardware,3dqyes,Tuna-Fish2,10,Sun Jul 19 14:53:09 2015 UTC,"This article comes across as just a bunch of speculation, coupled with some factual inaccuracy"
hardware,3dqyes,Exist50,2,Sun Jul 19 08:27:58 2015 UTC,Yeah for one the wii u is not x86  Why did they even publish this
hardware,3dqyes,Tuna-Fish2,1 point,Sun Jul 19 17:43:48 2015 UTC,Page views. There is literally nothing new in this article. I'd rather we wait till there's something a bit more concrete before rushing to speculate. And let's just say that some of the ideas being thrown around in this thread are a bit far fetched...
hardware,3dqyes,zmeul,7,Mon Jul 20 10:26:39 2015 UTC,Welcome to /r/hardware
hardware,3dqyes,Tuna-Fish2,6,Sat Jul 18 22:12:03 2015 UTC,"AMD claims that the new SoC “expands the base” of its semi-custom business beyond gaming, which means that the APU is not developed with a game console in mind.   http://www.kitguru.net/components/anton-shilov/amd-begins-development-of-its-third-major-semi-custom-design/"
hardware,3dqyes,zmeul,0,Sat Jul 18 23:48:05 2015 UTC,"That was the third, iirc this is the fourth."
hardware,3dqyes,Exist50,2,Sun Jul 19 00:12:10 2015 UTC,"one is PS4  2nd is XB1  3rd?!?! if you're thinking of Wii U, it uses a IBM CPU with a TeraScale GPU"
hardware,3dqyes,xAsianZombie,3,Sun Jul 19 12:13:30 2015 UTC,"The third is something not yet publicly released. All we know about it is that AMD said they were working on two new semicustom APUs, one of which was something beyond gaming in their investor briefing a while back.  The timescale for new designs is years, so it's not exactly surprising that we haven't heard anything concrete about that third design yet."
hardware,3dqyes,QuantumInteger,1 point,Sat Jul 18 16:51:44 2015 UTC,We sure it's an APU? http://www.kitguru.net/components/cpu/anton-shilov/amd-develops-custom-server-microprocessor-for-facebook-source/
hardware,3dqyes,Exist50,-2,Sat Jul 18 17:49:13 2015 UTC,"I did remember that they talked about an APU, but went back to check and didn't find it mentioned. You're probably right."
hardware,3dqyes,wickedplayer494,2,Sat Jul 18 21:49:57 2015 UTC,"again, what is the 3rd? there is no 4th  as for official, there's no confirmation on it; not even on Nintendo actually doing the NX .. it's a rumor still"
hardware,3dqyes,everyZig,-2,Sat Jul 18 21:55:29 2015 UTC,"At their last investor relations meeting, AMD had previously released two semicustom designs, and stated that they were working on two new ones. One of those new designs was stated to be based on the ARM architecture, and another on the x86. And one of the designs was stated to not be a gaming device.  AMD officials cannot flatly lie to investors in a investor relations meeting, or they risk going to prison. (However, presenting all information with the most positive spin they can imagine is normal.) Therefore the idea that there are two new semicustom designs is pretty well confirmed. We don't know what they are, though."
hardware,3dqyes,Exist50,2,Sat Jul 18 22:00:31 2015 UTC,unless Nintendo will use the ARM APU since the rumor said it won't be in the same performance league as PS4/XB1
hardware,3dqyes,everyZig,5,Sat Jul 18 22:11:24 2015 UTC,"If Nintendo were to theoretically use a K12-based APU (I don't think this is likely, but for argument's sake), then it could definitely be more powerful than the PS4/XB1."
hardware,3dqyes,Exist50,6,Sat Jul 18 22:00:10 2015 UTC,Wouldnt it be funny if nintendo releases a console more powerful than the X1 and PS4
hardware,3dqyes,everyZig,1 point,Sat Jul 18 22:10:44 2015 UTC,It'll be even more hilarious if they get zero third party support because of their archaic approach to 3rd party games.
hardware,3dqyes,Exist50,1 point,Sat Jul 18 22:50:29 2015 UTC,"Not going to happen, at least not in 2016 and probably not 2017."
hardware,3dqyes,everyZig,3,Sun Jul 19 23:57:33 2015 UTC,"PowerPC needs to die in a fire already. Apple realized it needed to way back in 2005 and already had x86 versions of Mac OS X ready to go in the event Apple became unhappy with IBM, which they did because they could only meet 2.5GHz instead of the 3 hoped for a year earlier. Nintendo needs to burn PowerPC too."
hardware,3dqyes,Exist50,1 point,Sun Jul 19 01:14:43 2015 UTC,"PPC isnt nintendo's problem, their restrictions on physical size, cooling and power use are.  The wii-u couldve been a lot more powerfull if nintendo was OK with hardware using more then a few dozen watt"
hardware,3dqyes,everyZig,1 point,Sun Jul 19 01:19:06 2015 UTC,"It could also have been much better if Nintendo had used newer, more efficient tech. They easily have the headroom for something like Carrizo, but Nintendo doesn't go with new tech."
hardware,3dqyes,alainmagnan,1 point,Sun Jul 19 01:22:12 2015 UTC,"The wii-u was launched in 2013 though, two years ahead of Carrizo, and also before Kaveri. Kabini might have been possible, but only having your chip available a few months before launch is kinda tricky with regard to devkits and such. (and stock Kabini SUCKS in terms of GPU power, so they wouldve needed a PS4/X1 custom APU too)"
hardware,3dqyes,n00b_reker_Porygon2,1 point,Sun Jul 19 12:34:24 2015 UTC,"I don't mean that they could have used Carrizo specifically, just that it is possible to fit a reasonable amount of performance in that power envelope. IMO, you can have low power or old tech. Trying to fit both will not produce good results."
hardware,3dqyes,omarfw,1 point,Sun Jul 19 14:17:32 2015 UTC,"Oh yeah, things like the 45w TDP-configurable Kaveri chips are a good indicator of what can be done in a low power envelope, but going non IBM/AMD wouldve made Wii backwards comp nearly impossible (or at least hard, given nintendo's already overstretched software teams another emu to write/port)  Here's to hoping they make some good choices for their next gen hardware, at this point Nintendo really needs to pull off a home-run if they want to stay in the home console business."
hardware,3dqyes,everyZig,1 point,Sun Jul 19 23:08:30 2015 UTC,"Microsoft got backwards compatibility working, but I was shocked when I heard that. Sounds like a massively impressive feat of software work. I doubt Nintendo can do the same."
hardware,3dqyes,MilkyTones,1 point,Mon Jul 20 05:49:11 2015 UTC,"Oh yeah. i am massively impressed by MS getting 360 emulation/BC (however it works) working on the X1, amazing stuff"
hardware,3dqyes,hdshatter,1 point,Mon Jul 20 05:55:24 2015 UTC,"I do wonder if it wouldn't be possible for Nintendo to brute force emulation if they update to new enough tech. It'd hardly be ideal, but is at least doable from the Wii back."
hardware,3dqyes,Scrabo,1 point,Mon Jul 20 06:00:32 2015 UTC,"Oh, that should be possible, given that Dolphin is open-source they could even fork that (license permitting), and enough power to emulate the Wii shouldnt be an issue for their next machine (i bloody well hope anyway), but the software...  So far Nintendo has only been able to do emulators on a game by game basis for the virtual console, delivering and maintaining a Wii emulator that works for 90% of titles out there is no easy feat, look at how well xbox BC worked on the 360, a relatively short list of supported games, and bugs and known issues in roughly half of those.  The hardware shouldnt be a problem, but the software? i doubt nintendo could pull it off given their current situation."
hardware,3dqyes,Exist50,1 point,Mon Jul 20 06:11:34 2015 UTC,"Apple ditched PowerPC because IBM killed its mobile roadmap, which Apple would've hated since more than half of its PC sales are from mobile."
hardware,3dqyes,Schmich,1 point,Mon Jul 20 06:23:00 2015 UTC,ohh my god. I am so surprised.
hardware,3dqyes,Exist50,1 point,Mon Jul 20 06:29:10 2015 UTC,"If they're smart, they'll produce something that is superior to the Xbone and PS4, AND offers an experience that PC doesn't already do but better.  Right now they've got that second one in the bag, while Microsoft and Sony suck at it. They just need to work on the first part. I think Nintendo knows this and that the ""family"" budget gaming market isn't going to get their products off the shelves anymore like it used to.  Then again, it's Nintendo. Their decisions are either genius or baffling."
hardware,3dqyes,tru3gam3r,1 point,Mon Jul 20 06:32:44 2015 UTC,"Right now they've got that second one in the bag   And look how that is working out for them, their first party games (one half of that experience) is the only reason the wii-u even sells at all, the other half, the gamepad, is the albatross around its neck.  I do like the gamepad, playing MK8 without needing the TV is nice, but nintendo's implementation of the off-screen feature is half-arsed as always, and if i could have bought a wii-u without the gamepad for $75-100 less, i wouldve gotten one waaaay sooner, and it would sell a lot more (i think)"
hardware,3dqyes,Exist50,1 point,Mon Jul 20 06:46:15 2015 UTC,"You guys keep saying how much more power Nintendo needs, but that unfortunately isn't enough to boost their sales."
hardware,3dqyes,dylan522p,1 point,Sun Jul 19 00:28:38 2015 UTC,It will be newer but still have worse specs than a ps4.
hardware,3dqyes,Scrabo,4,Sun Jul 19 05:52:29 2015 UTC,So what could it be? I imagine it isn't going to be a powerhouse because Nintendo will want to keep it cheap ($300-$350) so 28nm and no massive 8core+2048-4096SP APU or HBM.    CPU Cores Process Clock GPU    Jaguar(PS4) 8 28nm 1.6GHz 1152SPs GCN 1.0   Puma (NX) 8 28nm up to 2.5GHz 1280-1536SPs GCN 1.2?   Excavator (NX) 4 28nm up to 3.4GHz 1280SPs GCN 1.2?   ZEN (NX) 4-8 14nm up 4 GHz? 2048SPs GCN1.3/2.0?    Zen is supposedly coming q3 2016. It's possible for anyone aiming for holiday 2017 but I doubt it would be ready for holiday 2016. The processor would need to be ready a long time before launch so they can test their case setup/cooling and also need a long run up for mass production.
hardware,3dqyes,dylan522p,0,Sun Jul 19 14:29:18 2015 UTC,Zen will probably be far too expensive for Nintendo to stomach.
hardware,3dqyes,Scrabo,1 point,Sun Jul 19 07:27:19 2015 UTC,"Zen is an architecture, no a CPU. They could make a light version and possibly even make it for 28nm.  For the sake of AMD and Nintendo I hope they will be able to pump out something that smashes the other consoles and doing it at low heat and price points."
hardware,3dqyes,dylan522p,1 point,Tue Jul 21 21:38:29 2015 UTC,"I doubt 28nm would resolve the cost issue. If it's even possible. There hasn't been any precedent for a node enlargement. And what would a ""light"" version even be?"
hardware,3dqyes,kalfis049,0,Sat Jul 18 20:23:12 2015 UTC,"A Pentium g3258 is haswell, just like a 5960x is haswell. You can make different processors from an arch."
hardware,3dqyes,dghelprat,1 point,Sat Jul 18 21:12:31 2015 UTC,"Understood, but that's largely the result of binning. For a custom chip, AMD would have to start with something marginally more powerful and only use one version for the soc. People don't seem to understand how small and cheap the cat cores are. http://chip-architect.com/news/2013_core_sizes_768.jpg  Zen will be a relatively large core on a cutting edge node. That isn't cheap."
hardware,3dqyes,NBurg,1 point,Sat Jul 18 21:35:04 2015 UTC,Where are you getting these stream processor and power counts from?
hardware,3dqyes,Exist50,2,Sat Jul 18 21:55:24 2015 UTC,"Guessing, which is why there's a ? at the end. Most of the things that don't have a question mark I got the info from launch reviews/wiki spec lists."
hardware,3dqyes,CRTsdidnothingwrong,1 point,Sun Jul 19 15:35:32 2015 UTC,Wherever you got those power consumption numbers for the ps4 are just wrong.
hardware,3dqyes,salgat,1 point,Sun Jul 19 15:43:52 2015 UTC,It was wrong. I think what happened is that I tunnel visioned tdp on the Jaguar wiki page right next to the PS4 section. None of the tdps made sense so I removed the column. Carrizo with 20CUs obviously wouldn't still be 35w. Thanks for pointing it out.
hardware,3dqyes,CRTsdidnothingwrong,0,Sat Jul 18 21:08:11 2015 UTC,Exactly even        ~1000 stream processors will consume quite a bit of power
hardware,3dqyes,destraht,2,Sat Jul 18 21:15:53 2015 UTC,Nintendo doesn't have the balls to outspec the Xbone and PS4.  NX will be slightly less powerful. You can count on that.
hardware,3dqyes,Schmich,1 point,Sat Jul 18 22:37:54 2015 UTC,"Graaaaaammar!  Other than that, there's a big chance that AMD will survive by becoming a console peasant hardware provider."
hardware,3dqyes,destraht,-4,Sat Jul 18 23:13:18 2015 UTC,I read that they dont make a whole lot of money on console hardware... NVidea said they didnt push hard this gen necause there isnt much profitability.
hardware,3dqyes,CykaLogic,12,Sat Jul 18 23:27:57 2015 UTC,"Then again, that could be a bit of damage control on Nvidia's part."
hardware,3dqyes,barack_ibama,-4,Sun Jul 19 16:34:09 2015 UTC,"This is their bread and butter, I bet we're going to see AMD shed off other areas over the next year or two leaving them mostly a custom x86 based APU developer for consoles and embedded systems."
hardware,3dqyes,BrainSlurper,4,Sat Jul 18 19:22:46 2015 UTC,"Not really. Consoles sell in the millions per year, while laptops and desktops sell in the hundreds of millions (which is why Intel is pushing so hard for embedded GPUs). It's definitely a great source of income but only makes up a small fraction of their total income."
hardware,3dqyes,bizude,5,Sun Jul 19 14:14:02 2015 UTC,Do you know anybody who's bought a laptop or desktop with an AMD CPU in the past 5 years? I don't.
hardware,3dqyes,dylan522p,1 point,Sun Jul 19 17:33:31 2015 UTC,"In poor countries they are popular because for around $500 you could get a big 15.6"" system with 768x1068 1366x768 that would actually place some decent games.  Over in Ukraine they light their notebooks big and portability isn't top on their top list nor are premium materials."
hardware,3dqyes,bizude,0,Sat Jul 18 18:07:23 2015 UTC,I'm confused at your resolution! Don't you mean 1366x768?
hardware,3dqyes,jorgp2,1 point,Sat Jul 18 19:33:30 2015 UTC,Yes.
hardware,3dqyes,dylan522p,1 point,Sat Jul 18 19:39:13 2015 UTC,"In poor countries people are just buying phones and tablets instead of laptops, you can't beat $200 price point that does everything a casual user would need to do."
hardware,3dqyes,jorgp2,3,Sat Jul 18 20:02:11 2015 UTC,"Eh, you still can't do much productivity work in a tablet, cheap laptops are still a go for third-world students who want to do their coursework plus some light gaming."
hardware,3dqyes,dylan522p,-4,Sat Jul 18 21:37:43 2015 UTC,third-world students who want to do their coursework plus some light gaming. shit talk me in dota
hardware,3dqyes,LiberDeOpp,5,Sun Jul 19 01:30:14 2015 UTC,"The only laptops I'll even consider are AMD laptops because laptops with dedicated graphics are way too expensive and Intel's iGPUs aren't good enough for gaming.   Sure, an a10-based laptop isn't going to be able to hold a candle to my i5-4690k/r9 290x rig, but for basic gaming @ 768p the APUs do a decent job."
hardware,3dqyes,nolander,1 point,Sat Jul 18 21:20:50 2015 UTC,Have you seen iris 6200? Haha amd cant even compete with that plus skus with 6200 have a Intel quad core. You can have a fairly thin laptop with a 35/45w mobile processor that knocks the socks off anything amd can put out.
hardware,3dqyes,comcast_ebola_tyson,1 point,Sun Jul 19 11:07:16 2015 UTC,"No I haven't seen it, because there aren't any laptops for sale with said CPUs yet.   I've seen a few reviews, sure, but nothing a typical consumer can purchase.  Googling for Core i5-5675R, Core i5-5575R, & Core i7-5775R brings up nothing but announcement pages.  Edit: Instead of downvoting me, how about showing a single laptop with an Iris 6200 that can be purchased?"
hardware,3dqyes,Petrieiticus,1 point,Sun Jul 19 12:10:38 2015 UTC,"35w ha, ha.  Meanwhile AMDs newest APU is just behind Haswell in performance. Can decode two 4k videos at once, has lossless color compression, proper driver support, an integrated south bridge, has support for Explicit Multi Adapter, and runs off of 15w."
hardware,3dqyes,CRTsdidnothingwrong,0,Sat Jul 18 21:28:36 2015 UTC,Its a little bit behind in gpu but cpu gets obliterated. The only way carried is better is price or you're kidding yourself. Even with the higher Tdp the Intel processor is likely more efficient
hardware,3dqyes,JaketheAlmighty,1 point,Sun Jul 19 02:34:41 2015 UTC,"No, AMD has always obliterated Intel's mobile processors."
hardware,3dqyes,bizude,1 point,Sun Jul 19 03:55:53 2015 UTC,Lol which why they have less than a tenth the market share Intel has and amd laptops get signifipless battery life and performance.
hardware,3dqyes,jakobx,1 point,Sun Jul 19 13:43:16 2015 UTC,I have an amd laptop because I wanted to play games while away from home cheaply. I dont use amd in my desktops but they aren't bad.
hardware,3dqyes,namae_nanka,3,Sun Jul 19 16:02:50 2015 UTC,I've got two in the last few years
hardware,3dqyes,CRTsdidnothingwrong,5,Sun Jul 19 16:11:59 2015 UTC,I did.
hardware,3dqyes,bizude,1 point,Sun Jul 19 16:35:40 2015 UTC,"Problem is that in order to be competitive AMD chose to sell these to Sony and MS at very close to cost.  That's one of the many reasons, despite owning majority of the console market, that AMD has posted such high losses the past few quarters."
hardware,3dqyes,LiberDeOpp,2,Sat Jul 18 21:36:27 2015 UTC,"I don't think those orders are why they posted losses.  I mean if they didn't make money at all on those deals they suck pretty hard at business.  Losses were probably more so on GPU and CPU r&d. The Fury X might be sold at a loss, but not consoles."
hardware,3dqyes,bizude,2,Sat Jul 18 23:13:30 2015 UTC,they make close to nothing on the consoles - margins are razor thin.  nVidia had the PS4 if they wanted (continuation of the deal they had when they did the PS3) but they chose to walk away entirely.
hardware,3dqyes,LiberDeOpp,1 point,Sun Jul 19 00:40:50 2015 UTC,"They've still made $600 million in profit from the consoles, and if they didn't have those sales they'd be much further into the red."
hardware,3dqyes,dylan522p,1 point,Sat Jul 18 18:45:30 2015 UTC,And what exactly could nvidia offer at that time? A tegra powered console?
hardware,3dqyes,Schmich,2,Sat Jul 18 19:12:00 2015 UTC,No no nvidia totally walked away from it!! To spread the glory of gameworks on PC.
hardware,3dqyes,gibonez,0,Sat Jul 18 19:41:55 2015 UTC,"Ok, but close to nothing is still something, it's not a loss, so therefore those losses they're posting aren't likely due to the consoles. Even if they only broke even on the consoles, their huge losses are coming from somewhere else."
hardware,3dqyes,coolsilver,1 point,Sat Jul 18 21:33:36 2015 UTC,"Problem is that in order to be competitive AMD chose to sell these to Sony and MS at very close to cost.   Yes, but not nearly as ""close to cost"" as you might think. ~20% profit margin.  http://www.kitguru.net/components/anton-shilov/amd-console-chip-business-is-thriving-margins-set-to-grow-to-over-20/"
hardware,3dq6pz,bphase,18,Sat Jul 18 11:05:37 2015 UTC,"Good to hear, look forward to seeing how the new products perform. Just looking it up:  http://www.tsmc.com/tsmcdotcom/PRListingNewsAction.do?action=detail&newsid=6181   Hsinchu, Taiwan – October 24, 2011 –TSMC (TWSE: 2330, NYSE: TSM) today announced that its 28nm process is in volume production and production wafers have been shipped to customers   The 7970 (first 28nm GPU) was paper launched late December 2011 with a full release early Jan 2012. So if things are going well on the side of AMD and Nvidia (are they both still using them?) we could see new stuff before the new year if the wafers aren't gobbled up by competitors."
hardware,3dq6pz,Blubbey,8,Sat Jul 18 12:54:53 2015 UTC,"Would be great, but rumours are saying Q2/16 for nVidia at least. I suppose it's possible we might get some lower-power GPUs before that, though."
hardware,3dq6pz,TaintedSquirrel,7,Sat Jul 18 13:47:10 2015 UTC,we could see new stuff before the new year if the wafers aren't gobbled up by competitors.   Doubt it.  Both Nvidia and AMD have said Arctic Islands/Pascal won't be out until 2016.  Unless they mean January.  The process might be ready by the end of 2015 but I would be surprised if the GPUs themselves are.
hardware,3dq6pz,everyZig,2,Sat Jul 18 21:27:06 2015 UTC,"Awesome, my 960 will serve me well for quite a while, but im a sucker for powerful low power cards, just imagine a 750Ti shrunk to 16nm, or what a new <75W maxwell chip"
hardware,3dq6pz,anon1821,1 point,Sat Jul 18 13:41:12 2015 UTC,When the 960 was launched it had horrible performance but lately i see in benchmarks with newer drivers that is holding up well(or they are gimping similar performance kepler cards like the 760)
hardware,3dq6pz,everyZig,-1,Sat Jul 18 16:38:56 2015 UTC,"When the 960 was launched it had horrible performance   Eh, not really, it was/is on par with the 285, the 280/x were a better deal, but also just AMD clearing old stock, and in terms of heat/power the 960 is leagues better then anything AMD has."
hardware,3dq6pz,TaintedSquirrel,3,Sat Jul 18 16:54:50 2015 UTC,B-Stock has them for about $160:  http://www.evga.com/Products/Product.aspx?pn=02G-P4-2966-RX  You can also get them brand new today from Staples @ $160 with the 50-off-200 Visa checkout discount.  If you want a 960 priced competitively with the 280 it's pretty easy to do.
hardware,3dq6pz,Blubbey,0,Sat Jul 18 21:28:36 2015 UTC,"Speculation on my part but I'd expect 960/285/280 performance to be possible with the next cards assuming they use HBM, Nvidia may even get around 970 performance there but that's pushing it. Nvidia have an easier time of getting there given their current (quite significant) perf/w advantage though."
hardware,3dq6pz,dudemanguy301,1 point,Sat Jul 18 14:17:25 2015 UTC,"Well that's one peice of the puzzle but I'm sure we'll be waitin on HBM2 production for pascal, and whatever comes after Fiji."
hardware,3dq6pz,TaintedSquirrel,1 point,Sat Jul 18 18:28:35 2015 UTC,I would like to see mid-range Pascals on GDDR5 just to save some money (HBM is expensive and low supply).  But I doubt it's that easy to mix and match GDDR5 and HBM on the same GPUs.
hardware,3dq6pz,arikv2,22,Sat Jul 18 21:25:51 2015 UTC,This is why I won't drop ludicrous money on a 980ti. Next gen cards will be the first all new chips in 3 years. I'd rather wait on the performance jump until then.
hardware,3dq6pz,CRTsdidnothingwrong,33,Sat Jul 18 13:23:17 2015 UTC,"This is why I won't buy new computer parts, because newer better computer parts will be out soon...   Repeat to infinity."
hardware,3dq6pz,TheRealRacker,34,Sat Jul 18 14:29:46 2015 UTC,"There are always better parts, yes, but sometimes there are just slightly better parts and sometimes there are actually big technological jumps and provide a much greater boost."
hardware,3dq6pz,CRTsdidnothingwrong,15,Sat Jul 18 15:24:29 2015 UTC,"But it's wrong to imply it's some bad time to buy a card.  It's completely about your personal situation, me personally, I've got a 680 and I'm in the same boat, I'll probably just wait until Pascal.  But for someone without an already decent build? There's nothing bad about right now to buy a 980ti. At $650 with it's performance is pretty much the greatest deal of all time for a GPU up until now, and we're looking at at least near a year until Pascal, if not more than that, so you'll likely be gaming on the top dog chip of the industry for 12 months, that's a great run.  It always feels like there's a huge leap just around the corner, because there is, 680-780ti-980ti, if you were serious about not spending money because a big leap is coming then it wouldn't have made since to buy any of these chips, but they were all badass and a good value in their time.  TL;DR Just because waiting is the best decision for your situation, don't sit there and justify your decision by trying to assert it's the right decision for everybody."
hardware,3dq6pz,TheRealRacker,4,Sat Jul 18 16:58:40 2015 UTC,I never said it was a wrong time?  I simply said why many people are holding out until 16nm.
hardware,3dq6pz,OSUfan88,2,Sat Jul 18 17:09:11 2015 UTC,"What you said is fine. CRT is making a good point about what many people say and think, and projected it to your post. I don't think it really disagrees with you post. IMO, you both made good points.  I agree with you. I currently don't have a gaming pc (I have a pc from 2008 with a Nvidia 640 gpu, and a sawzaw'd pc case to make it fit). I plan on building an oculus rift PC in 2016, when the next generation of hardware comes out. I agree with you, it seems to be the biggest jump in the past few years.  New intel chipset, DDR4, Fast SSD, Pascal GPU arch in a smaller node. Count me in!"
hardware,3dq6pz,TheRealRacker,2,Sat Jul 18 17:29:55 2015 UTC,"I really hope 16nm brings at least decent gpu performance ugrades.  I'm on a 280x(7970), which is first gen 28nm and still plenty for just about every game on 1080p 3 years later.  Also when the 7000 series launched the midrange 7870 was equal to the previous fastest card the 580, and there were two cards above it.  Since then each new fastest card has mostly just been one step up faster and not two.  The midrange 16nm graphics cards should be as fast as a 980 or fury at around $300, that would make me happy."
hardware,3dq6pz,CRTsdidnothingwrong,2,Sat Jul 18 17:37:48 2015 UTC,"really hope 16nm brings at least decent gpu performance ugrades. I'm on a 280x(7970), which is first gen 28nm and still plenty for just about every game on 1080p 3 years later. Also when the 7000 series launched the midrange 7870 was equal to the previous fastest card the 580, and there were two cards above it. Since then each new fastest card has mostly just been one step up faster and not two. The midrange 16nm graphics cards should be as fast as a 980 or fury at around $300, that would make me happy.   Lol, here you go again...  This is my whole problem with your comments, you're making it out like the GPU industry is seeing disappointing incremental gains, when it's completely the opposite, GPU performance is jumping forward in leaps and bounds, aside from mobile processing GPU's are the most actively progressing field of computing power."
hardware,3dq6pz,OSUfan88,1 point,Sat Jul 18 18:04:11 2015 UTC,"Yeah. Should be pretty incredible. I am going to save up and buy the most powerful gaming CPU I can buy in Q1/Q2 2016, and a power supply which can operate at least 2 high end GPU's. I'll buy the equivalent of the 980ti or FuryX at the time as well, and may buy a second one in 2017 or 2018 to SLI.  I'm pretty excited as this will be the first build I've done in about 10 years. I think VR is going to really put a strain on computers, and I think we are getting ready to see the biggest computer boom that we've seen in the last 20 years."
hardware,3dq6pz,Evidence_Of_Absence,5,Sat Jul 18 21:20:04 2015 UTC,"At $650 with it's performance is pretty much the greatest deal of all time for a GPU up until now   Depends how you look at it, of course it might look great because tech keeps advancing, but I wouldn't call $650 a great deal. There has been time a plenty when you could get top or close performance for about half that.  Such was the case with the R9 290, as well as the older HD 4870/4890 and 5850/5870. The GTX 680 vs 7970 had decent price wars as well.    Those have all been times with great competition going on, which means excellent price/performance and affordability. Such is not the case with the 980 Ti IMO. Granted these top dog chips are monstrous these days which is part of the reason for high prices."
hardware,3dq6pz,CRTsdidnothingwrong,3,Sat Jul 18 17:24:39 2015 UTC,"There has been time a plenty when you could get top or close performance for about half that.  Granted these top dog chips are monstrous these days which is part of the reason for high prices.   The 8800 Ultra launched in 2007 with an MSRP over $800.  I absolutely agree with your point about none of these cards being a ""great deal"", but this is not a recent thing.  It's been going on 8 years now, even if it did skip a few generations."
hardware,3dq6pz,Seclorum,2,Sat Jul 18 17:29:53 2015 UTC,"The 680 was a small chip, gk104, that's why it was cheaper, about the same as the 980, the small maxwell.  Comparing big maxwell to big kepler, 980ti vs 780ti, it's a better value than the 780ti was."
hardware,3dq6pz,Schmich,14,Sat Jul 18 18:01:14 2015 UTC,"This is true, but you gotta admit 16nm + HBM2 + new µarch is enticing and might be worth waiting for. Unlike waiting for new CPUs or something where there's nothing interesting on the horizon.   But yes, generally you should upgrade when you aren't happy with your performance anymore."
hardware,3dq6pz,CRTsdidnothingwrong,2,Sat Jul 18 15:31:46 2015 UTC,"Unlike waiting for new CPUs or something where there's nothing interesting on the horizon.    Well there is the possibility of Zen, but yeah. From announced and known things we dont have much to be excited about in the CPU sector."
hardware,3dq6pz,Roph,2,Sat Jul 18 18:32:19 2015 UTC,I disagree. The industry moves like a staircase than a straight line. It's silly to buy when there's just about to be a price-cut for example. Or would you have bought a Titan X mid-May? :S
hardware,3dq6pz,CRTsdidnothingwrong,3,Sat Jul 18 21:44:01 2015 UTC,"The Titan X was pretty obviously outside the realm of value.  Yes the industry moves like a staircase, and the 980ti is a fresh new stair. Saying it's better to wait is just looking up at that stair from below and going ""nah, I'm good, one more step bro, that's where it's gonna be"".  Pascal is a year away, if there's any good time to buy a high end GPU like a 980ti it's now, when it's just come out."
hardware,3dq6pz,arikv2,1 point,Sat Jul 18 21:48:11 2015 UTC,I wouldn't buy a 28nm GPU in 2015.
hardware,3dq6pz,Evidence_Of_Absence,1 point,Mon Jul 20 09:21:42 2015 UTC,So wouldn't buy the best thing available because something better is around the corner?  Same bullshit.
hardware,3dq6pz,arikv2,6,Tue Jul 21 01:26:13 2015 UTC,No it's why I won't buy the highest tier when it's obviously overpriced.
hardware,3dq6pz,Evidence_Of_Absence,2,Sat Jul 18 15:07:21 2015 UTC,So overpriced that the highest tier offerings from both companies are flying off the shelves.
hardware,3dq6pz,CRTsdidnothingwrong,3,Sat Jul 18 16:28:25 2015 UTC,"Good for company, bad for the consumer.  60fps@1440p takes a 700$ single component.  It's too much!"
hardware,3dq6pz,arikv2,6,Sat Jul 18 16:43:30 2015 UTC,"I'd argue it's great for the average consumer actually.  The margins on those high-end products are much better than the others.  Titan buyers are funding R&D for everyone at a much higher rate than anyone else.  It's one hell of a progressive performance tax.  Judging price by some arbitrary performance threshold like 60fps at 1440p is asinine, both because it's going to move around every year, and because it affects a very small section of the market."
hardware,3dq6pz,EnsoZero,1 point,Sat Jul 18 16:58:09 2015 UTC,"You can make all the arguments you want for buying mid range cards, but you'll never have the joy of having THE BEST.  It's just two different strategies, I could've bought a 660ti then a 970 a few years later for the same price just buy the 680 on launch day and hold on to it.  Sure, I'd have a 970 now, which would be better than my 680. But it was worth it for the years of having the 680 instead of the 660ti, and the 680 is still good enough to play most things."
hardware,3dq6pz,CRTsdidnothingwrong,2,Sat Jul 18 17:02:33 2015 UTC,680 was not 700$ at launch...
hardware,3dq6pz,EnsoZero,2,Sat Jul 18 17:11:28 2015 UTC,"Typically true, but this is a major generational shift and not your standard iterative upgrade."
hardware,3dq6pz,CRTsdidnothingwrong,0,Sat Jul 18 19:22:36 2015 UTC,But this time it's different   Repeat every 2-4 years to infinity.
hardware,3dq6pz,EnsoZero,6,Sat Jul 18 19:39:55 2015 UTC,"Closer to 4-5 but there are generational cycles and you should try to time your upgrades with those if possible. As far as this is concerned this is the first big step in memory in 8 years, first big step in chipsets in 4 years and the first new major Windows API update in 6 years."
hardware,3dq6pz,rePAN6517,1 point,Sat Jul 18 19:48:47 2015 UTC,"Are you talking about Pascal or Skylake?  Waiting for Skylake makes sense for everybody, it's only a month away, I'm talking about Pascal."
hardware,3dq6pz,Blubbey,3,Sat Jul 18 19:50:06 2015 UTC,"Talking purely about Pascal.  No point in waiting for Skylake as over the past 8 years or so there is very little reason to upgrade CPUs other than motherboard features.  I'm also not saying people shouldn't buy a 980Ti, just that they should understand that they're buying in on the tail end of a new tech and if they don't need to upgrade right now they should strongly consider waiting."
hardware,3dq6pz,imallin,2,Sat Jul 18 19:53:22 2015 UTC,Exactly.  Plus I want to save for every drop of GPU power I can get when it comes time for the Oculus & HTC VR headsets to be released.
hardware,3dq6pz,Schmich,1 point,Sat Jul 18 13:33:21 2015 UTC,"Yeah good thinking, for Nvidia it'll be the HBM2 + new node combo and AMD have said their new GPUs will be double the perf/w so they should be worth the wait."
hardware,3dq6pz,Yearlaren,1 point,Sat Jul 18 14:08:11 2015 UTC,"Except that TSMC is routinely 2 years behind their claims.  For reference;  TSMC entered ""volume production"" of 20nm in January 2014.    If we're lucky,  we'll see a new GPU on something other than 28nm by the end of 2016.   The other issue with new nodes as of late, is that they offer only a slight reduction in power consumption without offering any sort of ability to clock higher (see intel's 14nm process, or Samsungs 14nm process in the S6 phones) which will limit just how much they'll be able to do with the new nodes.  We're not going to see the same type of leaps we've seen in the past."
hardware,3dpc4q,Reapexx,63,Sat Jul 18 03:54:59 2015 UTC,"Full copper heatsink, that thing is fuckin legit."
hardware,3dpc4q,the_unusual_suspect,1 point,Sat Jul 18 04:00:03 2015 UTC,"Couldn't agree more.  However, a fair amount of people like to watercool these cards.  The obvious replacement is a full-copper waterblock; however, that almost seems like a waste."
hardware,3dpc4q,Stone_The_Rock,24,Mon Jul 20 15:21:02 2015 UTC,The purpose of this card is benching under water/ln2 or not? What's the point of a really nice air cooler? So gamers will buy it too?
hardware,3dpc4q,BlayneTX,18,Sat Jul 18 04:35:13 2015 UTC,"It's all about the crazy vrm so yeah ln2 cooling, the copper heat sink is just to rope in average Joe who wont get much of a boost in clocks over a standard acx cooled card. Unless it's binned like the gigabyte g1."
hardware,3dpc4q,Gunjob,24,Sat Jul 18 04:57:47 2015 UTC,Supposedly these cards are made from chips that get an ASIC of 85 or more. They are as overkill-binned as one can get.
hardware,3dpc4q,CaptSkunk,7,Sat Jul 18 05:26:37 2015 UTC,My GTX 770 has an ASIC score of 92. It's a mediocre overclocker.
hardware,3dpc4q,SeaJayCJ,9,Sat Jul 18 11:44:50 2015 UTC,"That's why I think that the ASIC Score is just a useless number...  This is just something, that popped up out of the blue, nobody can really explain clearly, how the score is achieved and doesn't even always give you an indication of anything."
hardware,3dpc4q,CaptSkunk,2,Sat Jul 18 11:49:41 2015 UTC,I agree. It's another number for people to circlejerk over. Who cares what your ASIC score is? I want to know what your core clock is!
hardware,3dpc4q,SeaJayCJ,10,Sat Jul 18 11:57:59 2015 UTC,What rating system is that?
hardware,3dpc4q,Exist50,19,Sat Jul 18 06:05:16 2015 UTC,"GPU-Z has a feature that lets users check the ""ASIC quality"" of their GPUs.  Here's a screenshot of me checking my 980's ""ASIC quality"".  How this is determined exactly is beyond my knowledge.  Whether or not it actually impacts overclockability is also beyond my knowledge."
hardware,3dpc4q,lucyrion,11,Sat Jul 18 06:47:44 2015 UTC,"Wait, this chart suggests that lower ASIC Quality benefits Overclocking under water or ln2?  Does anyone know how that´s supposed to work and shouldn´t the K|NGP|N cards be binned for lower ASIC quality if that´s the case?!"
hardware,3dpc4q,hans_ober,17,Sat Jul 18 07:24:06 2015 UTC,"Not too sure if I'm correct, but anyway:  Leaky transistors = Low ASIC = ""Hot silicon"" = High static power leakage = low threshold voltage = lower voltages required for high clocks.  Non-leaky transistors = High ASIC = ""Cold silicon"" = Low static power leakage = high threshold voltage = needs more voltage for clocks.  In short, some transistors leak less i.e. use less static power, but require extra voltage when overclocking. On the other hand, some leak more, but are overclocker friendly because they can hit high clocks at lower voltages.  That is why High ASIC chips have lower power consumption (less leaky), low default voltages and are fine if you overclock on air, as thermals NOT voltages/clocks are the limiting factor.   If you overclock on water/LN2; you've got all the thermal headroom you need, and if you have a good power supply, you can push the silicon to it's limits.. and this is where leaky transistors perform better. High ASIC chips are more power efficient, but cannot be pushed too much.   Think of 2 pipes: thin and thick.  thin= high ASIC = less water (power,voltage) flow, but can't handle high volume.. if you want more water to flow, you'll have to increase the pressure. thick = low ASIC = under normal conditions it lets more water get through (leaky), but if you want high volume flow, it needs less pressure for equal flow compared to a thin pipe.  Analogy? Ferrari vs Prius. A Prius is fine for most people because they don't need the performance (lots of voltage, high clocks for overclocking) and the Prius uses less fuel too.. cheaper to run too(less power, less heat). If you can afford a guzzler (high static leakage) and want performance (overclocking perf), choose the Ferrari."
hardware,3dpc4q,andromeduck,8,Sat Jul 18 08:36:07 2015 UTC,"Not 100% correct. They also sample the transistor speeds, clock distribution and the like and from those data points plug into a model which then decides what to do with the chip.   Sometimes a module/core/bank is defective or doesn't have the same properties as the rest so it that module gets cut.  It's one of the reasons you see so many SKUs for one chip sometimes - especially with Intel and Nvidia though it's also party market segmentation when you see some professional features axed on consumer chips IMO.   When they decide to put out a product it's usually a after they've gone through a run of the chips, made a model of yields, and divided up the performance brackets to roughly correlate with market demand.  Having more flexibility and redundancy helps us waste less good parts but too much and you're going to end up wasting them again.   It's really complicated balance.  I also remember reading somewhere Nvidia was a pioneer for this process but don't remember specifics other than it was a competitive advantage in the early days."
hardware,3dpc4q,hans_ober,4,Sat Jul 18 09:26:36 2015 UTC,"Yeah, that happens too. Different i5s, i7s with different clocks (100Mhz) apart, depending on their binning characteristics.  AMD does it with their CPUs, which is why you could re-enable a disabled core on their CPUs to make it a quad. You could also reflash a 7950 into a 7970, provided that the 7970 had been reduced to a 7950 not because of defective units, but because it didn't hit the clocks/voltages/power that it was needed to.  The 7970Ghz edition was basically a 7970, but they were using cherry picked samples that could run at higher clocks without using too much power. Remember the 7990? Cherry picked samples.  IIRC, the GTX 590 / Radeon 6990 also had a similar binning process. I think the 590 had less leaky chips (idk why, ask Nvidia) compared to the GTX 580 because using leaky chips wouldn't be of any use cause they needed to fit 2 on a board, power was a limiting factor; and the 590 wasn't designed to/couldn't overclock anyway."
hardware,3dpc4q,tedlasman,1 point,Sat Jul 18 09:34:33 2015 UTC,"So, can the fury be unlocked to become a fury x?"
hardware,3dpc4q,lucyrion,1 point,Sat Jul 18 12:53:14 2015 UTC,Awesome explanation thank you!  Dou you know of anyone actually binning cards for lower ASICs then? wouldn´t that be more interesting for many people since the yould watercool such a high end card anyways?
hardware,3dpc4q,hans_ober,2,Sat Jul 18 08:47:49 2015 UTC,"I'm not too sure about low ASIC binning.  Static leakage is becoming a big problem and power losses due to static leakage are increasing faster than dynamic power due to smaller transistors, which is why you see FinFETs. FinFETs manage to reduce static leakage and have better gate control.  Don't worry to much though, as most chips are power gated; which means that instead of keeping a certain section working at low clocks (power loss is high due to static leakage); those sections are literally turned off when not needed, which saves a lot of power.  This actually makes it beneficial to have leaky transistors (low ASIC) because when they're needed (high perf), they can hit high clocks at lower voltages (saving power compared to less leaky transistors). When not needed, they're just power gated (turned off), so the part where they leak (high static power = idle) does not increase power, as they're not idling, but are off.  I know that when it comes to phone SoC, chips are binned into several categories (PVS bins), and depending on their bin, voltages for the respective clocks are set. The highest bin (most leaky, but requires low voltages) might hit a particular frequency at 0.875V; but the worst bin might require 1.125V. Since power scales quadratic with voltage, calculate and you'll notice the lower bin (higher voltage) uses 60% more power than the highest (low voltage) bin.  IIRC, some GPU manufacturers were advertising cherry picked dies for overclocking cards, but I don't know on what basis they do so."
hardware,3dpc4q,andromeduck,1 point,Sat Jul 18 09:06:21 2015 UTC,Intel and AMD both sell these chips to enthusiasts at least in the CPU space where it matters more.   It's how AMD especially gets all those frequency records.
hardware,3dpc4q,hans_ober,7,Sat Jul 18 09:28:44 2015 UTC,¯\_(ツ)_/¯  it doesn't make sense to me either
hardware,3dpc4q,Overcloxor,2,Sat Jul 18 07:33:48 2015 UTC,I replied here
hardware,3dpc4q,JaketheAlmighty,1 point,Sat Jul 18 08:36:42 2015 UTC,Hotter running ASICs are less likely to get too cold under LN2 and cease to be a semiconductor. Silicon below a certain temperature becomes just a conductor. That's why hotter chips like AMDs bulldozer chips were able to clock to 8Ghz with LH3.
hardware,3dpc4q,KMKtwo-four,2,Sat Jul 18 16:27:37 2015 UTC,you're correct.  high asic = easier overclocking without raising voltage much. better for average user  low asic = voltage cranked reliant overclocking that shines under water/ln2/helium because it can keep the cards stable
hardware,3dpc4q,JarJarBanksy,2,Sat Jul 18 08:21:46 2015 UTC,Using ASCIC to determine overclockability is like using fuel efficiency to determine how fast your car will go.
hardware,3dpc4q,w00t692,1 point,Sat Jul 18 15:27:31 2015 UTC,No. Lower Asic quality means that you need more voltage to do the same work. This would be because the transistors did not form as well.   The lower quality transistors also will not reach the same clock speeds.
hardware,3dpc4q,CaptSkunk,1 point,Sat Jul 18 15:55:54 2015 UTC,Higher ASIC (only for maxwell) = better overclocking on water and air Lower ASIC (only for maxwell) = better overclocking on LN2 as it can be fed voltage and handle more of it   This is straight from kingpin himself on a facebook post.  Maxwell is different than past chips in that you NEED LN2 to get extreme clocks and you cannot push voltage to get clockspeeds at 40-60c.
hardware,3dpc4q,fuccboi9000,1 point,Mon Jul 20 18:05:10 2015 UTC,"ASIC is supposed to be an indication of how good your card can use the power that it receives. It is also supposed to be an indication of how well your card can overclock, both with and without adding extra voltage.   It's a rating that has not been around for a very long time; probably only 3 years or so. It came from techpowerup gpu-z and can be read by using GPU-Z. Just google ASIC rating and you'll find videos and links that may help you further. You'll also find a lot of arguing about if it is really even relevant.   A higher rating is better but a low one is not bad. For example, I've got a MSI 970 Gaming 4G with a 61.3% ASIC. It runs everything fine and handles my light OC without problems. Every benchmark I've ran on it, shows that it is a normal 970 and performs as I would expect."
hardware,3dpc4q,Exist50,1 point,Sat Jul 18 06:25:19 2015 UTC,tfw my evga 980 is 84% asic. and my two evga 670s were in the 80s asic.
hardware,3dpc4q,imallin,3,Sat Jul 18 06:57:08 2015 UTC,I do think they bin these.
hardware,3dpc4q,TRUE_BIT,1 point,Sat Jul 18 05:17:55 2015 UTC,"The G1 is nuts,  i got one in yesterday,  and it will maintain a 1554 boost OC without even having to touch the voltage."
hardware,3dpc4q,ShinyNewThrowavvay,2,Sat Jul 18 06:29:06 2015 UTC,I was going to say the same.
hardware,3dpc4q,ReallyObvious,2,Sat Jul 18 13:17:52 2015 UTC,It may be an LN2 focused card but the majority of buyers will never have it near a pot.
hardware,3dpc4q,chetoflep,1 point,Sat Jul 18 20:57:12 2015 UTC,Most nvidia cards aren't thermally limited when it comes to overclocking.   Although I wish they sold these with Evga's hybrid cooler. It's a match made in heaven.
hardware,3dpc4q,jus1072,9,Sat Jul 18 06:11:50 2015 UTC,"God damn, that thing is sexy."
hardware,3dpc4q,ShotgunPanda,1 point,Sat Jul 18 05:35:32 2015 UTC,Amen
hardware,3dpc4q,Reporting4Booty,16,Sat Jul 18 09:06:40 2015 UTC,Looks heavy =/
hardware,3dpc4q,benb4ss,10,Sat Jul 18 04:13:10 2015 UTC,"Yeah, where are the diamond heatsinks EVGA?"
hardware,3dpc4q,Techdestro,6,Sat Jul 18 05:33:03 2015 UTC,"Fun fact: Noctua showed a diamond/copper heatsink during computex 2013.   One of the highlights of this year's line-up was a novel heatsink base plate with a diamond-copper composite insert. Made from pure diamond and copper powder by state-of-the-art hot pressing technology, this material offers a thermal conductivity of about 500W/mK, which is 25% higher than pure copper."
hardware,3dpc4q,TheImmortalLS,5,Sun Jul 19 00:25:55 2015 UTC,Maybe you'll get the Zotac Cane thingy with ;)
hardware,3dpc4q,PhilipK_Dick,1 point,Sat Jul 18 14:18:28 2015 UTC,The gigabyte 3 980 watercooled had theirs right :/
hardware,3dpc4q,Exist50,9,Mon Jul 20 04:11:18 2015 UTC,I have the 980 ti 2.0 acx and it is a great card - but I have to say that this is one sexy beast.  Curious to see how much more power that connection can draw...
hardware,3dpc4q,andromeduck,3,Sat Jul 18 05:18:35 2015 UTC,450w if it remains within spec.
hardware,3dpc4q,SeaJayCJ,1 point,Sat Jul 18 06:06:24 2015 UTC,Isn't 375 the PCIe spec?
hardware,3dpc4q,vullnet123,5,Sat Jul 18 09:30:18 2015 UTC,"It has two 8-pins and a 6-pin, so 150 + 150 + 75 + 75 (slot) = 450W.   Of course, it will be able to draw much more in practice."
hardware,3dpc4q,SeaJayCJ,3,Sat Jul 18 11:48:29 2015 UTC,Is it me or is it smaller then the other cards?
hardware,3dpc4q,vullnet123,9,Sat Jul 18 10:06:35 2015 UTC,"Nope. Look at the PCIe bracket here, and then here. The PCB is physically very tall, which is why it ""looks short""."
hardware,3dpc4q,butterymouse,2,Sat Jul 18 11:53:35 2015 UTC,Thank you for clearing that up for me.
hardware,3dpc4q,SeaJayCJ,1 point,Sat Jul 18 12:49:50 2015 UTC,"I've had two classified cards and they are almost impossible to install in every case I've ever owned. The distance between the lip of the side of the case and the top of the gpu is so small that it's impossible to ever screw in the gpu because there's no damn space to even fit your fingers. I've had this problem in a NZXT switch 810, fractal R5, corsair 250D, Antec 900 I think?, and my cooler master HAF XB bench case."
hardware,3dpc4q,butterymouse,1 point,Sat Jul 18 22:08:19 2015 UTC,"I always use a nice big screwdriver anyway, it's just easier."
hardware,3dpc4q,SeaJayCJ,1 point,Sat Jul 18 22:17:30 2015 UTC,I mean it's literally impossible to fit anything in there to even screw it in. Because the card is so tall you can't get an angle to fit any screw driver in and there's not even enough room to use your fingers to screw it in either.
hardware,3dpc4q,Hidden__Troll,1 point,Sat Jul 18 22:19:25 2015 UTC,"I don't know what you're talking about, honestly. I've installed Classy 780s and 780 Tis before in a few different cases using a screwdriver with no problems at all."
hardware,3dpc4q,BrockYXE,2,Sat Jul 18 22:36:35 2015 UTC,"Whats the expected price on this thing? Also, when can I buy it? :)"
hardware,3dpc4q,BalrogTheLunchbox,1 point,Sat Jul 18 15:17:44 2015 UTC,"The 780Ti Kingpin was like $800, so around there probably."
hardware,3dpc4q,Stone_The_Rock,2,Sun Jul 19 00:37:59 2015 UTC,"It's.... it's beautiful. This just spurs me on even more to do a black and copper colored build much like NZXT Noctis build. Biggest thing is I have been looking for at is looking for a nice motherboard that would compliment the colors and maybe one other cooler to kind of help bring it together. Looking at this, I want it, and another one of it."
hardware,3dpc4q,continous,1 point,Sat Jul 18 16:45:54 2015 UTC,"Damn that's fantastic.  But he really should have done the same thing to the motherboard heatsinks, the blue is throwing it off for me completely."
hardware,3dpc4q,SeaJayCJ,9,Mon Jul 20 15:18:41 2015 UTC,But does it beat the G1?
hardware,3dpc4q,continous,7,Sat Jul 18 04:36:11 2015 UTC,"Yes, but this is an overkill and very expensive card. The G1 is much cheaper."
hardware,3dpc4q,SLNC,16,Sat Jul 18 11:47:31 2015 UTC,It almost certainly will.
hardware,3dpc4q,continous,4,Sat Jul 18 06:11:06 2015 UTC,"Wow, people took me a little too seriously. I was poking fun that the G1 has been beating out most the other 980Tis lately, even some of the more expensive ones."
hardware,3dpc4q,blueiron0,13,Sat Jul 18 06:27:43 2015 UTC,Graphics Cards are serious business. Gotta enlarge my e-peen. :^)
hardware,3dpc4q,curiositie,-1,Sat Jul 18 07:24:03 2015 UTC,Apparently.
hardware,3dpc4q,plagues138,4,Sat Jul 18 07:33:59 2015 UTC,god i love evga so much. that thing is fucking SEXY
hardware,3dpc4q,Stone_The_Rock,1 point,Sat Jul 18 11:56:47 2015 UTC,"Dang, this thing has more PCB than my motherboard!"
hardware,3dpc4q,spellstrike,1 point,Sat Jul 18 14:27:06 2015 UTC,not too shabby.
hardware,3dpc4q,Bounty1Berry,1 point,Sat Jul 18 15:54:28 2015 UTC,"IIRC, other K|NGP|N cards offer a single-slot bracket for some single-slot goodness on water.  Does anyone know if this will include said bracket?"
hardware,3dpc4q,SeaJayCJ,1 point,Mon Jul 20 15:21:32 2015 UTC,drool.
hardware,3dpc4q,DiggingNoMore,-1,Mon Jul 20 15:32:18 2015 UTC,"Y'know what I'd like to see-- a heatsink system designed to be easily and thoroughly cleaned with minimum risk of damage to the card.  I see these shrouds and elaborate fans and think ""how the hell do I get the dust out of it"""
hardware,3dpc4q,PhxkinMassacre,5,Sat Jul 18 07:28:03 2015 UTC,"I just take the whole thing apart every so often. EVGA are cool guys, your warranty should be fine. Not so much with other AIBs (eg. Asus, which has been known to put stickers on the screws)  You might like that Phantom cooler Gainward makes/(used to make?) that allows you to swap out the fans."
hardware,3dpc4q,DetectiveSnowglobe,-6,Sat Jul 18 11:51:35 2015 UTC,Requires three power connectors?  Is that the thing now?
hardware,3dpc4q,Lee1138,8,Sat Jul 18 07:50:21 2015 UTC,"It's for extreme overclocks. Does the average person need 3 connectors capable of delivering 450w? Certainly not, but when going for oc records under ln2 etc, they come in handy. If I'm not mistaken the 780 ti kingpin also had 3 connectors"
hardware,3dpc4q,Techdestro,4,Sat Jul 18 09:00:27 2015 UTC,How dare you ask a question? DOWNVOTE
hardware,3dpc4q,Dstanding,3,Sat Jul 18 23:53:32 2015 UTC,"Isn't HBM on top of the chip? Unless nVidia suddenly came out of left field with a new 980ti chip with HBM imtegrated on the gpu, that's pretty much impossible."
hardware,3dnshm,TehRoot,29,Fri Jul 17 19:50:46 2015 UTC,cough Nintendo cough   sorry let me get a lozenge
hardware,3dnshm,troublegoats,-7,Fri Jul 17 22:39:43 2015 UTC,"But Nintendo is rarely on the forefront of tech, and a new APU design would probably be at least GCN 1.0-based, if not later."
hardware,3dnshm,Exist50,8,Fri Jul 17 23:00:29 2015 UTC,"It would probably use whatever amd has latest, just they won't put that much money into the SOC. So sure it will have the best tech but it will draw less power and use less transistors than a xbone and ps4 chip would"
hardware,3dnshm,dylan522p,7,Fri Jul 17 23:28:29 2015 UTC,"I wouldn't be so sure. Remember, the Wii U (came out at the end of 2012) has a 45nm Power based CPU and a Radeon r700 based GPU. The GPU, the better of the two, is based on a design that came out in 2008. If Nintendo uses a CGN-based APU, well, Kabini came out in 2013, and Kaveri in 2014. Either the new system is a while out, or Nintendo's stepped up its hardware game if that's the chip in question."
hardware,3dnshm,Exist50,2,Sat Jul 18 00:15:08 2015 UTC,"With GPU and CPU being integrated these days and the emergence of HBM, there is a simple and massive efficiency gain to be had in combining these technologies in a custom hardware platform.  Money can still be saved by scrimping on the size of the chip."
hardware,3dnshm,waiting4myteeth,1 point,Sat Jul 18 15:56:11 2015 UTC,But AMD has yet to make a mass market hbm product. Plus it's still expensive seemingly
hardware,3dnshm,Exist50,1 point,Sat Jul 18 16:14:44 2015 UTC,That's not a problem: design stage now means release in two to three years most likely.
hardware,3dnshm,waiting4myteeth,1 point,Sat Jul 18 16:30:51 2015 UTC,APU's have been around for quite some time man. They'd also have quite a bit of access to internal roadmaps so they can plan 2-3 years ahead.
hardware,3dnshm,andromeduck,2,Sat Jul 18 03:31:24 2015 UTC,"Still, Nintendo generally goes for the really cheap tech. Look at the Wii U."
hardware,3dnshm,Exist50,5,Sat Jul 18 04:00:59 2015 UTC,"Maybe I'm missing something, but is there a particular reason they assume these are for game consoles?"
hardware,3dnshm,Exist50,4,Fri Jul 17 22:39:37 2015 UTC,"Google translate isn't the best, are is the bit about it being a console speculation or part of the news?  I would guess that not a lot of other industries order custom chips.  Game consoles, cryptocurrency miners, and specialized research stuff is really I can think of that would pay for the custom work."
hardware,3dnshm,Farlo1,3,Sat Jul 18 00:13:03 2015 UTC,I heard rumors about Facebook ordering a custom chip for its data-centers. They at least probably have the money for it.   Edit: http://www.kitguru.net/components/cpu/anton-shilov/amd-develops-custom-server-microprocessor-for-facebook-source/
hardware,3dnshm,Exist50,2,Sat Jul 18 00:21:10 2015 UTC,"There are plenty of companies with the money for it, but most of the time general purpose CPUs and GPUs are cheap enough to counteract any performance increases you'd get from custom chips, although I can definitely see that changing when you're working at the huge scale that those data centers are.  I wish your link had more information on what exactly they're doing, that's really interesting."
hardware,3dnshm,Farlo1,1 point,Sat Jul 18 02:44:49 2015 UTC,"I'm really curious if it is K12, given the delay to 2017. If not, then why would Facebook go to AMD for a chip using none of AMD's cores? That's assuming it's purely CPU, though."
hardware,3dnshm,Exist50,1 point,Sat Jul 18 06:17:36 2015 UTC,ASIC are expensive and they can't be modified but most the big players are going into FPGA with cpu
hardware,3dnshm,dylan522p,2,Sat Jul 18 17:30:46 2015 UTC,They could also use decent APUs in Steam Boxes.
hardware,3dnshm,jman583,2,Sat Jul 18 05:03:39 2015 UTC,I can't imagine anyone paying for a custom chip just to use in a steambox. Way too expensive.
hardware,3dnshm,Exist50,1 point,Sat Jul 18 05:21:58 2015 UTC,It would make sense for Valve to make their own Steam machine. It would make even more sense for AMD to make one themselves.
hardware,3dnshm,MarcusOrlyius,1 point,Sun Jul 19 00:41:43 2015 UTC,"Valve had its Steam machine prototype, while AMD has its Project Quantum. I'm going to address Valve specifically. In order for a custom chip to make sense, it's a) need to be clearly superior to market solutions and b) ship in enough volume to offset the cost. Steam machines (with AMD Linux drivers and general Linux gaming in mind) satisfy neither."
hardware,3dnshm,Exist50,1 point,Sun Jul 19 01:17:19 2015 UTC,Not many people are going to invest in a custom APU for anything else. AMD particularly with HBM and their APU might have a small window of opportunity before other manufacturers start using the technology to make something half decent for a console.  It could also be for laptop/tablet/semi-embedded devices but I doubt it.
hardware,3dnshm,godsayshi,4,Sun Jul 19 13:29:09 2015 UTC,"I don't know if they can afford to not compete on the technology front this time around, plus they probably want to make it easier for third parties to work with."
hardware,3dnshm,troublegoats,7,Fri Jul 17 23:45:39 2015 UTC,A large part of the issues with 3rd parties is Nintendo's fault. Have you read this?
hardware,3dnshm,Exist50,2,Sat Jul 18 00:25:41 2015 UTC,Thanks for the link Exist50. First time hearing about this.
hardware,3dqrx8,Fatigue-Error,37,Sat Jul 18 15:34:14 2015 UTC,"The iPod touch used to make some sense to me when cheap, usable smartphones didn't exist. With the Galaxy Nexus and the foray of Chinese companies like Xiaomi (holy shit I spelled that right on the first try) into the market, who exactly is buying these right now? Even Apple has a decent alternative in the form of the iPhone 5C."
hardware,3dqrx8,Reporting4Booty,21,Sat Jul 18 16:01:10 2015 UTC,"There are a bunch of reasons that you'd want an iTouch over something else:    You are invested in the iOS ecosystem and want a cheap device for your kid (they're still cheaper than a new 5c, more powerful, and more mobile than an iPad mini).  You are an app dev and already own a nice flagship non-iOS device and want a cheap device to test your apps on that has the current hardware.  You want a decent standalone music player (there really aren't any great standalone music players these days, makes even more sense if you're already invested in the iOS ecosystem).  You just want to get into the iOS ecosystem for some reason and want both current hardware and a cheap device.   It really comes down to someone wanting current hardware in the iOS ecosystem on a cheap device.  If you check those three boxes then the iTouch makes sense for you."
hardware,3dqrx8,TimeTravellerSmith,18,Sat Jul 18 18:16:20 2015 UTC,"There are plenty of decent stand alone music players about, look at stuff from FiiO.  Anything that forces you to use itunes is hardly a great music player regardless of the decent DAC on board."
hardware,3dqrx8,SCREAMING_FLESHLIGHT,10,Sat Jul 18 18:32:24 2015 UTC,"look at stuff from FiiO   There are a handful of players out there, and the FiiO is fine and all but it's expensive and limited to music only.  Which is fine, but the added value of being able to use an iTouch as a gaming device, internet/email machine and even an internet phone puts the edge to the iTouch overall.  If you strictly needed a music player then the FiiO would make sense, sure.  EDIT:  And playlists.  I remember looking into these devices at one point in time and lack of playlist support was a dealbreaker.  So unless that's changed that kinda puts the FiiO even farther behind.   Anything that forces you to use itunes is hardly a great music player regardless of the decent DAC on board.   Please, iTunes is not that bad.  For the people who really have a stick up their ass about iTunes there are alternatives."
hardware,3dqrx8,TimeTravellerSmith,6,Sat Jul 18 18:38:17 2015 UTC,I actually like iTunes. I've tried plenty of alternatives on Mac and they are just awful. Only thing that comes close was foobar on PC and even that fell short on what I want out of a music player. Also the podcasts I listen to are on itunes.
hardware,3dqrx8,IsaacJDean,3,Sat Jul 18 23:33:08 2015 UTC,"Yeah, I like iTunes. Clementine is pretty similar in features to iTunes, but it doesn't have that visualizer(seriously, why does no music player other then iTunes have a good visualizer? >.>)"
hardware,3dqrx8,wagon153,0,Sun Jul 19 00:02:01 2015 UTC,"I really hate iTunes, it constantly locks up and crashes more than any other program I use on a low to medium basis (Windows version). That being said, you are far from being restricted to iTunes these days for iOS devices (for music/videos/books at least)."
hardware,3dqrx8,pb7280,1 point,Sun Jul 19 10:34:14 2015 UTC,"Idk, I have a large-ish library (40GB, ~5000 songs) and I rarely have issues with iTunes on Windows, and almost never had issues on OSX and I'm using it pretty much daily.  But yes, you don't have to use iTunes anymore if you don't want to."
hardware,3dqrx8,TimeTravellerSmith,1 point,Sun Jul 19 15:04:48 2015 UTC,I find the OS X version is better than the Windows version. I have around 12000 songs now but I always remember it being this way. Now I just use Apple music and forget about it lol.
hardware,3dqrx8,pb7280,1 point,Sun Jul 19 21:21:28 2015 UTC,"I think it depends on the computer too. I've got 21K songs (about 300GB) and iTunes is quick and responsive on my hackintosh under OS X and Windows. However my computer is an expensive, overpowered editing rig... iTunes on a cheap Windows laptop is not so great."
hardware,3dqrx8,Stingray88,2,Mon Jul 20 17:04:05 2015 UTC,"I think it's good in OS X, my Windows desktop runs circles around my Macbook but is way slower (i5 2500k @4.8GHz quad vs i5 @2.6GHz dual)."
hardware,3dqrx8,pb7280,11,Mon Jul 20 18:20:05 2015 UTC,"1 is a stretch.  People invested in the Apple ecosystem already are not looking for the cheap option.  Buying an iPod touch device for your Son/Daughter that cannot ever make calls means you are purchasing an iPhone for them later anyway.  If you're gonna have a device that can't ever make calls, why not get an iPad or iPad mini that they can actually watch media on?  Or buy an iPhone and don't get it a data plan so you have the option to get one later?  2 -  This is where I sit.  I work for a company that develops software that targets iDevices.  The only reason I had to upgrade to an iPod touch 5 over my gen 3 or 4 was because we needed to support iOS 8.  If iOS 9 is coming to the iPod touch 5 then this new device serves no benefits.  Unless you are making a high-end game (good luck), anything that runs on the iPod touch 6 will run on 5.  3 -  Niche market.  In the world of smart phones, sd cards, pandora, spotify, Google Music, and Apple Music, what is stopping you from listening to your music that you need an extra device?  If I didn't have a 64GB OnePlus One that doesn't have an SD card slot, I would have no issue putting a 32GB - 64GB microSD card in there.  4 -  This is just silly.  iOS leverages their ecosystem in a uniquely approachable way, and for that I commend them.  But please don't think there are people out there who are dieing to enter the iOS ecosystem so bad that they will buy a cellphone sized tablet.  They'll buy an older iPhone."
hardware,3dqrx8,Petrieiticus,2,Sun Jul 19 00:22:56 2015 UTC,"1 is a stretch. People invested in the Apple ecosystem already are not looking for the cheap option. Buying an iPod touch device for your Son/Daughter that cannot ever make calls means you are purchasing an iPhone for them later anyway. If you're gonna have a device that can't ever make calls, why not get an iPad or iPad mini that they can actually watch media on? Or buy an iPhone and don't get it a data plan so you have the option to get one later?   A 10 year old doesn't need a phone.  Hell, you can argue a 14 year old doesn't need a phone.  So by the time they do need a phone the iTouch will have served its job for a long time, and it's safe to say that giving a 10 year old an iTouch to play with and listen to music will be a perfect stopgap between a cheap device and a full fledged phone.  Or your kid's first phone is a cheap Nokia candybar.  Then the iTouch makes even more sense.  You don't get than an iPad because it's not nearly as portable as an iTouch is, and the entry level options for the iTouch are still cheaper than the cheapest iPad.  Plenty of people look at media on their phones, so it's not too much to ask your kid to watch stuff on an iTouch.  And you don't by an iPhone because that's even more expensive.  Like hell I'm spending $500+ on an iPhone to give to a kid.   This is where I sit. I work for a company that develops software that targets iDevices. The only reason I had to upgrade to an iPod touch 5 over my gen 3 or 4 was because we needed to support iOS 8. If iOS 9 is coming to the iPod touch 5 then this new device serves no benefits. Unless you are making a high-end game (good luck), anything that runs on the iPod touch 6 will run on 5.   The iTouch 6 is miles ahead of the 5 and actually runs the upcoming OS.  So as a dev you'd be kinda dumb to get the 5 over the 6 even though power might not factor in for a simple app.   Niche market. In the world of smart phones, sd cards, pandora, spotify, Google Music, and Apple Music, what is stopping you from listening to your music that you need an extra device? If I didn't have a 64GB OnePlus One that doesn't have an SD card slot, I would have no issue putting a 32GB - 64GB microSD card in there.   This is for the people who don't want or don't care to own a full fledged smartphone, believe it or not there are still people like this.  Or, someone with a smartphone doesn't feel like putting all their media on their phone and would rather just have a dedicated music device that they can burn the battery on all day without killing their phone.  And IMO, iOS devices have one of the better music apps going for them.  I have yet to find a device I like as much as my iPhone/iTouch/iPod barring my old Zune Mini which unfortunately isn't made anymore.  My old Lumia 920 had a shit music app, and my experience with the Android player (gf's Note 3) isn't much better.   This is just silly. iOS leverages their ecosystem in a uniquely approachable way, and for that I commend them. But please don't think there are people out there who are dieing to enter the iOS ecosystem so bad that they will buy a cellphone sized tablet. They'll buy an older iPhone.   Why the hell would you bother to buy an old iPhone when you can spend nearly the same amount on a completely up to date piece of hardware?  That's what's silly, not the idea that someone wants to get into the ecosystem on a cheap device.  iOS is constantly rated as one of, if not the best mobile ecosystem out there.  iOS devices are constantly receiving high marks in usability, build quality and feature set.  It completely makes sense that someone would want to get in on that, and the iTouch is a great device to enter that ecosystem.  I can understand why a lot of people wouldn't want an iTouch, I really do.  Of course it's not for everyone.  However, you need to read that last statement in my comment as to why it probably doesn't make sense to you.  If you can't see the allure of a cheap iOS device with current hardware then of course this product isn't for you, but that doesn't mean that no one else would want something like this.  Hell, for less than a brand new entry level iPhone you can get a full 128GB iTouch.  There is most certainly a market segment for that."
hardware,3dqrx8,TimeTravellerSmith,5,Sun Jul 19 01:04:28 2015 UTC,"10 years old and 14 years old in the same sense don't need ipod touch. Why would they need mobility? Where do they watch video, web outside of their home? in  school or playground?     Why the hell would you bother to buy an old iPhone when you can spend nearly the same amount on a completely up to date piece of hardware?   because people who are desperately want to enter iOS ecosystem yet don't have enough money are those want it for the look. And you can't show off something that can't make a call."
hardware,3dqrx8,sterob,1 point,Sun Jul 19 09:53:13 2015 UTC,"Why would they need mobility? Where do they watch video, web outside of their home? in school or playground?    Your kid never leaves the house?  Never takes the bus to school?  Never goes on a vacation?  Never has after school activities where they take the team bus on a road trip?  Shit, when I was a kid I was in the car constantly.  My old classic iPod and my Gameboy were always on me.   because people who are desperately want to enter iOS ecosystem yet don't have enough money are those want it for the look. And you can't show off something that can't make a call.    This is stupid.  There are a lot of reasons people would want to get into iOS but not want to spend a ton of money on an iPhone.  You're pretty ignorant to think that only rich people who can afford $500+ devices would want an iDevice for reasons other than aesthetics."
hardware,3dqrx8,TimeTravellerSmith,1 point,Sun Jul 19 15:10:48 2015 UTC,"Kids leave the house to play at their playground, reading/using book/phone in a moving car is simply bad, and ipad will be inside your luggage during vacation.  Ask yourself, were you spending time with your gameboy because you have nothing else to do or  you simply was addicted to playing game and refuse to do anything else?"
hardware,3dqrx8,sterob,1 point,Sun Jul 19 17:15:19 2015 UTC,you sound quite out of touch with current kids. Playgrounds arent remotely as popular as you think.
hardware,3dqrx8,HavocInferno,1 point,Mon Jul 20 05:21:33 2015 UTC,"Kids leave the house to play at their playground   So because kids play at the playground automatically means they never want to be entertained by anything else, ever?   reading/using book/phone in a moving car is simply bad   Old wives' tale.  At worst you're the kind of person that gets carsick from staring at text.   ipad will be inside your luggage during vacation   So?  And an iTouch can stay in your pocket.   Ask yourself, were you spending time with your gameboy because you have nothing else to do or you simply was addicted to playing game and refuse to do anything else?    Ugh, seriously?  Now you're trolling.  You were never stuck in a car for long trips as a kid?  You never sat on an airplane for hours?  I did plenty of reading (shit, probably read all of Michael Crichton's and Tolkien's works ten times over through elementary and high school), but that doesn't mean that's the only thing I ever wanted to do while traveling or riding the bus to school.  A lot of times it's nice to just sit and listen to music or play a game rather than read a book."
hardware,3dqrx8,TimeTravellerSmith,5,Sun Jul 19 17:26:33 2015 UTC,iTouch    ಠ_ಠ
hardware,3dqrx8,missch4nandlerbong,0,Mon Jul 20 00:12:26 2015 UTC,They're good for texting and calling between iPhones. my mother has one and since everyone else has iphones she uses it to communicate.
hardware,3dqrx8,Theodoros9,0,Mon Jul 20 00:31:22 2015 UTC,You want a decent standalone music player    itune already invalided any point related to music player.
hardware,3dqrx8,sterob,3,Sun Jul 19 09:41:27 2015 UTC,If you don't like iTunes you don't have to use it.
hardware,3dqrx8,TimeTravellerSmith,2,Sun Jul 19 15:01:58 2015 UTC,"iPod Touch was originally introduced for the sole purpose of getting people who werent on ATT (back then the iPhone was locked to them) the same iPhone-level experience and features minus the cell radio.   So back then many people had an android/winmo/compromised shitphone for their carrier, and then an iPod Touch for their actual enjoyable media use.   It was also a great device for kids or as a competitor to handheld consoles."
hardware,3dqrx8,poematik,1 point,Mon Jul 20 19:49:18 2015 UTC,Now try pronouncing Xiaomi right the first time.
hardware,3dqrx8,Jauris,4,Sat Jul 18 17:06:18 2015 UTC,Zie-oh-me?
hardware,3dqrx8,spamyak,8,Sat Jul 18 17:49:25 2015 UTC,"See-ow-me.  But minimize the gap between see and ow. It's one word.  There is also no ""h""."
hardware,3dqrx8,thaw12,6,Sat Jul 18 20:24:59 2015 UTC,"Hey look, someone that actually knows Chinese."
hardware,3dqrx8,Aedesius,4,Sun Jul 19 06:03:49 2015 UTC,"show-me, with the first half pronounced like shower."
hardware,3dqrx8,Jauris,0,Sat Jul 18 18:01:34 2015 UTC,it's $200. The cheapest iPhone is about $800.  People buying it will be people who want to use iOS but don't want to pay lots of money for it.
hardware,3dqrx8,feelix,2,Sun Jul 19 12:22:54 2015 UTC,Lol you can get a 5c for like $400 new (maybe even less?)
hardware,3dqrx8,tru3gam3r,2,Sun Jul 19 15:37:10 2015 UTC,"well fair enough, but the ipod is still by far the cheapest iOS device, which is why people would buy it"
hardware,3dqrx8,feelix,2,Sun Jul 19 23:20:48 2015 UTC,5c is $450.
hardware,3dqrx8,Stingray88,0,Mon Jul 20 17:09:37 2015 UTC,iphones are free on contract. who the hell pays 800 dollars for a phone?
hardware,3dqrx8,poematik,-2,Mon Jul 20 19:50:47 2015 UTC,Do any of those Samsung/Xiaomi phones run iOS?
hardware,3dqrx8,58592825866,0,Sat Jul 18 21:25:46 2015 UTC,"Have you felt or used the 5c? It's honestly the 2nd worst Apple product that's still around. The 5c uses terribly cheap plastic, is heavier than it looks and feels awful in the hand. At least the iPod is impossibly light, has a metal body and is mind blowingly thin.  I'm not saying the iPod is good value but the 5c shouldn't be considered an alternative to anything, even within the Apple ecosystem.   The 5c uses the same internals as the 5 with a worse body. Even the sim tray is a paper thin piece of crap plastic piece. The worse part is the 5 is cheaper than the 5c right now but somehow people were convinced the 5c is cheaper than the same phone made out of metal."
hardware,3dqrx8,Pinecone,1 point,Mon Jul 20 13:38:42 2015 UTC,"Have to disagree on this, I've set up numerous 5c's and the body is fairly solid and durable, so much that you don't need a case for it. It's one of my favorite plastic devices aside from the late 2009 white macbooks."
hardware,3dqrx8,poematik,1 point,Mon Jul 20 19:51:47 2015 UTC,"I use one every day, and look forward to upgrading if I can sell my 5th gen. Not everyone has an iPhone."
hardware,3dqrx8,NakedSnakeEyes,-2,Tue Jul 21 06:16:09 2015 UTC,Faster when you inbox it. Just wait until an iOS update and they'll slow it right back down for ya.
hardware,3dqrx8,gluckh,0,Sat Jul 18 17:47:52 2015 UTC,Which is pretty much pointless if you're not gaming on it.
hardware,3do5y7,conradsymes,23,Fri Jul 17 21:34:42 2015 UTC,"Unfortunately it's all we really have to look at until the product is released.  Sure, it's not a solid indicator of performance, but it can provide some indication of performance relative to the previous version of the same product line in most cases where the designs are very iterative."
hardware,3do5y7,Farlo1,3,Sat Jul 18 00:06:45 2015 UTC,I wouldn't know about that either. Transistor counts have been staying near Moore's law and the performance increases remain around 10% per generation. So a 70% increase in transistors doesn't translate anywhere near 70% increase in performance.
hardware,3do5y7,Tzahi12345,5,Sat Jul 18 03:28:15 2015 UTC,"If the transistors were purely for the CPU - then we would see a bigger increase. But intel has been just adding a bigger iGPU instead, that's why the gains are small in performance of the cpu"
hardware,3do5y7,Anterai,-1,Sat Jul 18 21:49:07 2015 UTC,But they keep adding cores too. Does that mean that the transistors per core go down?
hardware,3do5y7,Tzahi12345,3,Sun Jul 19 00:22:29 2015 UTC,What Cores? Intel hasn't changed the amount of cores on its CPU's since 2010.
hardware,3do5y7,Anterai,1 point,Sun Jul 19 09:12:44 2015 UTC,I mean for example the 4960X vs 5960X (6 vs. 8) or 4820K vs 5820K (4 vs. 6).
hardware,3do5y7,Tzahi12345,4,Sun Jul 19 14:50:14 2015 UTC,"Yup! I don't think the phrase keep applies, as it only happened twice in the last 6 years.  But you are right, they are indeed adding cores to the top-enthusiast segment of CPU's.    The thing is tho, tat for non-enthusiast models, the 4 cores per CPU is still going strong. Back from Bloomfield."
hardware,3do5y7,Anterai,-1,Sun Jul 19 15:19:05 2015 UTC,Yup. Most programs aren't able to utilize more than four cores so the real performance improvement of adding cores is minimal. Those who use programs that can buy Xeons anyways.
hardware,3do5y7,Tzahi12345,4,Sun Jul 19 15:28:21 2015 UTC,"It's a chicken vs egg thing though.  If more people had 6 cores, then more software would be built with 6 cores in mind. If more people had 8 cores? same."
hardware,3do5y7,Anterai,2,Sun Jul 19 15:31:57 2015 UTC,"Certain types of calculations can only run on one core. There's no way to thread sequential calculations.  Things that can be multithreaded are pretty much already multithreaded. Video editing, transcoding, particle simulation, batch submission, etc. are already threaded for the most part."
hardware,3do5y7,CykaLogic,0,Sun Jul 19 23:56:02 2015 UTC,At least Intel didn't seem to keep up with Moore's Law (some numbers) as far as I can see. Please correct me if I'm wrong.
hardware,3do5y7,Mr_s3rius,1 point,Sat Jul 18 10:26:20 2015 UTC,"They'd just stopped, but they were migrating their consumer cpus to be lower amd lower end in their relative product stack.their consumer cpus have incredible margins."
hardware,3do5y7,dylan522p,13,Sat Jul 18 17:32:16 2015 UTC,"Also, teraflops."
hardware,3do5y7,TaintedSquirrel,8,Fri Jul 17 23:07:14 2015 UTC,"Also, gigahertz."
hardware,3do5y7,ascii,1 point,Sat Jul 18 01:19:07 2015 UTC,*jigahertz
hardware,3do5y7,JaketheAlmighty,7,Sat Jul 18 02:53:12 2015 UTC,"Well, it's useful for compute..."
hardware,3do5y7,Exist50,4,Fri Jul 17 23:44:38 2015 UTC,not really with raw values  I think it's better now but I remember when people were comparing Fermi's 500 something cuda cores to AMD's 3k stream processors and raw FLOPS way back when and we all know how that ended.
hardware,3do5y7,andromeduck,2,Sat Jul 18 03:28:28 2015 UTC,970 gtx < 290X in FLOPS but we know that it doesn't directly translate into gaming performance.
hardware,3do5y7,TheImmortalLS,10,Sat Jul 18 03:41:56 2015 UTC,"Transistor count roughly aligns to cubic inches displacement for car enthusiasts.  Sure, it's not the whole story and engine management can make a huge difference but it's a pretty good baseline when everything else is unknown."
hardware,3do5y7,myztry,1 point,Sat Jul 18 03:51:41 2015 UTC,"Right. More transistors in an area of the same size means it has the potential to do more, but depending upon design changes it might do some things better, but other things worse. I think if you look at one line of processors that has a more specific usage than ""general desktop computing"" then transistor count can have much more meaning. For example, when we made the change from a single 64 bit ALU to multiply 32 bit numbers into chaining 33 32 bit ALUs in a parallel tree to multiply 32 bit numbers."
hardware,3do5y7,homelesspieceofshit,2,Sat Jul 18 05:11:39 2015 UTC,So what is a good way to compare chips?
hardware,3do5y7,mastermikeee,9,Sat Jul 18 02:18:53 2015 UTC,Benchmarks.
hardware,3do5y7,hurleyef,4,Sat Jul 18 02:25:01 2015 UTC,"Depends on the benchmark though. Cinebench is the most commonly used, yet it is biased toward Intel.  GPUs is a bit different, some games runs better on AMD or Nvidia and will clearly show this during a benchmark.  Still, it means you should pick the GPU that works best for the games or game engines you actually play on."
hardware,3do5y7,Bloodcore911,3,Sat Jul 18 16:16:34 2015 UTC,"The benchmarks should be representative of how it plans to be used, ideally using the very programs that one intends to use.  I certainly wouldn't compare rendering times to guage gaming performance, nor would I subject a cpu to a battery of gaming benchmarks if I'm not going to be using it for gaming."
hardware,3do5y7,hurleyef,1 point,Sun Jul 19 00:05:21 2015 UTC,"True. But saying Cinebench is a good way to compare chips from seperate brands is not true.  Gaming benchmarks with the same GPU and different CPU models are fine in most cases.  The best way is to find benchmarks in the games/game engine you love and then take it from there.  Still, comparing Intel vs Intel or AMD vs AMD in Cinebench shouldn't be an issue. But, it won't tell you how it performs in games."
hardware,3do5y7,Bloodcore911,1 point,Sun Jul 19 15:54:26 2015 UTC,Cinebench isn't biased to Intel it's biased to good cpus.
hardware,3do5y7,dylan522p,4,Sat Jul 18 17:34:46 2015 UTC,"It was biased in CB11.5 and might still be.  They were using Intel libraries instead of the open standard that AMD can support.  Still, I prefer Intel CPUs. Though Intel has never been ""clean"" in its way to conduct business.  There is a reason why they've been fined multiple times in the past."
hardware,3do5y7,Bloodcore911,2,Sat Jul 18 18:23:39 2015 UTC,Isn't there a relationship between transistor count and manufacturing cost?
hardware,3do5y7,cp5184,1 point,Sat Jul 18 17:57:26 2015 UTC,"I'm not the person to ask about this, but I'm pretty sure the answer is yes and no.  Transistor count has grown almost every generation of chips, yet the price remains mostly the same.  But, if you go for the enthusiast-line (Haswell-E, etc) then you'll be getting a higher transistor count/core count, additional instruction-sets and it'll require a more expensive motherboard.. and the price for the enthusiast CPU will be much higher.  It is all about how many chips they get out of a single wafer. Bigger chips costs more.  Smaller production node (32nm->22nm) means more chips on the same wafer OR same sized chip with more ""stuff"" inside it."
hardware,3do5y7,Bloodcore911,7,Sun Jul 19 16:00:54 2015 UTC,"It might be a bad way to compare between brands such as AMD/Nvidia or AMD/Intel.  But, if you compare a previous GPU, current and upcoming GPUs from the same brand.. It might give you a hint on what to expect.   GTX 580 - 3000 Million (40nm) GTX 680 - 3500 Million (28nm Node shrink, New Arch, TextureMapping Units doubled, ShaderUnits Tripled.) GTX 780 -  7000 Million (28nm, Massive die size, Increases all around, New arch for some chips) GTX 980 - 5200-8000 Million (28nm, efficiency, 35% lower TDP and higher performance. ) GTX 1080 - 7000-9500+ Million? (14/16nm FinFet, New arch, HBM2 Memory, DX12, Lower TDP and higher performance due to all points.)   No, I am not a fanboy. GTX 1080 will be my first Nvidia card since the 8800 GT.  2016 will be the year for GPU upgrades and complete platform upgrades if you've got something older than IvyBridge and not overclocking."
hardware,3do5y7,Bloodcore911,16,Sat Jul 18 01:32:39 2015 UTC,Why didn't you just say billion and not have useless zeros
hardware,3do5y7,dylan522p,4,Sat Jul 18 02:41:59 2015 UTC,"Maybe million is the ""historic"" way to talk about transistor count, wasn't that long ago when you had 100,800 million transistors and he could have tried to avoid ""floating point' numbers like 3.5 billion."
hardware,3do5y7,rndnum123,3,Sat Jul 18 07:00:32 2015 UTC,Yeah millions was what it was for a long time. But as soon as people crossed into billion they advertised it
hardware,3do5y7,dylan522p,1 point,Sat Jul 18 08:22:51 2015 UTC,"Well, potentially this has to do with the definition of ""billion"" varying from place to place.  For example, in many EU countries (and according to the SI), a billion is a million millions, so 1 000 000 000 000. In the US, a billion is a thousand millions, so 1 000 000 000.   Had the poster said 7 billion instead of 7000 million, a lot of people would run the risk of misunderstanding him or her."
hardware,3do5y7,Timiniel,1 point,Mon Jul 20 11:45:32 2015 UTC,I've literally never seen billion mean anything other than a thousand milions
hardware,3do5y7,dylan522p,2,Mon Jul 20 12:19:31 2015 UTC,"https://en.wikipedia.org/wiki/Billion  Where are you from, exactly, if I might know?"
hardware,3do5y7,Timiniel,1 point,Mon Jul 20 12:35:57 2015 UTC,Us
hardware,3do5y7,dylan522p,1 point,Mon Jul 20 13:38:15 2015 UTC,"There is the reason :)  I literally had never seen anyone call 1 000 000 000 a ""billion"" until I first interacted with someone from the US."
hardware,3do5y7,Timiniel,1 point,Mon Jul 20 13:44:13 2015 UTC,I have extra zeroes in stock.  Need to put em to use.
hardware,3do5y7,Bloodcore911,0,Sat Jul 18 16:04:18 2015 UTC,"Eh, works as well. He has to type the zeros, we just have to read."
hardware,3do5y7,TheImmortalLS,7,Sat Jul 18 03:42:50 2015 UTC,"How is a 780 new arch, wasn't the 680 the introduction of Kepler?"
hardware,3do5y7,wowseriffic,0,Sat Jul 18 04:25:29 2015 UTC,"Corrected.   Geforce 700 series. ""The GM107-chips are Maxwell-based, the GKxxx-chips Kepler."""
hardware,3do5y7,Bloodcore911,3,Sat Jul 18 16:07:42 2015 UTC,"The GTX 980 is actually 'Little Maxwell' (GM204), which has 5.2 billion transistors. The GM200 chip ('Big Maxwell', 980 Ti/TITAN X) has 8 billion.  NVIDIA tends to do a tick-tock progression with a new architecture every couple of years, the little chip first, followed by the big chip."
hardware,3do5y7,Aldarro,1 point,Sat Jul 18 09:16:58 2015 UTC,"Don't think it'll be that high, if you compare GK104(680) to GF110(580), the former had a full node jump and is still only slightly larger than the latter. Perhaps 8.5B transistors, and architectural increases on top of that.  Maybe even less, if AMD fails to deliver and NVIDIA somehow can use GP106 or cut down GP104 as the 1080."
hardware,3do5y7,CykaLogic,1 point,Sun Jul 19 23:58:55 2015 UTC,"True. Still, this is a 1.5-2x node shrink. We skipped 20nm. (You can theoretically quadruple everything in the GPU.. not that it would happen though.)  We're also getting FinFet, which will help with efficiency and heat. And then there is HBM2 which will increase memory bandwidth to silly amounts, lower power/heat and save alot of space.  The new Pascal architecture will have some nice increases if the Nvidia conference was atleast half-true.  And DX12+Windows 10 is an indirect performance increase with better multithreading and memory control.    Still, all these will add SOMETHING. Which should add up to something quite noticable.  Nvidia will most likely hold back some of their tricks so that the ""Pascal Refresh"" will seem like a nice upgrade.  We can atleast agree upon that 2016 will either be a fantastic year for gamers, or a failure of expectations.  I'll be crossing my fingers that I'll be able to run 1440p at 120fps without too many issues. (considering they are aiming for 4K 60fps for single-gpus.)"
hardware,3do5y7,Bloodcore911,1 point,Mon Jul 20 00:53:34 2015 UTC,"I dunno, I don't see any compelling reason to upgrade from Sandy-E any time soon.  I guess I could pop in a 4930K, but that's a lot of money for a chip that likely as not will overclock worse and be slower."
hardware,3do5y7,BloodyLlama,1 point,Sat Jul 18 04:02:45 2015 UTC,"You don't need to buy a new system unless you actually want to.  Skylake-E will be a noticable boost most likely.  Also, Windows 10 combined with DirectX 12 will start using multithreading more efficiently and the new recommended amount of cores for best performance in DX12, is between 6-8 CPU cores."
hardware,3do5y7,Bloodcore911,3,Sat Jul 18 16:09:05 2015 UTC,There has been so little difference in the last few processors I have had that I don't really even notice it.  Bummer.  If transistors don't count what exactly can we look forward to?  I don't want a built in GPU and I don't want any more cores.  Maybe they can paint it a different color or something =\
hardware,3do5y7,jdw101,1 point,Sat Jul 18 03:28:18 2015 UTC,"Yeah, I don't like the GPU either.  It makes no sense. Integrated graphics are decent enough for old games."
hardware,3do5y7,Bloodcore911,1 point,Sat Jul 18 08:03:31 2015 UTC,"The biggest difference in the last couple of years have been the platforms/chipsets themselves.  We've received DDR4, M.2, SATA Express, NVMe, Upgraded UEFI, etc.  Also, if I remember correctly.. Broadwell has built in audio in either the CPU or chipset.  Yet another step toward a complete SoC."
hardware,3do5y7,alainmagnan,4,Sat Jul 18 16:14:48 2015 UTC,I thought we stopped caring years ago.  Didn't it die with the GHz race? specifically Itanium?
hardware,3do5y7,Nixflyn,0,Sat Jul 18 01:52:50 2015 UTC,"No, just look at all the ""leaks"" leading up to the Fury (x) launch. Transistor count and GHz were main topics."
hardware,3do5y7,ModernRonin,0,Sun Jul 19 06:12:22 2015 UTC,"Yeah, look at actual benchmark tests.  Then you'll see the slowdown in computing power even more clearly."
hardware,3dmweq,ryerocco,19,Fri Jul 17 15:51:03 2015 UTC,"It's not a GPU that the company intends to sell to consumers, but rather as a proof of concept   and I was like ... oh, fck you!"
hardware,3dmweq,zmeul,18,Fri Jul 17 18:06:05 2015 UTC,"If you read on though, they plan to license it to other manufacturers of graphics processors. Including this module in the architecture of a desktop gpu would massively increase shadow fidelity, without pulling more than 5-10 watts away from the GPU. It's an awesome step forward, and I hope both Nvidia and AMD use it."
hardware,3dmweq,RaulNorry,-3,Fri Jul 17 18:22:57 2015 UTC,I'd rather have them invest into a new lineup of desktop GPUs - we need a 4 3rd competitor
hardware,3dmweq,zmeul,6,Fri Jul 17 18:24:18 2015 UTC,Rasterized graphics don't seem to suited as well to scaling up so large. It's mostly the software side they lack though.
hardware,3dmweq,dylan522p,1 point,Fri Jul 17 19:47:10 2015 UTC,Dafaq I read?
hardware,3dmweq,andromeduck,1 point,Sun Jul 19 04:40:57 2015 UTC,Why try to compete with your competitors when you can sell directly to them?
hardware,3dmweq,animeman59,3,Sun Jul 19 02:48:40 2015 UTC,"I translate ""Ray tracing"" as CG motion picture, so this card can become a competitor to strictly Quadro or Firepro cards. Or driver support permitting, Xeon Phi parallel processor card."
hardware,3dmweq,Xskills,7,Fri Jul 17 23:12:40 2015 UTC,Depends on how you see the future. Real-time raytracing with small samples sizes is already a thing.
hardware,3dmweq,continous,3,Sat Jul 18 04:02:36 2015 UTC,Not really. Current GPUs are still very good at vector-based graphics and point-based graphics. They just aren't specialized for it as much as they are for rasterization.
hardware,3dmweq,continous,1 point,Sat Jul 18 05:05:29 2015 UTC,In what terms have we hit the wall?!
hardware,3dmweq,andromeduck,1 point,Sat Jul 18 06:02:31 2015 UTC,"Not yet, but sometime when we're all climbing out the other side of the uncanny valley. It's only a matter of time until the GPU fabricators can't get the finFET process any smaller and we have to go in another direction."
hardware,3dmweq,Xskills,2,Sun Jul 19 04:42:23 2015 UTC,That doesn't even make sense.
hardware,3dmweq,andromeduck,3,Sun Jul 19 04:52:58 2015 UTC,"O.K. I concede, I'm failing to state things the way I mean them and I'm ~60% pulling it from my a**. My comments were made out of hubris and ignorance. I did not in anyway mean to even make any claims out of trolling. I am sorry."
hardware,3dmweq,Xskills,2,Sun Jul 19 04:56:50 2015 UTC,How do you come to that conclusion?
hardware,3dmweq,__Cyber_Dildonics__,0,Sun Jul 19 05:13:00 2015 UTC,"From what I've heard, ray tracing is not done on any game engines not even the intense ones (UE4, Cry Engine 3, Fox Engine, etc..)."
hardware,3dmweq,Xskills,3,Sat Jul 18 04:44:10 2015 UTC,That is true.  Where does the connection to being a competitor to workstation cards come in?
hardware,3dmweq,__Cyber_Dildonics__,1 point,Sat Jul 18 05:07:18 2015 UTC,Quadro and Firepro are used for CG animated films. I believe there's a demonstration by Pixar on Nvidia's YouTube channel.
hardware,3dmweq,Xskills,4,Sat Jul 18 05:09:57 2015 UTC,"They are used for opengl and some interactivity but not final rendering.  This card is not on the same scale in transistors, power, or software."
hardware,3dmweq,__Cyber_Dildonics__,1 point,Sat Jul 18 05:13:13 2015 UTC,The point of the article I think is just that they have architecture functioning which is a potential stepping stone to a (possibly) game-changing product.
hardware,3dmweq,Xskills,1 point,Sat Jul 18 05:17:04 2015 UTC,You should see iray and associated software.
hardware,3dmweq,andromeduck,1 point,Sat Jul 18 06:19:04 2015 UTC,You should try to use it for an animated film.
hardware,3dmweq,__Cyber_Dildonics__,1 point,Sun Jul 19 04:44:55 2015 UTC,Oh whoops didn't see it specified animation. I know iray is used in a lot of static stuff bit most of he big houses have heir own tools.  Pixar has yet to GPU accelerate their final pipeline and I don't think the other's have either.
hardware,3dmweq,andromeduck,1 point,Sun Jul 19 04:50:39 2015 UTC,Ray tracing is a method that is used in many games but in a very limited fashion.
hardware,3dmweq,andromeduck,-2,Sun Jul 19 04:56:15 2015 UTC,"It is the most realistic of image generating techniques, but it comes at the cost of needing a huge amount of processing power. Therefore, video games hardly use it because it is almost impossible to achieve it in real-time on today's hardware while keeping up a decent framerate, without going overboard on hardware.   Wat"
hardware,3dmweq,jorgp2,17,Sun Jul 19 04:44:13 2015 UTC,What it is exactly as they say. GPUs simply don't have the performance to pull off high framerate ray tracing there are only a few tech demos on the highest end hardware. Just like they wrote.
hardware,3dmweq,MINIMAN10000,0,Fri Jul 17 21:18:34 2015 UTC,"What they wrote was Screen space Ray Tracing, which is already used heavily by games to calculate shadows   You're thinking about the same kind of Ray Tracing that is used in professional rendering which is done in world space with each Ray having multiple bounces."
hardware,3dmweq,jorgp2,5,Fri Jul 17 21:39:10 2015 UTC,The article is not talking about screen space ray tracing. All it's talking about is standard ray tracing.
hardware,3dmweq,fastcar25,1 point,Fri Jul 17 21:45:03 2015 UTC,Well that's what I get for not actually reading the article.  I was thinking 1 2 3  Whereas they were discussing shadows. Which I can find nothing on raytracing for shadows.
hardware,3dmweq,MINIMAN10000,1 point,Sat Jul 18 04:41:52 2015 UTC,Check this out: https://docs.unrealengine.com/latest/INT/Engine/Rendering/LightingAndShadows/RayTracedDistanceFieldShadowing/index.html
hardware,3dmweq,altgr_01,1 point,Fri Jul 17 22:00:31 2015 UTC,"Ray marching through an SDF is a valuable technique, but I don't think that is what this is about.  It is different from Ray tracing geometry directly."
hardware,3dmweq,__Cyber_Dildonics__,0,Fri Jul 17 23:04:18 2015 UTC,"It's all true. Something that's been talked for ages, but hasn't been viable at all until now, apparently. Amazing if they've really cracked it, I thought it was still years away."
hardware,3dmweq,bphase,-1,Sat Jul 18 04:51:52 2015 UTC,What kind of games have you been playing?
hardware,3dmweq,jorgp2,1 point,Fri Jul 17 21:42:00 2015 UTC,Any. Or can you give examples where ray-tracing is used nowadays?
hardware,3dmweq,bphase,-1,Fri Jul 17 21:46:47 2015 UTC,"Most games use Screen Space Ray Tracing to calculate accurate shadows.  Shadow maps were used with forward rendering, which was not friendly with multiple light sources.  What games don't use are World Space Rays with multiple bounces, those would make GPUs crumble to their PCI slots."
hardware,3dmweq,jorgp2,2,Fri Jul 17 21:49:27 2015 UTC,Almost nothing in your post comes together as a coherent truth.  Why go to the trouble of posting when you are playing a game of telephone between Reddit and a mismash of articles you've barely understood?
hardware,3dmweq,__Cyber_Dildonics__,2,Fri Jul 17 22:20:22 2015 UTC,"Most games use Screen Space Ray Tracing to calculate accurate shadows.   No. Shadow maps are used almost exclusively. The only instances of screen space raytracing I've seen for shadows is a few instances of local character shadows, or for occluding reflections.   Shadow maps were used with forward rendering, which was not friendly with multiple light sources.   True, forward lighting has problems with many light sources, but shadow maps are still the dominant method of casting dynamic shadows."
hardware,3dmweq,fastcar25,0,Sat Jul 18 04:48:33 2015 UTC,"Yeah, it reminds me of https://www.youtube.com/watch?v=00gAbgBu8R4"
hardware,3dmweq,retolx,4,Fri Jul 17 23:13:49 2015 UTC,"Fucking shit man, how many times is that snake oil horrible looking video going to keep coming back?"
hardware,3dmweq,__Cyber_Dildonics__,3,Fri Jul 17 22:58:44 2015 UTC,Not again...
hardware,3dmweq,coffee_can,0,Sat Jul 18 04:50:08 2015 UTC,"interesting tech, kinda hope its real."
hardware,3dmweq,CommanderArcher,1 point,Sat Jul 18 20:22:12 2015 UTC,"Probably not. It's hard to tell, but it seems unlikely that shifting from polygons to vertices made that huge a difference."
hardware,3dmweq,continous,1 point,Sat Jul 18 00:14:39 2015 UTC,It's not :)
hardware,3dn5i7,LulusPanties,19,Fri Jul 17 17:00:50 2015 UTC,IPC is 5% but lower power with same tdp mean it can clock higher when it needs to. It also sustains boosts better. The main thing is less power and the iGPU is really good
hardware,3dn5i7,dylan522p,5,Fri Jul 17 19:18:31 2015 UTC,How is the 6200 vs the 5200?  I heard not much difference
hardware,3dn5i7,dylan522p,11,Fri Jul 17 19:27:48 2015 UTC,"Quite the opposite, a large difference. There's benchmarks out that compare the two  http://www.anandtech.com/show/9320/intel-broadwell-review-i7-5775c-i5-5675c  Most comprehensive review I've read, it's for the desktop version but it's the exact same chip just higher clocks."
hardware,3dn5i7,dylan522p,3,Fri Jul 17 19:45:37 2015 UTC,I don't see a comparison here with the Iris 5200.    The Iris 5200 is already on the same level as a GT 640
hardware,3dn5i7,dylan522p,7,Fri Jul 17 20:04:42 2015 UTC,Not really. Actual performance in games is much less because of the worse software. Really shitty website it was just the first thing that showed up on google. 5200 vs 6200 is 40eus vs 48eus but the EUs in 6200 and better aswell as having an improved front end and better process allowing higher clocks.   http://www.game-debate.com/gpu/index.php?gid=2978&gid2=1783&compare=iris-pro-graphics-6200-mobile-vs-iris-pro-graphics-5200-mobile
hardware,3dn5i7,everyZig,1 point,Fri Jul 17 20:23:08 2015 UTC,"The 6200 also has a lower clock speed despite having 8 more eus right?  In terms of actual fps during gaming, what is the difference in performance between the 6200 and 5200"
hardware,3dn5i7,dreiter,4,Fri Jul 17 20:44:11 2015 UTC,Higher clocks not lower. It will sustain turbo much better. 20-60% difference depending on the game. In terms of compute it's 20-120% depending on what it is. There's quite a huge range of how much it has improved.
hardware,3dn5i7,KMKtwo-four,2,Fri Jul 17 20:46:20 2015 UTC,"Not sure about the 5200, but the 6200 is the current king of the hill in APU land, it absolutely blows by anything from AMD, and gets frighteningly close to discrete cards like the GTX750"
hardware,3dn5i7,barthw,0,Sat Jul 18 08:22:59 2015 UTC,If it's anything like the IGP improvements on the desktop then it will be a big upgrade.
hardware,3dn5i7,LiberDeOpp,3,Fri Jul 17 22:12:39 2015 UTC,That's not a fair comparison because the 5775c uses iris pro graphics compared to the 4790k's regular integrated graphics.
hardware,3dn5i7,barthw,11,Sat Jul 18 02:12:43 2015 UTC,"Mobile CPU performance has not improved really, but power consumption is a marginally better. Iris 6000 Series is quite a step up from the 5000 series though."
hardware,3dn5i7,tru3gam3r,7,Fri Jul 17 18:49:45 2015 UTC,"In mobile power consumption is everything. You get better boosts, longer boosts, and more battery time. I think laptop and smaller has been the real winner of cpu tech in the last 5 years."
hardware,3dn5i7,Exist50,1 point,Fri Jul 17 22:36:58 2015 UTC,Yeah i think its amazing that the 15Watts tdp CPU in my broadwell macbook air is faster than my old Desktop core 2 quad Q9550 with 95 Watts tdp.
hardware,3dn5i7,ZeroCool2u,11,Sat Jul 18 05:56:48 2015 UTC,"The difference? Broad well has a seriously shrunk CPU die. What do they do with the extra space? Fit a big ass GPU in it, and have it compete with a 750ti."
hardware,3dn5i7,sofawall,19,Fri Jul 17 17:30:53 2015 UTC,"More like 750, and still not even that. Also, not many chips have Iris Pro."
hardware,3dn5i7,ZeroCool2u,12,Fri Jul 17 19:23:17 2015 UTC,The fact that an integrated solution is at all approaching a discrete solution in performance is impressive by itself.   ti vs non-ti = ¯_(ツ)_/¯
hardware,3dn5i7,narwi,5,Fri Jul 17 20:17:49 2015 UTC,ti vs. non-ti for the 750 is generally a larger than 10% difference.
hardware,3dn5i7,dudemanguy301,0,Sat Jul 18 07:15:11 2015 UTC,"We're not comparing those cards to each other, just to the integrated solution."
hardware,3dn5i7,KING_of_Trainers69,1 point,Sat Jul 18 16:26:28 2015 UTC,And the difference there is even larger due to bandwidth effects.
hardware,3dn5i7,jman583,-3,Sun Jul 19 00:07:37 2015 UTC,"Don't let the branding fool you, the gtx 750 is a Kepler card, while the 750ti was Infact our very first look at maxwell. It was a significantly larger jump in performance and even more impressively energy efficiency compared to what you would expect from a ""ti"" branding for an nvidia card."
hardware,3dn5i7,CykaLogic,2,Mon Jul 20 14:36:13 2015 UTC,"750 is GM107/Maxwell, same as the 750 ti just with 512 cores instead of 640."
hardware,3dn5i7,TheImmortalLS,2,Mon Jul 20 19:10:40 2015 UTC,The best Iris Pro is weaker then a Radeon R7 250.
hardware,3dkmr0,random_digital,220,Fri Jul 17 01:06:34 2015 UTC,"That's a total 361 million loss for AMD so far in the year, and that's just sad. We need companies like AMD to instigate competition.   Nvidia and Intel shouldn't monopolize their respective markets. Hopefully AMD gets back on their feet.   By the way, does anyone feel like they have a solid grasp about what went wrong?"
hardware,3dkmr0,BuilderBrother,77,Fri Jul 17 01:29:56 2015 UTC,"I don't think Nvidia/Intel will ever become monopolies in their respective markets as neither Nvidia nor Intel wants that.  Eventually if push comes to shove, somebody will scoop up AMD for their x86 license and go from there.  Although by that point AMD might be so uncompetitive that the monopoly-esque price gouging will already be happening."
hardware,3dkmr0,TaintedSquirrel,44,Fri Jul 17 01:38:36 2015 UTC,AMD x86 isn't transferable if a company buys AMD. Intel would have any company that acquired them negotiate new terms for the license.
hardware,3dkmr0,LiberDeOpp,38,Fri Jul 17 04:19:17 2015 UTC,I have a feeling that Intel would be accommodating to avoid monopoly problems.
hardware,3dkmr0,nicholsml,26,Fri Jul 17 08:00:59 2015 UTC,That and being able to sell their own CPUs is probably important to them.
hardware,3dkmr0,mack0409,25,Fri Jul 17 08:32:15 2015 UTC,"Intel does also license some a ton of 64 bit instructions from AMD, they are having a cross license agreement after all. So it isn't like AMD hasn't also some leverage. Both companies need those cross license deal to continue in some form or another."
hardware,3dkmr0,rndnum123,33,Fri Jul 17 09:02:47 2015 UTC,"and by that you mean Intel licenses the ENTIRE AMD64 instruction set, and then they renamed it Intel EM64T just to show off how big an asshole they can be"
hardware,3dkmr0,1337Gandalf,9,Fri Jul 17 11:26:18 2015 UTC,"Yeah, that was a pretty dick move."
hardware,3dkmr0,gotamd,10,Fri Jul 17 13:56:13 2015 UTC,"How? It was in the agreement AMD signed, more like why is AMD stupid?"
hardware,3dkmr0,brightboy,11,Fri Jul 17 15:53:53 2015 UTC,"I have no idea how those negotiations went, but there was no need for Intel to rename it.  The world seems to have standardized on a combination of amd64, x64, and x86-64 anyway."
hardware,3dkmr0,gotamd,3,Fri Jul 17 17:35:11 2015 UTC,"Ha yeah I always wondered why I always saw driver folders labeled ""amd64"" when I was using an Intel processor.  Damn - so Intel is basically using AMDs 64bit architecture to make their own CPUs?"
hardware,3dkmr0,mastermikeee,3,Fri Jul 17 21:05:49 2015 UTC,Just instructions.
hardware,3dkmr0,Schmich,7,Fri Jul 17 22:58:57 2015 UTC,True but Intel also really needs the amd64 license.  Otherwise they can't sell any of their CPUs.
hardware,3dkmr0,im-a-koala,2,Fri Jul 17 12:15:12 2015 UTC,And they would because the license amds 64 bit architecture
hardware,3dkmr0,spikerman,36,Fri Jul 17 14:42:09 2015 UTC,"I don't think Nvidia/Intel will ever become monopolies in their respective markets as neither Nvidia nor Intel wants that.    That's quite true I feel, they would be heavily penialized if that did happen, and they would just not allow it at all. I imagine AMD might even be bought out if they become really in trouble.   Although by that point AMD might be so uncompetitive that the monopoly-esque price gouging will already be happening.   That's what I fear the most. But hopefully another company comes in to bring some competition, or there is some sort of legal action"
hardware,3dkmr0,BuilderBrother,5,Fri Jul 17 01:51:08 2015 UTC,"If AMD goes out of business, who will penalize Nvidia and Intel?"
hardware,3dkmr0,PigSlam,5,Fri Jul 17 17:00:50 2015 UTC,Whoever buys AMD.
hardware,3dkmr0,BuildYourComputer,1 point,Fri Jul 17 17:08:03 2015 UTC,"So you're suggesting I could wait for AMD to get into a worse position, then buy AMD, and my next step would be to penalize Intel and Nvidia? Can you tell me how you think I would do that?"
hardware,3dkmr0,PigSlam,1 point,Fri Jul 17 17:11:58 2015 UTC,"Yep. Although it's really difficult for people to conceive of how there can be competition without AMD. But the sooner this happens, the better."
hardware,3dkmr0,Darius510,2,Fri Jul 17 17:16:45 2015 UTC,"The government? Idk about US monopoly laws specifically, but some places will forcibly split up companies to avoid monopolies."
hardware,3dkmr0,Hovsky,19,Fri Jul 17 23:25:22 2015 UTC,"Intel will never be a monopoly at this point, they are up against mobile chip makers that are sliding into server chips as well.  They may have a stranglehold on PC CPU's, but that is just one part of the business, they have multiple competition on all other fronts.  Nvidia though...  no one else makes high end graphics cards.  The good news is that if AMD goes under then someone will pick up their graphics cards divisions, if anything just for the chip technology.    In fact, it may be good if they sell off the GPU division.  For years they have done nothing but make cards that are almost as good as Nvidia, but they do it a year or two late.  Any innovation they have had quickly becomes obsolete because they just can't get it to market, or get other companies to adopt their ideas.  Another company driving the GPU side of the company might actually come up with some good ideas."
hardware,3dkmr0,Thunder_Bastard,40,Fri Jul 17 03:28:26 2015 UTC,"Any innovation they have had quickly becomes obsolete because they just can't get it to market, or get other companies to adopt their ideas.   I wouldn't even say they're a year or two late. If the Fury X came out three months earlier, the entire narrative would have been different. They could have said with confidence that the Fury X was on par with the Titan X and hundreds of dollars cheaper. It took them too long to release and the GTX 980 TI became a thing. They lost out on a lot of early adaptor and enthusiast sales."
hardware,3dkmr0,sk9592,32,Fri Jul 17 03:38:49 2015 UTC,"Yeah, NVIDIA timed and priced the 980 Ti in a very clever manner, which had a huge impact on the Fury X's launch.  I'm sure AMD would've wanted the Fiji cards out to follow the TITAN X, but couldn't get them ready in time. Or maybe they didn't expect the 980 Ti to come out so early, and at such a (relatively) low price?  When compared to the 980, which I believe was Fiji's target, the Fury cards are very good. The 980 Ti, however, has been a huge spanner in the works."
hardware,3dkmr0,Aldarro,18,Fri Jul 17 04:12:58 2015 UTC,"Yeah, NVIDIA timed and priced the 980 Ti in a very clever manner  Or maybe they didn't expect the 980 Ti to come out so early, and at such a (relatively) low price?   This is all true, but should AMD or anyone else have been surprised by this?  Personally, this is exactly what I expected. It's not like the release cycles of these companies is unpredictable. Back in 2013, Nvidia released the original Titan based on the brand new GK110 core. Three months later, they released the GTX 780 with the same GK110 core (cut down, crippled double precision, half the vram). The gaming performance difference was negligible.  Fast forward to 2015:  Nvidia releases the Titan X based on the brand new GM200 core. Three months later, they released the GTX 980 TI with the same GM200 core (cut down, half the vram). The gaming performance difference is negligible.  Literally the same story. If AMD couldn't see it coming, no wonder they can't compete.  However, what you said is more likely. They were trying to make an earlier deadline and just couldn't do it."
hardware,3dkmr0,sk9592,7,Fri Jul 17 04:36:34 2015 UTC,"Fury X is sold out everywhere though, so for now supply is not even meeting demand."
hardware,3dkmr0,jaju123,14,Fri Jul 17 08:37:53 2015 UTC,Might be a big supply problem. When you hear of geographical regions only getting 1 card for review it makes you wonder how many are really available.
hardware,3dkmr0,PappyPete,3,Fri Jul 17 10:29:12 2015 UTC,"I was completely floored by that, how could AMD possibly think that was okay?"
hardware,3dkmr0,Jamolas,7,Fri Jul 17 12:16:26 2015 UTC,"Sometimes going with the latest and greatest (see HBM) just isn't the best thing to do businesswise when you are stuck in the rut like AMD. I personally think it was a mistake to go HBM for AMD, as their margins on each card are significantly less, and HBM has a significant impact on the yield rate."
hardware,3dkmr0,Kinaestheticsz,2,Fri Jul 17 12:24:36 2015 UTC,They wanted to have bad yields?
hardware,3dkmr0,innerknowing,7,Fri Jul 17 16:00:08 2015 UTC,Its easy to sell out of something when there is next to no supply.  Their vendors can't even get enough Fury XT chips to produce cards.  Notice only 2 vendors launched the base Fury initially?
hardware,3dkmr0,innerknowing,30,Fri Jul 17 15:59:22 2015 UTC,"For years they have done nothing but make cards that are almost as good as Nvidia, but they do it a year or two late.   That's not true. Nvidia and AMD/ATI trade blows back and forth all the time. The biggest issue is that Nvidia has much better marketing and owns a larger portion of GPU's used. Both companies have their fanboys but there are A LOT more Nvidia fanboys. Also it gets old hearing people say ""I hate AMD's driver issues"".... yeah that was a thing back with the 9800 pro's... give it a fucking rest already, it's been over a decade!"
hardware,3dkmr0,nicholsml,1 point,Fri Jul 17 07:58:22 2015 UTC,"it's been over a decade!   Unless you're on Linux. And isn't AMD slow to update/release crossfire profiles for new games, too?"
hardware,3dkmr0,Mr_s3rius,12,Fri Jul 17 09:59:44 2015 UTC,"Quality over Quantity, for me at least. I don't have Crossfire and I don't pre-order games, so I don't need day-one profiles.  But that differs for everyone of course."
hardware,3dkmr0,LongBowNL,4,Fri Jul 17 10:20:47 2015 UTC,"Unless you're on Linux   Loook... I love Linux distros and would love to go that way at home. The problem is we are talking about GPU's and gaming. I know some people like to buck the system and game on linux anyways, but let's be real, if you're a gamer and interested in GPU's you're most likely running windows. I know there are outliers, but we aren't really focused on that here."
hardware,3dkmr0,nicholsml,6,Sat Jul 18 07:01:42 2015 UTC,"as a linux user, ill only buy AMD now  theyre the only one with an open source driver that works"
hardware,3dkmr0,electricheat,7,Fri Jul 17 13:37:01 2015 UTC,"AND is decently supportive of their open source drivers, while nVidia isn't nearly as much. In my opinion, AND is supporting Linux drivers in the right way, they just don't have the resources to make the open source driver as good as nVidia's closed source driver."
hardware,3dkmr0,JDAndChocolatebear,6,Fri Jul 17 19:00:35 2015 UTC,Bo who is on linux really. 1% of market?
hardware,3dkmr0,jakobx,5,Fri Jul 17 11:57:37 2015 UTC,"Most high performance computing clusters will be using Nvidia. Where I work, we exclusively buy Nvidia GPUs (usually Titans or K40s/K80s). AMD's linux drivers just don't compare to Nvidia's. Plus, CUDA."
hardware,3dkmr0,invisiblemovement,4,Fri Jul 17 14:54:45 2015 UTC,Every fucking server on the planet
hardware,3dkmr0,Spidertech500,6,Fri Jul 17 14:23:03 2015 UTC,"In fact, it may be good if they sell off the GPU division. For years they have done nothing but make cards that are almost as good as Nvidia   Um, say what?  The 290x was a more powerful card than the GTX 780, and went toe-to-toe with the 780ti which Nvidia released in order to catch up to the 290x (and it still had less VRAM)  The only ""gap"" they had in competitive cards was when the 980 was released."
hardware,3dkmr0,bizude,3,Fri Jul 17 15:43:44 2015 UTC,"The 290x was a more powerful card than the GTX 780, and went toe-to-toe with the 780ti    The 780 wasn't competing with the 290X as it was the neutered big Kepler (like the 290 being a neutered 290X), 780 Ti and 290X were in direct competition."
hardware,3dkmr0,Blubbey,16,Fri Jul 17 17:00:11 2015 UTC,"obsolete because they can't get it to market...   Do you not remember the R9 290x out performing the $1,000 Titan when it was released? And it did it for like $499."
hardware,3dkmr0,entropicresonance,10,Fri Jul 17 06:18:12 2015 UTC,"I remember that the Titan wasn't really put out as a gaming card, and pretty much every high-end gaming card beat it at gaming.  Rendering and design was another matter."
hardware,3dkmr0,Thunder_Bastard,3,Fri Jul 17 06:35:24 2015 UTC,Yeah but the 780ti came out later and didn't beat it by a lot
hardware,3dkmr0,entropicresonance,6,Fri Jul 17 06:44:34 2015 UTC,The titan was beaten by the 780Ti by quite a bit the the Titan Black however was more or less identical.
hardware,3dkmr0,Harag5,5,Fri Jul 17 07:42:15 2015 UTC,"Nvidia though... no one else makes high end graphics cards. The good news is that if AMD goes under then someone will pick up their graphics cards divisions, if anything just for the chip technology.    I dream of the day that Samsung makes GPUs for PCs. That would put Nvidia on their toes, no way they'd get away with the 3.5 fiasco with Samsung as opposition."
hardware,3dkmr0,ExogenBreach,11,Fri Jul 17 05:49:01 2015 UTC,Personally I want Intel to make discreet GPUs. If anyone would wipe the floor with nvidia it would be Intel.
hardware,3dkmr0,entropicresonance,11,Fri Jul 17 06:16:06 2015 UTC,They were going to then ended up scrapping the project because their GPUs weren't as good as the competition. They used the knowledge from the project though and started making APUs.
hardware,3dkmr0,acu2005,8,Fri Jul 17 07:42:37 2015 UTC,"Nah, they were already making integrated graphics before that and APUs were just moving that on chip. Intel's failed GPU project went on to be the Xeon Phi, which competes with NVIDIA's Tesla product for supercomputers."
hardware,3dkmr0,SomeoneStoleMyName,2,Fri Jul 17 12:02:05 2015 UTC,"The Xeon Phi is based off their GPU efforts? Hot damn, I had no idea! Though, I guess that makes sense considering how parallel the phi is."
hardware,3dkmr0,hak8or,4,Fri Jul 17 13:21:58 2015 UTC,search Larrabee: http://www.anandtech.com/show/3738/intel-kills-larrabee-gpu-will-not-bring-a-discrete-graphics-product-to-market  nVidia shat their panties when saw what Intel could do
hardware,3dkmr0,zmeul,3,Fri Jul 17 14:17:02 2015 UTC,"Why? Nvidia has been doing it for years, invested alot of money researching, I really doubt Intel would get toe-to-toe with Nvidia."
hardware,3dkmr0,makegr666,10,Fri Jul 17 06:53:19 2015 UTC,"Intel owns fab plants and THOUSANDS of engineers, they could undercut the shit out of nvidia. Their software might not be as good for a number a years but hardware wise they could destroy them quickly and easily."
hardware,3dkmr0,entropicresonance,11,Fri Jul 17 06:59:22 2015 UTC,"Their software might not be as good for a number a years   This is the hardest part of making GPUs. Game makers produce absolutely shitty code that the driver writers then have to manually code for. This is why you need a new driver update for each major title. Building up a driver library would take a massive amount of time and research.  Intel already has lots of areas to expand into, areas that are part of their core competency: CPUs. There is no reason for them to start making GPUs other than the APUs they already make."
hardware,3dkmr0,thewaterbottle4,4,Fri Jul 17 08:17:44 2015 UTC,"Another company purchasing AMD voids the x86 license. How it affects intel's x64 license however, I do not know."
hardware,3dkmr0,malted_rhubarb,5,Fri Jul 17 04:11:47 2015 UTC,I think it goes both ways. Intel and whoever bought AMD's CPU division would have to renegotiate the x86 and x64 license.
hardware,3dkmr0,screwyou00,1 point,Fri Jul 17 07:33:49 2015 UTC,If anyone baby AMD neither Intel or the new owners of AMD could use IP of the other company.
hardware,3dkmr0,mack0409,13,Fri Jul 17 08:31:35 2015 UTC,"New CPUs haven't been released in years, GPUs started losing when the 290X came out due to the bad press that came with huge power consumption, 290x prices lowered to insanely low prices=shitty margins on a 400mm2 chip.  All started with Phenom I being delayed and poor performing, then AMD's GPUs started becoming less competitive. AMD also offered an inferior software solution."
hardware,3dkmr0,CykaLogic,3,Fri Jul 17 01:48:08 2015 UTC,"GPUs started losing when the 290X came out due to the bad press that came with huge power consumption,   Which was equal to the consumption of its competition, the 780/ti."
hardware,3dkmr0,bizude,2,Fri Jul 17 15:51:23 2015 UTC,"Which was equal to the consumption of its competition, the 780/ti.    Not the 780, 780 Ti it was similar to.  http://images.anandtech.com/graphs/graph7492/59709.png  http://www.anandtech.com/show/7492/the-geforce-gtx-780-ti-review/15  http://media.bestofmicro.com/W/R/408123/original/04-Power-Consumption-Gaming.png  http://www.tomshardware.co.uk/geforce-gtx-780-ti-review-benchmarks,review-32820-17.html  http://tpucdn.com/reviews/MSI/GTX_780_Ti_Gaming/images/power_peak.gif  http://www.techpowerup.com/reviews/MSI/GTX_780_Ti_Gaming/22.html  http://techreport.com/r.x/geforce-gtx-780ti/power-load.gif  http://techreport.com/review/25611/nvidia-geforce-gtx-780-ti-graphics-card-reviewed/11"
hardware,3dkmr0,Blubbey,8,Fri Jul 17 17:06:29 2015 UTC,"By the way, does anyone feel like they have a solid grasp about what went wrong?   Not really a mystery. Declining share of the CPU and GPU market. This has been going on for a few years, but it keeps getting worse.  AMD hasn't released any actual new CPU tech since Kaveri APUs last year. Even that wasn't revolutionary from a CPU performance sense. FX CPUs are essentially using the same tech from 2011, which was behind Intel then. We probably won't see Zen until mid 2016.  New GPUs have either been rebrands or the lackluster Fury X launch. (Although, this doesn't factor into Q2)  They may be the exclusive chipset making for the gaming consoles, but profit margins on those are razor thin."
hardware,3dkmr0,sk9592,5,Fri Jul 17 03:56:30 2015 UTC,"Their problem is that they don't know who their market is, which is absolutely bizarre to me in 2015.  This is a leadership problem from the top-down.  For a company with annual revenue around 1 billion, it should be a drop in the bucket to hire a consulting firm to conduct market research on who is currently using, and who needs, their components.  Assuming that they are doing this already, they need to fire those guys and start fresh.   Without some direction, their R&D team just makes stuff that doesn't really have a clear purpose or true benefit for the audience they're selling it to."
hardware,3dkmr0,Spreadsheeticus,6,Fri Jul 17 17:05:30 2015 UTC,Amd basically hasn't released much in the past few years.
hardware,3dkmr0,Cozmo85,27,Fri Jul 17 02:05:03 2015 UTC,"You could make an argument that their high end GPU tech has been impressive, but fell short in a couple of critical ways.  Fiji GPUs would have been seen as much more impressive if they were released right after the Titan X. Timing is everything, and those 3 months killed them. The GTX 980 Ti is too compelling a product to ignore.  Hawaii GPUs had the same story. They were 6 months too late. The GTX 780 was already on the market. AMD was bragging in their marketing about how the R9 290X was a Titan killer, all the time willfully ignoring the existence of the GTX 780.  AMD's refusal to allow non-reference coolers on launch also killed them. The R9 290 and 290X aren't actually all that much more power hungry than GK110 GPUs. However, for several months, the only coolers available for them were the loud, hot reference cooler. Once upon a time, Nvidia used to have the reputation for having the hot noisy cards (Fermi). The bad rep that the Hawaii reference coolers got follows AMD to this day.  Another thing that hurt AMD in the long run was the mining boom. While it sold a lot of cards in the short term, it kept AMD cards out of the hands of gamers for several months. If you wanted to buy a gaming GPU in late 2013-early 2014, your only option was Nvidia. A lot of people stopped even considering AMD as a viable option. This is long term damage. If you look at AMD's marketshare overtime, it never really recovered the loss of marketshare it suffered during that period."
hardware,3dkmr0,sk9592,8,Fri Jul 17 03:49:42 2015 UTC,The 290X was $100 cheaper than the 780.
hardware,3dkmr0,TaintedSquirrel,5,Fri Jul 17 04:17:40 2015 UTC,"When it launched yes. Within a couple weeks Nvidia dropped the price of the 780 to $500.  However, the real issue was that AMD kept comparing the 290/290X to the Titan and not the more closely priced 780.  They did the same thing this time around with the Fury X and Titan X. I don't know who their marketing team is trying to fool. Everyone knows that the GTX 980 Ti exists.   Saying that you can beat the Titan or Titan X at nearly half the price is just willful ignorance of the marketplace."
hardware,3dkmr0,sk9592,6,Fri Jul 17 04:26:46 2015 UTC,"They really wanted to push the bang for buck angle. It's a bit silly to us because we know better but to most people, they'd likely see AMD cards as a better deal."
hardware,3dkmr0,Anaron,6,Fri Jul 17 05:50:24 2015 UTC,"Yeah, but a high end end video card isn't generally something that your average person will buy without asking their friend ""who is good with computers"".  If I am going to recommend a high end gaming card to my average joe friend its going to be a 980 ti at this point. Not even because its the ""best"". Its because I feel that games generally work better with Nvidia out of the gate, less waiting for game ready drivers, more developer support, etc...  AMD may release a bunch of beta driver updates but my average joe friend is only going to look for the official releases and thus will have the perception that AMD never releases new drivers.  I think AMDs biggest problem isn't their hardware, they make great stuff. Its the perception that they are always a step behind Nvidia and Intel."
hardware,3dkmr0,hizaed,3,Fri Jul 17 15:43:41 2015 UTC,well even if intel/nvidia monopolize the government can break them up at that point so to stay alive (or at least whole) they would need to not kill AMD and there's a reason i'm making a full AMD PC to add my 2 cents to the pile and help the little guy instead of possible future monopolies.
hardware,3dkmr0,Cynical_Ostrich,13,Fri Jul 17 02:29:07 2015 UTC,"well even if intel/nvidia monopolize the government can break them up at that point so to stay alive   You make it sound so easy. This isn't like breaking up Standard Oil or AT&T. You can't just make it smaller versions of the existing company with a smaller marketshare.  Intel is only able to make the advancements it's making in chip tech because it has 20,000 engineers to throw at a problem and owns its own fabrication plants. If you break it up into 4 companies with 5000 engineers each and spin off the fabrication plants, you will kill any ability Intel has to innovate."
hardware,3dkmr0,sk9592,14,Fri Jul 17 04:03:26 2015 UTC,Being a monopoly isn't even illegal. You have to abuse it.
hardware,3dkmr0,Cozmo85,4,Fri Jul 17 04:25:08 2015 UTC,"At this point, AMD is a lost cause. They lack the funding to do the R&D necessary to keep up. Fury X might have been great if it had a little more RAM, but people are still skeptical of the 4gig limit. ThThe complete lack of overclocking still has everyone concerned. If these were ""overclocker's dream"" cards, we would have seen unlocked voltage by now. As it stands, the 980 ti is an overclockers dream. 40% overclock is pretty much the norm.  I've worked in enough large corporations to know that what is promised is almost always not what is delivered. AMD promised a lot that did not come true.   The only real solution is a bigger company swallows them up and invests heavily."
hardware,3dkmr0,XaeroR35,13,Fri Jul 17 06:07:27 2015 UTC,AMD is a big company and isn't just a high end card card manufacturer.  Writing them off cause they fell a little short on the highest end GPU which is a very niche market is a bit shortsighted.  Debt is cheap for big companies right now and AMD could arguably borrow until they hit another home run.  I'm not saying everything is rosy over ay AMD  I'm not even saying they'll be around in their present form in a couple years.  But calling AMD a lost cause is a bit hyperbolic.
hardware,3dkmr0,mulholland,5,Fri Jul 17 11:14:41 2015 UTC,"Writing them off cause they fell a little short on the highest end GPU which is a very niche market is a bit shortsighted.   If that was the only thing they ""fell a little short on"" nobody'd be worried about AMD. They've been falling a little short on everything, for several product generations now. Even their solid products have trouble taking off. Let's not downplay the extent of their problems; they've fallen far more than ""a little"" short."
hardware,3dkmr0,SPOOFE,3,Fri Jul 17 17:18:00 2015 UTC,"I agree, their products have consistently fallen a little short.  Leading to them lose market share.    I was replying to the extent that Xaero was weighting  the Fury(a niche product) in his conclusion that AMD is ""a lost cause""."
hardware,3dkmr0,mulholland,2,Fri Jul 17 18:52:39 2015 UTC,I don't think the majority of AMD and Nvidia's sales come from high end video cards.
hardware,3dkmr0,youstolemyname,2,Fri Jul 17 11:38:30 2015 UTC,"Flagship products never are, but they help drive consumer perceptions of lower-tier products. Further, the margins tend to be nicer, and I bet the bragging rights go over well with investors."
hardware,3dkmr0,SPOOFE,2,Fri Jul 17 17:20:35 2015 UTC,"Race on sunday, Sell on monday"
hardware,3dkmr0,Cozmo85,0,Fri Jul 17 22:11:45 2015 UTC,The 4GB limit is a hardware limitation of HBM Gen 1.
hardware,3dkmr0,TehRoot,67,Fri Jul 17 10:41:39 2015 UTC,"Such a shame that they keep losing money. Lets hope Zen is good, else AMD may cease to be a thing unless a buyout happens, with them losing $181 Million but Intel making $2.7 Billion in Net income. Thats huge."
hardware,3dkmr0,danzas4321,28,Fri Jul 17 01:08:35 2015 UTC,I believe after 2 more quarters of these kinds of losses AMD goes bankrupt. There's a $600M minimum level of cash reserves they have to maintain as a deal with investors(afaik). They only have like 1B left after this quarter's losses.  Not quite enough time for Zen. Perhaps someone else will buy AMD and make it competitive.
hardware,3dkmr0,CykaLogic,23,Fri Jul 17 01:50:48 2015 UTC,"I wouldn't be surprised if there are already talks behind closed door of acquisition options if the shit hits the fan.  I could definitely see the argument that it is already too late to save AMD in its current form. Quite frankly, if AMD is acquired by a company that has the R&D budget that could allow it to success, that wouldn't be the worst thing."
hardware,3dkmr0,sk9592,10,Fri Jul 17 04:08:32 2015 UTC,Samsung?
hardware,3dkmr0,aloha013,15,Fri Jul 17 05:44:59 2015 UTC,"Samsung definitely has the R&D AMD needs, and If anyone were to buyout AMD I would want it to be them, but I fear Samsung would slowly kill off the dGPU section of AMD and focus on the CPU section only :("
hardware,3dkmr0,screwyou00,15,Fri Jul 17 07:27:07 2015 UTC,"Heck, i dont think that would happen, i think they might kill of both and use the engineers for their mobile chips, that is a very real possibility."
hardware,3dkmr0,olavk2,6,Fri Jul 17 10:32:41 2015 UTC,Microsoft perhaps?
hardware,3dkmr0,YeezusTaughtMe,43,Fri Jul 17 04:28:26 2015 UTC,"Microsoft CEO Satya Nadella just did a ton of house cleaning. Balmer was a lot more passionate about entering the hardware market than Nadella is. Hardware has mostly been a money loser for Microsoft. The exception being Xbox, which took 9 years to become profitable. The hardware Microsoft has sold has always been consumer facing.  Anyway, Microsoft is currently in the process of slimming down and doubling down on being a software and services company. The last thing they want to do is acquire a failing hardware company, a non-consumer facing one at that. Despite how people on this subreddit might feel, CPUs and GPUs are not consumer facing products. They are components sold in mass to OEMs, who then make consumer products out of them."
hardware,3dkmr0,sk9592,7,Fri Jul 17 04:44:16 2015 UTC,What an informative thread.
hardware,3dkmr0,Sandoness,5,Fri Jul 17 05:32:32 2015 UTC,"Microsoft would be wiser to build on ARM for specific user base, such as, light consumer and data entry machines than buy AMD for their CPU/GPU tech.  Even Samsung for that matter. AMD has to catch up on their own merit to survive. Even IBM (a non x86 manufacture) likely won't touch that consumer market of x86 without a bright future. If Zen rocks, a buy out might be what their investors want. Because even if Zen is badass, people might not put faith in them right way."
hardware,3dkmr0,Step1Mark,2,Fri Jul 17 06:39:05 2015 UTC,Oh no please no.
hardware,3dkmr0,hustl3tree5,1 point,Fri Jul 17 06:40:31 2015 UTC,Qualcomm. Already market leader in mobile cpus
hardware,3dkmr0,elk-x,11,Fri Jul 17 11:25:51 2015 UTC,"Wrong. People told you and showed you were wrong in /r/AdvancedMicroDevices but you peddle the same BS here.  Quotes...    What are you talking about? Bankrupt is when you can't service debts, they don't have any large notes due until 2018.      Actually its estimated actual valuation is 3.5 mills. However, when this report was written, there were no details on the future roadmap. A lot of the R&D fat cutting was Rory Read's boneheaded projects, which is now evident after the last investor's day.      AMD's ""losses"" are in stock prices for the most part, which in no way represent real money lost by the company. People tend to not realize how much of a fantasy land Wall Street is."
hardware,3dkmr0,TehRoot,13,Fri Jul 17 10:37:48 2015 UTC,"the guy is wrong and it's easily proven with the latest quarterly summary.  http://phx.corporate-ir.net/External.File?item=UGFyZW50SUQ9Mjk1MjQ0fENoaWxkSUQ9LTF8VHlwZT0z&t=1&cb=635726621818808154  assets valued at ~$3.4 billion, including cash (or equivalent) $829 million.  current  liability ~1.4 Billion.  long term ~$2.1 billion.  constantly having losing quarters doesn't mean you will go bankrupt, but they wont get to keep borrowing money unless they can prove themselves.  but the timing of it is anyones guess and certainly not half a year.  they might be bought by then, but not go bankrupt."
hardware,3dkmr0,logged_n_2_say,2,Fri Jul 17 14:12:00 2015 UTC,That link just says Dummy pdf.
hardware,3dkmr0,TehRoot,6,Fri Jul 17 14:14:27 2015 UTC,"weird.  http://quarterlyearnings.amd.com/phoenix.zhtml?c=74093&p=quarterlyearnings  -> q2 ""financial tables"" pdf."
hardware,3dkmr0,logged_n_2_say,2,Fri Jul 17 14:23:29 2015 UTC,http://quarterlyearnings.amd.com/phoenix.zhtml?c=74093&p=quarterlyearnings   Thanks.
hardware,3dkmr0,TehRoot,10,Fri Jul 17 14:26:36 2015 UTC,"A lot of people say that a buyout would be good. A company like Samsung buying AMD's CPU division and injecting money for R&D, plus Samsung has fabs if I'm not mistaken."
hardware,3dkmr0,Yearlaren,13,Fri Jul 17 02:50:24 2015 UTC,"Yep, Samsung owns fabs. Retooling fabs is not a simple process, but at least owning your own fabs again is a start. Intel, by their own admission, says that they wouldn't be able to achieve a lot of what they do if they didn't have complete control over their fabs.  AMD needs to negotiate with fabs and work on the fab's timeline, not their own."
hardware,3dkmr0,sk9592,3,Fri Jul 17 04:10:24 2015 UTC,What's a fab? Fabrication...?
hardware,3dkmr0,Jadis,19,Fri Jul 17 05:31:11 2015 UTC,"Yep, processor fabrication plants. They are extremely expensive and difficult to own and maintain. It also takes a lot of money and work to transition fabs over to a new manufacturing process. Often, even minor changes can cost billions of dollars to implement in a fab. For this reason, most of the companies that design processors will contract the actual manufacturing out to a fab. Apple does this with the A8 and A8X, and Nvidia does this with their GPUs.  Samsung and Intel are exceptions. They are companies that design processors and manufacture them in their own fabs."
hardware,3dkmr0,sk9592,6,Fri Jul 17 05:40:05 2015 UTC,"Yea, a fabrication plant for* semiconductor chips. CPU fabs ludicrously expensive (a few billion dollars). Intel has 12, and AMD used to have some but they sold them. GlobalFoundries ( an AMD spinoff) makes AMD's chips right now."
hardware,3dkmr0,qwerqmaster,2,Fri Jul 17 05:44:47 2015 UTC,"Samsung makes a lot of stuff. Their own CPUs, Apple's A7 CPUs, the memory modules on some RAM, LCD/LED panels, flash memory, CMOS sensors..."
hardware,3dkmr0,seargentcyclops,43,Fri Jul 17 09:33:24 2015 UTC,"For me, it's been the same story for the past few years  Good hardware but <insert product that offers better value for me>  They gotta deliver with Zen. I'm done waiting for AMD to release something that I genuinely believe in. Carrizo seems to be in the right step, but just need that bit more."
hardware,3dkmr0,kjoro,30,Fri Jul 17 01:42:22 2015 UTC,How many swings and misses has AMD had over the last 5+ years?  I'm done giving them the benefit of the doubt.  You're safer just assuming Zen is going to be a failure just like every other CPU they've released in nearly a decade.
hardware,3dkmr0,TaintedSquirrel,26,Fri Jul 17 01:52:13 2015 UTC,"Absolutely, I still however am willing to give Zen an honest look."
hardware,3dkmr0,kjoro,18,Fri Jul 17 01:55:35 2015 UTC,"AMD took a gamble on multithread, and if software and game devs properly multithreaded their applications AMD 8core cpus would have competed agaisnt the i5s much better."
hardware,3dkmr0,entropicresonance,7,Fri Jul 17 06:26:51 2015 UTC,"Besides bulldozer and gen 1 APUs? nothing.   Every product  they've released so far has been good performance, great value per dollar compared to other offerings."
hardware,3dkmr0,TehRoot,5,Fri Jul 17 10:43:21 2015 UTC,great value per dollar compared to other offerings.   this is mostly because they missed mass market appeal and had to reduce prices.  you can have the best product in the world but if it doesnt sell that's a miss.  the 7000 and 200 series were pretty good.  cpu's (the biggest problem) havent really made waves in a long time.
hardware,3dkmr0,logged_n_2_say,5,Fri Jul 17 14:17:33 2015 UTC,"CPU architectures take years to develop. They've been working on zen since Keller came on in 2012.   They tried to bridge the gap by increasing the IPC of each individual revision of bulldozer.   The APUs started hitting stride by Llano.   They weren't exactly selling GPUs at a loss, but they were selling them with a smallish margin to regain some market share.   The biggest downer is the fact that we've been stuck on 28nm so it's been hard for AMD to break through."
hardware,3dkmr0,TehRoot,3,Fri Jul 17 14:19:37 2015 UTC,"i think that apu's are good products, but so far they dont have mass market appeal or they haven't educated their OEM's and the public about their advantages.  they should be more for mobile/laptop markets and ""facebook machines"" but the cpu performance is under similar priced intels and the power consumption is higher.  hopefully zen will address the shortcomings of am3/+ and it should be interesting to have a unified socket.  as we were discussing elsewhere, AMD isn't dead but they need a big win and maybe zen can provide it."
hardware,3dkmr0,logged_n_2_say,28,Fri Jul 17 14:29:06 2015 UTC,"Aah, fond memories of my old Thunderbird are coming back. We all thought the Intel/AMD competition would go on forever, they were so good at the time. Sigh..."
hardware,3dkmr0,IronMew,13,Fri Jul 17 02:34:04 2015 UTC,Dat pencil mod
hardware,3dkmr0,sishgupta,4,Fri Jul 17 04:14:13 2015 UTC,That's how one unlocked it right?  High school was a long time ago for me :(  I shoulda gotten a t-bird when I had the chance....on my old SK-6 with a 60mm delta...  Such good days...
hardware,3dkmr0,Cheeze_It,7,Fri Jul 17 07:09:13 2015 UTC,"Yes. You bridged the pins to unlock the multiplier, if I remember correctly.  I never did the mod, I was too scared about the thing's complete lack of thermal management to risk an overclock. People tried to see what happened if you removed the heatsink, and lots of expensive magic smoke was lost...  Still have that computer somewhere, with some old GeForce and Win98SE installed to the hard disk, waiting for me to make it into a retrogaming box..."
hardware,3dkmr0,IronMew,1 point,Fri Jul 17 10:01:04 2015 UTC,? You always needed a heatsink.   I only ever did my pencil trick after I had cascaded the chip to my server.   But yeah you definitely had to re-do the thermal paste and HSF on the cpu after you did the pencil trick.
hardware,3dkmr0,sishgupta,3,Fri Jul 17 13:46:01 2015 UTC,"? You always needed a heatsink.    Yes, but Thunderbirds needed imposing chunks of metal with big fans to dissipate all that heat, and many questionable designs existed that only clipped on the socket's plastic tabs. And if the tabs gave under the weight of the heatsink the whole thing could drop to the bottom of the case, leaving the processor uncovered.  If this happens to a modern CPU it thermal-throttles, then hangs (a few low-power models will actually soldier on with no heatsink at greatly reduced speed), and is good to go as soon as you replace the heatsink. If it happened to a thunderbird it would instafry itself and release clouds of magic smoke.   But yeah you definitely had to re-do the thermal paste and HSF on the cpu after you did the pencil trick.   These days I have applying paste down to an art, but in my younger years it was something that filled me with dread. In a world of bits and digital things with binary states, the idea of messing around with greasy, dirty goop was the most inelegant and unfamiliar thing I could think of."
hardware,3dkmr0,IronMew,1 point,Fri Jul 17 14:52:25 2015 UTC,Eh the hsf I used then was a fraction of the size of what's normal now. Not even a quarter.  But yes I do remember concerns around cracking the die because of the stupid hsf clip.
hardware,3dkmr0,sishgupta,1 point,Fri Jul 17 15:01:58 2015 UTC,I had a tbird and an ATI Radeon 64MB. Good times.
hardware,3dkmr0,sishgupta,1 point,Fri Jul 17 13:47:02 2015 UTC,Going from 1.33 to 1.4 Ghz made all the difference!
hardware,3dkmr0,okieboat,1 point,Fri Jul 17 16:28:51 2015 UTC,"I had a Thunderbird, and then the first ""Sledgehammer"" (Opteron). AMD was on top then. I've stuck with them trough the Phenom stuff, but sadly I've lost faith in AMD after FX. x86 is a complex arch, it takes years to design a new architecture and then many years of incremental improvements to rival Intel.  Don't think we'll see a competitive non Intel x86 CPU ever again. Intel's only competition at this point is ARM."
hardware,3dkmr0,noiserr,1 point,Fri Jul 17 13:46:33 2015 UTC,"I wonder if at some point desktop computing will migrate to Arm as well. There have been a few attempts, but they've all ran Linux  or Android - the former is good for many things but mainstream user-friendliness isn't one of them, and the latter is good for mobile but not for generic computing.   As long as mainstream Windows and games don't get ported to run on Arm, I'm afraid we can only look forward to more Intel. Not that it's bad - Intel is making some impressive hardware these days - but I for one am bored of evolution and would love some revolution."
hardware,3dkmr0,IronMew,22,Fri Jul 17 14:44:37 2015 UTC,That's depressing. I'd like to see something good about AMD for once.
hardware,3dkmr0,__________________99,16,Fri Jul 17 03:19:07 2015 UTC,"The Phenom II x4 and x6 were great for us budget animators, editors, or people that had software that could do multi threading. When the 6-8 core stuff was coming out, not many people could take advantage or them.  AMD tends to be the best value but their marketing does not convey that.    AMD's FX-9590 if the beginning of them starting to have alright single thread performance. When speaking of multi threading, AMD's FX-9590 is 225 USD where as Intel's matching point for performance is the Core i7-4771 that goes for 315 USD. Unfortunitly Intel's single threaded performance matched with their multi threading is superior and is generally better than AMD's locked thread to core count.  Zen will have multi threading and a lot more performance per clock than their current stance.  If their 40% faster than their previous arch holds true, their Cpus will match Intel's current performance per clock.  So if they can pull Zen off, they are back and better than ever.    Intel recently an announce their backstep on the tick tock release cycle. This could be what AMD needs over the next 18 months to match and by chance step ahead ... That is if sales follow.  AMD sucks at marketing though, so it will be hard for them to be in as many consumer products as Intel's Core brand."
hardware,3dkmr0,Step1Mark,5,Fri Jul 17 07:02:53 2015 UTC,1Gb HBM + Zen should give them the iGPU market. Won't happen.  Is it not possible to put a DDR5 controller on it ala PS4? At least give the motherboard manufacturers the option of selling a board with some fast RAM soldered on?
hardware,3dkmr0,PlankWithANailIn,9,Fri Jul 17 09:52:29 2015 UTC,"GDDR5 != DDR5. GDDR5 is super high bandwidth DDR3 in the most basic sense in order to service the massive data throughput needed by GPUs. The trade off how ever is that GDDR5 sacrifices latency in retrieving data, which is critical in getting smaller jobs loaded from RAM, to the cache, processed, and back to RAM in a timely fashion.    As the PS4 is a gaming device, CPU can suffer a bit in order to facilitate a simpler programming environment, hence the pooled. GDDR5 used by CPU and GPU.    A smarter option would be to give a huge L4 cache much like Intel's Iris Pro. HBM inclusion could be strapped to the die, but that would increase size considerably, while strapping it to the motherboard essentially renders HBM's biggest benefit, super short paths to the die, virtually useless. Not to mention that all iGPUs produced would need to share a standardized bus width, meaning more sockets available or more frequent updates in order to keep up with constant revisions.    This ended up way longer than I intended. Tl;Dr adding HBM would require a whole lotta bs that wouldn't be worth it."
hardware,3dkmr0,Fragarach7,8,Fri Jul 17 16:47:28 2015 UTC,Their 390 is the best bang for the buck card at the moment :D yay AMD!
hardware,3dkmr0,ubern00by,2,Fri Jul 17 08:38:33 2015 UTC,"Well they have managed to keep the lights on in the office, so that's something good."
hardware,3dkmr0,RiffyDivine2,4,Fri Jul 17 12:06:20 2015 UTC,We all would.
hardware,3dkmr0,YeezusTaughtMe,35,Fri Jul 17 04:37:29 2015 UTC,"AMD's investors are pissed, this is what happened during the earning calls:   you say you guys are on track for six months, but clearly you've just shown you can't predict two and a half months ahead and don't have a history of being stable even when the market is   AMD also deflected a lot of questions about Fury (X) supply"
hardware,3dkmr0,zmeul,2,Fri Jul 17 02:49:11 2015 UTC,"Yeah it was kind of funny, time to dump stock any stock I got left."
hardware,3dkmr0,RiffyDivine2,28,Fri Jul 17 12:18:25 2015 UTC,i just brought 2 r9 290's. I feel that to much of us are rooting for AMD but not supporting them in anyway.
hardware,3dkmr0,tickletaylor,37,Fri Jul 17 05:33:20 2015 UTC,"Everybody wants to support AMD , but nobody wants to SUPPORT AMD"
hardware,3dkmr0,Shade_Raven,17,Fri Jul 17 05:57:07 2015 UTC,Like the comments in some other forums.  I hope that Fury smashes 980ti so I can buy a 980ti at a reduced price
hardware,3dkmr0,Hagtzel,1 point,Fri Jul 17 23:08:51 2015 UTC,Yeah I see what you mean
hardware,3dkmr0,pwnegekill,1 point,Fri Jul 17 09:08:55 2015 UTC,Everybody doesn't want to support them and that's the problem they have. Well along with other ones that just start fights so will skip them.
hardware,3dkmr0,RiffyDivine2,5,Fri Jul 17 12:02:51 2015 UTC,"Companies should not be supported, but they should earn your money. I'll buy an R9 390 because its an actual good card and not because I want to ""donate"" to AMD."
hardware,3dkmr0,JonF1,3,Fri Jul 17 21:02:03 2015 UTC,"waiting for the fury to be available , i'm going for it, i bet in q3-4 they will get a profit if fury/fury x/nano is actually available for purchase"
hardware,3dkmr0,Perunsan,4,Fri Jul 17 11:25:43 2015 UTC,Even with an amazing year it won't be enough to turn around the cost they are out from R&D and other expenditures the company has had lately. In the next Q4 you may see a new CEO however. But the reviews to see them bankrupt by 2017 is looking more realistic.
hardware,3dkmr0,RiffyDivine2,1 point,Fri Jul 17 12:04:34 2015 UTC,"getting a new CEO would be suicidal since any change he would make you would observe it by 2018-2020 . I don't think it will go bankrupt yet, there is a lot of goodwill toward Amd they just need to release new products on par with the competition.  I do think the apus kinda failed because people moved away from notebooks to tablets and smartphones, though the deal with xbox/ps4 was a plus so maybe they'll  get a deal with Nintendo next year..."
hardware,3dkmr0,Perunsan,2,Fri Jul 17 13:53:50 2015 UTC,"drooling over the nano since the announcement, not available.  Reminds me of when the 970 hit, and amd was supposedly releasing in February."
hardware,3dkmr0,PaulTheMerc,7,Fri Jul 17 18:17:57 2015 UTC,Capitalism and idealism don't go hand in hand.
hardware,3dkmr0,IronMew,3,Fri Jul 17 10:16:33 2015 UTC,"I bought a athlon 860k recently, hopefully AMD pulls through :("
hardware,3dkmr0,1337Gandalf,2,Fri Jul 17 11:42:36 2015 UTC,"I'm buying one soon for a ""console replacement"" HTPC for my brother. It's an awesome little processor for the price. Throw in the very solid mini-ITX FM2+ options that have built in WIFI and Bluetooth for under $100, and a Thermaltake Core V1, and you have yourself the makings for an awesome little computer for the living room."
hardware,3dkmr0,IndigoMoss,3,Fri Jul 17 22:15:34 2015 UTC,"Look on the bright side, if they fail then you got collector rare video cards never to be made again."
hardware,3dkmr0,RiffyDivine2,3,Fri Jul 17 12:01:46 2015 UTC,Just finished installing my Fury X :)
hardware,3dkmr0,Nimelrian,11,Fri Jul 17 12:20:55 2015 UTC,"What if the worst does happen, and AMD goes bankrupt after zen fails? Intel and Nvidia both lost their main competitor. (Nvidia just lost it's only competitor?)  Would there be any chance of Intel taking their isis pro graphics and making cards out of them, instead of just using integrated graphics? This would allow them to enter the GPU market, and therefore prevent Nvidia from dominating it with a monopoly.   On the flip side to that, what if Nvidia were to start making CPU's, to then carrying on competition in the processor market. I do release there is more competition in the CPU market than there is in the GPU market, so this could be a harder, and maybe more unlikely, situation, but is there any chance something like this might happen? (Though, Intel is mostly dominating the market... I feel like we'd be held back from technological breakthroughs if Intel were to get broken up by anti-trust laws...)  I'm not really into theoretical business/marketing, so I have no idea what might happen. Any input some somebody with my insight than me is appreciated."
hardware,3dkmr0,SolarAir,15,Fri Jul 17 02:58:45 2015 UTC,"Far more likely other tech companies like Samsung would buy up the remains of AMD and enter the market. AMD's GPU technology is competitive with Nvidia and someone would certainly absorb it and try to make it profitable. They're further behind in the consumer CPU market, but a company attempting to enter the market (which would be a very appealing market if it was only inhabited by a single firm) would have a much easier time building off of what AMD has than trying to start from nothing."
hardware,3dkmr0,Nerdsturm,2,Fri Jul 17 03:28:58 2015 UTC,A lot of people seem to forget that AMD has decades of research and experience that would be incredible difficult to start from nothing now.
hardware,3dkmr0,nav13eh,3,Fri Jul 17 16:18:59 2015 UTC,"Microsoft, Sony and Nintendo are dependent on AMD existing to supply GPUs for their consoles. I can't see how they would allow AMD to go completely bankrupt or to be purchased by a 3rd party that is unfriendly to their interests."
hardware,3dkmr0,p4r4d0x,3,Fri Jul 17 06:11:30 2015 UTC,"Those contracts are already written this wouldn't effect them, the only way it would is if Sony or Microsoft bought AMD."
hardware,3dkmr0,LiberDeOpp,4,Fri Jul 17 06:50:48 2015 UTC,"When a company goes bankrupt, those contracts don't mean a lot. During a take-over, yes."
hardware,3dkmr0,LongBowNL,3,Fri Jul 17 10:29:21 2015 UTC,"On the flip side to that, what if Nvidia were to start making CPU's, to then carrying on competition in the processor market. I do release there is more competition in the CPU market than there is in the GPU market, so this could be a harder, and maybe more unlikely, situation, but is there any chance something like this might happen?   Nvidia is already making CPUs and has been for the past few years. Look at their Tegra line. They even started showing up in chromebooks. I hope they succeed and give Intel a run for their money."
hardware,3dkmr0,tempose,3,Fri Jul 17 14:29:00 2015 UTC,It'd be interesting if Nvidia could pick up the CPU team/IP and Intel the GPU team/IP.  That'd be the best case scenario IMO. Or maybe Qualcomm or Samsung.   I duno.
hardware,3dkmr0,andromeduck,4,Fri Jul 17 21:18:40 2015 UTC,"What if the worst does happen, and AMD goes bankrupt after zen fails?   At this point, most people are just hoping AMD survives long enough to bring Zen to market. Even if Zen succeeds, AMD is still in the dog house.   Would there be any chance of Intel taking their isis pro graphics and making cards out of them, instead of just using integrated graphics?   I don't think that's a market Intel has any interest in entering. They would have/ could have done this a long time ago if they wanted. For whatever reason, they decided it wasn't for them."
hardware,3dkmr0,sk9592,4,Fri Jul 17 04:18:08 2015 UTC,"I don't think that's a market Intel has any interest in entering. They would have/ could have done this a long time ago if they wanted. For whatever reason, they decided it wasn't for them.     It is (comparatively) a really small market. They would have to pry away huge market share from nVidia for it to make a noticeable difference in their revenue, and getting to that point would cost a ton of money. They are better off continuing to flush billions down the drain hoping they can get their mobile chips to take off, where even a small bite of the market would be worth much more."
hardware,3dkmr0,whywhywhywhywhynot,2,Fri Jul 17 07:29:11 2015 UTC,Intel could chime in on GPUs though. Their Iris 6k show they can pack a lot of power into a small die with very low energy consumption.   I wonder if they ever tried putting that onto a large die with dedicated cooling.
hardware,3dkmr0,HavocInferno,2,Fri Jul 17 06:52:49 2015 UTC,Intel actually did try to enter the dGPU market with the i740 in the late 90s
hardware,3dkmr0,mack0409,2,Fri Jul 17 08:46:45 2015 UTC,Wasn't Larrabee intended to be a standalone graphics card before it evolved into Xeon Phi?
hardware,3dkmr0,king_of_blades,5,Fri Jul 17 13:23:55 2015 UTC,"Intel would not get broken up by anti-trust laws. Intel is the heart of the PC market for desktops, workstations, and a lot of servers. Breaking up Intel isn't the same as breaking up a fast food chain, a diamond cartel, or something similar. It can and will hurt the US and other major US corporations.    At most they'll be forced to open up the x86 license to other American companies, but that's still a stretch because the government would be telling other companies ""Hey, jump into that cage with a lion in it. Oh, hold on while we drench you in the blood of animals and tie steaks around your neck. It's for the greater good.""  Also, if AMD goes bankrupt, they'll most likely get a loan or the debt they currently have will get restructured. Selling AMD would be hard to do because of the cage people will be entering into with a lion, covered in animal blood, with steaks tied around their neck.   If a company does buy AMD, they'll not compete with Intel. They might compete with Nvidia, because AMD's offerings are competitive with Nvidia and breaking into the HPC market would be incredibly profitable even if they only get a few sales a year.   The problem AMD has ATM is that their APU's, which should be their biggest bulk of revenue are not doing well because of the CPU side of it. If they can last another 2 years, they should start rebounding with Zen.   I don't see Nvidia doing well in the CPU business. Not after their history with Tegra. I might be bias against them because I'm an AMD guy, but I don't see them doing well in the desktop or laptop space."
hardware,3dkmr0,kennai,6,Fri Jul 17 04:12:05 2015 UTC,"Intel would not get broken up by anti-trust laws   You're right, but to add on to this, the thing everyone misses is that the government doesn't even give a shit if you have a monopoly as long as you don't abuse it.  Even if AMD goes bankrupt, Nvidia and Intel have a lot of incentive to keep doing exactly what they're doing and not raise prices, slow the pace of progress, etc."
hardware,3dkmr0,Evidence_Of_Absence,2,Fri Jul 17 04:51:48 2015 UTC,"Most of their buyers are other massive corporations anyway, right? Selling directly to the consumer has to be a smaller percentage."
hardware,3dkmr0,naanplussed,20,Fri Jul 17 13:47:59 2015 UTC,"i know everyone is super concerned for AMD as they represent ""competition"" which drives innovation and lowers prices, but exactly how much competition is a dieing rival with almost no R&D budget going to represent? at this point wouldn't it just be better if AMD whent under and a company with much deeper pockets bought them out? just imagine samsung writing the checks for R&D."
hardware,3dkmr0,dudemanguy301,40,Fri Jul 17 04:10:42 2015 UTC,"At least for now AMD is still providing somewhat serious competition for GPUs, my 980Ti would have certainly been ~$200 more without FuryX. CPU wise they don't impact Intel much beyond Atom and i3 but that might become better with Zen and Intels recent delays (that might be wishfull thinking though).  My fear with a buyout is mostly that whoever buys them might decide to focus on less areas and ignore the desktop market and focus on stuff like server CPUs/enterprise and mobile APUs/SOCs/whatever.  But if their numbers keep beeing this bad I guess will soon find out what happens after a buyout."
hardware,3dkmr0,hobbldygoob,9,Fri Jul 17 04:41:48 2015 UTC,"Well Intel delayed cannonlake, and is going to be stuck on 14nm for a while. If AMD is able to release chips with 14nm from Samsung and Global Foundries, they will be on a similar processing node, Meaning that if AMD plays it's cards right, and if they are lucky, I'm sure they can gain back some marketshare and actually have money to spend on R&D.   Seeing as Intel has been focusing on graphics performance for it's Laptop CPU line, 14nm APUs could give AMD some marketshare back. Even a $200 14nm FX-8350 would give Intel at least a little competition.  This, of course depends on AMD playing it's cards right and getting lucky. Hopefully they get that much needed die shrink, and over deliver on Zen."
hardware,3dkmr0,somerandomguy101,9,Fri Jul 17 05:44:41 2015 UTC,"Doubt it. Intel's 14nm is closer to what Samsung's 10nm will be. But yes, they won't be as behind as they are now."
hardware,3dkmr0,random_guy12,5,Fri Jul 17 05:57:01 2015 UTC,Samsung has their own superior 14nm fabs and already spends tons on SoC R&D. They would gain very little if they were to purchase AMD.
hardware,3dkmr0,Sephr,6,Fri Jul 17 06:06:26 2015 UTC,Third party vendors lowered their GTX980 prices to better compete with the Fury X. Every bit of competition can only be good for customers.
hardware,3dkmr0,SpiderFnJerusalem,2,Fri Jul 17 09:51:14 2015 UTC,Samsung has said countless times they aren't going into the GPU market.
hardware,3dkmr0,RiffyDivine2,10,Fri Jul 17 12:05:31 2015 UTC,Time to buy stocks? All-time low at $1.87 now.
hardware,3dkmr0,elk-x,11,Fri Jul 17 11:28:23 2015 UTC,That's what I thought when it fell from 3$ to 2.5$ and then to 2$. Wait until there are at-least a few positive signs from then. It has no where to go but down until then.
hardware,3dkmr0,tempose,4,Fri Jul 17 14:35:44 2015 UTC,"If you got some hardcore faith in them, yeah it would be. I can't believe it's so low a kid off the street could buy a share with only part of his lunch money."
hardware,3dkmr0,RiffyDivine2,4,Fri Jul 17 11:59:53 2015 UTC,"Would it be unwise to buy shares now and hope for a buyout? They would go up if AMD got bought out correct?  Just curious, I'm not too well-versed on stocks."
hardware,3dkmr0,breakingbroken,6,Fri Jul 17 14:11:02 2015 UTC,"That's only if someone decides to buyout the company as a whole. The buyer pays a premium on the current stock price to encourage the shareholders to sell. An equally probable scenario is that said buyer waits for bankruptcy, where they can pick up the company (or parts of it) cheap from the bankruptcy court. It basically ends up a question of how much the buyer is willing to spend. In this second case, you're probably going to lose most of your investment."
hardware,3dkmr0,AlchemicalDuckk,2,Fri Jul 17 15:57:23 2015 UTC,Can I personally wait for bankruptcy and buy stock just as the new buyer would?
hardware,3dkmr0,breakingbroken,2,Fri Jul 17 16:21:02 2015 UTC,"Not super familiar with bankruptcy, but at point the stock is worthless because the company doesn't exist (if its full bankruptcy and not restructuring). A company could come along and buy all the patents/IP they wanted, I guess you could too but it would be expensive."
hardware,3dkmr0,siblbombs,5,Fri Jul 17 17:07:34 2015 UTC,"It's your choice to buy or not, but I don't expect a buy out to happen. Everyone thinks samsung is going to kick in the door and take over but they have expressed no interest in moving into the GPU market like that.   If you want buy a few shares and just sit on them, worst is you are out a few bucks."
hardware,3dkmr0,RiffyDivine2,4,Fri Jul 17 18:22:08 2015 UTC,"Doesn't seem like a bad idea, like you said, if they go bankrupt I lose $10. If somehow Zen is decent and they get their shit together, I make a couple bucks. Not a big deal."
hardware,3dkmr0,breakingbroken,2,Fri Jul 17 18:27:35 2015 UTC,"Pretty much a way to look at it when the cost is so low right now, pretty much just playing the long game."
hardware,3dkmr0,RiffyDivine2,11,Fri Jul 17 18:46:00 2015 UTC,"I think a focus on energy efficient cards would help a lot.  Currently, I own a 7770 which has been rebranded several times.  When it launched the 7770 received fairly positive reviews for its efficiency .   Right now, the 750ti is the only real upgrade path for people looking for energy efficient cards- though a new card from Nvidia is probably in the works.     Unfortunately, the consumer segment is an afterthought.  It's hard to send a market signal to a company like AMD with the purchase of an energy efficient video card when their customers include Microsoft.    Anyway, AMD seems to have neglected this market even as quiet cool cards are essential for many people including Streamers.    Microsoft seems to care less than ever about the PC- a fact the PC industry seems slow to realize."
hardware,3dkmr0,toresimonsen,26,Fri Jul 17 01:57:13 2015 UTC,"Really if you are concerned with energy efficiency, you cant beat an IGPU nowadays. The latest Intel IGPU's take around a single watt to run their IGPU's. One, Watt."
hardware,3dkmr0,Seclorum,5,Fri Jul 17 02:36:24 2015 UTC,Where did you get that number? Even the iGPU in the new Atom chip takes more than 1 watt at full usage.
hardware,3dkmr0,jmac,1 point,Fri Jul 17 13:54:59 2015 UTC,"Anandtech, and a couple other reviews."
hardware,3dkmr0,Seclorum,6,Fri Jul 17 15:36:21 2015 UTC,"and they aren't really shit, they're starting to actually be good. Not like 'oh look at it try' but 'oh, you can play real games, well, on this ( granted older games, but still, who's had that much growth in graphics in 5-ish years?)"
hardware,3dkmr0,marrio91,3,Fri Jul 17 05:18:30 2015 UTC,I know. They are a viable option for many things now. Before onboard video meant that NOTHING would run.
hardware,3dkmr0,Clob,6,Fri Jul 17 13:43:01 2015 UTC,I think the segment of consumers who care about energy efficiency in the discrete GPU market is way smaller than you think. These cards are primarily sold to gamers.
hardware,3dkmr0,Benbulthuis,4,Fri Jul 17 04:34:14 2015 UTC,The Fury Nano is a power efficiency card and its being released next month
hardware,3dkmr0,entropicresonance,2,Fri Jul 17 06:23:58 2015 UTC,"Well, we know nothing about its performance yet. Or have there been reviews/benches by now?   But it seems to be rated at 175W TDP which is 3x as much as the 750Ti. So for someone who seriously considers the 750Ti for its low power requirements, the Nano might not be a good alternative."
hardware,3dkmr0,Mr_s3rius,1 point,Fri Jul 17 10:17:56 2015 UTC,"hopefully it will be competitive with maxwell from a watt/performance standpoint. the 980 uses more than the 750ti but it's also really efficient for it's performance level.  from what we know amd is aiming more at 970/980 levels of performance, but we have no idea if it does until august."
hardware,3dkmr0,logged_n_2_say,1 point,Fri Jul 17 14:21:54 2015 UTC,"It isnt. Entirely different GPU. All HD7000 cards are GCN, HD6000 and 5000 were before GCN."
hardware,3dkmr0,HavocInferno,1 point,Fri Jul 17 06:48:14 2015 UTC,You're right my bad I was thinking of the 5770 and 6770.
hardware,3dkmr0,LiberDeOpp,2,Fri Jul 17 06:54:48 2015 UTC,"For those of you hoping for the FTC to step in and prevent Intel and Nvidia from having effective monopolies should AMD go under, think again.  Nvidia's primary competition in the GPU market is Intel, not AMD, with Intel supplying the graphics processing power to over 60% of the world's computers. On the CPU side, Intel cites Nvidia, Qualcomm, VIA, and several other smaller chip makers as their competition. If anything, the collapse of AMD might force Intel to open up x86 to other companies, but that's about it.  Long story short, because of technicalities that judges don't fully understand, there's not much the US government will be able to do to stop Intel from cleaning up once AMD goes under.  The best case scenario at this point is for Samsung to step up and buy AMD, which has been a long-standing speculative rumor for a few years now."
hardware,3dkmr0,Oafah,3,Fri Jul 17 07:05:12 2015 UTC,"Nvidia's primary competition in the GPU market is Intel, not AMD, with Intel supplying the graphics processing power to over 60% of the world's computers.   I agree. I'm sure we'll never quite see discrete GPUs replaced entirely, but integrated graphics have grown by leaps and bounds. I can easily envision in 10 years that iGPUs will be the norm in all but the highest end enthusiast gaming builds or specialized workloads.    Even today stuff like the Intel Iris isn't exactly spectacular, but its good enough for popular F2P titles like DoTA2, CS, and pretty much any smaller budget indie title. Saw some videos with people turning down some settings and playing Battlefield 4 just fine."
hardware,3dkmr0,vir_papyrus,2,Fri Jul 17 11:16:34 2015 UTC,"Why is AMD trying to fight Intel/Nvidia in the traditional sense? They could be going full Open Source Software and open standards hardware, and provide a reason to buy their product over others. Plus their drivers could be improved by the community."
hardware,3dkmr0,Muirbequ,20,Fri Jul 17 17:50:43 2015 UTC,"Open source is irrelevant to 99% of the buyers  Even as a linux user myself, i rather have nvidia's closed binaries then AMDs open source drivers purely based on performance, and that is before getting into hardware aspects."
hardware,3dkmr0,everyZig,6,Fri Jul 17 06:20:36 2015 UTC,"I fucking wish, we might actually get some decent 3D support in Linux. Ati/AMD drivers for Linux have always been disgustingly awful (and I'm not too fond of their Windows drivers, either).  There are probably massive legal issues against going open source, though, even if they wanted to - which they absolutely don't. No big corporation ever wants to relinquish any sort of control over their stuff.  Open hardware, specifically, is never gonna happen because it would sink them instantly. The reason there's only two manufacturers these days is because of the immense amount of super-expensive R&D required for modern processors, but if CPU blueprints became public anyone could start a fabless business and copy their products, robbing AMD of what little market they still have."
hardware,3dkmr0,IronMew,1 point,Fri Jul 17 10:18:12 2015 UTC,"They've been reporting losses for years now, I'd be shocked if they made a profit."
hardware,3dm83i,MarcellusCrow,21,Fri Jul 17 12:14:09 2015 UTC,"Have room for at least a reference length GPU. They are usually about 10.5"" long I believe.   Since it will be moved around potentially, take a look at the silverstone rvz-01's GPU bracket. I don't know if youll be using some sort of riser for the GPU, but the one in that case has this included bracket with rubber feet that clamps on to your GPU at some point in the middle of the card to restrict movement. I travel a lot with my case, and it's lived through tons of airline trips in checked baggage because of this. Not saying yours has to, because no one should check a monitor, but GPUs are heavy and should be restricted as much as possible.  Also, maybe include a 2 way power cable that can feed the power supply and monitor at the same time. It would be nice :)  Have room for at least one 3.5"" drive, and at least two 2.5"" drives.  SFX-L power supply support would be good as well."
hardware,3dm83i,ImSurroundedByBosons,7,Fri Jul 17 15:56:40 2015 UTC,"Thanks a million, we really appreciate all of the feedback and ideas :)"
hardware,3dm83i,ImSurroundedByBosons,6,Fri Jul 17 16:19:28 2015 UTC,"Hey, NP. I like this idea. If done right, it could make for a very nice LAN rig.  I also realized, you should include some sort of cable management into the stand, and perhaps even include a very short HDMI, DP, and DVI cable with the product."
hardware,3dm83i,hyperiron,5,Fri Jul 17 16:27:34 2015 UTC,"Including short cables is an excellent idea, since the monitor will be quite close to the IO."
hardware,3dm83i,hyperiron,2,Fri Jul 17 16:42:38 2015 UTC,dont know how far you are going but put a usb cable and hdmi input on retractable spools
hardware,3dm83i,TheImmortalLS,1 point,Sat Jul 18 02:23:41 2015 UTC,"I thought of this a while ago and dismissed it, but since it's come up in feedback we may well do it, thanks!"
hardware,3dm83i,Idkidks,2,Sat Jul 18 16:12:52 2015 UTC,I even have a few more ideas also some more inexpensive ways for it to work. Give me a few days to draw something up
hardware,3dm83i,CLOliver,1 point,Sun Jul 19 07:46:44 2015 UTC,"Excellent, thanks for the input bro!"
hardware,3dm83i,Idkidks,2,Sun Jul 19 10:41:52 2015 UTC,dust filters are blessings from god
hardware,3dm83i,DdCno1,12,Sat Jul 18 03:51:54 2015 UTC,"Take my advice with a bit of salt, but why not have the case be on the bottom (as a base) and then a modified stand to hook the monitor onto? I'd be a bit worried about the computer heating up the monitor too much tbh."
hardware,3dm83i,Idkidks,7,Fri Jul 17 15:34:18 2015 UTC,"I think that would compromise the aesthetics, and we will have to do some testing of the thermals. Thanks for the input!"
hardware,3dm83i,DdCno1,1 point,Fri Jul 17 15:39:53 2015 UTC,No problem :)
hardware,3dm83i,Idkidks,3,Fri Jul 17 15:41:56 2015 UTC,"It's definitely something we'll look into and give some thought, cheers!"
hardware,3dm83i,ShortySim101,1 point,Fri Jul 17 16:20:32 2015 UTC,Sounds like the standard computer setup until at least the mid '90s.
hardware,3dm83i,Idkidks,1 point,Fri Jul 17 20:20:32 2015 UTC,"Well I want even born then, so yeah :P"
hardware,3dm83i,sinnerhp,4,Fri Jul 17 20:22:38 2015 UTC,I'm sure you've seen something like this in old movies or TV shows.
hardware,3dm83i,CLOliver,1 point,Fri Jul 17 20:45:45 2015 UTC,Ohhh yeah :D
hardware,3dm83i,DdCno1,1 point,Fri Jul 17 21:30:44 2015 UTC,"This just made me feel old.   I'm 18, that's saying something."
hardware,3dm83i,CLOliver,1 point,Fri Jul 17 22:57:54 2015 UTC,"Well I'm 16, you just must have had older computers than I did :)"
hardware,3dm83i,MichaelDeucalion,7,Fri Jul 17 23:12:08 2015 UTC,"In your renders the monitor being attached to has a completely flat back. A current popular monitor is the Asus VG248QE 24"", which has a curved back. The VESA points are located at the only flat section of the monitor, if you were to add your case to the back of that monitor it would be CRAZY wide and look a bit ridiculous.  How are you planning to counter something like that?"
hardware,3dm83i,CLOliver,2,Fri Jul 17 16:20:27 2015 UTC,"I actually have an Asus monitor myself and I was thinking about this issue. On mine (which is 18.5"") it wouldn't be too much of a problem since the curve isn't that harsh. We could look into something to extend into that gap even if it was just more mesh, thanks!"
hardware,3dm83i,MichaelDeucalion,2,Fri Jul 17 18:37:45 2015 UTC,Perhaps an optional flexible rubber lip could bridge the gap.
hardware,3dm83i,Clob,1 point,Fri Jul 17 21:27:34 2015 UTC,Definitely something we'll consider as it may help reduce vibrations also
hardware,3dm83i,CLOliver,6,Fri Jul 17 21:38:19 2015 UTC,"broski you might want to post this on /r/pcmasterrace, I can guarantee you'd get A LOT of input from there."
hardware,3dm83i,Clob,1 point,Sat Jul 18 01:11:08 2015 UTC,Thanks for the suggestion! We might wait until there are further developments before posting there rather than duplicating this thread
hardware,3dm83i,Surprentis,3,Sat Jul 18 07:40:46 2015 UTC,"I'm just recommending that because,from what I've seen, the guys on that subreddit would be the most likely to buy/be interested in this."
hardware,3dm83i,CLOliver,3,Sat Jul 18 13:50:12 2015 UTC,What ever happened to CrateComputers that did this? Linus did a video review of it on YT. I've not been able to find their site. Did they go out of business?
hardware,3dm83i,Surprentis,5,Fri Jul 17 14:55:33 2015 UTC,"/u/spellstrike pointed them out to us up above, although they sell a full computer, not just a case, and the budget model costs 1.9k so we think our product still has a market. Their website is still up btw!"
hardware,3dm83i,CLOliver,1 point,Fri Jul 17 15:03:31 2015 UTC,Yeah I just found it! I looked at the parts and they are really expensive. The stand is $175... And the other parts look to be missing pieces to make a case (the walls)
hardware,3dm83i,DdCno1,3,Fri Jul 17 15:11:26 2015 UTC,Just make it AFFORDABLE.  Thats what matters.  Great concept!
hardware,3dm83i,CLOliver,1 point,Fri Jul 17 16:29:27 2015 UTC,"Thanks, it means a lot! Yeah this is something we're aiming for, it should be reasonable but i can't offer and accurate quote at this stage."
hardware,3dm83i,DdCno1,2,Fri Jul 17 18:40:42 2015 UTC,Just look at other cases and try to compare prices somehow that way.  Also think about how some monitors are curved and consider a way to add some sort of arms with rubber ends to help stabilize the case on the back if that happens.
hardware,3dm83i,CLOliver,1 point,Fri Jul 17 18:48:04 2015 UTC,"Yeah in theory it should be priced similar to most cases, but that depends on the scale. Stability shouldn't be an issue with VESA mounts but for aesthetics we'll definitely look at some sort of solution!"
hardware,3dm83i,DdCno1,2,Fri Jul 17 18:52:37 2015 UTC,"Definitely interesting. Personally, I need a big tower (easier to work on, more space for stuff), so I'm not necessarily your target audience, but I definitely like the concept.  One possible issue for many people: I do like how my main screen can be adjusted and even rotated:  https://i.imgur.com/qWc53xv.jpg  Your stand appears to be fairly rigid in comparison. Are there any plans for a more flexible design?   Speaking of flexibility, at least two USB 3 ports, headphone and microphone jacks in the stand are a must - nobody wants to reach behind the screen to plug in basic periphery. A card reader would be a nice addition (but probably costly due to licensing fees). I'd also suggest some more advanced cable management ideas: For example, a small hole at the top for the wire of a webcam and perhaps one or more of the case's panels having a hinged door that covers up USB, Ethernet and other ports - while leaving an opening at the bottom - for a clean look.  Another idea: Include a removable board for the mainboard and other major components that can be detached from the rest of the case without any tools. This would make upgrades and maintenance much more user-friendly. Generally, there should be as few screws as possible."
hardware,3dm83i,CLOliver,2,Fri Jul 17 20:43:37 2015 UTC,"Thanks for the detailed response and I glad you like it! The stand is be redesigned as the render was mainly to explain the concept and will include the ability to tilt/adjust height. We are definitely have USB's, audio & microphone jacks along with the power and maybe reset switch on the base, we are also considering a card reader since it has been suggested. Cable management is one advantage of this design that we're looking to capitalize on and I hadn't actually thought of webcams so thanks for that!  In terms of ease of building, we're thinking of having all but the bottom panels being removable for easy fan installation and for working on the PC with just a frame. It's is part of our plan to make extensive use of thumbscrews throughout, do you think this would be satisfactory for replacing regular screws or should we look into other fixtures?"
hardware,3dm83i,XLShadow,2,Fri Jul 17 20:56:08 2015 UTC,"Good to hear.  In my opinion, thumbscrews are a lesser evil, mainly intended to replace regular screws in existing cases. I personally wouldn't make them a cornerstone of a new design. Instead, I'd recommend hinges, rails, mechanical pushbuttons, etc. as much as possible, which is of course vastly more complicated and expensive to design and produce compared to drilling a few holes.   The only two components that need to be screwed on are mainboard and power supply (if it's internal), everything else and certainly every part of the case can be designed without any screws. At the very least have screwless drive bays. That's essential for any decent case these days."
hardware,3dm83i,CLOliver,2,Fri Jul 17 21:16:14 2015 UTC,"Yeah we should be able to sort out screwless drive bays at the least, will look at fans, the side panel and some other parts also."
hardware,3dm83i,juustincase,1 point,Fri Jul 17 21:20:31 2015 UTC,I'm actually interested in hearing from you in the future. Your open and friendly responses to suggestions are quite refreshing in this day and age. Do you have a website?
hardware,3dm83i,CLOliver,2,Fri Jul 17 21:24:47 2015 UTC,"We do, my partner will post it later since Reddit has rules saying only 10% of posts can be self promotion, and I've been a lurker up until now! I can PM it to you for the time being."
hardware,3dm83i,john_dune,3,Fri Jul 17 21:31:32 2015 UTC,My friends and I LAN once a month.  The MSI all-in-one gaming series PC has captured our attention and our dreams but it is far from perfect.  Gaming laptops (and MSI AIOs) are very convenient but desktops typically hold up a lot longer.  A few of us also really enjoy custom builds and custom water cooling.  Both the NCASE M1 and Kimera Industries Nova are on our short list of future chassis.  OP already mentioned a handle but I'd like to stress this.  I really appreciate when a case has obvious handle(s) and the proper reinforcement so it doesn't flex when… um… handled.  BenQ gaming monitors have handles but they aren't above the center of mass so they hang funny.  A single solid handle with the ability to adjust and center its position so I can carry my rig to and fro like a six-pack would be a wet dream.
hardware,3dm83i,CLOliver,1 point,Sat Jul 18 04:24:59 2015 UTC,"A carry handle is one of the most important features to us, and will be present in the next renders. I hope we can solve this problem of yours!"
hardware,3dm83i,lucun,2,Sat Jul 18 07:52:50 2015 UTC,"This looks amazing. If you finalize this, please post it back here. I would absolutely buy one.   Might I suggest putting a card reader in the base near the power button? It would be easier to access there than having to reach around the sides to get at the bays. Nothing crazy, maybe just SD and micro SD. Or perhaps a handful of 3.0 ports down there? One way or another, that base seems like a killer place to throw some tech."
hardware,3dm83i,CLOliver,1 point,Fri Jul 17 18:59:59 2015 UTC,"Thanks for your kind words! We were planning on having some USB's down there but we'll consider a card reader too, we'll try keep you all updated while staying with the rules about self promotion."
hardware,3dm83i,lucun,2,Fri Jul 17 19:06:51 2015 UTC,"My Input:  Your motherboard options would probably be best served by doing mITX.    Graphics wide, at least have room for an 11 inch videocard, that'll cover most of the standard high end cards nowadays.  Standard ATX power supply would be dope, but I'll understand if you can't use something that size.  Hard drives: I'd recommend 2x2.5"" bays, and 1x5.25"" bay with an adapter to fit a 3.5"" hard drive in it.  Since it will be hard to access things to add to your computer, I'd recommend having 4x USB 3.0 ports on the side near the bottom (whichever side works well for you).  One thing that would definitely make this case even more amazing would be including some useful, but short cables, like a 12"" HDMI or displayport cable.  Another user mentioned a passthrough powercable to supply power to the monitor too, which would be awesome."
hardware,3dm83i,CLOliver,1 point,Fri Jul 17 22:28:31 2015 UTC,"At the moment we're looking at a mATX motherboard, standard reference length GPU, ATX PSU, some 2.5"" bays and one 3.5"" bays. We'll definitely have those posts on the base of the stand, and are looking to implement those cables as suggested elsewhere. Thanks for going into so much detail!"
hardware,3dm83i,Geronimo_at,2,Sat Jul 18 07:46:22 2015 UTC,"It looks more like a monitor that you could build a computer in. Honestly, I'd prefer doing a DIY drilling some holes and putting some sort of flexible VISA mount for a monitor on a solid metal case. That way you can choose your own monitor and some of the cases out there rather than locking into what you have chosen. Bulkier yes, but it's got a solid metal case to keep it from mostly looking top heavy... which your early renders look like to me."
hardware,3dm83i,CLOliver,2,Sat Jul 18 00:13:44 2015 UTC,"The monitor does not come with it, it's just to demonstrate. Again, our renders were purely to show our concept, they will be tested both virtually and physically for stability."
hardware,3dm83i,haekuh,5,Sat Jul 18 07:50:38 2015 UTC,"Just a heads up, if you guys do pitch this for investors, people will still see and analyze the render as if it was the final product."
hardware,3dm83i,CLOliver,1 point,Sat Jul 18 16:46:43 2015 UTC,Thanks for the advice! We're looking to run a small crowdfunding campaign to fund a prototype for which we will have final renders and then a campaign to go into production which will have those renders along with photographs of a physical prototype.
hardware,3dm83i,haekuh,2,Sat Jul 18 17:59:31 2015 UTC,"Cool idea, do you have any plans for protecting the screen?"
hardware,3dm83i,RedCurtainGuy,2,Sat Jul 18 19:56:36 2015 UTC,"Someone brought this to our attention at a forum post we made, we're thinking of including some sort of cover or bad since it is targeted at LAN gamers. Since we have two people saying it know it's definitely on our radar!"
hardware,3dm83i,Ofactorial,2,Sat Jul 18 21:36:13 2015 UTC,so some questions. I know the render is just to explain the idea so I have no idea if you guys have already covered this.   you may want to think about having a rotated motherboard design so that the PCI/PCIE slots are facing downwards(towards the base of the stand). As I am sure you already know most monitors have their ports facing downwards parallel to the display panel. If the PCI/PCIE slots were facing any other direction the display cables would need to be routed around the case in order to reach the bottom. Do you guys have anything planned for monitors whose display connectors are perpendicular to the panel(stick straight out the rear of the monitor)? If A monitor with this port orientation were mounted to the case then there would be no way to connect the display cables. I would absolutely LOVE if you guys put a USB hub in the base of the monitor stand just for aesthetics and to avoid running all the cables up to the case.
hardware,3dm83i,CLOliver,1 point,Sat Jul 18 20:15:14 2015 UTC,"We are rotating the motherboard that way and having USB's in the base. As for the display connectors, I did some research regarding that and couldn't find any monitors like that on a quick search. I don't think that is common since it would render VESA wall mounts unusable but I'll have another look, thanks for the reminder!"
hardware,3dm83i,Raise_your_donger,1 point,Sat Jul 18 21:34:11 2015 UTC,I have a monitor like that but now that I think about it I do not think it has VESA mounting spot anyway...
hardware,3dm83i,VagSmoothie,2,Sat Jul 18 22:38:47 2015 UTC,"This seems like a good idea, would save space on my desk for sure! Out of curiosity what type of engineering degree do you have?"
hardware,3dm83i,CLOliver,1 point,Sat Jul 18 23:15:56 2015 UTC,"This will either impress people or turn them off; he has none, nor do I in graphic design and marketing! We're both self taught, and picked up a lot of what we know doing a global technology challenge: http://www.f1inschools.com/"
hardware,3dm83i,VagSmoothie,2,Sat Jul 18 23:40:06 2015 UTC,"What's the max monitor size this thing could hold? Because if it can hold an HDTV, not just a monitor, this thing would kill in the HTPC market."
hardware,3dm83i,CLOliver,1 point,Sun Jul 19 17:47:02 2015 UTC,"I'm still working on that but it'll be a challenge to hold a HDTV as well as a PC. If I can do it it'd be great, I hadn't thought of the HTPC market actually!"
hardware,3dm83i,Spekl,2,Sun Jul 19 18:49:02 2015 UTC,"Am I already late to the party?  What I would like to see would be the ability to mount 2 120mm Rads, 1 for the CPU and 1 for the GPU, since a lot of people use AIO coolers for SFF PC's."
hardware,3dm83i,CLOliver,1 point,Mon Jul 20 09:19:33 2015 UTC,"Never too late! There will most certainly be that possibility, as the case will have four mounting places for 120mm fans."
hardware,3dm83i,MasterBettyFTW,1 point,Mon Jul 20 14:38:33 2015 UTC,Would it 'hook-up' to the back of a monitor via the vesa mounts?
hardware,3dm83i,MasterBettyFTW,3,Fri Jul 17 13:24:03 2015 UTC,"It sure would! I'm the engineer working on this :) A stand similar to that on a monitor will be attached to the case and will replace the one that comes with the monitor, and it will have the power button and some ports on it"
hardware,3dm83i,CLOliver,2,Fri Jul 17 13:32:03 2015 UTC,Can confirm CLOliver is the engineer.
hardware,3dm83i,MasterBettyFTW,1 point,Fri Jul 17 13:34:06 2015 UTC,Good call on having the case have its own stand! What dimensions are you guys looking at?
hardware,3dm83i,monkeybannanna,1 point,Fri Jul 17 13:40:23 2015 UTC,"Thanks! At the moment the actual case (without the stand) is 446L X 271H X 122D (In mm) 17.6"" X 10.7"" X 4.8"" for anyone that uses inches. It takes 120mm fans, ATX PSU, mATX motherboards and should have ample room for beefy GPU's, since we're using a PCI cable to change the orientation of it. A low profile CPU cooler will be needed however."
hardware,3dm83i,CLOliver,2,Fri Jul 17 13:48:06 2015 UTC,"Would it work with closed loop CPU coolers, like the H100 for example? What about 280mm ones, like the NZXT kraken?"
hardware,3dm83i,monkeybannanna,2,Fri Jul 17 14:09:05 2015 UTC,"It certainly should! We'll know for sure when we get a prototype, and we'll make every effort to make sure they do work"
hardware,3dm83i,CLOliver,1 point,Fri Jul 17 14:36:07 2015 UTC,Really old version of this idea still on sale  Chieftec BL-01B
hardware,3dm83i,thinkythought,1 point,Fri Jul 17 14:09:56 2015 UTC,Yeah but...... look at it :P
hardware,3dm83i,CLOliver,3,Fri Jul 17 14:18:57 2015 UTC,"Oh yeah, it's hideous"
hardware,3dm83i,monkeybannanna,1 point,Fri Jul 17 14:22:43 2015 UTC,"We've seen that when we researched to see if anyone had done it, but we don't think it is implemented well, maybe due to when it was designed. Thanks for the heads up though!"
hardware,3dm83i,CLOliver,1 point,Fri Jul 17 14:20:46 2015 UTC,Your renders are much nicer looking
hardware,3dm83i,cp5184,1 point,Fri Jul 17 14:23:18 2015 UTC,I remember seeing a design long ago. The case was an L shape with Vesa mounts for a monitor. That was a big case though.
hardware,3dm83i,CLOliver,1 point,Fri Jul 17 16:39:30 2015 UTC,"Yeah when we were checking for competition we came across it, it's the Chieftec BL-01B you're think of I believe."
hardware,3dm83i,DdCno1,1 point,Fri Jul 17 18:41:59 2015 UTC,"Chieftec BL-01B   Yes, exactly this. I almost purchased it, but my desk was too small for it. What motherboard support will your case have? Will you make a post when it goes up?"
hardware,3dm83i,CLOliver,1 point,Fri Jul 17 18:58:00 2015 UTC,"It will take a mATX motherboard. We'll likely be doing a crowdfunding campaign in order to make a prototype and then one to ship the case but I'm not sure about reddit's rules regarding promoting those kind of things, I'll look into it!"
hardware,3dm83i,fane42,3,Fri Jul 17 19:01:26 2015 UTC,"I really hope you guys make a smaller version. I'd be super interested in an ITX version that takes SFX(or SFX-L) power supplies, maybe even with the GPU laid out flat on a riser like this."
hardware,3dm83i,CLOliver,2,Fri Jul 17 21:22:07 2015 UTC,The GPU will be laid out like that! We considered a smaller power supply but we thought that would decrease our target market as they are not as common. An ITX version may be on the cards in future but it's not planned at the moment.
hardware,3dm83i,HedonismBott,1 point,Fri Jul 17 21:37:39 2015 UTC,"Another note, I think putting fans on the bottom as intake would work. Maybe support for 3 80 mm fans would be nice. Because heat rises, the vents on top and on the sides naturally let the heat escape."
hardware,3dm83i,CLOliver,1 point,Sat Jul 18 01:22:55 2015 UTC,"We're thinking of having intakes low down on the sides, that way the heat will rise and travel diagonally across the case, like what most cases now do. I think that's the most efficient way to do it, thanks though!"
hardware,3dm83i,xtothemess,1 point,Sat Jul 18 07:43:21 2015 UTC,Would the dust reduction be because of the height?
hardware,3dm83i,CLOliver,1 point,Fri Jul 17 18:12:09 2015 UTC,Yeah apparently just having the case a few inches up reduces dust by 80%! We'll be including dust filters too!
hardware,3dm83i,xtothemess,1 point,Fri Jul 17 18:43:07 2015 UTC,I'm curious about how you calculated the number. Testing? Simulation?
hardware,3dm83i,CLOliver,1 point,Fri Jul 17 21:31:41 2015 UTC,"I read it online actually, I'll try find a source for you. Edit: Can't seem to find the source, I will get the OP to change that once he's back until I can find the source of the statistic."
hardware,3dm83i,spellstrike,1 point,Fri Jul 17 21:36:14 2015 UTC,I understand those are very early renders but the stand should be extended backwards to compensate for the weight of the parts.
hardware,3dm83i,spellstrike,1 point,Sat Jul 18 00:00:47 2015 UTC,Yeah we're redesigning the stand as we speak!
hardware,3dm83i,CLOliver,1 point,Sat Jul 18 07:47:31 2015 UTC,How does it achieve taking in 80% less dust?
hardware,3dlmm5,DGXTech,28,Fri Jul 17 07:10:34 2015 UTC,"I might be wrong but all I see here is   ""Case labs is technically right, but since they didn't patent any of their designs we threatened to sue""  Can someone weigh in on this?"
hardware,3dlmm5,haekuh,35,Fri Jul 17 13:14:36 2015 UTC,"Nothing CaseLabs creates is unique enough to patent.  EDIT: You go CaseLabs for build quality, not for innovation."
hardware,3dlmm5,ryno9o,4,Fri Jul 17 13:42:46 2015 UTC,I can understand that completely. What I meant though is even if their designs are unique but not unique enough to patent TT is still copying them? Basically TT is still copying caselabs but since nothing caselabs makes is incredibly unique it can't be patented?
hardware,3dlmm5,haekuh,7,Fri Jul 17 21:17:02 2015 UTC,But that's kind of the point of patents and trademarks. It's a system where you draw a line and say if you are inventive enough then we say people can't copy you. Otherwise it's just too generic to be patented. Without lines like that someone could monopolise a box and say that everyone who wants to use the box shape must pay that person.
hardware,3dlmm5,lucifeil,3,Sat Jul 18 02:25:12 2015 UTC,"yea i understand that, but I am saying that caselabs is saying in moral terms TT is copying their shit. In legal terms TT did nothing wrong."
hardware,3dlmm5,haekuh,3,Sat Jul 18 02:29:32 2015 UTC,"On of my law professors: ""The law and ethics rarely line up"" Also if you look through the USPTO website ThermalTake has patents on front panels for cases and other stuff, Case Labs has 0 patents."
hardware,3dlmm5,xtothemess,2,Sat Jul 18 17:09:40 2015 UTC,Caselabs copied Lian Li anyway and the expensive Lian Li cases are built just as well.
hardware,3dlmm5,TRD099,1 point,Sun Jul 19 13:20:08 2015 UTC,"I love Lian-Li, they make really nice quality cases, really damn nice cases."
hardware,3dlmm5,xtothemess,8,Mon Jul 20 04:15:05 2015 UTC,Tell that to Apple.
hardware,3dlmm5,The-Hunting-Party,11,Fri Jul 17 17:29:55 2015 UTC,The joke is that Apple doesn't create anything new but patents it and sues people anyway
hardware,3dlmm5,Silas13013,1 point,Fri Jul 17 17:57:04 2015 UTC,"Ah, I remember when the samsung something (iirc their first) came out. People criticized it for being ""too similar"" to the iPhone, but fast forward a few years, and no one gives a shit in the mobile phone market because they all look the same."
hardware,3dlmm5,TheImmortalLS,1 point,Fri Jul 17 17:59:46 2015 UTC,Wasn't Samsung eventually sued for trying to make the Galaxy look too much like the iPhone?
hardware,3dlmm5,pb7280,0,Sat Jul 18 03:27:10 2015 UTC,"Maybe, I don't know. I remember not caring because it was something frivolous."
hardware,3dlmm5,TheImmortalLS,8,Mon Jul 20 09:09:30 2015 UTC,"Lawyers laying the beat down, CL without patents has little ground to stand on other than consumer boycotts."
hardware,3dlmm5,noobas4urus,11,Tue Jul 21 05:36:24 2015 UTC,Ahh yes I remember the big sink they raised.  Looks like case labs got put in their place by thermaltake's lawyers.
hardware,3dlmm5,attomsk,4,Fri Jul 17 13:33:31 2015 UTC,Why is caselabs even targeting them? Thermaltake and Caselabs don't target the same demographic.
hardware,3dlmm5,hdshatter,7,Fri Jul 17 07:50:05 2015 UTC,"Yeah, this would be like if DW got mad at some shitty drum company because they make drums that are the same color as theirs."
hardware,3dlmm5,BrockYXE,2,Fri Jul 17 18:04:18 2015 UTC,"DW.. Whoa, just had flashbacks to when I was a kid watching Arthur."
hardware,3dlmm5,hmmorly,4,Fri Jul 17 18:08:58 2015 UTC,It is like Rolex targeting someone who makes watches that look like a rolex...
hardware,3dlmm5,XorFish,3,Sat Jul 18 00:40:00 2015 UTC,This is blatently legally enforced by thermaltake's legal department
hardware,3dlmm5,natalo77,8,Sat Jul 18 10:22:14 2015 UTC,"Sure, ""business is business"" I guess, you see the same sort of copying going on with phones all the time.  But if TT is stooping as low as to rip stuff off, then I think CL should be able to talk shit on it freely.  And hey, being copied by a Chinese knock-off company is typically a sign your product is pretty good."
hardware,3dlmm5,Helrich,21,Fri Jul 17 23:12:28 2015 UTC,"Saying it's plagiarised is one thing, but claiming there's a patent infringement when you don't actually have one is another thing entirely. Defamation laws exist."
hardware,3dlmm5,YennoX,3,Fri Jul 17 10:02:09 2015 UTC,"their   Why is it stooping low? If they as a company can provide a more affordable alternative that seems profitable to them then why not? The problem is what you think CL should be able to do is not legal, whereas what TT did is legal.  Take the TT f51 which is a knockoff of the FD r5, it could be argued that they actually improved upon the design, and obviously this is subjective but the fact remains that some people prefer the TT design for price and design. Personally I'd say this is good for competition, and good for the buyer."
hardware,3dlmm5,Hidden__Troll,0,Fri Jul 17 16:13:33 2015 UTC,"You quoted a single word from my post, which, ironically, doesn't even appear in my post.  I'm well aware that competition is good, and you're right, you can get a chinkphone that looks like the real thing and is as good as the real thing, but at the end of the day, it's still a chinkphone."
hardware,3dlmm5,Helrich,1 point,Sat Jul 18 16:34:20 2015 UTC,This whole drama has been great advertisement for both Caselabs and Thermaltake.
hardware,3dlmm5,Theodoros9,1 point,Sat Jul 18 17:26:59 2015 UTC,Publicity stunt? I didn't even know CaseLabs existed.
hardware,3dlmm5,behrangsa,0,Sat Jul 18 11:01:06 2015 UTC,Caselab's obvious use of lawyers in the writing of this letter is Caselab's obvious use of lawyers in the writing of this letter.
hardware,3dl0gq,TaintedSquirrel,10,Fri Jul 17 03:09:37 2015 UTC,Well what about 380x full Tonga?
hardware,3dl0gq,ominouschaos,6,Fri Jul 17 04:45:19 2015 UTC,Forget full Tonga. Where's full Bonaire and Pitcairn?
hardware,3dl0gq,Exist50,1 point,Fri Jul 17 15:02:08 2015 UTC,Pitcairn is GCN 1.0 and Bonaire is GCN 1.1  Tonga is GCN 1.2.
hardware,3dl0gq,ominouschaos,1 point,Sat Jul 18 02:14:57 2015 UTC,I know. The 300 series has neither full Pitcairn nor full Bonaire
hardware,3dl0gq,Exist50,7,Sat Jul 18 02:18:12 2015 UTC,"Knowing AMD, they'll probably release it in 6 months. Remember the R9 285?"
hardware,3dl0gq,Anaron,3,Fri Jul 17 05:51:28 2015 UTC,They are all going to the 5K imac
hardware,3dl0gq,ZgMc,1 point,Fri Jul 17 23:23:14 2015 UTC,"from the looks of it, it's about the same as a 285 :\"
hardware,3dl0gq,ominouschaos,1 point,Sat Jul 18 06:13:15 2015 UTC,Dell also has a PC with full Tonga.
hardware,3dl0gq,Exist50,1 point,Sun Jul 19 18:45:27 2015 UTC,Which model?
hardware,3dl0gq,thinkythought,1 point,Mon Jul 20 01:31:47 2015 UTC,Alienware 15 laptop.
hardware,3dl0gq,Exist50,23,Mon Jul 20 01:35:31 2015 UTC,Of all the new cards Nano could be the real earner for AMD if it is competitive with the 970 and priced accordingly.  That tiny size is just amazing to me.
hardware,3dl0gq,valaranin,25,Fri Jul 17 03:55:15 2015 UTC,No way will they launch it at 970 pricing. It's full Fiji for crying out loud. I bet they're aiming to about equal or even beat the 390x at a price roughly around that of the Fury.
hardware,3dl0gq,Exist50,9,Fri Jul 17 04:11:16 2015 UTC,"It might be full Fiji, idk why they would cut down fury if they didn't have to. Looks like until they get caught up on production fury and fury x will be limited."
hardware,3dl0gq,LiberDeOpp,2,Fri Jul 17 04:26:03 2015 UTC,Well they had to have some cut down product.
hardware,3dl0gq,Exist50,2,Fri Jul 17 04:27:30 2015 UTC,"Yeah, the regular Fury is cut down."
hardware,3dl0gq,Slyons89,-7,Fri Jul 17 21:20:01 2015 UTC,Yeah and Nano will probably be the same. AMD is having a hard time keeping Fiji cool since now you have the heat of everything compressed into that little die area.
hardware,3dl0gq,LiberDeOpp,6,Fri Jul 17 04:31:09 2015 UTC,"Well the Nano will be downclocked, which has to help. While the package is small, the die itself is huge."
hardware,3dl0gq,Exist50,4,Fri Jul 17 04:34:51 2015 UTC,"It has to be downclocked, because it's TDP is 175 as opposed to the fury and fury x that both had 275.   So if it's core count isn't dropped then they had to drop the frequency to get it under.   Makes me wonder just how well a Nano can be pushed with the dinky ass cooler removed from the equation."
hardware,3dl0gq,Seclorum,4,Fri Jul 17 04:37:22 2015 UTC,"It'll probably be limited by the 8 pin power, and possibly weaker power delivery/vrms as well."
hardware,3dl0gq,Exist50,5,Fri Jul 17 04:39:45 2015 UTC,Custom designs ahoi!
hardware,3dl0gq,HavocInferno,5,Fri Jul 17 06:48:12 2015 UTC,Will AMD even allow that?
hardware,3dl0gq,Exist50,3,Fri Jul 17 06:55:56 2015 UTC,"Well considering the fury and fury x really aren't drawing two 2pins of power anyway... maybe with proper voltage control sorted, but yeah."
hardware,3dl0gq,Seclorum,-2,Fri Jul 17 04:43:13 2015 UTC,The overall area has shrunk meaning they have less area to dissipate the heat.
hardware,3dl0gq,LiberDeOpp,6,Fri Jul 17 04:42:19 2015 UTC,"But the GPU generates by far the most heat. Since it has relatively low heat density, I see no reason why the Nano's cooler can't handle 175w. If it's a premium product, I think there's a good chance of there being a vapor chamber in there."
hardware,3dl0gq,Exist50,1 point,Fri Jul 17 04:54:19 2015 UTC,I think it should've been the other way around but that would've eaten into Fury X sales.
hardware,3dl0gq,Anaron,1 point,Fri Jul 17 05:52:02 2015 UTC,Yeah but can you imagine the people lined up for a custom pcb fury x? I really don't understand the business side of AMD they just don't get it. Like would the 390 and 390x be cheaper if they had 4gb of ram? They would dominate price/performance ratio.
hardware,3dl0gq,LiberDeOpp,1 point,Fri Jul 17 06:07:09 2015 UTC,"I agree. If the Fury had just been a full Fiji implementation with board partners able to put out custom PCB and cooler designs, it would sell like crazy.   If anything, the Nano should be the cut down part so what is left doesn't have to get clocked so much lower to maintain cooling in such a small space.   And while the 390 and 390x would probably be cheaper with a couple less GDDR5 chips, they would have even less to differentiate them from the 200 series then.   It really goes to show had crazy the whole 300 series is, for trying to keep up the pretense of them releasing a bunch of new products, by just bumping existing products down the scale of value and then only coming out with new products in a separate line entirely."
hardware,3dl0gq,Seclorum,9,Fri Jul 17 19:12:09 2015 UTC,"yup, this is going to be a very ""premium"" product and not a price:performance champion. If you want this kinda power, with low wattage in an incredibly small package, you're going to pay extra for it."
hardware,3dl0gq,niioan,5,Fri Jul 17 05:43:49 2015 UTC,"Honestly I don't know why anyone cares when it is ""launching"". The Fury X and Fury have launched but they have had so little stock of them at retailers.  I imagine the nano won't be much different."
hardware,3dl0gq,melgibson666,1 point,Fri Jul 17 07:27:29 2015 UTC,"Same happens for Nvidia GPUs. The 980 Ti was ""sold out"" for quite some time after its launch-- some people got them but large numbers of people had to wait"
hardware,3dl0gq,Unique_username1,3,Sat Jul 18 13:01:13 2015 UTC,Bah. Just want price info!
hardware,3dl0gq,maskey,-8,Fri Jul 17 14:24:37 2015 UTC,"Too little, too late.   Pun definitely intended."
hardware,3dl3ld,rockycrab,24,Fri Jul 17 03:38:25 2015 UTC,"Would make a lot of sense. With the massive delay of broadwell in general, as well as general lackluster broadwell performance in the mainstream sector, I cant really see the Xeon versions of them doing that well.   That and looking at that roadmap image and the stuff that's slated for Xeon Skylake chips, like Six Channel DDR 4... Wow. Also 28 cores with HT... might make them more attractive to server operators ."
hardware,3dl3ld,Seclorum,3,Fri Jul 17 03:50:02 2015 UTC,6 channel DDR4? I thought that was Knights Landing.
hardware,3dl3ld,Exist50,5,Fri Jul 17 04:38:36 2015 UTC,http://www.kitguru.net/wp-content/uploads/2015/07/intel_xeon_skylake_purley_2.png  Right there on the right side under skylake.
hardware,3dl3ld,Seclorum,2,Fri Jul 17 04:44:19 2015 UTC,Thanks. Looks like Intel's pushing for that on multiple fronts.
hardware,3dl3ld,Exist50,4,Fri Jul 17 04:52:08 2015 UTC,Or they just changed their mind again considering all the changes for post-skylake that have already been announced or leaked.
hardware,3dl3ld,Seclorum,1 point,Fri Jul 17 05:03:03 2015 UTC,What do you mean changed their mind? I don't think we've had leaks on this before.
hardware,3dl3ld,Exist50,2,Fri Jul 17 05:11:50 2015 UTC,"The whole Kabby Lake derailing Cannonlake, the announcement that there will be a Skylake refresh as well thrown in before Cannonlake, making at least 3 releases of chips scheduled @ 14nm after Broadwell before we can get down to 10nm."
hardware,3dl3ld,Seclorum,2,Fri Jul 17 05:22:39 2015 UTC,"Oh, I thought you were referring to 6 channel DDR4. Knights Landing actually seems to be on schedule."
hardware,3dl3ld,Exist50,2,Fri Jul 17 05:25:21 2015 UTC,"architecture wise, yes, but it not being produced in high quantity because of the massive die leading to poor yields. It will ramp up pretty soon though. Intel is just starting to get ~200mm2 dies on 14nm, Knights landing is 600."
hardware,3dl3ld,dylan522p,2,Fri Jul 17 18:26:31 2015 UTC,"Yields actually seem pretty good. Intel confirmed there will be a product with a fully enabled die, unlike current Xeon Phi products. Also, shipping times are unchanged."
hardware,3dl3ld,Exist50,2,Fri Jul 17 18:31:08 2015 UTC,"You're wrong. It's Broadwell, Skylake, Kabylake, then 10nm with the tock that was supposed to come after cannonlake. 10nm was delayed, but only one generation."
hardware,3dl3ld,dylan522p,1 point,Fri Jul 17 18:25:11 2015 UTC,So the Skylake refresh morphed into Kabbylake then?
hardware,3dl3ld,Seclorum,3,Fri Jul 17 19:04:26 2015 UTC,"Yes, usually ticks are like a minor tweak or arch and die shrink. With kaby Lake it will be that tweak and a Devils Canyon type optimization for clocks on the desktop side."
hardware,3dl3ld,dylan522p,0,Fri Jul 17 19:33:25 2015 UTC,"Broadwell is just fine performance wise, the crippling by marketing department is what is holding it back."
hardware,3dl3ld,narwi,2,Fri Jul 17 13:30:54 2015 UTC,Its a chip line that is desperately in need of marketing.   Except you have Skylake coming out to steal it's thunder.
hardware,3dl3ld,Seclorum,1 point,Fri Jul 17 15:40:28 2015 UTC,Skylake / socket 1151 seems to only have marginal improvements in chipset capabilities over broadwell / 1150 ... its hard to see how they will offer more in server chips where it is even more stagnated as a rule.
hardware,3dl3ld,narwi,3,Fri Jul 17 16:09:45 2015 UTC,Most of Skylake is the board connectivity from what we know of prerelease.   The chip speeds are also much closer to Haswell which makes it more attractive than broadwell which was slower except in IGPU performance.
hardware,3dl3ld,Seclorum,1 point,Fri Jul 17 17:07:04 2015 UTC,Broadwell clocks on the two desktop chips released being low is not a technical issue but entirely marketing.
hardware,3dl3ld,narwi,2,Fri Jul 17 18:36:59 2015 UTC,It's a marketed TDP issue. Those chips were limited to like 65w of TDP as opposed to 90-165 of other desktop chips.
hardware,3dl3ld,Seclorum,7,Fri Jul 17 19:03:04 2015 UTC,"This article is wrong: ""However, the problems of “Broadwell” may be far worse as Intel is reportedly mulling to cancel introduction of Xeon chips based on the micro-architecture.""  This has nothing to do with problems, but simply not releasing a Xeon, which will be superseded by a new Xeon (skywell) in 3 months anyway.   I would guess almost nobody really expected a broadwell based xeon anyway."
hardware,3dl3ld,rndnum123,3,Fri Jul 17 10:59:13 2015 UTC,"Broadwell xeons would be due out pretty soon, but becasue big dies are still low yields, by the time they expect yeilds to be good enough, skylake xeons would be due. No point in pushing back skylake xeons when the team that makes them will have met their tape out date regardless of yields on the process node."
hardware,3dl3ld,dylan522p,5,Fri Jul 17 18:28:36 2015 UTC,"Hope this is true.  Work has a whole stable of 1s Haswell/Ivy/Sandy and 2S Westmere/Harpertown workstations. Some of them are getting pretty long in the tooth, but there's been no value-for-money replacement that justifies the upgrade - even for the elderly 2S 8C Harpertowns. The workloads we run scale nicely for both thread-level and data-level parallelism.  1S 6-8C Skylake Xeon is likely to be the first chip fast enough to justify replacing them - these machines have a long life cycle, so it's worth waiting an extra year for the benefits of DDR4, AVX512f etc., we're talking about $5000 machines that get replaced every 5-8 years."
hardware,3dl3ld,wulfhound,1 point,Fri Jul 17 09:43:51 2015 UTC,Might as well wait for both Skylake-E/EP/EX and Zen Opterons. Hardly seems like you're in a hurry.
hardware,3dl3ld,Exist50,0,Fri Jul 17 19:18:56 2015 UTC,Both the advantages you mentioned are not going to effect you at all. I highly doubt the software you run will utilize AVX512 instructions. DDR4 is really just lower power with slightly higher frequencies.
hardware,3dl3ld,dylan522p,1 point,Fri Jul 17 18:30:07 2015 UTC,Ddr3 started off likethat compared to ddr2.  Look at where ddr3 is now. Same will happen with ddr4
hardware,3dl3ld,JarJarBanksy,2,Sun Jul 19 20:10:55 2015 UTC,Its a combination of the ram and the memory controller though. Skylake e isn't going to be able to utlaize the really good ddr4 that will come out in a few years nor would you upgrade to it
hardware,3dl3ld,dylan522p,1 point,Sun Jul 19 21:18:15 2015 UTC,Fair point. It isn't designed for that which does not exist.
hardware,3dl3ld,JarJarBanksy,3,Sun Jul 19 23:11:43 2015 UTC,There have been similar rumours of Broadwell-E being cancelled in favor of Skylake-E. Seems quite possible.
hardware,3dl3ld,bphase,3,Fri Jul 17 09:08:29 2015 UTC,"Although it's self serving, I certainly hope this is the case. I'm hoping for a full PC rebuild sometime in 2016 or early 2017 and this might be the first time I step into an extreme series (if I was building today, I would definitely be getting a 5820k)"
hardware,3dl3ld,an_angry_Moose,1 point,Sat Jul 18 02:51:27 2015 UTC,I would think this confirms it as Haswell-E and Haswell-EP (at least 6 + 8 cores) are effectively the same except for frequency and ECC memory support.
hardware,3dl3ld,EERsFan4Life,1 point,Fri Jul 17 11:20:27 2015 UTC,"I'm waiting for the Purley platform for my next upgrade, so this is good news."
hardware,3dl3ld,PologizeForThat,1 point,Fri Jul 17 05:55:33 2015 UTC,This delay is only for higher-end Xeons or also to E3 Xeons of 1151 socket that are equal to i5~i7?
hardware,3dl3ld,sinholueiro,0,Fri Jul 17 16:45:27 2015 UTC,it's not true.
hardware,3dhz6m,Andrej_ID,73,Thu Jul 16 12:44:12 2015 UTC,So the download is still Late July right? This is just if you want a physical copy right?
hardware,3dhz6m,veyron3003,26,Thu Jul 16 14:12:24 2015 UTC,Yes
hardware,3dhz6m,TriGeo,3,Thu Jul 16 14:29:37 2015 UTC,You can get the .esd file now (build 10240) and decrypt it to .iso.
hardware,3dhz6m,iktnl,5,Fri Jul 17 00:06:24 2015 UTC,"I haven't updated in ages (still on build 10130), let me check:   Updating: TH1 Professional 10240   giggling intensifies"
hardware,3dhz6m,xxfay6,2,Fri Jul 17 00:18:12 2015 UTC,Can you update and apply the build 10240 right now to regular 7/8?
hardware,3dhz6m,lynxz,1 point,Fri Jul 17 00:40:00 2015 UTC,Yes. You must perform an upgrade in order for your key to activate windows but while performing the update you have the option to not keep files/apps which does the same thing as performing a clean install.
hardware,3dhz6m,Salander27,3,Fri Jul 17 01:02:38 2015 UTC,So we do not have to wait until the 29th to get full fledged Win 10?  Where do we get the files to do this? I would like to pop it on my tablet.
hardware,3dhz6m,lynxz,3,Fri Jul 17 01:17:18 2015 UTC,"You will need to find a created Windows 10 build 10240 ISO on the internet somewhere, likely from a torrent site or MEGA upload. You can then create a bootable USB drive with Rufus and use that to upgrade your tablet. Make sure you initiate the upgrade by opening the USB drive in My Computer and not by booting from it."
hardware,3dhz6m,Salander27,2,Fri Jul 17 01:26:48 2015 UTC,I went ahead and did some googling on the issue. I'm actually creating the Windows 10 x86 usb drive right now for my tablet. I pulled my Windows 8.1 product key and activation key off it already in case I run into any issues.  edit: good thing I pulled my keys off the 8.1 version.. it's not working properly and I'm calling Microsoft now..
hardware,3dhz6m,lynxz,53,Fri Jul 17 01:39:19 2015 UTC,Why is MS still shipping 32 bit versions of Windows? Is there no way to drag existing 32 bit systems and force them to upgrade?
hardware,3dhz6m,mmencius,39,Thu Jul 16 15:34:42 2015 UTC,"The more systems you can support, the larger than audience you have that are now potential customers... Its sort of a no-brainer to support them."
hardware,3dhz6m,thepoener,9,Thu Jul 16 15:53:53 2015 UTC,OK is there no way to cleanly upgrade 32 bit to 64 bit?  Also as time goes the 4GB RAM limit is going to become a serious issue. Even accessing PAE means each application can only use 2 GB memory each which will also become a serious issue too right? How is continued 32 bit OS feasible?
hardware,3dhz6m,mmencius,19,Thu Jul 16 15:57:38 2015 UTC,32bit OS is to support older machines. Some users might already have an existing 32-bit configuration and just wish to roll an upgrade over their system without wiping their data rather than backing up their data on an external device / cloud solution and just doing a clean install.
hardware,3dhz6m,thepoener,10,Thu Jul 16 15:59:22 2015 UTC,"I know that 32 bit systems must be upgraded to 32 bit systems. My stupid question is why can't 32 bit OSes be upgraded to 64 bit without backing up and replacing files (except for applications, I guess those must be replaced)"
hardware,3dhz6m,mmencius,17,Thu Jul 16 16:09:22 2015 UTC,It's not a stupid question. 32bit OSes can be upgraded to a 64bit OS without backing up and replacing files. OS X and iOS have done it. Microsoft chooses not to.
hardware,3dhz6m,Stingray88,4,Thu Jul 16 17:20:03 2015 UTC,Fwiw you lose 16 bit applications if you do that unless you run them in some emulation layer.
hardware,3dhz6m,ItzWarty,9,Thu Jul 16 23:01:48 2015 UTC,"If you're still using 16 bit applications in 2015, you're company needs to get off their asses and pay for an upgrade"
hardware,3dhz6m,reddit_reaper,3,Thu Jul 16 23:27:23 2015 UTC,Or go virtual...
hardware,3dhz6m,Klathmon,2,Fri Jul 17 00:48:37 2015 UTC,My dad does. I believe the application he uses is from 2000...
hardware,3dhz6m,Yearlaren,1 point,Fri Jul 17 02:05:47 2015 UTC,Virtual machine and upgrade! Lol
hardware,3dhz6m,reddit_reaper,7,Fri Jul 17 02:20:52 2015 UTC,Which in 2015 is not a big deal.  Companies still running 16bit software likely have a lot more issues than just that.
hardware,3dhz6m,Stingray88,2,Thu Jul 16 23:59:09 2015 UTC,"Is MS just being annoying here? I remember becoming incredibly frustrated last year when I wanted to install 64 bit Windows 8 on my newly built PC. The only Windows environment I had lying around to create the installation drive using MS' method for doing this was an old 32 bit Win 7 laptop, so that couldn't work. Had to either download a third party ISO burner (which didn't work) or just borrow someone else's 64 bit laptop."
hardware,3dhz6m,mmencius,6,Thu Jul 16 19:32:35 2015 UTC,You can always grab windows installation ISOs online for every windows version. You just need your own key. I'm not talking about an ISO burner.
hardware,3dhz6m,Nixflyn,1 point,Thu Jul 16 22:33:49 2015 UTC,"Well you need an ISO burner to make a USB installation drive out of the downloaded ISO file, right? I tried to do that using a recommended utility and it didn't work. It just got error at around 40% each time. Followed the formatting instructions and everything."
hardware,3dhz6m,mmencius,1 point,Fri Jul 17 06:32:53 2015 UTC,No you don't. You can literally just copy and paste the files from the ISO into a USB stick. Installed 8.1 pro just fine doing that.
hardware,3dhz6m,dan4334,1 point,Fri Jul 17 06:52:05 2015 UTC,"ISO burner? No, you need a utility to format a USB drive as a bootable device, if that's what you mean.  Use Rufus, it's way better than the official Microsoft one."
hardware,3dhz6m,Nixflyn,-7,Fri Jul 17 06:53:44 2015 UTC,"In my opinion, yes. Windows has always been absurdly annoying in my opinion with the dozens of versions it has, and the lack of ability to upgrade between them effectively.  edit: Haha oh /r/hardware... the downvotes for not being pleased with Microsoft... they make me laugh"
hardware,3dhz6m,Stingray88,0,Thu Jul 16 19:37:58 2015 UTC,"Video of a guy upgrading from Windows 1.0 to Windows 8.0.  Upgrading has never been much of an issue unless you were trying to upgrade from a server edition to home edition (because they were two totally different products).  I've known far more people running into problems with mac hardware because their laptops that work just fine cannot upgrade to a newer OS (and not use any new software) because of the hardware changes they made.  There also were not dozens of editions.  Vista, 7 and 8 all had 3 main publicly available  editions; home, professional and enterprise. Even XP had home and professional , and 2000 had pro, server and adv server.  The only time there is any difficulty is not knowing if your hardware is 32bit or 64bit which can be found in a matter of seconds if you know where to look."
hardware,3dhz6m,urbn,0,Fri Jul 17 01:06:29 2015 UTC,"Video of a guy  upgrading from Windows 1.0 to Windows 8.0. Upgrading has never been much of an issue unless you were trying to upgrade from a server edition to home edition (because they were two totally different products).    I said it was annoying and ineffective. Not impossible.  I've upgraded literally hundreds if not thousands of PCs from XP to Vista, Vista to 7 and 7 to 8/8.1. ""Never been much of an issue"" isn't how I would describe it. Usually it goes OK, but not always. I've had certain upgrades in the past just straight up bone the install for inexplicable reasons, just had to do a clean install. Doesn't happen often, but it's absolutely not ""never"".   I've known far more people running into problems with mac hardware because their laptops that work just fine cannot upgrade to a newer OS (and not use any new software) because of the hardware changes they made.   Not every company works the same as Microsoft supporting all hardware forever. In fact, most companies don't.   There also were not dozens of editions.   No shit. It was an exaggeration. Do I really need to spell that out?   Vista, 7 and 8 all had 3 main publicly available editions; home, professional and enterprise.   Er... no.  Vista had 12 versions ignoring the server, embedded and special market versions.   Starter 32bit & 64bit Home Basic 32bit & 64bit Home Premium 32bit & 64bit Buisness 32bit & 64bit Ultimate 32bit & 64bit Enterprise 32bit & 64bit   7 had 11 versions, the same as Vista however Starter was 32bit only.  8 had 7 versions.   Windows 8 32bit & 64bit Windows 8 Pro 32bit & 64bit Windows 8 Enterprise 32bit & 64bit Windows RT    Even XP had home and professional   Those were just the first two versions. They didn't stop there. They also had:   Home Edition ULCPC Starter Edition Media Center Edition 32bit & 64bit of home and professional   Again we can ignore the embedded, server and tablet editions.   The only time there is any difficulty is not knowing if your hardware is 32bit or 64bit which can be found in a matter of seconds if you know where to look.   There is also incompatibilities between different editions of the different versions, it's not just when going from 32bit to 64bit."
hardware,3dhz6m,Stingray88,1 point,Fri Jul 17 02:21:15 2015 UTC,"OS X and iOS have the enormous advantage of running on a set variety of hardware.   Windows runs on hardware that can literally be in millions of different combinations.  Microsoft did choose not to, but they probably had a very good reason."
hardware,3dhz6m,sk9592,8,Fri Jul 17 04:56:57 2015 UTC,Not all CPUs that are capable of running windows 10 otherwise support 64bit OS.
hardware,3dhz6m,wub_wub,3,Thu Jul 16 18:17:11 2015 UTC,"There's too many configurations to test, they could almost never test every shitty proprietary software to see if the upgrade breaks a minor feature of the software."
hardware,3dhz6m,technewsreader,1 point,Thu Jul 16 18:11:55 2015 UTC,Oh. Why is that? Even if you choose to use the 32 bit version of Chrome?
hardware,3dhz6m,mmencius,2,Fri Jul 17 00:35:21 2015 UTC,"There are actually some corporations out there that still use 32-bit operating systems on their equipment.  I have no idea why they would do that, but I know a larger PR firm deploys 32-bit Windows 7 images on new laptops with 8GB of RAM (and pre-installed with 64bit).  That makes no sense to me."
hardware,3dhz6m,Bonowski,2,Fri Jul 17 06:30:16 2015 UTC,My work computer is 64bit thank god.  But the diag computers we deploy are 32bit machines.  There's just too much old crap out there that needs old software.  Lots of it doesn't work on 64bit machines.
hardware,3dhz6m,thedonik,2,Thu Jul 16 19:40:24 2015 UTC,Why not used emulators or virtual machines
hardware,3dhz6m,reddit_reaper,3,Thu Jul 16 22:43:03 2015 UTC,"To be honest... The user base for this stuff is stupid. Things need to be as absolutely simple as possible, especially when you have 4000+ machines out there."
hardware,3dhz6m,thedonik,1 point,Thu Jul 16 23:28:38 2015 UTC,True but still it would probably be cheaper to update the software to 32 or 64 bit by now. We're talking about something that was meant to be dropped back in 2006
hardware,3dhz6m,reddit_reaper,1 point,Thu Jul 16 23:31:07 2015 UTC,"We can't control third party software. If that brake system on a 2004 vehicle uses xyz software from 2002 for diagnostics, that's what we have to work with.  Granted, we can update our in house software all we want. And we do."
hardware,3dhz6m,thedonik,1 point,Thu Jul 16 23:33:28 2015 UTC,Well that sucks. I actually think you can run seemless virtual machines where you have a shortcut for the app in the virtual machine on your host machine and it just runs like a regular application
hardware,3dhz6m,reddit_reaper,1 point,Thu Jul 16 23:41:48 2015 UTC,"we did experiment with it, but when it came to plugging in certain usb devices, and having drivers installed, etc.  we just couldn't get everything to work 100%.  and it needs to all work 100%."
hardware,3dhz6m,thedonik,1 point,Fri Jul 17 00:05:15 2015 UTC,"Ah okay, that makes a little more sense.  It's still weird that it's 2015, and there are applications that are 32-bit dependent, however."
hardware,3dhz6m,Bonowski,7,Fri Jul 17 00:10:51 2015 UTC,"Not really, it would entirely depend on how much money it costs to develop and maintain 32-bit versions. Once it's not profitable they'll stop. That's pretty much it."
hardware,3dhz6m,Benbulthuis,-4,Fri Jul 17 17:34:03 2015 UTC,That's literally what I said. As long as it's a viable and revenue generating user base (with opportunity costs taken in) - it'll be supported.
hardware,3dhz6m,thepoener,10,Thu Jul 16 16:48:37 2015 UTC,"Eh, that's not really what you said in the original comment I replied to, but okay."
hardware,3dhz6m,Benbulthuis,-12,Thu Jul 16 17:18:41 2015 UTC,K
hardware,3dhz6m,thepoener,1 point,Thu Jul 16 17:55:59 2015 UTC,Short-term yes not long-term where we want to see 32bit die.
hardware,3dhz6m,Schmich,10,Thu Jul 16 18:00:35 2015 UTC,"I'm betting its along the lines of corporate support. My company for instance still requires 32 bit unfortunately... We have a lot of computers in all of the offices and due to our software being 32 bit, they haven't certified it for 64 bit usage. Even though the software works fine on 64 bit, our IT guys refuse to upgrade until its been 100% cleared.  I imagine there are a lot of companies in a similar situation. They would just stay on Windows 7 32 bit if they didn't provide 32 bit for Windows 10. I'm sure our company will be on Windows 7 forever, similar to how everyone stayed on XP forever."
hardware,3dhz6m,Stephenishere,1 point,Thu Jul 16 21:13:51 2015 UTC,"Oh god, same here. It's ridiculous when the standard specs for our units now include 8GB memory and its only 32 bit..."
hardware,3dhz6m,CptRedBeard4,4,Thu Jul 16 16:55:29 2015 UTC,Because 32-bit systems still exist and were sold as recently as Windows Vista.  EDIT: According to the link below from Microsoft:  89% of Windows Vista computers were 32-bit 54% of Windows 7 computers were 32-bit   99% of Windows XP systems were 32-bit   Windows 10 is designed to replace XP-7. You cannot upgrade 32>64 bit without a full reinstall. It would be overbearing for many users to go through that process and a non-trivial % of target PC's do not support x64.  http://blogs.windows.com/windows/b/bloggingwindows/archive/2010/07/08/64-bit-momentum-surges-with-windows-7.aspx
hardware,3dhz6m,LUMiNARY_ATL,3,Fri Jul 17 00:05:39 2015 UTC,Also most cheap windows 8 tablets with 1-2gb RAM are 32bit.
hardware,3dhz6m,ScottieNiven,1 point,Thu Jul 16 18:08:35 2015 UTC,So for those machine not using 64bit saves a bit of HDD space? Any reason why they'd want to use the 64bit over 32bit even with only 1-2g?
hardware,3dhz6m,elimi,1 point,Thu Jul 16 22:15:51 2015 UTC,"32bit uses less HDD and less ram, which makes is suitable for machines with <3gb ram.  On the low ram machines, there is no real benefit to using 64bit. (To my knowledge)"
hardware,3dhz6m,ScottieNiven,3,Sun Jul 19 01:02:10 2015 UTC,"Because 32-bit systems still exist and were sold as recently as Windows Vista.   As recently as 8.1*  Although it has a 64 bit CPU, my tablet has 32 bit firmware, and so as far as I can tell, can only boot a 32 bit OS.  It's probably possible to chainload a 64 bit OS from a 32 bit bootloader, though I'm not really sure as I haven't tried, but I do know that it can't boot directly into a 64 bit OS, and installing an extra bootloader is a lot to ask if you're trying to make something user friendly to install."
hardware,3dhz6m,Anon123212321,0,Sun Jul 19 01:15:58 2015 UTC,"Yup, I wasn't aware that some of the really low end ones are 32 still. Even better reason to keep 32 in existence."
hardware,3dhz6m,LUMiNARY_ATL,2,Thu Jul 16 23:02:29 2015 UTC,One of Intel's recently released Atom chips is 32bit only.
hardware,3dhz6m,PlankWithANailIn,1 point,Thu Jul 16 23:31:52 2015 UTC,Which one? I was under the impression that Atom was 64-bit.
hardware,3dhz6m,Exist50,1 point,Thu Jul 16 23:08:30 2015 UTC,"Sorry my bad..turns out its just the UEFI that a lot of these SOC's come with is 32 bit only. An example of this is the recent Intel Compute stick, you can't install 64bit windows but you can install 64bit linux..All because the UEFI is gimped.  So in 2015 it is totally possible to still buy PC's which can only run 32bit windows. Why? I have no idea!"
hardware,3dhz6m,PlankWithANailIn,11,Fri Jul 17 03:02:15 2015 UTC,"I work as a missionary, and I've never seen a lack of computers over seas, even in very poor places. However, most of them are Pentium 4 or lower. This makes me think it's probably not the American/western population that they're concerned about when selling 32 bit."
hardware,3dhz6m,PVonMuter,9,Fri Jul 17 09:10:16 2015 UTC,A pentium 4 wont be running windows 10.
hardware,3dhz6m,sterffff,19,Thu Jul 16 16:49:52 2015 UTC,"Sure can, all you need is a single core crawling at 1ghz and a gb of ram.  Also, your definition of fast and theirs are completely different."
hardware,3dhz6m,PVonMuter,11,Thu Jul 16 17:18:31 2015 UTC,"Well, not every pentium 4. Only the newest version as far as I remember (need SSE2, XD, PAE and earlier version of P4 didn't have XD)."
hardware,3dhz6m,o_x,3,Thu Jul 16 17:33:40 2015 UTC,And today I learned something! Thanks stranger! Happy cake day.
hardware,3dhz6m,PVonMuter,1 point,Thu Jul 16 18:48:32 2015 UTC,And it would have to have one of the later chipsets in order to have PCIe to communicate with a sufficiently modern GPU.
hardware,3dhz6m,wtallis,11,Thu Jul 16 19:23:08 2015 UTC,Nope. I had the windows 10 preview running on an old system with a radeon 9800 pro AGP card. Had to use generic drivers but a pci-e graphics card is certainly not required.
hardware,3dhz6m,Slyons89,2,Thu Jul 16 19:20:35 2015 UTC,"Happy Cakeday.  The ATi Radeon 9800 Pro was a fantastic card.  My first proper card, actually. While mine's long since been retired, I do have it in a box collecting dust somewhere - I never could bring myself to get rid of it. I'm glad to see there's still one getting some love.   I'd drop mine in a server or something, just to say it's there, but there's nothing with AGP support nowadays."
hardware,3dhz6m,Error400BadRequest,2,Thu Jul 16 20:54:04 2015 UTC,"Yes! I originally got the card for Half Life 2, and it was wonderful. I reconstructed my old Athlon 64 3200+ system and stuffed it in a cardboard box to practice with Linux. It's still serving me well over a decade later =)"
hardware,3dhz6m,Slyons89,2,Fri Jul 17 07:25:21 2015 UTC,"Yes, Windows 10 technically does still have basic SVGA/VBE drivers, but there's a lot that doesn't work without hardware acceleration and it really isn't worth bothering with and definitely isn't something to be inflicting on unsuspecting novices."
hardware,3dhz6m,wtallis,1 point,Fri Jul 17 14:15:36 2015 UTC,"I agree with you, but nobody said this was for a novice! I'm just saying a newer chipset with PCIe isn't needed."
hardware,3dhz6m,Slyons89,1 point,Thu Jul 16 20:54:20 2015 UTC,"Not that I've tried it on hardware -that- old, but my experience with Windows 10 Preview on a Notebook from ~2008 is quite noticeably faster than Windows 7 Ultimate SP1.  It boots/wakes from standby and browses through Windows Explorer so fast it's as if it has an SSD ... but it's using a slow 5400RPM drive."
hardware,3dhz6m,Ubel,1 point,Thu Jul 16 21:35:25 2015 UTC,"I have an old pc which has Intel Pentium 4 and it can run windows 8.1, haven't tried windows 10 preview on that.  Windows 10 should work on that."
hardware,3dhz6m,varuntis1993,1 point,Thu Jul 16 22:22:17 2015 UTC,Actually right now my little brother's gaming PC is an old p4...running Windows 10. Minecraft and fallout 3 work decently enough for him
hardware,3dhz6m,kapsklok,1 point,Thu Jul 16 18:43:31 2015 UTC,My guess would be its older computers in the west rather than computers in the developing world that are sustaining 32 bit Windows in larger numbers.
hardware,3dhz6m,mmencius,1 point,Thu Jul 16 20:41:34 2015 UTC,"Windows 10 is cross platform, there are mobile devices shipped today that are 32-bit."
hardware,3dhz6m,alpacIT,1 point,Fri Jul 17 03:17:18 2015 UTC,"It's almost certainly due to enterprise customers who are dependent on 32-bit drivers for niche hardware, and badly-written 32-bit software that finds ways to break when run under 64-bit Windows. Though the latter should have been dealt with through virtualization by now."
hardware,3dhz6m,wtallis,1 point,Thu Jul 16 17:05:29 2015 UTC,Some software requires 32 bit systems to work. Think government programs. Thats a huge customer right there
hardware,3dhz6m,Mczern,1 point,Thu Jul 16 18:09:38 2015 UTC,It seems crazy that any software which isn't super-niche would not be designed to run on 64 bit OSes. Also isn't there virtualization?
hardware,3dhz6m,mmencius,1 point,Thu Jul 16 19:24:43 2015 UTC,There is visualization and we use it a lot but that is mostly for servers or service driven platforms and not for individual workstations.   It does seem crazy and there aren't too many examples of it in my job but the fact I'm on a 32bit machine right now shows they're out there lol.
hardware,3dhz6m,Mczern,1 point,Thu Jul 16 19:52:46 2015 UTC,"they want my 2006 macbook pro to run windows?  10.6 runs like dog shit on it, and barely any software still runs on that old of an os"
hardware,3dhz6m,stealer0517,1 point,Fri Jul 17 06:31:40 2015 UTC,"Hell, are there even any 32-bit only processors out there at this point that can run Windows 10?"
hardware,3dhz6m,Kezika,1 point,Fri Jul 17 13:54:16 2015 UTC,Why is MS still shipping 32 bit versions of Windows?   For Legacy 16-bit application support.  https://en.wikipedia.org/wiki/Windows_on_Windows
hardware,3dhz6m,WhiteZero,1 point,Thu Jul 16 20:51:38 2015 UTC,Most atom tablets are 32 bit...
hardware,3dhz6m,thinkythought,36,Fri Jul 17 04:46:28 2015 UTC,"Nice, now I'm confident I won't need a DVD drive in my next PC."
hardware,3dhz6m,DeeJayDelicious,64,Fri Jul 17 14:48:10 2015 UTC,You know it's been possible to put Windows on USB for ages now. There's been no need for a DVD drive for a long while now unless you need that media due to data caps or slow internet speed.
hardware,3dhz6m,bphase,10,Mon Jul 20 05:50:36 2015 UTC,"I thought that, but when I built my last computer I needed a CD drive to install the network drivers for the motherboard. Without those, i couldn't access the internet to install any other drivers."
hardware,3dhz6m,11_22,33,Thu Jul 16 14:01:17 2015 UTC,Most people have another machine available though. Just download the drivers to a USB stick on that ? My usual goto anyway.
hardware,3dhz6m,MaDpYrO,35,Thu Jul 16 14:06:52 2015 UTC,"Or download them on your phone, and once you're finished downloading, plug your phone into your PC and copy them over from that."
hardware,3dhz6m,BillionBalconies,18,Thu Jul 16 14:45:43 2015 UTC,Get out of here with your common sense.
hardware,3dhz6m,Iliketrainschoo_choo,2,Thu Jul 16 14:57:13 2015 UTC,I did this with my current PC when I got it a few years ago.
hardware,3dhz6m,Yearlaren,5,Thu Jul 16 16:41:06 2015 UTC,Don't think that works for iPhone users though.
hardware,3dhz6m,Fragmentalist,7,Thu Jul 16 17:04:22 2015 UTC,Don't use an iPhone if it limits what you're able to do?
hardware,3dhz6m,leviwhite9,3,Fri Jul 17 02:07:07 2015 UTC,I don't.
hardware,3dhz6m,Fragmentalist,8,Thu Jul 16 17:57:07 2015 UTC,I was just giving general advice.
hardware,3dhz6m,leviwhite9,2,Thu Jul 16 18:07:16 2015 UTC,OP was incorrect. There are plenty of ways to download arbitrary files on the iPhone and move them to a computer.
hardware,3dhz6m,rnawky,1 point,Thu Jul 16 18:13:48 2015 UTC,Sure you can. There are applications for that sort of thing.
hardware,3dhz6m,bfodder,-4,Thu Jul 16 18:21:23 2015 UTC,Doesn't make a whole lot of sense for someone tech savvy enough to build a computer to have an iPhone.
hardware,3dhz6m,letsgoiowa,6,Thu Jul 16 21:30:52 2015 UTC,Been building gaming PCs since I was a teenager (33 now) and I have an iPhone.
hardware,3dhz6m,TabsAZ,0,Fri Jul 17 12:37:47 2015 UTC,"And can you use it for storage?  We plugged my friend's iPhone 6 into my computer and tried everything and it was impossible from what I saw, I had iTunes installed, tried YamiPod (which claimed to support it) and nothing worked.  From what I saw Apple had it so locked down that you literally couldn't store files on your own phone via USB, that to me is beyond ridiculous and one of the MANY reasons I would never buy one of their products.  I can't tell you how many times I've been working on a computer and used my phone to download/transfer drivers that Windows didn't come with by default.   (So many times Windows doesn't have the correct wifi/ethernet drivers so you're forced to get them remotely.)  I know it was possible in the past, especially on iPods, could just plug it in and drag files over, but it hasn't been possible for a long time now AFAIK."
hardware,3dhz6m,Ubel,2,Thu Jul 16 18:09:26 2015 UTC,"And can you use it for storage?   Only if you Jailbreak, in which case you can essentially use the phone like an USB memory stick"
hardware,3dhz6m,Protonion,2,Thu Jul 16 18:23:29 2015 UTC,"Tried iFunBox? Yes, it connects using the same API's iTunes has, but it allows for far more control."
hardware,3dhz6m,xxfay6,2,Thu Jul 16 18:49:21 2015 UTC,"No, but I didn't buy it so that I could transfer files between computers with it. A USB stick is like $10, no need."
hardware,3dhz6m,TabsAZ,1 point,Thu Jul 16 19:02:25 2015 UTC,And can you use it for storage?   There are ways of doing it without jailbreaking.
hardware,3dhz6m,bfodder,6,Fri Jul 17 00:16:12 2015 UTC,"Oh please. While I'm not a fan of Apple's products myself, plenty of tech savvy people, especially in the western markets, own iPhones."
hardware,3dhz6m,Fragmentalist,2,Thu Jul 16 19:29:16 2015 UTC,"Sure, while that's true, it doesn't mean it makes the most sense if they plan to use it with Windows and Linux PC's. That's something they should understand when buying it. Basically, they understand that they're buying a very closed and limited system specifically for what iOS offers, which does NOT include basic features like these.   TL;DR: Clarifying: tech savvy people that buy iPhones buy it for reasons other than making it work with various things on Windows and Linux. They understand that it's not going to do that."
hardware,3dhz6m,letsgoiowa,2,Fri Jul 17 14:34:56 2015 UTC,"I mean, they might be aware and accepting of these limitations and their workarounds. No reason to implicate that they are in any way inferior for using an Apple device or that their device of choice is nonsensical due to their desktop operating system."
hardware,3dhz6m,ItzWarty,0,Thu Jul 16 18:16:21 2015 UTC,I don't see how the two are related.
hardware,3dhz6m,jmac,-5,Thu Jul 16 18:41:25 2015 UTC,"You're right, tech savvy people are much more likely to have phones that phone home to Google all of your personal details and prevent users from limiting what data third party apps have access to.  The iPhone is infinitely more secure than any Android phone."
hardware,3dhz6m,rnawky,1 point,Thu Jul 16 23:13:23 2015 UTC,Or install a cheap DVD drive... oh wait.
hardware,3dhz6m,fishbulbx,6,Thu Jul 16 18:38:18 2015 UTC,But that ruins my beautiful cable management.
hardware,3dhz6m,Nixflyn,8,Thu Jul 16 21:30:15 2015 UTC,"Looking back, I should have done that. Oh well."
hardware,3dhz6m,11_22,2,Thu Jul 16 20:20:15 2015 UTC,"Yeah that's been my fresh install strategy for a while - Windows bootable USB stick, Networking drivers, and a Ninite installer.  Although I suppose the smarter choice still would just be to create a disk image of the freshly installed state :/"
hardware,3dhz6m,radient,1 point,Thu Jul 16 22:36:22 2015 UTC,Always find out what drivers you need for a new motherboard before upgrading. I've made that mistake more times than I care to recall.
hardware,3dhz6m,ewood87,1 point,Thu Jul 16 15:01:45 2015 UTC,"When I upgraded my PC a few years ago I had to ditch my CD/DVD drive because it was an IDE drive and my new motherboard didn't have an IDE port.  I understand if most hardware still come with CDs for the drivers but I don't understand when they don't tell you where to download them. I mean, come on. A lot of people don't have CD/DVD drivers nowadays."
hardware,3dhz6m,Yearlaren,3,Thu Jul 16 16:57:17 2015 UTC,"Haven't got one in my laptop or HTPC and I haven't used the one in my desktop for about 2 years. Dead format*, thank god. I just recently threw out all of my cases and scratched blank CDs.  *unless you watch high quality films and don't have decent Internet speed."
hardware,3dhz6m,DomUK89,2,Thu Jul 16 21:28:51 2015 UTC,I just bought a USB Bluray drive in 2010 and that's all I've needed since then. No optical drive in my desktop or laptops.
hardware,3dhz6m,Stingray88,1 point,Fri Jul 17 02:11:12 2015 UTC,"I got a slim laptop from work the other week and it came with an external DVD-Burner. It was then that I realised that up until then I did not own any device (aside from an old Wii) that could read an optical disc. No CD player, No DVD player, nothing. I then realized how networked devices have made removable media redundant."
hardware,3dhz6m,Supersnazz,14,Thu Jul 16 14:57:41 2015 UTC,"That is so good to hear that it is not being sold on CD's, now I don't have to keep a CD drive around. It's especially useful for compact devices such as laptops."
hardware,3dhz6m,commanderkull,14,Thu Jul 16 17:22:05 2015 UTC,"You haven't needed a CD drive since before Windows 7, there are ISOs you can download from Microsoft and make your own USB"
hardware,3dhz6m,TriGeo,9,Fri Jul 17 01:57:04 2015 UTC,"Good point, but it is easier now that that is done for you."
hardware,3dhz6m,commanderkull,1 point,Thu Jul 16 14:07:32 2015 UTC,Some people don't have 2 computers.
hardware,3dhz6m,Grenne,1 point,Thu Jul 16 14:30:27 2015 UTC,"borrow a friends, use your phone etc  and who doesn't have some old computer lying around or a laptop?"
hardware,3dhz6m,stealer0517,3,Thu Jul 16 14:34:48 2015 UTC,Also libraries typically have computers available to use
hardware,3dhz6m,TriGeo,1 point,Thu Jul 16 19:41:35 2015 UTC,I think keeping a CD drive around so you don't have to go to the fucking library to download a driver for your computer is reasonable.
hardware,3dhz6m,bfodder,1 point,Thu Jul 16 20:55:57 2015 UTC,Not if you don't have a CD drive
hardware,3dhz6m,TriGeo,1 point,Thu Jul 16 21:11:43 2015 UTC,$20 I think that amount is worth not having to go to the damn library to download a driver.
hardware,3dhz6m,bfodder,1 point,Fri Jul 17 13:57:31 2015 UTC,What's your beef with libraries? They're nice places with nice people.
hardware,3dhz6m,TriGeo,1 point,Fri Jul 17 14:08:47 2015 UTC,"I don't want to have to put my pants on, get in the car, drive to the libary. Get on one of their computers from 2008. Download the driver. Drive back home. Finally get the driver on my computer.  Not to mention most of the time I'm doing this sort of thing it is well past the hours that a library would be open.  If I can spend $20 to not have to do that then I will."
hardware,3dhz6m,bfodder,0,Fri Jul 17 14:32:45 2015 UTC,make your own USB   Seriously? This is the second time in here.
hardware,3dhz6m,bfodder,1 point,Fri Jul 17 14:39:26 2015 UTC,"Guess I should say ""make your own bootable USB partition""?"
hardware,3dhz6m,TriGeo,0,Fri Jul 17 15:43:47 2015 UTC,You're going to make a partition on a connector and communications protocal?
hardware,3dhz6m,bfodder,1 point,Fri Jul 17 12:38:50 2015 UTC,"Universal means it can do anything, right? /s"
hardware,3dhz6m,TriGeo,13,Fri Jul 17 12:56:50 2015 UTC,"Quick question, slightly off topic, but if I get the free update notification, then go through the steps to reserve my copy, but then have to reformat, do I have to install windows 7, update it completely, then go through the steps again, or is it already tied to my product key?  Can I just download an iso and install it straight from that in the future?"
hardware,3dhz6m,fuelvolts,14,Fri Jul 17 13:55:22 2015 UTC,"When you upgrade from 7 to 10 the key become a win 10 license, you can format and reinstall win 10 directly, it won't activate a win 7 installation anymore after the 30 days where you can downgrade back to 7."
hardware,3dhz6m,MeVe90,3,Fri Jul 17 14:08:08 2015 UTC,"Not that I don't believe you, but where did you learn this?  And do you know how it affects the 3-computer license versions of Win7 (since it's a single license key that can be used on up to 3 computers)?"
hardware,3dhz6m,dlightle,4,Thu Jul 16 15:55:08 2015 UTC,"I follow /r/Windows10 If you google ""windows 10 30 days to revert upgrade"" you should find several article saying this. For 3-computer license I believe they haven't said anything yet."
hardware,3dhz6m,MeVe90,5,Thu Jul 16 17:43:26 2015 UTC,"The references I can find to the 30-day window appear to be referring to Windows 10 keeping around a snapshot to do an in-place revert of the upgrade, and even that stuff is all unofficial. There's nothing clear about the actual legal status of the licensing."
hardware,3dhz6m,wtallis,4,Thu Jul 16 18:06:15 2015 UTC,"Well this just makes me conflicted now.  I've been looking at cases for my long overdue build refresh this fall and one of my requirements has been the need for a DVD drive so I can install Windows as well as watch the occasional movie from the family's deep collection.   I've even considered the option of getting an external drive I can hook up when needed then remove when not needed.   But if they are shipping Windows on thumb drives to start with, I'm much more tempted to go optical-less and make my build more compact and streamlined, at least until I can get to installing a custom loop in it which will inflate the size again.  The external option is looking better and better because it would also benefit my laptop for times when I dont want to be sitting at a desk to watch a movie."
hardware,3dhz6m,Seclorum,10,Thu Jul 16 18:52:28 2015 UTC,"Thats only 1 month from now. Assuming they have to manually put windows onto all the usbs, the final beta release should be pretty soon."
hardware,3dhz6m,Sundays_evening,14,Thu Jul 16 19:29:54 2015 UTC,the final beta release should be pretty soon   It's already out on the Technical Preview. Build 10240 is RTM. Retail release might be a higher build though.
hardware,3dhz6m,WhiteZero,9,Thu Jul 16 19:52:13 2015 UTC,"Build 10240 is RTM and from here on out all updates will be pushed via windows update. They might roll the updates into the retail image, but it will still be build 10240, afaik."
hardware,3dhz6m,alexmoda,4,Thu Jul 16 13:02:40 2015 UTC,"I find this to be unsettling.   The previews still very...unrefined. cortona is the biggest piece of shit, for example."
hardware,3dhz6m,Rebeleleven,2,Thu Jul 16 13:52:36 2015 UTC,"I do, too. There is still no way to manually pick and choose updates. The OS keeps updating drivers I don't want updated, and the option to disable such driver updates are hidden. It's such a pain in the ass."
hardware,3dhz6m,Masteryeoj,1 point,Thu Jul 16 14:36:17 2015 UTC,What's so bad about Cortana in your experiences?
hardware,3dhz6m,ItzWarty,3,Thu Jul 16 19:15:57 2015 UTC,"Well, for one it takes priority over full screen applications and can trigger at odd times.   So, for example, playing CSGO, cortona will MINIMIZE THE GAME TO GOOGLE SOME PHRASE it thinks I asked.   You can see how this could leave me very underwhelmed."
hardware,3dhz6m,Rebeleleven,1 point,Fri Jul 17 01:02:20 2015 UTC,"Please submit this to the Feedback app - there's also some awesome thoughts in there about ""Cortana, show me an offlane build for Sniper"" and having that appear on your secondary monitor."
hardware,3dhz6m,Diarrg,2,Thu Jul 16 23:18:49 2015 UTC,"Is it still possible to download 10240, was my understanding they removed the downloads."
hardware,3dhz6m,SirCrest_YT,2,Fri Jul 17 00:54:52 2015 UTC,if you have a technical preview build installed (with fast or slow) windows update will download it
hardware,3dhz6m,DirtAndGrass,2,Fri Jul 17 04:25:09 2015 UTC,"Could one still load the technical preview at this time? I installed it on a VM when it when live, but never loaded it on my main machine."
hardware,3dhz6m,FapFlop,2,Thu Jul 16 15:25:16 2015 UTC,"Not sure about right now, but I installed it yesterday, and am currently waiting for 10240 to finish installing"
hardware,3dhz6m,DirtAndGrass,-6,Thu Jul 16 17:38:21 2015 UTC,Torrents are your friend
hardware,3dhz6m,jaju123,7,Thu Jul 16 17:50:30 2015 UTC,"I imagine manually is still even then quite automated. Even if they needed a human to do it, I'd imagine they'd have a huge USB board and a worker in china just plugs it in until a light shows up saying the drive has been formatted, written to, and then verified, and then maybe locked somehow. They then take it out dump it into a bin and it moves on to packaging.  What would large scale USB flashdrive software delivery look like? I'm curious  Edit: Don't know why i'm upvoted, literally just guessing."
hardware,3dhz6m,SirCrest_YT,9,Thu Jul 16 17:55:34 2015 UTC,Would they even need factory workers? Seems like something that could be 100% automated.
hardware,3dhz6m,hdshatter,2,Thu Jul 16 16:15:36 2015 UTC,"Just for the process of plugging in a vanilla flashdrive into a giant industrial sized USB board. I don't know if a robot could do it, cheap enough. I mean theoretically yes, but a robot might not have enough productive enough to be worth it.   Just have an underpaid worker 60 hours a week plugging in and unplugging drives, it's mindless work."
hardware,3dhz6m,SirCrest_YT,11,Thu Jul 16 13:41:59 2015 UTC,"Just for the process of plugging in a vanilla flashdrive into a giant industrial sized USB board   No need, if you need to do this in bulk enough you plug the circuit board right in a socket hard wired then place the usb port and case on after."
hardware,3dhz6m,slyf,3,Thu Jul 16 14:28:34 2015 UTC,Probably faster than the DVD and case process.
hardware,3dhz6m,naanplussed,0,Thu Jul 16 15:24:08 2015 UTC,"onto all the usbs,   Jesus Christ, now people in technically focused subreddits are doing this now? It is in the title too."
hardware,3dhz6m,bfodder,4,Thu Jul 16 16:00:50 2015 UTC,I've not even had my dvd drive connected for years.   I think this is a good move
hardware,3dhz6m,InsightfulLemon,4,Thu Jul 16 13:46:33 2015 UTC,In reference to the USB part. I was just thinking to myself why doesnt everything come on USB now? They seem to be easier to deal with and seem to be getting cheaper and cheaper.   Kinda wish everyone would goto this.
hardware,3dhz6m,The_derp_train,8,Fri Jul 17 12:36:28 2015 UTC,Probably still a lot cheaper to burn a bunch of DVDs rather than use USBs.
hardware,3dhz6m,TimeTravellerSmith,2,Thu Jul 16 17:50:05 2015 UTC,I was just thinking to myself why doesnt everything come on USB now   Because USB is a protocol.
hardware,3dhz6m,bfodder,2,Thu Jul 16 22:28:29 2015 UTC,About time.
hardware,3dhz6m,orezavi,4,Thu Jul 16 23:36:58 2015 UTC,I wonder how the download on the 29th will go.. because I want to be able to fully download the OS installation files to say... a USB drive before attempting to actually install it.
hardware,3dhz6m,The-ArtfulDodger,4,Fri Jul 17 12:40:56 2015 UTC,IIRC the entirety of 8.0 and the 8.1 upgrade were downloaded to a directory in the root of the C: drive. There might be a way to finesse that into a bootable install image.
hardware,3dhz6m,specter800,2,Thu Jul 16 21:07:00 2015 UTC,dism maybe?
hardware,3dhz6m,onlyhtml,1 point,Thu Jul 16 14:32:50 2015 UTC,"Yep, did this a month ago."
hardware,3dhz6m,ItzWarty,1 point,Thu Jul 16 18:31:24 2015 UTC,"I can't edit on my mobile phone with Reddit flow so I'll have to double post.  I didn't upgrade to 10 from 8, I downloaded the is Idaho, created a partition for 10, used dism /apply-image, set it as bootable with EasyBCD, and restarted."
hardware,3dhz6m,ItzWarty,2,Thu Jul 16 21:45:19 2015 UTC,"You probably will not be able to install on the 29th, they said they'll be releasing in waves through Windows Update. And I believe you will have to upgrade through the Update before you can use an ISO to do a clean install."
hardware,3dhz6m,jmac,3,Thu Jul 16 23:19:44 2015 UTC,I'm dying to update from Win 7 for various other reasons.. But I don't feel comfortable updating the OS without a copy of the OS at hand.
hardware,3dhz6m,The-ArtfulDodger,0,Thu Jul 16 23:24:06 2015 UTC,Oh my god thank you for saying USB drive and not just USB.
hardware,3dhz6m,bfodder,2,Thu Jul 16 18:40:31 2015 UTC,"Does this change expectations for Surface Pro 4 release date? I heard people pegged Windows 10 for a 2016 release, and they thought the next Surface would be released alongside it."
hardware,3dhz6m,aleatoric,1 point,Thu Jul 16 18:59:06 2015 UTC,"Next surface is slated for October, releasing windows on USB drives doesn't change a thing about that since it will come pre-installed."
hardware,3dhz6m,Turtlecupcakes,1 point,Fri Jul 17 13:58:41 2015 UTC,This says possibly 2016 for Surface 4.  http://masterherald.com/microsoft-surface-pro-4-release-date-reportedly-pushed-back-by-a-year-due-to-new-plan-to-use-samsung-technology/21974/
hardware,3dhz6m,alpacIT,1 point,Thu Jul 16 16:33:53 2015 UTC,The idea that Microsoft is going to delay the release of the Surface 4 because of the new Samsung tech is nothing more than wild speculation.  Its based on nothing more than Microsoft bought a bunch of SSDs from Samsung and will get them in 2H 2016.
hardware,3dhz6m,blergleblarg,1 point,Thu Jul 16 17:41:55 2015 UTC,Well any release date is speculation since nothing has been announced.
hardware,3dhz6m,alpacIT,1 point,Thu Jul 16 18:24:15 2015 UTC,I wonder if the usb drive will be read only.
hardware,3dhz6m,ReallyObvious,1 point,Thu Jul 16 18:45:58 2015 UTC,I'd say 99.99% yes.
hardware,3dhz6m,hdshatter,0,Thu Jul 16 18:56:22 2015 UTC,I live in France and want a French version of Windows 10: is it safe to buy the USB stick with Windows 10 on it from Amazon USA?
hardware,3dhz6m,freretoque,4,Fri Jul 17 03:04:42 2015 UTC,Will it still come with the keylogger?
hardware,3dhz6m,__thiscall,1 point,Tue Jul 21 21:46:39 2015 UTC,That was the Technical Preview only and it wasnt even a true keylogger. It didnt make a record of all your key strokes. Only a small percentage for testing reasons.
hardware,3dhz6m,Asahoshi,0,Fri Jul 17 10:32:33 2015 UTC,"The current Privacy Statement still allows that, and looking for more information revealed that MS is neither open, nor too honest about it.  The statement allows the implementation of a ""true"" keylogger, and dependence on whether users find one or not is not something I'd like to have."
hardware,3dhz6m,__thiscall,2,Thu Jul 16 22:17:37 2015 UTC,Will come on USB   Will come on USB what?
hardware,3dhz6m,bfodder,-2,Thu Jul 16 23:43:42 2015 UTC,Presumably a thumb drive.
hardware,3dhz6m,Seclorum,1 point,Fri Jul 17 00:22:43 2015 UTC,You know.. smart idea.. but I will just keep an ISO on one of my flash drives and call it good when the free standard version finally comes out.
hardware,3dhz6m,ajac09,0,Fri Jul 17 12:39:17 2015 UTC,Free standard version?
hardware,3dhz6m,weks,1 point,Fri Jul 17 19:29:01 2015 UTC,the ISO for the standard aka home version.
hardware,3dhz6m,ajac09,1 point,Thu Jul 16 20:06:19 2015 UTC,All windows versions are paid. Unless you are pirating it.
hardware,3dhz6m,americosg,1 point,Fri Jul 17 08:56:10 2015 UTC,umm no. Upgrades for the first year are free. You can pay for it retail of course. oh and btw upgrade is a version.
hardware,3dhz6m,ajac09,1 point,Fri Jul 17 12:10:08 2015 UTC,You mean trial is free.
hardware,3dhz6m,americosg,1 point,Fri Jul 17 16:06:37 2015 UTC,https://www.microsoft.com/login.srf?wa=wsignin1.0&rpsnv=12&ct=1437157485&rver=6.5.6509.0&wp=MBI&wreply=https:%2F%2Fwww.microsoft.com%2Fen-us%2Fwindows%2Fwindows-10-upgrade&lc=1033&id=74335 looks like free complete software to me
hardware,3dhz6m,ajac09,0,Fri Jul 17 16:13:44 2015 UTC,Nice broken link.
hardware,3dhz6m,americosg,1 point,Fri Jul 17 18:22:25 2015 UTC,Works fine for me here something easier for you: https://www.microsoft.com/en-us/windows/windows-10-upgrade
hardware,3dghj0,Kodiack,113,Thu Jul 16 02:12:11 2015 UTC,So you're saying that AMD has a chance?
hardware,3dghj0,namae_nanka,48,Thu Jul 16 02:47:50 2015 UTC,I like where your head's at. Realization that 10nm is postponed like many said it would be - meh. Noticing that this may help keep Intel and AMD somewhat close for the near future- whoopee!!!!!
hardware,3dghj0,carly_rae_jetson,45,Thu Jul 16 02:58:08 2015 UTC,"And there's basically no chance that 10nm is going to be any cheaper per-transistor when it does finally ship, so Intel's fab advantage will probably be a fairly minor factor in the desktop market where power consumption is much less important than performance and price.  Beyond the CPU market, right now is the best time for chip start-ups since the mid-'90s, because 28nm is a cheap commodity process while also being pretty close to state of the art, and TSMC's 16nm FinFET process and Samsung's 14nm FinFET will have attained very nearly the same status by the time Intel moves off 14nm. (It's looking like nothing will be cheaper than 28nm, so that's here to stay. AMD is going to keep making Pitcairn chips until the masks wear out.)"
hardware,3dghj0,wtallis,21,Thu Jul 16 03:23:58 2015 UTC,"Nice to hear some optimism about the industry for once. I'm liking the look of GlobalFoundries new 22nm FD-SOI process. Seems like it could really lower the barriers to entry of the microprocessor industry in the long run. http://scr3.golem.de/screenshots/1507/GloFo-22FDX/thumb620/GloFo-22FDX-03.png  Edit: Also, one has to wonder what's going to happen with TSMC's 28nm. AMD and Nvidia will abandon it for FinFETs, and undoubtably some of the fabs will be shifted over to accommodate, but you're left with a fair amount of production capacity on a process that can reliably produce 600mm2 chips. Will anyone else use that capability?"
hardware,3dghj0,Exist50,10,Thu Jul 16 04:15:48 2015 UTC,InGaAs CPUs :D
hardware,3dghj0,Idkidks,1 point,Thu Jul 16 05:58:34 2015 UTC,AMD is still making its FX processors on 32nm right?
hardware,3dghj0,daubertMotion,2,Thu Jul 16 15:25:43 2015 UTC,"Yup, 32nm SOI from Globalfoundaries. This 22nm isn't suitable for a high power chip, though."
hardware,3dghj0,Exist50,1 point,Thu Jul 16 16:36:07 2015 UTC,"Yeah, a move from 32 to 28 is probably too little too late with the promise of Zen on the horizon"
hardware,3dghj0,daubertMotion,1 point,Thu Jul 16 17:21:36 2015 UTC,Didn't help Steamroller one bit.
hardware,3dghj0,Exist50,-2,Thu Jul 16 17:54:58 2015 UTC,chip start-ups    This is good for Bitcoin.
hardware,3dghj0,conradsymes,13,Thu Jul 16 10:37:30 2015 UTC,14NMBOWL GET HYPE
hardware,3dghj0,BrainSlurper,2,Thu Jul 16 03:45:09 2015 UTC,Isn't AMD saying they're next line of processors will be 22nm?
hardware,3dghj0,nav13eh,2,Thu Jul 16 16:58:24 2015 UTC,"No. FinFETs, so 14nm/16nm."
hardware,3dghj0,Exist50,3,Thu Jul 16 17:49:55 2015 UTC,"Oh, well that's a lot more exciting. AMD will finally have the chance to be caught up to Intel."
hardware,3dghj0,nav13eh,21,Thu Jul 16 18:07:37 2015 UTC,I could call AMD and tell them I'm leaving my house at 5:00 PM for 3 hours and they are free to come and steal whatever they want.  I would get home at 8:00 PM and they would still be outside trying to figure out how to use the doorknob.  What I'm saying is -- Give AMD the biggest opening in the world and they will find a way to fucking blow it.  They're going to blow it again and Intel knows it.
hardware,3dghj0,TaintedSquirrel,7,Thu Jul 16 12:17:26 2015 UTC,"It's strange seeing them in this situation considering how they're beating Intel senseless during the P4 era and then on the graphics front with the 5xxx series to nvidia.   Both cases the rival did something underhanded and now they're up against the wall finding themselves in reverse situation. And curiously there's opening on both sides, with the new node on GCN they could get clocking them higher and close the deficit against maxwell and Zen could be far less handicapped if Intel went another node ahead."
hardware,3dghj0,namae_nanka,3,Thu Jul 16 14:17:46 2015 UTC,"It's strange seeing them in this situation considering how they're beating Intel senseless during the P4 era   And yet they weren't able to really build much on this advantage, despite having a definitely superior product, were they?  Read the Ars Technica series The Rise and Fall of AMD, it really highlights a lot of the internal problems AMD has had over the years."
hardware,3dghj0,SPOOFE,-5,Thu Jul 16 19:03:46 2015 UTC,"I told you to never reply to me, especially with articles that I've read already.   And yet they weren't able to really build much on this advantage, despite having a definitely superior product, were they?   Indeed, and if you're to read carefully you'd see why."
hardware,3dghj0,namae_nanka,2,Fri Jul 17 01:18:45 2015 UTC,I told you to never reply to me   Log off
hardware,3dghj0,atomicthumbs,1 point,Fri Jul 17 12:26:07 2015 UTC,I told you to never reply to me   Oh boo-hoo.
hardware,3dghj0,continous,1 point,Sat Jul 18 07:08:59 2015 UTC,"He's a retard who thinks that AMD's mantle is thanks to nvidia's CG, and I've had enough run-ins with this stupidity already to understand that he's a lost cause.  https://www.reddit.com/r/hardware/comments/3d051b/amd_r9_fury_vs_gtx_980_overclocked_benchmarks/ct0u46s"
hardware,3dghj0,namae_nanka,1 point,Sat Jul 18 14:12:56 2015 UTC,"I'm confused, was this supposed to make you look good? You're entitled to your opinion and all, but don't expect people to agree with you in order for them to not be dumb. Are you always like this? You really should look into being less aggressive and more open to foreign ideas."
hardware,3dghj0,continous,1 point,Sat Jul 18 14:19:30 2015 UTC,"He's a retard and that was the most recent bit of stupidity from him. I am quite less aggressive, unless the other guy has shown himself to be a fucking buffoon."
hardware,3dghj0,namae_nanka,0,Sat Jul 18 14:26:23 2015 UTC,"He's a retard   Then you go on to say,   I am quite less aggressive   Contradictory are we?"
hardware,3dghj0,continous,3,Sat Jul 18 14:31:55 2015 UTC,"at least they have been saying the right things recently, by acknowledging the gap in ipc and that zen is trying to correct them.  however, their pr snafus with the recent gpu launch has me unsure as to whether or not they will/can execute them. one step in right direction was to halt following the path of bulldozer and to regroup, so they are walking the walk somewhat.  they are the underdog which typically means they are more dynamic but the heavy burden debt restructuring has placed on them leaves them much more restricted."
hardware,3dghj0,logged_n_2_say,1 point,Thu Jul 16 14:18:55 2015 UTC,"Exactly. IIRC, bulldozer performance was actually worse than Phenom II. They've caught up to Phenom II now but Phenom II was like 5 years ago. They literally haven't taken a single step forward."
hardware,3dghj0,leoniciard,5,Thu Jul 16 16:51:04 2015 UTC,"In IPC, that is (though no longer true with Steamroller). Other things have improved a lot."
hardware,3dghj0,Exist50,-1,Thu Jul 16 17:50:49 2015 UTC,AMD certainly didn't blow it in the mid-00's when their K8 architecture was destroying Netburst in just about every metric.
hardware,3dghj0,gotamd,9,Thu Jul 16 14:32:03 2015 UTC,"Yeah, but at this point, it's starting to look like they were a one hit wonder. I have an 8350 and 290x myself, and I really want to upgrade to Zen next year, but I'm starting to lose faith in them."
hardware,3dghj0,dreakon,5,Thu Jul 16 15:52:45 2015 UTC,"It's less that they're one hit wonders and more that they were never able to grow their marketshare during that time because A. Intel had anti-competitive agreements in place with OEMs like Dell and B. the inability to grow marketshare meant they couldn't afford to reinvest in fabs, which further limited their ability to grow share.  Cashflow eventually got bad enough that they had to spin off Global Foundaries. This was arguably necessary to save the company, but made the situation even worse since they now had to rely on third party fabrication.  But what's really killed them is that Intel has been literally a few steps ahead of the rest of the industry in process tech for years now. That means that even if AMD was building chips that were as efficient as -- or even moreso than-- Intel's, Intel is still able to crank out more chips at a lower cost so AMD can't even compete on price as they'd done in the K6 era.  tl;dr - AMD's problems are more fundamental than not being able to design the chips themselves."
hardware,3dghj0,commandar,5,Thu Jul 16 17:54:00 2015 UTC,"AMD were competitive before then as well.  The Athlon/XP and Pentium III competed pretty well.  However, ever since Intel switched to the Core series, AMD has been hurting.  What's most strange is that AMD did the same thing with the K10 architecture that Intel did with Netburst: high clocks, low IPC, high power consumption.  It's a losing formula, and AMD should have known better."
hardware,3dghj0,gotamd,1 point,Thu Jul 16 16:30:14 2015 UTC,"AMD were competitive before then as well.   Good products can only do so much when you have bad management. AMD's bottom line has been awful for decades; there's no point since the early '90s that they've gone more than a couple years in a row with their financials in the black.  It's great that the A64's were excellent products, and the X2's and X4's were killer. But AMD wasn't able to capitalize on those products in any meaningful sense."
hardware,3dghj0,SPOOFE,2,Thu Jul 16 19:07:18 2015 UTC,"They made money for a while, and their stock broke into the 30s (so their market cap was an order of magnitude larger than it is today).  But yeah, agreed.  AMD has never been particularly strong from a financial perspective, especially when you compare them to Intel."
hardware,3dghj0,gotamd,2,Thu Jul 16 19:15:24 2015 UTC,"AMD's definitely not a one-hit wonder. All but one of their big architectural gambles has succeeded at overcoming Intel's fab advantage for at least a while. The Bulldozer architecture that didn't pay off was just the most recent one. K8 kicked the pants off the dead-end P4s, the early Athlon XPs were competitive with the much more expensive RDRAM-based P4 systems, the first Athlons beat the P3 to 1GHz, and the K6 was competitive with the P2 without requiring a new motherboard.  And don't ignore that AMD has been doing a pretty good job of competing with the Atom processors all along, to the point that they got picked to supply the game console CPUs."
hardware,3dghj0,wtallis,1 point,Thu Jul 16 19:43:20 2015 UTC,"Doubt it, the moment AMD competes Intel will up die size to 400mm2 + and increase IPC. Recently Intel has focused on power consumption, so it should be somewhat easy to increase IPC. Remember that Vishera is 50% slower than Haswell IPC-wise, and I doubt Zen will make up enough."
hardware,3dghj0,CykaLogic,1 point,Thu Jul 16 23:30:45 2015 UTC,"The problem with so much IPC is that software doesn't actually use it. It's hard to schedule instructions to fill what we have now, adding more is definitely diminishing returns and may not even do anything. On top of that, the CPU's own instruction reorder window can't work miracles, it needs the compiler to help. That means even if it would be possible for regular (non-benchmark) programs to fill the CPU completely they'd need to be recompiled with a new compiler targeting the new CPU at the possible expense of older generations and AMD CPUs."
hardware,3dghj0,SomeoneStoleMyName,-5,Fri Jul 17 10:35:55 2015 UTC,"Yes. AMD may very well survive and hasten the transition of making big x86 cores commodity products. I would love to proudly trumpet my part in saving the people a non-permanent 30% markup on high margin intel parts. Heck, maybe someday us amd fans will be able to stand above the legion of intel lackeys with our 3% faster per core processor."
hardware,3dghj0,maseck,11,Thu Jul 16 04:14:46 2015 UTC,Sometimes it feels as if this industry is just a game and entertainment for some.
hardware,3dghj0,zxcdw,12,Thu Jul 16 08:56:31 2015 UTC,I will cut off an Intel users head if our team doesn't win the cpu-bowl this year
hardware,3dghj0,AssCrackBanditHunter,-1,Thu Jul 16 11:08:58 2015 UTC,"I sure donated my savings for a FX8350, I mean I also needed it. But it felt good, ya know? Stickin' it to the green man."
hardware,3dghj0,IC_Pandemonium,4,Thu Jul 16 08:49:52 2015 UTC,Stickin' it to the green man.   Isn't Intel's logo blue?
hardware,3dghj0,aaron552,1 point,Thu Jul 16 13:04:27 2015 UTC,I think he meant money man (green=money)
hardware,3dghj0,superjojo29,4,Thu Jul 16 13:17:08 2015 UTC,Team green is nvidia...
hardware,3dghj0,dudemanguy301,1 point,Thu Jul 16 13:11:37 2015 UTC,Well... AMD's color used to be green.
hardware,3dghj0,Exist50,-3,Thu Jul 16 14:37:46 2015 UTC,"Things have been looking up for AMD lately. HBM is very good and space efficient and AMD supposedly has priority on HBM 2.0 units, and now Zen is competing on the same process as Intel."
hardware,3dghj0,Dragon_Fisting,12,Thu Jul 16 06:52:23 2015 UTC,"On the other hand, their recent quarterly financial statement shows losses due to weak mobile sales (if I remember correctly) and the rumors that Fury cards are scarce seem to have some truth to it.  We've almost no information on Zen right now, except for the few bits that AMD has teased. I hope it'll be competitive but there's no saying it yet."
hardware,3dghj0,Mr_s3rius,29,Thu Jul 16 09:36:45 2015 UTC,"and now Zen is competing on the same process as Intel.   There's more to a process node than just the nanometers. Even if GloFo and Intel are both manufacturing at 14nm, you can be more than certain that Intel still has a notable advantage.  It is very hard to compete when your competitor can reach higher clocks on lower voltage while producing a smaller chip with more transistors.  Zen, 14nm, whatever. I still have my money on Intel. I see no rational reason to root for AMD for desktop/mobile CPU parts except some niche cases. Their track record isn't convincing at all, even if you exclude the anti-competitive bullshit Intel has pulled off against AMD in the past.  Not to say AMD's situation is only getting worse, but sometimes I've gotten the feeling people are expecting AMD to make some sort of a ""comeback"". Like they were supposed to do with K10, right? Bulldozer, right? Zen, right?   And I'm saying this as someone who has poured more money on AMD (+ ATI) parts than Intel and Nvidia combined. But well, that was like a decade ago when it made sense."
hardware,3dghj0,zxcdw,7,Thu Jul 16 08:49:40 2015 UTC,"AMD doesn't need to beat Intel. They just need to be a bit more competitive.  I recently bought a used 2500k because Intel doesn't offer any cheap, overclockable quad core. Had I chosen from Intel's new CPUs I would only get something like a locked i3 for the same price."
hardware,3dghj0,Kagemand,3,Thu Jul 16 15:36:16 2015 UTC,"I think people are more interested in AMD getting closer to the same IPC levels as Intel parts with Zen. They're not looking for anything world shattering, just some catch up, so that there's competition in the mid-range space, which is where most gamers want to be.  If there's an under $150 Zen part that is an unlocked quad core and has similar IPC to Broadwell currently has, then that'll be absolutely huge for PC builders.  Obviously, it is still ""ifs"" and ""whens"", but from reports, Zen looks to be about that type of IPC. That's all most people really need. Here's an awesome round-up that PCPer did on CPUs. I can confirm that the Athlon 860k is a great CPU for budget gaming (just built my brother an HTPC console replacement with one and an R9 270x).  I currently run an Intel-based machine, but Zen could be very intriguing, since I do a lot of builds for people that aren't looking to spend $1000 or more on a computer."
hardware,3dghj0,IndigoMoss,7,Thu Jul 16 12:52:25 2015 UTC,"Tl;dr:  Zen will be around Nehalem-Sandy Bridge.   and has similar IPC to Broadwell currently has   Not gonna happen.   but from reports, Zen looks to be about that type of IPC   http://www.anandtech.com/show/9320/intel-broadwell-review-i7-5775c-i5-5675c/4  http://images.anandtech.com/graphs/graph9320/74925.png  7870k has a max turbo of 4.1GHz:  http://images.anandtech.com/doci/9307/Slide%202%20-%20A-Series%20APUs_575px.png  http://www.anandtech.com/show/9307/the-kaveri-refresh-godavari-review-testing-amds-a10-7870k  5775C has a 3.7GHz max:  http://www.anandtech.com/show/9320/intel-broadwell-review-i7-5775c-i5-5675c/2  So roughly, multiply the 7870k scores by 0.9, round it down to 70. 126/70 (5775C/7870k roughly) = 1.8, clock for clock the 5775C is about 1.8x the performance (little bit more seeing as we're generous towards AMD). Comparing the 5775C to 4790k, which is 3.7GHz vs 4.4GHz max that's roughly 1.2x the clock rate for 1.16x the score which sounds roughly inline with recent Intel IPC increases. Granted that's Kaveri if I'm not mistaken not Carrizo, but AMD say it's ~5% for that.   AMD's said Zen is about 1.4x the IPC (benchmarks pending of course) so best cast scenario it looks like we're talking 1.5x Kaveri for Zen. If we assume Skylake's another 1.05-1.1x that roughly eliminates the Carrizo increase, which would mean Intel are about 1.25-1.3x that of Zen's IPC - 1.9/1.5 (roughly Skylake vs Zen, I'm using Kaveri as 1x IPC). And looking things up:  http://www.anandtech.com/show/8426/the-intel-haswell-e-cpu-review-core-i7-5960x-i7-5930k-i7-5820k-tested/2  ""This also means an overall 13% jump from Sandy Bridge-E to Haswell-E. From Nehalem, we have a total 28% raise in clock-for-clock performance."" - Nehalem -> Haswell-E is roughly 28%, which would mean Zen is roughly Nehalem-Sandy IPC.  If AMD can maintain or even increase clocks which is a big if and are priced well they will be a compelling solution. If they can OC well in addition to that they will be a very compelling solution, an 8 core/16 thread Sandy ~4.5GHz OC air/5GHz water priced well?"
hardware,3dghj0,Blubbey,0,Thu Jul 16 13:24:14 2015 UTC,"but sometimes I've gotten the feeling people are expecting AMD to make some sort of a ""comeback"".    In F1 terms, McLaren Honda hype."
hardware,3dghj0,Blubbey,8,Thu Jul 16 12:58:00 2015 UTC,"What are you talking about? AMD posted almost a $200 million loss last quarter, and the Fury X launch was a huge disappointment riddled with fuck ups.  Nothing about them is looking up except their belly."
hardware,3dghj0,Whats_a_narwhal,6,Thu Jul 16 11:55:31 2015 UTC,"Nothing about them is looking up except their belly.   Is it okay if I start stealing that, from now on ?"
hardware,3dghj0,SomniumOv,1 point,Thu Jul 16 12:17:28 2015 UTC,Fine by me.
hardware,3dghj0,Whats_a_narwhal,0,Thu Jul 16 12:35:29 2015 UTC,"The Fury X doesn't appear to be a financial disappointment (say what you will about supply, but it's sold out), and despite what many were saying previously, it proves that AMD can provide Maxwell-level efficiency at 28nm."
hardware,3dghj0,Exist50,-1,Thu Jul 16 14:42:53 2015 UTC,No.  Intel releasing a 3rd 14nm chip does not mean AMD has a chance.  The two are not even in the same league
hardware,3dghj0,Bob_Fucking_Ross,25,Thu Jul 16 13:31:00 2015 UTC,"IMHO, I think that Intel will ditch silicon after 10nm. R&D will probably just be too expensive and time consuming. They will probably do some changes in terms of microarchitecture (lots of 'tocks') to perfect the 10nm node, but probably won't push silicon past that 10nm mark. Who knows what might be used after that? I have heard that Silicon-Germanium is what IBM used in their 7nm chips, InGaAs (Indium Gallium Arsenide) looks interesting as well. Also, there will be a point where you can't make transistors any smaller. What then? Making the dies bigger? Stacking the cores? It will be interesting and very exciting to see what happens."
hardware,3dghj0,SchrodingersCat_,12,Thu Jul 16 06:12:30 2015 UTC,"Is stacking even viable with such high power stuff? I mean CPUs are already hot things, how would stacking work? Maybe it's possible to cool them somehow but that wouldn't help with power requirements... I just can't see stacking being such a big deal as it is in memory. You can't really make the transistors slower either, since you need that IPC.  I'm hoping for some serious frequency improvements from new materials, but we shall see."
hardware,3dghj0,bphase,13,Thu Jul 16 09:28:45 2015 UTC,IBM wants to run water through the interconnect forest between chips (the inner surface would have a coating to make it water proof).  There's no reason they couldn't run other liquids through there as well.
hardware,3dghj0,Dr_professional,-1,Thu Jul 16 13:03:08 2015 UTC,Cool. I was just think that would be a cool way to solve the problem. Glad someone is looking at it.
hardware,3dghj0,OSUfan88,1 point,Fri Jul 17 02:54:58 2015 UTC,"Also, there will be a point where you can't make transistors any smaller. What then? Making the dies bigger? Stacking the cores? It will be interesting and very exciting to see what happens.    I'm on strong painkillers at the moment, so I might not make much sense, but I think the next step after we hit the maximum reasonable transistor limit for a single die will be to move into distributed computing. We're actually already doing this with the whole Software as a Service butt(cloud) computing thing. It just seems inevitable to me that eventually, everything will connect to a powerful cluster somewhere, be it in a datacentre, or in your own home somewhere. It's very rare that most people actually use their computing resources to its full potential, so it would be more efficient, and thus cheaper, to share a much higher performing system with a load of other people."
hardware,3dghj0,Anon123212321,3,Thu Jul 16 14:48:53 2015 UTC,"If this were to happen (and it quite conceivably could), wouldn't the main issue become bandwidth?"
hardware,3dghj0,docbrown88mph,6,Thu Jul 16 15:41:19 2015 UTC,"I wound think latency would be a bigger issue, you can usually solve the bandwidth issue by multiplexing and throwing more fibre at it."
hardware,3dghj0,Anon123212321,2,Thu Jul 16 15:50:56 2015 UTC,"Ok, bear with me as I don't know much of the complexities of IT and networking, but how would one address the issue of latency?"
hardware,3dghj0,docbrown88mph,7,Thu Jul 16 16:22:54 2015 UTC,"You can't, really. The speed of light is a physical limit. You just have to move everything closer. A lot of caching."
hardware,3dghj0,omnilynx,1 point,Thu Jul 16 16:38:26 2015 UTC,i wonder what kind of usecases would need less than <10ms latency which is already possible today. You can pretty much stream entire desktops and games with that kind of latency with only the most hardcore of users being able to feel a difference.
hardware,3dghj0,barthw,1 point,Thu Jul 16 19:16:52 2015 UTC,"Computing use-cases. We're not talking about UX, we're talking about distributed computing where a few instructions are executed on one chip, then the result is passed to another which does a few others, etc. 5ms between instructions is forever."
hardware,3dghj0,omnilynx,1 point,Thu Jul 16 23:16:26 2015 UTC,"Even today you can offload massive computing projects to Amazon EC2 clusters where each node has up to 40 cores and 160GB of RAM. Of course communication between nodes even on the the same network is slow, but that is a problem that can be solved by the application developer who has to find a smart way to balance loads across nodes to minimize waiting time. You can do a lot of things until you reach the limits of an architecture like that. I think this is the kind of ""connect to a powerful cluster in the cloud"" scenario that op meant anyway."
hardware,3dghj0,barthw,5,Fri Jul 17 06:30:29 2015 UTC,"Move stuff closer. Alternatively, increase the speed of light.  Improvements in networking equipment (end user, datacenter and ISP/backbone) can help improve latency, but by far the largest factor with latency-sensitive cloud computing is how long it takes the data to go from user to server then back to user."
hardware,3dghj0,FartingBob,2,Thu Jul 16 16:49:36 2015 UTC,"As others have said, move stuff closer.  If we were to move to a system where everything is in the ""cloud"", I suspect we'd probably end up with loads of mini datacentres dotted around the place in the same way telephone exchanges are to minimise latency.  An added benefit of this would be that the excess heat could easily be recycled to heat nearby homes."
hardware,3dghj0,Anon123212321,2,Thu Jul 16 17:07:19 2015 UTC,granted a latency of <10ms is enough for most tasks and it's totally possible with todays technology. Even realtime game streaming could work at 60fps with a 10ms latency.
hardware,3dghj0,barthw,10,Thu Jul 16 19:13:55 2015 UTC,"Is it becoming technologically unfeasible or financially unfeasible to keep the trend going?  The trend continued because companies (Intel) poured R&D money into it, but if the costs increase (exponentially?) from shrink to shrink, then if they shy away from the investment, is it more of a technological statement or a financial one?"
hardware,3dghj0,daubertMotion,14,Thu Jul 16 03:54:51 2015 UTC,"It's a bit of both. R&D is expensive, and even the best research in the world wouldn't be able to push things further if we truly are reaching silicon limitations.  IBM's already eyeing 7 nm with silicon-germanium, however. If silicon alone isn't going to get us comfortably to 7 nm and beyond, then something else will.  Moore's Law may well have faltered for the consumer market. These next couple of years will be interesting to watch. If nothing else, though, this announcement will probably look like nothing more than a minor bump in the road a decade from now. :)"
hardware,3dghj0,dudemanguy301,2,Thu Jul 16 04:35:38 2015 UTC,Has the CPU industry ever floundered this hard in the past?
hardware,3dghj0,flukshun,10,Thu Jul 16 13:17:34 2015 UTC,I seem to remember the Pentium 4 reaching a point where it was considered a hot mess with only marginal improvement over previous gens. Seems like AMD is what got things back on track
hardware,3dghj0,andromeduck,0,Thu Jul 16 13:37:33 2015 UTC,It's been like this for the last 7 years or so.
hardware,3dghj0,Exist50,0,Thu Jul 16 13:38:43 2015 UTC,"Bit of both. It's becoming both harder and more expensive (they are linked) to design and produce chips on smaller nodes. Some things like EUV will help this, but silicon is quickly approaching it's limits, hence why IBM used a silicon germanium alloy for its demo."
hardware,3dghj0,everyZig,0,Thu Jul 16 04:19:19 2015 UTC,"financially unfeasible   I think financially suboptimal  With the gap Intel has on AMD, there is little point in Intel going all ahead full steam when they can use a process for a bit longer and get some more mileage from their R&D money."
hardware,3dghj0,sfacets,14,Thu Jul 16 06:38:20 2015 UTC,Moore's law is largely a marketing technique at this stage.
hardware,3dghj0,mikemol,7,Thu Jul 16 10:51:01 2015 UTC,Has been for a while.
hardware,3dghj0,Washington_Fitz,0,Thu Jul 16 11:38:46 2015 UTC,For a while? Not really only recently.
hardware,3dghj0,mikemol,2,Thu Jul 16 12:39:40 2015 UTC,"It keeps getting redefined. Hell, 20yrs ago, I started noticing a discrepancy between Moore's original observation and what people were describing it as."
hardware,3dghj0,Washington_Fitz,3,Thu Jul 16 13:02:07 2015 UTC,What do you mean it keeps getting redefined? It has always been about transistors doubling.
hardware,3dghj0,mikemol,1 point,Thu Jul 16 13:03:02 2015 UTC,"I can't find references I trust right now (such is the problem with marketing and highly-hyped terms, as well as a highly wikified knowledge base), but originally the time frame for transistor spacial density was X (readily-available references say 18mo), but it was reported and repeated as Y for a time. I remember Y being around 24mo. And then there were graphs illustrating the growth rate as far north of 18mo, even though article meat was saying ""see? It's all about exponential growth. It's holding up!""  Now, do I think transistor spacial density is is undergoing exponential growth? Of course. Do I think it's happening every 18mo? I don't know. Do I think people like to play the role of Chicken ""Clickbait"" Little? Of course. Do I think the people saying we're holding fast to a particular growth rate are correct? I'm less confident."
hardware,3dghj0,AlchemicalDuckk,3,Thu Jul 16 13:23:26 2015 UTC,"Moore only reformulated his law once. His initial prediction was every 18 months, but in 1975 he changed it to every 24 months. Since then it has remained unchanged."
hardware,3dghj0,mikemol,1 point,Thu Jul 16 13:50:51 2015 UTC,"Well, that's amusing, because the first reference I found (and most references I see) still define it as every 18mo. (Just making an observation on how confusing the resources available are.)"
hardware,3dghj0,troublegoats,45,Thu Jul 16 14:03:38 2015 UTC,"The Moore things change, the Moore things stay the same."
hardware,3dghj0,transmitthis,-1,Thu Jul 16 03:31:28 2015 UTC,"I hear your voice.. I listen to you and I want some more.  Intel will always carry on Tick is lost, tick is found They will keep on speaking Intel's name Some things changed, some stay the same."
hardware,3dghj0,pb7280,21,Thu Jul 16 06:56:42 2015 UTC,"Didn't IBM keep Moore's Law going with that new 7nm chip? I know it's not consumer but it still exists.   EDIT lots of people are saying this wrong, just something I saw in an article (so it has to be true right???)."
hardware,3dghj0,brookllyn,28,Thu Jul 16 03:29:51 2015 UTC,"Moore's law is a number of transistors as function of time. Even if we eventually get to .5 nm, if we don't keep doubling the number of transistors every year, moore's law doesn't apply.   In layman's terms, Moore's law falters as soon as it starts taking more and more time to double the number of transistors(Which I believe was inaccurate as of a while ago)."
hardware,3dghj0,ayylmaozedongayy,10,Thu Jul 16 03:44:11 2015 UTC,"Did you mean 5nm?  I think that 4nm was the limit imposed by quantum tunneling, but I could be wrong."
hardware,3dghj0,pwnegekill,13,Thu Jul 16 05:35:21 2015 UTC,"Actually most experts think that 5nm would be the limit of Moore's law, transistors smaller than 7nm will experience quantum tunneling (even though Australian engineers made a 3nm logic gate) then maybe some time later we can continue more research into optical computing."
hardware,3dghj0,ablatner,5,Thu Jul 16 09:43:07 2015 UTC,"He's exaggerating and saying that even if we manage to get incredibly small, raw feature size doesn't tell the whole story."
hardware,3dghj0,ayylmaozedongayy,3,Thu Jul 16 09:10:35 2015 UTC,that's what she said  I am so sorry
hardware,3dghj0,ixid,3,Fri Jul 17 00:18:53 2015 UTC,"Moore's law is a number of transistors as function of time.   More specifically it's the number of transistors that can be produced cheaply doubles roughly every 18 months. Halving the price of the 28nm node would be following Moore's Law, it says nothing about performance nor single vs multicore."
hardware,3dghj0,nexusheli,3,Thu Jul 16 09:03:44 2015 UTC,"Moore's law says the number of transistors doubles approx. every two years, not annually."
hardware,3dghj0,brookllyn,1 point,Thu Jul 16 14:02:47 2015 UTC,"Ahh, first site I clicked on said it was one year not two. But my point still stands, just because we eventually get the tech to double transistors doesn't mean it will come out quick enough to follow the law."
hardware,3dghj0,nexusheli,3,Thu Jul 16 14:11:47 2015 UTC,"Technically I believe it's 18 mos, but you know.    I understand your point, just wanted you to be aware.  We're certainly at the point where it's not sustainable, and back in the '60s when Moore made the observation he said ""for the foreseeable future"" so I don't think he expected it to get to femto-scale."
hardware,3dghj0,pb7280,2,Thu Jul 16 14:18:02 2015 UTC,"Right, I just remember reading in an article that the IBM chip keeps Moore's law, could be wrong though."
hardware,3dghj0,brookllyn,5,Thu Jul 16 03:48:06 2015 UTC,"I saw that too, it was more of a click baity type headline. Moore's law(strictly speaking, transistor count keeps going up, just doesn't double every year) hasn't been in effect in a while."
hardware,3dghj0,seriouslulz,1 point,Thu Jul 16 04:13:30 2015 UTC,every 2 years*
hardware,3dghj0,Blubbey,1 point,Thu Jul 16 12:22:52 2015 UTC,"Which I believe was inaccurate as of a while ago).   Yeah iirc 40nm was delayed for maybe a year or so (TSMC?), been on 28nm since late 2011 when AMD introduced the 7970 and we're still on it. Next year with finfets since they're skipping 20nm, so more than 4 years on 28nm for GPUs at least."
hardware,3dghj0,SomeoneStoleMyName,1 point,Thu Jul 16 13:43:51 2015 UTC,"They're not really skipping 20nm though, are they? I thought they were taking their 20nm design, reworking it to use FinFETs, and calling it 16nm. Basically just a marketing thing to make it clear this new process improves over their shitty 20nm one almost as much as a real node shrink would. That would also explain why rumors have both AMD and NVIDIA looking to Samsung/Global Foundries to make their GPUs at 14nm. Those still aren't as dense as Intel's 14nm but should be a lot better than TSMC is doing."
hardware,3dghj0,Blubbey,1 point,Fri Jul 17 10:52:50 2015 UTC,"They're not really skipping 20nm though, are they   If they aren't using products on a 20nm node, they are skipping it."
hardware,3dghj0,salgat,0,Fri Jul 17 12:13:42 2015 UTC,It's more accurate to say transistor density. They can easily keep your version of the law going by simply stacking two dies on top of each other.
hardware,3dghj0,valaranin,15,Thu Jul 16 12:42:50 2015 UTC,Yeah but it's a prototype and a silicon germanium alloy so it'll be a few years before it hits full production.
hardware,3dghj0,pb7280,1 point,Thu Jul 16 03:44:30 2015 UTC,True that makes sense
hardware,3dghj0,LiberDeOpp,11,Thu Jul 16 03:48:13 2015 UTC,They dont have a true 7nm chip and there is a huge difference between one off chips and production lithography.
hardware,3dghj0,imallin,2,Thu Jul 16 04:28:23 2015 UTC,"That was just a bunch of PR bullshit.  They may have made one in a lab somewhere,  but I'm sure Intel and the like have as well.  IBM isn't even making their own chips anymore (Glofo will be doing it now).    Making something in a lab is not even close to the same thing as figuring out how to reliably mass produce something and make it profitable.  I think I read that they used quadruple patterning to do it?  Lol  edit: no samsung"
hardware,3dghj0,Exist50,0,Thu Jul 16 14:58:25 2015 UTC,That deal was solely with Glofo. No Samsung.
hardware,3dghj0,paperscratcher,0,Thu Jul 16 16:04:43 2015 UTC,I read the same thing about the ibm 7nm chip
hardware,3dghj0,dukeofwhales,5,Thu Jul 16 03:43:57 2015 UTC,Should we expect a significant performance increase with Cannonlake in 2017?  My Core i7 920 from 2009 is still fine and am struggling to see any reason to upgrade anything except video cards in the near future.
hardware,3dghj0,Techonup,6,Thu Jul 16 03:54:27 2015 UTC,X58 Xeons are cheap and give a decent performance boost. LGA1366 is a very strong socket.
hardware,3dghj0,Maysock,4,Thu Jul 16 12:28:21 2015 UTC,X58 mobos are not sadly :c
hardware,3dghj0,Exist50,3,Thu Jul 16 13:02:06 2015 UTC,"Not likely. Maybe if Intel makes some more architectural changes, but I fear 10nm will be a repeat of Broadwell without that."
hardware,3dghj0,Anaron,3,Thu Jul 16 04:17:13 2015 UTC,"Are you a gamer? If so, then you'll notice a huge improvement if you get a Core i5-4690K. For everyday computing tasks, then you'll barely notice a difference."
hardware,3dghj0,christes,19,Thu Jul 16 06:52:24 2015 UTC,"huge improvement   That depends very much on the games, settings, and GPU(s) that it's paired with."
hardware,3dghj0,Anaron,7,Thu Jul 16 07:21:26 2015 UTC,You're right. Some games are less CPU dependent than others.
hardware,3dghj0,dukeofwhales,6,Thu Jul 16 07:34:32 2015 UTC,"How big a difference do you reckon? With a 1440p monitor and a GTX670, I feel like the video card is the bottleneck. I don't really play MMOs which I thought were generally the CPU-heavy genre."
hardware,3dghj0,Anaron,8,Thu Jul 16 07:47:33 2015 UTC,"You'll only notice an improvement in CPU-dependent games which are few compared to GPU-dependent ones. For now, you're good with the i7-920.   Also, at 1440p CPU performance becomes less important."
hardware,3dghj0,christes,3,Thu Jul 16 08:04:07 2015 UTC,"Have you overclocked your i7-920?  If not, it's worth considering.  Those chips generally overclock very well."
hardware,3dghj0,dukeofwhales,2,Thu Jul 16 09:51:45 2015 UTC,"I did for a while but was getting instability with even very mild overclocks. She's getting on in years and before I replaced the stock cooler the matted dust was causing her to run very hot, possibly for a number of years."
hardware,3dghj0,christes,2,Thu Jul 16 14:17:44 2015 UTC,"hmm, is it C0 or D0?  The C0 doesn't overclock nearly as well."
hardware,3dghj0,Trollatopoulous,1 point,Thu Jul 16 17:28:19 2015 UTC,Think like 10 fps difference. +- depending on your OC
hardware,3dghj0,RearmintSpino,6,Thu Jul 16 12:49:33 2015 UTC,"How does EUV process tech play into this, especially seeing how Intel said they weren't going to need to use it on their way to 7nm, but IBM did use it and they have working 7nm silicon?  http://mobile.extremetech.com/latest/222044-intel-forges-ahead-to-7nm-without-the-use-of-euv-lasers?origref=  Is this one of the major things Intel was wrong about?"
hardware,3dghj0,DrPizza,2,Thu Jul 16 04:01:48 2015 UTC,"There's no indication at all that IBM has a commercially viable EUV process. Putting a few transistors onto a wafer is not really the issue. The issue is commercially putting many billions of transistors on a wafer. IBM hasn't shown that, and there's no particular reason to believe they're anywhere closer to doing so than anyone else."
hardware,3dghj0,Triptolemu5,22,Thu Jul 16 18:41:27 2015 UTC,Hate to break it to you but Moore's law faltered awhile back. It's just been kind of a dog and pony show since about 05 or so. There's a reason the 7 year old first gen i7 is still not obsolete.
hardware,3dghj0,wtallis,54,Thu Jul 16 04:16:41 2015 UTC,"Moore's Law has held surprisingly well up until very recently. It's a fairly common misconception that Moore's Law relates to performance, but it actually pertains to transistor count. Transistor count has had a fairly linear correlation with performance until the last half of a decade or so, but has tapered off within recent years in favour of multi-core CPUs and improving power efficiency.  People have always clamoured that we've been approaching the limits of silicon, but it seems like this time they may be right.  Moore's Law also isn't completely bunk. IBM's racing to 7 nm with silicon-germanium transistor channels."
hardware,3dghj0,Mr_s3rius,14,Thu Jul 16 04:26:12 2015 UTC,"Moore's Law did also originally have an element of cost to it.  Dennard Scaling is what projected the performance and efficiency gains resulting from Moore's Law. It died a decade ago, leading to multi-core processors and more recently FinFETs."
hardware,3dghj0,III-V,6,Thu Jul 16 05:52:46 2015 UTC,"Some numbers, since I had a similar conversation two weeks ago.  Intel i7-920 (released 2008): 730m transistors  Intel i7-4770K (released 2013): 1.4b transistors  So the absolute transistor counts pale in comparison to what Moore's Law prescribes. When taking the different die sizes into account things look a bit better.  920: 263 mm2  4770K: 177mm2  So, if I haven't miscalculated, the transistors on the 920 are 35% as dense as on the 4770k. In other words, transistor counts with respect to physical die size ""only"" tripled, whereas Moore's Law would have suggested an eightfold increase (since it's roughly three 2-year periods inbetween the releases).  The i7-5960X (released 2014) has 2.6b transistors but a die size of 355mm2 which brings it up to 2.6x the adjusted transistor count of the 920. Actually a bit worse than the 4770K.  The 4770K's numbers should have been reached in 2011."
hardware,3dghj0,kinghajj,1 point,Thu Jul 16 10:02:40 2015 UTC,"Design goals are always changing, so you can't really compare two chips -- even by the same company, targeted at the same audience -- and use it to say if Moore's Law is or isn't being kept up with. The process density metrics are published by every manufacturer (although they can be a bit tough to find), and they're really the only fair comparison."
hardware,3dghj0,Mr_s3rius,-1,Sun Jul 19 01:18:43 2015 UTC,"That's because it's transistor density per watt. That brings the growth to about 4.5x. It'd probably also be better to look at Xeons, since that's where Intel puts their top-of-the-line tech."
hardware,3dghj0,seabrookmx,3,Thu Jul 16 19:31:51 2015 UTC,"Is it? I've checked the Wiki page and it doesn't seem to be related to power usage at all:   The complexity for minimum component costs has increased at a rate of roughly a factor of two per year. Certainly over the short term this rate can be expected to continue, if not to increase. Over the longer term, the rate of increase is a bit more uncertain, although there is no reason to believe it will not remain nearly constant for at least 10 years.  G. Moore, 1965   [In 1975, he revised the forecast doubling time to two years.]  (Funny enough, that means that Moore's Law (in its original form) actually holds true no matter what happens, since he only projected it for '65-'75; after that it grows more 'uncertain'.)  I've done a very shallow check of the modern Xeons but I'm not very familiar with these processors. Thus I'll refrain from doing any calculations for them."
hardware,3dghj0,ocshoes,7,Thu Jul 16 20:24:32 2015 UTC,"but has tapered off within recent years in favour of multi-core CPUs and improving power efficiency   These have nothing to do with reducing transistor count - in fact, adding more cores increases transistor count."
hardware,3dghj0,sir_drink_alot,15,Thu Jul 16 07:15:08 2015 UTC,"He's talking about performance's linear correlation with transistor count tapering off, not the transistor count itself, I think. Just didn't word it exactly right."
hardware,3dghj0,bb999,8,Thu Jul 16 07:31:55 2015 UTC,"Yeah, this is what I meant. We're seeing improvements with transistor count, but it's not in the form of massive raw performance gains."
hardware,3dghj0,DrPizza,1 point,Thu Jul 16 08:11:54 2015 UTC,"half decade, that's at least 10 half years... if not more!"
hardware,3dghj0,zzzoom,1 point,Thu Jul 16 18:14:44 2015 UTC,"Most people don't know IBM even makes CPUs on the same level as Intel and AMD. The fact is, they don't make x86 CPUs, but they're still making mainframes and supercomputer/servers which require pretty fast processors. For example, If you thought the Pentium 4 was crazy for having a stock 3.8GHz clock speed, IBM's Power6, which came out in the same timeframe, hit 5 GHz. It's not surprising IBM's technology is still on par or better than Intel's."
hardware,3dghj0,TetsuoSama,5,Thu Jul 16 17:12:22 2015 UTC,"It's not surprising IBM's technology is still on par or better than Intel's.   It isn't.  But IBM is content to build very large chips which dissipate a lot of power, and need substantial cooling, with relatively low yields. IBM can do this because IBM is not aiming at the mass market.  The first generation POWER8 processors, for example, were 649mm2 4.2 billion transistor monsters built on 22nm and running at up to 4.35 GHz. Their TDP was probably about 200W, maybe as much as 250W.  Intel doesn't make anything resembling that, because Intel's aiming at a mass market. And the thing is, Intel doesn't really need to. For example, compare SPECint_rate between http://spec.org/cpu2006/results/res2014q4/cpu2006-20141006-32081.html (POWER8: 4 chip, 64 cores per chip, 8 threads per core, running at 4.35 GHz, using 4 instances of SPECint per core) and http://spec.org/cpu2006/results/res2015q2/cpu2006-20150513-36359.html (Xeon E7-8867 v3: 8 chip, 16 cores per chip, 2 threads per core, running at 2.5 GHz, using 2 instances of SPECint per core). The POWER system base a base rate of 4130. The Xeon system has a base rate of 4920. (I picked these examples because they are both used to run 256 instances of the test, so are comparable from a ""machine size"" perspective).  Difference is, Intel's processors have close siblings that'll scale down to below 5W and can be used in a fanless handheld. And hell, for the Intel systems you can actually get pricing. Dell will sell you quad E7-8893 v3 with 1.5 TB RAM for about $90K before discounts, $65K with discounts, and that's before you even get the sales person on the line and start negotiating. IBM? Gotta pick up the phone, and you know that means they're ludicrously expensive."
hardware,3dghj0,nexusheli,14,Thu Jul 16 19:28:12 2015 UTC,"Moore's law is about transistor density, not performance. But even looking at performance, there has been steady progress over the last few years: If you look at server processors, where there isn't an integrated GPU and transistors are spent on more cores, everything has been fine.  The 7 year old first gen desktop i7 isn't obsolete because each of its cores is fast enough (frequency has reached a thermal and power efficiency limit, and improving IPC is hard) and consumer applications don't have much use for more cores, so Intel spent all those extra transistors from smaller processes on the integrated GPU."
hardware,3dghj0,zzzoom,-13,Thu Jul 16 05:49:01 2015 UTC,"Moore's law is about transistor density, not performance.   It's not about transistor density.  It's about transistor count."
hardware,3dghj0,andromeduck,4,Thu Jul 16 07:30:40 2015 UTC,"Do you know what density means?  Moore's Law:   the number of transistors per square inch on integrated circuits had doubled every year since the integrated circuit was invented.   Density, loosely, is the amount of something in a given space.  Moore's law is about density of transistors in a given space on a chip.  More transistors in the same space = higher density."
hardware,3dghj0,TetsuoSama,21,Thu Jul 16 14:01:34 2015 UTC,"You just solved Moore's Law then, all we needed was larger processor dies. /s"
hardware,3dghj0,Blaze9,2,Thu Jul 16 07:39:57 2015 UTC,Or bulk TSVs actually.
hardware,3dghj0,veyron3003,0,Thu Jul 16 13:31:40 2015 UTC,"Sarcastic but correct.  Yes, Moore's law can be satisfied by having larger processor dies (though I never said that's the only way).  Because it's not about processor density - it's about transistor count.  You were wrong.  It's not the end of the world.  Get over it."
hardware,3dghj0,xtothemess,0,Thu Jul 16 09:33:58 2015 UTC,Yup! Rocking an i7 950 and I can't complain about anything.
hardware,3dghj0,CeeeeeJaaaaay,-1,Thu Jul 16 15:38:54 2015 UTC,Well what helped them is that software was way behind the progress of hardware. Win 10 is the first OS to actually be with the times.
hardware,3dghj0,IWorkForTheInternet,4,Thu Jul 16 14:16:36 2015 UTC,I don't think anyone actually expected Intel to go from 14nm to 10 nm as fast as they went from 22nm to 14nm
hardware,3dghj0,slyf,1 point,Thu Jul 16 09:35:14 2015 UTC,"""Fast"". They had Haswell Refresh too last year."
hardware,3dghj0,defiance158,2,Thu Jul 16 22:49:50 2015 UTC,I hope Intel uses this pause to implement things like cryptographer DJB's suggestions for the Intel instruction set.
hardware,3dghj0,bphase,1 point,Thu Jul 16 15:09:55 2015 UTC,They are likely going to be calling out to their architecture R&D folks for new stuff to put in their CPUs so it definitely makes new instruction extensions more likely.
hardware,3dghj0,defiance158,3,Thu Jul 16 15:35:55 2015 UTC,"Forgive me if I'm wrong, but isn't this a bit sensationalist?  Here is the definition of Moore's Law from Wikipedia:   ""Moore's law"" is the observation that, over the history of computing hardware, the number of transistors in a dense integrated circuit has doubled approximately every two years.    Perhaps someone with more knowledge than I can break it down more thoroughly. But it seems to me that someone is confusing transistor count with process size. Or rather, confusing Moore's Law with Intel manufacturing goals.  Moore's Law seems to still be going strong regardless if Intel can get a high enough, efficient enough yield to get a consumer chip to market on time.   I don't think Moore's Law is dead.  I think that 1 company is failing to bring Moore's Law to the mass-produced consumer level because they lack competition and did not properly plan and invest for the future. Intel is facing technical hurdles brought about by business-influenced decision making.  I think that this pattern is going to be unbroken for quite some time, and I don't think 3 generations of 14nm consumer chips changes that at all.  To make an admittedly bad analogy; you wouldn't say that The Law of Gravity faltered when a person catches himself on a handrail after falling down half a flight of stairs.  It's incorrect to say Moore's Law falters because Intel breaks it's own tick-tock cycle."
hardware,3dghj0,CommanderArcher,2,Thu Jul 16 16:55:13 2015 UTC,"It's not just Intel. They're just referenced usually because they're  ahead of everybody else. Look at TSMC; they've been stuck at 28 nm for about 4 years now and so transistor density hasn't increased much on their progress. I'm not sure how well Samsung is doing, but it's unlikely anybody will be able to keep up with Moore's Law in a bit."
hardware,3dghj0,magusg,1 point,Thu Jul 16 19:00:00 2015 UTC,"Going back to the picture that I linked; look at the cluster of the AMD K5, Pentium II, Pentium III, AMD K6, AMD K6-III, and AMD K7. This looks like a case of industry wide failure to uphold Moore's Law, because transistor count didn't meet the trend line. This was pre-2000. I'm sure there were ""Moore's Law is ending"" doom news then too.  But we can clearly see that Moore's Law didn't fail, it ultimately held true. As soon as the manufacturing tech got on the right track, future processors met and even surpassed the expected Moore's Law trend line.  If Intel releases 3 iterations of 14nm, all that (likely) represents is a temporary shift of processors to the right side of the trend line which is ultimately going to look ridiculously insignificant only 10 years from now. If I was a betting man, we will almost assuredly make the technological breakthroughs in manufacturing to get back on or surpass (to the left of) the trend line yet again.  The article uses words such as ""tick-tock shattering"" and ""faltering Moore's Law"", when in reality all that truly damn means is that the next Intel processors are just going to be half a milimeter to the right side of the trend line on that graph."
hardware,3dghj0,Monkeyfeng,2,Thu Jul 16 19:25:39 2015 UTC,"the problem that they are going to and have already run into is that silicon causes problems the smaller you go, you actually have to make a mix of things to go to 7nm and after that you need a whole new material such as graphene"
hardware,3dghj0,PeregrineFury,4,Thu Jul 16 23:55:56 2015 UTC,You mean they're gonna milk another product cycle out of this fab process? Color me surprised.
hardware,3dghj0,imallin,1 point,Thu Jul 16 10:05:06 2015 UTC,Moore's law runs into the law of diminishing returns!
hardware,3dghj0,jdw101,1 point,Thu Jul 16 22:21:52 2015 UTC,It was bound to happen eventually.
hardware,3dghj0,DeeJayDelicious,1 point,Thu Jul 16 12:19:40 2015 UTC,It's pretty hard to count Broadwell as a generation.
hardware,3di9st,Deathspawner,5,Thu Jul 16 14:19:42 2015 UTC,"When I was shopping for my NUCs, I noticed a review of the 4th gen Core i5 NUC that mentioned doing the same:  http://www.newegg.com/Product/SingleProductReview.aspx?ReviewID=4201462  I have 3 in a test cluster config. I saw a ~12.5% improvement in OpenCL performance just by increasing the TDP.  They've been running at ~87C constantly for a couple months without issue.  I like the NUC form factor a lot for a portable /r/homelab setup.  Just wish they'd add a 2nd NIC or something so I could use one as a nat/firewall box. I don't like using a USB nic ever since I had my network taken down by my cats batting the USB nic around like a toy."
hardware,3di9st,IWorkForTheInternet,2,Thu Jul 16 15:05:35 2015 UTC,"They've been running at ~87C constantly for a couple months without issue.   That's impressive, and good to hear. I'd start shaking if I saw my ordinary desktop CPU running at that :D   I don't like using a USB nic ever since I had my network taken down by my cats batting the USB nic around like a toy.   Haha. I haven't had that, per se, but I do find that if you are a very heavy traffic user, you're inevitably going to have issues. When I was forced to use one for my regular desktop, sometimes the entire NIC would just disappear to the computer, requiring me to either replug it or reboot (usually the latter)."
hardware,3dhmaq,ZanQuance,11,Thu Jul 16 10:00:00 2015 UTC,"How i remember A3d. what beautiful positional audio :)  this card wasn't ""forced into abstract obsolescence"" creative killed this company (the bastards) bought up all the IP and afaict buried it.  though i don't really see the point in reviving this beautiful card, A3d wouldn't work on modern games unless you wrote some sort of sound API -> A3d shim."
hardware,3dhmaq,shrewduser,4,Thu Jul 16 13:27:18 2015 UTC,"Right, my fault I didn't state it clearly. I was speaking of any sound hardware which can still function today, but lacks the software support.  As per bringing back the Vortex2's. They offer something no other soundcard has (that I am aware of), real hardware HRTF (all others are DSP mixed). Having the API hook modern sound API's is planned and shouldn't pose major problems. This way you can play games such as HalfLife2 (Miles) or Skyrim (XAudio2) with real HRTF/Reverb by letting A3D do all the work."
hardware,3dhmaq,shrewduser,1 point,Thu Jul 16 21:32:58 2015 UTC,that would be fantastic! i've been waiting for something as good as A3d for years (got so excited about amd's trueaudio then heard nothing more about it)  i'm still very confused as to why no other positional audio sounded as good.
hardware,3dhmaq,continous,1 point,Fri Jul 17 01:41:04 2015 UTC,"Hardware HRTF filters vs software. Same as emulating audio chips like the OPL3 and SID, software only gets you so far but never quite the same level of quality. This is also why programmable DSP's aren't perfect for HRTF either, they are entirely dependent on the quality of the software driving them."
hardware,3dhmaq,continous,2,Fri Jul 17 07:38:51 2015 UTC,"Idk about that. While it definitely is more efficient to have a separate chip for that work load, many processors could handle it I believe."
hardware,3dhmaq,continous,1 point,Sat Jul 18 07:24:47 2015 UTC,"Of course, however dedicating a chunk of CPU resources to applying HRTF is something devs wanted to avoid. Even with faster CPU's it's still a resource hungry operation when dealing with many sound sources. You'll get performance if you only apply light HRTF but it doesn't sound as nice.  You can emulate anything on a CPU, but sometimes it's nice to have the real hardware on hand working for you."
hardware,3dhmaq,bulgogeta,1 point,Sat Jul 18 16:51:54 2015 UTC,"Yes, which I guess is the core of my point. Is it really a meaningful investment or would a more powerful processor better fill the role? I guess we'll never know."
hardware,3dhmaq,zxcdw,1 point,Sat Jul 18 17:10:12 2015 UTC,"Dedicated filters or a DSP will eventually make its way into a CPU. AMD seems to like merging techs together, so they will probably do it. I guess AMD TrueAudio is close to what your talking about for now."
hardware,3dhmaq,ZarK-eh,2,Sat Jul 18 22:30:15 2015 UTC,I guess I'm just trying to see if this is a real reason to justify a soundcard over my adequate on-board sound.
hardware,3dhmaq,Spacebotzero,1 point,Sat Jul 18 23:38:28 2015 UTC,"Once this project is complete you can compare the differences and see if it fills any need to have role :) Onboard audio has certainly come a long way since then.  If you would like to hear what A3D once sounded like, check out the many Aureal 3D videos we all uploaded to youtube.  Hira and Philscomputerlab have some good ones.  [A3D 2.0 rooms tech demo] https://www.youtube.com/watch?v=-oSlbyLAksM  [Quieter environment, easier to hear positional audio] Unreal Tournament: https://www.youtube.com/watch?v=U38uTnbarqk  [HL is always fun] HalfLife:https://www.youtube.com/watch?v=AAOcdfCN-RU  [Compares with and without A3D] Quake3:https://www.youtube.com/watch?v=S2OPnEsbRrc"
hardware,3dhmaq,shrewduser,7,Sun Jul 19 08:29:12 2015 UTC,A3D FOR LIFE  FUCK CREATIVE AND THEIR LAWSUITS  Best positional audio ever
hardware,3dhmaq,Spacebotzero,4,Thu Jul 16 13:54:35 2015 UTC,"Remember those beloved Aureal Vortex2 soundcards   Yes, and the IRQ conflicts on Windows XP on Asus A7V-E motherboard. BSOD and IRQL_NOT_LESS_OR_EQUAL, never forget. Without BIOS update it'd just hang.  It was just the other day that I was trying to remember what on earth was this cursed sound card that I had, and yup, it was Aureal Vortex2. :)"
hardware,3dhmaq,Spacebotzero,4,Thu Jul 16 10:47:39 2015 UTC,"Aye, those bsods are caused by the beta v5.10.2500.49 drivers. The ones microsoft released with xp vanilla work well. au8830.sys 730kb [v5.12.1.2560] is the one you want, avoid all the others. Poor cursed Vortex2 :( they need some tlc. [edit]Added Version info"
hardware,3dhmaq,thinkythought,2,Thu Jul 16 13:01:00 2015 UTC,/r/retrobattlestations might enjoy your work
hardware,3dhmaq,thinkythought,2,Thu Jul 16 15:03:30 2015 UTC,I loved A3D. I use to sit and chat with the folks from A3D on mIRC. I remember when he started talking about SLI-ing A3D cards together.
hardware,3dhmaq,thinkythought,1 point,Thu Jul 16 23:54:25 2015 UTC,really? i wonder for what purpose... maybe you could increase the fidelity of the simulation.
hardware,3dhmaq,MumrikDK,2,Fri Jul 17 03:17:59 2015 UTC,"If I remember correctly, it was the ability to add more channels and share the processing load. So yes...perhaps in a sense...more fidelity of the simulation. In fact I think you nailed it."
hardware,3di3jv,Muorman,8,Thu Jul 16 13:26:37 2015 UTC,"The SandForce SF3000 series has become the unicorn of the SSD industry. For the past two years there has been a lot of hype about the new controller, but Seagate/SandForce has kept missing deadlines one after the other. Initially the third generation SandForce controller was supposed to ship in mid-2014, but obviously that didn't happen. Next we heard Q4'15, which was then changed to mid-2015 and the latest word I got at Computex is expected mass production in Q4'15, meaning that we could see first products shipping in early 2016.   http://www.anandtech.com/show/9362/seagate-announces-sandforce-sf3500-ssd-controller-series-mass-production-expected-in-q415"
hardware,3di3jv,III-V,5,Thu Jul 16 14:00:01 2015 UTC,"Thanks for posting, I had missed that news."
hardware,3dhex2,Tracypaltrow,4,Thu Jul 16 08:11:21 2015 UTC,"This has been long overdue, 16GB just isnt enough, especially with all the preinstalled apps and such"
hardware,3dhex2,olavk2,1 point,Fri Jul 17 15:39:18 2015 UTC,"Hmm seems like the SSD version of a SSD cached HDD. But a TLC NAND with a SLC cache. I'm sure the prices of memory on smartphones won't change but they do say they upped the minimum capacity of this ssd from 16 GB minimum to 32 GB minimum. I don't know how prominent sandisk is in smartphones so I can't say weather or not consumers are likely to actually end up with a larger SSD.  As with ssd cached hdd I don't like it. I'm fine when you use DRAM, FRAM, MRAM as cache due to their immense speeds they work really well. These mid speed with upper mid speed as a cache is bothersome because when I don't have control of ""high"" and ""slow"" speed like separate HDD and SSD then I don't know if the benefits I want out of the faster memory is even realized. Thing's like booting and launching applications."
hardware,3dgus5,zmeul,5,Thu Jul 16 04:10:11 2015 UTC,"Pretty stable income, they know what they're doing :P."
hardware,3dgus5,makegr666,8,Thu Jul 16 07:54:41 2015 UTC,And yet their stock has to ride a roller coaster because big investment firms make their money by spreading panic.  They hint that declining sales in PC's is going to ruin the company.
hardware,3dgus5,Thunder_Bastard,1 point,Thu Jul 16 13:28:55 2015 UTC,To be fair there was quite a large dip in the graph around 2010.
hardware,3dgus5,CykaLogic,1 point,Thu Jul 16 23:28:18 2015 UTC,buy some stocks while everyone is panicking
hardware,3dgus5,NLWoody,1 point,Fri Jul 17 10:33:00 2015 UTC,Doesn't Intel make the majority of their money on enterprise chips anyway?
hardware,3decwd,SOOcreative,23,Wed Jul 15 16:43:48 2015 UTC,"""Setup questions"" which is the name of those settings get saved in NVRAM on a non write locked part of the spi chip such as one of these http://www.digikey.com/product-detail/en/W25Q256FVFIG/W25Q256FVFIG-ND/4037430 right next to the bios. The RTC and a few things to support legacy are saved in cmos but in general it's mostly unused. Computer manufacturers can also choose to store some information there."
hardware,3decwd,AmericanMegatrends,5,Wed Jul 15 16:58:58 2015 UTC,Do you work for AMI?
hardware,3decwd,sishgupta,1 point,Thu Jul 16 14:09:31 2015 UTC,"I am employed by AMI in an engineering role.  Comments or opinions expressed on the Reddit are those of their respective contributors only. The views expressed on Reddit do not necessarily represent the views of AMI, its management or employees. AMI is not responsible for, and disclaims any and all liability for the content of comments written by contributors to Reddit."
hardware,3decwd,AmericanMegatrends,1 point,Mon Jul 20 23:41:34 2015 UTC,Thats awesome! Glad to have you here.
hardware,3decwd,sishgupta,5,Mon Jul 20 23:47:39 2015 UTC,"that one time, the kid that just graduated ITT, and no one else could figure out how to fix the computer, and I took out the battery for a few and put it back in and everything booted up fine. my one shining moment."
hardware,3decwd,ericb45696,4,Wed Jul 15 23:41:47 2015 UTC,Did you take your rightful dump on his face after that? That's one of the rules when a tech school boy fuck up
hardware,3decwd,AssCrackBanditHunter,5,Thu Jul 16 11:14:39 2015 UTC,"My mobo has an UEFI and still uses a CMOS battery, so I figure it also saves to CMOS RAM.  It's an average H87 though, higher end boards might use EEPROM or similar these days. you could hit up manufacturers of higher end boards like Asus, MSI, Gigabyte etc, maybe they can tell you."
hardware,3decwd,HavocInferno,1 point,Wed Jul 15 20:24:49 2015 UTC,My Z97 mobo still has a CMOS battery and therefore presumably CMOS RAM too.  It's a low end ASRock Z97 though.
hardware,3decwd,valaranin,2,Thu Jul 16 02:20:04 2015 UTC,"Yes, there is still a battery. The UEFI is loaded into active DRAM just like BIOS, hardware hasn't changed at all except the SPI bus has gotten wider and ROM size has increase from 1MB to 16MB."
hardware,3decwd,xtothemess,1 point,Thu Jul 16 02:09:56 2015 UTC,"Most consumer boards I believe Will only need 8 MB parts. Most servers use 16 MB parts but Skylake will likely use 32mb parts. Keep in mind the spi part holds a lot of things such as the bios, the Intel ME region, nvram, built in shell, and in the future possibly more."
hardware,3decwd,spellstrike,1 point,Thu Jul 16 02:32:17 2015 UTC,"Well for X99 it has been 16MB, before it was mostly 8MB."
hardware,3decwd,xtothemess,2,Thu Jul 16 09:34:20 2015 UTC,"X99 cpu's, I believe, are cut down server chips from the silicon standpoint. Everybody wins."
hardware,3decwd,AmericanMegatrends,1 point,Mon Jul 20 23:50:39 2015 UTC,Yup!!!! Do you work for AMI?
hardware,3decwd,xtothemess,4,Tue Jul 21 00:57:00 2015 UTC,That's a lot of acronyms...
hardware,3decwd,Moaz13,39,Wed Jul 15 19:05:03 2015 UTC,"CMOS = Complementary metal-oxide-semiconductor. A way of making computer chips.  CMOS-RAM a little memory chip in computers powered by a battery, that holds very basic booting information and settings of the computer. If you remove the battery or it gets empty, the saved data is lost.  UEFI = Unified Extensible Firmware Interface. The basic program your computer runs when you start it, checks all the hardware etc. Successor of the BIOS  BIOS = Basic Input Output System. An older version of the first thing your computer runs when starting. Manufacturers switched from BIOS to UEFI a while ago  EEPROM = Electrically erasable programable read only memory. Contrary to it's name it's not read-only, but can be erased electronically, by stuff inside your computer. It's usually only read though.  OP wants to know if new computer that have an UEFI still use the old battery powered chips, or if they switched to using EEPROMs, which have the advantage of NOT needing a battery, but still being able to hold data in a non-volatile way (after you power off your PC). These CMOS-RAM chips were typically used since forever to save these basic settings."
hardware,3decwd,Ramuh,2,Wed Jul 15 19:19:50 2015 UTC,Thanks a lot for that info! I'm saving up for a new PC so this will be helpful in the building process I'm sure.
hardware,3decwd,Moaz13,13,Wed Jul 15 19:32:18 2015 UTC,"The only acronyms you'll really be using are UEFI or BIOS, all the other things aren't really something you would need to know to build a pc. But its always good to know them!"
hardware,3decwd,spencer32320,5,Wed Jul 15 19:37:24 2015 UTC,"The cmos-ram stuf and the batery explanation are useful in case you screw up an  overclock. Can say it by experiente, read half a manual cause didnt knew what t look for."
hardware,3decwd,americosg,1 point,Wed Jul 15 20:25:55 2015 UTC,You're not the only one.  I have a fair amount of experience building PCs for myself and friends over the last 10 years and didn't know what EEPROM was...
hardware,3decwd,PhilipK_Dick,1 point,Wed Jul 15 21:38:28 2015 UTC,"Technically flash memory (and SSDs) are a kind of EEPROMs. That's why SSDs erase the way they do (in large blocks), because they can't be directly re-written byte by byte."
hardware,3decwd,Baloroth,6,Thu Jul 16 01:19:32 2015 UTC,"Better to think of it that NAND is a type of EEPROM type memory. Although that doesn't really do it justice on the technicalities...  Lets see how well I can explain this:  EEPROM is generally really-really low level (and generally small, ~4Meg is ""huge"" in this space) data storage. How it works so long as it supports the EEPROM standards (basically besides the wiring/access protocols, few writes, but many many reads) it doesn't matter how it does this. Before this was via some technology that could only sustain ~10,000 writes total, now days we have flash memory technology to thank (first NOR, then yes, NAND based) for tens of millions of re-writes.  Its major point is for the very-very low level initial bootstrapping code, lets take desktop usages in BIOSes/UEFI systems:   EEPROM stores BIOS code (with BIOS settings in CMOS-RAM) EEPROM sortes UEFI ""First Stage"" start-up which then it allows loading from other locations (generally a larger flash chip on the order of ~64-256MB or in some rare cases from HDDs) After UEFI ""second stage""/BIOS your bootloader takes over. UEFI has its custom UEFI partition and code per operating system. BIOS uses things related to the MBR to pass off to the bootloader (which then lets you choose your OS or continues to the first choice if it is the only one generally)   Blarg, I don't remember as much about how boot works on x86 as I thought. I probably butchered quite a few details here..."
hardware,3decwd,admalledd,3,Wed Jul 15 23:26:13 2015 UTC,Also has a UEFI partition on your hard disk.
hardware,3decwd,kanliot,12,Thu Jul 16 00:42:50 2015 UTC,"That only stores bootloaders, not settings. And if your operating system is installed to a filesystem that your UEFI has support for, then the bootloader can be on that filesystem instead of on the EFI system partition. It's pretty common for the EFI system partition to be empty or very nearly so."
hardware,3decwd,wtallis,1 point,Wed Jul 15 18:21:17 2015 UTC,"In many cases, you can boot with the cmos battery removed these days."
hardware,3decwd,spellstrike,3,Wed Jul 15 20:48:22 2015 UTC,But then you get cmos errors every boot q.q
hardware,3decwd,DrDan21,2,Wed Jul 15 19:51:05 2015 UTC,Hit F1 to continue
hardware,3decwd,itsaride,2,Wed Jul 15 20:45:18 2015 UTC,"depends on the platform and bios configuration. If you do odd things like I do, many-times the system will not boot at all without the cmos being cleared each time. In those cases, it's often easier to leave the cmos battery removed.  go open up windows system event viewer, Cmos would probably be the least of your concerns.  Though I can't remember if ""fast boot"" (lets you not check all of your hardware configuration every boot to boot faster) uses cmos so if you use that you need the battery.  YMMV if your oem does weird things like actually use the cmos for anything other than the RTC(even that is ignored by windows in many cases)."
hardware,3decwd,spellstrike,1 point,Wed Jul 15 21:26:11 2015 UTC,RTC   It's all about that High Precision Event Timer now :D
hardware,3decwd,DrDan21,0,Wed Jul 15 21:54:47 2015 UTC,These days? You have been able to do this for at least a few decades.
hardware,3decwd,homingconcretedonkey,1 point,Wed Jul 15 22:13:05 2015 UTC,"When you ask about ""UEFI"" in general, there is no universal location for all BIOS.  You can implement a UEFI BIOS that stores all settings on a non-volatile I2C EEPROM, or in a region of SPI flash, or in CMOS-RAM."
hardware,3dfmf8,LiberDeOpp,7,Wed Jul 15 22:01:40 2015 UTC,I'm wondering how noticeable the coil whine would be when installed in a proper case as opposed to on a test bench with a microphone right next to it.
hardware,3dfmf8,Seclorum,8,Wed Jul 15 22:30:33 2015 UTC,I think he was just pointing out how companies haven't nailed down coil whine yet. The card was still really quiet.
hardware,3dfmf8,Seclorum,4,Wed Jul 15 22:53:12 2015 UTC,"Yeah at first I couldn't hear it until he started switching it out live and going back and forth a few times before GTA5 broke on him.   I've never known what that noise was properly called until recently. I just attributed it to electronic noise that most electronics have been making for years. I could close my eyes in Wallmart/Kmart and find the electronics section just by seeking out that whine. :)   Most of the time I tune it out unless it's made very obvious by someone.   But beyond the whine, the card is DAMN quiet. If I wasn't paying attention to the video I wouldn't even have noticed the fans turning on."
hardware,3dfmf8,buildzoid,3,Wed Jul 15 23:37:05 2015 UTC,Most of the time companies try to tune it above human hearing or insulating it. People say its a lack of quality control but there is a definite cost vs benefit analysis going on there.   Really it doesn't bother most people enough to warrant rmas or choosing another model but every card sounds different depending on construction and components.
hardware,3dfmf8,Hombremaniac,1 point,Wed Jul 15 23:42:43 2015 UTC,Which honestly blows my mind because coil whine is literally just the iron core of an inductor bouncing at an audible frequency inside the coil. There are so many different ways to fix this yet coil whine continues to be a problem.
hardware,3dfmf8,Seclorum,1 point,Thu Jul 16 17:32:11 2015 UTC,It's all about money.
hardware,3dfmf8,Pyrominon,3,Thu Jul 16 18:01:11 2015 UTC,"I heard no damn coil whine! Anyway in closed case, I doubt it would be a problem."
hardware,3dfmf8,Nerdsturm,1 point,Thu Jul 16 08:00:24 2015 UTC,I could hear it but it was real subtle and only noticable when your deliberately forcing a load on and off the card. but it was real damn subtle.
hardware,3dfmf8,rationis,10,Thu Jul 16 19:39:58 2015 UTC,"I like how he states that the 980 retails for $500, but uses the $580 EVGA Classified version in the review to compare the two cards."
hardware,3dfmf8,Pyrominon,4,Thu Jul 16 02:13:52 2015 UTC,"When he does the overview he mentions that the 980 he tested was the same price as the Fury, as well as that relying on the OC abilities of the 980 isn't necessarily representative since many users won't see the same performance."
hardware,3dfmf8,Pyrominon,5,Thu Jul 16 02:44:51 2015 UTC,"To add to that, it appears he also won the silicone lottery with his 980. I see users and comments asking him how the hell he got it to  1580Mhz when a lot of them can only manage 1450Mhz."
hardware,3dfmf8,Pyrominon,1 point,Thu Jul 16 09:37:26 2015 UTC,"If he's going advise people based on the MSRP of the reference versions of each card, then he should run both cards at their reference speed, not factory OC'd. Personal OC results are fine as long its bundled with a disclaimer."
hardware,3dfmf8,Tetsudothemascot,1 point,Thu Jul 16 03:00:46 2015 UTC,"then he should run both cards at their reference speed, not factory OC'd.    Did you even read my post? This is strictly for performance benchmarks. The Fury doesn't have a reference cooler so obviously he can't do an apples to apples comparison for temps/acoustics."
hardware,3dfmf8,Pyrominon,1 point,Thu Jul 16 03:22:54 2015 UTC,"You're completely missing my point. He describes the 980 as a $500 card, then represents it's stock (aka factory OC) and OC performance with a version of it that costs $580. His 980 is unusually good even amongst other Classified's (1580mhz OC) . While he does the same with the Fury, the Tri-X has a significantly smaller factory OC and price premium so the discrepancy isn't nearly as big."
hardware,3dfmf8,Hombremaniac,2,Thu Jul 16 03:59:43 2015 UTC,"If you really want an apple to apple comparison, he should do benchmarks comparison between 2 cards WITHOUT adding any extra voltage. Then OC them then bench but then it is not apple to apple anymore. Also, 1580Mhz is extremely rare, it is highly likely that he has a very well binned GPU, furthermore the Fury came out for about 2 days fyi. Also please post your Unigine valley score with your 1500Mhz 970."
hardware,3dfmf8,Mighty-Tsu,1 point,Thu Jul 16 04:04:48 2015 UTC,"I think using the factory OC performance of a $580 card to represent the stock performance of a $500 card is an oversight, i wouldn't go as far as calling it bias. Like i said he does the same thing for the Fury, but it's not an apples to apples comparison because the Fury has a much smaller factory OC and price premium over reference. Other reviewers like anandtech.com will down clock to reference clocks when forced use aftermarket cards to compare between brands."
hardware,3dimk8,ryerocco,3,Thu Jul 16 15:58:24 2015 UTC,"Except the 5820K isn't 5th generation but it's Haswell-E instead. Pretty weird naming from Intel for Haswell-E, maybe they knew from the beginning there wouldn't be Broadwell on desktop and just decided to use 5xxx for the Extreme line."
hardware,3dimk8,AndreyATGB,9,Thu Jul 16 17:31:38 2015 UTC,They have done that in the past too.  Ivy Bridge-E is I7-4xxx with an 'X' suffix.  I think they want to make the Extreme line seem one tier above the 'normal' CPU's of that same architecture.
hardware,3dimk8,aziridine86,1 point,Thu Jul 16 18:03:53 2015 UTC,Yeah but the 5820K isn't an X processor which suggests it's some kind of broadwell processor simply reading the naming scheme described by the article. I suppose there's no x8xx normal processor though.
hardware,3dimk8,AndreyATGB,5,Thu Jul 16 19:49:02 2015 UTC,There's the 4820K Ivy and 3820 Sandy
hardware,3dimk8,rockycrab,2,Fri Jul 17 00:45:14 2015 UTC,Not accurate.
hardware,3dimk8,spellstrike,1 point,Thu Jul 16 18:58:18 2015 UTC,"And the famous i7 980x??? Generation 0?  There is too many exceptions to this article...usually Intel XE market runs one ""generation"" number above... That does not mean that it is that generation.  Now we have the XE, P2, P1 that are Haswell-E, and P1+ and MS2+ that are Broadwell, and P1, P1K, MS1, MS2, and MS2K are Haswell Refresh... Yet we have Haswell-E AND Broadwell with 5 prefix... So... Intel is weird..."
hardware,3dcrcu,MajorRIce,41,Wed Jul 15 06:39:18 2015 UTC,"Guys, I got it!  At first, I was scratching my head and wondering who spends that amount of money for a CPU but not a dollar for a dedicated GPU. I came up blank. No one in their right mind would do that.  This is a desktop CPU that's good at gaming for PCs without dedicated a GPU. Gaming PCs without real GPUs – is this madness?  No, it's the next iMac.  This is a CPU designed to be stuffed into an iMac. For the rest of us, the 4790K is still available, it's cheaper and it's still king of the hill in single-threaded performance that's so important for gaming.  Edit: Also, the relatively low power consumption makes it ideal for the badly ventilated crammed case of an iMac. In a regular PC, even a 4790K is easily cooled but my iMac at work heats up quite a bit under load, so a 65 watt CPU looks good in this regard, too. And while it may be roughly a 25% decrease in power over the 4790K's 88 watts, it's still just 23 watts or the amount of power a couple of drives and vents sip up. Saving 23 watts on the CPU side is far less relevant than saving over 100 watts on the GPU (comparing a 980 to a 290x, for example)."
hardware,3dcrcu,hibbel,2,Wed Jul 15 13:50:24 2015 UTC,Think the next apple will have broadwell and not wait for Skylake? Will the Skylake cpus have the same igpu?
hardware,3dcrcu,luger718,-10,Wed Jul 15 15:08:41 2015 UTC,"Apple is almost always behind the tech curve, sometimes up to a year. Doesn't matter one bit, as people will still buy it."
hardware,3dcrcu,Logical_Phallusy,22,Wed Jul 15 15:15:27 2015 UTC,"IIRC, Apple used Haswell pretty much the day it came out."
hardware,3dcrcu,Exist50,0,Wed Jul 15 17:07:19 2015 UTC,"Look at their refresh cycles, they'll go a year and a half at least before updating. They only recently upgraded from the GT 750m."
hardware,3dcrcu,Logical_Phallusy,6,Wed Jul 15 17:08:37 2015 UTC,"And yes, the new 15"" Macbook's top GPU is based on Cape Verde -_-"
hardware,3dcrcu,Exist50,1 point,Wed Jul 15 17:36:51 2015 UTC,"IIRC, the 5K iMac was also the the launching platform of Tonga."
hardware,3dcrcu,Soytaco,3,Thu Jul 16 02:34:00 2015 UTC,"Don't think so. Full Tonga, maybe."
hardware,3dcrcu,Exist50,3,Thu Jul 16 03:06:01 2015 UTC,They did release one of the first 5k displays (with a computer in it!). I don't think anyone buys Macs for the computational bits but for practically everything else (primarily aesthetics and ecosystem).
hardware,3dcrcu,milo09885,2,Wed Jul 15 20:19:44 2015 UTC,Ecosystem being the big one. Unix-like for nerds out of the box while still being useful as a daily driver with support from most business applications. Also seems amazing if your family has bought into the ecosystem. Similar logic follows for some of my friends whose entire families are in the Microsoft ecosystem.
hardware,3dcrcu,ItzWarty,1 point,Thu Jul 16 23:32:41 2015 UTC,Nearly all Macs come with a PCIe ssd and they are usually the first to use a new Intel refresh.
hardware,3dcrcu,NoAirBanding,1 point,Wed Jul 15 20:32:55 2015 UTC,Why they but garbage? No one knows. A laptop with a desktop cpu costing upwards of 3k is ridiculous
hardware,3dcrcu,reddit_reaper,27,Thu Jul 16 23:37:57 2015 UTC,"I'm more impressed Intel put together a graphics core that is fucking competitive on both power consumption and horsepower to offerings by amd and nvidia.  Seriously, that is one hell of an igpu for a company that doesn't focus on that sort of stuff."
hardware,3dcrcu,SR71-boss,61,Wed Jul 15 07:50:21 2015 UTC,Seems these days that the iGPU actually is more of their focus than anything.
hardware,3dcrcu,Stingray88,23,Wed Jul 15 08:04:01 2015 UTC,The goal is powering 4k and 5k displays.
hardware,3dcrcu,medikit,9,Wed Jul 15 13:27:12 2015 UTC,That is true. And that's a worthy goal.
hardware,3dcrcu,Stingray88,4,Wed Jul 15 15:30:50 2015 UTC,Just this past year.... And they've already caught up this much.... I guess that's what happens when a company that big and so full of geniuses throws manpower at something
hardware,3dcrcu,SR71-boss,20,Wed Jul 15 08:06:47 2015 UTC,Yeah the iGPU on Broadwell 14nm is already half of the damn die. One can only imagine what we can expect with Ice Lake 10nm...  Would be nice if they would use the space savings for some mainstream 6 core and 8 core chips.
hardware,3dcrcu,Stingray88,5,Wed Jul 15 08:15:15 2015 UTC,Would be nice if they would use the space savings for some mainstream 6 core and 8 core chips.    They tend to save that for their -E releases.
hardware,3dcrcu,Seclorum,1 point,Wed Jul 15 17:14:33 2015 UTC,"I know they have thus far, but if they did offer 6 core and 8 core chips in the mainstream lineup, they could offer 12, 16, 18, etc. core chips in the E series.  It's not like they wouldn't have an option."
hardware,3dcrcu,Stingray88,6,Wed Jul 15 18:04:16 2015 UTC,Well technically they do have chips with that kind of core count that fit in the -e series socket. The Xeon chips.   But Intel charges a hefty premium for them.
hardware,3dcrcu,Seclorum,1 point,Wed Jul 15 18:44:27 2015 UTC,"Right, but if they were increasing core count on the mainstream line, you would see that reflected down the line. So more cores for mainstream, more cores of E series, and more cores for Xeon. That's what I'm suggesting, and hoping."
hardware,3dcrcu,Stingray88,3,Wed Jul 15 19:44:09 2015 UTC,"Eh, mainstream doesn't really benefit from more cores.   4+HT is plenty for people, considering most want higher individually threaded performance rather than massive multithreading.   By keeping the cores down to sensible levels it makes the chip more attractive for different uses, such as laptops and tablets. Especially with such efficient IGPU performance so that both dont need discrete GPU chips for most use cases.   But for a desktop chip, it definitely feels like their mainstream line could start transitioning to 6-8 cores with a more sensible sized IGPU for edge cases.   But without viable competition or some compelling software need for massive multithreading, there really is no point for Intel to slap more cores onboard their mainstream chips."
hardware,3dcrcu,Seclorum,2,Wed Jul 15 20:42:19 2015 UTC,"The large majority of people don't need an i7 either, but Intel offers those too.  Honestly I just want a 6 or 8 core chip without paying the price for the E-series. It would be nice for my editing rigs.  Also, 5 years ago very little software could utilize multiple cores and multithreading very well. Today, because the chips were available, a ton of software can use it well. It's a chicken/egg scenario, something has to come first. Whatever Intel makes available is what software is going to start gearing itself toward."
hardware,3dcrcu,Stingray88,2,Wed Jul 15 21:25:45 2015 UTC,"6 core i5s would mean the end of AMD, combined with NVIDIA's push to kick AMD out of the graphics market."
hardware,3dcrcu,CykaLogic,2,Thu Jul 16 23:44:50 2015 UTC,This would be terrible for everyone involved.
hardware,3dcrcu,an_angry_Moose,3,Fri Jul 17 16:02:28 2015 UTC,5820k :)  Seriously - once DDR4 ram comes down in price it's worth it. I grabbed one because I am a fool and it's really just ridiculous overkill for basically everything except running Blender simulation.
hardware,3dcrcu,Modna,3,Wed Jul 15 20:59:19 2015 UTC,"Or when you can throw tons of transistors at the problem because of your leading edge fabs. Also, that eDRAM doesn't come cheap."
hardware,3dcrcu,Exist50,1 point,Wed Jul 15 16:05:22 2015 UTC,"Right, no real gains to be made on the general cpu side so may as well.  Bummer."
hardware,3dcrcu,jdw101,1 point,Wed Jul 15 19:18:53 2015 UTC,Yeah they haven't done shit except increase their iGPU performance in...how many years?  I don't consider Haswell / Refresh anything to write home about.
hardware,3dcrcu,innerknowing,1 point,Thu Jul 16 00:48:49 2015 UTC,"To be fair, CPU performance has improved with every generation. It's just not a lot."
hardware,3dcrcu,Stingray88,3,Thu Jul 16 00:49:41 2015 UTC,Yeah to be fair they did provide something while they kept taking our money.  But thanks to the lack of competition we haven't seen anything aside from TIM changes and single percentage IPC improvements in longer than I care to remember.  Oh and those whole iGPU things that do nothing but take away my overclocking headroom.
hardware,3dcrcu,innerknowing,3,Thu Jul 16 01:37:18 2015 UTC,"Given how scalable GPUs are, do you think that they would be able to make a competitive standalone gpu if they wanted to? Looks like it to me."
hardware,3dcrcu,king_of_blades,3,Wed Jul 15 18:14:46 2015 UTC,I think they already kinda do in the server market  Read about Intel's Xeon phi here : http://www.admin-magazine.com/Articles/Exploring-the-Xeon-Phi  Very interesting stuff
hardware,3dcrcu,SR71-boss,2,Wed Jul 15 18:41:03 2015 UTC,"Alright so I remembered correctly   Like the Larrabee, the Xeon Phi’s CPU cores are based on first-generation Pentium (P54C) technology   So it's basically just a cpu with a lot of cores. Not a gpu.  I imagine server markets are willing to fork over the $1,000 it costs to grab these as they are price competitive with server grade GPU's while being able to program with more familiar technologies.  But if you could pull off consumer grade GPU for computing you would have better price performance. But servers aren't known to buy consumer grade."
hardware,3dcrcu,MINIMAN10000,1 point,Wed Jul 15 20:25:29 2015 UTC,Even the next generation of Phi (knights landing) is significantly less efficient than already existing GPU compute solutions.
hardware,3dcrcu,Exist50,1 point,Wed Jul 15 21:09:54 2015 UTC,Yeah its more like a cpu doing gpu work
hardware,3dcrcu,SR71-boss,1 point,Wed Jul 15 22:48:08 2015 UTC,"There are issues with that. Despite the obvious IP limitations, that eDRAM is a major boost to the performance, but can't scale to a full fledged graphics card."
hardware,3dcrcu,Exist50,2,Wed Jul 15 21:09:03 2015 UTC,"...but that's exactly what they're focusing on right now, and have been for the last few years.  Is this post meant to be ironic?"
hardware,3dcrcu,58592825866,1 point,Wed Jul 15 21:48:09 2015 UTC,"They've been doing integrated graphics since 1999, is this some kind of joke?"
hardware,3dcrcu,zzzoom,1 point,Thu Jul 16 06:03:27 2015 UTC,Not nearly this well
hardware,3dcrcu,SR71-boss,51,Thu Jul 16 07:25:02 2015 UTC,"So pretty much a power efficient update of Haswell that costs more, but performs less than the i7-4790K.  The only major performance boost is to the IGP, and it's graphics performance is better than the A10-7870K Godavari APU.  Except, the the i7-5775C is $360, while the A10-7870K is $150.  So you're paying a $200 premium for IGP performance.  Or you can spend that $200 on a decent GPU to outperform the Broadwell GT3e.  Looks pretty unimpressive.  If they paired that GT3e IGP to a low cost quad-core i5, or dual-core i3, then it would a very interesting option for small form factor gaming PCs."
hardware,3dcrcu,animeman59,11,Wed Jul 15 07:24:53 2015 UTC,I'm interested to see what that 128mb eDRAM can do as a cache...
hardware,3dcrcu,krista_,22,Wed Jul 15 07:47:40 2015 UTC,"Except, the the i7-5775C is $360, while the A10-7870K is $150. So you're paying a $200 premium for IGP performance.   No, as CPU performance is also much better than the 7870K."
hardware,3dcrcu,barthw,13,Wed Jul 15 10:45:49 2015 UTC,"Is there really a point to buying a CPU+GPU combo at such a high price though? I thought the Godavari was aimed at people who were on a really tight budget and gave them a price efficient solution for gaming without GPU, but Intel kind of blows that whole premise with this one."
hardware,3dcrcu,ubern00by,5,Wed Jul 15 11:22:47 2015 UTC,"Workstation.  http://www.tomshardware.com/reviews/intel-core-i7-5775c-i5-5675c-broadwell,4169-7.html"
hardware,3dcrcu,logged_n_2_say,29,Wed Jul 15 11:43:52 2015 UTC,You're paying a $200 premium for the IGP performance AND the CPU performance.
hardware,3dcrcu,YennoX,18,Wed Jul 15 07:44:37 2015 UTC,And power consumption. And heat output.
hardware,3dcrcu,viperabyss,13,Wed Jul 15 09:16:45 2015 UTC,And my axe?
hardware,3dcrcu,CantaloupeCamper,4,Wed Jul 15 15:13:02 2015 UTC,Same thing. No need to be redundant.
hardware,3dcrcu,Exist50,2,Wed Jul 15 16:02:30 2015 UTC,"Yeah, its form is similar.  recapitulation is unnecessary."
hardware,3dcrcu,StellaTerra,-4,Wed Jul 15 18:56:46 2015 UTC,"And form factor, and possibly noise"
hardware,3dcrcu,Athikitos,3,Wed Jul 15 09:57:09 2015 UTC,which can be made irrelevant with a $150 7970.
hardware,3dcrcu,sterob,10,Wed Jul 15 11:22:55 2015 UTC,"You're still missing the cpu.   These iris pros are a bad idea for gamers, but so are Apus."
hardware,3dcrcu,logged_n_2_say,-4,Wed Jul 15 11:56:14 2015 UTC,"I thought AMD fanboys recommended the 300 series? Oh wait, it's overpriced rebranded trash. 380 $230 performs the exact same as a $200 960 and $150 7970. Don't forget the $250 290.  On topic, the EDRAM is pretty useful for CPU performance and when DX12 comes out the IGP will be useful. For the extra $40 or so I think it's worth it if you need the CPU performance."
hardware,3dcrcu,CykaLogic,7,Fri Jul 17 00:03:19 2015 UTC,If you look at the clock for clock comparison there are some gains over the 4790k. The reason the 4790k is still outperforming is the 4.0 base clock. Compare that to the 3.3 base clock of the broadwell.
hardware,3dcrcu,logged_n_2_say,5,Wed Jul 15 11:42:23 2015 UTC,"it still only trades blows with the 4.2 OC, and that's against a 4790K without OC"
hardware,3dcrcu,Chipay,9,Wed Jul 15 11:58:50 2015 UTC,"The 4790k has a boost of 4.4 ghz. Click the""clock for clock"" page."
hardware,3dcrcu,logged_n_2_say,3,Wed Jul 15 12:03:56 2015 UTC,"Note that depending on settings, the boost is only 4.4GHz for single core loads. I believe it's 4.3GHz for 2/3-core loads and 4.2GHz for full quad core load. But as said, depending on motherboard settings it could be 4.4GHz at all loads. Not sure how they have it set."
hardware,3dcrcu,bphase,2,Wed Jul 15 13:49:29 2015 UTC,"right, although even a single core at 4.4ghz could still skew results possibly in favor for the 4790k compared to a 4.2 on all 4 cores 5775c if we are trying to do a clock for clock comparison."
hardware,3dcrcu,logged_n_2_say,2,Wed Jul 15 13:55:28 2015 UTC,Mine runs at 4.7GHz on all cores at once. I told the ASUS-software to do its thing and that's what it came up with. No expertise required on my side.
hardware,3dcrcu,hibbel,4,Wed Jul 15 13:53:26 2015 UTC,I would check what your vcore is running at just to be safe...
hardware,3dcrcu,attomsk,1 point,Wed Jul 15 17:10:38 2015 UTC,What cooling do you run?
hardware,3dcrcu,CEKPETHO,1 point,Wed Jul 15 16:57:30 2015 UTC,Noctua NH-U14
hardware,3dcrcu,hibbel,1 point,Wed Jul 15 17:09:51 2015 UTC,Looks like you're using the overclocking utility.
hardware,3dcrcu,Exist50,5,Thu Jul 16 03:04:59 2015 UTC,"4.4GHz is the turbo fequency, the 4790K can actually OC to 4.7-4.8 GHz, compared to the 5775C's 4.2GHz.  There's not much point in comparing the same clockspeeds when one can easily get a higher clockspeed."
hardware,3dcrcu,Chipay,4,Wed Jul 15 12:11:53 2015 UTC,"I'm speaking of a clock for clock comparison so we can get a fair comparison of features and architecture.  But I assume the 4790k will overclock better, as it's the best binned haswell after almost a year of producing the 4770k not to mention it's larger size with a smaller igpu.   But I would not claim that 4.2 ghz is the ceiling for the i7 broadwell. The sample size is one, and I can't find were they talk about overclocking. Did they disable the igpu? What was happening at what voltage and clock?  With that said I don't believe the broadwell will overclock as well, but I also wasn't expecting it to."
hardware,3dcrcu,logged_n_2_say,3,Wed Jul 15 12:21:59 2015 UTC,"Haven't read this article,because Guru3D also has one that i did read.   our findings in short, its not the best overclocker. You'll reach 4200 maybe 4300 MHz stable and will need roughly 1.350~1.400 Volts for that.   Raising just the multiplier will get you 4.1 GHz without extra voltage. At 4.4 GHz we needed 1.375V. We benchmarked at 4.4 and got the results, but in the end it was not stable enough. So we had to revert back towards 4.3 GHz to keep things stable"
hardware,3dcrcu,Pinksters,4,Wed Jul 15 14:15:12 2015 UTC,"1.375V sounds extremely high for a 14 nm chip. That's pretty much the maximum people give 4790Ks on traditional cooling, and voltages tend to go down a lot by decreasing nodes.  From my limited research they do seem to max out at 4.2-4.4 often, which is quite a bit lower than 4790Ks which tend to go to 4.6-4.8ish."
hardware,3dcrcu,bphase,4,Wed Jul 15 14:28:51 2015 UTC,Many people say any more than 1.3-1.35V will cause noticeable long term issues for Haswell. Probably quite dangerous for Broadwell.
hardware,3dcrcu,Exist50,1 point,Wed Jul 15 16:04:08 2015 UTC,"Yup, was gonna say I actually have my 4790k at 4.7Ghz with a custom cooler.  Great processor and worthy replacement to my old i7 920.  I REALLY wanted to feel motivated to upgrade to something with DDR4 but the price of upgrading gives such little performance gains as to be worth it again.  Maybe it'll be another 5 yr cycle in-between CPU upgrades again."
hardware,3dcrcu,GeneticsGuy,1 point,Thu Jul 16 01:42:33 2015 UTC,When did you upgrade to the 4790k? Is it still worth upgrading to the 4790k? I'm still currently using my i7 920. :P
hardware,3dcrcu,damnshoes,2,Fri Jul 17 00:30:11 2015 UTC,"Basically, when they showed the i7 5000 series as not being much better at all and DDR4 was expensive as hell, I decided not to wait for Skylake and just buy at the next sale, so it's been like 6 or 8 months I think. Got the 4790k for a deal I think at 279. Got the ""Krait"" motherboard and ram and my whole upgrade process only put me back 500 since I didn't need anything else as I already had tower, power,gpu, etc...  It was worth it for the transcoding performance alone. But ya, great chip, I'd recommend it."
hardware,3dcrcu,GeneticsGuy,1 point,Fri Jul 17 01:20:55 2015 UTC,"Except, the the i7-5775C is $360, while the A10-7870K is $150. So you're paying a $200 premium for IGP performance.    You can purchased a $244 i5-5575R with the same iGPU. That's only a slight premium for some serious iGPU performance."
hardware,3dcrcu,LFKhael,4,Wed Jul 15 18:22:59 2015 UTC,"5575R is bga, only the i5-5675C is lga 1150."
hardware,3dcrcu,Myrang3r,10,Wed Jul 15 18:59:50 2015 UTC,That OCability doesnt bode well for Skylake
hardware,3dcrcu,everyZig,5,Wed Jul 15 09:07:29 2015 UTC,Hopefully the lack of a voltage regulator should help
hardware,3dcrcu,Begoru,1 point,Wed Jul 15 12:14:27 2015 UTC,"Yeah, hopefully skylake overclocks well, because once an X99 equivalent releases, I am upgrading my home PC. All the additions/features made are great since I am doing a lot of productivity at home (which in itself is a mixed bag).   Also the adoption of DDR4 is fantastic, hopefully prices keep going down, because I am probably going to get 16gb or more ram too."
hardware,3dcrcu,BuilderBrother,4,Thu Jul 16 15:02:47 2015 UTC,"Meh, the top binned 6700Ks already have stock turbo speeds of 4.2 GHz which is what this review managed to OC the broadwell to. Agreed it doesn't bode well, but Skylake has already surpassed these SKUs."
hardware,3dcrcu,PolyWit,3,Wed Jul 15 10:59:44 2015 UTC,"Still unclear whether it surpasses the 4790K, and it'll be honestly very disappointing if it doesn't."
hardware,3dcrcu,bphase,5,Wed Jul 15 14:37:48 2015 UTC,"I would love this in a laptop, but it the desktop usage seems pretty limited.  Might be useful for a nice silent mini-ITX HTPC/Steam machine, but I can't think of much else."
hardware,3dcrcu,satertek,8,Wed Jul 15 14:15:18 2015 UTC,"That is a significant jump in IGP performance. This will shake up the discrete GPU market in the coming years, if Intel puts them in everything going forward.   Anything nVidia or AMD puts out, even at the sub-$100 level, will have to perform better than these."
hardware,3dcrcu,okron1k,10,Wed Jul 15 14:44:01 2015 UTC,"Intel and AMD have already basically killed the <$60 GPU market. Intel'll probably continue to focus on graphics for the consumer market, while AMD will probably lag more behind (CPU's more important right now) until 14nm and HBM, which'll give a huge boost."
hardware,3dcrcu,Exist50,1 point,Wed Jul 15 17:43:11 2015 UTC,I hope they do; this could be a very good thing for consumers.
hardware,3dcrcu,Cymry_Cymraeg,2,Wed Jul 15 17:06:53 2015 UTC,"Aside from it not being available yet, is there any reason to buy a Broadwell chip over Skylake?"
hardware,3dcrcu,MelGibsonDiedForUs,6,Wed Jul 15 08:12:24 2015 UTC,Cheaper mobos and RAM. Don't know how much DDR4 or DDR3L will go for but sticking some second hand (or your existing) DDR3 RAM in will be cheaper regardless.  Basically you buy the CPU and you're good to go. Skylake requires new hardware to run.
hardware,3dcrcu,meowffins,1 point,Wed Jul 15 08:38:19 2015 UTC,But why not just get a 4790k?
hardware,3dcrcu,luger718,5,Wed Jul 15 15:10:25 2015 UTC,"Because OP asked about broadwell and skylake.   And speaking of the generations generally, the main change is the much more powerful iGPU (2-3x) compared to existing intel CPUs including the 4790k."
hardware,3dcrcu,meowffins,-1,Wed Jul 15 15:38:41 2015 UTC,"I think a comparison between a socketed LGA CPU and a soldered BGA CPU would be cool for science purposes. We could expect that there should be no noticeable difference if the CPUs are otherwise similar, but we cant know for sure unless somebody puts it to the test."
hardware,3dcrcu,PadaV4,3,Wed Jul 15 11:11:39 2015 UTC,I don't think the comparison is needed. BGA vs LGA has always been about reducing manufacturing expenses.
hardware,3ddgs6,zmeul,2,Wed Jul 15 12:19:12 2015 UTC,Still funky pricing on the large capacities  256 version is cheaper than 2 128's.  512 is significantly more expensive than 2 256's. You could almost get 3 256's for the price.  Does anyone know why this is? Different controller? more RAM for the controller? I have seen this pattern with Samsung recently so I really interested in finding out what's going on.
hardware,3ddgs6,PlankWithANailIn,2,Thu Jul 16 23:19:46 2015 UTC,"If my possibly outdated data is correct, since memory controllers can provide better performance with larger capacities. Crucial M500 for example. I suspect the same happens here."
hardware,3ddgs6,aranurea,2,Fri Jul 17 00:50:03 2015 UTC,"Its really confusing from a consumer's point for view to be paying more for less. When you add on the fact that the performance difference will only be slight and only observable in benchmarks then it's a bit annoying.  It would be better if they just changed the branding, at least then it would be more obvious that the big drive was the (imperceptible) performance version."
hardware,3ddgs6,PlankWithANailIn,1 point,Fri Jul 17 09:26:54 2015 UTC,Do they perform just as well if not better as the Samsung Evo SSDs?
hardware,3ddgs6,sojerboy08,-8,Thu Jul 16 20:44:25 2015 UTC,SSD prices are in free fall right now. They seem to be dropping 10% in price every month and this trends looks to continue until the end of the year.  Bye bye HDDs.
hardware,3ddgs6,DeeJayDelicious,9,Wed Jul 15 18:57:41 2015 UTC,Still going to be quite a while before they catch up to HDDs. And that's the current price of HDDs. Manufacturers may be willing to lower their prices quite a bit.
hardware,3ddgs6,Stingray88,-1,Wed Jul 15 22:10:12 2015 UTC,"Yes, but with the average consumer HDD already at 50$, how much lower can they go? I mean the benefits of an SSD cannot simply be ignored. Base-line SSDs are what helped Macbooks gain popularity because they felt much more responsive.  Seagate has also reported its first operating loss in ....a while."
hardware,3ddgs6,DeeJayDelicious,7,Wed Jul 15 23:13:12 2015 UTC,"Average consumers aren't the only ones buying hard drives. Many people still need very large capacities, and those don't cost anywhere near $50. And for a lot of those usage cases, the performance benefits of an SSD mean nothing, $/GB is all that matters."
hardware,3ddgs6,Stingray88,-3,Wed Jul 15 23:22:36 2015 UTC,"To a few maybe, but to most the benefits of large storage simply aren't there any longer. They used to have massive MP3 collections where they now have a Spotify account, they used to have a huge movie collection where they now have a Netflix account. It used to take them days to redownload a game on Steam so they never deleted it, now it takes them an hour or two.  For the vast majority of private users there is little need to vast HDDs anymore."
hardware,3ddgs6,DeeJayDelicious,5,Thu Jul 16 07:32:46 2015 UTC,"You're just repeating what we all already know, but you're missing the point.  Hard drives aren't going anywhere for the same reason data tapes still exist. Somebody still needs them.   Yes I'm well aware of the fact that most people won't need them. But some people do. And those people will keep the hard drive market alive."
hardware,3ddgs6,Stingray88,1 point,Thu Jul 16 07:47:36 2015 UTC,But some people do.   /r/DataHoarder :D
hardware,3ddgs6,wewd,-8,Thu Jul 16 17:53:24 2015 UTC,"not likely, don't ssd drives have issues with long term storage?  6+ months  edit: getting downvoted so:  According to a recent presentation by Seagate's Alvin Cox, who is also chairman of the Joint Electron Device Engineering Council (JEDEC), the period of time that data will be retained on an SSD is halved for every 5 degrees Celsius (9 degrees Fahrenheit) rise in temperature in the area where the SSD is stored.  Consumer class SSDs can store data for up to two years before the standard drops, but when it comes to SSDs used by enterprises, the drives are only expected to retain data for a period of three months – a fact confirmed by Samsung, Seagate and Intel's own ratings on their products.  source"
hardware,3ddgs6,PaulTheMerc,9,Wed Jul 15 20:57:52 2015 UTC,"This is false.  You're citing the article accurately, but the writer of that article is misinterpreting the results of the presentation she is citing. In that presentation, on page 27, it is explained that the data retention expectation for an SSD without power stored at 30C (86F) or a powered SSD running at 40C (104F) is fifty-two weeks for consumer drives, and ten weeks for enterprise drives.  So, for every 5C above that, the data retention expectation is halved. In other words, if you're running an SSD daily at 45C (113F), the data retention expectation is twenty-six weeks (six months) for consumer drives, five weeks for enterprise drives. Normal internal ambient temperatures are well below 40C (104F), unless you have poor ventilation and/or are heavily overclocking."
hardware,3ddgs6,scarthearmada,0,Wed Jul 15 23:41:51 2015 UTC,"Sorry for bothering. I was looking the table at page 27 of the source ( https://web.archive.org/web/20150428040623/http://www.jedec.org/sites/default/files/Alvin_Cox%20%5BCompatibility%20Mode%5D_0.pdf ). Does it mean that the hotter my pc is while it being powered on the longest my data will be retained? And while the power is off, the cooler the better?"
hardware,3ddgs6,IPBTaliz,2,Thu Jul 16 08:09:30 2015 UTC,"No. Hot is always bad for data retention, every extra 5C always halves the retention. The extra problem with badly ventilated cases is the cooldown after power off."
hardware,3ddgs6,narwi,-5,Thu Jul 16 10:34:40 2015 UTC,"Yeah, I had Skyrim on my SSD and decided to play it after not playing it for 5-6 months, and the damn game took about two minutes to load.  EDIT: I posted this before he edited his comment, so I didn't know that he/she was talking about data loss."
hardware,3ddgs6,Durka_Durk_Dur,8,Wed Jul 15 21:26:39 2015 UTC,"Then you have a different problem.  Because this issue will cause data loss, not slowing down. You wouldn't be able to play your skyrim at all if that happens, let alone loading it in two minutes."
hardware,3ddgs6,Killmeplsok,1 point,Thu Jul 16 01:29:41 2015 UTC,Bit rot on SSDs does show up as reduced speed though.
hardware,3ddgs6,narwi,2,Thu Jul 16 10:35:21 2015 UTC,If you have a samsung 840 evo there is a bug fixed by the last firmware that slow down performance for data stored after some months. Install Samsung magician 4.6 to fix it. :)
hardware,3ddgs6,Myron-BE,2,Thu Jul 16 07:13:34 2015 UTC,Will do :)
hardware,3ddgs6,Durka_Durk_Dur,2,Thu Jul 16 14:35:18 2015 UTC,"Confirming, and after the firmware update I would recommend a full secure erase and reinstall of Windows to get peak performance again."
hardware,3dburj,fRl1jOE_,25,Wed Jul 15 01:39:09 2015 UTC,Probably with Skylake laptops.
hardware,3dburj,Exist50,11,Wed Jul 15 01:42:41 2015 UTC,"One thing to remember is that Skylake supports both DDR3 and DDR4, so probably only the high end laptops will have it for some time. Also there is no significant difference in real world speed (if any) between them so it will probably take a while before DDR4 is common in laptops.  DDR2 lasted quite a while longer in laptops than it did in desktops for example."
hardware,3dburj,kittybeard,23,Wed Jul 15 05:57:18 2015 UTC,Lpddr4 would probably be used in nearly all ultrabook and high end non performance laptops. The power savings from that are quite significant.
hardware,3dburj,dylan522p,6,Wed Jul 15 05:58:28 2015 UTC,Not to mention potential integrated graphics performance gains.
hardware,3dburj,LFKhael,11,Wed Jul 15 18:07:24 2015 UTC,"Note that these modules are ECC, and the laptops will likely use non-ECC modules I think."
hardware,3dburj,yuhong,6,Wed Jul 15 02:55:06 2015 UTC,yeah those are server DIMMs. Laptops don't support ECC because both the CPU and chipset doesn't support it.
hardware,3dburj,Jack_BE,9,Wed Jul 15 06:30:45 2015 UTC,About that... http://www.eurocom.com/ec/productsg(6)ec  It won't do the registered dimms but it will do ecc.
hardware,3dburj,MandaloreZA,7,Wed Jul 15 06:40:20 2015 UTC,"that thing is literally a ""mobile server"", it even uses the full-power Xeon E5 CPUs instead of a normal Core i5 M, U or Y.  Not really something even a developper or CAD designer would use."
hardware,3dburj,Jack_BE,5,Wed Jul 15 06:44:29 2015 UTC,"I know, just thought it would be funny. :)"
hardware,3dburj,MandaloreZA,2,Wed Jul 15 06:47:36 2015 UTC,Wonder what it uses for graphics.
hardware,3dburj,Exist50,2,Wed Jul 15 17:46:08 2015 UTC,Up to 2 980m 8gb in sli or a couple of quadros.
hardware,3dburj,MandaloreZA,1 point,Wed Jul 15 19:36:04 2015 UTC,Yes- most likely they will.
hardware,3dburj,saiki4116,22,Wed Jul 15 02:59:43 2015 UTC,"Forget about DDR4, I am still waiting for the day, they would stop using 1366*768 panels"
hardware,3dburj,thatdudeyouhate,8,Wed Jul 15 15:09:39 2015 UTC,This bothers me immensely. Why do I have to buy a 17 inch laptop to get the resolution I want?
hardware,3dburj,aloha013,0,Thu Jul 16 01:25:43 2015 UTC,"Yep, when i bought my laptop it was either touchscreen or 1080p. And I wanted the touchscreen for windows 8. But at least games play slightly better."
hardware,3dburj,Dragonsong,0,Thu Jul 16 06:05:32 2015 UTC,What's the new standard supposed to be?
hardware,3dburj,Seclorum,10,Thu Jul 16 07:28:30 2015 UTC,When laptops ship with boards based on X99 (Not likely) or z170 and skylake.
hardware,3dburj,ikjadoon,2,Wed Jul 15 02:03:11 2015 UTC,Looks like DDR4 coming to the Skylake NUC (which is just a laptop in a small form-factor case and sans display):  http://www.fanlesstech.com/2015/05/exclusive-skylake-nuc-specs.html
hardware,3dburj,xtothemess,3,Wed Jul 15 12:53:52 2015 UTC,"Have you seen the price on DDr4 SODIMMs? http://www.crucial.com/usa/en/memory-DDR4/CT4K8G4TFD8213?gclid=CJ67muCq3MYCFYMWHwodlEUNPQ&cm_mmc=shopping-_-tsa-_-null-_-us&ef_id=VZi71gAABIIW4EYG:20150715044854:s  First you need a good supply of memory, then it will get integrated."
hardware,3dburj,dylan522p,6,Wed Jul 15 04:49:24 2015 UTC,"With massively popular phones like the s6 having 3gb of lpddr4 of it amd the BOM of the whole phone being ~$250, lpddr4 has to ve fairly cheap and very plentiful. Just noone is making sodims because there's no point. Where or who is going to buy them when they can't use them anywhere."
hardware,3dburj,Exist50,5,Wed Jul 15 06:01:47 2015 UTC,Don't forget that Samsung is highly vertically integrated. Being the producer of that DDR4 has to help prices.
hardware,3dburj,omniblue,1 point,Wed Jul 15 17:46:54 2015 UTC,ECC is always expensive.
hardware,3da074,ryerocco,27,Tue Jul 14 17:39:53 2015 UTC,Con: Significantly more expensive that a 2TB HDD  Lol?
hardware,3da074,catcher6250,19,Tue Jul 14 20:26:38 2015 UTC,"it had to be something, I guess."
hardware,3da074,luger718,13,Tue Jul 14 20:32:42 2015 UTC,Sata 3 is a con isn't it? Nvme drives look awesome.
hardware,3da074,chapstickbomber,11,Tue Jul 14 22:14:19 2015 UTC,"I was considering getting a nice PCIE SSD to run a clean Windows 10 install and really refresh my rig.  But, after doing a lot of research, I've discovered that for 8k reads/writes and under, regardless of queue depth, there is no difference between SATA 3 and PCIE right now in terms of performance.  And if the queue depth is low for higher block sizes, then you still see no benefit. Since one of the most important storage tasks is 4k fully random writes (for log files, OS metadata, indexes, application logs, etc), it would be nice to see a benefit there for a PCIE drive that is three times the price of a similar capacity SATA3 drive, but those don't seem to exist. Marginal improvements, sure. But nothing really substantial on the low block size.  So the PCIE SSD's really only have an advantage when you are dealing with a huge amount of data at once, which is an atypical use case in terms of frequency. I mean, it is doubtful that you will be IO limited with video encoding/processing unless your CPU is just an absolute monster god machine. And even then, you were already committed to waiting a minute or so. A PCIE SSD doesn't have an order of magnitude advantage in anything, so it doesn't seem worth it.  I mean, internally, these drives are running on the same base NAND chips, logic, and caching technology, so of course if interface bandwidth is not an issue for a use case, you are not going to see a difference.  I would say that SATA3 is a con in the same way that having a Ferrari with a governor set to 110mph is a con. Well yeah, it's a con. But there are so few instances where going faster than that is even relevant. The acceleration and handling at ""lower speeds"" is a much bigger deal in practice.  On that note, modern SATA SSD's handle 4k 4 queue read write over 300MB/s and cost like 30 cents a GB."
hardware,3da074,assangeleakinglol,8,Wed Jul 15 07:36:55 2015 UTC,Was your reasearch based on AHCI or NVMe based drives? I thought the parallel nature of NVMe would really help at higher queue-depths.
hardware,3da074,chapstickbomber,1 point,Wed Jul 15 09:47:53 2015 UTC,"What I'm saying is that for small blocks regardless of queue depth and larger blocks with low queue depth, the bandwidth limitation of SATA3 isn't the bottleneck, it is the controllers in the drives, even for pcie drives.   That tech won't change with nvme. In a few years, we'll see some consumer hardware that will allow 4k random writes of 1TB/s which sata would bottleneck, but until then nvme and pcie only have a factor of two over an already blistering half a GB per drive per second on large operations."
hardware,3da074,assangeleakinglol,1 point,Wed Jul 15 13:31:02 2015 UTC,AFAIK NVMe isn't available on SATA interface. So even if it can't bottleneck sata3 at 4K random it's still quicker at larger queue-depths than what AHCI based communcations would be. So I wouldn't say that it's completely a waste to go for PCIE-NVMe even if your main worry is <8K IOPS. Also considering the reduced CPU-overhead.
hardware,3da074,bphase,4,Wed Jul 15 14:04:38 2015 UTC,"Still, current tests haven't shown much of an improvement between the AHCI/NVMe versions of PCIe drives . It could become meaningful in the future, but for now, for a regular enthusiast there just isn't much of a difference between a SATA drive vs. a NVMe PCIe one. Certainly not worth the price difference you pay.  One such test here: http://www.anandtech.com/show/9396/samsung-sm951-nvme-256gb-pcie-ssd-review"
hardware,3da074,chapstickbomber,1 point,Wed Jul 15 14:42:22 2015 UTC,"SATA overhead is not trivial, but I doubt that the measurable gains from PCIE NVME even when multitasking a mammoth drive operation with a CPU benchmark would be more than 2 to 3%.  A big factor at present is price. I could get 3 850 Evo 500GB drives and stripe them for the same price as a PCIE 480GB HyperX Predator and have the same large block high queue performance and small block performance with 1.5TB available. CPU usage goes up, yeah, but any i7 or FX8 can certainly find some spare integer throughput to handle that."
hardware,3da074,assangeleakinglol,1 point,Wed Jul 15 19:46:11 2015 UTC,http://images.anandtech.com/graphs/graph9396/75315.png  http://media.bestofmicro.com/R/1/494749/gallery/image005_r_600x450.png  For me the difference looks higher than 2 to 3%.
hardware,3da074,hibbel,1 point,Wed Jul 15 20:49:54 2015 UTC,"No, it's a pro. I can place the drive anywhere in the case and run a cable to it. I don't have to worry about the m.2 slot or PCIE slot possibly being covered by one of the huge radiators I strapped to both the CPU and GPU. I can use as many of them as I have SATA3 ports on my motherboard, which is way more than I have M.2 / PCIE ports.  And I get all of that for a very small reduction in performance, in some select scenarios.  I'll take SATA3 over PCIE / M.2 any day."
hardware,3da074,dylan522p,-3,Wed Jul 15 09:09:41 2015 UTC,So you'd rather take an iGPU over a add in dedicated gpu for same reasons right....
hardware,3da074,hibbel,5,Wed Jul 15 13:17:13 2015 UTC,"Huh? I can place an iGPU anywhere in the case? I can use more iGPUs than dedicated GPUs? Those are two of the main arguments in favor of SATA3. The comparison just doesn't hold up.  But more importantly, SATA3 is fucking fast already. I rarely look at my loading times and think ""man, this sure took long."" Even if PCIe SSDs had unlimited bandwidth and zero latency (which they don't) so everything would load instantly, the most this could save me is the amount of time a fast SATA3 SSD takes. Which is usually very little. Remember that loading times with a SATA3 SSD are likely mostly the data being processed, not the data being shovelled over the bus.  So, my windows boot time would go down from 10 (at best, I never timed it) seconds to what? 7 seconds via PCIe?  iGPUs on the other hand are vastly and noticably slower than dedicated GPUs.  Your comparison just makes no sense whatsoever on so many levels."
hardware,3da074,TripleBrass,1 point,Wed Jul 15 13:35:40 2015 UTC,"It's SATA 6 Gbps, no?"
hardware,3da074,CallMePyro,10,Wed Jul 15 00:47:44 2015 UTC,"?? SATA 3 is the version of the SATA connection, which has a 6 gigabit per second transfer rate."
hardware,3da074,TripleBrass,1 point,Wed Jul 15 01:17:21 2015 UTC,"I meant SATA 3, or the the 6 Gbps version, yes.  Easy mistake.  I was confusing SATA I, II, and III (revision #s) vs the data transfer rate.  I was just looking at a lot of drives around the time SATA III was overtaking SATA II a few years ago, so it was in my head."
hardware,3da074,luger718,5,Wed Jul 15 01:24:13 2015 UTC,"Yeah, I guess the con is ""Not PCIe"" although those have their issues as well, mainly cost."
hardware,3da074,Hoooooooar,3,Wed Jul 15 02:26:56 2015 UTC,A lot of cost.
hardware,3da074,dylan522p,1 point,Wed Jul 15 04:02:30 2015 UTC,Yeah still very limiting
hardware,3da074,insanity5902,5,Wed Jul 15 00:52:53 2015 UTC,"You still have backups, right?"
hardware,3da074,ElvarP,9,Wed Jul 15 04:03:00 2015 UTC,Yeah? why not? HDD's have higher failure rates then SSD's...
hardware,3da074,hibbel,4,Wed Jul 15 05:20:00 2015 UTC,"Well, this is a Samsung drive.   Admittedly, it's an Evo, not a Pro, so hopefuly the asynchronous TRIM bug won't erase your data. And it's an 850, so hopefully it won't slow down to a crawl eventually. Hey, maybe it'll even be a bug-free Samsung SSD after all. That'd be something new!"
hardware,3da074,moozaad,11,Wed Jul 15 04:32:13 2015 UTC,Sucks to be downvoted for the truth.   Samsung's SSD reputation is bad. 840 evo has a design issue where cells lose voltage over 9 months and read throughput tanks. There are 'fixes' but they're a compromise. Other drives (can't remember which models) have a proven trim bug the randomly zero's out sectors (4KB).
hardware,3da074,hibbel,5,Wed Jul 15 06:53:31 2015 UTC,Reddit loves Samsung. I knew what I did when I posted criticism of their SSD lineup.
hardware,3da074,Stingray88,2,Wed Jul 15 08:57:13 2015 UTC,Shouldn't really matter because all your data should have adequate backups.
hardware,3da074,underthesign,-1,Wed Jul 15 12:38:58 2015 UTC,"It's really not an issue any more, thankfully. I'm waiting for these to hit 4gb and I will then replace all the storage drives in all of my workstations, render nodes and NAS boxes. The prices are finally coming down to reasonable levels at last. I can't wait to shove all my media in my home NAS on an ssd. Will make my media theatre so much snappier!"
hardware,3da1p0,ryerocco,26,Tue Jul 14 17:50:09 2015 UTC,"It wasn't just the fact that they started the review out by bench marking cars, its their results in general that are odd. Compare Techpowerup's benchmark of BF4 with Tech Report's benchmark  Why is the 970 over 20% faster in Tech Report's benchmark compared to TPU's? Not to mention the 980 is 10% slower than the Fury in TPU's benchmark, but it is ahead in Tech Report's. I understand they used a G1 980 which has about a 10% factory oc, so thats somewhat understandable.  The other benchmark that is odd is the one of Crysis 3. Tech Report's results at 4K for the cards in general seem way too high, and also much higher than TPU's results. The cards are over 50% faster, and some (970) are closer to twice that. Also, TPU actually has AA disabled, but Tech Report is running 1X AA. So that makes even less sense.  Am I missing something here? Feel free to point it out!  Editsies for linkies"
hardware,3da1p0,rationis,7,Tue Jul 14 20:01:58 2015 UTC,What is even more interesting about those BF4 benches is that the Techpowerup bench has no AA and shows the 970 at 25.1 average FPS while the Techreport bench has 4x AA and shows the 970 at 31 average FPS.  It would make much more sense if the no AA bench was 31 and 4x AA at 25.1 but it is actually the opposite.
hardware,3da1p0,BlackKnightSix,1 point,Tue Jul 14 21:31:14 2015 UTC,Doesn't that also depend which scene they benchmark?   Some scenes and areas stress completely different parts of the gpu.
hardware,3da1p0,mightbeover9000,1 point,Thu Jul 16 23:48:56 2015 UTC,"Depends. If they use a benchmarking tool, it should pretty quite repeatable. Bf4 has one I believe. Not sure if that's what they use though."
hardware,3da1p0,BlackKnightSix,10,Fri Jul 17 00:26:09 2015 UTC,"The thing I like about 'The Tech Report's' article, was the addition of a full breakdown of settings as well as frame time charts which can point out issues with cards that a standard, Min-Avg-Max graph cant.   And 'TechPowerup' leaves a bad taste in my mouth with graphs that only emphasize Avg framerates, with no breakdown of settings used, and i've seen their graphs include SLI setups randomly just to skew the appearance of the charts!"
hardware,3da1p0,Seclorum,4,Tue Jul 14 21:17:36 2015 UTC,"The issue isn't so much setting, a trusted author will use the same settings, the issue is wether they went back and retested previous cards at the same time. Many sites sell off their cards, and for that matter other samples, and never have the ability to retest with updated BIOS (very important), drivers (super important), same motherboard and CPU and memory and SSD (super important), or OS updates (actually make a difference),. If you don't do this then the review is invalid, that is why i think review result ""databases"" are worthless."
hardware,3da1p0,xtothemess,6,Wed Jul 15 04:46:50 2015 UTC,"Of course I retest, a lot, always on the same everything.  15.7 was used for all important cards in my Fury non-X review  http://img.techpowerup.org/150715/Capture392.jpg No you can't touch my cards :P   include SLI setups randomly   Ugh, that reminds me, I wanted to make 970 SLI a permanent addition, and forgot to buy a 2nd card"
hardware,3da1p0,WizzardTPU,2,Wed Jul 15 10:38:54 2015 UTC,"Oh, I'm not calling you out, haha I like your reviews :)  I want to add that I think you are one of the best GPU reviewers and i love how simple and plain TPU is, there aren't ads all over and the news is direct (a little bit too editorialized, but direct most of the time)."
hardware,3da1p0,xtothemess,12,Wed Jul 15 12:58:11 2015 UTC,I applaud them for that article. what a great reconsideration of the data and openness to the community from that mag
hardware,3da1p0,robertotomas,18,Tue Jul 14 20:40:59 2015 UTC,"They pretty much had to. They're the only outlet that had the Fury lose to the 980 in average framerate, so they had to revise it or no one would take them seriously anymore."
hardware,3da1p0,masturbateAndSwitch,5,Tue Jul 14 21:29:52 2015 UTC,"So its around equal to a card from last year, but costs more and uses more power. Although its not as fast as aftermarket 980's"
hardware,3da1p0,random_digital,9,Tue Jul 14 20:46:28 2015 UTC,"I've seen reviews where they compared it against an Evga ACX 2 cooled 980, so very much not a stock card, and the Fury came out ahead or equal to the 980.   And who cares if the 980 was released last year, they wont have anything to replace it's price point until next year anyway.   The cost difference is less than $50 and given the performance difference it easily is worth that much over a straight 980. If you want to spend 150 more than the 980 you would of course enter the arena of the 980 ti and Fury X, but at the pricepoint it's selling at and the competition Nvida has on the market for it, it's very competitive.   As to using more power, really? It doesn't use that much more power than the 980 does, and if power saving is really your thing your not going to be buying either card in the first place."
hardware,3da1p0,Seclorum,2,Tue Jul 14 21:22:38 2015 UTC,"Well, it's 40w more at any state but idle. For me that would probably translate into an extra 10-12€ per year, or 40-50€ over the estimated lifetime of the card (~4 years).  The higher power draw does indirectly translate into higher fan noise. Under load it's several times louder than the 980. (The STIX, that is.)  Performance per dollar is also better for the 980, so whether it's worth the extra money is debatable.  I'd say the 980 is objectively the better card but if you've got a reason to support team red (like wanting a FreeSync display or preferring their business practices) the Fury is certainly worth it."
hardware,3da1p0,Mr_s3rius,1 point,Tue Jul 14 23:04:50 2015 UTC,"Yeah the Techpowerup guys are not that reputable imho. Too many shenanigans behind the scenes, no good detailing of settings, focus on average FPS only. (This is for your power draw/noise) point.   As for performance per dollar, it's entirely subjective as to which reviewer you trust more. But it's very competitive for some, and utter trash level for others...  Both the Fury and the 980 are good cards and easily competitive with each other."
hardware,3da1p0,Seclorum,2,Tue Jul 14 23:13:01 2015 UTC,"You may be right about that. Anandtech's review put the two cards at almost equal noise levels under load (although neither site mentions what kind of 980 they were using). The 980 is still ahead in price/performance but the gap is smaller.  Welp, I'll twiddle my thumbs and wait for reviewers to finally be able to overclock the Furies."
hardware,3da1p0,Mr_s3rius,1 point,Tue Jul 14 23:32:42 2015 UTC,"Welp, I'll twiddle my thumbs and wait for reviewers to finally be able to overclock the Furies.   It boggles my mind that AMD made that a point during their release, that they were ""Fantastic Overclockers"" yet they did nothing to help people get proper OC controlls enabled or working...  If it was a big enough deal to make a selling point, they should have been working with these people to get it ready..."
hardware,3da1p0,Seclorum,-3,Tue Jul 14 23:50:22 2015 UTC,"You sir and people like you are killing competition. The longer people like you claim AMD is competing with Nvidia the longer it will take to get real competition.   People like you want to sweep everything under the rug and paint a picture so perfect for AMD it sickens me.   First off you can get brand new 980s for right at 400 right now hell I have seen G1 going for 449. Are you saying a fury is worth 100 more than a G1 980.  EVGA is releasing B stock 980s nearly every day are you saying super clocked 980 for 400 is the lesser buy compared to 550 for the fury.   Or are you trying to tell a me a product released a year late and for 50 bucks more is the better product simply because it has AMD stamped on it.  If this is the card you want go for it but when you people try and twist facts to make it seems like it's the only logical choice, it makes me sick."
hardware,3da1p0,Griffith1984,2,Wed Jul 15 00:47:03 2015 UTC,"The 980 is a great card, especially with the recent price drops, but with the Fury you can do 4K (VSR if you don't have a 4K monitor). And at 1440p, there are some games where the Fury and 980 are tied, but some games where the Fury is a full tier above. If I were in the market, I would probably buy a G1 980 for 450 (actually, I would hold off for DX12 benchmarks), but I can see why people would pay the price premium."
hardware,3da1p0,masturbateAndSwitch,1 point,Wed Jul 15 01:57:11 2015 UTC,"The 980 has DSR which is enables the exact thing you are talking about. It also uses much less power so SLI becomes easily implemented if one wants or needs it. I have no problem with the card being more money, my problem is the card is more money for the same FPS.   If the Fury used less power or had an easily reproducible 5-10 percent FPS I could easily see the extra money, in fact I would recommend it but it doesn't have anything rock solid above the 980"
hardware,3da1p0,Griffith1984,13,Wed Jul 15 02:22:38 2015 UTC,"The 980 doesn't scale nearly as well to 4K, though, and DSR has a performance hit while VSR does not. DSR is also criticized for the way it blurs the image, while VSR's scaling is sharper.    And the price-performance ratio shifts toward the Fury when multi-GPU is considered, because 2x Fury nearly matches 2x 980Ti, and is well ahead of 2x 980.    The difference in power consumption and PSU requirement makes 980 SLI better in those categories, but if you're spending >$1000 on GPUs and an expensive SLI/Crossfire supporting motherboard, you probably already have a huge PSU and don't care about the extra few dollars in electricity.    I feel like part of it is that the 980 has been on the market for a while, so you can sometimes find them at $450, or even less used, while the Fury is new and demand is higher than supply, so no discounts. I think the $50 difference in MSRP is fair, but a $100 difference in street price makes the 980 a better buy in a lot of cases. Like I said, right now, I'd buy one for $100 less than the Fury, but I still think the Fury is the better card of the two."
hardware,3da1p0,masturbateAndSwitch,3,Wed Jul 15 02:44:09 2015 UTC,"Everything you said is rational and well articulated, I disagree with your DSR comment but I respect it as a valid opinion. But the other poster Seclorum is talking out his ass"
hardware,3da1p0,Griffith1984,-2,Wed Jul 15 02:55:20 2015 UTC,Your face this evening...
hardware,3da1p0,Seclorum,2,Wed Jul 15 03:00:07 2015 UTC,"Question about the VSR vs DSR stuff: How does VSR not have a performance hit? Doesn't it render stuff in 4k(4x the pixels) and then downscale it? Maybe I'm misunderstanding what you mean by performance hit. From my understanding, VSR and DSR do the same exact thing, with the same exact drawbacks. I do have to admit that Fury 4k performance does scale better than Maxwell cards. Is that what you mean by performance hit? The wording is definitely throwing me off D:  Also, about the blur with DSR, nvidia control panel allows you to control the sharpness/smoothness of the downscaled image."
hardware,3da1p0,seanderp,2,Wed Jul 15 04:36:04 2015 UTC,"I was referring to the performance impact of the scaling operation itself, so the difference between running an actual 4K display and running the simulated 4K display. I thought that VSR used the card's built-in hardware scaling logic so it was free, while DSR was done in software on the GPU, so it added a small amount of latency to each frame. I just googled a little and couldn't find anything about this, so I may or may not be right."
hardware,3da1p0,masturbateAndSwitch,2,Wed Jul 15 07:20:13 2015 UTC,I believe he was referencing the performance hit that sometimes shows up because of having to downscale at all.
hardware,3da1p0,Seclorum,-4,Wed Jul 15 04:54:56 2015 UTC,"But I will be damned if I am going to tell people to go buy one of their inferior half assed attempts simply to keep AMD living.   AND THE SINNER HAST REVEALED THINESELF!   REPENT, AND THOU SHALL BE SAVED..."
hardware,3da1p0,Seclorum,-4,Wed Jul 15 02:39:58 2015 UTC,"You sir and people like you are killing competition. The longer people like you claim AMD is competing with Nvidia the longer it will take to get real competition.    Well fuck your hyperbolic bullshit too.    People like you want to sweep everything under the rug and paint a picture so perfect for AMD it sickens me.    Do explain please. Because honestly it intrigues me how referencing fact from empirical 3rd party reviews is suddenly painting a picture to you.   First off you can get brand new 980s for right at 400 right now hell I have seen G1 going for 449. Are you saying a fury is worth 100 more than a G1 980.   And it took me all of 3 seconds to go to google, enter the search term ""GTX 980"" and filter for shopping prices. Go ahead, it takes NO TIME AT ALL TO DISPROVE YOU.   https://www.google.com/search?q=GTX+980&ie=utf-8&oe=utf-8#q=GTX+980&safe=off&tbm=shop  Cheapest possible link is 450, average is bang on MSRP @500 with many many aftermarket cooled variants above 550!   Just because you can find a super cheap card on sale does not mean the price across the board at every outlet will equal or beat that price!    EVGA is releasing B stock 980s nearly every day are you saying super clocked 980 for 400 is the lesser buy compared to 550 for the fury.    Only expedient link that comes CLOSE to 400 is the 450 link from before of an EVGA REFURBISHED ACX 2.0 Classified edition.   So if you want everyone to buy 50 buck above your statement, and a refurb at that... but a REFURBISHED CARD DOES NOT EQUAL AN ACROSS THE BOARD PRICE CUT!   Most cards are well above WELL above 400 dollars.   So again, your point debunked.    Or are you trying to tell a me a product released a year late and for 50 bucks more is the better product simply because it has AMD stamped on it.   What does being late have anything to do with it? By the time Nvidia replaces the 980 it will have been on the market for 2 years...  And when comparing the price Nvidia is asking vs AMD's asking price, and the performance delta between the two cards, Right now at this moment yes the AMD is a good value.   Your Bias is showing mate.    If this is the card you want go for it but when you people try and twist facts to make it seems like it's the only logical choice, it makes me sick.   Only one twisting facts is you bub.   You make unsubstantiated claims about Nvidia price points that are hilariously easy to disprove,  Then you go on to bitch about AMD's product because it was ""Late"" from when you expected it. Not that it's performance at it's price range is very competitive with the asking price for Nvidia's card. No, all you care about was that AMD didn't put out a product when you wanted them too so therefore the entire company is shit and nothing they ever put out has any redeeming value!   WAAAAHHH! Bitch and WHine! My feelings are hurt because AMD made something decent for once! Whhhahhh!"
hardware,3da1p0,Seclorum,-6,Wed Jul 15 01:49:03 2015 UTC,"You are a lost cause your started off on the wrong foot with your filth and just kept sinking.  You admitted finding 980s for 450 and and never once did you answer the question is the fury worth 100 more.   You whole argument is not everyone can find a cheap 980. Well let me borrow a phrase from your post ""Well fuck your hyperbolic bullshit too""  If someone is going to drop 400 plus bones on a GPU I think they can find the 450 price if not something better.   The 980 comes with drivers updated nearly every month, uses less power, is readily available and cheaper.   Now please sir explain to me what the Fury offers that is better.  Because not once in your filth laden tirade did you give one reason why it's better.   You say one thing in one sentence and then you contradict it in another.   Let me spell this is out for you the day AMD makes a card better than everything in it's category. I will be the first one in line to buy it.   But I will be damned if I am going to tell people to go buy one of their inferior half assed attempts simply to keep AMD living."
hardware,3da1p0,Griffith1984,2,Wed Jul 15 02:11:09 2015 UTC,"You are a lost cause your started off on the wrong foot with your filth and just kept sinking.   So lets compare and contrast Hyperbole so everyone here can see how much of an Ass you are.   Me:    I've seen reviews where they compared it against an Evga ACX 2 cooled 980, so very much not a stock card, and the Fury came out ahead or equal to the 980.    You:    You sir and people like you are killing competition. The longer people like you claim AMD is competing with Nvidia the longer it will take to get real competition.    So who is starting on the wrong foot again? You literally said I was killing competition because I was expressing a fact from a review...   Pot Kettle much?      You admitted finding 980s for 450 and and never once did you answer the question is the fury worth 100 more.    I found a single link for a Refurbished Unit on Ebay, hardly reputable and hardly representitive of 980's as a whole. Would I pay 100 dollars more to avoid a refurb?  Yes.   Would I pay 50 dollars more than MSRP to beat the 980 by a respectable margin in some things?   Sure I would.     You whole argument is not everyone can find a cheap 980.    Ahem, no. Your whole Arguement revolved around the  980 being widely available at 400 dollars. Let me quote you for reference because you are stuttering...   First off you can get brand new 980s for right at 400 right now    That's a direct quote of yours. And quite clearly, you are wrong. It takes less than 3 seconds to prove it.      The 980 comes with drivers updated nearly every month, uses less power, is readily available and cheaper.    Oh wow you actually are making points! Sit down please before you overtax yourself!   Drivers - Not the issue that was ever being discussed. This is your personal Axe to grind because apparently not having constant updates because they cant write something good in the first place is the Nvidia way!   Less Power - Do you seriously tell me you sit there and micromanage how much power your system is using? Please tell me you do. Because you would know that you wasted more power on your bullshit spewed here and elsewhere than the AMD card would pull extra!   Redilly Available and Cheaper - You forgot that the Fury just launched tomorrow... and it's faster! Oh and there are plenty of 980's that exceed the Fury's asking price but you cant be bothered to do any searching. It's past your bedtime.     Now please sir explain to me what the Fury offers that is better.   Frame rates, runs cooler (Sapphire Edition), and is to quote one review, ""The Quietest card they have ever tested!"" Anandtech.com     Because not once in your filth laden tirade did you give one reason why it's better.    Namely because you never gave reason why the 980 was better when you called me an enemy of competition for pointing out empirical 3rd party review data.   Who has a tirade of filth again?  Oh yes, YOU DO.     You say one thing in one sentence and then you contradict it in another.    Citation Needed!     Let me spell this is out for you the day AMD makes a card better than everything in it's category. I will be the first one in line to buy it.    Your Bias is showing... Your leaking a little green there bub!     But I will be damned if I am going to tell people to go buy one of their inferior half assed attempts simply to keep AMD living.   AND THE SINNER HAS REVEALED HIMSELF!   REPENT, AND THOU SHALL BE SAVED...."
hardware,3da1p0,Seclorum,-2,Wed Jul 15 02:34:30 2015 UTC,"what you've just said is one of the most insanely idiotic things I have ever heard. At no point in your rambling, incoherent response were you even close to anything that could be considered a rational thought. Everyone in this room is now dumber for having listened to it. I award you no points, and may God have mercy on your soul."
hardware,3da1p0,Griffith1984,1 point,Wed Jul 15 02:52:30 2015 UTC,Did you just quote Billy Madison?
hardware,3da1p0,jawshwa,1 point,Thu Jul 16 14:39:41 2015 UTC,Fuck yeah I did
hardware,3da1p0,Griffith1984,0,Thu Jul 16 15:57:11 2015 UTC,Your face these past few hours
hardware,3da1p0,Seclorum,-5,Wed Jul 15 02:53:28 2015 UTC,Are you saying you been sitting here this whole time and this is all you came up with. Good day sir I did not mean to take up so much of you time.
hardware,3da1p0,Griffith1984,1 point,Wed Jul 15 03:01:03 2015 UTC,No. I simply no longer have the time or inclination to debate with trolls of your caliber.   From the moment you started posting you have rambled on and on about how horrible I am for referencing 3rd party reviews of a product you already have judged as inferior.  Your own personal bias has been lain clear for all to see.   Begone foul troll lest I taunt you another time with Turtle porn!
hardware,3da1p0,Seclorum,-10,Wed Jul 15 03:03:51 2015 UTC,"But I will be damned if I am going to tell people to go buy one of their inferior half assed attempts simply to keep AMD living.   AND THE SINNER HAST REVEALED THINESELF!   REPENT, AND THOU SHALT BE SAVED..."
hardware,3da1p0,Seclorum,1 point,Wed Jul 15 02:40:30 2015 UTC,"Seems fucking pointless.  Why remove this outlier for one review for one card?  Did people really complain so hard that a simple reviewer was forced to change his numbers because ""they didn't like them"".  That's all it looks like."
hardware,3da1p0,w00t692,7,Wed Jul 15 00:49:56 2015 UTC,"Because Project cars cannot be run apples to apples. It forces unnessicary features to be run on one card, and not the other. Meaning the performace is wildly different simply because AMD cannot run the gameworks code in the same way as Nvidia. It's a biased title and very well known for being a biased title.   Unlike Witcher 3 you cant just turn gameworks features off in Cars to get an apples to apples comparison."
hardware,3da1p0,Seclorum,1 point,Wed Jul 15 01:55:41 2015 UTC,Still not including OC results: a joke.  Completely disingenuous and misleading with a chart showing OC average performance.
hardware,3da1p0,Potss,1 point,Wed Jul 15 19:35:44 2015 UTC,"Because half the problem with OC results for the Fury, is that devs dont have tools ready to actually properly overclock it. Namely they still haven't figured out the Voltage Controller.   Which just paints AMD in a bad light for including OC potential as a big feature, then dropping the ball by not having any support for doing it properly.   And sure Nvidia chips OC like monsters out of the box, and you dont have to touch voltage unless your going balls to the wall. But AMD even included 2 8pin adapters to supply more voltage to the card, yet the card in stock format is not drawing anywhere near two 8pins worth. So why have the extra power on tap if it's never ever going to be used?"
hardware,3da1p0,Seclorum,3,Wed Jul 15 22:41:52 2015 UTC,"Ok, then go back and remove every outlier for every card.  Picking and choosing to just subjectively remove one test does not make the card ""better"".    I guess AMD did a better job of threatening to pull review copies of their products than anyone realizes."
hardware,3da1p0,Thunder_Bastard,9,Tue Jul 14 23:49:31 2015 UTC,"That's an extreme outlier, though, and a special case. It performs horribly on AMD cards because of a special function that they don't have, which is a proprietary Nvidia technology. In those cases, I think it's best to mention it, and leave it out of the averages. That way, the reader gets a full picture of the card's performance, and they're notified that if they care about that one specific game, it won't run well unless they have Nvidia hardware."
hardware,3da1p0,masturbateAndSwitch,0,Wed Jul 15 00:49:17 2015 UTC,"Or, turn off the damned feature. How hard is that? A Dutch reviewer had their benches re-ran after heavy criticism about using Hairworks in a Witcher 3 benchmark for benching the Fury X."
hardware,3da1p0,iktnl,5,Wed Jul 15 00:57:56 2015 UTC,You can turn off hairworks in Witcher 3.   You cant turn off Gameworks for Project Cars. Using that title at all skews the numbers. Hence the Author of the review eating some crow and showing what it comes to without the biased title in the lineup.
hardware,3da1p0,Seclorum,1 point,Wed Jul 15 01:52:36 2015 UTC,"I guess AMD did a better job of threatening to pull review copies of their products than anyone realizes.   And it still ""loses"" to the 980 in the reworked graphs, pure avg FPS might be slightly higher, but not enough to justify the price increase, never mind the 99 percentile graphs  Honestly, all these average comparisons and ratings are all stupid, just look at the benchmarks for games that interest you and forget the rest."
hardware,3d9p63,namae_nanka,40,Tue Jul 14 16:23:51 2015 UTC,So on their graphs they list the cards from highest performing to lowest performing... and yet on the 2nd graph they have the Fury XFire at the top when its not the highest performing?
hardware,3d9p63,Stingray88,33,Tue Jul 14 16:51:50 2015 UTC,"This man asking all the right questions. I was thinking that they might be following the bit-tech strategy of ranking the cards by the minimum fps, so Fury might have edged 980Ti by that. But apparently that hasn't happened in other graphs so most probably incompetence."
hardware,3d9p63,AdmiralApocalypse,4,Tue Jul 14 17:23:47 2015 UTC,Little things like that set off my bullshit detector.  I'll just wait for another site to validate those numbers.
hardware,3d9p63,BakingLoaves,7,Tue Jul 14 20:41:41 2015 UTC,"Well, the numbers seem expected. They also do that mistake in another graph, 4k SoM bench where Fury is below 980Ti despite being faster.  http://core0.staticworld.net/images/article/2015/07/mordor-high-4k-100596284-large.png  They were rather quick before with the fury card reviews and have seen them posted here often."
hardware,3d9p63,XorFish,2,Wed Jul 15 01:44:35 2015 UTC,"I noticed this BS aswell, they change ordering 4x with a quick browse. They even change ordering within a single game (BF4). At first I thought just making the Fury look as good as possible. But now just incompetence"
hardware,3d9p63,myodved,1 point,Wed Jul 15 07:53:29 2015 UTC,And they act as if a 5960X can not be a bottleneck. With a turbo of 3.5ghz it will bottleneck games that don't use more than 4 threads in high fps with powerfull cards.
hardware,3d9p63,ducttape36,2,Wed Jul 15 07:27:28 2015 UTC,"Perhaps they ordered everything by minimum framerate, and then, when a tie was hit, it was sorted alphabetically? That is the only possibility I could see off the top of my head. Of course, that theory goes to crap with the later charts, where most are by avg and they have one, the 4k Mordor chart, with completely switched on both... shrug"
hardware,3d9p63,Stingray88,1 point,Tue Jul 14 22:06:22 2015 UTC,I just figured it was for consistency. They are in the same order on every graph.
hardware,3d9p63,ducttape36,13,Tue Jul 14 21:23:43 2015 UTC,"No they're not. They put whichever cards performed best on top, and the second goes second. Sometimes that the Fury X crossfire and sometimes it's the 980 Ti SLI. It's not the same order on each graph.   They just messed it up on the 2nd graph."
hardware,3d9p63,litehound,1 point,Tue Jul 14 21:34:01 2015 UTC,"my bad, i guess i wasnt paying very close  attention."
hardware,3d9p63,Pyrominon,9,Tue Jul 14 21:36:37 2015 UTC,"Hold up, what happened to give the 980Ti SLI the lowest minimum FPS in Shadow of Mordor 1440 and Last Light 1440? And that the Crossfire Fury got the lowest minimum in 4k Shadow of Mordor, 4k GTA V and 1440 Battlefield? And why was it worst in GTA V 1440? Genuinely asking all these."
hardware,3d9p63,litehound,7,Tue Jul 14 19:40:02 2015 UTC,The Fury likely has lower minimums at 4k due to vram bottlenecks.
hardware,3d9p63,spencer32320,1 point,Tue Jul 14 22:21:04 2015 UTC,But why was it worst overall in GTA at 1440?
hardware,3d9p63,litehound,3,Wed Jul 15 02:10:43 2015 UTC,"Cause that game uses a ridiculous amount of vram, even at 1440p."
hardware,3d9p63,hibbel,2,Wed Jul 15 03:21:29 2015 UTC,Then why wasn't it worst in 4k? I'm confused.
hardware,3d9p63,spencer32320,3,Wed Jul 15 03:29:26 2015 UTC,"Maybe because at that point, 6GB wasn't cutting it anymore, either, so both cards running out of VRAM, the faster VRAM won?  Just speculating."
hardware,3d9p63,rationis,2,Wed Jul 15 04:30:00 2015 UTC,"Not sure, I would chalk it up to a bad benchmark, where something else made it lag for a second. Unless those outliers are in other reviews I would ignore them."
hardware,3d9p63,spencer32320,2,Wed Jul 15 04:20:06 2015 UTC,"Is it just me, or is that game getting harder and harder to run well as the release patches and updates?"
hardware,3d9p63,Seclorum,2,Wed Jul 15 09:29:59 2015 UTC,No its not you. They keep trying asinine things to try and stop modders. It's been effecting performance.
hardware,3d9p63,NLWoody,1 point,Wed Jul 15 16:55:50 2015 UTC,I heard the latest patch really tanked performance across the board to let Rockstar run a DRM service on the game to prevent modding via the auspices of preventing 'hacking' in their multiplayer component.
hardware,3d9p63,ant51508,3,Thu Jul 16 03:13:27 2015 UTC,probably ran out of video ram.
hardware,3d9p63,spencer32320,1 point,Tue Jul 14 22:42:33 2015 UTC,with 6GB ? no way.
hardware,3d9p63,I-never-joke,3,Wed Jul 15 01:10:51 2015 UTC,Definitely possible with shadow of mordor on ultra. The textures are completely uncompressed
hardware,3d9p63,benb4ss,23,Wed Jul 15 03:22:14 2015 UTC,Overclocking all the cards by 4% because the Fury has a 4% over stock isn't remotely a fair comparison.  You don't get to gimp the greater (overclocking) potential of the compitition because the card your reviewing struggles with it.
hardware,3d9p63,d2_ricci,1 point,Tue Jul 14 23:03:39 2015 UTC,"Yeah, 4% OC on a stock cooler vs a beefy aftermarket cooler. I would have liked to see an aftermarket 980ti with a clock core at 1500Mhz."
hardware,3d9p63,I-never-joke,-1,Fri Jul 17 12:47:27 2015 UTC,"4% seems a bit modest for nVidia but I think the idea would be to ensure every card in the market can hit the numbers presented. I don't know, it could also be that what you are saying. I don't want to use the excuse that voltages are locked currently but I can imagine that would play into the hands of the reviewer.    I do like to see close to full potential OCs and stock in reviews but what would you suggest is a fair % to show what all readers can do?"
hardware,3d9p63,d2_ricci,3,Tue Jul 14 23:23:27 2015 UTC,"Like you said both a full and stock would be fair, theres such a huge variety in performance and price for the same card depending on the cooler you buy."
hardware,3d9p63,Nixflyn,2,Tue Jul 14 23:45:47 2015 UTC,Well like full oc should be a little less than what you achieve stable to cover for the most of your readers hitting those clocks
hardware,3d9p63,litehound,2,Tue Jul 14 23:55:57 2015 UTC,"Modest isn't a strong enough word here. It's more like 25-30%, depending on manufacturer."
hardware,3d9p63,rotorain,-4,Wed Jul 15 05:27:01 2015 UTC,"They overclocked the other cards for comparisons? wow wtf and then call it 'fair'.   Not remotely a fair comparison indeed, but not for your reason and they just label it as 980Ti and 980 a la Techreport's review. AMD can't catch a break this time round with reviewers hell bent on showing them in the worst light."
hardware,3d9p63,handlewithnocare,5,Wed Jul 15 02:29:59 2015 UTC,"He's saying it's not fair that they aren't pushing the nVidias to their limits, the opposite of what you said."
hardware,3d9p63,R403Q,2,Wed Jul 15 03:24:15 2015 UTC,"He said that the sapphire fury comes with a 4% OC, but not all of the brands will so he just matched the mild OC on the other cards for comparison. I'm sure you can buy a 980ti that comes with a mild OC like that, so it's not entirely ridiculous. It also wouldn't be fair to underclock the fury 4% to match it to ""stock"" clock just to compare, so I think his decision was fine there."
hardware,3d9p63,capn_hector,2,Wed Jul 15 03:27:24 2015 UTC,"Then they should put the clocks in there like legit reviews do. And no it isn't fine, their graphs are then misleading."
hardware,3d9p63,masturbateAndSwitch,18,Wed Jul 15 06:33:02 2015 UTC,"I'm wondering how the Fury is faring so well (even in single card vs the 980Ti) when everyone was quick to jump to calling the Fury X garbage because it didn't outright stomp on the 980Ti.  I've been considering buying a 980Ti but hate NVidia as a company, but I don't want to just openly buy an inferior product due to brand loyalty.   When voltages are unlocked for overclocking the core/memory of the Fury X I'll probably leap on that, considering how well the Fury is supposedly performing."
hardware,3d9p63,d2_ricci,29,Tue Jul 14 16:48:53 2015 UTC,"I think the Fury X hype got out of hand, and coupled with the ""overclockers dream"" controversy, many people felt let down by it. I guess people are expecting less from the Fury. You really can't go wrong with either. They're both excellent cards."
hardware,3d9p63,milo09885,3,Tue Jul 14 17:02:55 2015 UTC,"The Fury X seems to be bottlenecked by something (ROPs?  geometry processing?) whereas my impression (potentially unfounded) is that the Fury seems to have less of the extraneous (bottlenecked) elements.  Much like the 980 Ti vs the Titan X, it's pretty close to the same performance for less money.  Particularly after the AIBs got their hands on them.  In percentage terms, of course, it's not quite as steep a discount as the 980Ti is, relative to its full-chip counterpart, because the Fury X isn't as overpriced as the Titan X.  But it doesn't have 12GB of memory or 20-30% overclockability either, though.  To agree with mAS,expectations also likely play a not-insignificant role as well, I was not happy with all reviewers reporting a 5% overclock margin after the ""overclocker's dream"" comment, especially when GM200 overclocks so well."
hardware,3d9p63,d2_ricci,10,Thu Jul 16 05:50:21 2015 UTC,"Yeah, I think it's the difference in expectations. The Fury is doing what people are familiar with AMD for, which is being the price/performance champion, delivering near Nvidia flagship performance for a price closer to Nvidia's 2nd tier card. The Fury X is targeted right against the 980Ti and loses most benchmarks (but the AIO cooler and better multi-gpu gives it a niche, preventing its price from being totally unjustified), and it was hyped out of control by fanboys"
hardware,3d9p63,thinkythought,5,Tue Jul 14 17:52:15 2015 UTC,"Not sure which drivers were used for the fury but the 15.7 unified drivers raise minimum fps in most games which have been increasing performance by up to 10% in some games. I have yet to see 15.7 not raise min fps in a game so far. These drivers have been great for all GCN based AMD cards, including my old R8 280x."
hardware,3d9p63,d2_ricci,1 point,Tue Jul 14 16:59:06 2015 UTC,Can you expand on the all GCN part a bit? I've yet to get the new drivers for my HD 7950 and wasn't expecting any performance gains at all.
hardware,3d9p63,rotorain,2,Tue Jul 14 22:48:21 2015 UTC,7800/7900 and up  Also make sure you run DDU in safe mode before install
hardware,3d9p63,Stingray88,1 point,Tue Jul 14 22:50:36 2015 UTC,"I saw a definite boost from it overall, but also weird hitching/minimum framerates. It's enough of a boost that i can get 58-60gps non stop in gta 5 at mostly ultra settings with a 290x at 1080p.  I'm on windows 10 though so who knows, my experience is probably pointless."
hardware,3d9p63,C4ples,1 point,Wed Jul 15 00:33:27 2015 UTC,I've been reporting any anomalies to the feedback app. It's seems like issues like this will get resolved though. I don't have gta so I wasn't aware but check the changelog since there are a few other games like witcher 3 that have known issues.  EDIT:  Also disable game dvr in 10166 win10. That causes issues. Type Xbox under start menu and click settings
hardware,3d9p63,CaptSkunk,5,Wed Jul 15 01:04:57 2015 UTC,"Addressing the brand thing, I'll probably pick up a fury soon just because it isn't nvidia. I agree, the 980ti is probably the card to get right now considering it's performance and pricing (at least here in the US), but I really want AMD as a company to succeed so nvidia will have competition.   Imagine how hard we'd all be shafted on price/performance if there was only one gpu manufacturer. The competition between these two has had such a great effect on technology advances, performance increases, and competitive pricing that no matter which card you buy you are never being taken advantage of.  It's not gonna keep me up at night knowing I could be getting 3 more fps if I had just bought an nvidia card, and I'm willing to make that tiny sacrifice so that PC gamers as a whole are better off."
hardware,3d9p63,Anally_Distressed,8,Wed Jul 15 04:18:29 2015 UTC,I didn't see anyone calling the Fury garbage and getting upvoted. Most people have been cautiously optimistic.
hardware,3d9p63,grogleberry,6,Tue Jul 14 16:53:40 2015 UTC,"It happens more in BAPC and gamingpc, which gives you some metric to understand what the people doing it are like."
hardware,3d9p63,Anally_Distressed,2,Tue Jul 14 21:10:16 2015 UTC,"It was just way too much hype. Everybody expected AMD to have this card that would just annihilate the competition and when the Fury X showed that it just runs neck in neck with the competition (sometimes it wins, sometimes the 980ti wins), people started grabbing pitchforks and burning the village down...  Then, many people expected way too much of HBM. What so many people did not realize is that HBM is a new technology and it needs time to mature. Let's see what HBM 2.0 brings to the table.  Lastly, there is an issue of serious pump noise/coil whine on Fury X cards. It's like nobody sat down, listened to the card in operation and thought, ""this is kinda loud; gamers might be annoyed by such noises."" AMD has already said that they are looking at the pumps and what can be done about them.  In the end, too much hype and way too high expectations for the card."
hardware,3d9p63,C4ples,4,Wed Jul 15 04:42:14 2015 UTC,"The Fury X isn't garbage from purely a performance perspective, but its price to performance ratio is terrible. It's competing at the same price point as the 980ti while being outperformed by it on almost all fronts.   The Fury X should honestly have released at the Fury's price point for it to have been widely successful."
hardware,3d9p63,Gunjob,22,Tue Jul 14 16:57:39 2015 UTC,"It isn't ""terrible"".  The 980Ti amazing. The FuryX is a bit worse.  And it's murkier still in places like Europe, where the Fury X is as much as €100 cheaper than the Ti."
hardware,3d9p63,makar1,12,Tue Jul 14 17:26:34 2015 UTC,"I'm not calling the Fury X terrible, I'm saying the pricing, at least in the US, doesn't make sense.   The Fury makes far more sense in that regard. It performs a bit worse than the 980ti and is a cut above the 980, and its price reflects that, whereas the Fury X's doesn't. This is why OEMs slashed the prices on the 980ti and 980s when the Fury came out and didn't react to the Fury X whatsoever."
hardware,3d9p63,Colorfag,4,Tue Jul 14 20:47:26 2015 UTC,"It's been that way for a while no matter what manufacturer it is. When it came to performance/$ in the same architecture the 570 was better than the 580, the 670 was better than the 680, the 7950 was better than the 7970, and the 290 was better than the 290X.   To address the Fury X performance comment, not at UHD. It had demonstrated itself to be on-par with the 980ti. It all just depends on what you want the card to do. I'll most likely be getting the Fury X because AMD has a better multi-screen environment."
hardware,3d9p63,C4ples,7,Tue Jul 14 21:14:15 2015 UTC,"It's cheaper in the UK than the 980ti. Once the stock shortage is sorted it will be £509.99(can be found for that in many places.), a bunch of retailers are gouging due to the stock issues and selling for £545.  Cheapest 980ti is 530ish and if you want a ti with a decent cooler or just an evga acx you're looking at £600."
hardware,3d9p63,R2G3,1 point,Tue Jul 14 17:42:29 2015 UTC,"980Tis have been £509.99 several times too, and are now £499.99."
hardware,3d9p63,BlayneTX,2,Thu Jul 16 13:00:07 2015 UTC,The price and the subpar sub 4k performance drove people nuts.   I havent been paying attention to the fury as much to see if the performance problems are the same with sub 4k benchmarks?
hardware,3d9p63,R2G3,2,Tue Jul 14 23:26:37 2015 UTC,People are extremely quick to vocalize their wanton hate for AMD numbers-be-damned.  Most of it was also pre-15.7 Catalyst.
hardware,3d9p63,RyanBlack,4,Tue Jul 14 21:07:14 2015 UTC,"Call me when you get this type of 4K performance out of one card. I've had nothing but trouble with dual GPU configurations and consider them to be the hardware equivalent of beta software, where it works 90% of the time but 90% of your time is spent dealing with the other problems. I desperately want to invest in 4K gaming but it's just not ready if you need a 2 card config to get it at over 60 FPS consistently."
hardware,3d9p63,R2G3,3,Tue Jul 14 21:39:22 2015 UTC,I've had 980 SLI for about a year now and have never had a single issue other than the top card getting hot. Maybe things have gotten better? This is my first dual GPU setup and I only did it because I wanted a big 4k monitor.
hardware,3d9p63,BlayneTX,2,Tue Jul 14 22:05:57 2015 UTC,"It could be, I had some 5850s I think that crashed all the time. It seemed to be a problem with the Catalyst drivers and it made it wholly worthless to have invested in a second card. I swore dual GPU rigs off since I'm still somewhat price conscious."
hardware,3d9p63,Colorfag,3,Tue Jul 14 22:56:28 2015 UTC,Crossfire has come a long fucking way since the 5000 series man. Its pretty great now after the Omega driver release.  My advice? Pick up some cheap 280s/290s on /r/hardwareswap and have ridiculous performance for a few hundred bucks.
hardware,3d9p63,Tylerdurden516,1 point,Tue Jul 14 23:54:58 2015 UTC,"I've got a Titan Black now so I'm not exactly hurting for performance (I know, I know, it's a pro card that's just been repurposed for gaming, but I got a good deal on it.) I just can't do screaming 4K yet, and I'm so reluctant to make that leap without it. I'll probably go piecemeal anyway - monitor first, then GPU, then look at a system with DDR4 and all that good stuff since I'm on Haswell as it is."
hardware,3d9p63,atriax,1 point,Wed Jul 15 02:00:18 2015 UTC,"Ah, I have not tried a Crossfire setup and frankly all the driver issues I read about makes me afraid to invest that much money if it might not work sometimes."
hardware,3d9p63,hibbel,1 point,Tue Jul 14 23:00:43 2015 UTC,"Its not bad, but if youre wanting to play the latest and greatest games, youll run into problems. It sometimes takes months for it to get fixed, if at all (like Titanfall). If youre lucky, they get fixed before the game launches (like GTAV)."
hardware,3d9p63,R2G3,1 point,Tue Jul 14 23:29:20 2015 UTC,"does your case have a side vent for the gpu's? If it does, throw an EKWB FF5 Furious Vardar fan in it. The thing is the highest performing 120mm fan i could find, and at full blast my top 980 ti card is either the same temp as the lower, or at most 3 degrees higher. Also took my average temps down from the mid 70's to the mid-high 60's."
hardware,3d9p63,Bitech2,1 point,Wed Jul 15 03:20:23 2015 UTC,Gonna be a while dude. Dunno what you're expecting. Pretty much miracles honestly. 4k is fucking insane when you try to render it.
hardware,3d9p63,sdguero,3,Tue Jul 14 22:20:48 2015 UTC,"I don't know what he's expecting but years ago, the situation was similar for Full HD, which can now be achieved on a single card.  I'll wait for it, too.  And it will happen, with consoles holding back game development and GPUs rushing forward. I expect the next generation to be true 4k cards, allowing highest settings on games designed to run at low-equivalent on consoles, at 4K, at 60FPS. Simply because the spread in settings between highest and lowest can only ever be so wide before you'd have to basically develop two completely different engines, which isn't going to happen."
hardware,3d9p63,BinaryRockStar,1 point,Wed Jul 15 04:34:56 2015 UTC,"I'm aware. It's definitely going to take some sort of breakthrough to get there. I'm not even sure if Moore's Law will make it happen sooner than later. Maybe the demand for mobile VR will drive the innovation to make it happen. Alternatively, FreeSync and Gsync might reduce the need for hitting a full 60 FPS though I understand anything under 40-45 can still look choppy. Otherwise it's just upgrading to a 4k monitor and dealing with hit or miss titles at 4k or 1440p."
hardware,3d9p63,cvance10,4,Tue Jul 14 22:54:54 2015 UTC,2Fast2Fury
hardware,3d9p63,XaeroR35,2,Wed Jul 15 02:25:37 2015 UTC,"I don't think it matters (totally dependent on the application and I don't know what the requirements are for those GPU tests) but only using 8GB of system RAM when testing 2x GPUs that each carry >=4GB of RAM seems kinda silly.   I would spend another $200 on system memory (and get to 16GB) before dropping $1000+ on GPUs. And if I was building some mega-rig that PC World is paying for, I'd definitely have more RAM."
hardware,3d9p63,Potss,2,Tue Jul 14 23:04:57 2015 UTC,What difference does system RAM size make to video card performance?
hardware,3d9p63,Seclorum,1 point,Wed Jul 15 00:20:16 2015 UTC,"Maybe nothing, but it could rule out any potential bottlenecks."
hardware,3d9p63,Noirgheos,1 point,Wed Jul 15 07:05:05 2015 UTC,What is with the min FPS on GTA ? Is that what superb optimization looks like?
hardware,3d9p63,Seclorum,1 point,Wed Jul 15 16:50:25 2015 UTC,"Sigh...more reviews with no OC numbers.  In other words a misrepresentation of true performance.  Stock and OC should be put side by side, so people know what they are actually getting depending on their needs."
hardware,3d9p63,TheImmortalLS,0,Wed Jul 15 19:38:02 2015 UTC,Because the Fury cant be properly tested for overclocking potential because nobody has the ability to voltage control yet.  It boggles my mind that they made it a major selling point and yet did absolutely nothing to help devs or even 3rd party AIB manufacturers a heads up for figuring out the new voltage controller.
hardware,3d9p63,Raising,1 point,Thu Jul 16 03:07:35 2015 UTC,"Right now, the G1 Gaming 980 is $600, and the Sapphire R9 Fury(non-x) is $750. The 980 may go even lower due to a summer sale from the local store.  Which one should I get for 1080p gaming?"
hardware,3d9p63,DHFearnot,1 point,Wed Jul 15 19:40:21 2015 UTC,"For strict 1080p only you get better performance out of the 980. But the trick is, both perform well above 60hz at that resolution. So unless your looking to max out a 144hz panel at that rez it doesn't much matter which one you pick. You can always DSR/VSR a 1440p resolution down to 1080p for better sharpness as well, which puts the Fury very slightly in the lead in some titles.   Personally, given the price disparity in your area, I would go for the 980 g1. $150 is a bit steep for just slightly better at resolutions your not running.   $50 - $100 maybe depending on secondary factors but not $150."
hardware,3d9p63,Raising,1 point,Thu Jul 16 03:05:40 2015 UTC,Are they using absolute minimum fps or 99th percentile fps?
hardware,3d9p63,DHFearnot,-8,Thu Jul 16 20:19:56 2015 UTC,pump whine times two ?
hardware,3d9p63,Raising,5,Wed Jul 15 02:46:21 2015 UTC,It's fury not fury X you retard.
hardware,3d9p63,DHFearnot,0,Wed Jul 15 03:06:20 2015 UTC,"Jokes on you, it still has coil whine."
hardware,3d9p63,Raising,1 point,Mon Jul 20 16:27:20 2015 UTC,You said pump whine there is no pump.
hardware,3d9p63,BlayneTX,1 point,Mon Jul 20 17:09:06 2015 UTC,but there is whine.
hardware,3da2m1,joetemus,12,Tue Jul 14 17:56:00 2015 UTC,Aaand it's gone.
hardware,3da2m1,justinxduff,4,Tue Jul 14 19:36:34 2015 UTC,Meanwhile in Canada $729 each.
hardware,3da2m1,arrise,4,Tue Jul 14 20:58:44 2015 UTC,"Holy shit the exchange is so bad, 5 cents more than when I last checked two weeks ago."
hardware,3da2m1,pb7280,1 point,Tue Jul 14 23:53:27 2015 UTC,"Not just Canada, but many currencies are losing lots of power to the dollar atm. Pretty bad for the world economy."
hardware,3da2m1,pabloe168,0,Wed Jul 15 06:21:34 2015 UTC,That's not how it works. Not even close.
hardware,3da2m1,Pinecone,2,Thu Jul 16 02:46:51 2015 UTC,"That's totally how it works, and its super close."
hardware,3da2m1,pabloe168,1 point,Thu Jul 16 03:08:04 2015 UTC,How does it work?
hardware,3da2m1,MrsTtt,2,Fri Jul 17 10:08:00 2015 UTC,Isn't it supposed to be 550?
hardware,3da2m1,tedlasman,5,Tue Jul 14 19:06:31 2015 UTC,MSRP emphasis on the S.  AIBs and retailers can charge whatever they want.  Sapphire charges exactly the MSRP from what I have seen.  Newegg has and will charge over the MSRP.  This is likely a Newegg tax.
hardware,3da2m1,Maldiavolo,2,Tue Jul 14 19:19:52 2015 UTC,559.99 for non-OC version here: http://www.newegg.com/Product/Product.aspx?Item=N82E16814202157
hardware,3da2m1,rockycrab,3,Wed Jul 15 01:15:06 2015 UTC,$730 in Canada. Wooooo.
hardware,3da2m1,I_Xertz_Tittynopes,1 point,Wed Jul 15 02:24:03 2015 UTC,"I KNOW, RIGHT!"
hardware,3da2m1,tedlasman,1 point,Wed Jul 15 03:12:07 2015 UTC,619€ in Europe Not bad at all.  Thinking so much of pulling the trigger... but I'm afraid of having a giant leapfrog next year with HBM2
hardware,3da2m1,felixwraith,-20,Wed Jul 15 18:06:04 2015 UTC,What a rip off.
hardware,3da2m1,LiberDeOpp,9,Tue Jul 14 18:31:25 2015 UTC,"It went out of stock within like 1 hour, so apparently not."
hardware,3d8dzd,nwgat,12,Tue Jul 14 08:42:39 2015 UTC,"Hilbert makes some interesting comments on the conclusion page:   Lets talk reality first, 20nm fab nodes should have been here, they failed. So now the GPU industry is waiting on 16 and 14nm nodes to become ready for the big gun GPUs.    ...   The end result is a chip sized roughly 5x5cm; imagine this chip at 14nm, yup it would be half the size with reduced voltage needed.   All other things equal, going from 28nm to 14nm wouldn't result in half the size; it would only be a quarter as large. I'm not sure if the author is simply forgetting to square that ratio, or if he's alluding to the confusing details regarding 16nm/14nm processes.  My impression is that TSMC's ""16nm"" finfet process uses the same lithography hardware as 20nm planar, resulting in similar feature sizes. Calling it ""16nm"" is almost like marketing ploy to advertise its performance, but the areal density stays just like 20nm.  If that's what Hilbert is getting at, then yes 28nm -> 20nm is about twice the areal density."
hardware,3d8dzd,Kaldskryke,3,Tue Jul 14 15:25:54 2015 UTC,But apparently 16nm FF+ is smaller than 16nm FF.
hardware,3d8dzd,Exist50,1 point,Tue Jul 14 18:32:14 2015 UTC,"It would be somewhere between half and quarter the size because interconnects don't scale as well as transistors. As the number of computing units in a chip increase (CUs or textures), the interconnect area explodes. And unlike transistors, you cannot halve the length of wires on a new process due a number of reasons, some of them being increased risk of electromigration, cross interference, complications in location of chip hotspots, clock skews etc."
hardware,3d8dzd,Chunke,10,Tue Jul 14 19:18:31 2015 UTC,"It sounds like a really sloppy release from them, they really layed into AMD in the conclusion."
hardware,3d8dzd,Jamolas,14,Tue Jul 14 12:07:54 2015 UTC,"I really wish they'd just stick to the performance of the card itself.  Okay, the business side of the launch could've been done better but no one will care about that when they're deciding on what to add to their cart."
hardware,3d8dzd,Nutchos,4,Tue Jul 14 17:02:01 2015 UTC,"yeah, but still a good review"
hardware,3d8dzd,SllepsCigam,3,Tue Jul 14 12:29:57 2015 UTC,Looking at these benchmarks kind of just makes me want to get a 295x2
hardware,3d8dzd,Brparadox,-2,Tue Jul 14 20:50:09 2015 UTC,"Bought it, returned it and bought two 970s. I'd much rather be bottlenecked by the vram than the 100s of problems with the 295x2"
hardware,3d8dzd,Anaron,7,Wed Jul 15 01:09:39 2015 UTC,What problems did you experience?
hardware,3d8dzd,BlayneTX,7,Wed Jul 15 12:29:48 2015 UTC,I love this guy's reviews. He writes like he's just talking to his buddy.
hardware,3d8dzd,logged_n_2_say,8,Tue Jul 14 12:49:48 2015 UTC,"It's trouncing the 980 at basically everything. Fcat data looks amazing, albeit at 1440p. They aren't high on the idea of good overclocking with voltage control. Also the reason it's late is they got the dates mixed up."
hardware,3d8dzd,weks,17,Tue Jul 14 11:47:29 2015 UTC,As it should considering it's $50 more expensive (up to a $100 more depending on where in the world you live).
hardware,3d8dzd,Cassiuz,9,Tue Jul 14 13:30:00 2015 UTC,"It's £100 more than the 980 here, so around $150 (I think, haven't checked the exchange rates recently).  It's just too much for the extra performance over the 980. It's a shame as it's a nice card otherwise."
hardware,3d8dzd,gahhg,1 point,Tue Jul 14 15:49:14 2015 UTC,yeah but there has hardly been any stock in the UK. With more supply the price should drop to something more reasonable.
hardware,3d8dzd,Cassiuz,1 point,Wed Jul 15 15:10:19 2015 UTC,"There's hardly any stock of the FuryX anywhere really. That did cause some high prices at the start, with some retailers charging £100 extra to get one at launch. Prices are back to the manufacturers RRP now though.  Stock of the Fury is much better, most places have some in stock and have since the launch yesterday. The prices seem to be what AMD had indicated they would be in the launch presentation, it's just baffled me that they actually followed through with prices that high.  Nvidia must be laughing at the moment, AMD are basically doing their market for them!"
hardware,3d8dzd,logged_n_2_say,2,Wed Jul 15 15:16:32 2015 UTC,"I personally prefer reference in there somewhere, since to me it's easier to compare across different reviews. In a perfect world you would all of the cards in a single chart but that can also get cluttered.   But I do like head to head non reference reviews, it's just nice to have reference models as well so you can get lots of information."
hardware,3d8dzd,LiberDeOpp,4,Tue Jul 14 16:49:24 2015 UTC,"Any review that I see that compares a reference card to OC board gets an automatic down vote.  Unfortunately, this sub seems to be amd favored right now probably due to new cards and r amd going down."
hardware,3d8dzd,namae_nanka,2,Tue Jul 14 20:31:11 2015 UTC,Fcat data looks amazing   and comparing it against 980Ti instead of 980.
hardware,3d8dzd,spyder256,1 point,Tue Jul 14 18:37:13 2015 UTC,"I think one of the big things that will determine whether I get this as an upgrade to my gtx 970 will be if they can fix the overclocking. I mean, I thought that was one AMD's like, things. You know? Like that their cards are soooo great for overclocking, and no limits and such and such.  Overall not anywhere near as disappointing as the Fury X (fuck that card). I mean it's good enough that I'm actually considering getting it. But still just a couple things holding it back."
hardware,3d8dzd,MutantDoughnut,4,Tue Jul 14 15:47:51 2015 UTC,Supposedly its less AMD outright locking voltage control (thus overclocking ability) and more the people working on overclocking software having trouble sussing out how to do so.
hardware,3d8dzd,spyder256,0,Tue Jul 14 17:41:26 2015 UTC,"Well, let's hope they figure it out because that could really help this card"
hardware,3d8dzd,everguyagain,-38,Tue Jul 14 18:29:38 2015 UTC,1st
hardware,3dae9m,Exist50,5,Tue Jul 14 19:14:11 2015 UTC,Appears to be using a reference PCB at reference clocks.
hardware,3dae9m,TaintedSquirrel,2,Tue Jul 14 19:15:19 2015 UTC,Looks very similar to the Tri-X model.
hardware,3dae9m,ZgMc,0,Tue Jul 14 21:52:12 2015 UTC,They sure worked hard on that design... To bad because I love the 290/290x pcs+ cooler.
hardware,3da9t2,Oafah,10,Tue Jul 14 18:44:48 2015 UTC,"Possibly, but the GT730/740 are still relatively new, i dont see them being pushed aside that quickly, Nvidia ran their entire GTX7 series with the GT6s servicing the lower end.  That said, the GT740 is pretty much a GTX650 in disguise, so the 750 getting rebadged to a GT940 wouldnt be unprecedented."
hardware,3da9t2,everyZig,3,Tue Jul 14 19:15:33 2015 UTC,"GT730 is a great card, runs many games fluently. Of course, this level of hardware isnt popular on this board"
hardware,3da9t2,poematik,4,Tue Jul 14 20:31:08 2015 UTC,"Yeah, the gddr5 one is pretty much a GT640, quite a decent card for the $55 or so it costs, and not bad for entry level gaming."
hardware,3da9t2,everyZig,3,Tue Jul 14 20:32:52 2015 UTC,"I hope they do.  The GTX 950, having a cut-down GM206 GPU is most likely going to need a power connector. The GTX 750 Ti and 750 being first generation Maxwell don't have things like third generation delta color compression, DSR, MFAA and VXGI.  I'm particularly interested in DSR and MFAA, so if I'm going to buy video card I want it to have those features."
hardware,3da9t2,Yearlaren,2,Tue Jul 14 23:09:24 2015 UTC,But the gtx 750 does come with DSR and those non significant features.
hardware,3da9t2,MilkyTones,1 point,Thu Jul 16 03:47:03 2015 UTC,I read it doesn't.
hardware,3da9t2,Yearlaren,2,Thu Jul 16 03:53:26 2015 UTC,DSR is available for 400 series and up.  http://www.geforce.com/hardware/technology/dsr/supported-gpus  MFAA is Maxwell 2.0 exclusive though.
hardware,3da9t2,ThePsycho2,1 point,Thu Jul 16 07:53:21 2015 UTC,"I have a 750 ti and can assure you this one does DSR with no problems. I imagine the regular 750 can too, but i'm not too interested in checking that."
hardware,3da9t2,dajigo,1 point,Mon Jul 20 06:59:49 2015 UTC,No MFAA though?
hardware,3da9t2,Yearlaren,1 point,Mon Jul 20 16:39:26 2015 UTC,"Nope, just the usual AA and TXAA (which I don't really like and never use)."
hardware,3da9t2,dajigo,2,Mon Jul 20 18:51:02 2015 UTC,"If you want to build a Steam Machine, have you considered Fractal Design Node 202? The volume is a bit higher than the ML06-E, but in my case I would consider that an acceptable tradeoff to be able to use full-size GPUs."
hardware,3da9t2,ButteredNani,2,Tue Jul 14 23:01:03 2015 UTC,"I have, as well as the RVZ02 and the Azza case of a similar layout. I'm really not a fan of how long those cases are, and the goal isn't necessarily to build a full-strength machine. As my primary, I already used a dual-chamber layout Core V1 with a GTX 970. This secondary build is meant to be for my girlfriend to use when we play lan games, and it really doesn't need anything quite that beefy.  Plus, I sort of just want to build a system in the ML06-E because it's neat. No other reason."
hardware,3da9t2,cardfire,1 point,Wed Jul 15 01:07:17 2015 UTC,"I built my GF a G3258 with an nV 750 Ti, into the qui sleet of a Raijintek Metis case.  It's great as a secondary. The whole system, under load, fits inside 125w."
hardware,3da9t2,Belmonkey,2,Thu Jul 16 08:58:24 2015 UTC,"It's difficult to guess what they will do with the 950, but I'm of the opinion that Nvidia might do something like this if there is a 950:  Cut the GM206 die in half to a 512 shader GPU like the 750; if the same perf / W is maintained from the 960, it would end up a ~60W GPU like the 750 ti that runs on only power from the PCI slot. Unlike the 750 (ti), the 950 would have 2x as many ROPs / ~2x the pixel fillrate, just like the still 128-bit GM206 960 has. It would run at similar clock speeds as the 960: ~1200 MHz boost clock and 7GHz memory clock, a big jump from the 5/5.4 GHz mem clock of the 750 / 750 ti. As a 2nd gen Maxwell GPU, it would also have the type of overclocking headroom expected of one (~1400+ core clock and up to 8GHz mem, perhaps requiring a power connector by that point). Considering an overclocked 750 can manage around stock 750 ti performance, I'm sure the 950 could muster similar performance to it with its boosted clock speeds and go a bit further with overclocking (perhaps towards performance of a 750 ti with a modest OC?). At any rate, it'd probably replace the 750 ti's performance area and introduce the GPU at maybe $120, leaving a decent price gap between it and a potential 950 ti.  I kinda have my doubts that a 640+ shader Maxwell GPU could again manage to work without a power connector unless it's clock speeds / overclocking got held back like the 750 (ti), so I think a potential 950 ti would require a power connector while a 950 could just get by without one. I personally don't think the 950 will have over 512 shaders (unlike some rumors) because it just would seem a bit too much to put in the $110-$120 price range when  just a bit more performance potential than the 750 would be needed to match AMD's $107 R7 360."
hardware,3da9t2,asfocgf,1 point,Wed Jul 15 18:34:00 2015 UTC,isn't the 750ti already on Maxwell? I personally don't think its going to happen.
hardware,3da9t2,himmatsj,2,Wed Jul 15 04:14:16 2015 UTC,It's a previous version of maxwell with many generation 2 features not available. It's definitely a candidate for a refresh.
hardware,3da9t2,CykaLogic,1 point,Wed Jul 15 11:05:31 2015 UTC,"I personally think the GTX 950 will be an update over the 750/Ti, with performance on par with a GTX 660."
hardware,3d8uh1,speckz,1 point,Tue Jul 14 12:14:29 2015 UTC,"I remember that 10 years ago, I had to recycle components from old systems in order to put together an HTPC. Then I'd still be missing one or two key components and had to buy those. Now I can get an HTPC for only a few hundred. How times have changed..."
hardware,3d8uh1,bobbokelso,1 point,Tue Jul 14 16:15:01 2015 UTC,I think the power consumption also is awesome. Makes a great home server. If i used my old core2duo with the radeon 4850 as an home server the power bill alone would justify buying a cheap barebone. (but i have to admit that a kwh ovet here is much more expensive than in the US)
hardware,3d9ijn,gute_idee,2,Tue Jul 14 15:37:23 2015 UTC,wasn't this question asked like 1 or 2 days ago with the same title?
hardware,3d6xbd,WafflePerson,36,Tue Jul 14 00:17:07 2015 UTC,Zen is their next step. That's sometime next year.
hardware,3d6xbd,monkeybannanna,2,Tue Jul 14 00:24:39 2015 UTC,Probably october-ish
hardware,3d6xbd,rePAN6517,1 point,Tue Jul 14 12:55:12 2015 UTC,Probably 2017 /s  ah well im totally willing to wait
hardware,3d6xbd,computeBuild,3,Tue Jul 14 16:17:52 2015 UTC,2016 to be specific
hardware,3d6xbd,pwnegekill,22,Tue Jul 14 06:46:03 2015 UTC,found the timelord
hardware,3d6xbd,weez09,1 point,Tue Jul 14 07:01:55 2015 UTC,Q1 but expect it to be Q2.
hardware,3d6xbd,veyron3003,56,Tue Jul 14 12:45:53 2015 UTC,"H1 2016, it's called Zen.  EDIT: Not too sure about H1, most likely only the server grade hardware will be released by then."
hardware,3d6xbd,CeeeeeJaaaaay,35,Tue Jul 14 00:23:27 2015 UTC,"H1 2016, it's called Zen.   And it's supposed to produce 40% more IPC that the current AMD lineup, which would put it squarely on par with Intel's offerings. It's also dropping that CMT (Clustered Multithreading) bullshit for SMT (Symmetric Multithreading, like Hyperthreading).  http://www.anandtech.com/show/9231/amds-20162017-x86-roadmap-zen-is-in"
hardware,3d6xbd,Exist50,31,Tue Jul 14 02:01:04 2015 UTC,"Simultaneous multithreading, not Symmetric Multithreading."
hardware,3d6xbd,jigssaw,12,Tue Jul 14 05:17:03 2015 UTC,CMT isnt bullshit - its pretty damn good idea actually. Just the execution wasnt that good.
hardware,3d6xbd,CaptSkunk,16,Tue Jul 14 06:12:16 2015 UTC,It was one of those ideas ideas that sound great on paper but not so good in real-world application.
hardware,3d6xbd,Kisame9734,8,Tue Jul 14 07:29:43 2015 UTC,Like communism!
hardware,3d6xbd,BrainSlurper,8,Tue Jul 14 08:21:01 2015 UTC,The FPUs belong to the motherland!
hardware,3d6xbd,CaptSkunk,6,Tue Jul 14 08:23:36 2015 UTC,Communist computers are best computers:  https://en.wikipedia.org/wiki/History_of_computer_hardware_in_Soviet_Bloc_countries  http://www.15kop.ru/en/
hardware,3d6xbd,Rygerts,3,Tue Jul 14 09:05:28 2015 UTC,"In soviet Russia, computer overclocks you!"
hardware,3d6xbd,dudemanguy301,4,Tue Jul 14 21:32:45 2015 UTC,That sounds useful. I run faster and I'm hotter! You jelly?
hardware,3d6xbd,rePAN6517,6,Tue Jul 14 23:53:54 2015 UTC,Coming from AMD in 2016: The Comrade Coprocessor
hardware,3d6xbd,jigssaw,-4,Tue Jul 14 12:54:07 2015 UTC,communism doesnt even sound good on paper though ... :-)
hardware,3d6xbd,olavk2,3,Tue Jul 14 10:06:45 2015 UTC,"uhm, how so? not that i want to start a political debate(i kinda am though) but communism is basically everything is equal for everyone, which in theory is good, free healthcare and a lot more benefits, however there is always some people that ruin it, that is why we have never had a true communistic country."
hardware,3d6xbd,Timiniel,3,Tue Jul 14 11:44:22 2015 UTC,"Careful there, you are confusing communism, which is a monetary system, with socialism, which is a political system. You need to differentiate properly between the two, or do you believe that capitalism is the same as a republic?  What you described, free healthcare and benefits, is seen in the capitalistic scandinavian countries, for example."
hardware,3d6xbd,Rygerts,1 point,Tue Jul 14 11:54:45 2015 UTC,"In Sweden we only pay to see the doctor and for a 12 month period you don't need to pay more than ~$130. It is similar with medication too, you don't need to pay more than ~$260 per 12 months. The numbers might be a bit off but they are roughly correct."
hardware,3d6xbd,Timiniel,1 point,Tue Jul 14 21:38:19 2015 UTC,"Ehm, yes? Why is this relevant?"
hardware,3d6xbd,olavk2,-3,Wed Jul 15 11:04:59 2015 UTC,"iirc that leans to the communism side though, what america has is pretty extreme right wing politics(coming from the view of a north european)"
hardware,3d6xbd,Timiniel,2,Tue Jul 14 12:13:35 2015 UTC,"iirc that leans to the communism side though   What is the ""that"" which you are referring to here?"
hardware,3d6xbd,jigssaw,1 point,Tue Jul 14 12:34:17 2015 UTC,everything is equal for everyone doesnt sound good for hardworking people :-)  because it simply isn't fair - no matter how hard you work you get rewarded the same as a joe down street who sits on his ass all day. And once people realize it nobody will work hard and the whole system crumbles - its simply a problem of incentives...   Meritocracy is a system that sounds good :-) Everyone gets rewarded according to how they contribute.
hardware,3d6xbd,olavk2,1 point,Tue Jul 14 18:47:43 2015 UTC,"well if it worked that way, sure it would be good but it sure as hell doesnt work like that in our society in a lot of cases."
hardware,3d6xbd,jigssaw,2,Tue Jul 14 18:56:08 2015 UTC,Yeah cause we don't live in a meritocracy :-) ....
hardware,3d6xbd,andromeduck,1 point,Tue Jul 14 19:01:53 2015 UTC,https://scalibq.wordpress.com/2012/02/14/the-myth-of-cmt-cluster-based-multithreading/
hardware,3d6xbd,Exist50,7,Tue Jul 14 06:19:09 2015 UTC,"All I learned from that is that someone doesn't have a good idea of how to compare processors. You can't just compare transistor count alone, and you sure as hell can't compare ""disabling"" CMT to disabling SMT."
hardware,3d6xbd,andromeduck,1 point,Tue Jul 14 06:35:22 2015 UTC,You actually pretty much can for these purposes.  The more interesting comparison would be with Zen's pipeline but this is valid vs their claims before bulldozer.
hardware,3d6xbd,Exist50,1 point,Tue Jul 14 08:08:07 2015 UTC,But it ignores other major factors like the fab process. Remember that AMD targeted a much higher clock speed than ended up being possible. One can't pretend that didn't impact the final design. Something like Excavator (still Bulldozer-based) seems to be much better.
hardware,3d6xbd,OftenSarcastic,1 point,Tue Jul 14 08:13:32 2015 UTC,"The person who wrote that blog post obviously started from the position of SMT being better and started writing from there. It might as well have been titled: ""CMT doesn't work like SMT and is therefore bad"".  When you get to the part about comparing ALUs per thread it should be obvious that the writer doesn't understand that the two technologies solve two different problems and they're not mutually exclusive. CMT squeezes more cores into a tighter space by sharing resources. SMT better utilizes leftover resources in any single core during multi-threaded workloads.   Since CMT doesn’t share the ALUs, it works exactly the same as the usual SMP approach. So you would expect the same scaling, since the execution units are dedicated per thread anyway. Enabling CMT just gives you more threads.          At the same time, CMT is not actually saving a lot of die-space: There are 4 ALUs in a module in total. Yes, obviously, when you have more resources for two threads inside a module, and the single-threaded performance is poor anyway, one would expect it to scale better than SMT.   Judging by these quotes they completely missed the part about shared resources. And the second part could just as easily be inverted to say ""Yes, obviously, when you have more resources for a single thread inside a core, one would expect it to run faster during single-thread workloads"" if someone wanted to skew the blog in the opposite direction."
hardware,3d6xbd,idoithere,-4,Tue Jul 14 09:21:04 2015 UTC,the guy in that blog surely show an anti amd bias. I would not dig too deep into his comments.  He spent a whole blog post complaining that amd does not have conservative rasterization to a standard that has not been released....
hardware,3d6xbd,andromeduck,2,Tue Jul 14 14:57:32 2015 UTC,conservative rasterization has been a longstanding request from the community and has become an EXT extension in OGL 4.5 IIRC  complaining about that is perfectly reasonable considering how useful it is
hardware,3d6xbd,idoithere,0,Tue Jul 14 17:36:42 2015 UTC,"the extension is support on one hw generation on nvidia.  Consoles do not have support of this extension.... Even if it is really useful. Without widespread support, any extension become useless."
hardware,3d6xbd,andromeduck,1 point,Wed Jul 15 03:01:39 2015 UTC,"Well no, there are soft paths. This just makes it faster."
hardware,3d6xbd,idoithere,0,Wed Jul 15 03:16:09 2015 UTC,there are soft paths. This just makes it faster.   and there is the point.  Cant amd implement software workaround on their cards in the meantime?  hardware is not made overnight.
hardware,3d6xbd,Blubbey,2,Wed Jul 15 03:28:22 2015 UTC,If they can keep the clocks similar that is.
hardware,3d6xbd,reynardtfox,1 point,Tue Jul 14 12:13:18 2015 UTC,What's the difference between CMT and SMT?
hardware,3d6xbd,reynardtfox,3,Tue Jul 14 14:34:13 2015 UTC,What's the difference between CMT and SMT?   It's in the linked article.
hardware,3d6xbd,imallin,1 point,Tue Jul 14 14:44:03 2015 UTC,"Ah okay, I'll go read it there.  Thanks!"
hardware,3d6xbd,Klorel,15,Tue Jul 14 14:49:18 2015 UTC,I'd be very surprised if we see it that early.
hardware,3d6xbd,CeeeeeJaaaaay,8,Tue Jul 14 02:29:09 2015 UTC,Source?  Everything i read says LATE 2016. So for sure not Q1/2.
hardware,3d6xbd,WillWorkForLTC,1 point,Tue Jul 14 06:28:02 2015 UTC,"From what I've heard servers will get Zen first, then consumer PCs, with the server CPUs coming early 2016. I hope desktops will get Zen before the end of the year, but I can't find a source so I'll edit my comment, thanks."
hardware,3d6xbd,Soytaco,-5,Tue Jul 14 14:34:38 2015 UTC,Sow what you're saying is that in H1 or 2016 Zen (stars with Z) will be released for the 1st time? H1 Z 1. I think we know what game /r/conspiracy wants us to play.
hardware,3d6xbd,Exist50,12,Tue Jul 14 05:54:42 2015 UTC,"In addition to what others have said, you might keep an eye out for AMD press after their earnings call on the 16th. They're dealing with some considerable losses, so to boost investor confidence we might see some new product updates (if there's good updates to give).  Also, with regards to AM4, my money's on July '16."
hardware,3d6xbd,Eastwood6,3,Tue Jul 14 03:02:58 2015 UTC,Why that day?
hardware,3d6xbd,Exist50,8,Tue Jul 14 05:19:13 2015 UTC,'16 is the year not day
hardware,3d6xbd,Eastwood6,5,Tue Jul 14 06:31:20 2015 UTC,"Alright, well in that case, why July? I know you're not the OP, but that's a rather specific guess."
hardware,3d6xbd,Exist50,3,Tue Jul 14 06:32:14 2015 UTC,Time it with gpu release maybe? (assuming amd releases gpus same time next year as this year)
hardware,3d6xbd,Eastwood6,6,Tue Jul 14 06:36:51 2015 UTC,"I don't think they have any reason to do that. The sooner they can get both out, the better, and since Zen has no GPU, the development is probably largely separate."
hardware,3d6xbd,Exist50,2,Tue Jul 14 06:38:42 2015 UTC,"Well if there isn't an igpu then that means you need one, right? I 100% agree with you that they are most likely being developed separately, but if they release a full gpu series with hbm and new cpu line up all on a new process then I think they would make a killing. As long as they performed of course.  Edit: if they get GPUs out before nvidia"
hardware,3d6xbd,Eastwood6,4,Tue Jul 14 06:44:36 2015 UTC,"I think your perspective is a bit too narrow. Zen seems to target the server market as well, which doesn't care about new gaming GPUs. And even on the desktop, every day spent waiting is a day without sales. AMD can't afford more delays."
hardware,3d6xbd,Exist50,2,Tue Jul 14 06:47:22 2015 UTC,"Ok I see now, I was completely ignoring server earlier and it would just be best to release when ready instead of wait for both to finish to get most $$. I think that's what you're saying; correct me if I'm wrong."
hardware,3d6xbd,Andrej_ID,3,Tue Jul 14 06:56:59 2015 UTC,"That's exactly what I'm saying. Also, in a more indirect way, AMD needs to show investors that it can release a product in a timely manner."
hardware,3d6xbd,BrainSlurper,10,Tue Jul 14 06:58:52 2015 UTC,I was always a fan of AMD and I hope Zen will compete well with Intel CPUs.
hardware,3d6xbd,capn_hector,5,Tue Jul 14 05:48:26 2015 UTC,"If it doesn't, then we are effectively fucked on that front."
hardware,3d6xbd,dajigo,2,Tue Jul 14 08:24:57 2015 UTC,"On that front, the good news is that Skylake isn't making much of an improvement in CPU performance (mostly GPU improvements).  So if AMD's IPC improvements bring them up to par with Haswell as speculated - they should compete with Skylake too."
hardware,3d6xbd,eric1rr,1 point,Tue Jul 14 16:53:21 2015 UTC,"On that front, the good news is that Skylake isn't making much of an improvement in CPU performance   I fail to see how that's good news."
hardware,3d6xbd,capn_hector,3,Tue Jul 14 21:22:40 2015 UTC,It means AMD has a chance to compete for once.
hardware,3d6xbd,dajigo,2,Wed Jul 15 00:06:05 2015 UTC,It gives AMD a bit of a chance to clean up their act.  With a few exceptions (AM1 is nice for home fileservers) I'm not buying their products right now.  But nobody wins if they go under and Intel is left as the only player in the x86 market.
hardware,3d6xbd,Exist50,1 point,Tue Jul 14 22:43:23 2015 UTC,"nobody wins if they go under and Intel is left as the only player in the x86 market   Agreed, I think the wall that intel's coming up against nowadays with silicon is going to be a pretty hard one. Single thread performance is not going to increase tremendously unless we see a real breakthrough at this point. It may just be what AMD needs at this point, although I'd prefer stronger single threads with 6 GHz CPUs or stuff like that :P. We'll see."
hardware,3d6xbd,Seclorum,13,Wed Jul 15 01:25:58 2015 UTC,AM4 is the next socket.
hardware,3d6xbd,Exist50,-1,Tue Jul 14 01:18:51 2015 UTC,"Its coming with zen, sometime this Q4 or next year."
hardware,3d6xbd,Seclorum,11,Tue Jul 14 00:27:12 2015 UTC,Not this year.
hardware,3d6xbd,Exist50,0,Tue Jul 14 05:18:48 2015 UTC,So it's slipping again. Damn.
hardware,3d6xbd,Seclorum,8,Tue Jul 14 05:25:52 2015 UTC,"Did you miss the financial analyst day? Zen CPUs 2016, Zen APUs 2017, K12 (ARM) 2017."
hardware,3d6xbd,Exist50,2,Tue Jul 14 05:29:29 2015 UTC,Didn't see it. Last I heard was in a Maximum PC article a couple months ago.
hardware,3d6xbd,veyron3003,5,Tue Jul 14 05:31:06 2015 UTC,"Well to sum things up and cut through the corporate speak, they didn't have the resources to do both Zen and K12 at once, so they decided to focus on Zen (apparently consumes the majority of AMD's R&D), and incorporate what they learn in K12. Zen CPUs (not APUs) will launch in 2016 along side Excavator-based APUs on AM4, to be followed with Zen based APUs in 2017. Zen is a SMT design, and they gave +40% IPC over Excavator. Possible use of HBM in some products."
hardware,3d6xbd,WillWorkForLTC,3,Tue Jul 14 05:41:23 2015 UTC,Its always have been 2016
hardware,3d6xbd,KeyserSOhItsTaken,1 point,Tue Jul 14 12:47:14 2015 UTC,The slip and slide of hardware releases.
hardware,3d6xbd,TriGeo,-21,Tue Jul 14 05:55:15 2015 UTC,"[–]CeeeeeJaaaaay [score hidden] 44 minutes ago   H1 2016, it's called Zen.  permalinksavereportgive goldreply   [–]monkeybannanna [score hidden] 43 minutes ago   Zen is their next step. That's sometime next year.  permalinksavereportgive goldreply   [–]Seclorum [score hidden] 40 minutes ago   Its coming with zen, sometime this Q4 or next year.  permalinksavereportgive goldreply"
hardware,3d6xbd,KeyserSOhItsTaken,15,Tue Jul 14 01:08:00 2015 UTC,Dafuq?
hardware,3d6xbd,BrockYXE,-15,Tue Jul 14 01:17:30 2015 UTC,Clearly the looking at the timestamps the people could see the correct answer had already been stated. Each person responded with the exact same response in different wording. So I just copy and pasted all their responses since everyone just wants to copy everyone.
hardware,3d6xbd,KeyserSOhItsTaken,7,Tue Jul 14 01:30:44 2015 UTC,They all could've had the tab open (and unrefreshed) for a few minutes and then replied without seeing the other posts.
hardware,3d6xbd,Ivanjacob,-6,Tue Jul 14 05:53:34 2015 UTC,"Interesting, I think I heard it's going to be called Zen, and release in 2016 if you were curious."
hardware,3d6xbd,ryemigie,0,Tue Jul 14 08:21:00 2015 UTC,Clearly there is already information about Zen which includes that it will be released in 2016.
hardware,3d6xbd,pabloe168,-9,Tue Jul 14 07:47:32 2015 UTC,/r/shitpost  Edit: I guess no one knows what the search function or Google is? :)
hardware,3d40eu,buildzoid,57,Mon Jul 13 10:03:59 2015 UTC,"Well, that was kind of to be expected. CFX needs the same base GPUs to work, not specific models.  Which is why a 7970 can CFX with a 7950 (i.e. cut down 7970), and why e.g. a 280X can CFX with a 7970 (same GPU), and now why a 390(X) can CFX with a 290(X) (all Hawaii), etc."
hardware,3d40eu,HavocInferno,11,Mon Jul 13 11:11:44 2015 UTC,7870 xt will work with a tahiti.
hardware,3d40eu,veyron3003,12,Mon Jul 13 13:35:04 2015 UTC,Because it's also Tahiti (LE version).
hardware,3d40eu,HavocInferno,4,Mon Jul 13 13:41:47 2015 UTC,So an R9280 and an R9380 would work together?
hardware,3d40eu,IAMA_Ghost_Boo,26,Mon Jul 13 12:37:24 2015 UTC,"Someone please correct me of I'm wrong, but no because they aren't the same GPU. IIRC, the 380 is a rebranded 285, which has a different architecture than the 280."
hardware,3d40eu,VLAD1M1R_PUT1N,6,Mon Jul 13 12:39:18 2015 UTC,"If correct, what would work with a 280?"
hardware,3d40eu,IAMA_Ghost_Boo,14,Mon Jul 13 12:40:18 2015 UTC,"280, 280X, 7950, or 7970 for sure. Maybe some of the lower end cards as well, but that's probably not worth bothering with. If you have a 280 and want to Crossfire, I'd recommend picking up a used 7950 for cheap."
hardware,3d40eu,VLAD1M1R_PUT1N,5,Mon Jul 13 12:41:56 2015 UTC,I already have two 280s in crossfire I was just wondering what would work with it but it seems AMD has since upgraded. Looks like I might upgrade within the year.
hardware,3d40eu,IAMA_Ghost_Boo,1 point,Mon Jul 13 12:46:33 2015 UTC,7870 XT will also work.
hardware,3d40eu,n3x_,1 point,Mon Jul 13 19:15:42 2015 UTC,"Ah yes, thank you. I was pretty sure they made something like this, but couldn't remember what they called it."
hardware,3d40eu,VLAD1M1R_PUT1N,1 point,Mon Jul 13 19:36:12 2015 UTC,Wot about the 380x? What's that a rebrand of?
hardware,3d40eu,PTFOholland,7,Mon Jul 13 18:12:29 2015 UTC,There's no such thing.
hardware,3d40eu,kennai,2,Mon Jul 13 18:36:20 2015 UTC,"Nothing, 380x is full tonga which has not been previously released"
hardware,3d40eu,slapdashbr,3,Mon Jul 13 19:25:51 2015 UTC,285
hardware,3d40eu,n3x_,1 point,Mon Jul 13 19:16:48 2015 UTC,"Unfortunately that will be available only on Apple's iMac. AMD is trying to replace the 280X with the 390 (even though the 280X is ridiculously cheap now compared to the 390, their initial release prices were similar). It's likely that the 390 will go down in price soon after the 280X is sold out.  Basically, AMD made the 290X/290s (390X/390) their new mid-high end tier, while replacing their highest end tier with FuryX/Furys.  It's similar to how AMD made the 7970/7950s (280X/280) their mid-high end tier, while replacing their highest end tier with 290X/290s."
hardware,3d40eu,rockycrab,6,Mon Jul 13 18:43:02 2015 UTC,"380 is Tonga, 280 is a rebadged 7950 (Tahiti?) if I'm not mistaken, so no, would not work together."
hardware,3d40eu,HavocInferno,2,Mon Jul 13 12:43:39 2015 UTC,No. Those are not the same die. 380 will xfire with a 285 (tonga)
hardware,3d40eu,slapdashbr,1 point,Mon Jul 13 19:25:14 2015 UTC,"No (Tahiti/Tonga). A 285 might CFX with a 380, though."
hardware,3d40eu,SeaJayCJ,20,Mon Jul 13 15:43:27 2015 UTC,Didn't it work that way with the 200 series? Like a R9 270 and a 7870?
hardware,3d40eu,maxt0r,21,Mon Jul 13 10:50:30 2015 UTC,have r9 270x + 7870. works. its the same chip why shouldn it. somebody has to try triple crossfire through all the generations :D
hardware,3d40eu,qwortz,4,Mon Jul 13 11:08:22 2015 UTC,The GTX 770 and 680 are the same chips but AFAIK don't SLI.
hardware,3d40eu,wagon153,30,Mon Jul 13 12:13:29 2015 UTC,Nvidia is more strict afaik.
hardware,3d40eu,PTFOholland,2,Mon Jul 13 12:43:24 2015 UTC,Should be their motto..
hardware,3d40eu,qwortz,7,Mon Jul 13 18:13:07 2015 UTC,sli != crossfire
hardware,3d40eu,Spidertech500,2,Mon Jul 13 14:20:54 2015 UTC,True
hardware,3d40eu,Ellimis,1 point,Mon Jul 13 14:32:37 2015 UTC,I know that but for Nvidia users it's odd when you hear about people Cross Firing cards that only share code names.
hardware,3d40eu,slapdashbr,3,Mon Jul 13 16:36:38 2015 UTC,NVIDIA used to be able to do this too
hardware,3d40eu,melgibson666,2,Mon Jul 13 17:57:05 2015 UTC,Really? I thought you could. I know you cant do something like 670+680 (which would be the nvidia alternative to 7950+7970) but I thought you could do 680+770 (which would be like 7970+280x)
hardware,3d40eu,dreiter,-1,Mon Jul 13 19:28:06 2015 UTC,I once heard that AMD and nVidia had different requirements for multi gpu rendering. Weird right? I must be going crazy.
hardware,3d40eu,GeckIRE,1 point,Mon Jul 13 17:26:43 2015 UTC,Sadly there is no 1280 core R3XX GPU.
hardware,3d40eu,Yatlol,10,Mon Jul 13 20:33:34 2015 UTC,I think you were able to crossfire a 280x with a 7970 but I'm not sure.
hardware,3d40eu,SilverAcez,10,Mon Jul 13 10:54:13 2015 UTC,you can. 7970 is just a rebrand of 280x. same chip
hardware,3d40eu,Yatlol,-1,Mon Jul 13 11:02:04 2015 UTC,I think you mean the 280 is a rebrand of the 7970.. but to cfx the 7970 has to be in top spot with the 280 below
hardware,3d40eu,SilverAcez,2,Mon Jul 13 23:11:11 2015 UTC,no the 280 is the 7950. 280x is 7970. 7970 doesnt have to be in top spot. itll work either way
hardware,3d40eu,BoiledFrogs,-1,Mon Jul 13 23:15:37 2015 UTC,Well let me know how it goes and report back here :-)
hardware,3d40eu,SilverAcez,1 point,Tue Jul 14 00:31:19 2015 UTC,"Nothing to report on, he's right."
hardware,3d40eu,BoiledFrogs,1 point,Tue Jul 14 08:37:46 2015 UTC,Try it then lolz
hardware,3d40eu,Omnislip,1 point,Tue Jul 14 22:40:15 2015 UTC,I ran that setup before. It works like he said.
hardware,3d40eu,Omnislip,1 point,Wed Jul 15 04:21:39 2015 UTC,yep it did
hardware,3d40eu,mack0409,10,Mon Jul 13 12:12:06 2015 UTC,Can someone draw up or link to a table that has information about all compatible cards? Would sure be useful!
hardware,3d40eu,VanayadGaming,6,Mon Jul 13 11:40:22 2015 UTC,"HD 7870/7850+R9 270(X)/265+R7 370  R9 285 + R9 380  R9 290(X) + R9 390(X)  HD 7790 + R7 260(X) + R7 360  There, that's basically all the cards that are worth cross firing."
hardware,3d40eu,RAIKANA,4,Mon Jul 13 12:11:36 2015 UTC,My good value 280X purchase now seems rather less excellent :(
hardware,3d40eu,RAIKANA,3,Mon Jul 13 12:23:31 2015 UTC,You can get a 7950 (which should CXfire with your 280) for $150 from gpushack
hardware,3d40eu,christes,2,Mon Jul 13 16:09:52 2015 UTC,7970 ;)
hardware,3d40eu,Powerpuncher,6,Mon Jul 13 13:09:36 2015 UTC,7950;)
hardware,3d40eu,CosmonautLaika,4,Mon Jul 13 13:23:22 2015 UTC,7870xt ;)
hardware,3d40eu,logged_n_2_say,4,Mon Jul 13 13:23:31 2015 UTC,"It's not a new one, but for completeness since some commenters here are confused:  HD 7950/7970 + R9 280(X)"
hardware,3d40eu,Yearlaren,2,Mon Jul 13 19:25:02 2015 UTC,I didn't know there was a 7790 :o  The more you know...
hardware,3d40eu,Powerpuncher,4,Mon Jul 13 14:51:40 2015 UTC,"Yeah! It was a funny card, actually. They released a while before the whole R9/R7 rebranding, positioned between the 7770 and 7850, and nobody knew it was the newer architecture until later on. It had stuff like the TrueAudio / freesync compatibility way before those things were even announced (obviously, not enabled though)."
hardware,3d40eu,xa3D,35,Mon Jul 13 15:29:12 2015 UTC,"You should be able to crossfire a 7850, 7870 with a 370 too. The beauty of the double rebrand."
hardware,3d40eu,Powerpuncher,12,Mon Jul 13 11:32:15 2015 UTC,Yes that works
hardware,3d40eu,Yearlaren,13,Mon Jul 13 12:13:40 2015 UTC,"AMD prefers the term ""refresh""."
hardware,3d40eu,piovocsic,22,Mon Jul 13 12:58:17 2015 UTC,"Because it technically isn't a rebrand. A rebrand would be taking a card that was labeled as a 280, put a different sticker on it and sell it as a 380."
hardware,3d40eu,FallenAdvocate,6,Mon Jul 13 14:49:40 2015 UTC,"This is getting me confused, so what exactly is it? I mean.. they are the same chips right?"
hardware,3d40eu,Lelldorianx,13,Mon Jul 13 16:52:20 2015 UTC,"But not the same cards and the chips themselves have small improvements due to a better manufacturing process.  Edit: I'm not trying to defend AMD here, I'm just as bummed about the pretty much one to one reuse of the old architecture. But the r9 300 series doesn't deserve to be called a rebrand nonetheless as I've explained above."
hardware,3d40eu,xa3D,6,Mon Jul 13 17:08:19 2015 UTC,How are Curacao and Trinidad different?
hardware,3d40eu,Weeberz,2,Mon Jul 13 17:54:01 2015 UTC,I think the most recent 290x cards have the improved chips and ram. I just bought a 290x and it hits 1500mem easy and 1130 core.
hardware,3d40eu,logged_n_2_say,1 point,Mon Jul 13 22:14:32 2015 UTC,JayzTwoCents hit 1200 on the core of his 390 which is pretty crazy. But hopefully more of the 300 series can hit these types of numbers.
hardware,3d40eu,Teethpasta,2,Thu Jul 16 20:51:01 2015 UTC,AMD has made thermal and power envelope modifications to the 300 series cards and increased the clockrates.
hardware,3d40eu,piovocsic,1 point,Mon Jul 13 20:11:48 2015 UTC,"So I can just basically overclock a 200 series and pretty much have a 300 series? Literal technicalities aside, that is?"
hardware,3d40eu,dreiter,3,Mon Jul 13 20:32:19 2015 UTC,"Kinda, except that the 300 series is slightly more power efficient, put off slightly less heat, and have better coolers"
hardware,3d40eu,waynestream,1 point,Tue Jul 14 02:05:14 2015 UTC,"Has better slightly power management and better binned. It might be more efficient and put out less heat, but we're talking less than a percent or two different."
hardware,3d40eu,0pyrophosphate0,2,Tue Jul 14 11:17:40 2015 UTC,Yes sadly
hardware,3d40eu,waynestream,1 point,Mon Jul 13 20:39:37 2015 UTC,I've done exactly that. I have a 390x and a 290x at my house right now. I OC'd the 290x to the same ram and higher clock rate than the 390x(factory OC'd) and the 290x gets better scores in fire strike and heaven.
hardware,3d40eu,0pyrophosphate0,1 point,Mon Jul 13 22:13:35 2015 UTC,"How would that work, since the 7870 has 1280 cores and the 370 only has 1024?"
hardware,3d40eu,waynestream,6,Mon Jul 13 20:34:30 2015 UTC,How would the performance of a 290+390 CF look like? Would it be limited to the power of 2 290s (apart from the VRAM) or would you see a better performance since the 390 is a bit faster?
hardware,3d40eu,PTFOholland,6,Mon Jul 13 13:56:06 2015 UTC,Would it be limited to the power of 2 290s (apart from the VRAM)   Yes (including the VRAM).
hardware,3d40eu,christes,1 point,Mon Jul 13 15:33:22 2015 UTC,"So a 290+390 Crossfire could only use 4 GB of VRAM? I thought the limitation was that you could only use 1 card's VRAM, so I didn't think it would matter how much the other card has."
hardware,3d40eu,sinnerhp,5,Mon Jul 13 16:40:32 2015 UTC,"Each card has a local copy of the same data in memory, and you can only do that if each card is capped at whatever the lower amount is. 4GB, in this case."
hardware,3d40eu,christes,3,Mon Jul 13 16:56:51 2015 UTC,"Ah that makes sense, thanks for the explanation.  Seems like x-firing 290s and 390s is pretty much a waste then, if you could just crossfire two 290s for the same performance (unless there is absolutely no way you can get a 290)."
hardware,3d40eu,metadex,-7,Mon Jul 13 17:02:50 2015 UTC,"Nono wait. You could set the 8GB in the 16x slot and it will take 8GB, I remember doing this with a 2GB and a 1GB card back in the day."
hardware,3d40eu,metadex,2,Mon Jul 13 18:14:34 2015 UTC,Never seen any tests on this so IDK. I might try test it myself but I make no promises because testing like that is expensive.
hardware,3d40eu,insaneHoshi,1 point,Mon Jul 13 16:37:43 2015 UTC,"I've been very curious about this too.  If I had the money, I'd go and do it :þ  I'm actually in the process of doing some benchmarks with my 280 / 7950 system set at varying clock speeds.  That might give some insight into how performance is with mis-matched pairs."
hardware,3d40eu,SilverAcez,3,Tue Jul 14 10:02:04 2015 UTC,"Now, what about with DX12? shouldnt' the new api allow joining of basically ANY GPU to another? Green and Red team in the same box playing together?   Old Reference: http://www.tomshardware.com/news/microsoft-directx12-amd-nvidia,28606.html  I can't say that I've heard much past that, anyone have any knowledge of this?"
hardware,3d40eu,jorgp2,1 point,Mon Jul 13 16:26:00 2015 UTC,"There are so many promises swirling around about DX12, that I'm not going to believe anything until I see it.  But this article is specifically about Crossfire, which is a very specific way of combining AMD cards."
hardware,3d40eu,StellaTerra,2,Tue Jul 14 10:04:03 2015 UTC,I wonder if it would be worth it for me to Crossfire with my R9 270... What kind of gains could one expect?
hardware,3d40eu,0pyrophosphate0,2,Mon Jul 13 14:30:57 2015 UTC,probably around 75-90% more FPS in games where CF works and no benefit in games where it doesn't work.
hardware,3d40eu,StellaTerra,1 point,Mon Jul 13 16:39:39 2015 UTC,I'm assuming the card in the first PCI slot will take over in non-CF games? I'll have to see what the performance will be for just the single card to see if it will be worth the upgrade.
hardware,3d40eu,christes,1 point,Wed Jul 15 04:49:33 2015 UTC,yep top card does all the work when CF isn't running.
hardware,3d40eu,cp5184,1 point,Wed Jul 15 12:14:53 2015 UTC,I'm curious too
hardware,3d40eu,StellaTerra,2,Mon Jul 13 15:38:40 2015 UTC,Guys what they didnt exolain is that the older card has to be in the first slot leading down to the newest at the bottom of crossfire
hardware,3d40eu,cp5184,-5,Mon Jul 13 23:08:02 2015 UTC,"Can you, yes.  Is there any reason to, no.  Most of the new cards have more VRAM."
hardware,3d40eu,jorgp2,2,Mon Jul 13 11:48:32 2015 UTC,Which you don't need unless you're at 4K
hardware,3d40eu,SecretDragoon,4,Mon Jul 13 12:09:33 2015 UTC,"Question, not rhetorical: Why does VRAM have anything to do with display resolution?  As I understand it, the vast majority of VRAM is occupied by models and textures during gameplay, and neither of those is directly correlated to display res.  The only thing I can think is the actual display buffer, but I don't think that's a significant size in RAM.  Some math: 4k is 3840 X 2160 or 8,294,400 pixels, I think most people are at something like 32 bits per pixel (could be totally off with that) and let's assume we're double-buffering.  That's 530,841,600 bits, or 66,355,200 bytes, meaning ~63 MB all told.  Edit: Ooops!  Fixed some of the numbers.  Also, for comparison, 1920x1080 is 2,073,600 pixels, 132,710,400 bits, ~16 MB."
hardware,3d40eu,Whats_a_narwhal,5,Mon Jul 13 14:54:43 2015 UTC,"You also have shadow maps, depth buffers, etc. that scale with the resolution. Exactly what else you need depends on the game, but there's a lot more than color that needs to be stored per-pixel while rendering the scene."
hardware,3d40eu,veyron3003,1 point,Mon Jul 13 15:32:54 2015 UTC,"Ah, gotcha, thanks!"
hardware,3d4d82,JackassWhisperer,21,Mon Jul 13 12:41:46 2015 UTC,Do you know why servers use ecc ram? The extra bit that adds error protection so your data isn't corrupted. It's very easy to have corrupt files on a flash drive but ram server ram is rarely corrupt to the point where the system has errors. These systems are run 24/7 and if the ram took a dive it costs a lot of money for them to be offline.
hardware,3d4d82,LiberDeOpp,12,Mon Jul 13 18:49:17 2015 UTC,They don't mean shitty flash drives with shitty controllers. Enterprise level SSDs have ecc ram in them too. Some have more exotic things too.
hardware,3d4d82,dylan522p,2,Mon Jul 13 20:38:40 2015 UTC,Like belly dancers?     first thing that popped into my head
hardware,3d4d82,meowffins,3,Wed Jul 15 16:56:22 2015 UTC,Mvram haha but sure
hardware,3d4d82,dylan522p,11,Wed Jul 15 17:34:43 2015 UTC,You do know that....ECC only makes sense when you statistically can have a flipped bit due to the amount of data transmission that occurs between the CPU and the RAM right?  If you're primarily pushing data to storage then you use the RAM as a buffer and not as anything else.  If you're processing data and moving it between the processor <--> RAM consistently for TONS of data for say...20 hours out of a day then statistically you might want ECC as there's a far higher likelihood of a bit flip then.
hardware,3d4d82,Cheeze_It,6,Mon Jul 13 20:57:41 2015 UTC,Flipped bits primarily occur in registers and cache due to cosmic rays
hardware,3d4d82,fumblesmcdrum,5,Tue Jul 14 07:36:22 2015 UTC,It's very easy to have corrupt files on a flash drive   No not really.  Flash drives have sector ECC too.  Enterprise drives also use ECC RAM.  SATA/SAS/FC links are ECC/CRC protected.  There's very little opportunity for data corruption along the entire chain.
hardware,3d4d82,Y0tsuya,2,Mon Jul 13 19:29:53 2015 UTC,"Normal ram does have error correction, but is limited to one bit. Server rams corrects up to two bits in ddr3."
hardware,3d4d82,JarJarBanksy,3,Tue Jul 14 23:25:44 2015 UTC,Now you just need a bunch of these and the circle will be complete!
hardware,3d4d82,Exist50,2,Tue Jul 14 07:33:38 2015 UTC,I wonder how long it lasts in terms of write cycles. 2-10k cycles is tons for storage but could be used up fast in this application.
hardware,3d9dql,zmeul,26,Tue Jul 14 15:01:58 2015 UTC,tl;dr  no differences until dx12 games come out next year
hardware,3d9dql,nonameowns,8,Tue Jul 14 15:46:21 2015 UTC,"The 290x (and I hope all radeons), has a significant boost in some games, like attila and project cars. I assume this is wddm 2.0 reducing cpu overhead.   I've seen anecdotal posts of improvements in gta and arma too, even though this source doesn't seem to reinforce that. Good news for me and my 290 I guess."
hardware,3d9dql,MaybeIShouldSleep,3,Tue Jul 14 16:34:00 2015 UTC,"tl;dr no differences until dx12 games come out next year   The one thing they don't address and that is really hard to quantify, is that 10 feels much more responsive than 8.1 does. I have to flip back and forth doing basically the same tasks on the same two brand-new Dell E7540 laptops all day, one running 8.1 and one running 10, and the 10 laptop seems to be noticeably faster to me at basic desktop tasks."
hardware,3d9dql,TheMW28,2,Tue Jul 14 18:53:01 2015 UTC,"as those have loading times you can often count in seconds.   This is not the case, even on my wimpy Surface Pro 3. If any sort of control panel app takes seconds to load you have a problem elsewhere."
hardware,3d9dql,bentan77,10,Fri Jul 17 10:11:35 2015 UTC,Could you summarise the results of this test? I don't speak polish and i'm sure a lot of people in this sub can't read this.
hardware,3d9dql,SirMaster,15,Fri Jul 17 17:04:47 2015 UTC,Most games don't see much of a difference except in these games where the 290X gets a big boost from W10:  Total War: Attila http://pclab.pl/art63999-67.html  Project CARS http://pclab.pl/art63999-62.html
hardware,3d9dql,dd_123,2,Tue Jul 14 15:09:37 2015 UTC,http://pclab.pl/art63999-52.html    http://pclab.pl/art63999-53.html
hardware,3d9dql,SteelChicken,-2,Tue Jul 14 15:47:32 2015 UTC,"not polish either, that's why I'm using google translate, you could too: https://translate.google.com/#pl/en/http%3A%2F%2Fpclab.pl%2Fart63999.html  and besides, there are a lot of graphs that don't need translation to understand them"
hardware,3d9dql,hdshatter,16,Tue Jul 14 17:02:37 2015 UTC,"The website is shit, translated or not. 85 pages? Come on."
hardware,3d9dql,Maldiavolo,5,Tue Jul 14 15:14:21 2015 UTC,Ridiculous.
hardware,3d9dql,Fastfingers_McGee,2,Tue Jul 14 17:30:21 2015 UTC,But the ad revenue ;)
hardware,3d9dql,Enderzt,13,Tue Jul 14 18:46:41 2015 UTC,"Pclab.pl is about the worst site for reviews you could go to.  Their numbers never look anything like any other site even with the same hardware.    In this case, they are using AMD 15.5 beta drivers even though 15.7 WHQL are the newest set and 15.6 beta before it.  They use an nondescript 15.200.1023 Win10 driver set (it's missing the last number/build).  There were several builds using that same branch.  Really only the last one prior to 15.7, 15.200.1023.7, didn't have glaring bugs or performance issues.       To add insult to injury they use Win10 build 10130.  That build was OK for stability, but nothing special.  It had a lot of bugs.  Win10 did not start to get really solid until the next build 10159, which had over 300 bug fixes.  10162 is better and the latest build 10166 is better still.    Case and point, they are testing way too early to make a solid conclusion about performance.    Edit:wording"
hardware,3d2diq,Gaget,27,Sun Jul 12 23:40:35 2015 UTC,I still want to see the mITX Z107 boards.
hardware,3d2diq,sk9592,3,Mon Jul 13 00:19:59 2015 UTC,"Yup. This is what I'm waiting for. I'm sitting on building something new, but I really want something small, Z170 i7 + 980 Ti in a small form-factor ITX is what I'm going for. It's the main reason I haven't gone X99 (yes I know about the ASRock X99 board, but that seems like a pain in the ass to use about cooling).  The EVGA Ranger looks like the one I'm going for."
hardware,3d2diq,Berzerker7,3,Mon Jul 13 04:01:19 2015 UTC,"What do you lose when going ITX, I always wondered, performance, features or just connectivity?"
hardware,3d2diq,chronaden,9,Mon Jul 13 11:03:21 2015 UTC,"Performance is identical. Motherboards don't really dictate performance (if at all), and OCing is very similar.  Features are often just making up for the small size. An example is the power daughter-board on some of ASUS's mITX offerings. Similar to their ATX boards, but they heavily market it on the box and in promo materials. But it's not like ATX motherboards has a massive deal of features that an average user would want, anyway. Perhaps voltage checkpoints, on-board buttons or extra headers, pretty much all of what you'll find on a normal ATX board, you'll find on an mITX board.  The only two real losses you end up with (or moreso, that I've encountered) are price and expansion. I paid an extra £40 for a comparable mITX board over an ATX one. Counter-argument: Cases are usually cheaper than their larger brethren, and there's loads more choice coming to the market these days.  For expansion, I have 4 SATA 3 ports, 1 M.2 port, 2 DDR3 DIMM slots and 1 PCIE slot. I already saturate all these, and I'm really yearning for adding more storage and RAM. Both are easily possible, and probably aren't issues for most users, but my £35 AM3 HTPC motherboard has more expansion than my mITX board, solely because of its size. Maybe not a big drawback for the average user, but its certainly there. Counter: Straight up connectivity is improved. You'll be hard-pressed to find a motherboard without good WiFi, Bluetooth and (maybe) M.2/mPCIE in mITX, without scraping the bargain bin of course."
hardware,3d2diq,JaffaCakes6,2,Mon Jul 13 11:37:23 2015 UTC,"Thanks for explaining! - What about aftermarket coolers, would you be able to mount, let's say the new Noctua d15 on such a board and in such a case?"
hardware,3d2diq,chronaden,2,Mon Jul 13 11:50:24 2015 UTC,"It varies from board to board, but quite a few mITX boards can accommodate bigger coolers, like the NH-D15.   One issue is that these coolers can breach the PCI-E slot, but manufacturers are slowly realising that not everyone wants to use a stock cooler or AIO. You do have to do your homework, but it's nice to see that manufacturers are really pushing for versatility in these small form-factors. Years ago it'd be laughable to put a R1 Ultimate on a mITX board, but not so much any more.    The bigger limitation is the case you're housing the board in, but with more and more smaller-sized cases being developed, I anticipate it'll become less and less of a problem. Right now, you'd struggle to accommodate something like the R1, but I think it'll change, especially with mITX cases now splitting into ""pretty small"" and ""teeny-tiny"" sizes as more are made."
hardware,3d2diq,JaffaCakes6,2,Mon Jul 13 12:03:50 2015 UTC,"Roger, and thanks again. I'll do my homework, planning a mITX skylake build :)"
hardware,3d2diq,chronaden,2,Mon Jul 13 14:05:11 2015 UTC,"Just connectivity. Usually only 4 SATA ports, 2 DIMM slots and a single PCI-e x16 slot for a GPU.   No performance or feature hit, aside from whatever gains you'd be missing from the missing connectivity."
hardware,3d2diq,Berzerker7,1 point,Mon Jul 13 11:25:01 2015 UTC,"Although the power delivery is likely to be less robust, so I suppose you can't overclock the CPU quite as much. Same with not being able to fit the most powerful coolers.  Still, not a huge tradeoff. I've been considering going ITX for Skylake myself. Maybe the biggest for me is no space for 2 GPUs, nor a soundcard.   Dual-GPU cards are one possibility though maybe unwise unless they're relatively low power due to cooling constraints. High-quality onboard sound with headphone amps probably isn't a thing for ITX, so you'd have to go with external solutions for that. Not a too big deal I guess.  But all in all ITX is a great choice for your usual CPU+1 GPU+SSD+maybe a HDD system. And that's what most modern systems have, really."
hardware,3d2diq,bphase,2,Mon Jul 13 11:37:55 2015 UTC,"Although the power delivery is likely to be less robust, so I suppose you can't overclock the CPU quite as much. Same with not being able to fit the most powerful coolers.   With the move to on-die VRMs with Haswell, this has become much less of an issue than with previous generations. However, some ITX boards come with daughterboards to help give extra power delivery/stability to the CPU. In terms of coolers, there are a bunch of ITX, small cases that can support, easily a 120mm CLC or even sometimes a 240mm CLC. Once you get to that level, the difference in performance in cooling is in the single digits.   Still, not a huge tradeoff. I've been considering going ITX for Skylake myself. Maybe the biggest for me is no space for 2 GPUs, nor a soundcard. Dual-GPU cards are one possibility though maybe unwise unless they're relatively low power due to cooling constraints.   At least for me, I'm running a 780 Ti SLI set up. Everything I've read shows the 980 Ti outperforms this. I've always wanted a downsize and it seems like I'd be losing almost no performance, if any, doing this at least now."
hardware,3d2diq,Berzerker7,3,Mon Jul 13 12:13:04 2015 UTC,"Skylake is moving those VRMs away again, though honestly I don't know how much difference that makes. But you're right, there are daughterboard solutions and some/many ITX cases can mount very powerful CLCs.  Also the 980 Ti is awesome indeed, but two are still a ton faster. Doesn't really matter unless you're going for 4K or 1440p@144Hz though. Compared to 780 Ti SLI, I guess a 980 Ti is pretty similar speed OC'd. Wins some and loses some depending on scaling, I'd assume."
hardware,3d2diq,bphase,3,Mon Jul 13 13:32:34 2015 UTC,There's one in the article. The EVGA Stinger (the article mistakenly calls it the Ranger).
hardware,3d2diq,BlayneTX,2,Mon Jul 13 04:03:29 2015 UTC,http://www.pcper.com/news/Motherboards/Computex-2015-ASRock-Shows-Skylake-Based-Z170-Gaming-Mini-ITX-Motherboard
hardware,3d2diq,blueluk,1 point,Mon Jul 13 10:13:12 2015 UTC,I'm kinda bummed that the product list from ASUS that came out a while back didn't include a new Maximus VIII Impact.
hardware,3d2diq,wewd,12,Mon Jul 13 05:07:18 2015 UTC,"Speaking strictly about color.  MSI, matched the color of the boards to the color of their GPU's. Gaming and Armor series. Good move.  I guess Asus has one that fits with the Strix. But the worse looking one...  Gigabyte went fully WTF. Quite a lot of red that doesn't fit their G1 and windforce series.  Rest of the manufacturers went with black, their best bet. Again decently good move from them."
hardware,3d2diq,PadyEos,9,Mon Jul 13 04:57:58 2015 UTC,"Aren't people tired of black&red? People love murdered out stuff... the high end gaming mobos should be all black, or start selling us shit we don't need like different color anodized heat sinks to swap out. You know people would drop money on that."
hardware,3d2diq,nekura_,1 point,Mon Jul 13 05:59:20 2015 UTC,"amen bruva.  Give me some purple, blue and green."
hardware,3d2diq,Le_rebbit_account,2,Mon Jul 13 07:37:36 2015 UTC,Like the Soyo P4X400?
hardware,3d2diq,dexter311,1 point,Mon Jul 13 10:39:55 2015 UTC,More like this beauty
hardware,3d2diq,Le_rebbit_account,-4,Mon Jul 13 16:43:28 2015 UTC,It sure pisses me off. All I have are blue fans.. You wouldn't paint the american flag green yellow and purple so why paint the intel boards red? fucking retarded as shit
hardware,3d2diq,GodKingThoth,18,Mon Jul 13 06:34:14 2015 UTC,"These guys are lazy as fuck. The EVGA Classy is E-ATX, not ATX... and it's Stinger, not Ranger."
hardware,3d2diq,nekura_,1 point,Mon Jul 13 04:54:18 2015 UTC,"If you love what you are doing then these misstakes shouldn't happen, they don't seem interested at all."
hardware,3d2diq,Techdestro,2,Mon Jul 13 08:06:08 2015 UTC,"Not to mention you can literally see an i in the photo of the ""Ranger""."
hardware,3d2diq,adaminc,1 point,Mon Jul 13 09:04:58 2015 UTC,its a click bait article with no new news from Computex.
hardware,3d2diq,xtothemess,-4,Mon Jul 13 10:35:22 2015 UTC,no one cares for overpriced evga to be honest
hardware,3d2diq,waregen,2,Mon Jul 13 10:09:16 2015 UTC,"Since they lost most of the design team to sapphire like 4 years ago, the releases have been mediocre, especially m-itx offerings.  They're definitely not the glory days of x58 when the boards were some of the best around."
hardware,3d2diq,Darkstryke,11,Mon Jul 13 21:12:45 2015 UTC,"Other than the USB 3.1 connectors, I dont really see what's so special about Skylake right now.   A bigger, beefier IGPU and maybe some nominal better stock clocks from Intel, but the actual chips themselves dont enthuse me as much as the current generation does with the 4790k.   For my new build I'll probably get Skylake anyway but I just find it sad that we are saddled with getting a bigger IGPU that a lot of people will never use, as opposed to a chip with more on die Cache or a couple more real cores.   And I know the -E platform does this. But with all the die area saved, you would think some of those -E improvements could trickle down into the mainstream sector."
hardware,3d2diq,Seclorum,3,Mon Jul 13 00:00:25 2015 UTC,"Its the natural evolution of Z97, and the chipset is very advanced compared to even X99."
hardware,3d2diq,xtothemess,1 point,Mon Jul 13 02:42:02 2015 UTC,"In terms of chipset advancement, I dont really see much beyond the change of 4 PLX PCIe 2.0 lanes to 4x PCIe 3.0 lanes.   Pretty much every Z170 board has multiple M.2 slots, but I dont really see them being that mainstream in the first place."
hardware,3d2diq,Seclorum,4,Mon Jul 13 02:48:33 2015 UTC,"That's the point though. Introducing m.2 en masse will help proliferate the standard, so that it becomes mainstream."
hardware,3d2diq,dexter311,2,Mon Jul 13 10:25:28 2015 UTC,"What are 4 PLX PCI-E 2.0 lanes???????   The Z97 PCH has 18 ports (for instance it can be configured at 6x PCI-E 2.0, 6x USB 3.0, and 6x SATA6G)  The X99 has 22 ports (for instance it can be configured as 6x USB 3.0, 6x PCI-E 2.0, and 10x SATA6g)  The Z170 PCH has 26 ports (6x USB 3.0 is required for 6 of the ports, the rest can be PCI-E 3.0 or SATA 6G)  THe Z170 PCH will have a better DMI bus and its PCI-E will be PCI-E 3.0, which is huge in terms of bandwidth.   Its much more equipped, and that is just for starters.   When you have a chance I would like to know what 4 PLX PCIe 2.0 is, because PLX is a company."
hardware,3d2diq,xtothemess,1 point,Mon Jul 13 10:34:09 2015 UTC,"Because the company PLX, provided a chip on the motherboard that independantly added more PCIe lanes to a motherboard."
hardware,3d2diq,Seclorum,1 point,Mon Jul 13 18:35:36 2015 UTC,The dmi is all Intel....
hardware,3d2diq,dylan522p,1 point,Mon Jul 13 20:40:27 2015 UTC,They have been adding PLX chips for years now...
hardware,3d2diq,Seclorum,1 point,Mon Jul 13 21:17:53 2015 UTC,On some motherboards to add more lanes than there are. Intel does not buy plx chips and add them on. They have their own pci controller of die.
hardware,3d2diq,dylan522p,1 point,Mon Jul 13 21:19:44 2015 UTC,"Exactly. It's not something that intel is putting on there, its something that mobo mfg's add."
hardware,3d2diq,Seclorum,1 point,Mon Jul 13 21:24:57 2015 UTC,We were talking about Intel dmi and the pci off the processor wtf does it have to do with plx chips.
hardware,3d2diq,dylan522p,3,Mon Jul 13 23:31:42 2015 UTC,I'm not even gonna do a new build at this point. Just gonna pop a 980 Ti and a new SSD into my old rig with my trusty i5-2500K.
hardware,3d2diq,Pufflekun,2,Mon Jul 13 09:11:48 2015 UTC,my trusty i5-2500K.     Best purchase ever. I only game on it so I've had no reason to upgrade yet. On one end that's a bit disappointing.
hardware,3d2diq,BootyCrab,11,Mon Jul 13 14:08:36 2015 UTC,"For my new build I'll probably get Skylake anyway but I just find it sad that we are saddled with getting a bigger IGPU that a lot of people will never use, as opposed to a chip with more on die Cache or a couple more real cores.   And I know the -E platform does this. But with all the die area saved, you would think some of those -E improvements could trickle down into the mainstream sector.    It's actually the opposite though. Almost noone is going to benefit from 6 cores standard. Your just gonna make all these consumer chips more expensive because of all the die space is wasted. Or you could invest it into iGPU which probably 99%+ of your customers will use and benefit from. Your kidding yourself if you think consumers outside a handful of gamers with $500+ in video card purchases over the past year would benefit much from 6 core cpu in the consumer line up. IGPU is getting closer and close to the cpu in terms of data too. Acceleration from iGPU for more and more tasks will happen. Calculations like physics (most calculations honestly) are not best served by just gpu or cpu a mix is what you really need. The way things are calculated is going to a hetrogenus way."
hardware,3d2diq,dylan522p,9,Mon Jul 13 01:28:46 2015 UTC,"Almost noone is going to benefit from 6 cores standard.   It's kind of a chicken and the egg problem.   No software is really written to exploit 6 cores except some specific uses like rendering, because there are hardly any tasks that are multi-threaded that much right now.  And IGPU's do have a use, but outside of extremely low end computing, people will tend to have some discrete graphics hardware.    Calculations like physics (most calculations honestly) are not best served by just gpu or cpu a mix is what you really need.   Honestly you need as fast a thread as possible for physics. This is because true N-Body physics is impossible to multi-thread properly. You run into way too many situations trying to multi-thread N-body physics, where you are stuck waiting on almost all cores for one calculation because true N-body physics requires previous calculations to determine the starting positions for following calculations.  Devs have been faking physics for years by using tons of pre-canned effects, or not running true N-body physics.   What's going to happen with IGPU's is that thanks to DX12, your going to get IGPU utilization at the same time as discrete graphics. It's not much but it's at least something. And thanks to the low DX12 overhead, you can toss things to the IGPU that wont impact the normal rendering pipeline that strongly.   Yeah devs will have to specifically optimize for it, but I see that as more a game engine problem that the engine designer needs to work out in the first place. And given the ubiquity of IGPU's the past few years, I can see most engines start to support this."
hardware,3d2diq,Seclorum,3,Mon Jul 13 01:46:34 2015 UTC,Real life has multithreaded physics. :-p  Information propagates at the speed of light. Any particle's position and momentum have a limited amount of precision.
hardware,3d2diq,HumanistGeek,2,Mon Jul 13 06:43:22 2015 UTC,God's dev level--Epic.
hardware,3d2diq,LintGrazOr8,1 point,Mon Jul 13 06:52:16 2015 UTC,"Actually real life has threaded physics, it's just very very fast.  You cant calculate the spray of glass from a windshield in a crash until you first calculate how the crash happens."
hardware,3d2diq,Seclorum,1 point,Mon Jul 13 18:43:27 2015 UTC,"The ""calculations"" are the interactions between subatomic particles. It's a neural network."
hardware,3d2diq,HumanistGeek,1 point,Mon Jul 13 19:05:27 2015 UTC,"Please find me something that runs all 4 Intel cores to max that's standard. It's rate to find anything that is threaded well. It's hard to do. It's really hard to thread something more than 2 ways but past 4 is... Well, you don't see it."
hardware,3d2diq,dylan522p,4,Mon Jul 13 04:28:17 2015 UTC,"Some games can, although all 4 cores dont stay maxed all the time.   Video encoding easily, it's been threaded for a very long time and very well too.   Storage I/O could be if we had wider adoption of NVME and SSD's on mainstream hardware.   Really past 4 threads you are looking at multitasking threaded applications anymore. So you have a game and then a video encode process going simultaneously and they can run just fine like that with enough Logical threads.   But there isn't much development for software than can thread past 4, simply because so few usable systems run with more than 4. You have 'workstation'/enthusiast boards that get 6-8 logical cores but devs aren't designing consumer grade software with them in mind simply because their market share is too low. The LCD is 2-4 cores nowadays."
hardware,3d2diq,Seclorum,-1,Mon Jul 13 04:40:38 2015 UTC,Even with every phone being 4-8 every consumer application out there can choke up my dual core low voltage. I can do so much. I'm on IRC Watchung a video on second monitor reddit + ~80 tabs 5 on chrome and 75 on Firefox. Like 6 extensions on each and that creates a fuck ton of threads because there is one for every tab amd every tab is its own process. I could play a league or Dota but the graphical settings limit me. My iGPU runs into a wall more than anything. I choke up sometimes with YouTube when I do youtube. Noone has discrete cards. Most people with them have the low end ones that are meh in laptops or desktops. The market for laptops or desktops with GPUs is fairly small and gamers are a small subset.
hardware,3d2diq,dylan522p,7,Mon Jul 13 05:06:09 2015 UTC,"While you may have a billion threads running, most of them will be sitting there attributed to the System Idle task because most of them dont need constant attention.   Systems nowadays are very good at bouncing threads around as needed because you dont need to be constantly running all of them all the time. And certainly not running them full bore.   But those people who run only on an IGPU will like the new processor, even if their current setup is just fine in every other regard. They would be better served by spending the 400-500 dollars for a new Skylake chip and motherboard, and investing that in a discrete GPU.   Of course laptop users are screwed, but they are always screwed. It's been decades and we still dont have discrete GPU standard for laptops. For a drop in replacement."
hardware,3d2diq,Seclorum,1 point,Mon Jul 13 06:06:32 2015 UTC,Maybe they will make some tablets based off skylake?
hardware,3d2diq,GodKingThoth,1 point,Mon Jul 13 06:28:25 2015 UTC,"Skylake iGPU is actually insane. There's going to be a 72EU model if we get direct scaling up from Broadwell EUs it's going to knock the pants off low end discrete. There wouldn't be a market for sub 150 GPUs. That's with direct scaling though, I doubt it will scale perfectly even with gen 10 HD graphics."
hardware,3d2diq,dylan522p,2,Mon Jul 13 08:26:19 2015 UTC,"The biggest things are massive improvements on the Intel HD Graphics performance (though still not able to replace a dedicated card), use of DDR4 and USB 3.1.   Other than that, the performance is going to increase maybe 5-10% over Haswell/Devil's Canyon. It seems like the only real exciting thing in terms of processors is going to be the eventual release of AMD Zen chips."
hardware,3d2diq,CaptSkunk,1 point,Mon Jul 13 04:08:42 2015 UTC,"It seems like the only real exciting thing in terms of processors is going to be the eventual release of AMD Zen chips.   I was thinking about the same. With DX12 making single-thread performance less important, Intel really ought to think twice about selling expensive CPUs that dedicate more transistors to mostly unused graphics than to the actual CPU.  My last PC was built with a 4790K like so many out there, but with the direction Skylake seems to be taking, Intel really isn't enticing me to give them any more money soon. I think it's great to have some integrated graphics so I can boot to the desktop if the GPU fails, but that's about it. My next PC should have a CPU that offers a significant performance boost over my current i7. If Intel designs ist next chips with graphics in mind, I might go with Zen, too."
hardware,3d2diq,hibbel,1 point,Mon Jul 13 13:32:24 2015 UTC,"I'm hoping that my 4690K can last a good 4 years or so. With a bit of luck, that might be possible. So far, it's shin itself to be a great chip.   The whole DX12 thing is all just statements and no hard facts/evidence yet. I'm not getting excited over it until I see that it can actually do half of the things that Microsoft claims it can do."
hardware,3d2diq,CaptSkunk,1 point,Mon Jul 13 13:43:55 2015 UTC,"Well Z97 boards today are shipping with 3.1 support. They may not get thunderbolt 3 connectivity but they can connect and charge 3.1 type c devices just fine.   And DDR4 doesn't really deliver any performance benefit over DDR3. So it's a wash there. Your buying nominally more expensive dimm's simply because that's what the board can take. Yet System memory hasn't been a bottleneck for awhile so it's not like even stock DDR4 Dimm's will hold you back any and your still going to pay more for them over DDR3 Dimms.   The performance increase clock to clock in leaked benchmarks we have seen show it's not any faster in tasks than Haswell, except tasks that are reliant only on IGPU performance at which point we still have discrete cards that beat that easily.   Zen is the next big hope or maybe Skylake/Broadwell -E chips and their new chipsets might have something interesting to bring to the table."
hardware,3d2diq,Seclorum,3,Mon Jul 13 04:26:04 2015 UTC,"I was waiting for skylake, but I'm underwhelmed. I've settled on an x99 build"
hardware,3d2diq,maybachsonbachs,4,Mon Jul 13 00:23:34 2015 UTC,"TBH, you are probably better off with that. Almost all of the features found on Skylake, are already on X99. Plus, you get more PCIe lanes and CPU's that perform just a tad bit better. Lastly, I would think that the price is not going to be too much off of what you'd have to pay for brand new Z170 hardware."
hardware,3d2diq,CaptSkunk,2,Mon Jul 13 04:12:46 2015 UTC,I might do that as well. It really depends on just how much DDR4 drops with Skylake's release.
hardware,3d2diq,Seclorum,1 point,Mon Jul 13 00:47:11 2015 UTC,"right now the cheapest ddr4 is only 25% more expensive/gb than the cheapest ddr3 stick, so..."
hardware,3d2diq,XorFish,2,Mon Jul 13 09:37:55 2015 UTC,"it's not just about price, it's speed and kit options. you can't really buy 16GB sticks yet unless you go 128GB or server RAM. I want 64GB(16GBx4)."
hardware,3d2diq,nekura_,1 point,Mon Jul 13 19:58:23 2015 UTC,It's been dropping for awhile. It was WAY more expensive a couple months ago.
hardware,3d2diq,Seclorum,1 point,Mon Jul 13 18:36:57 2015 UTC,That is right.   With Skylake we well probably see a break even followed by cheaper DDR4.
hardware,3d2diq,XorFish,1 point,Mon Jul 13 18:58:58 2015 UTC,The argument that an IGPU is useless gets a lot less compelling with directx 12 heterogeneous multi GPU support.
hardware,3d2diq,RearmintSpino,2,Mon Jul 13 03:32:27 2015 UTC,"Which means it at least gets some potential use, but it's entirely up to Engine devs and game devs to figure out how to use it properly and then task ti with something it can do.   With the varying capabilities of IGPU's it's going to be interesting just what tasks if any they toss onto it.   But for most things, it's going to be damn near useless. Only new titles or games updated specifically for DX12 even have a hope of getting that kind of support."
hardware,3d2diq,Seclorum,2,Mon Jul 13 04:05:31 2015 UTC,"Rule of thumb is buy hardware that you can make use of today.  People are hailing dx12 as the answer to everything, the reality is no games are going to be dx12 for a year, even longer for advanced features like being able to make use of the igpu. There's some parts of dx12 that current gen hardware can't even take advantage of today."
hardware,3d2diq,Le_rebbit_account,2,Mon Jul 13 07:35:46 2015 UTC,"It might not take a year, I think Battlefront has a good chance of being DX12. Deus Ex : Mankind Divided will be using DX12 and it should come early next year.  But otherwise, agreed. Remains to be seen how big of a deal DX12 is and how fast it profilerates."
hardware,3d2diq,bphase,1 point,Mon Jul 13 08:56:46 2015 UTC,iGPU power is still petty insignificant compared to any GPU over $200 or so and also seamless multi GPU I'll believe it when I see it.
hardware,3d2diq,hisroyalnastiness,0,Mon Jul 13 04:12:44 2015 UTC,"Sure, it's ""insignificant"" compared to a standalone video card. Now I don't know if it will work well in practice or not, but assuming it did, and you could get an extra 5fps for free off of a cpu/gpu you had anyway, why the fuck wouldn't you?"
hardware,3d2diq,RearmintSpino,-4,Mon Jul 13 05:34:05 2015 UTC,get an extra 5fps for free -you  Just pay us $60 a year and we'll give you all these old games for free-ms
hardware,3d2diq,GodKingThoth,10,Mon Jul 13 06:31:21 2015 UTC,"Can someone explain to me why this new socket for Skylake is supposed to be any better than the current X99?  If the rumors are true and Skylake just brings better integrated graphics performance, doesn't this prove that the ""enthusiast"" socket is a must have for the coming future?  The 5820K seems like the way to go these days, everyone still moans and bitches about more cores are useless and its all about single thread performance but the Q6600 wasn't viable for half a decade by accident."
hardware,3d2diq,barberstrisand,14,Mon Jul 13 01:38:46 2015 UTC,Because Z170 doesn't compete with X99. It competes with Z97.   Which isn't really saying much considering Z97 is just fine and is also getting goodies like USB 3.1 connectivity.
hardware,3d2diq,Seclorum,7,Mon Jul 13 02:38:25 2015 UTC,"I would not go as far as to say that it competes with Z97, rather that it is the evolution of Z97, as Z97 was the evolution of Z87 and so forth.  TBH, I just upgraded to a 4690K/Z97 combo, back in January. I have pretty much all I need, even an m.2 slot, should I need it. USB 3.1 would be nice but there isn't even anything 3.1 out there yet (what is, is super expensive)."
hardware,3d2diq,CaptSkunk,5,Mon Jul 13 04:17:05 2015 UTC,"Pretty sure the q6600 was viable for half a decade because it overclocked like a beast, not just because it had 4 cores."
hardware,3d2diq,wagon153,3,Mon Jul 13 02:10:57 2015 UTC,I'm still rocking a Q9650 @ 3.9Ghz :)
hardware,3d2diq,kr239,3,Mon Jul 13 09:09:03 2015 UTC,"If you were paying attention during those days it was the same rhetoric, everyone would say that the Core2 was better fer' gaymin cause it had faster single core performance and could overclock harder.  The Q6600 was sufficient in single thread performance and kept on when games starting utilizing more cores while the Core2 processors became unusable."
hardware,3d2diq,barberstrisand,0,Mon Jul 13 12:35:37 2015 UTC,That one pin adds 550 GB/s extra throughput for the internal diffraction combobulator.  Or it's planned obsolescence.
hardware,3d2diq,spiker611,1 point,Mon Jul 13 03:51:02 2015 UTC,don't encourage him
hardware,3d2diq,xtothemess,3,Mon Jul 13 07:26:52 2015 UTC,The picture of the ASUS Z170 Pro shows it with DDR3 Dimm slots. I think they may have just took a Z97 board and stuck a Z170 label on it so they had something to show. Unless it's actually going to be a DDR3 board.
hardware,3d2diq,BlayneTX,2,Mon Jul 13 10:36:42 2015 UTC,"Right now I really want to have the Asus pro, but it will be probably to expensive by far"
hardware,3d2diq,schneemensch,2,Mon Jul 13 09:43:20 2015 UTC,Anyone heard about when the Asus ROG boards will be coming?  Edit: Never mind I see the ROG VIII Hero on that poster. Weird the article doesn't talk about it. Here is the full list of Asus boards:  ASUS Z170 Pro Gaming ASUS Z170-A ASUS Z170-Deluxe ASUS Z170-G ASUS Z170i Pro Gaming ASUS Z170-K ASUS Z170M-E ASUS Z170-P ASUS Z170-Pro ASUS Sabertooth Z170 ASUS Maximus VIII Extreme ASUS Maximus VIII Gene ASUS Maximus Hero ASUS H170 Pro Gaming ASUS H170-I Plus ASUS H170M-E ASUS H170M-Plus ASUS H170-Plus ASUS H110H4-TM ASUS H110i-Plus ASUS H110M-C   That Maximus Extreme sounds juicy. I'm guessing that's the replacement for the Formula. No Impact thought so I guess they're skipping mITX for ROG boards for now.
hardware,3d2diq,BlayneTX,2,Mon Jul 13 06:38:45 2015 UTC,"That Maximus Extreme sounds juicy. I'm guessing that's the replacement for the Formula. No Impact thought so I guess they're skipping mITX for ROG boards for now.   Impact m-ITX & Formula ROG boards have been a delayed release vs hero / gene / extreme (if they do extremes) for a couple of generations now. They will come out, but it won't be for a good ~3 months post release."
hardware,3d2diq,Darkstryke,2,Mon Jul 13 09:35:43 2015 UTC,What's the price range for these bests?
hardware,3d2diq,behrangsa,2,Mon Jul 13 21:10:27 2015 UTC,22 -phase VRM seems a little over the top for Intel processors!
hardware,3d2diq,State_secretary,2,Mon Jul 13 09:48:17 2015 UTC,Will the release of new mobo + cpu drop the prices for current gen CPU/Mobo? Prices don't seem to drop when new stuff get released lately...
hardware,3d2diq,patronxo,2,Mon Jul 13 12:39:35 2015 UTC,"The price may drop a little across the board but don't expect it to ever drop enough to make it a really great deal compared to the new stuff. This is done intentionally to keep up profits.  With that said, retailers (like Microcenter) definitely will offer periodic sales of older generation stuff so they can clear out stock. My suggestion would be to setup a price watch alert with PCPartPicker.com on anything you've got your eye on. I've got tons of semi-unrealistic price watches setup on stuff I have my eye on in case theres a random sale somewhere."
hardware,3d2diq,Stingray88,2,Mon Jul 13 17:44:07 2015 UTC,Love the look of the new MSI Krait and Gigabyte boards
hardware,3d2diq,anosrep,2,Mon Jul 13 20:42:41 2015 UTC,agreed I think I'll go with the krait for my next build
hardware,3d2diq,Hubzee,1 point,Wed Jul 15 04:30:30 2015 UTC,Yeah I want to go black and white route. Just wish there were more options available for motherboards.
hardware,3d2diq,anosrep,-1,Fri Jul 17 11:23:43 2015 UTC,none of them have blue theme. All are red or black. retarded as shit since it's for a fucking intel
hardware,3d2diq,GodKingThoth,2,Sat Jul 18 23:35:17 2015 UTC,"I mean... I know Intel's color is blue... but their CPUs are silver and get completely covered anyways. So what would be the point of the mobo being blue simply because it's an Intel board?  Don't get me wrong though, blue/black is my favorite combo. I've got a Z77-UD5H with matching blue RAM personally."
hardware,3d7s9j,zmeul,13,Tue Jul 14 04:29:14 2015 UTC,"You don't have to editorialize the title. It's not Intel's 22nm, but that doesn't mean it isn't a 22nm process."
hardware,3d7s9j,Exist50,1 point,Tue Jul 14 06:06:48 2015 UTC,"No process is really whatever they claim it is. It's all marketing speak. The node name hasn't matched up with its historical meaning since at least 130 nm, and probably earlier, although I'm having trouble recalling specifics.  There are structures smaller/spaced less than 22 nm, and some larger (on the smallest layers). You aren't likely to find anything that's actually 22 nm though."
hardware,3d7s9j,III-V,-12,Wed Jul 15 00:29:22 2015 UTC,"I don't editorialize the title, it's KitGuru's title  also, GloFo is calling this 22FDX"
hardware,3d7s9j,Exist50,10,Tue Jul 14 06:11:50 2015 UTC,"Show me where in the title it says ""not really 22nm"". It doesn't."
hardware,3d7s9j,dylan522p,5,Tue Jul 14 06:19:10 2015 UTC,"This discussion is terrible beyond this point, op editorialized, the nm number is irrelevant. You have to look at gate characteristics length width height and interconnect scaling, how far transistors can reliably be put apart as well as sram cell size. GF seems to have combined the easier ability to make fdsoi and the minimum reachable without increasing mask number and multiple patterning. This is probably gonna be one of the lowest cost process nodes for a while."
hardware,3d7s9j,Exist50,2,Tue Jul 14 20:19:21 2015 UTC,"It's a shame too, because the article itself is actually interesting."
hardware,3d7s9j,Exist50,-6,Tue Jul 14 20:35:19 2015 UTC,that's my add because GloFo's process is not 22nm in any way shape or form - that's why I put it in parentheses    The 22FDX uses back-end-of-line (BEOL) interconnect flow of STMicroelectronics’ 28nm FD-SOI as well as front-end of line (FEOL) of STM’s 14nm FD-SOI.
hardware,3d7s9j,Exist50,6,Tue Jul 14 06:27:35 2015 UTC,"Your add=editorialization. And if you can please provide a widely accepted definition for what constitutes a 22nm process so we can compare, it'd be appreciated."
hardware,3d7s9j,Exist50,-4,Tue Jul 14 06:29:52 2015 UTC,I just quoted from the article what exactly doesn't constitute 22nm - did you even read the article?
hardware,3d7s9j,everyZig,6,Tue Jul 14 06:38:34 2015 UTC,"Yes, it's clear that it isn't a unified process, but you're acting like the term ""22nm"" is well defined, which it certainly isn't."
hardware,3d7s9j,cp5184,-4,Tue Jul 14 06:40:02 2015 UTC,"so, 28nm is ""defined""? how about 14nm?  how GloFo reached this 22 number: (28+14)/2=21, what's closer to 21? 22    what constitutes 22nm? how about this: https://www-ssl.intel.com/content/www/us/en/silicon-innovations/intel-22nm-technology.html    and let's address the ""editorialization"" you accuse me of   editorialize - make comments or express opinions rather than just report the news"
hardware,3d7s9j,Exist50,8,Tue Jul 14 06:50:06 2015 UTC,"So IBM's 22nm isn't 22nm either, since it isn't the same as Intel's (e.g. no finfets)? Give me a break.   And by that very definition   make comments or express opinions rather than just report the news   You can't argue you were ""just"" reporting the news. Saying this is ""not really 22nm"" isn't a fact, but an opinion, since there is no actual definition. Sure, point out its flaws, limitations, and differences, but try to keep it factual."
hardware,3d7s9j,cp5184,-6,Tue Jul 14 06:54:11 2015 UTC,"let me quote again from the article you clearly read   Despite of the fact that GlobalFoundries classifies its technology as a “22nm”, the process has nothing to do with 22nm production process used by Intel Corp. or other chipmakers   so yeah, gimme a break! it has nothing to do with FinFET"
hardware,3d2e44,nwgat,6,Sun Jul 12 23:45:55 2015 UTC,"Is there a such thing as a ""reference"" Fury, or do we just have manufacturer designs?"
hardware,3d2e44,sk9592,8,Mon Jul 13 01:56:20 2015 UTC,"There's no reference Fury, and AMD has already stated that only the Fury X is the reference card.  All Fury cards will have custom PCBs, custom coolers, or both."
hardware,3d2e44,animeman59,5,Mon Jul 13 08:25:18 2015 UTC,"not yet, the closest is the sapphire Tri-X non-OC, that uses the reference fury (x) pcb but with a custom cooler"
hardware,3d2e44,darrenphillipjones,4,Mon Jul 13 03:16:37 2015 UTC,All I wanted was a cheaper fury without some crazy cooler so I could watercool it on a budget. Really wish they did a reference. Oh well.
hardware,3d2e44,SHEADYguy,3,Mon Jul 13 13:27:53 2015 UTC,"The Sapphire Fury uses the same PCB as the Fury X, and there is already a waterblock announced for that PCB.   http://www.tomshardware.com/news/aquacomputer-r9-fury-x-waterblock,29569.html"
hardware,3d2e44,darrenphillipjones,1 point,Mon Jul 13 20:44:16 2015 UTC,yes but a reference non x would be a lot cheaper. Sapphire has always been on the higher side.
hardware,3d2e44,SHEADYguy,2,Tue Jul 14 02:52:27 2015 UTC,Sapphire is selling a non-oc version of the Fury at the AMD msrp of $549 in addition to an oc version for a couple bucks more
hardware,3d2e44,darrenphillipjones,1 point,Tue Jul 14 03:08:41 2015 UTC,"Pardon my French, where are these listed for sale? When the fury x came out it was listed at 650. Yet I always seem it for 699.  Or it's out of stock, because it seems their supply is terrible."
hardware,3d2e44,SHEADYguy,1 point,Tue Jul 14 03:13:47 2015 UTC,"Official launch is tomorrow, July 14. As I said I mentioned MSRP. I never claimed a street price."
hardware,3d2e44,darrenphillipjones,1 point,Tue Jul 14 03:29:33 2015 UTC,No problem. Just had a few beers. It's my real birthday! Go skylarks!
hardware,3d2e44,reynardtfox,2,Tue Jul 14 04:23:35 2015 UTC,The Fury isn't even full Fiji though.  It has cut down cores so why not just go Fury X with the built in AIO cooler?
hardware,3d2e44,darrenphillipjones,2,Mon Jul 13 15:41:30 2015 UTC,Custom loop.
hardware,3d2e44,reynardtfox,1 point,Mon Jul 13 16:06:54 2015 UTC,Ah okay.  That makes sense.
hardware,3d051b,XorFish,46,Sun Jul 12 10:47:27 2015 UTC,"With the 980 seeming ever more likely to drop to $479, it seems pretty obvious at this point.  At $550 the fury is a great card for sure, but the overclocking potential of the 980 just makes it nearly a straight match, even with an overclock on the fury."
hardware,3d051b,w00t692,23,Sun Jul 12 12:08:40 2015 UTC,Right now you can get one for 460$:  http://pcpartpicker.com/parts/video-card/#c=185&sort=a8&page=1
hardware,3d051b,w00t692,5,Sun Jul 12 12:39:30 2015 UTC,Dang :)
hardware,3d051b,thinkythought,4,Sun Jul 12 12:48:39 2015 UTC,Wow i expected that to be with rebate. I wonder what price they're gonna hit during the amazon sale...
hardware,3d051b,Choreboy,4,Sun Jul 12 20:46:18 2015 UTC,Don't jinx it!
hardware,3d051b,WillWorkForLTC,3,Mon Jul 13 00:37:36 2015 UTC,For Canadians we'll end up paying more. That's how it is up here. We get shafted quite a bit with mark ups and the like. I hope its good for my American brothers at least. You guys don't have it easy either in many other departments so I really shouldn't complain.
hardware,3d051b,SpartanXI,8,Mon Jul 13 06:14:05 2015 UTC,"Got a 980 yesterday, for ~$211 cuz of microcenter sale !  Edit: People messaging me for proof should checkout my previous post"
hardware,3d051b,Taubin,9,Sun Jul 12 19:55:37 2015 UTC,I wish I still lived in the states when I see things like that lol
hardware,3d051b,sbjf,4,Sun Jul 12 21:21:36 2015 UTC,"Wow, definite no-brainer. I'm jealous."
hardware,3d051b,epsys,0,Sun Jul 12 21:41:44 2015 UTC,"you want a 27"" at 1440p  blah blah    no! I just got my QNIX QX320QHD. 27"" 1440p pixels are way too small"
hardware,3d051b,epsys,-1,Mon Jul 13 00:22:40 2015 UTC,you're just telling yourself that O:)
hardware,3d051b,Corncove,0,Mon Jul 13 02:34:02 2015 UTC,Why would I want a 970 for 1440p when there are games it can't even maintain 60fps at 1080p?
hardware,3d051b,JeffroGymnast,3,Mon Jul 13 09:24:32 2015 UTC,There is such a thing as turning down AA...
hardware,3d051b,w00t692,1 point,Mon Jul 13 20:28:03 2015 UTC,"AA is not often the reason for low framerate anymore, most AA that is on is low impact shader types."
hardware,3d051b,JeffroGymnast,1 point,Mon Jul 13 04:29:37 2015 UTC,"Going back to what the parent comment said... The gtx 970 is incredibly capable at 1080p with post-processing type AA (FXAA for example). But I am referring to MSAA or SSAA, which when turned up most definitely affect performance by a great deal"
hardware,3d051b,w00t692,1 point,Mon Jul 13 05:04:26 2015 UTC,If you have SSAA on and you complain about framerates you're an asshole.
hardware,3d051b,JeffroGymnast,1 point,Mon Jul 13 19:50:41 2015 UTC,"Some gamers see an option called SSAA and they consider ""max settings"" to include that... It's dumb I agree"
hardware,3d051b,sterob,3,Mon Jul 13 20:18:33 2015 UTC,because one of the main plus for increasing resolution is you need less AA.
hardware,3d051b,w00t692,2,Mon Jul 13 20:20:08 2015 UTC,"Overclocking time! Both the 980 ti and 980 can pick up a game running at 50 avg fps to 60 avg fps with overclocking.  The 970 due to the lower shader amount and lower memory bandwidth doesn't pick up as large of a percentage, it doesn't seem to scale as well."
hardware,3d051b,JustaPassanger,-8,Mon Jul 13 20:25:42 2015 UTC,"Go ahead, buy the Zotac but I wanna see proof."
hardware,3d051b,JustaPassanger,0,Mon Jul 13 06:32:41 2015 UTC,I don't think they ship to Switzerland...
hardware,3d051b,namae_nanka,-8,Mon Jul 13 09:23:42 2015 UTC,"You wouldn't buy it anyways. Most go for the known and recommended brands like MSI, Asus, EVGA, Sapphire. So bringing a Zotac as a price example is as realistic as a compact nano version that's often on sale. Noone is going to buy it."
hardware,3d051b,w00t692,1 point,Mon Jul 13 19:51:20 2015 UTC,"No, I would probably get a 980 ti or 390 CX for the best price/performance."
hardware,3d051b,jinxnotit,-18,Sun Jul 12 21:35:34 2015 UTC,"All in vain    Yep  edit: Of course it's in vain, should've remembered that you seem to have habit of not understanding.  https://www.reddit.com/r/hardware/comments/3b8bid/amd_didnt_get_the_r9_fury_x_wrong_but_nvidia_got/cski55q  I doubt you'd need an indepth article because you seem constitutionally unable to understand them."
hardware,3d051b,BuildYourComputer,4,Sun Jul 12 21:40:30 2015 UTC,I said I'd like to see the review.  I saw the review.  The fury barely edges the 980 at a lower clockspeed than I'm running.  And he did all of 3 games.  So who knows how it does in other games.  All for what is now much more expensive :)
hardware,3d051b,jinxnotit,1 point,Sun Jul 12 21:52:21 2015 UTC,"20% difference in performance is ""barely"" edging out? Lol. On what planet?"
hardware,3d051b,w00t692,4,Sun Jul 12 21:55:43 2015 UTC,On a planet where overclocking headroom exists. Stop being a dick.
hardware,3d051b,BuildYourComputer,-4,Sun Jul 12 15:33:35 2015 UTC,"Are you honestly going to lie to our faces and tell us that Fiji, in any of its variations, has zero overclocking headroom based entirely on locked voltage controls?  And are you SERIOUSLY going to show how ignorant your statements are in comparing the way that the architectures respond to overclocking? And how overclocking an Nvidia card operates on a degrading curve with larger boosts, while GCN based processors operate on a smaller overclock boost, but larger performance gains per clock?  I think we all want to be enlightened on how you are drawing your own flawed conclusions on this issue."
hardware,3d051b,BuildYourComputer,5,Sun Jul 12 16:15:37 2015 UTC,The review he was talking about the fury was overclocked and so was the 980. The differences were small and my 980 is clocked 70 mhz higher than theirs was.  Basically the asshole is following me around with personal attacks and shit.
hardware,3d051b,jinxnotit,4,Mon Jul 13 00:27:26 2015 UTC,"I can see how this guy doesn't understand when he's wrong, and gives straw man arguments to cover it up. What an idiot."
hardware,3d051b,BuildYourComputer,6,Mon Jul 13 00:53:34 2015 UTC,"Literally every review points out that you can't hardly oc the fury, and that the 980 takes a lead once overclocked."
hardware,3d051b,jinxnotit,-2,Mon Jul 13 01:03:27 2015 UTC,And why can't you overclock the fury very much yet?
hardware,3d051b,namae_nanka,2,Mon Jul 13 01:18:55 2015 UTC,"Because at max I've seen them reach about 1050mhz, and adding voltage isn't going to give you another huge 100mhz, you're going to see about 20 to 60 if you're lucky. Equally, HBM doesn't handle overclocking as well as GDDR5.   It really sounds like you have no idea what this gpu is."
hardware,3d051b,w00t692,-1,Mon Jul 13 01:35:20 2015 UTC,"Lol!   To get right to the point here, overclockers looking at out of the box overclocking performance are going to come away disappointed. While cooling and power delivery are overbuilt, in other respects the R9 Fury X is very locked down when it comes to overclocking. There is no voltage control at this time (even unofficial), there is no official HBM clockspeed control, and the card’s voltage profile has been finely tuned to avoid needing to supply the card with more voltage than is necessary. As a result the card has relatively little overclocking potential without voltage adjustments.   http://www.anandtech.com/show/9390/the-amd-radeon-r9-fury-x-review/26  That is some amazing Crystal Ball you have..."
hardware,3d051b,namae_nanka,-7,Mon Jul 13 01:34:16 2015 UTC,"See the second link then for TPU's numbers which btw I did point out yesterday as well. If an OC'ed Fury, which is performing around Fury X and 980Ti, is to be overtaken by a OCed 980 which starts getting bandwidth starved at 1440p, you're living in lala land.  Hardwareluxx is the most Fury favoring review as of yet, still waiting for ixbt's, even if you aren't considering only 4k ultra. Taking 50fps avg as the desired,   Crysis 3: 1600p 1xAA 1xAF 27% Bioshock: 4k ultra 28% BF4: 4k 1xAA 1xAF 34% CoH2: 1600p 'kein' AA 1xAF 12% AA seems to cut down performance in half. Metro LL: 1600p 'kein' AA 1xAF 19% Tomb Raider: 4k FXAA 36%     http://www.hardwareluxx.de/index.php/artikel/hardware/grafikkarten/36032-asus-radeon-r9-fury-strix-im-test.html   All for what is now much more expensive :)    At least for the official word from nvidia."
hardware,3d051b,w00t692,2,Mon Jul 13 03:05:16 2015 UTC,"Official word? Zotac already has2 models for under 479.  Fucking 37-4x fps avg at 4k is not playable, so I don't give a shit about 4k results and neither does anyone else."
hardware,3d051b,namae_nanka,-10,Mon Jul 13 04:05:55 2015 UTC,"Official word?    Yes, a temporary drop in prices means nothing until it's confirmed.  https://www.reddit.com/r/hardware/comments/3d0g43/nvidia_partners_cut_prices_of_geforce_gtx_980_ti/ct0n938   Fucking 37-4x fps avg at 4k is not playable, so I don't give a shit about 4k results and neither does anyone else.    Yeah that's only relevant when it comes to how FreeSync can't go down to GSync's lower frame limit. ;)  I've had enough of your nonsense now, it's one thing to live in lala land another to not even read what's written. Fucking waste of time."
hardware,3d051b,w00t692,8,Mon Jul 13 04:14:08 2015 UTC,"I read everything you say, and I take it with a grain of salt because none of your results are anything but averaged and leveraged from games and settings where the fury does better.   Which is called making yourself an echo chamber for your feelz. I don't dispute it does better at 4k.   The fury X catches up a shit load to the 980 ti at 4k.  Following me around to post your tediously worked on shit is getting annoying.   Go bother someone else.  Getting a 980 to play 4k is stupid.   You should get 2."
hardware,3d051b,namae_nanka,-9,Sun Jul 12 16:29:35 2015 UTC,"I read everything you say   Nonsense, you're going on about somehow magically getting a 25% boost from 980 when it's bandwidth starved even with a Ghz overclock on memory. Versus me posting results where a Fury is assblasting the 980 with 50fps as average at lower resolutions lower than 4k. Same goes for the games in TPU's review with avg. fps over 50.   It's amusing how low fps is not a problem when the nvidia shenanigans are to be turned on to make 980 look like a viable competitor. Which I again wrote of before as well,   Performance jumped up on the ASUS STRIX R9 Fury. The Fury is now 31% faster than the GeForce GTX 980. The setting holding back performance seems to be the NVIDIA Depth of Field in this game. The GTX 980 can render it much better, the Fury not so much.   Nevermind that we should accept that 980 will magically catch upto Fury while Fury does perform right next to Fury X and 980Ti for a 100$ less on real confirmed pricing and not some temporary sale.   Following me around to post your tediously worked on shit is getting annoying.    Too bad your retarded shit was upvoted to the top despite me blowing away your delusions yesterday. But of course, it wouldn't matter to you."
hardware,3d051b,twatsmaketwitts,6,Sun Jul 12 16:59:22 2015 UTC,"I'm on my phone and I'm just not gonna reply to all these points right now.   Good luck.   The 980 has been out 9 months and an overclock makes all the benefits a wash and the recent price drop makes it even more competitive. I don't know about you but I like turning settings on sometimes, regardless of who paid for them."
hardware,3d051b,LiberDeOpp,-8,Sun Jul 12 17:12:29 2015 UTC,"The 980 has been out 9 months    290X has been around for even more. At a lower price. This kind of stupid point is not even a point.   and an overclock makes all the benefits a wash and the recent price drop makes it even more competitive   I think nvidia got traumatised with that ixbt review with Fury X trouncing 980Ti and often Titan X as well. nvidia dropping prices to become more competitive after 960 competing against a friggin' 290 in its price bracket, requires that kind of an explanation.   I don't know about you but I like turning settings on sometimes, regardless of who paid for them.    Then 2 980 aren't enough, you should get 4 Titans. I can play this game too."
hardware,3d051b,neurokirurgi,28,Sun Jul 12 17:15:45 2015 UTC,For someone about to upgrade my monitor and Gpu the Fury seems like the better deal when I'd have to include the g-sync premium in the equation.
hardware,3d051b,animeman59,2,Sun Jul 12 17:32:13 2015 UTC,If you get 120hz or above and a decent card you can go with either brand.
hardware,3d051b,Esyir,-40,Sun Jul 12 17:38:00 2015 UTC,G-Sync is better tech though.
hardware,3d051b,d2_ricci,56,Sun Jul 12 17:45:41 2015 UTC,"A monitor shouldn't determine your video card, and your video card shouldn't determine your monitor.  That proprietary BS is the antithesis of PC gaming."
hardware,3d051b,Esyir,11,Sun Jul 12 14:54:20 2015 UTC,"There's been a hell of a push in that direction though. G-Sync, Gameworks. I'm waiting for GPU exclusives as the next step."
hardware,3d051b,screwyou00,19,Sun Jul 12 23:52:05 2015 UTC,you talk like this would be a good thing.  proprietary (in general) means higher price premium
hardware,3d051b,CFGX,21,Sun Jul 12 14:54:54 2015 UTC,"Believe me, I'm of the complete opposite opinion. But considering that I've seen not only acceptance but praise for these tactics, I'm not expecting this to end anytime soon."
hardware,3d051b,TruckChuck,13,Sun Jul 12 15:14:08 2015 UTC,"It is really strange. Like how some people said Microsoft buying out AMD (when the rumour emerged) would be bad because it would create a closed ecosystem where MS has potential to fuck people over who don't use their CPUs, yet when you tell them NVIDIA is almost doing this with GameWorks/G-Sync, they'll say it's different because NVIDIA is innovating and you don't have to turn on GameWorks or buy GSYNC. Well if that is the case you don't have to buy Windows (if MS did buy out AMD) and ""turn on"" a Windows PC. Anyone should be worried when a hardware manufacturer releases closed ecosystem software that gets implemented into products that should be agnostic of hardware choice.  And I'm not putting on a tin foil hat and saying NVIDIA is evil or that AMD is the best. I'm just saying any closed ecosystem product being implemented into what should be agnostic products justifies scrutiny"
hardware,3d051b,cheekynakedoompaloom,12,Sun Jul 12 15:48:55 2015 UTC,"Stuff like G-Sync and Gameworks is why I'll never buy Nvidia, even if I set aside the fact that the last Nvidia card I had failed after only a year and Nvidia gave no fucks despite it being a range-wide flaw."
hardware,3d051b,TruckChuck,-15,Sun Jul 12 16:00:25 2015 UTC,Freesync is proprietary too  Oopsie poopsie I went against the anti nvidia circlejerk
hardware,3d051b,cheekynakedoompaloom,8,Sun Jul 12 16:35:08 2015 UTC,http://www.vesa.org/news/vesa-adds-adaptive-sync-to-popular-displayport-video-standard/  it's not at all proprietary.
hardware,3d051b,TruckChuck,-9,Sun Jul 12 22:34:44 2015 UTC,Nvidia cards can't use it.
hardware,3d051b,cheekynakedoompaloom,11,Sun Jul 12 19:53:20 2015 UTC,they're free to do so and it wont cost them a dime in additional royalties.
hardware,3d051b,Esyir,-12,Sun Jul 12 23:40:33 2015 UTC,Amd knows nvidia can't do that.
hardware,3d051b,TruckChuck,8,Mon Jul 13 01:18:28 2015 UTC,"shrugs  nvidia could, but they'd have to give up that sweet gsync module money and vendor lock in of gsync. one of the perils of being proprietary instead of encouraging open standards."
hardware,3d051b,TheBros35,6,Mon Jul 13 02:32:16 2015 UTC,Dat argument by /u/TruckChuck   Freesync is proprietary too  Nvidia cards can't use it.  Amd knows nvidia can't do that.  Gsync works a lot better. I'm glad they don't give it up.   Them goalposts just keep shifting.
hardware,3d051b,gizza,-8,Mon Jul 13 02:50:15 2015 UTC,Gsync works a lot better. I'm glad they don't give it up.
hardware,3d051b,kennai,1 point,Mon Jul 13 02:56:25 2015 UTC,The hell does that mean
hardware,3d051b,DRW_,1 point,Mon Jul 13 03:18:23 2015 UTC,NVidia choose not to use it. They could if they wanted to.
hardware,3d051b,kennai,19,Mon Jul 13 04:14:41 2015 UTC,"G-sync is better in the low end when you drop below the monitor's minimum refresh rate. Freesync is better in the high end when you have the choice to disable or enable v-sync or the framerate control thing in their drivers.   You can overcome the minimum weakness of freesync by lowering your graphical settings which you should be doing if you're dropping that low in FPS consistently enough for that minimum window to be a problem.   You can't disable v-sync if you have g-sync on.  IMO, free-sync in the better tech, because it is more flexible and you can just not let your setup dip below the minimum window as an intelligent user.   That has nothing to do with me buying 3 LG 29UM67-P monitors. I love these monitors.   The only other issue freesync has that you can't fix is ghosting which is another caveat of the monitor it is installed in, which depends on your perception. With my current monitors I have not noticed it, but my vision lets images burn in very quickly which means I always see ghosting. So telling what is caused by the monitor and what is caused by my eyes would be impossible for me to differentiate."
hardware,3d051b,DRW_,5,Mon Jul 13 03:22:09 2015 UTC,You can't disable v-sync if you have g-sync on.   Yes you can.   framerate control thing in their drivers.   What are you referring to with this?
hardware,3d051b,kennai,1 point,Mon Jul 13 03:24:14 2015 UTC,"Yes you can.   Oh, they updated that a month or two ago. Thanks for the catch.   What are you referring to with this?   It's like a more power efficient version of v-sync that you can enable through their drivers."
hardware,3d051b,DRW_,1 point,Mon Jul 13 05:01:04 2015 UTC,NVIDIA also have frame rate target control - they've had it for over three years now (introduced with the 600 series). The only real difference is that they just expose it via NVAPI (or similar) for applications like EVGA precision expose to the user.
hardware,3d051b,kennai,1 point,Sun Jul 12 16:03:05 2015 UTC,"Yes, I know.   I was talking about if you were an AMD user, you have two options you can enable v-sync in every individual game or turn that on in the driver software."
hardware,3d051b,Esyir,1 point,Sun Jul 12 20:49:06 2015 UTC,"Freesync is better in the high end when you have the choice to disable or enable v-sync or the framerate control thing in their drivers.   I guess this is what I'm confused by, why does this make Freesync better at the high end when you can do both of those things on NVIDIA too?"
hardware,3d051b,DRW_,2,Sun Jul 12 21:07:52 2015 UTC,"I was going off of old information where g-sync forced this on and did not allow you to remove them. Now they allow you to remove v-sync. So that's no longer really true.  There was some added latency when you hit near the limit of g-sync, but that's usually not a problem unless you're playing twitch shooters or competitively.   I would be interested if this added latency is also inside of freesync monitors compared to non-freesync. More specifically one monitor being tested for input latency with freesync enabled and disabled at various refresh rates."
hardware,3d051b,veyron3003,1 point,Sun Jul 12 21:19:58 2015 UTC,Lower price. Same Feature.
hardware,3d051b,neurokirurgi,1 point,Mon Jul 13 00:25:17 2015 UTC,That's not what I asked.
hardware,3d051b,veyron3003,8,Mon Jul 13 00:43:03 2015 UTC,not for $200 extra
hardware,3d051b,neurokirurgi,-9,Mon Jul 13 05:36:01 2015 UTC,I'm not here to debate what the customer should be buying and which alternative gives a better price to performance ratio. I am saying that G-sync is the winner when it comes to handling variable refresh rates and there is a legitimate reason to use the module.
hardware,3d051b,veyron3003,11,Mon Jul 13 04:15:23 2015 UTC,Its not $200 better.
hardware,3d051b,dumkopf604,-18,Mon Jul 13 04:31:57 2015 UTC,And others may disagree.  edit: how childish of you to downvote me
hardware,3d051b,Teethpasta,3,Sun Jul 12 15:17:02 2015 UTC,Are you sure about that?  http://i.imgur.com/Jr3qGwy.jpg
hardware,3d051b,neurokirurgi,7,Sun Jul 12 17:31:06 2015 UTC,"I'm not here to debate what the customer should be buying   You're doing exactly that. You're here evangelizing G-sync. And no, unless you've tested them side by side, you have no leg to stand on."
hardware,3d051b,psycho202,-1,Sun Jul 12 17:35:35 2015 UTC,Higher latency all for better performance at super low frame rates? Yeah if you have frame rates that low with a premium setup like that something is wrong.
hardware,3d051b,Astrobliss,-5,Sun Jul 12 17:44:48 2015 UTC,"Go play any newer game with ultra graphics where a single card setup will cough. Yeah, you might want to have something that handles low refresh rates gracefully.  Besides, there are numerous FreeSync monitors out there with a low cap of just 40hz, which is really bad indeed."
hardware,3d051b,twatsmaketwitts,4,Sun Jul 12 18:27:23 2015 UTC,"Well, if you get lower than 40FPS dips, you might want to change your graphical settings though."
hardware,3d051b,Peacefulchaos6,3,Sun Jul 12 21:02:40 2015 UTC,"When the refresh rate drops lower than the monitors refresh rate g-sync and freesync would make the monitor go black but the refresh rate gets doubled to go back into the monitor's refresh rate window. G-sync monitors are forced to do this and it's a good thing, freesync has it available but the monitor manufactures have to make the monitor compatible with it. iirc wendel made a freesync monitor that allows the doubling to take place which would make g-sync and freesync the exact same thing but freesync would be cheaper"
hardware,3d051b,downeverythingvote_i,3,Sun Jul 12 18:02:15 2015 UTC,"True, but I can't justify the £200 premium for the difference in a 1440p 144Hz monitor."
hardware,3d051b,autowikibot,-10,Sun Jul 12 18:05:14 2015 UTC,AS far as I know FreeSync only works up to 96hz or so. So there is a reason for the premium.
hardware,3d051b,masturbateAndSwitch,17,Sun Jul 12 20:26:30 2015 UTC,That's for the mg279q I'm pretty sure. Just bought the xg270hu (Still shipping) and the free sync range is 30-144hz.
hardware,3d051b,ICanHazTehCookie,16,Sun Jul 12 21:38:41 2015 UTC,"(FreeSync has a dynamic refresh rate range of 9-240Hz.)[https://en.wikipedia.org/wiki/FreeSync]  The current limitations you see in FreeSync are based on hardware implementations. FreeSync is quite new, give it a couple of years and you will be seeing monitors with a much higher range of support."
hardware,3d051b,twatsmaketwitts,3,Sun Jul 12 15:05:02 2015 UTC,"FreeSync:       FreeSync is an adaptive synchronization technology initially developed by AMD in response to NVidia's G-Sync for LCD displays that reduces screen tearing.  FreeSync is royalty-free, free to use, and has no performance penalty.  As of 2015, VESA has adopted FreeSync as an optional component of the DisplayPort 1.2a specification.  FreeSync has a dynamic refresh rate range of 9-240Hz.     Image i     Relevant: DisplayPort | Nvidia | G-Sync | UnboundID   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3d051b,twatsmaketwitts,6,Sun Jul 12 15:15:26 2015 UTC,"Depends on the monitor-- some have that limitation, some don't. Gotta read specs and reviews.    The only real, inherent difference between freesync and G-sync is how the tech handles it when the game's framerate drops below the monitor's minimum refresh rate. G-sync will double up frames and the refresh rate to keep it looking smooth, while freesync can't do that and will act like either V-sync on or V-sync off (configurable) when the framerate is below the monitor's minimum refresh rate.    Less importantly, when the game's framerate is above the refresh rate window, they handle that differently, too. G-Sync will act the same as V-sync on, while freesync is configurable, so it can either act like V-sync on or V-sync off."
hardware,3d051b,chapstickbomber,7,Sun Jul 12 15:28:39 2015 UTC,Thanks for the explanation.
hardware,3d051b,chapstickbomber,6,Sun Jul 12 16:14:01 2015 UTC,xl2730z's freesync range is 40-144hz.
hardware,3d051b,WillWorkForLTC,1 point,Sun Jul 12 16:14:57 2015 UTC,"Yeah that's true, but I think it depends on the scalers the monitors have at the moment.   The monitor I'm looking at is the MG279Q which has Freesync from 35-90Hz which is as high an FPS as I could hope for unless I Sli/Crossfire."
hardware,3d051b,Thiswontblowyourmind,8,Sun Jul 12 15:29:34 2015 UTC,And for CSGO or so you simply turn off FreeSync and can enjoy the full 144hz. I really can see you there.   If you look at price/performance you are probably better of with two r9 390. They will destroy the Fury X and GTX 980 ti.
hardware,3d051b,WillWorkForLTC,1 point,Sun Jul 12 15:30:45 2015 UTC,"I would, but I'm not sure if Freesync works with Crossfire yet."
hardware,3d051b,Thiswontblowyourmind,5,Sun Jul 12 19:52:18 2015 UTC,Seems the lates driver supports it:  http://www.anandtech.com/show/9425/amd-releases-catalyst-157-whql-drivers-crossfire-freesync-win10-support-more
hardware,3d051b,hadrianmt,2,Sun Jul 12 15:22:28 2015 UTC,"For any game that consistently runs above 90fps, I just run 144hz and vysnc the game. Drops below that hit 72hz, which I've never even noticed. Alternatively, use FRTC to max at 90. 144hz mode has a little less input lag so I usually do the former for twitch games. And by a little less, I mean like 7ms or so, so basically academic.   GTAV runs 35 to 80 fps pretty consistently on my 290x/9590 and freesync works perfectly. AA straight up murders your framerate though. Even the difference between 2x and nothing is extreme."
hardware,3d051b,amorpheus,2,Sun Jul 12 15:29:31 2015 UTC,Isn't screen tearing pretty minimal for above 90 fps? The difference between two frames becomes smaller and smaller the higher fps you get. So I would probably just turn it off or use something like adaptive V-Sync for 90+fps.
hardware,3d051b,w00t692,1 point,Sun Jul 12 16:22:36 2015 UTC,"It is minimal, but so is the latency introduced by capping at 90, so it's a pretty wonderful problem  to have to make a decision on, because the results are ultimately so similar."
hardware,3d051b,fanchiuho,15,Sun Jul 12 16:24:44 2015 UTC,TekSyndicate has been growing on me lately. They've really matured and their reviews are becoming lower quantity but are of a MUCH higher objective quality. This is the kind of reviewing that I love because they are truly showing what counts and leaving out the noise. Stock clocks. Some reviewers focus so much on stock clocks. If you're obsessed with stock clocks then price to performance isn't your #1 priority so the controversy is moot.
hardware,3d051b,WillWorkForLTC,6,Sun Jul 12 15:33:16 2015 UTC,"Tek Syndicate is good, but I always feel like they don't quite know what they're talking about. Some of their stuff is great, but other stuff, like when Logan called the (Chokes or VRMs?) along the side of the board the ""High bandwidth memory,"" something which I would have thought, at least in the tech world, every man and his dog would have known resided on the GPU die itself. Also, their 3 channel system is a bit weird, but does work reasonably well."
hardware,3d051b,fanchiuho,2,Sun Jul 12 15:43:33 2015 UTC,Logan is a scatterbrain so often he will gloss over specifics. Wendel is clearly the brains. Logan definitely needs to work from a script and then inject a little humor in between his lines rather than try to do everything all at once.
hardware,3d051b,iconic2125,2,Sun Jul 12 15:51:59 2015 UTC,Definitely.
hardware,3d051b,naanplussed,1 point,Sun Jul 12 21:15:54 2015 UTC,"All I can see in every Tek Syndicate's videos is Logan's neckbeard, I don't know why but it really bothers my eyes."
hardware,3d051b,e6600,2,Mon Jul 13 13:38:11 2015 UTC,"How are OC benchmarks more relevant than what the manufacturer chose as default? Unless you get multiple cards from various sources to benchmark with, it's just painting a single sample as representative of the whole. Which is ludicrous. Having an OC chapter in a review works just fine, we can simply extrapolate expectations from that."
hardware,3d051b,AndreyATGB,1 point,Mon Jul 13 16:23:07 2015 UTC,"Except with maxwell, it's more rare to get one that doesn't go to 1500 mhz than it is to get one that won't do 1400 mhz."
hardware,3d051b,kurosaki1990,0,Mon Jul 13 21:30:58 2015 UTC,"TekSyndicate has been growing on me lately. They've really matured and their reviews are becoming lower quantity but are of a MUCH higher objective quality.   The other major YouTube hardware reviewer selling 'a video a day', on the other hand..."
hardware,3d051b,frenchpan,1 point,Tue Jul 14 20:45:41 2015 UTC,Tinus Lech Tips?
hardware,3d051b,screwyou00,1 point,Mon Jul 13 08:15:12 2015 UTC,"You bet. The 1 video a day thing really hurts the time and attention drawn to each one.  Some reviews were so brief that you cannot derive any original perspective compared to the thousands of reviewers out there. Some were really cramped with info in the script that it becomes hard for me to catch everything on the first watch. Others are entertaining but maybe starting to lack in practicality. Not to mention the Nvidia bias brought up here every so often.  I've seen someone here recommend Jayz and stated 'LTT is just entertaining the lowest common denominator of the tech crowd'. Unfortunately, I wholeheartedly agree."
hardware,3d051b,frenchpan,2,Mon Jul 13 19:56:30 2015 UTC,"When I first started looking into building a PC last year I watched a lot of Linus's videos. Then I started watching Jay's videos and I can hardly stand to watch Linus any more. His reviews are still okay, but his procedural videos feel so basic."
hardware,3d051b,Funnnny,0,Mon Jul 13 03:32:55 2015 UTC,All I know is he thrashes a major brand mech keyboard if he doesn't like it
hardware,3d051b,0Ninth9Night0,19,Mon Jul 13 06:10:27 2015 UTC,"i could see the fury x being a great card a year from now with driver optimizations and voltage control, but right now i think they fall a bit short for their price"
hardware,3d051b,AndreyATGB,16,Mon Jul 13 08:04:07 2015 UTC,"A year is way too much, Nvidia will have HBM2 cards out in a years time and so will AMD most likely. 15.7 already is a lot better."
hardware,3d051b,TehRoot,2,Tue Jul 14 14:55:44 2015 UTC,I don't have much knowledge on this matter but isn't this HBM technology invented by AMD or i'm wrong?
hardware,3d051b,JustaPassanger,5,Mon Jul 13 13:28:37 2015 UTC,Hynix and AMD developed it.  https://en.wikipedia.org/wiki/High_Bandwidth_Memory
hardware,3d051b,e6600,3,Sun Jul 12 12:24:30 2015 UTC,Does this mean NVIDIA has to pay AMD and Hynix to use HBM?
hardware,3d051b,dylan522p,6,Sun Jul 12 18:26:16 2015 UTC,"I could be wrong, but I don't believe so. It was made an industry standard by JEDEC, an association of a bunch of companies.   standards that permit any and all interested companies to freely manufacture in compliance with adopted standards   From the JEDEC wiki page.   https://en.wikipedia.org/wiki/JEDEC#Industry_standards"
hardware,3d051b,Pinecone,2,Sun Jul 12 22:24:11 2015 UTC,"JEDEC doesn't include patents, but requires company to disclose all those patents so anyone interested in the technology knows what comes next.  So yes, you may have to pay money."
hardware,3d051b,Seclorum,1 point,Sun Jul 12 22:43:15 2015 UTC,Seems like JEDEC can make or break a company.
hardware,3d051b,CarVac,1 point,Mon Jul 13 00:02:27 2015 UTC,"To add to /u/frenchpan's reply, AMD has an exclusivity deal on HBM1 which is why Nvidia hasn't released a card of their own."
hardware,3d051b,ZapaSempai,2,Mon Jul 13 00:17:48 2015 UTC,voltage control should be here by the end of July
hardware,3d051b,CarVac,-1,Mon Jul 13 09:27:19 2015 UTC,And where does it exactly fall short? I don't follow you guys anymore. First you bash that AMD is cheap but is inefficient and not as fast. Well they released two very good cards that are pretty much as efficient as Nvidia and the Fury even stomps the 980 but you're still not satisfied. I really don't get it and I don't think there's any rational explanation that you can bring to make me understand so don't even bother.
hardware,3d051b,Seclorum,4,Mon Jul 13 08:01:25 2015 UTC,they fall short for price:performance.  http://www.techpowerup.com/reviews/ASUS/R9_Fury_Strix/33.html
hardware,3d051b,CarVac,5,Sun Jul 12 22:53:48 2015 UTC,Its less efficient even with a vastly more efficient memory subsystems. It performs slightly worse than its competitor and doesn't overclock nearly as well. Fury doesn't stomp the 980 when you look at OC performance and not ocing is flat out dumb imo.
hardware,3d051b,merolis,8,Mon Jul 13 00:03:39 2015 UTC,Shouldnt the 980 be compared to a 390x?
hardware,3d051b,cvance10,6,Sun Jul 12 21:33:48 2015 UTC,980 comes in with stock models smack dab between the  390x and the Fury.   So taking into account they got an expensive aftermarket 980 it's more comparable in price and apparently performance to the Fury.   Fury - 550  980 - 500  390x - 450
hardware,3d051b,merolis,3,Mon Jul 13 03:11:39 2015 UTC,"I'm curious about a noise comparison for these two cards in particular, both overclocked and not."
hardware,3d051b,merolis,1 point,Sun Jul 12 22:16:24 2015 UTC,http://images.anandtech.com/graphs/graph9421/75699.png This is a image that was linked from /r/AdvancedMicroDevices/comments/3ctgdt/oooh_the_silence/
hardware,3d051b,Seclorum,4,Sun Jul 12 16:21:15 2015 UTC,That's stock coolers for all other cards.
hardware,3d051b,Seclorum,4,Sun Jul 12 18:00:16 2015 UTC,And the Sapphire Fury is stated to be the quietest card they have ever tested. Ever tested.
hardware,3d051b,0Ninth9Night0,3,Sun Jul 12 15:50:39 2015 UTC,And I want to know by how much.   Unless they mean quieter than e.g. the 750ti which would be simply crazy.
hardware,3d051b,JustaPassanger,5,Sun Jul 12 16:13:06 2015 UTC,"Its 2 dBa louder than cpu watercooling setup that anandtech uses. To compare with other cards the sapphire card is 8 dBa quieter than other Fury cards, and is quieter in load than a 980 ti idles. The sapphire solution is a unicorn because its practically silent.  Edit: The article about the fury cards tested. http://www.anandtech.com/show/9421/the-amd-radeon-r9-fury-review-feat-sapphire-asus/17"
hardware,3d051b,ffca,-6,Sun Jul 12 16:32:05 2015 UTC,"Their CPU watercooling loop is 35dB loud?  That is really bad, a good loop should be below 20dB"
hardware,3d051b,sackrace,5,Sun Jul 12 18:01:09 2015 UTC,Ambient sound levels are higher than 20dB.  35dB would be barely noticeable with your ear close to the PC.
hardware,3d051b,JustaPassanger,3,Sun Jul 12 18:53:17 2015 UTC,"You must have a god tier loop if you can hit 20 db, I cant find a single review on any site showing lower than 26 dBa, with most at 33-38 dBa. I'd also be amazed if your computer is even in an place where you have 20 dBa."
hardware,3d051b,ffca,0,Sun Jul 12 19:01:04 2015 UTC,Here is a 18db system:  http://www.silentpcreview.com/article1437-page9.html  SPCR can measure down to 11db.  A 20year old man can notice sound levels down to 3dBA(depending on the frequency) if no other sound is present.
hardware,3d051b,JustaPassanger,3,Sun Jul 12 19:35:56 2015 UTC,Because tech review sites and general consumers buy 400 dollar open water loops. But sure if you have this aircooled card in a recording studio you might hear it unless it is idle because zerofan.
hardware,3d051b,ffca,2,Sun Jul 12 20:20:56 2015 UTC,I believe it was one of those AIO cooler systems. So you cant exactly expect ultra mega silence.   Nor were they testing everything inside of a sound isolated cell. They had background noise factoring in as well.
hardware,3d051b,bobthetrucker,6,Sun Jul 12 20:32:25 2015 UTC,"It is strange they show one chart and then talk about it with the hyperbolic arguement of it being the 'quietest ever tested' and yet they only have a couple stock values on the chart.   When you go and make a point like that, you kind of want to justify it with some actual data..."
hardware,3d051b,RoboTrigger,1 point,Sun Jul 12 20:49:32 2015 UTC,"I think I was reading (on anandtech as well) that giving a decibel test on a product is hard, because of the interfering noise of all the other parts in the computer. The ambient noise, and so on. Also maybe comparing water vs air is hard because they exhaust at different locations on the machine typically, so comparing between cooling method groups is inherently harder."
hardware,3d051b,ShadowthecatXD,1 point,Sun Jul 12 21:01:23 2015 UTC,"Doesn't matter, the Sapphire Fury is semi-passive and very silent under load."
hardware,3d051b,okieboat,4,Sun Jul 12 20:19:47 2015 UTC,"If the Fury was the same price, it would be a clear win. Price/performance seems to be a draw as it stands. 980 is only going to drop in price as well."
hardware,3d051b,bobthetrucker,1 point,Sun Jul 12 19:16:40 2015 UTC,The Fury surely has more room to drop in price given it's currently as high as it's ever going to be.
hardware,3d051b,okieboat,-11,Mon Jul 13 08:05:29 2015 UTC,"Wait, now you're arguing about price-performance? The 980 had a lot of time to get optimized drivers and sales, why don't you give the Fury the same chance?  Where was your price-performance argument when Nvidia lied about the 970 VRAM and people still were buying more of it than the R9 290, which destroys the 970 at price-performance."
hardware,3d1ryt,namae_nanka,6,Sun Jul 12 20:42:41 2015 UTC,"A few important tidbits in this review.   We've also included some data we have from a factory-overclocked GTX 980 (MSI's Gaming 4G model). We're missing Witcher benches here, but that doesn't really matter - the fact is that the 980 just doesn't scale well at 4K.   and,   There have been theories that AMD's slower DX11 driver may be to blame for the poor showings at lower resolutions. However, if that were the case, we would expect Fury and Fury X to perform at the same level at 1080p and 1440p as CPU would become the bottleneck rather than the GPU hardware - this does not happen: the top-tier card is still faster and once again, overclocking brings us pretty close to overall parity. Curiously, we actually find that Far Cry 4 is faster on Fury than it was when we tested it on Fury X - substantially so, in fact. We must assume a fault of some kind in our test, or an optimisation in the 15.7 driver vs the 15.15 we had to test with the Fury X.   So, the front-end seems to be the bigger culprit than CPU bottleneck and apparently the 15.7 do have improvement for FC4 as that hardwarezone guy saw, but not where it matters the most for Fury i.e. at 4k.  The best case scenario for Fury is VSR at lower resolutions considering how it doesn't seem to result in any appreciable performance hit unlike nvidia's DSR. The worst case being gaming at 120Hz on 1080p."
hardware,3d1ryt,Nimelrian,8,Sun Jul 12 20:45:56 2015 UTC,"We must assume a fault of some kind in our test, or an optimisation in the 15.7 driver vs the 15.15 we had to test with the Fury X.   Do they even read the driver changelogs?  From the 15.7 changelog   Performance Optimizations versus AMD Catalyst™ Omega Single GPU performance on Windows 8.1 based system:     Up to 7% in Far Cry® 4 on AMD Radeon™ R7 and AMD Radeon™ R9 200 series and up*"
hardware,3d1ryt,TaintedSquirrel,0,Mon Jul 13 05:59:19 2015 UTC,"There have been theories that AMD's slower DX11 driver may be to blame for the poor showings at lower resolutions.    Yeah DigitalFoundry hasn't stopped talking about this for the last 6 months.  They're very obsessive.  The ""theories"" they mention here are their own theories.  At least 3 separate articles, and now this one, all covering AMD's poor DX11 CPU optimization.  Pretty crazy.  At this point I feel like it's okay for me to say:  We get it.  You can stop saying it now.  Anyway there were apparently some CPU optimizations in the 15.7 driver which may explain their results."
hardware,3d1ryt,TruckChuck,2,Sun Jul 12 21:25:19 2015 UTC,Amd just seems to be excuses lately
hardware,3d076a,XorFish,5,Sun Jul 12 11:22:13 2015 UTC,"Saw this post here and /u/CarVac complained that the new Scythe Ninja was not included, so here is a good review of that exellent cooler."
hardware,3d076a,quadraphonic,5,Sun Jul 12 11:26:19 2015 UTC,"That's some pretty great cooling at a very low price. I'm not a big fan of the design, though it certainly seems to be doing the job."
hardware,3d076a,CarVac,3,Sun Jul 12 14:24:36 2015 UTC,"Personally, I like the new design better than the older Ninjas that had the slots at the corners."
hardware,3d076a,BannedWasTaken,2,Sun Jul 12 15:12:35 2015 UTC,I still have one of these running a lower spec PC. Works great fan less for it.
hardware,3d076a,stabsthedrama,-8,Sun Jul 12 19:24:41 2015 UTC,"The difference in temps between this and a CM 212 evo would be nominal, particularly if running the evo in push/pull.  Those are like $35, usually on sale with MIRs for like $25ish - and are about half the size.  This Scythe is pointlessly frikkin gigantic.  While that may have been hip 10 years ago - now you can spend ~$10 more and get a Corsair H60, which would outperform it without question and is very low profile."
hardware,3d076a,stabsthedrama,7,Sun Jul 12 21:44:15 2015 UTC,"Yeah, but water cooling will never, ever be as quiet as high end air cooling. If you want quiet, air and water performs about the same.   The H60 is extremely loud."
hardware,3d076a,stabsthedrama,-7,Sun Jul 12 22:15:19 2015 UTC,"That's just not true....  I have been running my h50 for about 5 years now, and a few friends run h60s (basically the same thing).  They barely make any noise.  Pair them with a push/pull setup of low db/high cfm fans like corsair or cougar and they're as silent as any air cooling setup.  Idk what you're referring to. The pumps don't (shouldn't) make any noticeable noise whatsoever, so all you hear is the fans anyway, much like air cooling..."
hardware,3d076a,CarVac,3,Sun Jul 12 22:48:25 2015 UTC,"They barely make any noise.   That's completely subjective. Look at the quietest AiO watercoolers and then look at the quietest air coolers. It's not even close. The water coolers are almost 5x as loud.  http://www.anandtech.com/show/9415/top-tier-cpu-air-coolers-9way-roundup-review/13 http://www.anandtech.com/show/7738/closed-loop-aio-liquid-coolers/10  Once you ramp the speeds up, the delta is even worse."
hardware,3d076a,stabsthedrama,-2,Sun Jul 12 23:29:09 2015 UTC,"Remember the stock fans that they come with (h75 for example) aren't the quietest fans you can use.... and at around 30-40db thats not even crazy loud anyway.    Your gpu fans will be kicking up louder than that by the time your water cooler will come even close to being as loud as your gpu fans.  I promise you that.  Every water cooling setup I've had, and every one I've seen in person - the LAST thing you would notice is the noise of it.  By the time you're under full load in a game (bf4, whatever) - your gpu fans are going to be ramping up to easily twice as loud as the loudest water cooling.  This is a silly argument.  Air coolers are quiet, water coolers are quiet - with the right fan placement and cases - the last thing you need to worry about with EITHER setup is the noise levels.  If you're that much of a db level purist - you're going to be running your board on ""silent"" mode anyway - in which case the low rpms and cfms would be best suited for WATER COOLING, not air cooling...."
hardware,3d076a,CarVac,2,Mon Jul 13 00:29:04 2015 UTC,"This is just a guess based on my own situation, but I bet the people who care about absolute silence don't care about it while gaming, since gaming comes with sound.   This is things like photo or video editing, or recording, or even (especially?) movie watching. In these situations, a closed loop cooler pump will be making more noise than any of the fans.  A big cooler like this won't need to speed up the fans at all. I run my Kotetsu between 450 rpm at idle (5 C temperature difference) to 550 rpm at full prime95 load (<40 C temperature difference with a non Devils Canyon Haswell i5) and it is completely inaudible (probably 12db on the SPCR ratings scale). The Ninja tested here would be even better under heavy loads.   No closed loop cooler yet can even get that quiet, and I don't need any more cooling power, since no real load (not prime95) ever gets my temps to even 30 C above ambient.   Since it's not a gaming load, the graphics cards can be running passively with applicable 3rd party designs, and a good case design (such as a rotated motherboard) will keep the air moving with minimal or no case fan effort (or the CPU cooler will take care of all the air moving needs).   There would be no other sound to mask the pump noise."
hardware,3d076a,CarVac,0,Mon Jul 13 02:49:10 2015 UTC,"Then revert to my original point.  The 212 evo is cheaper, smaller, lighter, and will have identical performance outside of gaming with heavy loads and nominal differences even with gaming."
hardware,3d076a,stabsthedrama,3,Mon Jul 13 02:58:01 2015 UTC,But it's louder.
hardware,3d076a,CarVac,2,Mon Jul 13 03:07:21 2015 UTC,I could mount some noctua industrial fans on my nh-d15 and it would blow any aio watercooler away. But it would also be pretty loud. The most important metruc for a cooler is performance/dBA.  And air coolers win their until you get to the high end ones that are 150$+
hardware,3d076a,twatsmaketwitts,3,Mon Jul 13 07:36:38 2015 UTC,"Just so you know, the Scythe fans on their new coolers are so quiet at low speed that I can only hear the one on my Kotetsu with my ear literally against my case. With all the other fans off (passive graphics card) and no hard disks, I can't tell if my computer is on or not. This is in a really quiet basement environment.   Quiet is subjective, but silent is another thing entirely."
hardware,3d076a,Klorel,0,Mon Jul 13 02:13:17 2015 UTC,"5 years and going strong is enough evidence for me.  Keeps my 8320 under 62 at full load and about 26-33c idle (cool n quiet), and kept my i5 750 at about the same for 5 years at 4.0ghz.  Sure, results may vary - but I absolutely love these things.  Nowadays though ya I'd get an h60 instead.  I think the 75s are just with 2 fans, which aren't even the fans I'd choose anyway."
hardware,3d076a,Seclorum,3,Mon Jul 13 00:40:05 2015 UTC,"I wonder if blocking off the sides would improve performance with one fan, or would it make more noise?   Also, I wonder if push pull is better, or pushing from two adjacent sides (with 120mm fans) would be better."
hardware,3d076a,luddist,1 point,Mon Jul 13 00:42:48 2015 UTC,I still have the Ninja from my old build in 2006/2007 and I still can't get over how big it is. Amazing cooling for the price back then though.
hardware,3d076a,CarVac,1 point,Sun Jul 12 15:15:04 2015 UTC,"just an old computer, or are there ways to attach the old coolers onto new 1150 boards?"
hardware,3d076a,Lonxu,1 point,Sun Jul 12 15:02:42 2015 UTC,God damn I remember my old Ninja I had mounted to an Athalon 64 chip. Good cooler.
hardware,3d076a,ikjadoon,1 point,Sun Jul 12 22:10:30 2015 UTC,"Holy shit Scythe is back with a good contender! My original Ninja is still cooling a Core 2 Duo semi-passively and the whole computer is dead silent. I'd like to see the Thermalright TRUE 140 compared with it though, since it's in the same price bracket and other reviews have shown it to have good performance."
hardware,3d076a,CarVac,1 point,Sun Jul 12 18:07:24 2015 UTC,"Many of their newer ones are quite good. I have a Kotetsu and it keeps my processor very cool while being so quiet that I can't hear it unless I turn off every other fan in my case and touch my ear to the case. Essentially, absolute dead silence.   Too bad I have hard disks in my case now... I can't achieve that level of ""can't tell if it's on or not"" silence anymore."
hardware,3d076a,ikjadoon,1 point,Sun Jul 12 19:00:48 2015 UTC,"Hehhe, I'm still using my Ninja 2(?) from like 2006. I had to replace the fan and mounting, but it's still doing the job well :d"
hardware,3d076a,ikjadoon,1 point,Sun Jul 12 20:02:28 2015 UTC,"While I love SPCR's reviews, I worry that their use of a real CPU to test has too many variables.   I think that's why FrostyTech and Anandtech switched to custom heat plates to remove inconsistencies."
hardware,3d076a,ikjadoon,3,Mon Jul 13 00:16:27 2015 UTC,"Maybe they need to retest some older ones to check the variance, such as whether the CPU has changed over time.   On the other hand, do the heat plates have the same thermal distributions as real CPUs, what with the very tiny chips compared to the size of the heat spreader?"
hardware,3d076a,luddist,2,Sun Jul 12 15:02:14 2015 UTC,"Agreed on your first point.  Regarding the second point, nope. They don't mention that, so presumably they don't account for that. Seems important, though: depending on how the base is constructed, synthetic vs. real-world might be really different...."
hardware,3cyw22,Fatigue-Error,16,Sun Jul 12 01:07:05 2015 UTC,"No Scythe heatsinks like the new Ninja? Their latest ones are awesome performing, lightweight, and inexpensive.  Not equal decibel testing?"
hardware,3cyw22,CarVac,4,Sun Jul 12 04:05:18 2015 UTC,"Yea, I wouldn't call this a good review"
hardware,3cyw22,XorFish,26,Sun Jul 12 08:57:05 2015 UTC,"no performance per noise chart, come on, that is the most important metric for cooling.  Also you can't just reduce the noise to one number and 35db is hardly inaudiable.  Guess there is no competition for www.silentpcreview.com when it comes to cpu coolers reviews."
hardware,3cyw22,XorFish,2,Sun Jul 12 08:56:11 2015 UTC,Since Anand himself left they haven't been anywhere near as good.
hardware,3cyw22,dexter311,1 point,Sun Jul 12 09:13:43 2015 UTC,"Any alternative sites you can recommend? Used to really love that site for stuff like this, now... not quite so much."
hardware,3cyw22,404fucksnotavailable,1 point,Sun Jul 12 19:11:02 2015 UTC,"Silent PC Review does pretty well for silence-oriented stuff, but they can lag behind the release dates sometimes. I just read a ton of different reviews (like 5-10 at least) to gauge how things are nowadays. Toms Hardware and Tweaktown come up pretty often, but make sure you compare them somewhere else!"
hardware,3cyw22,dexter311,1 point,Sun Jul 12 19:28:25 2015 UTC,They've got some great people still there. Andrei F. Is pretty new but so far his dives into mobile uarch are great.
hardware,3cyw22,dylan522p,7,Sun Jul 12 19:41:42 2015 UTC,Throw in a water cooler or two just to see how they compare on these tests.
hardware,3cyw22,BlayneTX,4,Mon Jul 13 09:44:22 2015 UTC,"I have the Be Quiet!  Dark Rock 3 in my system and I orcuahsed it specifically for its extremely quiet operation. Even when my CPU is being stressed I cannot hear it and the loudest fans in my system belong to the R9 390X which cranks up to 3000 RPM during benchmarks and 900 RPM during normal CS:GO, TF2 and LoL gaming.    Overall I am very pleased as the performance is better than my Corsair H50 and all while being silent."
hardware,3cyw22,FueledByBacon,2,Sun Jul 12 03:04:20 2015 UTC,Is this one customizable if you have profile ram etc? Like the Noctua is.
hardware,3cyw22,chronaden,2,Sun Jul 12 17:25:20 2015 UTC,"I have it too, and yes you can push the fan up to clear tall memory."
hardware,3cyw22,dexter311,1 point,Sun Jul 12 19:01:08 2015 UTC,"The one in this review is the pro version however which is larger, the one I have is smaller and it worked with RAM but my Corsair Dominator wouldn't fit even if I had the fan up due to the heatsink sitting over the DIMM slots on my gigabyte board.  Once I upgraded my motherboard and RAM it wasn't a problem though."
hardware,3cyw22,FueledByBacon,1 point,Sun Jul 12 19:17:04 2015 UTC,"Yeah that was exactly the same thing with mine and my Corsair Vengeance (but let's face it, Vengeance heatsinks are kinda dumb and I should never have got them). Buy it only blocked a single DIMM slot on my board so it was all good. Mine is the non-pro too. I've since changed to Vengeance LP."
hardware,3cyw22,dexter311,1 point,Mon Jul 13 02:32:21 2015 UTC,I ripped the heatsinks off my Corsair Vengeance kit and placed them back under the heatsink. I was super careful about it though which allowed me not to damage the memory modules by ripping them out.
hardware,3cyw22,FueledByBacon,15,Mon Jul 13 02:36:42 2015 UTC,I'd like to see a Hyper 212 and Freezer Pro 7 in that line up to compare with. Mainly because they are a ton cheaper.
hardware,3cyw22,clickwir,10,Mon Jul 13 02:43:32 2015 UTC,"It is called ""top tier"". But it would be nice to have a reference."
hardware,3cyw22,CarVac,13,Sun Jul 12 02:17:32 2015 UTC,"Well then let's show it. Show them dominating the Hyper 212.   Let's see them perform better, not just cost more."
hardware,3cyw22,clickwir,9,Sun Jul 12 04:34:00 2015 UTC,"Yep. I don't doubt that these coolers are better than a 212, but I want to know if they're better to a degree that justifies spending more than $30."
hardware,3cyw22,internetosaurus,3,Sun Jul 12 05:56:28 2015 UTC,"Well, if you're rocking a 6-core CPU @ 4.1GHz and pushing it to its limits, these ""Top Tier"" coolers can help.   http://tpucdn.com/reviews/Noctua/NH-U14S/images/CPU_OC_max.gif  These are maximum temps, but compare the NH-U14S's cool 62C to the Hyper 212's scorching 85C. The averages were within 3C: so, they're close overall. However, the NH-U14S is much quieter, too:  http://tpucdn.com/reviews/Noctua/NH-U14S/images/fan_noise100.gif  Full review: http://www.techpowerup.com/reviews/Noctua/NH-U14S/6.html"
hardware,3cyw22,ikjadoon,1 point,Sun Jul 12 06:17:24 2015 UTC,"I'm currently pushing the NH-D15 to it's limits(room temperature is 26°C, case is Define R4):  http://imgur.com/RyZ5bmh   It is clearly audiable, but I it is not that bad. If I step down to 4Ghz. It is very silent."
hardware,3cyw22,XorFish,1 point,Sun Jul 12 12:47:26 2015 UTC,"And no Cryorig H5, either?"
hardware,3cyw22,Pufflekun,3,Sun Jul 12 14:34:32 2015 UTC,"Grabbed a Noctua NH-D15 at launch, and not disappointed with the sound/performance. Nice to see it's still one of the best."
hardware,3cyw22,viscero,3,Sun Jul 12 12:31:10 2015 UTC,"Of course it's still one of the best.  Heck, replace the default fans with two Noctua Industrial PPC 3000RPM fans, and it'll beat the socks off of every cooler in that lineup."
hardware,3cyw22,Valridagan,3,Sun Jul 12 05:52:09 2015 UTC,©Noctua Inc.
hardware,3cyw22,n3x_,7,Sun Jul 12 06:43:45 2015 UTC,"Noctua spends tons of R&D on their stuff.  I suspect it's partially because they're Austrian, and the Austrian top tax rate is high enough that it's more beneficial for the company to take money that would be profit and invest it back into the company, especially as R&D budget, because that makes them even more money.  So yeah, Noctua does tend to consistently be one of the very best at this sort of thing."
hardware,3cyw22,Valridagan,3,Sun Jul 12 11:11:38 2015 UTC,Also their customer support is really great. Made a copy of my reciept of the X58 motherboard I got in 2010 and they sent me a kit for the 1366 socket for free a few days later.
hardware,3cyw22,XorFish,1 point,Sun Jul 12 13:18:43 2015 UTC,"Sweet!  Yeah, I hear that they do that.  Like, it's a feature of their products.  Full lifetime support for sockets and things."
hardware,3cyw22,Valridagan,1 point,Sun Jul 12 13:58:37 2015 UTC,Those fan colors though... Thank goodness for redux.
hardware,3cyw22,quadraphonic,0,Sun Jul 12 15:26:57 2015 UTC,Redux are grey.  Do you mean the Industrial line?  Grey isn't a very flattering color.
hardware,3cyw22,Valridagan,1 point,Sun Jul 12 14:04:27 2015 UTC,I like the grey better than brown and tan!
hardware,3cyw22,quadraphonic,3,Sun Jul 12 15:23:40 2015 UTC,Really?  I don't.  I think that the cream and rust colors are rather pretty.  But to each their own~
hardware,3cyw22,Valridagan,1 point,Sun Jul 12 16:08:08 2015 UTC,"Exactly! Just individual preference. Most PC parts are black, I like complimentary colors vs stand-out.   No question about their quality though."
hardware,3cyw22,quadraphonic,1 point,Sun Jul 12 16:18:53 2015 UTC,Thank you!
hardware,3cyw22,Valridagan,1 point,Sun Jul 12 20:05:30 2015 UTC,"Yeah, I had held off on buying Noctua for a while due to the premium pricing and the salmon-pink coloring. After seeing the NH-D14 as the reigning air cooler for year after year, I figured the updated version (D15) would last me a long while.  That, and my H100i had started leaking."
hardware,3cyw22,viscero,1 point,Mon Jul 13 00:40:23 2015 UTC,"Just throwing this out there for anyone interested/requiring a low profile air cooler: Raijintek Pallas, beats everything and is quiet to boot."
hardware,3cyw22,st31r,-5,Sun Jul 12 16:59:23 2015 UTC,"And all of these coolers are crap if you have medium to large hands. ""Bro, did you punch glass? No, just installed my heatsink made of razor blades."" CLC is the way to go."
hardware,3cyw22,veyron3003,1 point,Sun Jul 12 16:42:39 2015 UTC,Worth it.
hardware,3cyw22,viscero,1 point,Sun Jul 12 20:01:36 2015 UTC,"Yep, getting for the most part a worse and louder cooler because ""water cooled"" is not worth it at all to me.   Then again not really surprised the most popular ones are the corsair. :/"
hardware,3d124l,Fatigue-Error,13,Sun Jul 12 17:11:51 2015 UTC,"They opened with Project Cars.  Bold move, TechReport."
hardware,3d124l,TaintedSquirrel,8,Sun Jul 12 18:01:55 2015 UTC,"It's more than that.   TechReport is hands down the worst review for Fury. HardOCP at least test without nvidia settings to give a picture, TR are starting off with Project Cars.  If Fury stutters more than 980 that is a legitimate point, however their average numbers seem rather different from other reviews as well.  Fury at par or only a fps faster in games where it demolishing 980 in other reviews. So I go looking at the test notes, they are using OCed models of nvidia cards which behooves them to label them as such in the graphs where it looks as if the vanilla versions are being used.    https://np.reddit.com/r/AdvancedMicroDevices/comments/3cxdgg/regarding_the_techreport_review_of_fury_and_4k/"
hardware,3d124l,namae_nanka,-1,Sun Jul 12 19:02:43 2015 UTC,I stopped reading after the meth joke.
hardware,3d1rdc,Fatigue-Error,5,Sun Jul 12 20:37:49 2015 UTC,"I really hate how Im playing a game and someone messages me, so i switch to reply to them and then switch back not even 10 seconds later and the game has to restart."
hardware,3d1rdc,poematik,5,Mon Jul 13 22:35:56 2015 UTC,"Disappointing that the Air doesn't support multitasking, but understandable given its half RAM count.  I just want iOS 9 to not be the buggy, unoptimised mess that is iOS 8. Internet browsing has never been so inconsistent."
hardware,3d1rdc,BrotherSwaggsly,1 point,Mon Jul 13 09:59:34 2015 UTC,"This is nice. Not the life changing feature, but nice for chatting ..."
hardware,3d1rdc,Andrej_ID,1 point,Mon Jul 13 05:50:11 2015 UTC,"Can confirm, have an iPad air 2 with iOS 9 and have so far only used the multitasking for texting while doing other stuff. The PiP video looks cool too but I have yet to use it."
hardware,3cuwu8,DeeJayDelicious,126,Fri Jul 10 23:48:50 2015 UTC,"Pretty much expected.   Over half the die is dedicated to the IGPU despite shrinking the node.   They just invested all that saving into stuffing a bigger IGPU onboard.  But, Skylake platform and the new 100 series board are more about increased connectivity.   God it makes me dream that Zen will shake things up."
hardware,3cuwu8,Seclorum,19,Sat Jul 11 00:04:32 2015 UTC,"Hopefully this gives time for AMD to catch up. I'm an Intel guy, but there needs to be competition."
hardware,3cuwu8,kht120,16,Sat Jul 11 05:39:42 2015 UTC,Everyone keeps saying this but AMD isn't going survive on wishes.
hardware,3cuwu8,Abestar,6,Sat Jul 11 16:26:15 2015 UTC,"Good thing Zen will have IPC equal to Haswell, then. (And Skylake, if OP is correct). Since Zen CPUs will come in 4 & 8 core versions, all of them with hyperthreading, if they price them right we might finally have some real competition in the CPU market."
hardware,3cuwu8,bizude,2,Sat Jul 11 19:10:22 2015 UTC,So the only Zen info I've seen personally was here; has there been any additional confirmed details or specs?
hardware,3cuwu8,justafeather,3,Sun Jul 12 00:25:51 2015 UTC,"I was just saying I want people to actually buy in and support that competition, rather than just buying something else from Intel and giving AMD an e-pat on the head for trying."
hardware,3cuwu8,Abestar,6,Sat Jul 11 19:31:17 2015 UTC,That's not how competition works though. Supporting someone who is doing poorly despite them doing poorly is in itself anti-competition. If AMD released a cpu that wasn't underwhelming that is what would fix their money woes. And that's what their cpus have been. Underwhelming.
hardware,3cuwu8,LRed,12,Sat Jul 11 20:22:31 2015 UTC,"If AMD released a cpu that wasn't underwhelming that is what would fix their money woes.    As long as Intel doesn't bribe companies NOT to use AMD, like they did last time AMD was producing superior CPUs."
hardware,3cuwu8,bizude,8,Sat Jul 11 20:44:41 2015 UTC,And people still bought shitty Pentium 4's. Go figure.
hardware,3cuwu8,jagilbertvt,2,Sat Jul 11 20:25:51 2015 UTC,Asking the market to reward losers for trying hard generally isn't a viable strategy.
hardware,3cuwu8,TexasJefferson,1 point,Sat Jul 11 20:22:43 2015 UTC,I was about to say this.  I REALLY hope this gives AMD time to survive and catch up (surpass) intel again.
hardware,3cuwu8,Potss,1 point,Sat Jul 11 19:54:55 2015 UTC,This... And the IBM/GF 7nm node might equalize the process gap.
hardware,3cuwu8,krista_,7,Sat Jul 11 20:08:33 2015 UTC,Can't the new connections be brought to old boards by PCI?
hardware,3cuwu8,lead12destroy,19,Sat Jul 11 01:16:04 2015 UTC,"Most, yes, but it's obviously not as elegant a solution."
hardware,3cuwu8,Exist50,33,Sat Jul 11 01:31:22 2015 UTC,"Desktop Haswell has a total of 20 PCIe lanes coming off the CPU. 16 of them are PCIe 3.0 and are almost always routed only to slots on the motherboard (potentially through a PLX switch). The remaining 4 lanes are PCIe 2.0 and feed the chipset and everything connected through it: all the PCIe x1 slots, integrated Ethernet or SATA or SATA Express or Thunderbolt or USB 3.x. Yes, you can update the components on the motherboard to support newer connections, but those 4xPCIe 2.0 lanes are still the bottleneck.  Skylake includes the long-overdue upgrade of those 4 lanes to PCIe 3.0 speed, roughly doubling the available bandwidth for non-GPU peripherals. This is now sufficient to feed four USB 3.1 or two Thunderbolt 2 ports or one Intel SSD 750."
hardware,3cuwu8,wtallis,15,Sat Jul 11 01:32:33 2015 UTC,"Keep in mind Haswell-E has 40 lanes, with the 5820K at 28 lanes, soldered IHS and no iGPU (good price too)."
hardware,3cuwu8,TheBloodEagleX,10,Sat Jul 11 05:27:45 2015 UTC,"Yeah, but I said desktop Haswell, not re-badged and slightly crippled server Haswell. Moving up to that platform adds well over $100 to the cost of CPU, motherboard, and RAM (and more like $300 if you want all 40 PCIe lanes). That's an awfully expensive workaround for Intel's reluctance to upgrade a few PHYs."
hardware,3cuwu8,wtallis,12,Sat Jul 11 05:44:10 2015 UTC,"An extra $100 is nothing when you're already willing to spent over $1000. Might as well get exactly what you want. The i3, i5 and i7 are all cut from the same die & binned. So they'd put more lanes on the i3 also. I don't think that's cost beneficial to them, so they don't do it.   Technically the Extremes are for desktop too, not servers, even though, as mentioned, they're rebadged, lower binned Xeons. A lot of people use the Haswell-E for their gaming & workstation builds, test benches, or just like having the best consumer chip, nothing server-ish.  Would be overkill for a home NAS anyway.  People who do /r/homelab aren't buying these either. They're just getting actual cheaper Xeons with your classic ugly green pcb mobos & other hardware, shitton of ECC RAM for virtualization; not like they care about aesthetics or paying a premium for overclocking. You can get much cheaper SuperMicro boards and older Xeons for homelabs to get the same function."
hardware,3cuwu8,TheBloodEagleX,1 point,Sat Jul 11 06:23:42 2015 UTC,"So this means that, if you add a PCIe/NVMe (they are equivalent from a bandwidth perspective, right?) SSD then you can't use any of the USB 3.1 or TB2 ports?"
hardware,3cuwu8,Flayum,2,Sat Jul 11 19:05:08 2015 UTC,"NVMe is a software protocol that runs on top of PCIe connections of any speed or lane width.  If you've got enough USB 3.x/Thunderbolt/other I/O routed through the chipset, they won't be able to communicate simultaneously. You can still have several TB2 ports and have devices actively connected through each of them but if you try to read from all of them at once they won't each operate at theoretical speed because the reads have to all squeeze through the same 4 lanes of PCIe from the chipset to the CPU (and RAM)."
hardware,3cuwu8,wtallis,1 point,Sat Jul 11 19:32:07 2015 UTC,Some of them sure.   But Skylake compatible boards also have more PCIe lanes in general. In addition they are beefing up the PLX given lanes to full PCIe gen 3 lanes as opposed to using gen 2 lanes.   Then you have things like connecting and controlling Thunderbolt 3 and having native on board support for USB 3.1 and Type C.   Current boards are rolling with Type c connectors for USB 3.1 but none of the current boards are rolling with the chip nessicary to do thunderbolt 3.
hardware,3cuwu8,Seclorum,27,Sat Jul 11 02:06:25 2015 UTC,"http://www.digitaltrends.com/computing/ready-to-get-out-of-devils-canyon-core-i7-6700k-benchmarks-leak/  It's 4.2 Ghz vs 4.4 Ghz.  Anyone saying Skylake sucks is gonna be waiting until 2018.  Kabylake is Q4 2016 and is just a refresh of Skylake.  Cannonlake is just a die shrink like Broadwell was, hell it might even be skipped just like Broadwell was.  The i7 6700K is gonna be the gold standard of CPU for years to come."
hardware,3cuwu8,Begoru,22,Sat Jul 11 02:30:55 2015 UTC,"Basically Skylake seems like a nominally faster Broadwell which was just a slower Haswell with a huge IGPU slapped onboard.   Broadwell was a step back in performance in everything except IGPU tasks, so Skylake just gains parity again with Haswell refresh but has the big IGPU onboard.   Thank god DX12 will finally start using the IGPU for some tasks in conjunction with Discrete cards. At least if devs decide to use it for anything at all."
hardware,3cuwu8,Seclorum,22,Sat Jul 11 02:46:33 2015 UTC,Skylake and Haswell were designed by two completely different teams.  Skylake was designed by the Haifa team which created Conroe and the much beloved Sandy Bridge.  Haswell was designed by a Intel Hillsboro team who focused on laptops.   https://www.reddit.com/r/hardware/comments/2k9sdc/why_is_skylake_so_hyped/cljsydd
hardware,3cuwu8,Begoru,18,Sat Jul 11 03:14:12 2015 UTC,"Skylake and Haswell were designed by two completely different teams.   See, I just don't get this. Intel's desktop architecture has been iterative since SB. How can they be designed by different teams when they're so similar? There has to be some overlap."
hardware,3cuwu8,58592825866,80,Sat Jul 11 03:32:01 2015 UTC,"I'm part of the Hillsboro design team that worked on Haswell.  What Begoru says is accurate; there's a team in Israel and another in Oregon.  Basically what happens is after every ""tock"" (Nehalem, Sandy Bridge, Haswell, Skylake), the owners of the current tock go to work on the ""tick"" which has very few architectural changes, but derives improvement from the process shrink, which is a huge effort in its own right.  The next tock owner starts planning their set of architectural improvements.  They are also on the new process, but get to reap the learning done by the tick design team.  Things have changed recently for a few reasons I can't talk about, but notice that Kaby Lake is neither tick nor tock, and is designed primarily by the team in Israel."
hardware,3cuwu8,williadc,7,Sat Jul 11 04:18:51 2015 UTC,"I'd ask if you were up for doing an AMA, but recent events make that seem like a bad idea all around"
hardware,3cuwu8,justafeather,33,Sat Jul 11 05:11:13 2015 UTC,"I've been asked a few times to do an AMA, but there's already a better one than I could do from an architect in our group.  If there's something you're curious about, ask away.  I'll answer to the best of my ability."
hardware,3cuwu8,williadc,9,Sat Jul 11 05:18:07 2015 UTC,"Could you shed some light on what your role in the Hillsboro team is? I'm interested, but it also allows me to ask more directed questions.   What do you think about the feasibility of HBM as a L4 (or even L3) cache?   What was the technical reason for replacing the fluxless solder between die and IHS with TIM?   What do you think about this?   A pre-emptive thankyou for any questions you find the time to answer. :)"
hardware,3cuwu8,SeaJayCJ,6,Sat Jul 11 08:55:07 2015 UTC,"I've had multiple roles on the design team, all in back-end design.  Intel has already stated they will be using HBM in Xeon Phi.  There are plans for HBM in other products, but those haven't been announced yet.  It's my understanding that TIM is a cost decision, but I definitely don't have insider information on this.  I'm always surprised to see the voltages and frequencies that these parts actually sell at since we simulate and test at much tougher values.  The answer doesn't surprise me, but I am surprised to see that someone got it. :)"
hardware,3cuwu8,williadc,7,Sat Jul 11 19:46:31 2015 UTC,What will Kaby Lake be like? A refresh like Devil's Cannon? Or another tock?
hardware,3cuwu8,perfectdreaming,5,Sat Jul 11 06:12:36 2015 UTC,I answered this question in another comment:  https://www.reddit.com/r/hardware/comments/3cuwu8/skylake_vs_haswell_benchmarks_leaked_no_real/cszvf5o
hardware,3cuwu8,williadc,8,Sat Jul 11 18:03:40 2015 UTC,"Do you guys ever get frustrated how OEMs use Intel CPUs? The Yoga 3 Pro comes to mind: all that work by you all on power efficiency and then old Haswell full-voltage laptops had longer battery life because Lenovo stuffed a QHD+ screen in a 13"" laptop."
hardware,3cuwu8,ikjadoon,6,Sat Jul 11 06:52:42 2015 UTC,"We were definitely underwhelmed at the numbers we saw when the Yoga 3 Pro came out (very few people at Intel see system benchmarks until they come out in the press).  It felt like that took some of the wind out of the sails of Broadwell which, from an architecture perspective, was much more ambitious than our previous tick, Westmere.  A lot of us were very excited to see how it would do in the market.  No one was prepared for the yield challenges in 14nm, and a number of compromises had to be made to get Broadwell to market."
hardware,3cuwu8,williadc,7,Sat Jul 11 18:02:55 2015 UTC,"If you do not want to answer some of the following, or prefer answering by PM, please do.   Which server processors are the HEDT (the better 2 out of the 3) derived from? Are they really cut down or are the server variants the cut down ones? The AMA you linked to stated that Intel adds hardware for overclocking to the HEDT CPUs, which would make the server ones appear to be cut down. How will the Knight's Landing platform work as a socketed chip (considering the predecessor is a 300W chip with added VRMs ob the PCB)? Will it require a special variant of a motherboard or work with the X99/Z170 (or their successors)? Has Intel upgraded the hardware RNG since the Ivy Bridge fiasco (where it was pseudo-Random at best, pseudo meaning fake)? When the PCH is finally integrated on to the CPU die (whenever that may be), will the PC still require binary blobs to boot, or will using Libreboot be possible? How hard is it to implement the PCI-E specification on to a motherboard in terms of complexity, debugging and die space?  Will the e-DRAM ever be used as anything other than a penultimate eviction cache before being data is moved to DRAM (as it is currently used), but as a cache to store data before it is moved up the hierarchy closer to the ALUs (as would be desirable)? What modern books do your colleagues recommend internally as require reading? It is a common refrain that Intel ""designs"" two CPUs internally and all the commercial models are scaled from them. Which two models are these precisely out if the commercially available ones? Do you happen to know why the cache size on even the Crystalwell or other high end laptop chips are gimped? I understand that Caches are some of the densest structures found on a CPU die? Is that why? It seems strange that the highest the laptop CPUs go to is 6MB, even in the highest end variants. How do the architects and engineers feel about projects such as ""Libreboot"" et al.? Will we ever have a fully libre PC again? Also, is the McAffee group's ""hard work"" being integrated on to the CPU die? Or any other security related CPU logic, except the ""No Execute"" instruction? As a clarification, all the caches are in Mebibyte (MiB) or Megabyte (MB)? It would make sense that it was MiB, but always better to confirm. What are the best sources to understand new Intel CPU development? Microprocessor Report and Semi-Accurate feel so much better than the marketing slides/drivel/spoonfeeding to the gamers and pseudo-intellectuals that goes on at Anandtech et al.?   Also, please answer this one, thank you.  If you do not feel comfortable answering here feel free to PM me. I am a programmer and not the author of any blogspam website."
hardware,3cuwu8,Kaby_ybak,8,Sat Jul 11 11:11:37 2015 UTC,"HEDT is a funny beast.  The Nehalem HEDT was actually the first NHM-architecture CPU release, and it was designed by the client team. Westmere was also designed our team, but came after the 2-core parts.  From that point on, the HEDT parts have been cut-down server parts.  I believe they're all DP, but I can't say that with 100% certainty.  Knight's is designed by a subset of the server team, and we don't have a ton of interaction with that group.  The hardware RNG for Haswell was derived from the Ivybridge solution.  I only know this because I helped the designer who owned it.  There were a number of fixes to improve randomness, but I can't recall the details.  The architect of that feature gave a tech talk and his description of the workings of that feature made me feel confident in the randomness, but I'm not an expert in that field.  I don't know anything about Libreboot, or the boot process really, other than a few interactions with fuse and powergood signals that came into my blocks.  PCI-E controller has been on-die since Ivybridge, and has always been tough.  High-speed IO is a challenge on small processes and PCI-E is very fast and a complex spec.  You seem to know more about how we use e-DRAM than I do. :)  I work in backend, so the standard textbook is CMOS VLSI Design by Weste and Harris.  Digital Integrated Circuits by Rabaey, Chandrakasan, and Nikolic is good as well.  If you're looking at CPU architecture specifically, you can't beat Computer Architecture: A Quantitative Approach by  Patterson and Hennessy, though from your comments, you probably already know that material.  2 reference designs ==> a multitude of products is not quite accurate, but close.  We design in a modular way so that we can ""chop"" multiple parts out of as few supersets as possible, but the number of supersets vary, depending on which architecture you are talking about.  Haswell, for example had three designs: ""Standard"" which I think was a four core, ""Crystalwell"" which was the first e-DRAM part (our team also worked on the e-DRAM die, which was called Liberty Ridge.  If I remember correctly, it was a joint effort with some folks in Fort Collins, CO.), and lastly ULT, which was added very late in the Haswell timeframe, and was co-developed with the Israel team.  I think you are confused.  The e-DRAM die is 128MB.  The thinking behind that decision is that PS3 and X-Box 360 had 128MB of VRAM, so multi-platform games would optimize around that point.  Things have moved on from there, and you can expect future platforms to adapt. Wait, I was confused.  Yes, the answer to your question on why caches are capped around 1.5MB per core is that more cache == more cost to manufacture.  Smart people do lots of studies on existing and future-looking software to see if there's sufficient benefit to growing the caches, and to date haven't found it.  As I said before, I don't know about Libreboot.  Getting to your next question, the security concerns around the boot process have definitely increased, and Intel is leveraging McAfee's expertise here and in other parts of the CPU, so I don't see a fully-libre Intel platform.  That said, we're always looking to please our customers, and if a libre system becomes important to our customers we will find a way to make it happen.  McAfee has contributed many security features to CPU and PCH design.  I don't know the answer to this question.  I have a friend that can answer this question, so I'll check with him.  I'm inclined to agree with your assumption, but with all the ECC-type overhead, it could go either way.  I like Real World Tech.  He doesn't write often, but when he does, it's good, interesting stuff."
hardware,3cuwu8,williadc,5,Sat Jul 11 20:34:49 2015 UTC,"Has Intel upgraded the hardware RNG since the Ivy Bridge fiasco (where it was pseudo-Random at best, pseudo meaning fake)?   Got any source for what you're talking about? The only complaints I recall hearing about RDRAND is that many people are unwilling to trust any RNG from a source like Intel due to possible NSA involvement. I haven't seen any actual evidence of poor RNG performance, and Intel stated that the design uses a hardware entropy source and AES to smooth out any bias that the hardware entropy source may have."
hardware,3cuwu8,wtallis,2,Sat Jul 11 14:30:14 2015 UTC,How will the Knight's Landing platform work as a socketed chip (considering the predecessor is a 300W chip with added VRMs ob the PCB)? Will it require a special variant of a motherboard or work with the X99/Z170 (or their successors)?   I think I can answer this. No way will it use anything currently out. The 6 channels of DDR4 make that automatically clear. Looks like it'll just be a huge die using server cooling. See here: http://cdn.wccftech.com/wp-content/uploads/2015/03/Intel-Knights-Landing-Processor_Adams-Rack.jpg
hardware,3cuwu8,Exist50,2,Sat Jul 11 16:52:25 2015 UTC,"Do you know why Conroe required a VRM 11, making it not drop in compatible with older LGA775 boards?"
hardware,3cuwu8,yuhong,5,Sat Jul 11 18:03:32 2015 UTC,"Conroe was designed in Israel, so I don't know a lot about it."
hardware,3cuwu8,williadc,1 point,Sat Jul 11 18:04:37 2015 UTC,Do you know why there is no new stepping of Haswell or Haswell-E to fix TSX?
hardware,3cuwu8,yuhong,5,Sat Jul 11 18:11:20 2015 UTC,The Haswell design team was almost done with Broadwell by the time the TSX bug was discovered.  It's my understanding that this feature required more than a stepping to solve and we would have had to delay other products to fix TSX on Haswell.
hardware,3cuwu8,williadc,1 point,Sat Jul 11 20:58:43 2015 UTC,Another thing: Why did it took until Broadwell for 8Gbit DDR3 support to appear? Is there really a hardware limit or just in the Memory Reference Code?
hardware,3cuwu8,yuhong,3,Sat Jul 11 21:09:30 2015 UTC,"The memory controller team would not answer this if I asked.  Unfortunately, this will remain a mystery."
hardware,3cuwu8,williadc,2,Sun Jul 12 05:56:19 2015 UTC,recent events?
hardware,3cuwu8,lostheaven,1 point,Sat Jul 11 20:01:53 2015 UTC,The whole Victoria thing; reddit drama.
hardware,3cuwu8,justafeather,3,Sat Jul 11 21:11:22 2015 UTC,So Kaby Lake is not the Devil's Canyon of Skylake?
hardware,3cuwu8,KMKtwo-four,3,Sat Jul 11 14:56:39 2015 UTC,"Devil's Canyon was a stepping for us, a minor set of improvements to improve performance or (more commonly) power efficiency (which turns into performance, since the part can turbo higher).  I'm not sure what the package guys did, but it wasn't much effort for us.  Kaby Lake is its own project, so I expect any obvious deficiencies in Skylake architecture will be resolved.  There are always some of those that show up too late in the design process to solve ""the right way,"" so we hack around it, usually at the cost of performance/efficiency.  I'm guessing Kaby Lake will deal with those and maybe add some features that our customers don't want to wait for Cannonlake to deliver.  I don't know any details though."
hardware,3cuwu8,williadc,2,Sat Jul 11 20:39:02 2015 UTC,"If you are free to answer, can you please clarify what Kaby Lake is? Skylake refresh? Minor architectural tweaks? Something else?"
hardware,3cuwu8,Exist50,6,Sat Jul 11 06:18:07 2015 UTC,"I'm not part of the design team working on Skylake or Kaby Lake, so I can't say for sure what's in store.  My guess is that it will address any architectural weaknesses in Skylake that couldn't be resolved in that product."
hardware,3cuwu8,williadc,1 point,Sat Jul 11 18:03:12 2015 UTC,"Tremendous insight, thank you for sharing!"
hardware,3cuwu8,StellaTerra,1 point,Sat Jul 11 05:19:57 2015 UTC,As a big fan of ISPC (or maybe love/hate relationship would be closer...) what will be the state of skylake's SIMD units?  Are they going to be 16 wide for 32 bit floats? Will skylake have faster gather/scatter performance?
hardware,3cuwu8,__Cyber_Dildonics__,5,Sat Jul 11 16:33:40 2015 UTC,"I don't know the details of the SIMD units in Skylake, but I do know they're adding more AVX instructions."
hardware,3cuwu8,williadc,2,Sat Jul 11 19:26:30 2015 UTC,"AVX-512 also doubles the width of the SIMD registers, meaning some workloads can get about a 2x speedup over AVX & AVX2 (e.g. sound mixing).  Frankly, it's the most interesting upgrade of skylake. However, it also seems like only the Xeon variants will support AVX-512 (and even then, we have to wait for cannonlake for the really interesting stuff)."
hardware,3cuwu8,floopgum,10,Sat Jul 11 20:27:09 2015 UTC,"There really isn't.  From what I can gather there is almost zero communication between the two teams.   The Skylake team is removing the voltage regulator that was on the Haswell CPU, and the Hillsboro team is actually putting it back after Cannonlake. Ridiculous, but as long as you buy Haifa, you're good."
hardware,3cuwu8,Begoru,8,Sat Jul 11 03:34:19 2015 UTC,"Was there anything wrong with Haswell?  Devil's Canyon at least was pretty darn decent, AFAIK.  Especially the G3258."
hardware,3cuwu8,Valridagan,14,Sat Jul 11 03:43:31 2015 UTC,"Haswell's most important software-visible feature was broken, and the integrated voltage regulator was strictly harmful to its desktop performance."
hardware,3cuwu8,wtallis,5,Sat Jul 11 04:07:53 2015 UTC,Can you explain which feature that is?
hardware,3cuwu8,Valridagan,20,Sat Jul 11 04:21:59 2015 UTC,"TSX: it was supposed to bring hardware transactional memory to mainstream (ie. not an IBM mainframe) hardware, partially alleviating one of the biggest performance challenges of multithreaded programming. It allows programmers to play it safe and include a lot more locks than would otherwise be possible with acceptable performance, because the processor can just ignore the locks that definitely aren't necessary and can roll things back if it turns out to have skipped a lock that was necessary. You get code that's a bit simpler, a bit less buggy, and it still runs a bit faster.  Something was screwed up in the initial implementation, so Intel released a microcode update that disabled it. They fixed it in time for some of the later steppings of Broadwell and in Haswell-EX.  Haswell's other big software-visible feature was AVX2, but x86 SIMD extensions are well into the territory of diminishing returns."
hardware,3cuwu8,wtallis,8,Sat Jul 11 04:59:12 2015 UTC,"G3258 != DC. As far as we know, the G3258 is just a G3420 with different firmware. Its release coinciding with DC made a lot of people think it contained the DC tweaks."
hardware,3cuwu8,SeaJayCJ,6,Sat Jul 11 08:57:48 2015 UTC,The G3258 wasn't Devil's Canyon.
hardware,3cuwu8,Exist50,6,Sat Jul 11 20:40:46 2015 UTC,"The Skylake team is removing the voltage regulator that was on the Haswell CPU, and the Hillsboro team is actually putting it back after Cannonlake.   Do you know why the Skylake team took the VR out?"
hardware,3cuwu8,williadc,4,Sat Jul 11 06:58:54 2015 UTC,I heard it was some kind of thermal issue? Either it was just not getting cooled effectively on die or it was heating up nearby components too much.
hardware,3cuwu8,Seclorum,9,Sat Jul 11 04:26:02 2015 UTC,"Skylake team took FIVR out of the design before their first tape-in, so that seems unlikely, especially given the reliability of FIVR in Broadwell, which is on the same process as Skylake and would have had a similar implementation, given that the FIVR expertise is all in Oregon."
hardware,3cuwu8,williadc,1 point,Sat Jul 11 05:41:20 2015 UTC,http://investorshub.advfn.com/boards/read_msg.aspx?message_id=86312251
hardware,3cuwu8,yuhong,4,Sat Jul 11 06:03:03 2015 UTC,"The issue described in that link was actually partially solved by FIVR.  What happened was that IDC found the right sequence of FMA instructions (new feature on Haswell) that could cause sufficient droop in the power grid and the circuits would fail (not burn up or anything, just not produce correct results).  This would have happened with off-die VRs as well, and in fact, a big part of the solution, the non-linear VR control, would not have been feasible with off-die VR.  It detects droop in the grid and throws a switch which turns up the VRs ""full blast.""  Unfortunately, we also had to throttle known-bad sequences of instructions as well (I think why the author says 99% solution).  I've been coy with not answering why Skylake took FIVR out of their design, and the answer is they sometimes have NIH syndrome.  They didn't like our implementation, but didn't have the resources to make something they did like.  This is a similar story with SMT and enabling differing voltages between core and uncore.  There are other examples, but you wouldn't notice them in the products.  Eventually the features get in there and stick, but it takes a couple tries to get Israel to buy in if they feel the complexity is out of balance with the benefit.  I understand this mentality, but wish the products didn't have to suffer for it."
hardware,3cuwu8,williadc,1 point,Sat Jul 11 19:27:20 2015 UTC,Thanks for the explanation. This is exactly what I am looking for.
hardware,3cuwu8,yuhong,2,Sat Jul 11 20:56:20 2015 UTC,I was feeling good that I just went ahead and bought a cpu before seeing this comment....
hardware,3cuwu8,davidyoyo2,1 point,Sat Jul 11 20:57:45 2015 UTC,"Shit, I still have my old 2500k sitting in a file server somewhere. That thing was an absolute beast, but hardware- just like time, rolls on, and if there's one thing I have learned it's that I am a slave to both."
hardware,3cuwu8,C4ples,4,Sun Jul 12 03:52:22 2015 UTC,"That 2500k is still 100% capable today.  In fact, you could sell it for ~$150 tonight on /r/hardwareswap."
hardware,3cuwu8,JMPopaleetus,3,Sat Jul 11 05:29:47 2015 UTC,Still rockin mine at 4.6 GHz playing all the new releases.  No plans for an upgrade until intel releases something that blows it away.
hardware,3cuwu8,SirBuckeye,8,Sat Jul 11 13:01:01 2015 UTC,"I have a 2600k and have been running it at 4.4 ghz for 4 years.  Connectivity updates aside, the performance gains are soooo minimal for price.  Performance per watt is a lot higher, though, so there's that."
hardware,3cuwu8,TripleBrass,3,Sat Jul 11 18:38:55 2015 UTC,"same, 2600k @4.4 with a GTX980, and I've really been wanting an excuse to upgrade the motherboard, cpu, and mem but they really haven't even raised an eyebrow for me.  I hope some magic happens before release."
hardware,3cuwu8,illuxion,7,Sat Jul 11 04:14:28 2015 UTC,"And I pretty much expected ""Pretty much expected"" to be the top comment. Intel is really sad without proper competition. Not saying that its impossible that their engineers have hit some kind of tech/physics wall, but I think they would've made more progress performance-wise if AMD guys were doing some progress themselves (although they kind of did, just not enough to push Intel)."
hardware,3cuwu8,ivan0x32,21,Sat Jul 11 10:43:18 2015 UTC,"To be fair they are making considerable progress in the iGPU, the place where AMD is ahead of them. And in perf per watt, where ARM is ahead of them. They're not phoning it in, they're just focusing on the areas in which they're weak. Unfortunately those areas aren't the reasons we buy desktop CPUs."
hardware,3cuwu8,Darius510,2,Sat Jul 11 03:23:37 2015 UTC,Amd is not ahead in iGPU anymore
hardware,3cuwu8,dylan522p,1 point,Sat Jul 11 05:53:48 2015 UTC,Took a few years though.
hardware,3cuwu8,Darius510,1 point,Sat Jul 11 20:41:26 2015 UTC,It took a few years to go from nothing to ahead of amd which is a leader in gpu
hardware,3cuwu8,dylan522p,49,Sat Jul 11 22:11:27 2015 UTC,AMD please deliver for the sake of everyone.
hardware,3cuwu8,Tetsudothemascot,32,Sat Jul 11 23:28:38 2015 UTC,"No pressure, but don't suck or we'll bash you over the head with our shattered dreams, shower you with unending scorn, and ship you to Korea. No pressure."
hardware,3cuwu8,MrsTtt,11,Sat Jul 11 04:39:26 2015 UTC,amd budget is like 1% of intel. dont get your hopes up
hardware,3cuwu8,NLWoody,8,Sat Jul 11 05:22:11 2015 UTC,"What I don't like about this comparison is that it neglects the fact that Intel is in many more businesses than AMD. Lots of sever and enterprise stuff that isn't directly related to CPU development. Also, it's closer to a 13x difference."
hardware,3cuwu8,Exist50,43,Sat Jul 11 11:08:33 2015 UTC,"Looks like my i5 2500K will keep soldiering on for at least another 3 years. Still only ~15% slower clock for clock than these ""next gen"" CPU's. Shameful.  Sandy Bridge, 2011 - ?"
hardware,3cuwu8,complex_reduction,32,Sat Jul 11 17:01:43 2015 UTC,"It may be shameful, but that's when happens when Intel, for all intents and purposes, has a monopoly.   Give AMD a kick in the ass if you want that to change."
hardware,3cuwu8,ptd163,12,Sat Jul 11 06:42:09 2015 UTC,"Sadly, a kick in the ass doesn't help when you don't have the resources to execute. What they'd need is billions of cash."
hardware,3cuwu8,bphase,11,Sat Jul 11 08:12:53 2015 UTC,"Let's all hope Zen can create some proper competition for Intel, as well as save AMD."
hardware,3cuwu8,m1l4droid,3,Sat Jul 11 09:50:03 2015 UTC,I mean single threaded performance isn't going to increase much anymore unless there's a big change in architecture transmeta style. I'm sure there's people working on it bit it's a completely different beast with different tradeoffs.   More cores/threads are slowly trickling down but that makes sense as its generally moving same rate as consumer software.   That's why this stuff is happening. AMD knows it too which is why Mantle happened.
hardware,3cuwu8,andromeduck,5,Sat Jul 11 09:20:33 2015 UTC,"Eh they've gone in a different direction to try to stall until Zen comes out. And to be honest APU's are great, I mean the igpu in an a8-7650k can play GTAV so I'm interested to see what happens next year."
hardware,3cuwu8,wowseriffic,5,Sat Jul 11 17:32:26 2015 UTC,Yeah I think I may upgrade my 2500k to a 2600k now for hyperthreading for $140 on eBay.  It may last another 6 years at this rate.
hardware,3cuwu8,AimlessWanderer,2,Sat Jul 11 12:53:09 2015 UTC,"One one hand, I'd like to see CPUs advance, on the otherhand, another generation that my 2500k will last.  My wallet is happy.  Plus I have a soft spot for AMD. Maybe this will give them a chance to do something."
hardware,3cuwu8,ducttape36,1 point,Sat Jul 11 21:02:15 2015 UTC,"Man is that 15% true? If my 4 year old i5 @5GHz outpaces this 6700k, I'll be pretty happy indeed. I picked a great time to make a new build (although only having PCIE 2.0 x8 for my CrossFire cards is kinna shitty)."
hardware,3cuwu8,pb7280,4,Sat Jul 11 17:49:16 2015 UTC,"Yes. I'm sure there are some specific mathematical benchmarks where the new CPU's beat the old ones, but in general the difference is only 10 - 15%, if that. For gaming etc it means virtually nothing."
hardware,3cuwu8,complex_reduction,10,Sat Jul 11 15:58:17 2015 UTC,"Currently have an i7 950, got a 980ti in the mail, was planning on getting a 5820k but since have been waiting for skylake. Seems a bit pointless now, but maybe they'll drop in price atleast?"
hardware,3cuwu8,ultimation,6,Sat Jul 11 16:03:06 2015 UTC,Or just go the Xeon route with the X58 chipset...
hardware,3cuwu8,XorFish,4,Sat Jul 11 12:27:04 2015 UTC,"i7 930 here, At this point I feel the need to upgrade after 5 years just to get all the upgraded accessories.  I'm tired of sata 3g and usb 2.0  and having 6gb ram at 1300mhz.        I'm worried about my parts failing after running overclocked for 5 years straight, so I let up on that when I started hitting more frequent failed boots.  Gaming performance will likely get a 10-20% fps boost in heavy physics games and it will be a godsend for strategy games like civ.    It's just time to upgrade, put on windows 10, and let the energy savings flow through you"
hardware,3cuwu8,hardrock527,4,Sat Jul 11 13:08:55 2015 UTC,"Yeah, I know for a fact my SSDs are bottlenecked by my motherboard at this point, but it's not been enough to upgrade. Everything is still fast about it, just not nearly as fast as it could be.  My main concern is what'll happen when I go to 4k soon."
hardware,3cuwu8,ultimation,1 point,Sat Jul 11 17:42:18 2015 UTC,"I think 4K should be just fine, it just increases GPU load. Basically makes your CPU less of a bottleneck since you won't be hitting as high framerates. CPU is more necessary at hitting 120/144 fps which won't be happening or necessary at 4K.  I think X58 even has 40 PCIe lanes so you can SLI and still get x16 which is great."
hardware,3cuwu8,bphase,1 point,Sat Jul 11 18:01:12 2015 UTC,"I've noticed if I watch a 4k youtube vid on a 1080p monitor that my cpu goes to like 40%, but that could be due to it having to downscale."
hardware,3cuwu8,ultimation,1 point,Sat Jul 11 19:00:14 2015 UTC,"Or perhaps it's not fully GPU-accelerated. Don't know really, but it's not the same for games."
hardware,3cuwu8,bphase,1 point,Sat Jul 11 20:16:05 2015 UTC,"i7 2600k here.  Almost the same boat - desire to upgrade.  I don't have many pressing needs for the extra compute power, but the power savings mitigate the cost.  I'm also looking for a reason to go from 16 --> 32 GB RAM (lots of VM work)."
hardware,3cuwu8,TripleBrass,2,Sat Jul 11 20:38:33 2015 UTC,"I'm in the same boat as you.  I am holding off on getting a new GPU since i wanted to put money down on skylake.  I think I'm just going to continue to wait, see what official reviews turn up, and maybe just see if i5-4690k and a z97 boards drop in price..."
hardware,3cuwu8,Tortilla_King,2,Sat Jul 11 18:44:26 2015 UTC,"They won't drop in price. And wait for Skylake just for the chipset features. Your system will be much more future-proof, peripheral wise."
hardware,3cuwu8,random_guy12,1 point,Sat Jul 11 14:08:36 2015 UTC,"Unless you need the raw cpu power like video encoding, don't."
hardware,3cuwu8,ElDubardo,19,Sat Jul 11 20:52:16 2015 UTC,"If this is true, I feel like an idiot for waiting for Skylake... guess ill continue with my fx 6350 until Intel actually improves their cpus."
hardware,3cuwu8,skilliard4,30,Sat Jul 11 20:47:31 2015 UTC,get a 5820k or 4790k and you're set for the next 4 years.
hardware,3cuwu8,Le_rebbit_account,16,Sat Jul 11 01:47:42 2015 UTC,More like next 10 years. At least if DX12 takes off quickly.
hardware,3cuwu8,58592825866,13,Sat Jul 11 02:28:56 2015 UTC,5820k is overkill for most. The 4790k is an excellent chip.
hardware,3cuwu8,CptTinman,9,Sat Jul 11 03:37:47 2015 UTC,"More expensive mobos, and the binning isn't really comparable. The 5820K probably isn't that highly binned at all."
hardware,3cuwu8,Exist50,3,Sat Jul 11 03:39:55 2015 UTC,How about a 4690K...?
hardware,3cuwu8,urraca,2,Sat Jul 11 05:31:21 2015 UTC,"By the time Intel really improves CPU performance AMD will have viable CPUs out for gaming. Currently it looks like Intel is focusing on the places they are behind, which is performance per watt (ARM is doing much better here) and integrated graphics (AMD has been doing much better here)"
hardware,3cuwu8,mack0409,27,Sat Jul 11 06:28:27 2015 UTC,"As somebody who has been gaming on an I5's HD4000 I really do see the appeal of this. My ivy bridge can run some good games at decent settings and still get 40-60fps, and lower settings completely abd be able to play games like league of legends in a semi-competitive environment.  Improvements to the IGPU let very portable setups play better games while causing less heat and needing les power. I see advancements like this and low power Maxwell stuff from nvidia to be the real direction of computing, and thus the market."
hardware,3cuwu8,stickynipplemadness,3,Sat Jul 11 17:22:29 2015 UTC,"cough APU's cough I see where you're coming from, I've been gaming on a hd 3000 and really wish I had a a10-7850k on my laptop."
hardware,3cuwu8,stereosteam,2,Sat Jul 11 10:43:00 2015 UTC,"No way, iris6200 quad core in the mobile chips. Don't remeber the model number but it's way better in cpu and 40% better in igpu"
hardware,3cuwu8,dylan522p,34,Sat Jul 11 01:48:41 2015 UTC,"So if you care about gaming you'll get no benefit from Skylake since you'll use a proper graphics card, but if you DON'T care about gaming Skylake isn't for you since the older IGP's are fine for any non-gaming related task."
hardware,3cuwu8,Grummond,20,Sat Jul 11 13:13:38 2015 UTC,"its nice for people with OEM pcs that don't have an actual GPU. Obviously people like you and me hate this, but it really benefits people that buy OEM pcs  It used to be if you had a storebought pc, you couldn't play PC games well. At least with how much IGPs are improving, it's becoming a reality. This could be good for PC gaming as more people can get involved as it no longer will require figuring out how to build your own pc or buying an overpriced ""gaming computer"""
hardware,3cuwu8,skilliard4,2,Sat Jul 11 20:44:42 2015 UTC,"This could be good for PC gaming as more people can get involved    That is indeed the silver lining to this rather disappointing generation of CPUs; more casual PC gamers, and thus better casual PC games.   yay. :-/"
hardware,3cuwu8,ZeroWithEverything,11,Sat Jul 11 01:33:54 2015 UTC,"Doesn't DX12 have support for using iGPU at the same time, which could improve gaming perf marginally?"
hardware,3cuwu8,MaxGhost,14,Sat Jul 11 01:50:58 2015 UTC,"It does, but it requires game developers to code specifically for it. Given the sheer prevalence of iGPU these days I'd be surprised if developers didn't, but it's still not an automatic thing that all DX12 games can take advantage of an iGPU."
hardware,3cuwu8,TheExecutor,6,Sat Jul 11 16:55:33 2015 UTC,"VirtuMVP makes the iGPU and dGPU work together now without dev support. Although not as efficient or perfect, its still possible, just not all that worth it."
hardware,3cuwu8,TheBloodEagleX,10,Sat Jul 11 01:51:24 2015 UTC,Everything I've been hearing about DX12 leads me to believe that it will be able to make use out of both simultaneously.
hardware,3cuwu8,DrFunkenstyne,11,Sat Jul 11 02:59:57 2015 UTC,"It will, if the developers code for it. I expect this to be a pretty much negligibly adopted feature since the cost would be very high for minimal gain. Just look at the issues AMD's dual graphics has on the same architecture with years of driver refinements."
hardware,3cuwu8,Exist50,2,Sat Jul 11 05:36:22 2015 UTC,"It allows developers to specifically code for either, so they could offload specific parts to the iGPU (eg. Something similar to Hairworks, physics instructions, LOD rendering, etc) rather than the GPU at will. The reason dual graphics doesn't work amazingly is because you're trying to load balance two very different processors, this won't require that as it's more of allowing access to any additional processors you have enabled.  That's as I understand it, anyway."
hardware,3cuwu8,Democrab,2,Sat Jul 11 01:49:49 2015 UTC,"The developers code for the most common platforms. For a long time, that will be Haswell and earlier models, whose igpus aren't as strong.   And, you will presumably need engines to be built around it, so expect that to really effect games on the shelf a couple of years from now. Not, right after DX12 released."
hardware,3cuwu8,Fatigue-Error,1 point,Sat Jul 11 06:24:52 2015 UTC,"That's already possible with amd APUs and amd graphics cards: http://www.tomshardware.com/reviews/dual-graphics-crossfire-benchmark,3583-3.html sometimes gives a small fps increase, sometimes not at all."
hardware,3cuwu8,ICanHazTehCookie,1 point,Sat Jul 11 08:54:27 2015 UTC,You can use both right now too although not as well like DX12. My ASRock mobo came with VirtuMVP and it uses my iGPU with my dGPU.
hardware,3cuwu8,TheBloodEagleX,6,Sat Jul 11 13:09:49 2015 UTC,It does make me excited for super efficient mid-range gaming laptops that don't even need a dedicated gpu though.
hardware,3cuwu8,jji7skyline,2,Sat Jul 11 04:27:20 2015 UTC,What about high resolution displays?
hardware,3cuwu8,medikit,2,Sat Jul 11 05:35:30 2015 UTC,"Most ""gaming"" laptops have discrete GPUs barely more powerful than an Intel HD4400, there is a solidifying large section of the market that would benefit from using the iGPU in addition to the dedicated GPU."
hardware,3cuwu8,mack0409,2,Sat Jul 11 01:46:51 2015 UTC,Altough for low end gaming the new iGPU is ok I think. If you know you dont need more.
hardware,3cuwu8,willi_werkel,1 point,Sat Jul 11 11:53:19 2015 UTC,"The older iGPUs are not ok. Even the Haswell iGPUs (excluding Iris Pro) struggle to render the OSX and Windows desktop environments at very high resolutions (13"" rMBP 2560x1600, 4K, etc.) and that's where the markets, both desktop and mobile, are headed.   Like everything works fine, but you can see the animations struggling along at 15-20 FPS. Try entering mission control in OSX or scrolling through the Windows 8.1 start screen. Stutter, stutter, stutter.   And you're ignoring that more and more web apps are relying on the GPU to provide a smooth experience. A lot of WebGL apps, or animation heavy HTML5 pages struggle on iGPUs.   There's also a whole issue with HEVC video playback and proper hardware acceleration on that end (which only GM206/GTX 960 current offers).   By Cannonlake, Intel needs to make 48 EUs (or equivalent performance level from new arch) and at least 64 MB of eDRAM standard, even on the lowest end SKU."
hardware,3cuwu8,random_guy12,19,Sat Jul 11 10:49:42 2015 UTC,"Review covers desktop parts not laptop parts, which may turn out to be far more interesting."
hardware,3cuwu8,Sugar_Horse,7,Sat Jul 11 06:12:14 2015 UTC,Hopefully a large increase in battery life comes with Skylake in laptops. I am going to be going to college soon so I am looking for laptops that won't be completely obsolete in 4 years. Hopefully Skylake checks all the right boxes.
hardware,3cuwu8,SchrodingersCat_,6,Sat Jul 11 20:49:59 2015 UTC,Will skylake be better for overclocking? Can't read the article right now.
hardware,3cuwu8,willi_werkel,3,Sat Jul 11 00:50:33 2015 UTC,I suspect not if the trend continues. Intel's obviously had trouble hitting higher clocks on 14nm.
hardware,3cuwu8,Exist50,9,Sat Jul 11 02:20:52 2015 UTC,"I really hope AMD delivers, I say we would be easily up to 6-8 cores on the mainstream side by now if they would have remained competitive."
hardware,3cuwu8,niioan,2,Sat Jul 11 06:14:32 2015 UTC,Odds are we would be higher than that.
hardware,3cuwu8,mack0409,3,Sat Jul 11 06:32:19 2015 UTC,"Only server applications make serious use of multiple available cores. Desktop applications rarely use more than a single core.  Processing for games has shifted to the GPU, and too many cores has shown to be counter-productive.    AMD has really got to stop pushing the multiple cores, and has to focus on higher frequency if they want to survive.  They're currently developing polycore CPU's for a very tiny, almost non-existent, market that is already dominated by Xeon."
hardware,3cuwu8,Spreadsheeticus,4,Sat Jul 11 07:33:10 2015 UTC,"Gaming on multiple cores is starting to pick up, frostbite engine and Cryengine both see advantages and with consoles pushing 5 & 6 cores for gaming, eventually developers (at least the AAA ones) will get it sorted out in time. If Unreal Engine 4 also makes good use, it will be used in a lot of games as well.   As for AMD they have already said they did it wrong and are refocusing on high performance cores with Zen, but it's not really GHz either, it's all about architecture, AMD chips have just as high frequencies as intel."
hardware,3cuwu8,niioan,2,Sat Jul 11 10:51:01 2015 UTC,Right and quad cores with SMT have 8 threads...   Intel is pretty on point with this stuff.
hardware,3cuwu8,andromeduck,1 point,Sat Jul 11 17:02:40 2015 UTC,"Economics is the biggest factor when comparing apples to oranges-  Developing on the CPU is still cheap for consoles where you can't upgrade the GPU, but more expensive for PC's where the GPU cost is dropping fast.  There is a good chance that we could see a shift if the adoption rate for Steam Box is favorable over the next year or two."
hardware,3cuwu8,Spreadsheeticus,3,Sat Jul 11 17:23:24 2015 UTC,I wonder if this kind of performance comparison shows that the Surface Pro 4 will actually launch along side windows rather than waiting for Skylake.
hardware,3cuwu8,Spacebotzero,3,Sat Jul 11 17:38:51 2015 UTC,"I don't know if Microsoft is necessarily waiting for the performance boost (with how closely they work with Intel, I'm sure this chart is not news to them), but the other benefits that Skylake will bring (IGPU, wireless charging, etc.). I don't know if wireless charging will be available for the SP4, but they've had a lot of extra time between SP3 and SP4, so maybe they will go for it."
hardware,3cuwu8,mokasakin,7,Sat Jul 11 17:42:12 2015 UTC,It seems that desktop Skylake may be no more efficient than Haswell for the top SKUs if the tdps can be directly compared.
hardware,3cuwu8,Exist50,13,Sat Jul 11 02:03:42 2015 UTC,"It'll be more efficient if you subtract out the larger integrated graphics, I expect."
hardware,3cuwu8,CarVac,6,Sat Jul 11 11:41:24 2015 UTC,"The iGPU doesn't draw much power if it's not being used for 3d rendering. The idle power draw of the iGPU won't have been changed significantly by its growth, so it's not going to be eating a larger share of TDP during CPU benchmarks."
hardware,3cuwu8,wtallis,4,Sat Jul 11 00:04:20 2015 UTC,"I thought TDP includes full use of both CPU and GPU.   The GPU takes a larger portion of the TDP (not changed much) in the new parts, so if you don't use it, the CPU portion will use less power under full load."
hardware,3cuwu8,CarVac,9,Sat Jul 11 00:09:19 2015 UTC,"TDP is no longer empirically determined from any particular ""full"" workload. Instead, the chip is programmed to constrain itself to a particular TDP and it will adjust clock speeds and voltages to stay within those limits. When the iGPU isn't being used the CPU gets to use the slack. When only some CPU cores are active the rest get to run faster and draw more power. High-end desktop CPUs that include an iGPU are still able to consume the entire TDP with a CPU-only workload. CPU designers put a lot of work into ensuring that the chip can accurately estimate its power consumption to make full use of its power budget at all times.  To test this, I just fired up a software compilation job on my i7-4790K and watched the ACPI power meter. The default TDP of this chip is 88W. I've changed it to 125W in the motherboard's overclocking settings. The meter peaked at 127.13W and it's sustaining an average of around 125W, while the processor cores are running at 4.16GHz and the iGPU is off."
hardware,3cuwu8,wtallis,7,Sat Jul 11 01:36:34 2015 UTC,Performance gain in the i7 is pretty staled since its first launch...
hardware,3cuwu8,ElDubardo,17,Sat Jul 11 01:44:05 2015 UTC,"According to Anandtech bench, an i7 4790K is aroundabout double the performance of an i7 920 on average (88% faster on one test, 128% faster on another and so on)  That's a combination of IPC and ~1.75Ghz higher clocks though. Still a noticable performance difference, but nothing like the gap we'd see over a similar timeframe shifted by a decade.  i7 920 was late 2008, i7 4790K was mid 2014, so call it 6 years prior.  2004 to 1998 you'd be looking at a ~450Mhz Pentium 2 versus a ~2Ghz Athlon 64, with a comparatively VAST chasm of performance between the two."
hardware,3cuwu8,TorazChryx,16,Sat Jul 11 02:05:27 2015 UTC,"Pentium II 450 MHz to Athlon 64 2 GHz also had a massive jump forward in power draw, from 27w TDP to 89w. Since the i7-920/940 Intel have been working backwards from 130w to 88w for the i7-4790K.   At 160w you could get a 10 core/20 thread Haswell Xeon running at 3.1/3.5 GHz, sadly that's a $2000 chip."
hardware,3cuwu8,OftenSarcastic,6,Sat Jul 11 04:19:05 2015 UTC,"Even if you did do that would it be worth it? I've never used a >4 core computer, but I've heard that since quad core is the mainstream maximum, most software (particularly games) is only coded for up to 4 cores. Lots of games I've played seem to only use 2 cores. A 4GHz quad probably has better quad core performance than that Xeon, even if the Xeon has more raw power it may not have such high real world performance?  I do hope more things get >4 core optimization, I'm thinking of a hexa core for my next chip (which will be a couple more years by the looks of it)."
hardware,3cuwu8,pb7280,3,Sat Jul 11 06:01:51 2015 UTC,"No, it's not worth buying a 10 core CPU for desktop use or gaming. The problem is that not every task can be neatly multi-threaded and/or it's very hard to do, so we're kinda stuck with lower core count.  I just wanted to point out that we have progressed in CPU power if we expand to higher wattage CPUs.  Would be funny to see some 400w CPUs if we had done the same tripling in TDP over 6 years, but I doubt many people would want to buy one of those."
hardware,3cuwu8,OftenSarcastic,3,Sat Jul 11 09:36:19 2015 UTC,"Probably how AMD ended up missing out on the high end systems, they bet on multi core becoming better and focused on that instead of single core performance. Now you can get a octa core for half the price of some Intel quads that just gets destroyed in every day computing lol."
hardware,3cuwu8,pb7280,7,Sat Jul 11 16:08:06 2015 UTC,"There's also instruction set additions, but those have a habit of being more niche cases."
hardware,3cuwu8,Exist50,1 point,Sat Jul 11 16:31:30 2015 UTC,"Indeed, there are also rare corner cases where the newer architectures are wildly different in performance from previous ones.  i.e. Haswell was a substantial jump over Ivy Bridge when running Dolphin"
hardware,3cuwu8,TorazChryx,5,Sat Jul 11 18:03:15 2015 UTC,i7 920 to sandy was the biggest jump though :-)   If you take sandy to now it has slowed :-)
hardware,3cuwu8,jigssaw,2,Sat Jul 11 07:01:17 2015 UTC,"Well there have been IPC improvements over the generations but since haswell first came out, there really hasn't been much to be enthused about."
hardware,3cuwu8,Seclorum,1 point,Sat Jul 11 07:09:53 2015 UTC,Are you kidding? The original i7's have IPC's equal to AMD's FX CPUs.
hardware,3cuwu8,bizude,3,Sat Jul 11 08:38:10 2015 UTC,So I'm fine sticking with my i5-3470 OC'd to 4Ghz?
hardware,3cuwu8,spyder256,5,Sat Jul 11 05:33:44 2015 UTC,"Question from a fellow 3470 owner, how did you OC it?"
hardware,3cuwu8,everyZig,5,Sat Jul 11 19:19:56 2015 UTC,"First, I have an ASRock Z77 extreme 4 motherboard.  As far as I know all i5's and i7's can be overclocked, it's just that non K's have limited multipliers.  This CPU's multipliers limited to 38, so that will get you 3.8Ghz.  And I decided to turn the BCLK up to 105.3 to get it to 4Ghz. But be careful as this can be very dangerous, I'm lucky that this is stable, high as it is.  http://i.imgur.com/qRjmGJ9.png"
hardware,3cuwu8,spyder256,3,Sat Jul 11 04:22:23 2015 UTC,If things keep going like this you'll be fine into the next decade.
hardware,3cuwu8,Blubbey,2,Sat Jul 11 06:57:27 2015 UTC,Pretty much.
hardware,3cuwu8,Seclorum,3,Sat Jul 11 12:39:33 2015 UTC,"Sweet, my 4670k won't devalue as quick."
hardware,3cuwu8,zakats,3,Sat Jul 11 13:43:12 2015 UTC,Where are the overclocking results? That's what most enthusiasts are going to make their decision on.
hardware,3cuwu8,BlayneTX,1 point,Sat Jul 11 05:32:45 2015 UTC,well since its 14nm .... and early on with the process not being matured I would think it will overclock worse then last gen. But who knows :-)
hardware,3cuwu8,jigssaw,2,Sat Jul 11 04:59:25 2015 UTC,The 4.2 turbo clock suggests you may be right. I guess the 4790k will be the king for a while longer.
hardware,3cuwu8,BlayneTX,3,Sat Jul 11 04:59:32 2015 UTC,"Tis a shame. Oh well, there might be other benefits. I'll wait 'till it comes out though."
hardware,3cuwu8,Sandwich247,4,Sat Jul 11 08:37:03 2015 UTC,"I was seeing 8% rumors before, which I was pretty okay with, since I'd be going from a 3570k to a 6700k. If true, this is lame and I don't see why I shouldn't just go z97 or x99 at this point.   Edit: I was almost gonna double my k's! sob"
hardware,3cuwu8,Maysock,7,Sat Jul 11 08:56:04 2015 UTC,"Skylake is increasing the TDP by 8% and benefiting from a process shrink, so an 8% performance increase is pretty much the minimum necessary to justify shipping it at all in the desktop market."
hardware,3cuwu8,wtallis,6,Sat Jul 11 11:38:09 2015 UTC,"These tests aren't very reflective of high-end computing, including gaming using a discrete high-end gpu.  This is just a simple quality benchmark for comparing gains in day-to-day performance.   It's not important to my ego that Skylake significantly outperform Haswell, but we're a bit too early to hope to see any relevant benchmarks."
hardware,3cuwu8,Spreadsheeticus,2,Sat Jul 11 02:03:32 2015 UTC,"Zen better wake Intel up, if not I fear we won't see anything better CPU wise for a long time..."
hardware,3cuwu8,hdshatter,1 point,Sat Jul 11 03:22:25 2015 UTC,Wow... this is getting confusing for me.  So is it still good to have the latest and greatest or is the current gen good enough for all but the extremely wealthy?
hardware,3cuwu8,antemon,3,Sat Jul 11 02:31:20 2015 UTC,current gen good enough for all but the extremely wealthy   Definitely.
hardware,3cuwu8,bphase,1 point,Sat Jul 11 12:15:53 2015 UTC,"Hey guys. I have a i7 3770k (i think at least) with a 670geforce. I was thinking of some upgrades for fallout 4. Do you think ill need to upgrade the motherboard and the processor, or you think I could just slap a 970 in there and be done with it?"
hardware,3cuwu8,Archijslv,4,Sat Jul 11 06:02:39 2015 UTC,"overclock cpu, replace gpu."
hardware,3cuwu8,Pieloi,1 point,Sat Jul 11 09:02:46 2015 UTC,"Will that overclock bring some noticable increases? I honestly have no clue what hz its running, and im nottoo anal about the fps and stuff like that"
hardware,3cuwu8,Archijslv,4,Sat Jul 11 09:06:27 2015 UTC,"I think it depends on the game honestly, In games like Attila total war I can easily see a 4-10fps increase from 3.4ghz to 4.5ghz on my 2600k, Cities skylines will be a similar scenario with a lot of processes running on the CPU.  Games like Metro:2033 and The Witcher 3, not so much, very graphically intensive games which always benefit from a better GPU."
hardware,3cuwu8,Pieloi,1 point,Sat Jul 11 10:16:07 2015 UTC,"Yeah i play dota and h1z1 mainly, dont think it would matter that much then. And probably will play F4 or smth. Thanks anyways, a new gpu it is then"
hardware,3cuwu8,Archijslv,2,Sat Jul 11 11:56:55 2015 UTC,You should definitely check out the 390 as well. Looks to be a great card with better value and more future proofed than the 970.
hardware,3cuwu8,Redditor11,2,Sat Jul 11 12:09:30 2015 UTC,"Overclocking won't improve your performance in most games because you'd likely play at 1080p or above. At those resolutions, performance isn't CPU-limited."
hardware,3cuwu8,Anaron,1 point,Sat Jul 11 12:20:27 2015 UTC,I play at 1440p. I have a huge ass monitor tho :)
hardware,3cuwu8,Archijslv,3,Sat Jul 11 14:38:45 2015 UTC,just slap a 970 in there and be done with it
hardware,3cuwu8,Aurailious,1 point,Sat Jul 11 18:59:23 2015 UTC,"GPU by far, if you can OC your CPU do it."
hardware,3cuwu8,Blubbey,1 point,Sat Jul 11 19:48:58 2015 UTC,"I think i can, i have an aftermarket cooler, just dont know if its worth the bother"
hardware,3cuwu8,Archijslv,1 point,Sat Jul 11 10:54:17 2015 UTC,So is it worth buying a skylake processor or is the 4790k the best option? Might do a processor upgrade in a couple of months
hardware,3cuwu8,mermaliens,3,Sat Jul 11 13:44:30 2015 UTC,"I'd wait, max overclocks are unknown as of yet, and Skylake will bring some useful platform improvements that make it kind of worth it even if the CPU itself disappoints."
hardware,3cuwu8,bphase,1 point,Sat Jul 11 19:49:24 2015 UTC,Will skylake support ddr3?
hardware,3cuwu8,mermaliens,4,Sat Jul 11 10:11:51 2015 UTC,"Yes, though only low voltage DDR3."
hardware,3cuwu8,bphase,2,Sat Jul 11 10:40:23 2015 UTC,"The bigger question i'm seeing is, will skylake motherboards support ddr3.  Most motherboards i can find now don't seem like they support ddr3"
hardware,3cuwu8,Tortilla_King,1 point,Sat Jul 11 10:52:55 2015 UTC,"My 3450 is still going strong, seems like there is no reason to consider upgrading before Zen comes out."
hardware,3cuwu8,Sumsar_,1 point,Sat Jul 11 11:22:08 2015 UTC,"will ddr4 versions be able to use quad channel? i guess i got spoiled looking at all them x99 boards, and the (whatever chipset skylake uses) dim layouts looked last gen. unless im missing something x.x"
hardware,3cuwu8,bisjac,2,Sat Jul 11 14:11:02 2015 UTC,"Quad Channel is for enthusiast platforms (Haswell-E, Skylake-E possibly) only. It is also pretty much unnecessary, memory bandwidth isn't really a bottleneck in most situations. Even single channel tends to not affect performance much. Even less so with the high bandwidth DDR4 allows."
hardware,3cuwu8,bphase,1 point,Sat Jul 11 10:39:10 2015 UTC,"If I'm looking to upgrade to either a 4790k or a 6700k, would it be better to wait for the 6700k and buy one, wait for the 6700k for 4790k prices to decrease, or just go ahead and buy a 4790k right now?"
hardware,3cyiec,zmeul,8,Sat Jul 11 22:55:55 2015 UTC,This article doesn't mention a delay at all. Title's misleading or downright false.
hardware,3cyiec,Exist50,7,Sat Jul 11 23:10:57 2015 UTC,"According to local media reports in Taiwan on July 8, the TSMC will start risk production in its 10-nanometer processing lines in the second quarter of next year    the initial date was Q4 ""risk"" production and Q4 2016 for mass production - now, if ""risk"" is delayed to Q2 2016, mass production will go possibly Q2 2017  the original road map: http://en.ctimes.com.tw/DispNews.asp?O=HJZ4GC65UYSSAA00NW   TSMC still expected to go mass production in the Q4 2016"
hardware,3cyiec,unsaltedbutter,2,Sat Jul 11 23:46:22 2015 UTC,Yeah I was thinking the same thing reading it.  Doesn't tsmc have some 16nm thing they are readying?
hardware,3cyiec,Exist50,2,Sat Jul 11 23:14:57 2015 UTC,"Yup, 16nm FF and later FF+."
hardware,3cw761,m00nty,3,Sat Jul 11 07:55:35 2015 UTC,"Just read the review, this thing is beautiful.  The daughterboard is a bit inelegant and I'm not a fan of the blue PCB but speeds are insane and the 100core setup is pretty interesting."
hardware,3cw761,TheBloodEagleX,1 point,Sat Jul 11 22:41:17 2015 UTC,I write SW that is close to being an IMDB...I wonder if they'd give me one to write code for?
hardware,3cw761,krista_,1 point,Sun Jul 12 00:24:44 2015 UTC,"Mangstor also claims to offer much more performance than current-generation Fusion-io offerings. Unfortunately, we cannot confirm those claims because Fusion-io does not submit samples for independent third-party evaluation.   Mangstor is completely wrong here. I own a fusion-io generation 1 slc card. It achieves a 26 microsecond read latency.   The company (Mangstor) specs read latency at less than 90 microseconds and write latency at 15 microseconds   They just admitted right there this drive is not even better than the generation 1 fusion-io card. Which you can get on ebay for $400 now. The drivers are available for basically any OS you need as well."
hardware,3cw761,gjs278,5,Sun Jul 12 22:55:49 2015 UTC,"2,7TB mind you, so 2700GB. $6 per GB is still a lot more expensive than most other drives but i am sure some enterprises have usecases for this, like high load database servers."
hardware,3cw761,barthw,2,Sat Jul 11 14:38:32 2015 UTC,"It's an enterprise drive, what did you expect?"
hardware,3cud0e,Luckyduck1337,7,Fri Jul 10 21:15:18 2015 UTC,"TLDR ""Faster than the GTX 980"""
hardware,3cud0e,Scrimps,6,Fri Jul 10 23:57:55 2015 UTC,"Come on now.  TLDR   faster than gtx980 fixed high multimonitor idle power consumption average gaming power consumption only 44W above gtx980, 40W under r9 290"
hardware,3cud0e,DoTheEvoIution,5,Sun Jul 12 00:38:09 2015 UTC,Faster than a reference 980.  And only at 1440p and higher.
hardware,3cud0e,keeif,6,Sat Jul 11 00:08:35 2015 UTC,"Well, it's faster without project cars at 1080p, so it really depends on the game choice. Techpoweredup's game list currently has a lot of Nvidia titles."
hardware,3cud0e,Exist50,-2,Sat Jul 11 06:16:02 2015 UTC,Id say if you're spending $500+ on a gpu then you shouldn't be gaming at 1080p.
hardware,3cud0e,wiyumishere,4,Sat Jul 11 14:35:23 2015 UTC,144hz dude
hardware,3cud0e,TheRealRacker,-11,Sat Jul 11 21:05:11 2015 UTC,Faster than a reference 980. And only at 1440p and higher.   oh and cheaper.   Superior card is superior.
hardware,3cud0e,Scrimps,8,Sat Jul 11 00:14:13 2015 UTC,wat  Aftermarket 980s are $500-520.
hardware,3cud0e,keeif,4,Sat Jul 11 00:15:11 2015 UTC,Bunch of ppl on /r/buildapcsales picked up 980s for like 240$ today.
hardware,3cud0e,veritasen,2,Sat Jul 11 01:10:49 2015 UTC,Holy fuck I love that subreddit. Gotta watch it like a hawk though. Managed to snag an i7 4770k for $175 there.
hardware,3cud0e,Juheebus,3,Sat Jul 11 04:59:09 2015 UTC,damm americans.... :(
hardware,3cud0e,NLWoody,1 point,Sat Jul 11 11:31:26 2015 UTC,your comment made me go check it out.  All I can say is holy shit
hardware,3cud0e,PaulTheMerc,1 point,Sat Jul 11 01:53:10 2015 UTC,MSRP dropped $30. Now It's more like $470+.
hardware,3cud0e,Nixflyn,-22,Sun Jul 12 23:56:44 2015 UTC,"Make sure all the AMD apologists take a good, long hard look at these graphs.  http://i.imgur.com/mjqkHLR.png"
hardware,3cud0e,TaintedSquirrel,11,Fri Jul 10 22:09:10 2015 UTC,Is this where I say I'm really happy with the value of my AMD R9 290?
hardware,3cud0e,OftenSarcastic,9,Fri Jul 10 22:45:33 2015 UTC,"Same here. Great purchase, although I paid a full $399 for mine. At sub-$250 with rebates it is amazing.  I think people will always find a way to buy Nvidia.  When it was the 290(x) vs the 780, the 290(x) were cheaper, better performing, and had more vram. 780s were cooler and quieter and had a bit lower power consumption. Everyone bought 780s.  Flip the situation around with the Fury X vs the 980ti - 980ti has better performance, more vram, same price. Fury X is far quieter and cooler, but it does have a bit higher power consumption. Everyone wants 980tis.  If Nvidia has a performance lead, performance is important. If Nvidia has a cool/quiet lead, cool/quiet is important."
hardware,3cud0e,KKV,1 point,Fri Jul 10 23:11:16 2015 UTC,Where did you read the Fury X's are quiet? Even AMD admitted the pump and coil whine were too loud.
hardware,3cud0e,melgibson666,12,Sun Jul 12 11:14:32 2015 UTC,"Not sure if that's the best graph to stick it to ""AMD apologists"" as it seems to support the case for 290/290X/390/390X over 970/980. Fury apologists maybe ..."
hardware,3cud0e,iamstephen1128,9,Fri Jul 10 22:17:24 2015 UTC,And of course high performance will have worse perf/$ due to diminishing returns...if anything that sells it for AMD as they totally bottom out that chart.
hardware,3cud0e,glr123,2,Fri Jul 10 22:43:00 2015 UTC,"Which just makes it more crazy that the 290 does so well on those charts, usually even beating out lower end R9 2xx cards, given that it's also one of the more powerful cards on the market."
hardware,3cud0e,Nerdsturm,6,Sat Jul 11 01:13:05 2015 UTC,Nice edit BTW.  Lets get going on the apologetics! If someone bought the Sapphire R9 Fury Tri-X instead at the $549 launch price the relative $$ value for the GTX 980 would change to:   1080p   114.38% 1440p   106.37% 2160p    99.51%
hardware,3cud0e,OftenSarcastic,0,Fri Jul 10 23:24:06 2015 UTC,Where's the comparison for having the card for 9 months before the fury even existed? :)
hardware,3cud0e,w00t692,1 point,Sun Jul 12 17:49:12 2015 UTC,Probably in the archive of irrelevant stuff.
hardware,3cud0e,OftenSarcastic,0,Sun Jul 12 18:04:41 2015 UTC,You're right I totally haven't been enjoying this performance for almost the last year.   Weeeeee
hardware,3cud0e,w00t692,2,Sun Jul 12 18:55:56 2015 UTC,"alien iso= 20.8% unity = 22.7% batman = 20.4% bf3 = 24.9% bf4 = 11% bioshock = 29.8% cod aw = -3.5% civ = 30.3% crysis 3 = 23% dead rising 32.4% da:I = 4.6% far cry 4 = 29.9% gta v = 16.5% metro last light = 12.4% project cars = -15% ryse = 18.9% SoM = 25.6% Witcher 3 = 16.8% Tomb Raider = 23.8% Watch Dogs = 11.6% Wolfenstein = -10.3% WoW = -2.6%     Has a pretty impressive lead in some games at 4k that isn't reflected in the total.  edit: Metro Last Light seems off, Tom's and PcPer have it at more than 20% faster. DigitalFoundry have it around 30% faster in Ryse."
hardware,3cud0e,namae_nanka,1 point,Sat Jul 11 02:11:41 2015 UTC,Ryse is a piece of shit Game that no one plays. Who gives a shit about it.
hardware,3cud0e,w00t692,1 point,Sun Jul 12 16:11:01 2015 UTC,"Eh  That just makes me glad I've had my 290 for like two years or whatever, definitely  one ups a lot of my previous tech purchases, probably  in my top 5 of the decade, just behind an SSD, a 2500k, and a laptop with good Linux  support. 5th would be my Korean 1440p  monitor."
hardware,3ctimr,scotts06,15,Fri Jul 10 17:29:23 2015 UTC,"Not only do they continue with their POS Killer network controllers, but this time they have three of them. Ugh."
hardware,3ctimr,atomicthumbs,3,Fri Jul 10 20:25:39 2015 UTC,"I've been wondering about those. Seems like a bunch of marketing buzzwords. Are they actually worse than the standard Intel gigabit controller? Or are they better for someone with a shitty connection that chokes up on the lightest amount of traffic, letting games get priority?  Or perhaps they're just equal and a waste of money? That'd be my guess."
hardware,3ctimr,bphase,0,Fri Jul 10 22:39:14 2015 UTC,"If you have a pretty shitty/slow internet connection they have QoS software that can help (a tiny bit) with getting your internet gaming traffic prioritized over other traffic like torrents.  However there ahve been a number of people run long tests on Intel and Killer nics and come up with zero difference otherwise.  My suggestion to most people is to avoid either of them.  Pick up a Intel Gigabit CT pcie card (EXPI9301CT).  It has the controller built into it so it puts zero load on the CPU.  The load off a built-in nic might only be 1%, but it is still something you can offload into a card.  Plus it lets you buy the board you want without worrying what nic you get on-board."
hardware,3ctimr,Thunder_Bastard,0,Sat Jul 11 05:32:51 2015 UTC,They're probably better than the Realtek ones though.
hardware,3ctimr,TheBloodEagleX,1 point,Sat Jul 11 05:37:54 2015 UTC,"You realize Killer was spun off, right?"
hardware,3ctimr,Exist50,5,Fri Jul 10 20:41:07 2015 UTC,"Gigabyte tends to use them, so I say ""their."" I know MSI uses them too, and I know they're garbage."
hardware,3ctimr,atomicthumbs,1 point,Fri Jul 10 20:44:01 2015 UTC,"As in, it used to be a division of Qualcomm Atheros, but they were recently spun off again."
hardware,3ctimr,Exist50,4,Fri Jul 10 20:52:21 2015 UTC,good riddance?
hardware,3ctimr,atomicthumbs,8,Fri Jul 10 21:32:32 2015 UTC,Is SATA Express actually going to catch on?   Do we need boards with multiple SATA Express ports?
hardware,3ctimr,aziridine86,2,Fri Jul 10 23:03:49 2015 UTC,Should just replace them with mini-sas (U.2)
hardware,3ctimr,sr1030nx,1 point,Sun Jul 12 04:56:14 2015 UTC,"I don't get why we would want SATA express, SSD's are already nearly 3 times as fast as the port.  Seems like it needs to go away."
hardware,3ctimr,jdw101,6,Mon Jul 13 15:36:55 2015 UTC,Can their G-connector be a standard thing for all motherboards or cases?  Its the one thing about building a PC I don't like doing.
hardware,3ctimr,spydr101,3,Fri Jul 10 20:45:51 2015 UTC,All good mobos have then this days... Edit massive typo.
hardware,3ctimr,americosg,1 point,Fri Jul 10 22:19:03 2015 UTC,What's a jogo?
hardware,3ctimr,spencer32320,2,Fri Jul 10 23:57:07 2015 UTC,It was supposed to be mobo as in motherboard. But the correction tool on my tablet corrected it to jogo wich means game in portuguese.
hardware,3ctimr,americosg,1 point,Sat Jul 11 02:15:51 2015 UTC,"Really? Hasn't been a big issue for me in years. I mean the G-connector is certainly convenient, but its still not a huge hassle without it."
hardware,3ctimr,Stingray88,3,Fri Jul 10 22:54:46 2015 UTC,Wish more reviews would ding reviews for Killer NICs. Got a Killer and a Intel NIC on my gigabyte board. Killer NIC always uses more CPU then Intel.
hardware,3ctimr,DeadlyDuckie,2,Sat Jul 11 10:50:10 2015 UTC,When do 100 series motherboard become available in general? Around August?
hardware,3ctimr,behrangsa,3,Sat Jul 11 08:20:53 2015 UTC,Can we expect built-in Thunderbolt 3 on Z170 boards? I would think that would only appeal to those looking to go with the upcoming successor of X99.
hardware,3ctimr,Wusiji_Doctor,6,Fri Jul 10 18:53:12 2015 UTC,"Gigabyte has had Thunderbolt options in Z77, Z87 and Z97. You can definitely expect it in Z170.   Especially now that everything under the sun is being merged into Type C, it wouldn't make sense not to include it."
hardware,3ctimr,Stingray88,2,Fri Jul 10 19:47:10 2015 UTC,"Oh! That's right. Should've looked into that. I was gonna say it'd be a big price premium to include it, because look at the price of Thunderbolt drives, but apparently not.  What I think is even more interesting, though, is that I'm not finding any X79 or X99 boards with Thunderbolt on board. Seems you need an add-in card for those, but not for the Z77-Z97 boards. Ouch."
hardware,3ctimr,Wusiji_Doctor,2,Fri Jul 10 20:30:42 2015 UTC,"It started out as being insanely pricey on the Z77 and Z88, it's a bit less so for Z97. But I can see the Z170 boards with Type C really making Thunderbolt finally brush the mainstream, which is obviously Intel's goal. It'll never be completely mainstream, as most people simply don't need it. But it'll definitely get cheaper."
hardware,3ctimr,Stingray88,2,Fri Jul 10 20:40:39 2015 UTC,Does thunderbolt 3 require the the PCIe lanes are dedicated to it or can it share with a PCIe slot?
hardware,3ctimr,Dark_Crystal,2,Fri Jul 10 22:31:59 2015 UTC,That's a good question. I'm not sure where the lanes for Thunderbolt come from to be honest.
hardware,3ctimr,Stingray88,2,Fri Jul 10 22:53:21 2015 UTC,"That board seems to be shoving lots of lanes about, wonder if they are using a multiplex chip or 2"
hardware,3ctimr,Dark_Crystal,1 point,Fri Jul 10 23:29:56 2015 UTC,2011-v3 CPUs have more lanes than X97 CPUs.
hardware,3ctimr,Dippyskoodlez,2,Mon Jul 13 01:46:14 2015 UTC,Someone talk me out of wanting LED trace path's on my next motherboard.
hardware,3ctimr,PMmeyournavel,6,Fri Jul 10 19:33:10 2015 UTC,"I thought it was just for the sound ""firewall"" thing.  If they have one with all of it done, I'd never look at my screen again, just my case window."
hardware,3ctimr,Maysock,1 point,Fri Jul 10 19:54:19 2015 UTC,Yeah it would be crazy if you took the concept of a Corsair K70/K95 RGB or similar RGB keyboard and created a motherboard that had programmable RGB LED's all over it.
hardware,3ctimr,aziridine86,2,Fri Jul 10 23:03:05 2015 UTC,Isn't the MSI godlike exactly that?
hardware,3ctimr,americosg,1 point,Sat Jul 11 02:16:55 2015 UTC,"This is the first I've heard of it, but yeah it looks pretty good.  It does seem to already have a lot of parts painted red (as would be expected for MSI) so its not like you could completely customize the color scheme.   It would interesting if you had the PCB, caps, heatspreaders, etc. all just pained black (or alternatively white) and then had the RGB's generate the secondary color.  So you could use it to do a black and yellow or a white and purple color scheme for example. But perhaps it wouldn't look very striking if all the heatspreaders were just black."
hardware,3ctimr,aziridine86,2,Sat Jul 11 03:03:30 2015 UTC,Yeah.With that red profile the RGB has no point.
hardware,3ctimr,americosg,3,Sat Jul 11 03:15:40 2015 UTC,You want it. You need it in your life. Submit to the glowy goodness!
hardware,3ctimr,kr239,1 point,Fri Jul 10 21:02:40 2015 UTC,I'll keep my X99 deluxe. Doesn't look like I'll be missing out on anything...
hardware,3ctimr,hdshatter,1 point,Sun Jul 12 09:54:01 2015 UTC,4 dimm slots :( booo the nics are trash and the memory density is somewhat limiting I guess there won't really be any workstation boards for this chipset.
hardware,3ctimr,jdw101,-1,Mon Jul 13 15:36:19 2015 UTC,"Gigabyte: ""Look, we can be ASUS, too!"""
hardware,3ctimr,zoson,1 point,Fri Jul 10 19:58:03 2015 UTC,I have purchased 2 gigabyte motherboards but can't get any of their fan software/utility applications to work consistently or properly. Their HD BIOS is junk too. I don't know why more people don't complain about these things when it comes to motherboard purchases.
hardware,3ctimr,severinj,2,Fri Jul 10 21:53:27 2015 UTC,I've always had stellar experiences with Gigabyte motherboards... couldn't complain a thing honestly.
hardware,3ctimr,Stingray88,3,Fri Jul 10 22:55:45 2015 UTC,Do you use their utility apps? I don't seem to be the only one complaining on the internet about their overclocking utilities and fan profile software and I see no fix for why these applications don't work consistently.
hardware,3ctimr,severinj,1 point,Fri Jul 10 23:06:19 2015 UTC,"Windows utilities? No. I run Windows, Ubuntu and OS X on my machine so I set everything as far as overclocking and fan profiles in the BIOS. Everything on that front works great in all three operating systems."
hardware,3ctimr,Stingray88,1 point,Fri Jul 10 23:10:45 2015 UTC,"How are you setting custom fan curves in bios? Gigabyte bios seems to be missing the fan profiles that you can create for each header compared to Asus motherboards. All I see available is the choice for Normal, Perf, silent and manual and when you choose manual you're only able to choose one pwm % modifier for each header.  Edit: From 87 and 97 it seems you have to use their app utilities to set your own custom fan curves. So if you're using another chipset you might have more options."
hardware,3ctimr,severinj,1 point,Fri Jul 10 23:17:20 2015 UTC,This is on a Z77 so I'm sure it's the same. I set it a number of years ago so I couldn't really tell you until I get home from work.
hardware,3ctimr,Stingray88,3,Fri Jul 10 23:31:59 2015 UTC,"Their software is shit-tier.  Both motherboard utilities and video card OCing tools (not total garbage like a lot of the motherboard bundle, but leaves a lot to be desired and does some odd stuff)."
hardware,3csgse,bentan77,21,Fri Jul 10 12:12:43 2015 UTC,They don't mention noise at all. Anandtech says the new Sapphire cooler is crazy quiet.
hardware,3csgse,CarVac,18,Fri Jul 10 14:09:03 2015 UTC,Here's a couple reviews with the latest AMD drivers (Catalyst 15.7)  anandtech  hardocp
hardware,3csgse,wasdzxc963,15,Fri Jul 10 13:12:15 2015 UTC,"HardOCP on Dying Light performance:     Dying Light is a VRAM memory hog, this has been proven multiple times. Both video cards have 4GB of VRAM but one seems to bottleneck more than the other with it in this game.          At maximum settings the ASUS STRIX R9 Fury is being bottlenecked by the VRAM capacity. The GeForce GTX 980 however, isn't being held back so much.          What does this all really mean? It is an example where VRAM capacity of 4GB is already not enough in a game at 1440p. A sign of times to come? Of course we cannot predict, but it is here now in Dying Light.   What? How do they go from stating that both have the same amount of RAM to only one being bottlenecked by the amount? At the same settings.   +_+"
hardware,3csgse,OftenSarcastic,27,Fri Jul 10 14:53:02 2015 UTC,HardOCP   move along
hardware,3csgse,TehRoot,7,Fri Jul 10 14:53:59 2015 UTC,What's wrong with HardOCP?
hardware,3csgse,TehRoot,9,Fri Jul 10 17:41:08 2015 UTC,They have a well known nVidia bias.
hardware,3csgse,w00t692,-14,Fri Jul 10 18:12:31 2015 UTC,No they don't you fool.   Jesus christ.
hardware,3csgse,rationis,10,Fri Jul 10 18:26:04 2015 UTC,"Jess Christ, yes, they do"
hardware,3csgse,w00t692,-10,Fri Jul 10 19:19:57 2015 UTC,Honesty doesn't mean it's biased.   You should learn the difference.  The 980 has been out since October last year.   That simple fact makes this a non starter for many...
hardware,3csgse,rationis,21,Fri Jul 10 19:42:30 2015 UTC,"Just this year (March) he did another review of the ROG GTX 980 Platinum that retailed for nearly the price of a 980Ti ($639) and gave it his golden enthusiast seal of approval even though in some games the nearly 2 year old 290X was only 10% slower but $240 cheaper.  I'd hate to hear that many people paid for that card. Link  Then he does a review of the 390X and 980 during which the 390X actually wins a couple times at 1440p and if the 980 wins, its usually by a laughable margin. At 4K its not really a competition, the 390X handily outperforms the 980. And just for kicks, it ran cooler than the 980. He gave it a silver award. Link  He also tests the 290 against the 970, and at the conclusion, points out that it has nearly the same performance as the 970 but for $100 less. He gives it a silver award. Link He also tested the 970 agaisnt the 290X and 290 last year. Sometimes the 970 won, sometimes the  290X won, and once, so did the 290. the 970 ran a little cooler, but it had to run it's fans harder. Yet he declared the 970 stood out among the 3 cards and slapped his golden award on it. Link  Editsies for linkys"
hardware,3csgse,w00t692,-7,Fri Jul 10 21:04:08 2015 UTC,"Do you understand why the 390x is reviewed this way? It came out 9 months after the 980 and merely manages to trade blows. The rog 980 was out before the ti or the Titan X so price is reasonable based on liquid cooling, 550 base price and it overclocked very well. You know what AMD's mantra should be? Its not good now, but wait til our next driver! I'm tired of hearing it."
hardware,3csgse,Sayburirum,4,Fri Jul 10 22:46:01 2015 UTC,"They aren't saying one is bottlenecked and the other is not. They are saying one is more bottlenecked than the other.   Dying Light is a VRAM memory hog, this has been proven multiple times. Both video cards have 4GB of VRAM but one seems to bottleneck more than the other with it in this game.          At maximum settings the ASUS STRIX R9 Fury is being bottlenecked by the VRAM capacity. The GeForce GTX 980 however, isn't being held back so much.   GTX 980 is still bottlenecked, just not as much.  Probably have something to do with HBM and Dying Light."
hardware,3csgse,OftenSarcastic,5,Fri Jul 10 15:27:22 2015 UTC,"one is more bottlenecked than the other.   Which makes no sense if they have the same amount of onboard RAM and the same access to system RAM.  I don't see how HBM could make a difference.   It seems more likely that one of the settings they disabled is the reason for the switch in performance lead in the other two Dying Light tests. Edit: And if you look at the frame graph it follows the same overall pattern for both cards, the Fury is simply lower in their first test. No indication of extra frame drops that should show up if it was more bottlenecked by RAM."
hardware,3csgse,Mr_s3rius,6,Fri Jul 10 15:40:39 2015 UTC,"No indication of extra frame drops that should show up if it was more bottlenecked by RAM.   Not necessarily. The graph probably shows one data point per second. So micro stuttering might not show up. On the other hand, they didn't mention any micro stuttering.  The RAM could be a bottleneck even if both cards have the same amount. AMD and Nvidia likely handle memory management differently which could lead to any kind of results. Maybe AMD sacrifices some space efficiency for optimal read performance. Maybe the memory allocator has to work harder when the card is near its limit. Maybe its even just a certain usage pattern that Dying Light exhibits.   But there's not really enough info to say for certain why the Fury can't run at full speed without lowering settings."
hardware,3csgse,KoreanGirlLoL,2,Fri Jul 10 16:07:27 2015 UTC,If Fury is stronger than 980 and both are capped at 4k in dying light the fury is most likely being bottlenecked more...... this meaning it could provide more fps breaking the 4gb limit than the 980 could....  In other words 4gb are ok for what the 980 can do but Fury is powerful enough to drive settings that use more than 4gb
hardware,3csgse,Lanessar,1 point,Fri Jul 10 23:37:03 2015 UTC,4GB is more than sufficient for 1440p. Unless the game is completely unoptimized on the asset usage or you're trying for DSR to 4K or something.
hardware,3csgse,freaky88,1 point,Fri Jul 10 15:13:50 2015 UTC,but meh 800 gb vram
hardware,3csgse,w00t692,1 point,Fri Jul 10 22:00:48 2015 UTC,Drivers? It could literally be for any number of reasons. But they're saying this because it's the 2nd fury card that had issues.
hardware,3csgse,OftenSarcastic,2,Fri Jul 10 18:26:48 2015 UTC,"I agree it could be a lot of stuff, it could even be the way RAM is handled like someone suggested.   The two Fury cards share a lot of characteristics. They share much less with the GTX 980, one of which is RAM amount."
hardware,3csgse,KoreanGirlLoL,1 point,Fri Jul 10 22:40:07 2015 UTC,"Cards manage vram allocation differently, it is possible that HBM is not being handled correctly, especially in titles that existed prior to it"
hardware,3csgse,OftenSarcastic,1 point,Fri Jul 10 23:08:36 2015 UTC,"I get that the cards could have very different ways to allocate RAM and someone else mentioned that as well, but it's really just conjecture until someone shows some memory usage graphs at different settings showing the Fury cards using more RAM overall."
hardware,3csgse,namae_nanka,5,Fri Jul 10 23:53:17 2015 UTC,"The hardocp review is comical where 980 would beat or be at par with fury with gameworks and get pummeled without it.  Dying Light.   Performance jumped up on the ASUS STRIX R9 Fury. The Fury is now 31% faster than the GeForce GTX 980. The setting holding back performance seems to be the NVIDIA Depth of Field in this game. The GTX 980 can render it much better, the Fury not so much.   and Far Cry 4.   The ASUS STRIX R9 Fury is 18% faster than the GeForce GTX 980 at this lower setting.   Gold, Jerry, Gold!  And hardware canucks don't find any performance improvement with the new catalyst whql.  http://www.hardwarecanucks.com/forum/hardware-canucks-reviews/69792-amd-r9-fury-performance-review-20.html"
hardware,3csgse,waregen,-1,Fri Jul 10 15:41:16 2015 UTC,"wow, anandtech dont do 1080p?  I am more amd fanboy than nvidia, but if some other site would do just 1080p I would call bias, to hide the consistent beating that 980 gets from Fury from anything higher than 1080p resolution.   When they dont do 1080p they are hiding edge that 980 got there, though only sad bastards spend $650 on a gpu and play on 1080p    anyway  power consumption is damn impressive, both reviews give difference rather small [47W, 67W], I expected more."
hardware,3csgse,Nixflyn,8,Fri Jul 10 13:48:19 2015 UTC,"though only sad bastards spend $650 on a gpu and play on 1080p   144 FPS, dude."
hardware,3csgse,Luckyduck1337,26,Fri Jul 10 19:35:14 2015 UTC,And it landed right where it was to be expected. Slightly outperforms the 980 but costs slightly more. Doesn't have a lot of overclocking headroom but represents a solid option for those looking to game at 4k.  Not a bad card but sadly doesn't shake things up too much.
hardware,3csgse,Theodoros9,13,Fri Jul 10 12:23:51 2015 UTC,"They also keep talking about how it's filling a market hole, and it is, but I'm just not sure at this price range that matters all that much. These cards are an exercise in excess."
hardware,3csgse,Luckyduck1337,15,Fri Jul 10 13:09:14 2015 UTC,"I tend to agree. If you want to game at 1080/1440 you can easily get by with a 390, 290x or 970. If you want to go 4k then you should be comfortable going all the way with the 980 TI."
hardware,3csgse,BaconatedGrapefruit,8,Fri Jul 10 13:22:55 2015 UTC,"I tend to agree. If you want to game at 1080/1440 you can easily get by with a 390, 290x or 970.    For another year, maybe.   I'd buy the Fury knowing I could probably get a solid 3 years out of it. Maybe even run some 4k on it with some minor tweaks to the setting.   The 390/290x/970 are all 'right now' choices aimed at people who want enthusiast grade hardware but don't want to pay enthusiast grade hardware. The Fury (and by extension the 980) are for Enthusiasts on a hard budget."
hardware,3csgse,atriax,-10,Fri Jul 10 15:03:10 2015 UTC,"What? You realize the fury is going to get raped by new games more than a 390 will right? 4gb of vram is still 4gb of vram no matter how you use it. Once it's full, it's full and there's nothing you can do about it. That's going to be the choking point on this card @ 1440 in the coming year. The 390 has slower clock speed, but 8gb of vram so it won't be choked by HDR textures."
hardware,3csgse,LiberDeOpp,5,Fri Jul 10 19:57:32 2015 UTC,I agree with this 8gb >4gb no matter what and even hbm can't outperform 980ti 6gb capacity. Problem is no buys a 4k monitor and hardware and wants to play under ultra settings and no single gpu can handle that. We just need to forget about 4k until directx12 and new die shrinks.
hardware,3csgse,turtlespace,2,Fri Jul 10 21:53:33 2015 UTC,Hell you don't even need to go that high to comfortably run like 90% of games at nice looking settings and solid framerates. I think people overestimate requirements of most games.   I stuck with a GTS 450 for years until I wanted to play far cry 4.
hardware,3csgse,Luckyduck1337,-1,Sat Jul 11 03:16:17 2015 UTC,"Umm, yeah I guess?"
hardware,3csgse,Theodoros9,0,Sat Jul 11 03:27:32 2015 UTC,Exactly. No one buying a near $600 video card is going to care about $50 anyway. These things go in money is no object computers.
hardware,3csgse,cookwarestoned,8,Fri Jul 10 13:36:35 2015 UTC,I disagree. I have been budgeting since January for a desktop build that will most likely occur at the end of this year/ start of next year. I intend to get the 980ti and would like to save every penny I can.
hardware,3csgse,Seclorum,3,Fri Jul 10 15:27:04 2015 UTC,"These cards are an exercise in excess.   Any card over like 150 bucks is an exercise in excess.   Same thing with any processor over an i3.   The fact that there are huge performance gains to be had by spending more, means everyone has to do their own Price/Performance, calculations to determine if the card they are looking at is right for them and if they are willing to pay X more for Y more performance."
hardware,3csgse,screwyou00,2,Fri Jul 10 23:36:15 2015 UTC,"Based of these numbers do you think the Nano will perform similarly to a Fury, or do you think the Nano will perform a bit better than a 970 (lowest case)/close to a 980 (highest case)? I'm thinking its going to be the latter (970/980 performance), and if it is I think I'll just wait until Article Islands or Pascal before upgrading."
hardware,3csgse,Luckyduck1337,12,Fri Jul 10 12:41:12 2015 UTC,"The nano could be a very interesting card. From what I've read it will have a full core, like the Fury X and 4gb hbm. It should just be the power requirements that are reduced. I'm expecting it to fall closer to the 980 than the 970. It's also difficult to have any idea where this will fall on price. Is this supposed to be a niche product that has high performance at a small form factor or is it supposed to compete with the 980? I don't think AMD would release a product that's significantly less expensive than its current offering but performs on par which leads me to believe it's more of a niche offering.  To be honest this whole AMD fury release cycle is proving to be fairly frustrating as a consumer. If the nano comes out and out outperforms the Fury, you're going to have a lot of frustrated Fury X and Fury owners. Similar to how I'm sure a lot of Titan X owners were ticked off with the surprise 980 TI release. At least the 980 TI seemed like a reaction to the Fury X rather than AMD just slowly dropping cards into each price segment."
hardware,3csgse,greenlepricon,2,Fri Jul 10 13:01:01 2015 UTC,"One theory is that AMD is going to stick a TDP limit on the card, and I assume the power delivery is going to be much more anemic compared to the Fury and Fury X. I'm still expecting a good card, but on a PCB that size I'm thinking the clocks will be significantly down to meet its advertised specs.  On the other hand, there's a chance that it's operating in its sweet spot. We saw that cutting down 1/8th of the CUs of the Fury X only dropped average performance around 8% (per Anandtech's review).  If the Fury really isn't constrained by its CUs, then the Fury might still do great and perform closer to the Fury than the 980 where I think it will end up.  Regardless, I'm not gonna consider one of these cards until overvolting comes into play. I'm hoping those profiles get pushed forward soon and we can really see how AMD competes when each chip is pushed to the max."
hardware,3csgse,tedlasman,1 point,Fri Jul 10 14:32:51 2015 UTC,"Would amd allow an aftermarket pcb for the nano? Then you can basically get a Fury X with an air cooler and DVI, and cheaper to get to a custom loop setup."
hardware,3csgse,greenlepricon,1 point,Fri Jul 10 17:37:55 2015 UTC,"I'm really not sure what AMD wants from board partners right now. I'm hoping that they allow aftermarket PCBs and coolers like with the Fury, but I really don't know what their plan is at the moment. Hopefully we'll see leaks coming out shortly, cause I would love to make a tiny watercooled build if the nano was a good fit for it!"
hardware,3csgse,imoblivioustothis,2,Fri Jul 10 18:27:04 2015 UTC,Titanx owners would be delusional to think it wasn't going to happen
hardware,3csgse,TheImmortalLS,2,Fri Jul 10 15:29:46 2015 UTC,"I personally don't know how they will release a full chip (fury x) with a small cooler. Maybe it can happen with massive downclocks, but if that is the case, then a normal fury would have been better suited from a cost viewpoint."
hardware,3csgse,Exist50,2,Fri Jul 10 14:12:51 2015 UTC,"As this review shows, relatively minor cutbacks to clock speed and voltages produce big results, and that's all that AMD really needs to do to get it within 175w."
hardware,3csgse,TheImmortalLS,1 point,Fri Jul 10 17:19:26 2015 UTC,"They really do, but if they're cutting back, why not do it on a cut die? Less cores means more frequency on the remaining cores with more voltage, ending up with slightly more heat but at a cheaper cost."
hardware,3csgse,Exist50,1 point,Sun Jul 12 03:16:18 2015 UTC,"Well if they plan on selling the Nano for as much as the Fury or more, it doesn't seem so bad for AMD, does it?"
hardware,3csgse,TheImmortalLS,1 point,Sun Jul 12 03:20:39 2015 UTC,I guess not. It would be a very niche market though.
hardware,3csgse,Exist50,2,Sun Jul 12 03:37:02 2015 UTC,"Well that seems to fit well with the card, though. You'll only really want the Nano if you have a particularly small case and want to cram as much power in there as possible."
hardware,3csgse,Logical_Phallusy,1 point,Sun Jul 12 03:38:54 2015 UTC,My guess is similar to the 970. I have a 960 and I'm waiting until next year to upgrade.
hardware,3csgse,monkeybannanna,1 point,Fri Jul 10 12:59:50 2015 UTC,So pretty much a fury x
hardware,3csgse,BosnianMuslimNinja,1 point,Fri Jul 10 13:27:22 2015 UTC,"I would go with AMD just because Nvidia's shoddy business practices.   Man, fuck Nvidia."
hardware,3csgse,namae_nanka,-8,Sat Jul 11 19:17:28 2015 UTC,"Slightly outperforms the 980   If not for nvidia's shenanigans, it's pretty much smoking 980, by upto  30% in some games. And with AMD's drivers, it'd be extended even further.   About 20% better performance for 10% greater price.  http://hexus.net/tech/reviews/graphics/84512-sapphire-radeon-r9-fury-tri-x-oc/?page=12  http://www.eurogamer.net/articles/digitalfoundry-2015-amd-reveals-full-spec-for-cut-down-air-cooled-r9-fury-blog  Sounds pretty good to me."
hardware,3csgse,Luckyduck1337,7,Fri Jul 10 20:23:14 2015 UTC,"What ""nvidia shenanigans"" are you talking about? Also it's on average around 10% faster than the 980 and costs 10% more. That's of course only if we compare it to stock clocked, reference GTX 980s. If you compare it to custom 980s or overclocked 980s, that performance gap drops.   As for the drivers you speak of, I'm assuming you're talking about the 15.7 drivers, these drivers haven't provided the ""23% boost"" that was initially reported. As for the possibility that AMD drivers could mature in the future, the same could be said for Nvidia. I wouldn't put my money behind promises like that from either company.   https://www.youtube.com/watch?v=tTKOBAybsqk"
hardware,3csgse,namae_nanka,-8,Fri Jul 10 20:34:08 2015 UTC,"The one I referenced below in the thread.  https://www.reddit.com/r/hardware/comments/3csgse/sapphire_radeon_r9_fury_trix_4gb_review_bittech/csynz8o   Also it's on average around 10% faster than the 980   Nope. Not at 4k it isn't.   these drivers haven't provided the ""23% boost"" that was initially reported.   lol you do realize that this makes it look even better for AMD, jeez some guys like you here so fucking clueless.   As for the possibility that AMD drivers could mature in the future, the same could be said for Nvidia.   Nope, figure it out why."
hardware,3csgse,Luckyduck1337,4,Fri Jul 10 20:40:11 2015 UTC,"I'm fucking clueless? You seem to have a difficult time understanding performance numbers. Let me break it down for you so it's easier.  What is the average 4k performance difference between a GTX 980 and a Fury? Perhaps use a few sites to avoid possible bias for your calculations.  What driver gains are you talking about? I made an assumption that you were talking about the 15.7 drivers but perhaps you were referring to something else.  https://www.reddit.com/r/AdvancedMicroDevices/comments/3cou1i/furyx_has_23_boost_in_farcry_4_1440p_with_157/?ref=share&ref_source=link  Once you collect your thoughts. You'll likely find that price to performance, the 980, at around $480, and the Fury, at $550, line up. Take it a step further and consider the overclockability of the 980 compared to the Fury and the 980 kind of comes out ahead. That is not to say the Fury is a bad card, it just fills a price/performance gap between the 980 and 980 TI."
hardware,3csgse,namae_nanka,-5,Fri Jul 10 20:57:18 2015 UTC,"If AMD haven't released a performance boosting driver that gives a 23% boost in FC4 and yet Fury is blasting 980 by like 30%(OCed Fury is pummeling it by 40%) in digitalfoundry review at 4k, do you think it reflects well on Fury or not?   If Fury is already beating it without a driver gain, then what'd happen with one that boosts it further?  Cmon, this is enough spoon feeding.  It's not I who needs to collect my thoughts, you need to start thinking.  As for overclocking, as I just told someone else, It won't catch up a 30% deficit, even if you assume a paltry 7% increase for Fury with OC, that's like 39.1% higher than a stock 980. Besides the boost clocks of nvidia cards make their OCs seem way better in comparison than what happens really."
hardware,3csgse,Luckyduck1337,2,Fri Jul 10 21:05:47 2015 UTC,"Wow, these are a lot of made-up numbers. I'm going to go back to discussing hardware with people that live in reality now."
hardware,3csgse,namae_nanka,-3,Fri Jul 10 21:13:35 2015 UTC,"So apparently you missed the 31% faster than 980 in dying light with nvidia DoF turned off?  OCed Fury is already more than 40% faster in Far Cry 4 without a driver update, so make that of what you will.  http://www.eurogamer.net/articles/digitalfoundry-2015-amd-reveals-full-spec-for-cut-down-air-cooled-r9-fury-blog  As I said, fucking clueless and fucking waste of my time."
hardware,3csgse,innerknowing,11,Fri Jul 10 21:16:37 2015 UTC,"Goshdamnit, had to use the 15.5b drivers didn't they."
hardware,3csgse,Sayburirum,9,Fri Jul 10 12:56:16 2015 UTC,"Maybe AMD should've stick with July 14. Would've given reviewers time to test with the new driver. If it does improve performance across the board, it would've made their benches look much nicer.  Edit: seems like it might not matter for 300 / fury series, according to Anandtech when they retested with 15.7, http://anandtech.com/show/9421/the-amd-radeon-r9-fury-review-feat-sapphire-asus/4   On a brief note, since last month’s R9 Fury X review, AMD has reunified their driver base. Catalyst 15.7, released on Wednesday, extends the latest branch of AMD’s drivers to the 200 series and earlier, bringing with it all of the optimizations and features that for the past few weeks have been limited to the R9 Fury series and the 300 series.  As a result we’ve gone back and updated our results for all of the AMD cards featured in this review. Compared to the R9 Fury series launch driver, the performance and behavior of the R9 Fury series has not changed, nor were we expecting it to. Meanwhile AMD’s existing 200/8000/7000 series GCN cards have seen a smattering of performance improvements that are reflected in our results.   Seems like 15.7 boosted older series rather than new 300/fury. Now I want to see another 200 vs 300 series benchmark."
hardware,3csgse,Exist50,0,Fri Jul 10 13:17:56 2015 UTC,Anandtech retested with 15.7.
hardware,3csgse,Sayburirum,3,Fri Jul 10 17:21:19 2015 UTC,Yes. I quoted what they found when they retested.
hardware,3csgse,Stiev0Kniev0,1 point,Fri Jul 10 17:24:30 2015 UTC,Granted these tests take quite a while to run... but I was really hoping to see the july 8th drivers used...
hardware,3csgse,thedrivingcat,7,Fri Jul 10 13:22:42 2015 UTC,Anandtech and Hard used the latest drivers... probably why they've been around for almost two decades.
hardware,3csgse,Stiev0Kniev0,1 point,Fri Jul 10 14:31:39 2015 UTC,Thanks! I'll give those benchies a look :)
hardware,3csgse,Yearlaren,5,Fri Jul 10 15:02:09 2015 UTC,"I can see people choosing this card over the GTX 980.  The GTX 980 has the advantage of working on less powerful PSUs, though."
hardware,3csgse,milkteabear,3,Fri Jul 10 17:39:53 2015 UTC,"Yeah I see people choosing it also, but mostly due to the HBM.  I feel like the difference between the Fury and 980 isn't as big, since they only use referenced 980s when benching it."
hardware,3csgse,Yearlaren,-2,Fri Jul 10 18:07:05 2015 UTC,"Well, it doesn't make a lot of sense to compare non-reference cards. What you want to do is compare reference cards, but as far as I know there aren't reference Fury cards."
hardware,3csgse,Perunsan,2,Fri Jul 10 18:12:43 2015 UTC,"i don't see the length of the card written ....  edit : 12 "" / 304 mm"
hardware,3csgse,TheImmortalLS,1 point,Fri Jul 10 13:13:23 2015 UTC,"I'm guessing 320mm, from the sapphire nitro. The cooler looks similar. That's all I'm basing it on."
hardware,3csgse,Perunsan,1 point,Fri Jul 10 14:13:40 2015 UTC,"it's actually 12"" / 304 mm found it in a video review"
hardware,3csgse,Seclorum,1 point,Fri Jul 10 14:14:32 2015 UTC,It was also prominently displayed on the box art from yesterday.
hardware,3csgse,stereosteam,2,Fri Jul 10 23:28:13 2015 UTC,"Yeah, looks like I'm getting this baby for 1440p."
hardware,3csgse,DrexelDragon93,-28,Fri Jul 10 16:04:52 2015 UTC,"Will AMD ever release a card that would compel to switch from Nvidia?  Edit: Downvotes from AMD shills, figures. Please look at the top graphics card in each generation for the past several years. They have ALL been NVIDIA."
hardware,3csgse,PC4U,30,Fri Jul 10 12:36:34 2015 UTC,"390 beats the 970 at the $330 pricepoint. 280 & 380 beat the 960 at the $200 pricepoint. 370 beats the 750Ti at the $150 pricepoint.  AMD sucks with high end cards, but their mid-tier cards are extremely competitive."
hardware,3csgse,everyZig,7,Fri Jul 10 12:44:47 2015 UTC,"The 370 and 750Ti are pretty close though, the 370 is a rebadged 265 rather then a 270(x)"
hardware,3csgse,micbael,2,Fri Jul 10 12:55:05 2015 UTC,"In that case, 270 beats 750ti at $150 price point."
hardware,3csgse,XorFish,1 point,Fri Jul 10 16:45:52 2015 UTC,Last time I looked you could get a 750ti for 110$
hardware,3csgse,micbael,1 point,Fri Jul 10 17:35:15 2015 UTC,€140 over here.
hardware,3csgse,XorFish,3,Fri Jul 10 17:36:48 2015 UTC,"In Europe, everything is more expensive.  The cheapest  270 is 163€ , 24€ more than the cheapest 750 ti.  So the 270 is 17% more expensive, as is the r7 265.  review of the 750 ti    Game 750 ti r9 270 r7 265    Arma 3 100% 138% 122%   Assassin's Creed: Black Flag 100% 130% 118%   Battlefield 4 100% 137% 126%   BioShock Infinite 100% 115% 105%   Far Cry 3 100% 138% 123%   Grid 2 100% 137% 118%   Metro: Last Light 100% 127% 115%   Average 100% 131% 118%    So for someone from Europe the best performance per € is the r9 270 followed by the r7 265/750 ti. If you want to take the power consumtion into the consideration then the 750 ti will make up the difference there. If you game 10 h per week on average you will pay around 12€ per year more for the r9 270 in Germany."
hardware,3csgse,LiberDeOpp,2,Fri Jul 10 19:05:52 2015 UTC,"Amd has always won on price/ performance though, its probably why amd has such a terrible business management. If amd had furyx out before 980ti they would have stole the 980ti crowd. Then the fury regular would have been a bargin for those looking for cheaper hbm now. Instead nvidia has control to drop price because they have already been making full msrp on their cards for a while.  Nvidia is a business that happens to sell graphics cards. Amd is a graphics card maker trying to figure out business."
hardware,3csgse,DrexelDragon93,2,Fri Jul 10 22:07:36 2015 UTC,"Oh definitely they are competitive in the mid range, but I'm talking high end."
hardware,3csgse,YellowCBR,1 point,Fri Jul 10 12:53:12 2015 UTC,R9 290 destroyed Nvidia. $400 card beating their $650 GTX 780.  And without AMD the 780Ti and 980Ti would've never existed.
hardware,3csgse,Theodoros9,2,Fri Jul 10 17:15:00 2015 UTC,"Competitive, but the Nvidia options still have a fairly big power consumption advantage. I know people don't really care about that, but it adds up over the life of the card.   I was rooting for AMD here, as I think nvidia are getting lazy, but I just don't think being 'good enough' is enough to beat them."
hardware,3csgse,PC4U,12,Fri Jul 10 12:46:29 2015 UTC,"Let's do some math.  The 290 costs anywhere from $240 (on sale) to $300 on sale.  On the other hand, the 970 costs anywhere from $295 (on sale) to $330+.  The 290 is only 1% to 2% behind the 970 in terms of gaming performance, but costs anywhere from $90 cheaper to $5 more expensive.  The 275W TDP for the 290 will run you roughly $10 per year on electricity.  Assuming that you actually go out of your way to purchase a 290 at a decent price, you would have to run your 290 for like 9 or so years for the cost of power consumption to exceed over the cost of the 970.  For a 1% to 2% decrease in gaming performance, saving the $50 sounds like a pretty good deal.  People honestly think too hard into TDP.  Yes, nVidia cards are much more efficient thanks to the Maxwell architecture, but don't discount AMD cards just for their TDP."
hardware,3csgse,Theodoros9,2,Fri Jul 10 13:36:54 2015 UTC,"You can't even buy 290s in Australia any more. What is the equivalent 3xx card? A 390 here is a lot more expensive, I can get a 970 for $50 cheaper."
hardware,3csgse,PC4U,3,Fri Jul 10 14:00:52 2015 UTC,"Sorry, I am a US resident, so that is what I'm basing my argument on.  Yes, if the 970 is cheaper than the 390, get the 970 over the 390."
hardware,3csgse,Theodoros9,1 point,Fri Jul 10 14:03:34 2015 UTC,"Is the 390 cheaper in the U.S.? maybe Australia has weird prices, but here the cheapest 970 is $440 and that's a reference card and the cheapest 390 is $520 so that's a fairly big difference. The 390 is slower too."
hardware,3csgse,PC4U,4,Fri Jul 10 14:06:52 2015 UTC,"The 390 and 970 both sit at $330 USD, which gives the 390 a clear edge in price/performance."
hardware,3csgse,YennoX,-4,Fri Jul 10 14:07:50 2015 UTC,You forgot that 100W upgrade to the PSU. Or possibly an extra case fan or two to stop everything else from baking.
hardware,3csgse,PC4U,3,Fri Jul 10 13:47:39 2015 UTC,"That is true.  Those upgrades will be needed, but its not to say that you can't spend the $50 you save to get those well needed upgrades."
hardware,3csgse,Hariooo,5,Fri Jul 10 13:51:44 2015 UTC,You can pay for it with the money you save by buying Freesync over Gsync
hardware,3csgse,itguyy,0,Fri Jul 10 14:33:09 2015 UTC,"nVidia also has the advantage of developers using their software for game dev, and their drivers are on point most of the time. They also have shield and some other features that beat AMD."
hardware,3csgse,Exist50,4,Fri Jul 10 13:54:25 2015 UTC,Please look at the top graphics card in each generation for the past several years. They have ALL been NVIDIA.   290x and 7970 not worth a mention?
hardware,3csgse,Wu-Tang_Flan,6,Fri Jul 10 17:22:28 2015 UTC,Very few people care about who makes the fastest halo card each generation because they're always a bad deal. And I'm honestly shocked that people are still taking sides and picking a favorite company. Get a real hobby.
hardware,3csgse,DrexelDragon93,-7,Fri Jul 10 14:43:31 2015 UTC,What? I'm not taking sides. The only side I take is the one that has the best performing GPU (price notwithstanding). I desperately wanted the Fury X to come out and kick Nvidia's ass but it didn't.
hardware,3csgse,Wu-Tang_Flan,5,Fri Jul 10 14:44:58 2015 UTC,I disagree. I glanced at your posting history and see you getting in flame wars about who the best and worst phone makers are. You have no life.
hardware,3csgse,DrexelDragon93,-9,Fri Jul 10 14:46:22 2015 UTC,"I see you are taking things out of context. Again, I debate which phones are the best, I don't choose only certain companies to support. In the past year, I have owned LG, Samsung, Motorola, OnePlus, and HTC phones. I simply try everything and pick the best.  I have a great life thank you, I don't need your approval :)"
hardware,3csgse,smoothsensation,2,Fri Jul 10 14:47:58 2015 UTC,You get a new phone every two months?
hardware,3csgse,DrexelDragon93,-2,Fri Jul 10 15:39:24 2015 UTC,Pretty much.
hardware,3csgse,Darksider123,4,Fri Jul 10 15:41:01 2015 UTC,And yet you dont see how stupid that sounds? Or are u just being sarcastic?
hardware,3csgse,DrexelDragon93,-6,Fri Jul 10 15:53:31 2015 UTC,How does that sound stupid? What is wrong with changing phones and trying new phones as they come out?
hardware,3csgse,Wu-Tang_Flan,3,Fri Jul 10 15:56:15 2015 UTC,Ever heard of mindless consumerism? Buying shit doesn't really count as a hobby.
hardware,3csgse,smoothsensation,3,Fri Jul 10 15:59:21 2015 UTC,295X2??
hardware,3csgse,DrexelDragon93,2,Fri Jul 10 15:38:03 2015 UTC,Dual GPU.
hardware,3csgse,ryno9o,3,Fri Jul 10 15:38:34 2015 UTC,"Two die, but its still a single card. And crossfire scaling wipes the floor with anything SLI."
hardware,3csgse,atriax,0,Fri Jul 10 18:21:08 2015 UTC,Doesn't matter how many cards it is. It's still two GPU's and most people don't want to deal with that kind of thing. I hate crossfire/SLI. I will avoid it all costs.
hardware,3csgse,JakSh1t,6,Fri Jul 10 20:01:21 2015 UTC,You're not being downvoted by AMD shills. You're being downvoted because your comment contributes nothing to this discussion.
hardware,3csgse,bdt13334,1 point,Fri Jul 10 14:56:44 2015 UTC,Don't think I've seen that since the 7970 GHz edition. Man that card was amazing for the time.
hardware,3csgse,bphase,8,Fri Jul 10 12:39:59 2015 UTC,"Meh, the original 7970/7950 were pretty nice, GHz edition not so much IIRC. R9 290 custom models were great as well for the awesome price/perf, at least until the miners gobbled them all up and the prices skyrocketed.  But the last time they put out a REALLY good card was with the 58xx series. Those were able to dominate freely until the release of the disappointing and greatly delayed GTX 480. Alas, that was almost 6 years ago."
hardware,3csgse,namae_nanka,0,Fri Jul 10 12:44:46 2015 UTC,"AMD seem to have lost their way with the VLIW4 69xx series, should've gone with GCN sooner especially considering the advantage they have compared to kepler in feature levels and something that dx12 would exploit."
hardware,3csgse,Yatlol,1 point,Fri Jul 10 15:45:10 2015 UTC,390 is better than the 970. 380 is better than 960.
hardware,3csgse,waregen,1 point,Fri Jul 10 15:59:28 2015 UTC,"It did, many times. But wait for DX12 games and the raw power of TFlops showing whats what."
hardware,3cshif,kharner92,9,Fri Jul 10 12:21:39 2015 UTC,So its 549$ and benchmarks fall between the 980 and 980 TI?  That's a pretty good offer.
hardware,3cshif,TheDudeBeto,1 point,Fri Jul 10 12:44:35 2015 UTC,technically $569 for the sapphire.  prices should be close to reference price soon though.
hardware,3cshif,logged_n_2_say,6,Sat Jul 11 13:57:14 2015 UTC,"I really, really, really like the air pass through from having the short pcb."
hardware,3cshif,CarVac,2,Fri Jul 10 12:46:44 2015 UTC,That would be good for an matx crossfire setup.
hardware,3cshif,BlayneTX,1 point,Fri Jul 10 20:53:31 2015 UTC,"I thought I wanted the Nano, but dammit, that's one efficient cooler! And the newer pc towers are made with huge GPUs in mind anyway, so this solution is golden for people like me who loves good airflow."
hardware,3cshif,Mechdra,2,Mon Jul 13 13:18:06 2015 UTC,That was my impression. I want it just because it's a super quiet card more than any other reason.   Just waiting for the Linux drivers to hit...
hardware,3cshif,CarVac,1 point,Mon Jul 13 18:03:55 2015 UTC,Decent value.
hardware,3ct7uh,ryerocco,3,Fri Jul 10 16:08:49 2015 UTC,"What about RAID controllers? Onboard, yes or no?"
hardware,3ct7uh,bobbokelso,5,Fri Jul 10 16:29:09 2015 UTC,"a raid controller typically has more configurability and slightly faster performance. Note that Motherboard RAID can be good, but you need look at it on a case by case basis. Some mobos have good RAID controllers, some have terrible ones. Check out reviews for that specific model to know for sure."
hardware,3ct7uh,Cozmo85,2,Fri Jul 10 17:14:07 2015 UTC,Most all of them are the same Intel raid now if you are Intel based.
hardware,3ct7uh,spellstrike,1 point,Fri Jul 10 20:14:52 2015 UTC,also look at offboard reviews...
hardware,3ct7uh,cp5184,1 point,Fri Jul 10 17:38:54 2015 UTC,Isn't most (99.9%) of motherboard raid software raid?
hardware,3cr7gk,zmeul,31,Fri Jul 10 03:06:19 2015 UTC,I really really like these guys format for displaying frame times. It helps to show issues with a setup that a straight Min-Average-Max breakdown doesnt.   It's interesting that the 2 and 3 card scaling for AMD is better than Nvidia in this round of testing. In cases even to the point where 2 AMD cards surpass two Nvidia cards in raw performance and have better frame times for smoother gameplay!   Now if only they could stay on top of driver updates for day 1 releases...
hardware,3cr7gk,Seclorum,32,Fri Jul 10 03:59:06 2015 UTC,The new method of crossfire is simply better than the old way of using sli or crossfire bridges
hardware,3cr7gk,dylan522p,5,Fri Jul 10 04:14:26 2015 UTC,"Makes me wonder why they couldn't develop a faster 'bridge' solution rather than just shunting it through the PCIe bus.   Then again, I guess Nvidia is doing that with their NVlink tech last I heard."
hardware,3cr7gk,Seclorum,8,Fri Jul 10 04:40:40 2015 UTC,Pci is economical and its basically the biggest bus you have and you already have it on the chip implemented. The thing I wanna know is what happens on cards like the 295x. Is it literally sending to Pci then onto the same card again or what are there traces from both the GPUs to the other. A plx chip on board making lanes on the pcb?
hardware,3cr7gk,dylan522p,7,Fri Jul 10 04:55:18 2015 UTC,"Yep it's a PLX across bridging the two GPUs. When you add another 290X or 295X2, then the GPUs are connected with both an onboard PLX and onboard XDMA (and I'm not entirely sure how that works). You are definitely right that both of these are better than the crossfire/sli fingers we're used to seeing, and I hope Nvidia takes this route too."
hardware,3cr7gk,greenlepricon,-10,Fri Jul 10 05:22:30 2015 UTC,My personal idea would be to implement traces interconnecting the GPU's straight on the PCB itself. But It would not surprise me at all if it throws the interconnect through the PCIe bus.
hardware,3cr7gk,Seclorum,4,Fri Jul 10 05:21:42 2015 UTC,Amd has pretty much always had better scaling than Nvidia since the HD5000 series the last I recall.
hardware,3cr7gk,veyron3003,17,Fri Jul 10 13:07:01 2015 UTC,The Fury X and Crossfire frame times in GTA V seem practically unplayable.
hardware,3cr7gk,BlayneTX,27,Fri Jul 10 03:46:36 2015 UTC,GTA V is quickly becoming a hot mess with every new patch.  It's ridiculous.
hardware,3cr7gk,AMW1011,7,Fri Jul 10 03:57:41 2015 UTC,"SLI doesn't seem pretty either, though a bit better. Still, GTA does not look like a game one would want to use a multi-GPU configuration for, or at least not more than 2 GPUs."
hardware,3cr7gk,Exist50,11,Fri Jul 10 08:45:52 2015 UTC,"Why on earth are they using a Sandy Bridge-E processor on their test bench? As NCIX and JayzTwoCents recently demonstrated, multi-GPU configurations at this performance level are (purportedly) bottlenecked considerably by even an overclocked 5960X. You would have to think a 3-way SLI/Crossfire setup of this magnitude on a 4-year-old CPU would skew the results to a degree."
hardware,3cr7gk,Oafah,-2,Fri Jul 10 11:34:43 2015 UTC,"Honestly, 99% of users are still using a processor that is SLOWER than one we used in my story here. Also, in my experience, and based on another story we posted and pcper.com recently, I would be surprised if there was a measurable different between SNB, IVB and HSWE. Do you have a link to this NCIX/JayZ story?"
hardware,3cr7gk,inappropriatecontext,12,Fri Jul 10 12:56:00 2015 UTC,99% of users also aren't running tri-fire fury x's. Your point is kind of moot there.
hardware,3cr7gk,jamie1414,4,Fri Jul 10 14:12:50 2015 UTC,"The video Jay did was the one he shot just after upgrading his skunkworks machine. He claims (and I'm not necessarily saying I agree with him) that the 5960X is being taxed too heavily to make use of the Tri-SLI setup.  NCIX's video doesn't actually mention the 5960X as being the problem, but they do detail significant issues at triple-4K resolution, and the problem clearly isn't the limited GPU horsepower of the GM200. I can think of a few other limitations that might serve as a bottleneck here, but you'd have to at least consider the possibility that the CPU is one of them.  Long story short, I'm not necessarily saying that a Sandy Bridge-E CPU is going to be a problem. What I'm saying is, as a good scientist, why would you leave it to chance? The goal of any experiment is to uncover the truth, and you do that by placing as many controls on the experiment as possible in order to yield the most accurate results.  Also, it should be noted that this issues arose at triple 4K resolutions, but wouldn't that be the logical viewing progression of someone spending $4000 on graphics cards? You don't get 3 980 TIs to run games at 1440p.  Anyhow, not trying to be too critical here, but I thought it would be prudent to point out."
hardware,3cr7gk,Oafah,-1,Fri Jul 10 13:12:41 2015 UTC,"I think you'll find that the real ""bottleneck"" is in the software - whether it be drivers or the game engine. But it's worth checking out."
hardware,3cr7gk,inappropriatecontext,-2,Fri Jul 10 13:46:13 2015 UTC,"Jay is wrong. For it to be a bottleneck it would mean that adding extra GPU power wouldn't do anything because the CPU is holding it back. Quite clearly this isn't the case in many games & the ones where there seems to be a bottleneck could simply be due to bad optimization for multi-gpu. In short, he hasn't proved anything.  But I agree that the tests should be done with a 5960X (too)."
hardware,3cr7gk,Trollatopoulous,5,Fri Jul 10 13:25:10 2015 UTC,"Bottlenecking is not necessarily linear in that regard, which is a big reason people misunderstand the phenomenon. You can add a very strong graphics card to a weaker CPU and still see a performance boost over a weaker graphics card. If you'd like, I can set up a test example tonight and post the results."
hardware,3cr7gk,Oafah,0,Fri Jul 10 13:27:28 2015 UTC,"In don't think people misunderstand it, I think it's more a matter of semantics. But don't bother, save your time for something more useful/fun. :)"
hardware,3cr7gk,Trollatopoulous,31,Fri Jul 10 14:18:45 2015 UTC,"It's funny how reluctant pcper is to give AMD the kudos it deserves. 'Yeah AMD does multi-gpu better but OMGTHISSUXWTF DON'T DO IT UNLESS YOU'RE B R A V E !!""  I don't remember seeing this kind of reluctance in any article where Nvidia would be favoured. Heh.  Still, good on them for doing the work at least."
hardware,3cr7gk,Trollatopoulous,16,Fri Jul 10 05:17:04 2015 UTC,It's funny how reluctant pcper is to give AMD the kudos it deserves.    Every time I see that bias suggested I remember that pcper.com was originally amdmb.com back in the day.  I laugh a bit.
hardware,3cr7gk,jpmoney,-7,Fri Jul 10 11:16:44 2015 UTC,and bing once was popular. governments didn't spy on every citizen. I was a baby. I'm not saying they are biased though because i often don't read their articles because reading the comments most of the time seems to be enough. But just because something once was doesn't mean it still is. especially because I assume much of their staff has changed since.
hardware,3cr7gk,soidboerk,3,Fri Jul 10 12:17:39 2015 UTC,"Actually, just a note here, I clearly stated that: ""I am still usually in the camp that CrossFire and SLI are for the bravest among our ranks"" which includes both NVIDIA and AMD as far as I can tell. No?"
hardware,3cr7gk,inappropriatecontext,6,Fri Jul 10 12:58:12 2015 UTC,AMD just needs to keep on top of Crossfire profiles for new games. They have a tendency to lag behind releases whereas Nvidia gets them out day one or day two.   Nvidia looks to be going the route of making the multi-GPU interconnects simply faster through NVlink tech. Will be interesting to see just how it's going to work and what's required to make it work.
hardware,3cr7gk,Seclorum,15,Fri Jul 10 05:25:31 2015 UTC,"NVlink is not an SLI replacement. It's for compute/server use, and only works on specific IBM Power8 platforms."
hardware,3cr7gk,Exist50,1 point,Fri Jul 10 08:48:27 2015 UTC,"Exactly, NVLink is supposed to use a mezzanine connector, so you'd have to buy a board and processor that supports NVLink, likely in place of PCI-e.  MAYBE a cut down version that works through a modified PCI-e slot, but I doubt it.  Could you imagine that shitstorm considering the hate nV gets for PhysX? Since an nVLink board would basically be locked to nV GPUs."
hardware,3cr7gk,Fragarach7,1 point,Fri Jul 10 14:24:47 2015 UTC,I can so see them doing that anyway.
hardware,3cr7gk,Seclorum,8,Fri Jul 10 22:37:34 2015 UTC,"I think that unless DX 12 resolves the issue through software Cf/SLI will be the same as always, simply because the market for them is way too small and the necessary resources to get it to parity with single-gpu are too huge (if not impossible)."
hardware,3cr7gk,Trollatopoulous,6,Fri Jul 10 05:30:30 2015 UTC,"Well a lot of DX12 is putting more power in the developers hands...  So rather than needing constant driver updates from AMD/Nvidia, Engine and Game Devs would need to code it correctly in the first place.  And DX12 also looks to try and take advantage of IGPU's sitting idle in many boards... so again meaning Game and Engine Devs need to stay on top of things from the outset and not rely on MFG Drivers to fix things later on.   Perhaps it will go back to the old days where Driver 'optimizations' were anathema, and it was all up to Game and Engine devs to be the harbingers of performance increases as they fixed bugs and optimized their code properly in the first place."
hardware,3cr7gk,Seclorum,2,Fri Jul 10 05:45:52 2015 UTC,"These improvements will only come in native DX12 titles. I'm not sure how much of a fix you can expect from 'patched' DX11 titles, as I frankly don't know. (This consequently makes Windows 10 adoption very important)  I read through OC3D.net's take on DX12, and they had a rather pessimistic view on developers spending the resources to fully utilize SFR and take full advantage of pooled memory.   With DirectX 12 developers can specifically allocate tasks to a specific GPU, allowing them to control the GPU hardware in such a way that each GPU could use their full frame buffer independently, meaning that 2 4GB cards in SLI or Crossfire could create a setup with 8GB of effective GPU memory.  Right now I do not see many developers taking this approach to crossfire as the work required to assign tasks in such a way will not be cost effective as not many people use Multi GPU setups, so the effort would likely be placed in an area where more of an impact on sales could be achieved, perhaps improving the graphical fidelity or otherwise trying to improve the games framerate in other ways."
hardware,3cr7gk,terp02andrew,2,Fri Jul 10 12:42:50 2015 UTC,Doubt we will ever see NVLink in a consumer product.
hardware,3cr7gk,gizza,2,Fri Jul 10 09:17:15 2015 UTC,"I've always heard their bias against multi GPU of any type, at least on the podcast."
hardware,3cr7gk,Darius510,2,Fri Jul 10 11:42:58 2015 UTC,"The primary issue with Fury x-fire, is that it only has 4GB of memory. The only reason to need 2 of those, is if you want to use it for 4K, and at that resolution 4GB is already pushing it, which means it will become obsolete quickly."
hardware,3cr7gk,DrBoomkin,1 point,Fri Jul 10 11:41:58 2015 UTC,Or 144hz 1440p gaming
hardware,3cr7gk,Teethpasta,3,Fri Jul 10 23:10:08 2015 UTC,"Terrible chart colors. How hard is it to do light red, dark red, light green, dark green?"
hardware,3cr7gk,maybachsonbachs,3,Fri Jul 10 12:55:28 2015 UTC,Or four different colors. Since I'm color blind.
hardware,3cr7gk,BuildYourComputer,7,Fri Jul 10 19:47:20 2015 UTC,Happy to see that Fury X Crossfire might even be the better choice than 980Ti SLI because of comparable framerates and better frametimes. Now if we only could get an answer to that overclocking question...
hardware,3cr7gk,barthw,2,Fri Jul 10 07:35:41 2015 UTC,"Yeah, there's a pretty noticeable difference between a stock 980 and an overclocked one."
hardware,3cr7gk,BuildYourComputer,0,Fri Jul 10 19:45:50 2015 UTC,"I saw even though he said the Fury X had smoother frametimes and framerate that it also how the lowest minimum FPS, kind of odd."
hardware,3cr7gk,continous,8,Fri Jul 10 04:47:30 2015 UTC,"Not really. If you have, say, a million frames, and one drops to super-low levels, that one frame will be your ""minimum"" but have no practical impact on the statistics."
hardware,3cr7gk,SPOOFE,-4,Fri Jul 10 06:41:25 2015 UTC,"That isn't how it works though. FPS is a measurement of how many frames that are being generated per second. That means the sample size (time-wise) can't really go below a second without some really weird process of sampling. With that said, if the framerate drops low enough it will be perceived as a hitch or framedrop(-ping) which become more noticeable as their extremity and frequency increase."
hardware,3cr7gk,continous,9,Fri Jul 10 06:54:44 2015 UTC,"That means the sample size (time-wise) can't really go below a second without some really weird process of sampling.   They absolutely do get sub-second measuring, that's how they measure individual frame times."
hardware,3cr7gk,SPOOFE,-4,Fri Jul 10 07:04:52 2015 UTC,That wasn't what I was talking about though. I was talking about minimum framerates; which is measured in FPS.
hardware,3cr7gk,continous,6,Fri Jul 10 07:28:43 2015 UTC,"Frame times can also give you FPS with basic math, so it's a wash. ~16.something milliseconds is 60 FPS, for instance."
hardware,3cr7gk,SPOOFE,-1,Fri Jul 10 07:58:43 2015 UTC,"If that was the case, why then on their graphs is the AMD card illustrated with an obviously lower minimum FPS, but that isn't reflected in their other data?"
hardware,3cr7gk,continous,5,Fri Jul 10 08:19:36 2015 UTC,"It's reflected on the frame times graph, which is inverse to FPS: Higher frame times give you lower frames per second."
hardware,3cr7gk,SPOOFE,3,Fri Jul 10 08:28:14 2015 UTC,Oh that was my mistake then.
hardware,3cr7gk,continous,3,Fri Jul 10 08:32:48 2015 UTC,"That's precisely the reason I've always advocated for median frame rates. Median is selected from actual data points. Min/Max FPS without context - don't really say much, and many review sites, because they are still using the older bar chart method.  That said, PCPer's frame time graphs are already expressing this, but it'd be easier to comprehend if those charts could be expanded full-screen - think stock charts on either Yahoo or Google Finance."
hardware,3cr7gk,terp02andrew,1 point,Fri Jul 10 12:50:43 2015 UTC,"It was just shady in my opinion, to see a large drop in FPS, but not really seeing a similar drop of the frametime chart. While most of it was me being a tard, it still was sketchy."
hardware,3cr7gk,continous,1 point,Fri Jul 10 19:53:10 2015 UTC,TLDR ?
hardware,3cr7gk,THS89,12,Fri Jul 10 06:59:29 2015 UTC,AMD knows better how to dual-gpu setup. Don't be a fool buying a third card.
hardware,3cr7gk,MilkyTones,1 point,Fri Jul 10 08:36:54 2015 UTC,Supporting that dual gpu with regular updates still needs improvement though.
hardware,3cr7gk,I-never-joke,1 point,Sat Jul 11 08:45:09 2015 UTC,Damn. This is showing the harshest games can barely hold onto 60 FPS @ 4k even with CF/SLI (Crysis/GTA/BF4).   And some of these are pretty low at 1440p as well. That is something I didnt want to see.   How does Gsync/Freesync work in multi GPU configurations?
hardware,3cr7gk,XaeroR35,1 point,Fri Jul 10 15:48:08 2015 UTC,"The same way it works in single GPU configurations if it's supported. AMD's Freesync does work in crossfire configurations, but only the primary monitor will have Freesync in an eyefinity configuration."
hardware,3cr7gk,kennai,0,Fri Jul 10 17:08:02 2015 UTC,Crytek and Dice should license out their gpu optimization software to other developers. Rockstar's is atrocious for example.
hardware,3cr7gk,veyron3003,-31,Fri Jul 10 13:16:14 2015 UTC,"If you're gonna post something, at least write something the article talks about, your thoughts etc..  Anyway, long story short Fury X scales a little better than the 980Ti (95% vs 84%)."
hardware,3cr7gk,bbgun83,13,Fri Jul 10 03:47:20 2015 UTC,OP doesn't have to do anything. OP may have posted it in a hurry or something and figured he/she would post it to share it. You're more than welcome to start the discussion if that's what you'd like.
hardware,3cr7gk,aranurea,14,Fri Jul 10 03:51:34 2015 UTC,"is this a new rule or something?! must I have an opinion on everything now?  I thought this article might interest someone so, I posted it"
hardware,3cr7gk,feelix,3,Fri Jul 10 03:57:17 2015 UTC,NO WE JUST WANT TO KNOW YOUR OPINION
hardware,3cr7gk,continous,2,Fri Jul 10 06:57:47 2015 UTC,"OP did great.   95% vs 84%   This sounds small, I mean after all it is only a 11% difference, right? But take a second right now, go into your favorite overclocking software and underclock your card by 11% and tell me whether or not it is a small difference."
hardware,3cr7gk,Kaghuros,1 point,Fri Jul 10 04:50:24 2015 UTC,"Yeah, 11% is the kind of gain in performance we usually see between two card generations. That's categorically massive among supposed competitors."
hardware,3cr7gk,continous,2,Fri Jul 10 06:41:05 2015 UTC,"Well even just in FPS context, 10% of 60 fps is an extra 6 frames, that's actually pretty significant."
hardware,3cstoc,JitterAtt,2,Fri Jul 10 14:17:23 2015 UTC,Liquid cooling is completely unnecessary if it can't even overclock
hardware,3csgp2,Anergos,2,Fri Jul 10 12:11:37 2015 UTC,"AMD kinda kicked themselves in the ass with R9 Fury - you don't have to deal with all that liquid cooling and it's ~100$ cheaper than Fury X, with comparable performance  this is why AMD chose to separate the launches, people would've got for the air-cooler one hands down"
hardware,3csgp2,zmeul,5,Fri Jul 10 18:56:45 2015 UTC,They kicked themselves by not having proper over clocking controls. Fury x can easily get increased clocks and reviews would have talked about that.
hardware,3csgp2,dylan522p,3,Fri Jul 10 20:32:40 2015 UTC,Why they didn't work with devs to get OC suites updated for the new voltage controller... I'll never understand.
hardware,3csgp2,Seclorum,1 point,Fri Jul 10 20:53:49 2015 UTC,"I don't think AMD allowed user controllable voltages and all Fury cards are equipped with a limited controller  because none of the tools from Sapphire or ASUS, on their own cards, allow voltage control  one other thing to note: Sapphire's OC has the same core voltage as their ""regular"" version"
hardware,3csgp2,zmeul,1 point,Fri Jul 10 21:21:54 2015 UTC,"It's a matter of them not releasing detailed specs to developers on just how to control the voltage settings on the new voltage controler.   Unlike Nvidia, AMD doesn't use a unified architecture that is independent of physical hardware changes. So every time they release a product with a new component like a voltage controller, there is a huge flap over accessing it properly and getting it to work correctly with user settings.   So even 3rd party board partners dont get the info how to change it.   The guys over at OC3d.com are mapping it the hard way. Last I heard they have Ic2 level access so they can change the voltage but something there drops the card down into 2d mode whenever they try so the card never goes above 300mhz when they play with some of the settings. They really dont know why it's doing that."
hardware,3csgp2,Seclorum,3,Fri Jul 10 22:01:16 2015 UTC,"I think you're confusing reference cards with custom ones  ASUS made a STRIX version of R9 Fury with 12 phase design, here: http://anandtech.com/show/9421/the-amd-radeon-r9-fury-review-feat-sapphire-asus/3  are you telling me ASUS doesn't know what ASUS is doing ?!?!"
hardware,3csgp2,zmeul,1 point,Sat Jul 11 00:41:51 2015 UTC,"Then Asus hasn't released any information or updated it's tools to change voltages.  Sure they have the specs for the board itself, but none of the 3rd party AIB partners have said word one about getting proper voltage controls implemented."
hardware,3csgp2,Seclorum,1 point,Sat Jul 11 00:59:13 2015 UTC,"It's worth noting that the 290x did not have voltage control, but one of the first aftermarket cards on the market the asus dcuii did have voltage control built into the asus utility.   Doesn't mean fury won't get it, but I thought that was interesting."
hardware,3csgp2,logged_n_2_say,1 point,Sat Jul 11 12:39:42 2015 UTC,"this is why AMD chose to separate the launches, people would've got for the air-cooler one hands down   Good thing they took their time to have a big supply of their fury x for all the ""early"" adopters....oh wait."
hardware,3csgp2,iPlayRealDotA,1 point,Fri Jul 10 21:36:23 2015 UTC,"that ""rumor"" AMD had a limited supply of Fury Xes .. eh ;)"
hardware,3csgp2,zmeul,0,Fri Jul 10 21:38:14 2015 UTC,"Well, exactly what we've expected. Slower than the Fury X, faster than the 980 at the same price."
hardware,3csgp2,LiberDeOpp,6,Fri Jul 10 12:12:51 2015 UTC,Its 50 dollars more...
hardware,3csq30,lowcarb123,-3,Fri Jul 10 13:47:02 2015 UTC,"And yet more reviews where they refuse to cover 1080p resolutions...  because Nvidia crushes at that resolution.  Not to mention they can't seem to round up anything other than reference 980/980ti cards when 980 overclocks can make a HUGE difference on performance.  http://www.legitreviews.com/play-pc-games-resolutions-beyond-1080p-amd-nvidia-fight-4k-dominance_126494  Hardly anyone is playing one anything other than 1080p, with a few at 1440p.  That link is old but the numbers are still about the same.  Can be seen at http://store.steampowered.com/hwsurvey  If you're going to review an aftermarket card that is overclocked you need to also include the competitor's overclocked cards and the resolutions that the VAST majority of people use."
hardware,3csq30,Thunder_Bastard,4,Sat Jul 11 00:09:59 2015 UTC,I don't think people who play on 1080p tend to buy 500+ dollar cards.
hardware,3csq30,DHFearnot,3,Sat Jul 11 00:34:41 2015 UTC,"So you think 33% of the market, and probably another 30% that run dual displays that aren't counted in those numbers...  don't buy anything above a 970/390?  I assure you they do.  In fact, for 120Hz/144Hz gaming you need a card that will push 120/144 fps in games.  It is great to see benchmarks of games at 1440 and 4k with 50fps, but honestly it still looks like shit.  Running true 120Hz/144Hz at max graphical settings has a way bigger impact on gaming than simply moving up to 1440p or 4k (I have all 3, I use 1080p/120Hz exclusively because of how good it looks)."
hardware,3csq30,Thunder_Bastard,5,Sat Jul 11 00:55:58 2015 UTC,"I disagree. 1440p @ 60fps is better than 1080p @ 144fps IMO. It's opinion, but graphics usually have a bigger impact than fps in a game unless it's a competitive first person shooter."
hardware,3csq30,stereosteam,-2,Sat Jul 11 13:15:59 2015 UTC,Honestly not even worth discussing.  Unless you are sucking AMD's dick in this sub you get downvoted.  Just going to stop posting truthful and honest information and let people think everyone out there is running 1440p or 4k and listening to biased reviews.
hardware,3csq30,Thunder_Bastard,7,Sat Jul 11 14:18:50 2015 UTC,The amount of people running 1440p exceeds the amount running 1080p@ 120/144hz.
hardware,3csq30,stereosteam,1 point,Sat Jul 11 16:44:13 2015 UTC,"Not arguing that, it is total people running 1080p at any refresh rate.  I was giving you a reason why someone WOULD run 1080p over higher resolutions.  Not to mention people using TV's.  But apparently in this sub you're wrong even when you prove the statistics.  For every 1 person running better than 1080p there are 20-30 running 1080p.  Yet these ""reviews"" ignore those people.  Just guessing that none of those people will buy a Fury, Fury X, 980 or 980ti is just dumb, sorry.  And AMD fanboys please go ahead and downvote and bury this post too."
hardware,3csq30,Thunder_Bastard,1 point,Sat Jul 11 22:32:24 2015 UTC,"Yeah I agree, nobody is going to buy a Fury, Fury X, 980, or 980ti for 1080p. Most of the high end carpet is for above 1080p@60fps. Which is why the benchmarks are as such."
hardware,3csq30,stereosteam,1 point,Sat Jul 11 22:41:18 2015 UTC,They ignore them because these cards are overkill for 1080p 60hz. These are cards that have no problems running 1440p 60fps.
hardware,3csq30,hdshatter,1 point,Mon Jul 13 01:06:44 2015 UTC,"Yeah, there's an awful lot of people running QNIX 2710s.  VA 1440p@100+hz for <$250 is the steal of the century.  That's still more oomph than a single card can reliably give, except for MAYBE a 980Ti/Titan X."
hardware,3csq30,capn_hector,1 point,Sun Jul 12 21:47:15 2015 UTC,Are those QNIX korean monitors reliable? Looking into one but I don't want any deadpixels or something.
hardware,3csq30,stereosteam,1 point,Sun Jul 12 22:03:21 2015 UTC,"If you overclock them it causes some issues like ghosting, it also increases latency. I've never heard of anyone's panel breaking from overclocking.  Also good luck with warranty on them."
hardware,3csq30,hdshatter,1 point,Mon Jul 13 01:04:59 2015 UTC,"In terms of physical quality - usually, but in the worst case you do risk a couple dead pixels.  They're seconds-grade panels, that's why you're getting a 1440p VA panel for $250.  You can often gently massage the dead pixels out by hand with a microfiber cloth.  Mmm,  that's it, work it, who's a slut for a good deal?  You are.  * ahem *  The ""perfect pixel"" packages are a dubious value, as are the warranties.  Sure, they'll guarantee no dead pixels, all you have to do is pay to ship it back to Korea.  Ka-ching.  In terms of electronic performance - yes, very, with the one caveat that you need to buy the right monitor.  You want the one with ONLY a single DVI input - not displayport, not dual DVI, etc.  ""Evolution"", ""True10"" etc are all keywords you need to steer away from.  The reason they overclock is because QNIX cheaped out on the controller electronics and the monitor just displays whatever its one input drives.  The nicer models will not overclock.  If you get the right model you are pretty much guaranteed 90hz and most of them go to 100hz+."
hardware,3csq30,capn_hector,1 point,Mon Jul 13 02:36:41 2015 UTC,"The majority of gamers are at 1080p 60hz. No idea why your getting downvoted, its not secret AMD GPUs don't show their true power at 1080p."
hardware,3csq30,hdshatter,0,Mon Jul 13 01:02:46 2015 UTC,"I disagree, too. To me 60 fps isn't even good. The difference between 60 fps and 75 fps is absolutely clear, let alone 120 fps or 144 fps (the difference between these is much less notable, though). Racing games make this clear, FPS makes it clear, but even simulation games such as Cities Skyline benefit from this. Much less headaches. This is just like saying digital compression doesn't hurt video quality, some may not notice these things but I certainly do.  IME 30 fps is headache-inducing garbage, 60 fps is playable (as long as it's rock solid), 75 fps is good, 90 fps is great, 120 fps is awesome, 200 fps+ is where it's at. All this with v-sync OFF (for the love of all holy), and AA only if my pc can run the game at 90 fps or above with the setting on.  I can even tell which cars have pulsed versus cw brake lights (and it gives me a horrible headache), so it may be that I'm extremely sensitive to this kind of stuff. Stuttering or inconsistent frame rate is an absolute game breaker."
hardware,3csq30,dajigo,-1,Tue Jul 14 21:53:24 2015 UTC,I'm still upset that its a cut down fury x with 500 less cores.  It should be faster than the 980 though.  Hopefully the competition will drive down both prices.
hardware,3csq30,cvance10,8,Fri Jul 10 14:32:21 2015 UTC,"Why would it? The Fury beats it, but costs $50-$75 more.   Bums me out :c"
hardware,3csq30,Maysock,2,Fri Jul 10 14:41:49 2015 UTC,Soon to be 80 to 100 more
hardware,3csq30,w00t692,1 point,Sun Jul 12 17:52:24 2015 UTC,Thought that was a rumor?
hardware,3csq30,Maysock,2,Sun Jul 12 18:13:08 2015 UTC,Some board partners already cut prices.   We'll see.   Even if they don't the 980 still has good price to performance at this point
hardware,3crm47,BlayneTX,6,Fri Jul 10 05:26:55 2015 UTC,"I liked this bit...   Further, overclockers can use liquid helium or liquid nitrogen for extra cooling, thanks to a special memory defroster cooling system. Exact clock-rates for this bonafide ROG-branded product are again unknown.    They had to build in a defrosting circuit just on the possibility of LN2 maniacs."
hardware,3crm47,Seclorum,1 point,Fri Jul 10 05:54:01 2015 UTC,Yeah I had my eye on that Matrix but now I'm wondering if it's overkill for overclocking on air.
hardware,3crm47,dylan522p,1 point,Fri Jul 10 07:17:11 2015 UTC,You can always get another put them under water and get about 300mhz
hardware,3crm47,Marcgo2,1 point,Fri Jul 10 17:26:05 2015 UTC,http://www.galax.net/US/980tihofLN2.html
hardware,3crm47,knowledgestack,2,Fri Jul 10 12:59:35 2015 UTC,"The gold 980 is £600 on amazon, so god knows how much the ti will be."
hardware,3crm47,MerryLane,1 point,Fri Jul 10 11:13:01 2015 UTC,"I just want 2 poseidon 980ti ^  That would be like godlike performance, with high headroom, almost silent and easy/cheap to do (for a custom WC setup)"
hardware,3crm47,Juts,1 point,Fri Jul 10 19:14:56 2015 UTC,I'm concerned that the 1x6-pin pcie connection means that it will be TDP limited.
hardware,3cpbtu,buildzoid,26,Thu Jul 9 18:11:11 2015 UTC,Strix has a DVI-D port so the Korean monitor guys can be happy about that.
hardware,3cpbtu,jforce321,4,Thu Jul 9 18:38:04 2015 UTC,"Thank you. Now, let's see how to get it to be faster than than the Fury x."
hardware,3cpbtu,tedlasman,5,Thu Jul 9 19:57:43 2015 UTC,"Looks like a custom PCB, so provided the cooler doesn't suck (no promises there), this'll be the best chance at overclocking the Fury of the cards we know of. Also, they need to get the vcore modding utilities out."
hardware,3cpbtu,Exist50,2,Thu Jul 9 21:29:53 2015 UTC,"I'm hoping like the 980ti did with the titan, that the fury will perform better than the fury x thanks to the custom pcb."
hardware,3cpbtu,Colorfag,-3,Fri Jul 10 09:36:30 2015 UTC,Don't make me do my copy-pasta again
hardware,3cpbtu,TehRoot,6,Fri Jul 10 02:20:04 2015 UTC,"Hey now, I was careful with my language. No voltage lock here :)"
hardware,3cpbtu,Exist50,6,Fri Jul 10 02:33:21 2015 UTC,Is that Deus Ex: Mankind Divided added to the never settle bundle or just an advert for a game that uses TressFx? There was a rumor that SW:Battlefront was being added.  I wonder if this is another of those best case scenario benchmarks where they have Anisotropic Filtering disabled. I would guess this will be trading blows with the 980 rather than beat it and probably behind the factory overclocks at 1080p.
hardware,3cpbtu,Scrabo,8,Thu Jul 9 18:48:56 2015 UTC,"Deus Ex: Mankind Divided   Probably an ad for all the new tech it's using. TressFx 3.0, DX12, etc. Things AMD's been pushing.   Also, isn't AF very light on the GPU? Not sure why they'd bother disabling it."
hardware,3cpbtu,Exist50,1 point,Thu Jul 9 18:55:13 2015 UTC,"Wasn't there a video with three guys talking about the fact that disabling AF and turning up all the shader effects would make the Fury X look faster relative to the GTX 980 Ti due to huge shader count but relatively modest ROP count?  It was posted in this subreddit and they were claiming that AMD had specifically chosen settings for their benchmarks that would take advantage of the Fury X's shader power, which was supposedly the reason that the pre-release benchmarks made it look like it was beating the GTX 980 Ti.  Even though AF is super light, turning it down or turning it off could still help give the Fury the edge in benchmarks where every percentage point counts."
hardware,3cpbtu,aziridine86,3,Thu Jul 9 23:55:10 2015 UTC,"Maybe, but comparing shader and ROP count between architectures is not a valid method. Fiji and Hawaii have the same ROP count, but GCN 1.2 ROPs are vastly more effective. Fiji does have a lot of shader power, and AMD likely did cherry pick data, but it's not nearly as simple as a by-the-numbers comparison. Drivers probably make more of a difference than anything. Look at tessellation improvements with the current compared to release drivers for Hawaii."
hardware,3cpbtu,Exist50,2,Fri Jul 10 00:01:55 2015 UTC,"Yeah I don't really mean to just directly compare the shader:ROP ratio of Maxwell vs. Fiji.   I just mean that the video was expounding on the idea that the Fury X is shader-heavy and potentially ROP-bottlenecked. I'm not sure that has even been proven, but that was their argument as for why AMD would try to tweak benchmarks in a way that would favor their supposed shader advantage."
hardware,3cpbtu,aziridine86,2,Fri Jul 10 00:10:44 2015 UTC,"It would be lovely (and killer) if they included one of those 3 games with any fury gpu, but i don't think it's super likely."
hardware,3cpbtu,KibblesNKirbs,1 point,Thu Jul 9 20:08:44 2015 UTC,"Maybe closer to release. Those companies have a relationship with AMD, and it wouldn't be abnormal at all to bundle a game. BF4 got a bundle."
hardware,3cpbtu,Exist50,14,Thu Jul 9 21:30:58 2015 UTC,"The use of a 4K comparison doesn't have me overly optimistic, considering the 390x already basically matches the 980 at that level."
hardware,3cpbtu,Exist50,14,Thu Jul 9 18:30:10 2015 UTC,Yeah I wish they wouldn't even show 4k unless you can get 60 fps or at least show it in crossfire since no one will be trying to play 4k at 20fps.
hardware,3cpbtu,LiberDeOpp,11,Thu Jul 9 22:18:07 2015 UTC,"Well, those all seem to be >40fps (with many breaking 50) at high settings, which I'd consider extremely playable. Especially considering that fits in the range of a Freesync display."
hardware,3cpbtu,Exist50,8,Thu Jul 9 22:19:33 2015 UTC,High settings *<--this doesn't mean it actually runs at high settings plus you cannot trust non-independant benchmarks. I remember when the fury x was shown solidly beating the 980ti during amd press event too.  I still believe this is the fury card everyone should be excited about and it will be nice to see what custom pcbs/coolers the 3rd parties can make.
hardware,3cpbtu,LiberDeOpp,3,Thu Jul 9 22:22:40 2015 UTC,"Techreport commented on those benchmarks.  They said the Fury X benchmarks that were released before the NDA lifted were obtained by turning down (or off) texture filtering/anisotropic filtering, turned up the quality of shader effects in games,  and in most cases did not use MSAA (that used ROPs) but instead used FXAA/SMAA because the Fury has better shader throughput.   This was from the podcast -- there's a little bit of lead up explaining why their results were different but it's an interesting listen."
hardware,3cpbtu,PappyPete,1 point,Fri Jul 10 06:31:58 2015 UTC,"Well the Fury X is capable of 4K high-ultra gaming, so the Fury shouldn't be that far behind. I think it'll be able to hit the advertised numbers."
hardware,3cpbtu,Exist50,5,Thu Jul 9 22:24:01 2015 UTC,I'm just weary of any manufacturers advertised numbers. Should still be a decent card anyway. We're not at 4k gaming yet and the monitors are still expensive. Give it a year and 4k will be more common and cheaper.
hardware,3cpbtu,LiberDeOpp,3,Thu Jul 9 22:44:12 2015 UTC,"The monitors are now cheaper than the cards needed to run them at high-ultra, so I'd say that's something. I do like Samsung's $400 IPS Freesync one, though it's a bit small (24"")."
hardware,3cpbtu,Exist50,1 point,Thu Jul 9 22:49:11 2015 UTC,That's why i went with asus 144hz 1080p. I game close enough that 4k would look similar but cost huge performance. Then you have tech like vsr that can take a 1440p or large and scale it to your screen.
hardware,3cpbtu,LiberDeOpp,2,Thu Jul 9 22:59:42 2015 UTC,"If anything, 4K would be more noticeable closer. And since VSR runs the game at that higher resolution, the benchmarks are more relevant than ever."
hardware,3cpbtu,Exist50,8,Thu Jul 9 23:02:38 2015 UTC,I can't wait to buy one and strap a full cover waterbl.....oh hmmm...wait a sec
hardware,3cpbtu,innerknowing,4,Thu Jul 9 20:26:25 2015 UTC,That's actually my plan if one of these boards shows good OC potential.
hardware,3cpbtu,eric1743,1 point,Thu Jul 9 20:49:41 2015 UTC,?
hardware,3cpbtu,darrenphillipjones,4,Fri Jul 10 02:19:35 2015 UTC,Putting a $100 full cover block on this and adding to a custom loop and you end up with a....$650 video card that only has 87.5% as much power as an X.
hardware,3cpbtu,innerknowing,2,Fri Jul 10 02:40:41 2015 UTC,Well the other option is to get a fury x (if custom is your thing) and spend an extra $100 for 12.5% increase in performance. I'm doing full custom myself so I'd rather save $100. I want the small size and a custom loop.
hardware,3cpbtu,darrenphillipjones,2,Fri Jul 10 03:01:20 2015 UTC,"You know, you could probably sell off that AIO cooler to another Fury user and make part of that back too.  Same PCB I assume?"
hardware,3cpbtu,innerknowing,1 point,Fri Jul 10 03:09:47 2015 UTC,The Sapphire one appears to be a reference PCB.
hardware,3cpbtu,Exist50,1 point,Fri Jul 10 08:17:39 2015 UTC,only has 87.5% as much power as an X.   It's likely better than that.
hardware,3cpbtu,Exist50,1 point,Fri Jul 10 08:17:56 2015 UTC,Are you anticipating unlocking the extra shaders? For purposes of this discussion its exactly that since nobody can guarantee overclocking and its hard to out-overclock a water cooled solution across the board.  Disabled cores are disables cores.
hardware,3cpbtu,innerknowing,1 point,Fri Jul 10 12:33:48 2015 UTC,"No, but since the rest of the chip isn't cut like the shaders and TMUs are, that's the lower bound of its performance at a given clock speed."
hardware,3cpbtu,Exist50,2,Fri Jul 10 17:02:41 2015 UTC,I'm still waiting for my Fury X on back order
hardware,3cpbtu,kungfujedis,4,Fri Jul 10 03:41:28 2015 UTC,"Please AMD, don't make me buy 970 (handsome devil)."
hardware,3cpbtu,sverek,3,Fri Jul 10 03:37:43 2015 UTC,Even the 390 beats the 970.
hardware,3cpbtu,sverek,2,Fri Jul 10 08:51:24 2015 UTC,"you mean overclocked 290 with 8Gb beats 970?  yes it does. But you can also overclock 790 and see which  performs better.  I'd get 390 for the same price as 970, but where i live its like $100 more expensive."
hardware,3cpbtu,Shitpoe_Sterr,1 point,Fri Jul 10 09:30:29 2015 UTC,In that case I'd say just go for the best thing you know you can afford and by the sound of it it seems like its the 970. I got myself 390 but thats because 970s are actually even more expensive than it where I live (Yeah I know)
hardware,3cpbtu,jerryfrz,1 point,Fri Jul 10 16:18:32 2015 UTC,But muh PhysX and furry hairs!!!1
hardware,3cpbtu,cvance10,2,Fri Jul 10 09:03:40 2015 UTC,"Based on that article, AMD is not going to make a non-water-cooled version of the Fury with all of the cores activated and full power system.  The Nano looks to have the cores but with only a single 8-pin it can't have as much power to work with.  I want a Fury X air-cooled with all the bells and wistles."
hardware,3cpbtu,Darkstryke,8,Thu Jul 9 20:35:38 2015 UTC,"AMD did not do an AIO system because it looks cool or has quieter fans, it was done because their traditional blower coolers were not enough. It would be interesting to see just how much dissipation ability was lost with a smaller heatsink surface area due to the shorter pcb.  The AIB non-reference models we're seeing crop up on air are going with a traditional length cooling system that extends far past the actual PCB area of the card.  -- To expand on this since I've been asked in PM's; adding an AIO like this is probably the highest QA risk they can do with a  reference card. It's adding another couple points of mechanical failure (pump, fittings), AIO's are traditionally a bit of a gamble for the retail channel. I'd hate to see the RMA rates that will relate strictly to the pump, be it impeller noise from housing cavitation (which most basic end users won't understand and is the most common problem with AIO), faulty assembly, bad QA, tiny leaks that go undetected until enough coolant has evaporated, etc. This was done for only one reason, a traditional heatsink/pipe/fan assembly was obviously not up to snuff for the physical surface area available to keep the card at sane operating temperatures."
hardware,3cpbtu,Colorfag,1 point,Thu Jul 9 21:13:40 2015 UTC,"Well the coolers on the fury are actually pretty big, longer than the pcb.  The nano should stay cool enough with its reduced power draw."
hardware,3cpbtu,Le_rebbit_account,1 point,Fri Jul 10 09:41:09 2015 UTC,Anyone know of any reviewers who got samples sent to them?
hardware,3cpbtu,darrenphillipjones,-2,Fri Jul 10 02:13:25 2015 UTC,Yea all the ones that don't use sound equipment. /s
hardware,3cpbtu,ajac09,1 point,Fri Jul 10 02:20:13 2015 UTC,Not updating. But will keep my eye on the 2016 processors coming out.
hardware,3cpbtu,Trollatopoulous,1 point,Fri Jul 10 03:10:28 2015 UTC,"Looks like there's going to be another drop in AMD share price. Too bad, really, I like AMD but they're not making it easy."
hardware,3cpbtu,yinfung,1 point,Fri Jul 10 05:11:23 2015 UTC,Any info of Nano's launch date?
hardware,3cpbtu,Perunsan,1 point,Fri Jul 10 09:13:13 2015 UTC,i wonder if there is gonna be any discount if it's gonna be available on 15 july on amazon
hardware,3cpbtu,innerknowing,11,Thu Jul 9 19:53:39 2015 UTC,Absolutely not.  Unless Amazon just enjoys giving away their shareholders money.
hardware,3cpbtu,Exist50,5,Thu Jul 9 20:27:10 2015 UTC,"Well, they haven't exactly been shy about running a deficit."
hardware,3cpbtu,innerknowing,8,Thu Jul 9 21:42:56 2015 UTC,My understanding is that is largely due to reinvesting everything into expansion.  I suppose they could consider a discount on a day 1 product launch expanding their product.
hardware,3cpbtu,Exist50,1 point,Thu Jul 9 21:44:57 2015 UTC,"Yeah, you're right. I was just pointing out that Amazon has not been a profitable company. They'd only loose money on something like streaming or other media that they're not firmly established in."
hardware,3cpbtu,Hay_Lobos,1 point,Thu Jul 9 22:21:43 2015 UTC,"AWS is the shit, though."
hardware,3cpbtu,Vinnycabrini,4,Fri Jul 10 05:12:30 2015 UTC,I highly doubt it.
hardware,3cpbtu,BlayneTX,-7,Thu Jul 9 20:02:46 2015 UTC,This card should have been $450 and Fury X should have been $550. I'm not sure why AMD isn't still trying to be the value option anymore.
hardware,3cpbtu,AMW1011,11,Thu Jul 9 21:09:13 2015 UTC,"Fury X still sells at $650, why would they price it lower? When supply outpaces demand, then they can lower the price."
hardware,3cpbtu,atriax,20,Thu Jul 9 22:56:30 2015 UTC,And where do you have any proof that this is even financially viable? They can't just price cards wherever the fuck they want.
hardware,3cpbtu,brookllyn,31,Thu Jul 9 21:32:44 2015 UTC,"Fury X should have been $100. They won't sell any priced like that garbage overprice nvidia card. If they priced it at $100, that's like 1 billion sales with 100 billion dollars in their pockets. Why AMD doesn't play to the consumer is completely beyond me."
hardware,3cpbtu,Le_rebbit_account,8,Thu Jul 9 22:14:29 2015 UTC,"Can confirm, if they were $100 I'd buy 9 of them"
hardware,3cpbtu,MisterIncredible,5,Fri Jul 10 02:11:32 2015 UTC,Lmao
hardware,3cpbtu,TaintedSquirrel,17,Thu Jul 9 22:19:43 2015 UTC,Fury X is sold out everywhere.  There's no reason to price it lower.
hardware,3cpbtu,darrenphillipjones,-2,Thu Jul 9 23:04:18 2015 UTC,"If you have 8 units and sell out, it doesn't mean anything. I'm not saying there are only 8 units, but saying something is sold out doesn't amount to anything more than AMD not properly anticipating the launch. Or they rushed the launch ahead of schedule and didn't have enough boards made to accommodate for whoever just wants the newest card on the market."
hardware,3cpbtu,TaintedSquirrel,9,Fri Jul 10 02:24:21 2015 UTC,If its selling out then they dont need to cut the price...
hardware,3cpbtu,Anaron,3,Fri Jul 10 02:55:51 2015 UTC,I don't think you understand how supply and demand works.
hardware,3cpbtu,darrenphillipjones,1 point,Fri Jul 10 04:08:24 2015 UTC,"It's as simple as riding a bike. AMD controls the supply that they ordered to be made. If they barely produced any units it wouldn't be that hard to sell out.  Please try to be more constructive than saying I don't understand a very simple business concept.  The Titan x went through the same phase. They didn't compensate for the demand and were sold out very fast. It doesn't mean it was priced well, it just meant that enough people wanted the latest and greatest at any cost.  Price per performance, it's a little upsetting. They probably could have priced it at $800 and sold out. It doesn't mean Blayne's statement is false."
hardware,3cpbtu,weez09,1 point,Fri Jul 10 04:22:41 2015 UTC,"are you assuming that they are purposely limiting the amount of units they put out on the market? I'd wager they are supplying as many Fury X's as possible with their current line of production. Let's say they could only make 2000 Fury X's that they could guarantee viable for sale. If all 2000 sell out, why would they lower the price? Lets say in their next batch they are able to expand and make 4000 Fury X's. All sell out - why make the price lower? Now if it gets to the point where the rate of supply is greater than the demand, they can start lowering the price to make demand go up."
hardware,3cpbtu,darrenphillipjones,1 point,Fri Jul 10 18:14:11 2015 UTC,I said there are two options. Either limiting supply or underproduced.
hardware,3cpbtu,Crusty_Magic,2,Fri Jul 10 18:49:54 2015 UTC,They think it's worth that much. We'll see price drops on both sides eventually.
hardware,3cpbtu,LiberDeOpp,2,Thu Jul 9 23:21:07 2015 UTC,because they need profit to fund their R&D.
hardware,3cpbtu,skilliard4,2,Thu Jul 9 22:14:16 2015 UTC,They said they no longer want to be the low cost option and they can't afford being that either. Still if nvidia drops prices look for amd to follow.
hardware,3cnkxm,feelix,39,Thu Jul 9 08:13:48 2015 UTC,"The non-recurring engineering costs on this one must be insane. Then again if it's really a massive 50% improvement over 10nm, surely it'll be worthwhile sooner or later. Impressive tech all in all."
hardware,3cnkxm,bphase,23,Thu Jul 9 10:07:40 2015 UTC,"Super worthwhile considering that they don't have fabs anymore. They're just selling the IP to global foundries, so they don't even have to worry about production level bring up or anything."
hardware,3cnkxm,fuzz3289,3,Thu Jul 9 16:56:32 2015 UTC,They still have a research fab in Albany.
hardware,3cnkxm,tommdonnelly,1 point,Fri Jul 10 00:13:33 2015 UTC,"And from there, they shall take over the world."
hardware,3cnkxm,continous,3,Fri Jul 10 04:54:59 2015 UTC,"What are the actual dimentions of features on these? Larger than 7nm, I guess."
hardware,3cnkxm,Randomoneh,4,Thu Jul 9 18:30:21 2015 UTC,"Bulk 7nm transistors, with a 30nm pitch (the distance between the front edge of one transistor and the front edge of the next transistor)."
hardware,3cnkxm,polynomialpusher,3,Thu Jul 9 20:35:21 2015 UTC,Thanks. It seems overall size of a single transistor isn't being reduced as much as some of its features. Why is that?
hardware,3cnkxm,Randomoneh,5,Thu Jul 9 21:42:33 2015 UTC,"At a certain point it becomes difficult to stop electrons from crossing the gate (gap between two points of electrical connection in a transistor) all by themselves. Simply put, they have enough energy to jump the gate and that causes your switch (transistor) to be useless."
hardware,3cnkxm,polynomialpusher,2,Thu Jul 9 22:04:08 2015 UTC,"There is nothing on that wafer that measures 7nm.  Here is a good article by a notable scientist that describes why node names are just names that don't describe any dimensions on the chip.  The 2nd half of the article, which was written in 2009, is about how EUV illumination is required for future nodes and is about 2 years away from production. Hilariously, in 2015 it's still 2 years away from production."
hardware,3cnkxm,tommdonnelly,1 point,Fri Jul 10 00:21:43 2015 UTC,Why don't we use pitch instead of half-pitch?
hardware,3cnkxm,Randomoneh,3,Fri Jul 10 03:07:54 2015 UTC,"It would be simpler, wouldn't it. It's a holdover from the old days when the minimum gate length and the minimum metal1 half pitch were the same. It started diverging around 0.25 microns and was gone by 0.18, but the terminology remains."
hardware,3cnkxm,tommdonnelly,35,Fri Jul 10 04:02:31 2015 UTC,"Wow, that is seriously impressive.  I think my current 22nm CPU should be able to last till 2019, which should be roughly when Intel gets this stuff out on the market (on their own process)"
hardware,3cnkxm,everyZig,37,Thu Jul 9 08:55:06 2015 UTC,"Desktop cpus clearly don't need to get smaller with that much  of a hurry.  Not because there isn't a demand but because it's an engineering solution that can be easily circumvented by throwing some more power and physical space.  Many applications will be born from low power consumption  devices.   10 or 15 years in the future,  wireless technology will benefit from this,  but Imho what  is more interesting are  how embedded systems will change with this. So in a way it can be a crucial development for the stagnant field of robotics and automation.  Smaller faster chips, mean more systems capable of more complex task without the same computing limitations.   Also it would be awesome to have more full desktop computers on a phone, capable of doing everyday tasks without battery restrictions. Mind you  Intel already made a x86 phone in partnership with google. It's not perfect yet.  But, it was made with a 22mn manufacturing process, and it is competing with arm based products.  Make it smaller,  and more efficient to have full fledged windows in your pocket and you will open  the gates to Valhalla of software development. (not here yet with zenfone2 but it's probably on the way)   Your cpu may hold up till far in the future, but many industries are dying for smaller cpus. What a time to be alive.   EDIT http://www.xda-developers.com/windows-7-installation-achieved-on-asus-zenfone-2/  CALLED IT. To everyone who was bitching on that comment clusterfuck about windows reaching x86 phones. CALLED IT."
hardware,3cnkxm,pabloe168,12,Thu Jul 9 10:54:09 2015 UTC,easily circumvented by throwing some more power and physical space.    There's a limit to how large the die can be and keep in mind using are larger die means using up more of the wafer for each chip which means you get less chips out of the wafer which has a fixed cost.
hardware,3cnkxm,spellstrike,2,Thu Jul 9 14:52:01 2015 UTC,Regardless a phone can dissipate what like 2W and it can use one soc. A desktop can dissipate hundreds and can have multiple processors.
hardware,3cnkxm,dylan522p,1 point,Thu Jul 9 20:15:10 2015 UTC,I don't disagree.
hardware,3cnkxm,spellstrike,-2,Thu Jul 9 22:20:50 2015 UTC,"""Physical space"" as in ""physical space for more cooling""."
hardware,3cnkxm,SPOOFE,3,Thu Jul 9 15:04:26 2015 UTC,"No, as in physically larger die"
hardware,3cnkxm,ZeM3D,0,Thu Jul 9 21:41:58 2015 UTC,"The reason the CPU's don't need to be made smaller is because there's plenty of room in the form factor for fans and beefy heatsinks. Die size limits are a function of the likelihood of critical errors in the silicon, which grossly increases the cost. This is not easy to circumvent.  Power and cooling, for something (relatively) large and hooked up to the grid, IS easy to circumvent."
hardware,3cnkxm,SPOOFE,2,Thu Jul 9 22:03:31 2015 UTC,"You clearly don't understand what die size means. If you reduced die size 5x or increased it 5x, it wouldn't have much effect on space for cooling, because the chip is still tiny. Die size doesn't mean the size of the entire SoC, it means the size of the actual components on the chip in the SoC."
hardware,3cnkxm,xakh,0,Fri Jul 10 10:41:05 2015 UTC,"You clearly don't understand what die size means.   You're off to a great start!   If you reduced die size 5x or increased it 5x, it wouldn't have much effect on space for cooling, because the chip is still tiny.   Now that's just untrue. Heat transfer in this case is partly a function of surface area. More surface area means more heat can transfer. Less surface area... well, I hope you can figure it out, though your first sentence doesn't leave me with much confidence. ;)   Die size doesn't mean the size of the entire SoC   Yes it does.   it means the size of the actual components on the chip in the SoC.   No it doesn't.  Tell me moar about what I don't understand. :D"
hardware,3cnkxm,SPOOFE,5,Fri Jul 10 14:57:21 2015 UTC,"Desktop cpus clearly don't need to get smaller with that much of a hurry ... throwing some more power and physical space.    They do though. AMD's newest GPU, for example, is as large as you can make it on a 28nm process, and clocks in at just under 9 billion transistors. The interposer chip is actually larger than what should technically be possible. Rumors are it's not as powerful as it could have been because they were planning for a smaller process to be available, and just couldn't fit everything onto the die.   CPUs aren't growing quite as crazily as GPUs are but Intel isn't slacking off; that's how it been staying ahead of AMD's CPU department. With CPUs, it's the server CPUs which are really the cutting edge. They're always looking to cram more and more cores and cache onto a die."
hardware,3cnkxm,bb999,3,Thu Jul 9 16:46:53 2015 UTC,"Server CPU's definitely need the die change, but server cpu ≠ desktop cpu. While traditionally the ultra high end server/enthusiast CPU's are the same process (or larger) as the current desktop CPU's, I could very easily see that change, as Intel crams more cores onto smaller spaces. This doesn't apply to desktop, since nobody needs a 18 core CPU in their desktop. Already, Intel has shown that it is happy leaving leaving desktop CPU's behind to focus on mobile/high performance (still haven't seen Broadwell for desktop), simply because the decrease in TDP and slight increase in performance doesn't justify their production."
hardware,3cnkxm,for_lolz,11,Thu Jul 9 18:29:36 2015 UTC,since nobody needs a 18 core CPU in their desktop.   /r/pcmasterrace is going to have a problem with this statement.
hardware,3cnkxm,polynomialpusher,1 point,Thu Jul 9 20:30:55 2015 UTC,Broadwell for desktop keeps cpu the same with 5% IPC and lesser clocks but iGPU far surpasses anything even amd can put out
hardware,3cnkxm,dylan522p,1 point,Thu Jul 9 20:17:19 2015 UTC,"Oh yeah, i know the desktop market isnt in big need of increased performance, i just like this kind of stuff :)"
hardware,3cnkxm,everyZig,2,Thu Jul 9 10:56:07 2015 UTC,Your desktop will be in your pocket soon!
hardware,3cnkxm,pabloe168,20,Thu Jul 9 11:03:01 2015 UTC,People have said this for years it will never be true.   You'll never fit a high end fully featured gaming or workstation gpu in a pocket device.   You'll get devices with today's desktop performance in your pocket while actual desktops continue to get more power.
hardware,3cnkxm,TeutorixAleria,2,Thu Jul 9 13:28:00 2015 UTC,Weird. I have a very low-end Android phone and it does a lot more than my first Windows 95 desktop could do. The phone was cheaper too.  How is it not true?
hardware,3cnkxm,cadika_orade,4,Thu Jul 9 14:21:57 2015 UTC,"Okay so, clearly modern day cell phones are faster than personal computers (workstations) from last decade. But quite simply put, because of the laws of physics, a cell phone will never be as powerful as its' larger counterpart. Whatever tech you've got in your smart phone, why not have 20x that tech, put it in a tower, and call it a desktop?  Sure, the layman will not need a desktop if they've got a smart phone. But how does that smart phone work? It runs on software that was programmed by someone using a workstation.  You will never see a room full of code monkeys writing in Python on their mobile devices.... Just think about how silly it is to say ""we don't need desktops, we have cell phones"" when without desktops we could not make those cell phones work!"
hardware,3cnkxm,peopleandtheirfeels,3,Thu Jul 9 14:33:28 2015 UTC,You will never see a room full of code monkeys writing in Python on their mobile devices   I could honestly see this happening though. Dock your phone and it becomes a PC. Everything else I agree with.
hardware,3cnkxm,bfodder,-4,Thu Jul 9 20:39:48 2015 UTC,"Well obviously, but saying that current desktops will never be outmatched by future mobile devices is just absurd.  You're making a couterargument against something nobody said, and when people say things like ""because of the laws of physics"" usually means people have very little idea of what they're talking about.   Whatever tech you've got in your smart phone, why not have 20x that tech, put it in a tower, and call it a desktop?   That's exactly what we do.   code monkeys writing in Python on their mobile devices   Obviously that wouldn't happen, but lack of computing power is hardly why. Modern programmers, especially those working with such low-performance languages, rarely get very high-end workstations and probably could do their jobs entirely on mobile devices. The reason they would is because they'd need larger displays and full keyboards, not high-end processors.  Where I work, I do all my programming on a Core2 Duo running Windows XP. The mechanical engineers all have Xeon workstations with dual Quadro GPUs for doing 3D modeling and simulation.  I already have a mobile device more powerful than my workstation. Give it a decade and I'll have a mobile device more powerful than their workstations.  Lastly, nobody writes cellphone software in Python. It is a scripting language ideal for children, rapid prototyping, platform agnostic low-performance software, and server applications. Most modern mobile applications are written in Java, C++, or Objective C. Or that emojii-laced abomination Swift."
hardware,3cnkxm,cadika_orade,8,Thu Jul 9 14:45:05 2015 UTC,saying that current desktops will never be outmatched by future mobile devices is just absurd.   ... And no one but you is saying that.
hardware,3cnkxm,SPOOFE,-2,Thu Jul 9 15:03:26 2015 UTC,"This guy said it. I'm saying the opposite.  I collect antiquated computer hardware and I can say, for a fact, that modern mobile devices have long and far away surpassed several generations of early computers.  ...unless you really want to trade your smartphone for my spare TRS-80. It can output 16 colors! I could use a new phone, mine struggles with 1080p video."
hardware,3cnkxm,cadika_orade,8,Thu Jul 9 15:18:07 2015 UTC,Can it do what a current day desktop can?
hardware,3cnkxm,TeutorixAleria,9,Thu Jul 9 14:29:21 2015 UTC,"Of course not. Who said current day?  ""Your desktop will be in your pocket soon!"" implies that my current desktop will be matched or bettered by a mobile device ""soon"". My current desktop is already getting fierce competition from Nvidia's Tegra K1 processor, so I'd bet my current desktop will be beat within a few more years."
hardware,3cnkxm,cadika_orade,8,Thu Jul 9 14:49:48 2015 UTC,"""Desktop"" describes form factor. You'll always be able to pump more juice through a large chassis that can provide active cooling and fat heatsinks than you will through a dinky, sub-cm device."
hardware,3cnkxm,SPOOFE,-2,Thu Jul 9 14:58:11 2015 UTC,"Yes. Obviously.  But as technology advances, the same processing power can be achieved with less energy and less heat dissipation.  At my current job, I'm helping develop a fully-featured web server about 2"" wide. That's the whole unit, not the chip. It can run off a 9v battery if it needed too, and is a heck of a lot more powerful than Pentium 1 machine I keep for vintage games."
hardware,3cnkxm,cadika_orade,3,Thu Jul 9 15:05:00 2015 UTC,Who said current day?   It was pretty heavily implied. Otherwise the statement had been true almost 10 years ago.
hardware,3cnkxm,bfodder,-1,Thu Jul 9 20:38:23 2015 UTC,"TIL ""soon"" implies ""today""."
hardware,3cnkxm,cadika_orade,4,Thu Jul 9 20:43:56 2015 UTC,"""Your desktop will be in your pocket soon!"" implies that there won't be a need for a larger physical desktop, because everything can be done on the pocket computer.  THIS is the part that isn't true, because desktops will always, 100% of the time, outmatch small form factor devices."
hardware,3cnkxm,Ellimis,-1,Thu Jul 9 20:26:23 2015 UTC,"Well if you pick the most illogical interpretation of a statement, of course there'll be problems.   Your desktop   As in the desktop I currently use.   will be in your pocket   As in, matched or surpassed by a mobile device. Obviously I can't fit a whole desktop PC in my pocket, and the phrase has been used in mobile device marketing since the earliest PDAs.   soon!   As in, in the near future, near being a completely subjective value. Personally, I consider 5-10 years to be the near future for technology, but I doubt it will take that long for future tablets and phones to overtake mid-range PCs of mid-2015.     desktops will always, 100% of the time, outmatch small form factor devices.   Now that's just silly. My desktop at work runs off a Core2 Duo. My 5 CPU core, 128 CUDA core Tegra K1 tablet is definitely more powerful than this, and many other contemporary desktop PCs.  Try this:   High-end desktops will always, 100% of the time, outmatch small form factor devices released at roughly the same time.   See that? A factual, bullet-proof statement that I think is more in line with what you are trying to say."
hardware,3cnkxm,cadika_orade,2,Thu Jul 9 20:37:55 2015 UTC,"Because your first Windows 95 desktop was made in the 90s, using 90s tech and 90s understanding of what computers could do. Remember that Crysis came out only nine years after the original Half-Life. Also remember that as early as ten years ago, your cell phone had more power than the computer NASA used in the moon landing.  Maybe in ten or fifteen or twenty years, your phone will be able to run Witcher 3 or GTAV the way the Nvidia Shield can run Half-Life 2, but not the games coming out then."
hardware,3cnkxm,teuast,-2,Thu Jul 9 17:47:45 2015 UTC,"Obviously.   Maybe in ten or fifteen or twenty years, your phone will be able to run Witcher 3 or GTAV   Exactly my point. Thought, considering that technology advances faster every day, I doubt it will be 10 years or more. Xbox One can play GTAV already, and the Shield is already about as powerful, graphically. I give it 5 years.  5 years before the power of my current desktop is in my pocket. 5 years, also known as ""soon."""
hardware,3cnkxm,cadika_orade,2,Thu Jul 9 17:52:02 2015 UTC,Because you've got a god damn 20 year gap in there.
hardware,3cnkxm,bfodder,1 point,Thu Jul 9 20:37:24 2015 UTC,"Alright.  May tablet (bought by me in 2014) is significantly more powerful than my office desktop PC (bought by my company in 2012).  Tablet: 5 ARM cores, 128 CUDA cores. Tegra K1.  Desktop: 2 x86 cores. Core2 Duo.  The point remains that mobile devices can, have, and will continue to match the power and functionality of older PCs. They already match current low-end PCs and can hold their own against a mid-range prebuilt. Soon, i.e. in 5 years or so, I expect tablets to match the functionality of current, i.e. built in 2015, high-end desktop PCs."
hardware,3cnkxm,cadika_orade,2,Thu Jul 9 20:48:25 2015 UTC,"Your first problem is that no, the tablet is not more powerful than the desktop. More cores doesn't mean more power.   The point remains that mobile devices can, have, and will continue to match the power and functionality of older PCs.   No shit. A god damn calculator these days is more powerful than the first computers. That isn't what we are arguing. You are misunderstanding the statement."
hardware,3cnkxm,bfodder,0,Thu Jul 9 21:07:24 2015 UTC,"More cores doesn't mean more power.   More cores, more system memory, faster CPU clock, significantly higher benchmark scores...  Exactly how do you determine the power of two computing devices, then, if benchmarks and specs are invalid?  The desktop barely runs Windows XP because it's got a low-end Core2 Duo and 512MB of RAM. The tablet can play 4k video and decent-quality games in 1080p 60fps.  Source: A lifetime of computer experience, master's degree in computer engineering, and actually having used the hardware and benchmarked it.  I love that you miraculously know the performance of hardware you've never seen before. Where do I learn this skill?"
hardware,3cnkxm,cadika_orade,-1,Fri Jul 10 00:01:51 2015 UTC,Keep in mind most people aren't big on pc gaming.  Also graphics amplifiers are a thing.
hardware,3cnkxm,pabloe168,8,Thu Jul 9 13:31:15 2015 UTC,Which is why I mentioned workstations too.   The average pleb might have their main PC in their pocket even today but they are not desktop computers
hardware,3cnkxm,TeutorixAleria,3,Thu Jul 9 13:34:59 2015 UTC,Bet you didn't know how much of a shitstorm this comment would ignite.
hardware,3cnkxm,bfodder,1 point,Thu Jul 9 21:14:18 2015 UTC,"Yeah, I think this is an idea that has been sought after ever since the 80s. People have been let down countless times when it comes to this. As far as we know the x86 phone could be a bastard child project from Google and Intel."
hardware,3cnkxm,pabloe168,2,Thu Jul 9 22:35:46 2015 UTC,You'd need big pockets.
hardware,3cnkxm,PeregrineFury,2,Thu Jul 9 16:47:58 2015 UTC,Unless you have a bottomless pocket.
hardware,3cnkxm,damnshoes,2,Thu Jul 9 17:49:46 2015 UTC,"If a small computer is as fast as my desktop, my desktop by that time will be far superior, still. More cooling, more power, more space for more of everything will always be better. Unless we're at a point where we use some kind of computing dimension and our computers are just interfaces. I guess then size doesn't matter"
hardware,3cnkxm,SingleBlob,2,Thu Jul 9 18:04:52 2015 UTC,"True, but your phone doesn't have to faster. It just has to be good enough for comfortable browsing and general usage. Then when its compatible with the needs of major markets it can become widely adopted and those who chose so can get a simplified desktop. I am talking out of my ass with what I am about to say, but if there are graphics amplifiers for laptops, then who says that we are that far from graphics amplifiers for a x86 phone. Generally speaking processing workloads are falling more and more into the realm of gpus, so maybe your phone's cpu doesn't have to be that strong to drive 1080p. That is the amplifier's job.  Clearly this isn't meant to replace servers and render farms or anything of that sort."
hardware,3cnkxm,pabloe168,1 point,Thu Jul 9 22:33:18 2015 UTC,I'd settle for full fledged Ubuntu desktop.
hardware,3cnkxm,Nicolay77,1 point,Thu Jul 9 18:01:56 2015 UTC,"can be easily circumvented by throwing some more power and physical space   You know how the power you get in a desktop today is equal to a mainframe a decade ago? That means that throwing more ""power and physical space"" at something to get more processing power does not scale for consumers"
hardware,3cnkxm,1337Gandalf,-8,Sat Jul 11 13:07:38 2015 UTC,"No, x86 isn't going to go anywhere near mobile. it's simply too fat."
hardware,3cnkxm,pabloe168,11,Thu Jul 9 12:07:53 2015 UTC,What makes you think that phones like the zenfone2 aren't going anywhere? so many drivers and software already being x86 is a big motivator.
hardware,3cnkxm,1337Gandalf,-8,Thu Jul 9 12:23:29 2015 UTC,"Because what a smartphone needs, is 20 year old enterprise software... that'll surely do the trick."
hardware,3cnkxm,pabloe168,5,Thu Jul 9 13:13:04 2015 UTC,Most of the hardware in the planet is x86 compatible though.
hardware,3cnkxm,1337Gandalf,-7,Thu Jul 9 13:14:50 2015 UTC,"That's literally the stupidest thing I've ever heard...  Protip: hardware compatibility is dictated by physical connectors (SATA, PCIe, ACPI, etc) and drivers for that hardware, no where in that list is CPU architecture; because it's 100% irrelevant."
hardware,3cnkxm,pabloe168,2,Thu Jul 9 13:17:55 2015 UTC,I was obviously talking about the drivers.
hardware,3cnkxm,candre23,4,Thu Jul 9 13:19:21 2015 UTC,"Nonsense.  There have been cheap atom-based tablets for a couple years now, and the next generation of atoms have a line specifically targeting low cost smartphones."
hardware,3cnkxm,1337Gandalf,-8,Thu Jul 9 12:43:22 2015 UTC,"and those tablets have gone. nowhere.   x86 requires too much energy, and atom isn't even 64 bit. currently Intel is fucked, and will continue to be fucked for the foreseeable future. they may pull a rabbit out of their ass, who knows. but it won't be in the next couple years, that's for sure.  Also, Intel is far too expensive for a smartphone or tablet, no one will buy a $50 CPU for a mobile device, period."
hardware,3cnkxm,candre23,8,Thu Jul 9 13:12:26 2015 UTC,"Somebody hasn't been paying attention.  Since winRT was a complete flop, every windows tablet in the last couple years has been x86, and most of those have been atoms.    There's the Asus transformerbook and vivotab series, the Dell venue 8 pro, and the Acer aspire switch to name a few of the better ones.  At the low end, you can get any number of cheap Chinese atom-powered win8.1 tablets for well under $100.    Oh, and all of those atom tablets use 64bit chips.  I'd have to dig through intel's site, but I don't think their current lineup includes any 32bit chips not intended for embedded applications.  This is compared to ARM fabs, which have only just started to come out with 64bit chips in the last 9 months.  Both windows and linux for 64bit x86 are mature/mainstream and have been for many years.  Winphone and Linux (and by extension, android) for 64bit ARM are in their infancy.  Linux/android will improve quickly enough with new devices coming out for folks to fiddle with, but right now it's nowhere near mainstream.  As for price, you're way off base.  There are atoms priced as low as $17 (less, if you're a big manufacturer).  That's on par with similarly-powerful ARM chips.  With ~2W SDP, they're similar in power usage as well."
hardware,3cnkxm,TeutorixAleria,9,Thu Jul 9 13:50:36 2015 UTC,The atom chips in the zen phone run cooler than the snapdragon 810 you don't see anyone claiming arm is dead.
hardware,3cnkxm,surbryl,1 point,Thu Jul 9 13:26:03 2015 UTC,"Yeah, I'd much rather see desktops switch to RISC."
hardware,3cnkxm,frozenbobo,7,Thu Jul 9 12:20:14 2015 UTC,"x86 processors already translate the instructions into a completely different internal RISC instruction set anyway. At this point, the usage of CISC instructions is basically a means of compressing instructions and to maintain compatibility."
hardware,3cnkxm,1337Gandalf,-2,Thu Jul 9 13:17:18 2015 UTC,"I kinda hope they'll switch over to ARM when it's competitive with an i7 (apparently high end ARM cpus are on par with Core 2) in a few years, for the energy efficiency."
hardware,3cnkxm,Yourothercat,-1,Thu Jul 9 13:19:19 2015 UTC,"It will never happen though, the WIntel monopoly is here to stay."
hardware,3cnkxm,barthw,8,Thu Jul 9 13:09:02 2015 UTC,"Yep, that's my plan as well. My 4790K has plenty of power even at stock speeds, so as far as gaming and general computing goes it should be fine until at least 2018."
hardware,3cnkxm,everyZig,5,Thu Jul 9 09:08:36 2015 UTC,"Yeah, i just have a modest i5 3470, but given my current usage (some light casual gaming, borderlands is probably the heaviest thing i run untill Witcher 3 gets a linux port), even a celeron should be fine really"
hardware,3cnkxm,Ellimis,3,Thu Jul 9 09:25:05 2015 UTC,"Nah dawg, gotta get dat G3258. Pentium master race.   I actually bought like 4 extra last time they went on sale because I keep building computers for people with them.   I personally still rock an i7-2600k, but I also do heavy video editing."
hardware,3cnkxm,Nisc83,2,Thu Jul 9 20:38:23 2015 UTC,"I, like you, have a 4790k. This is a correct assessment, at least for cpus. Besides most of the computation power rests largely on the GPU now of days anyway."
hardware,3cnkxm,djsd,1 point,Thu Jul 9 09:43:22 2015 UTC,I have a 4770k that just chills these days. It is a streaming box for Netflix and YouTube. I should probably get back into gaming one day.    If this is all I do with it. It will easily last until 2019. I might get new fans or a new case and make it super quiet.  I am still running the stock fan :(  .
hardware,3cnkxm,spyd3rweb,5,Thu Jul 9 11:45:10 2015 UTC,"I have ye olde Phenom 1100T at 45nm even, no reason at all to upgrade for a long time."
hardware,3cnkxm,58592825866,4,Thu Jul 9 09:49:22 2015 UTC,"X4 965 here, with catalyst 15.7 + WDDM2.0 reducing CPU overhead, and GTA5 already performing nicely on my system, I really doubt I'll be upgrading before 2017. Not to mention DX12 which will completely eliminate the need for a CPU upgrade if it gains quick traction. If DX12 takes off, I could see myself keeping this CPU until 2020. 11 years, that must be some sort of record for how long a CPU had remained viable for modern games."
hardware,3cnkxm,Ellimis,1 point,Thu Jul 9 10:49:44 2015 UTC,Just don't try to play batman.
hardware,3cnkxm,everyZig,2,Thu Jul 9 20:39:13 2015 UTC,"Ah, ye olde phenoms, good chips! (the GF had a 940BE for years, very competent chip)"
hardware,3cnkxm,III-V,1 point,Thu Jul 9 09:54:32 2015 UTC,"The Phenom IIs that is, the first were garbage. I know first hand..."
hardware,3cnkxm,everyZig,2,Thu Jul 9 15:16:15 2015 UTC,"Indeed  I had an x2 7750 for a while, which was halfway decent despite being a gen I phenom at heart, at least it ran at 2.7 GHz and was cheap"
hardware,3cnkxm,III-V,2,Thu Jul 9 15:31:26 2015 UTC,"They were late, not really competitive on price/performance (particularly on AMD's side of things -- big die), slow, had the TLB bug that was stupidly responded to by chopping something like 8% of the performance of the chip (until a later revision fixed it), and they also consumed a lot of power.   It was even worse than Bulldozer, but at least AMD didn't pour as much money into the development of Phenom, and they were better equipped to tolerate the loss at the time..."
hardware,3cnkxm,everyZig,2,Thu Jul 9 17:44:35 2015 UTC,"Yeah, at least Phenom II was one hell of a comeback, increased IPC, increased clockspeeds, power usage under controll, good OCability  Bulldozer never really got significantly better, Vishera helped a bit, but they arent even bothering bringing Kaveri or Carrizo to AM3+"
hardware,3cnkxm,CaptSkunk,1 point,Thu Jul 9 18:03:59 2015 UTC,"I still have my old 965 BE. Sadly, it is now retired, after 5 years of good times. It got replaced with a 4690k, which is a more than worthy successor.  Phenom II, the last truly great lineup from AMD. Here's to Zen."
hardware,3cnkxm,BBLF5112,1 point,Thu Jul 9 12:14:13 2015 UTC,X4 9xx BE reporting in! Presses F
hardware,3cnkxm,CaptSkunk,1 point,Thu Jul 9 13:01:21 2015 UTC,"Best thing about that chip was that it only costed €149,99."
hardware,3cnkxm,BBLF5112,1 point,Thu Jul 9 13:09:16 2015 UTC,"Looking back at my build, it was a 'AMD Phenom II X4 965 Black Edition AM3 CPU' and I paid $179.99 USD back in 2010."
hardware,3cnkxm,mikami677,2,Thu Jul 9 13:11:31 2015 UTC,"I feel like I've been bringing this up a lot lately (on various subs, and IRL), but even my 45nm i7 920 is going strong enough that I don't see myself needing an upgrade for at least another few years."
hardware,3cnkxm,everyZig,5,Thu Jul 9 13:39:34 2015 UTC,"Yeah, you guys still sticking with 1366 are an inspiration in that regard, if you are still ok with that CPU these days, my ivy i5 should last for quite a while longer"
hardware,3cnkxm,XorFish,1 point,Thu Jul 9 13:41:40 2015 UTC,sure about the ivy?  The x5660 overclock really good and with growing multicore support. I mean I hit over 900 points in cinebench pretty easy.
hardware,3cnkxm,JaketheAlmighty,1 point,Thu Jul 9 17:08:09 2015 UTC,"Still using my 980X here. I might buy a new Skylake-E top end chip when they come out, but it'd be basically entirely for an up to date motherboard and X99 chipset. The cpu itself does not need to be upgraded yet for any reason."
hardware,3cnkxm,XorFish,1 point,Thu Jul 9 15:23:41 2015 UTC,"If you want to reduce power consumption and increase performance, you can get a x5650/60. With dx12/vulkan that 6core xeon might last a few more years with an reasonable overclock"
hardware,3cnkxm,sbjf,2,Thu Jul 9 17:03:03 2015 UTC,"My 45nm chip from 2008 is still serving me nicely, and my plan is to upgrade next year."
hardware,3cnkxm,XorFish,2,Thu Jul 9 10:42:20 2015 UTC,Are you already on x58?
hardware,3cnkxm,sbjf,1 point,Thu Jul 9 17:12:43 2015 UTC,"Yeah, why?"
hardware,3cnkxm,XorFish,2,Thu Jul 9 17:53:41 2015 UTC,"http://www.overclock-and-game.com/hardware/computer-tech-reviews/28-x5660-review  With dx12 and Vulkan, Skylake won't be that much ahead of this chip."
hardware,3cnkxm,sbjf,1 point,Thu Jul 9 18:07:11 2015 UTC,"Yeah, I've thought about upgrading to that chip (or rather the x5650), from my current i7 920, but I think I'll just wait, not really worth it IMO, while it has a lower power consumption and more cores, the single thread performance is basically the same, and I was looking for something with more boosh there."
hardware,3cnkxm,XorFish,1 point,Thu Jul 9 20:38:31 2015 UTC,If your i7 overclocks not well it might be worth it. My X5660 overclocks to 4.2 with 1.36 vcore and low temps. (Vdrop enabled)
hardware,3cnkxm,Abrum,1 point,Thu Jul 9 20:47:30 2015 UTC,"I'm on a 3570K, and was considering upgrading when a good -lake CPU dropped, but I'm not seeing any reason to honestly."
hardware,3cnkxm,skilliard4,17,Thu Jul 9 19:47:52 2015 UTC,"All told, IBM and its partners are targeting ""at least a 50 percent power/performance improvement for the next generation of systems""—that is, from 10nm down to 7nm.   Considering that germanium can't handle high temps as well as silicon, I doubt we'll see high performance CPUs resulting from this(as clock rates will need to remain low), but this could benefit markets that demand power efficiency."
hardware,3cnkxm,bardak,5,Thu Jul 9 11:02:05 2015 UTC,I'm willing to bet that the majority of the market today is with Mobile and other low power applications. Even if this process cannot hit high TDPs I don't think I will be an issue for IBM and there partner's.
hardware,3cnkxm,barthw,54,Thu Jul 9 14:44:00 2015 UTC,"Incredible, but yields are probably still pretty low, so some time will pass until anyone can ship 7nm CPUs at reasonable prices as we are still struggling with 10nm."
hardware,3cnkxm,skilliard4,54,Thu Jul 9 09:07:00 2015 UTC,"It uses silicon-germanium, the manufacturing process for it is likely quite different, we don't know if it has the same problems as pure silicon."
hardware,3cnkxm,TeutorixAleria,13,Thu Jul 9 10:51:59 2015 UTC,Isn't germanium bigger than silicon?
hardware,3cnkxm,cadika_orade,14,Thu Jul 9 13:20:17 2015 UTC,Bigger how?
hardware,3cnkxm,TeutorixAleria,24,Thu Jul 9 14:20:33 2015 UTC,"It's a larger atom, making smaller things with it should be more difficult."
hardware,3cnkxm,slapdashbr,34,Thu Jul 9 14:30:00 2015 UTC,"yes, but the electrical properties make up for the larger size. Ge has an easier time shedding electrons than Si, if I recall correctly, so even with fewer atoms there should be less resistance.   Making Si-Ge crystals instead of pure silicon might be the hardest part of the whole thing although the extreme UV lasers are pretty crazy too."
hardware,3cnkxm,TeutorixAleria,5,Thu Jul 9 15:03:39 2015 UTC,Less electrical resistance but more susceptible to the effects of heat.
hardware,3cnkxm,ritz_are_the_shitz,4,Thu Jul 9 15:16:49 2015 UTC,So lower temp safe maximums?
hardware,3cnkxm,TeutorixAleria,0,Thu Jul 9 17:26:36 2015 UTC,Which means lower clock speeds or in the case of servers massively more expensive cooling budgets.
hardware,3cnkxm,dylan522p,3,Thu Jul 9 17:29:28 2015 UTC,SiG transistors switch faster not slower. They have a lower temp though so maybe it balances out but Intel has papers saying it would be able to clock higher in a commercial product though.
hardware,3cnkxm,cadika_orade,6,Thu Jul 9 17:43:45 2015 UTC,"If my math is right, 7nm is roughly 70 atoms wide. You make a good point."
hardware,3cnkxm,TeutorixAleria,18,Thu Jul 9 19:46:01 2015 UTC,It's actually closer to 30 germanium atoms wide if you use the van der walls radius
hardware,3cnkxm,fattyforhire,7,Thu Jul 9 14:47:48 2015 UTC,That's ridiculous. I can't believe we can manufacture things that small...
hardware,3cnkxm,comickeys,9,Thu Jul 9 14:51:40 2015 UTC,I wonder what is the theoretical/practical limitation of how small these will get.  I remember long back that it was 4 atom wide and 1 atom tall.  http://www.purdue.edu/newsroom/research/2012/120105KlimeckPhosphorus.html
hardware,3cnkxm,Exist50,8,Thu Jul 9 14:57:59 2015 UTC,"Note, 7nm doesn't have a specific feature it refers to. Marketing can get in the way sometimes."
hardware,3cnkxm,cadika_orade,7,Thu Jul 9 16:28:55 2015 UTC,7 NANOMETERS!!  On one transistor
hardware,3cnkxm,alainmagnan,1 point,Thu Jul 9 16:43:35 2015 UTC,A transitor is probably bigger
hardware,3cnkxm,malicious_turtle,0,Thu Jul 9 16:46:11 2015 UTC,When you here the nanometers of something that's smallest part of the transistor so ya the transistor is definitely bigger.
hardware,3cnkxm,KibblesNKirbs,2,Thu Jul 9 17:46:01 2015 UTC,Doesn't it refer to smallest transistor gate width on individual transistors?
hardware,3cnkxm,Tuna-Fish2,2,Thu Jul 9 19:08:26 2015 UTC,"No, it originally used to refer to the width of a line + the space required between lines for the smallest conducting lines done on the process.  These days the number is mostly meaningless."
hardware,3cnkxm,polynomialpusher,1 point,Thu Jul 9 20:10:58 2015 UTC,"The image in the article shows it as a 30nm pitch.   Bulk 7nm transistors, with a 30nm pitch (the distance between the front edge of one transistor and the front edge of the next transistor).   So 7nm gap with a 30nm pitch is still insanely tiny."
hardware,3cnkxm,Randomoneh,2,Thu Jul 9 21:20:28 2015 UTC,Isn't this pitch what really matters in terms of size?
hardware,3cnkxm,polynomialpusher,1 point,Thu Jul 9 20:28:19 2015 UTC,Yes and also other features of the transistor itself.
hardware,3cnkxm,VirtualMachine0,1 point,Thu Jul 9 21:44:00 2015 UTC,Doing anything else would require tiny atoms... And have you prices those lately? I'm not made of money. Leave me alone.
hardware,3cnkxm,jingleberry512,1 point,Thu Jul 9 22:00:33 2015 UTC,"Yes but it isn't a full alloy, the silicon is just doped with the germanium to change its properties. I cant remember if germanium is an n or p dopant, though."
hardware,3cnkxm,TeutorixAleria,7,Fri Jul 10 03:11:10 2015 UTC,Germanium isn't a dopant it's a semiconductor.
hardware,3cnkxm,jingleberry512,2,Thu Jul 9 17:43:44 2015 UTC,I thought seniconductors could still be dopants? I might be remembering my nanotech course wrong though. You usually use elements from groups III and V to dope
hardware,3cnkxm,TeutorixAleria,3,Thu Jul 9 17:50:49 2015 UTC,Yeah and germanium is a group IV element doping silicon with it wouldn't do anything
hardware,3cnkxm,frozenbobo,4,Thu Jul 9 18:08:26 2015 UTC,"That's not true. Doping with germanium strains the silicon lattice, which increases the mobility. Yes it doesn't create N or P type silicon, but that's very far from not doing anything."
hardware,3cnkxm,jingleberry512,1 point,Thu Jul 9 18:25:02 2015 UTC,Oh right. Must have forgotten my periodic table for a moment there.
hardware,3cnkxm,TheImmortalLS,1 point,Thu Jul 9 20:14:59 2015 UTC,Not even the kids in my high school chem class dared to say that because they feared they'd look immature
hardware,3cnkxm,AstralElement,7,Thu Jul 9 18:29:24 2015 UTC,"It's Technology and Development. Meaning, the process hasn't even been fully established and taped out at a Manufacturing Fab at all. Proof of concept has been established, and now the process can begin to ramp with tool installation at a facility, likely shared between Samsung in Austin and Globalfoundries at Fab 8."
hardware,3cnkxm,veyron3003,1 point,Thu Jul 9 18:15:25 2015 UTC,Remember the a bad yield can be that it needs a little more power to be stable.
hardware,3cnkxm,JimJamJamie,34,Fri Jul 10 01:38:24 2015 UTC,From the article:   The gap between two silicon nuclei is about 0.5nm   Holy shit. The chip itself is on 7nm tech. We're going to need to start shrinking the atoms themselves!
hardware,3cnkxm,flukshun,26,Thu Jul 9 17:36:40 2015 UTC,Can't wait for the first batch of charged-quark processors
hardware,3cnkxm,1337Gandalf,8,Fri Jul 10 14:34:28 2015 UTC,Praise Yeezus
hardware,3cnkxm,BrainSlurper,7,Thu Jul 9 08:47:43 2015 UTC,Fuck what ever yall been hearin
hardware,3cnkxm,flyafar,3,Thu Jul 9 10:52:42 2015 UTC,we out here fam
hardware,3cnkxm,jesusisnowhere,1 point,Thu Jul 9 11:50:01 2015 UTC,y e z z y  represent fam
hardware,3cnkxm,GodKingThoth,1 point,Thu Jul 9 16:52:12 2015 UTC,Crikey call captain quark!
hardware,3cnkxm,Buffalox,11,Thu Jul 9 22:11:13 2015 UTC,"Or simply use elements that have smaller atoms. Silicon has atomic number 14 carbon has 6, and we already know that carbon can do amazing things depending on how it is packed, packed one way it's graphite which is a good electric conductor, another and it's diamond which is a good electric insulator, single layer it's graphene which has shown surprising properties, and all the forms have good heat transfer and resistance.  It is the most versatile to connect with other atoms to form composites, and with silicon it can be made into silicon-carbide that incidentally also is a semiconductor with great heat transfer and resistance properties.  http://accuratus.com/silicar.html  I'm not saying this as the way to go, I bet the experts in the area already know if it is, but I bet that we are at the point, where silicon will be replaced to higher degrees for each iteration of improvement, and carbon seems like the obvious base atom to make a material with properties as near perfect as possible."
hardware,3cnkxm,TeutorixAleria,7,Thu Jul 9 22:34:12 2015 UTC,We need smaller semiconductors not just smaller atoms. Silicon and germanium are all we have at present that can actually be used
hardware,3cnkxm,Buffalox,1 point,Thu Jul 9 13:20:11 2015 UTC,"I was merely responding to ""we need to start shrinking the atoms"", although I guess it was a joke, it can be done, but it can't be done practically, as in making a difference or be usable under normal conditions, at least not AFAIK.  Smaller semiconductors require smaller lanes, lanes are so narrow now they only span a few atoms. Therefore, to make smaller lanes, we may soon need to use atoms that are smaller, smaller atoms have loser electrons, which makes lithium seem like an obvious choice, but isn't for several reasons, it might however work as part of a composite.  To change the size of atoms, would require massive pressure, and when the pressure is released, I doubt the atoms will remain in their compressed state. I don't expect that solution to become viable for at least a couple hundred years. Therefore we may soon need to replace silicon as the main component of the semi conducting material to make better chips. My bet is on Carbon as key to the best probable solution."
hardware,3cnkxm,III-V,8,Thu Jul 9 12:52:12 2015 UTC,"""7 nm"" is just a marketing label. It has no bearing on the actual geometry."
hardware,3cnkxm,supermanunc,9,Thu Jul 9 13:33:38 2015 UTC,"The end of the article talks about competition for Intel.  Could anyone explain what kind of shifts in terms of business we could expect, or is it too early to say?   Sorry if it's a poor question, you might call me a layman when it comes to things like this :)  Edit: Thanks for the replies and discussion!"
hardware,3cnkxm,Typat,9,Thu Jul 9 15:54:03 2015 UTC,"Meh, while they both make hardware, they sell to very different markets. IBM specializes in server hardware. They sell their chips/boxes at enormous prices. I'm unsure if IBM plans to move into consumer hardware but I seriously doubt it."
hardware,3cnkxm,III-V,5,Thu Jul 9 12:54:31 2015 UTC,"IBM's basically a software company now, no (aside from R&D)? Intel gobbled their server market share ages ago."
hardware,3cnkxm,Typat,5,Thu Jul 9 11:01:26 2015 UTC,"*service company. They pride themselves on being a service company. But they still are developing (obviously) hardware for their servers. They are trying to move into cloud services soon, and I am assuming they will be using their own hardware.  They did just give GlobalFoundries a boat load of money to TAKE most of their hardware divisions. This includes ASICs and some server people."
hardware,3cnkxm,Moses89,9,Thu Jul 9 11:15:58 2015 UTC,Intel wants to move into IBM's market really.
hardware,3cnkxm,III-V,6,Thu Jul 9 15:17:40 2015 UTC,They already did...
hardware,3cnkxm,Evidence_Of_Absence,5,Thu Jul 9 16:20:12 2015 UTC,It's just speculation.  Just because IBM is the first to publicly unveil it doesn't necessarily mean they're the first to do it.  It's not at all uncommon to have a functional process with shitty yields years before commercial release.  Can't really speculate on a 3-5 year timeline when Intel and TSMC have been silent.
hardware,3cnkxm,III-V,2,Thu Jul 9 11:57:19 2015 UTC,"Yeah, Intel's been tinkering with this stuff for a while (if you look at some of the papers they've published), but they haven't even gone public with 10 nm, let alone their plans for 7 nm."
hardware,3cnkxm,Typat,0,Thu Jul 9 15:20:17 2015 UTC,"Very true. IBM has a 1.5-2 year lifecycle for their technologies. This includes the time of first fabrication to sales and finally field fail analysis. However, the entire time during that life cycle, yield analysis is being performed on the chips to get them as high as possible."
hardware,3cnkxm,pal25,3,Thu Jul 9 14:17:13 2015 UTC,"Could anyone explain what kind of shifts in terms of business we could expect   Not much. Chances are that Intel can make 7nm wafers or at least isn't very far behind. The trick, which nobody can AFAIK do, is to manufacture large amounts of them."
hardware,3cnkxm,Nisc83,10,Thu Jul 9 15:19:33 2015 UTC,How did they work around the errors caused by electron jump?
hardware,3cnkxm,cuddlefucker,20,Thu Jul 9 16:23:57 2015 UTC,They used silicon germanium instead of just silicon.
hardware,3cnkxm,therealbighairy,10,Thu Jul 9 16:52:17 2015 UTC,"They gave it Ritalin, and it calmed right down.  I'm actually curious about this too."
hardware,3cnkxm,polynomialpusher,1 point,Thu Jul 9 09:38:39 2015 UTC,Quantum Tunneling or are you referring to another process?
hardware,3cnkxm,Buffalox,1 point,Thu Jul 9 12:30:38 2015 UTC,"Is that the so called bleeding you are referring to? I was also a bit surprised to see the claim of more than 50% power saving over 10nm and even more compared to 14nm, and then zero mention of bleeding."
hardware,3cnkxm,Nisc83,2,Thu Jul 9 10:25:20 2015 UTC,"Yes, I was referring to the bleeding of electrons from one 7nm lane onto another."
hardware,3cnkxm,Pxorp,5,Thu Jul 9 20:36:06 2015 UTC,"Did Intel show their plan for ""after silicon"" already?"
hardware,3cnkxm,malicious_turtle,5,Thu Jul 9 12:59:56 2015 UTC,"Despite what the 2 circlejerky comments below me would have you believe Intel does have plans for ""after silicon"". Most likely they're spending more than anyone else (probably combined) since the first with a post silicon chip will destroy the competition. The next semiconductor is most likely going to be a ||| - V semiconductor, Intel (and everyone else) are researching them now. Intel is also pouring billions into next gen manufacturing technologies: Here."
hardware,3cnkxm,flyafar,1 point,Thu Jul 9 13:07:44 2015 UTC,Is it okay to be hyped for this? Will I finally be able to run Zoo Tycoon 2 (Heavily IPC-bound) at 60fps with a 5-star zoo? Will my 4790K last 4 years for gaming?
hardware,3cnkxm,zxcdw,-6,Thu Jul 9 13:14:45 2015 UTC,"Yes, by delaying their 10nm lineup and sticking with 14nm. That's their ""after silicon"" plan for now."
hardware,3cnkxm,lly091,5,Thu Jul 9 19:32:01 2015 UTC,Seriously wondering: what was IBMs last big breakthrough/contribution to the tech world? Were 10nm and 14nm chips also innovated by them or was that all Intel?
hardware,3cnkxm,Evidence_Of_Absence,8,Thu Jul 9 22:12:49 2015 UTC,"They pioneered the SiGe HBT, co-invented the copper damascene process that is used in basically every modern semiconductor process, and are probably #1 with SOI technology as well.  You don't hear as much about that them on this sub because a lot of their work (SiGe BiCMOS and, to some extent, SOI) is more RF-oriented than digital, and copper damascene was late 90s/early 2000s depending on where we're pinning invention vs. full-scale commercial roll-out across multiple companies.  But no, they haven't lead a CMOS node commercial release at any point in recent history."
hardware,3cnkxm,jaydub,3,Thu Jul 9 13:41:27 2015 UTC,"What exactly does the ""7nm"" mean? Is it the width of the trace of the 'wire', or something like distance between consecutive transistor?"
hardware,3cnkxm,Bondator,8,Thu Jul 9 14:22:11 2015 UTC,"Within a transistor, there's basically two electrically conducting wire ends right next to each other, and that 7nm is the distance between them, called gate width. When the gate is turned on, an electrically conducting channel is formed in the gate, connecting the two wires. When it is off, that conducting channel goes away. It's getting really difficult to minimize transistors further, since your everyday consumer cpu already has gate width in the ballpark of 50 atoms, and you still want the transistor to not leak current when it's turned off."
hardware,3cnkxm,Exist50,3,Thu Jul 9 14:26:39 2015 UTC,"It's not really anything well defined. IBM might publish it somewhere, but especially with the recent 16nm/14nm chips, the meaning of the term is muddled somewhat. TSMC's 16nm FF is basically identical in size to its 20nm planar."
hardware,3cnkxm,drumdubb,1 point,Thu Jul 9 16:08:30 2015 UTC,"It's more about optimization, they should be able to put twice as many transistors on a chip eventually."
hardware,3cnkxm,drumdubb,3,Thu Jul 9 17:18:10 2015 UTC,"It's the width of the smallest feature on a transistor, similar to line size in autocad.   EDIT: They're getting closer to the limits of the silicon material. There will have to be a breakthrough at some point."
hardware,3cnkxm,dghughes,1 point,Thu Jul 9 16:47:11 2015 UTC,"It's also impressive that stereolithography can even exist at such small wavelengths since 7nm crossed from extreme ultraviolet (EUV) to the top end of x-rays edit: uses complex optics not x-rays.   But I see in the article there is some fancy ""complex optics"" which allow a larger frequency to write smaller features so an EUV at 13.5nm can write sub 10nm features, I never knew, that's quite interesting!  If someday soon an actual x-ray laser process is used I wonder what mask material will be used since x-rays would go through most anything but lead."
hardware,3cnkxm,drumdubb,2,Thu Jul 9 17:24:15 2015 UTC,"The whole industry blows my mind. I worked at Globalfoundries in Malta, NY (mentioned in the article) for 2 years. That facility is amazing."
hardware,3cnkxm,tommdonnelly,2,Thu Jul 9 17:23:16 2015 UTC,"The problem with the terminology for process nodes is that it doesn't mean exactly anything anymore. In the old pre-deep submicron days the number was equal to 2 things: The transistor gate length and the half pitch of the first metal layer. So the 0.5 node had a gate length of 0.5 microns and a first metal pitch of 1 micron.  Every new node was the current node multiplied by 0.72 with a little rounding, which doubles the number of transistors. So you had 1 micron, 0.7 microns, 0.5, 0.35, 0.25, 0.18, 0.13 90, 65, 45, 32, 22, 16 and so on.   We could and did take existing designs and scale the layout by 0.72 and port a chip to a new process. That started breaking down at 0.35 but we could still scale layouts until 0.18. By 0.13 The gate lengths were much less than 0.13 and the half pitch was greater, so the node name has no more physical meaning. We persist in naming nodes based on the 0.72 scaling factor, but the name no longer describes the size of any feature."
hardware,3cnkxm,BBLF5112,5,Thu Jul 9 21:26:26 2015 UTC,moore's law?
hardware,3cnkxm,eleitl,6,Thu Jul 9 21:31:39 2015 UTC,Was over at 28 nm http://electroiq.com/blog/2014/03/moores-law-has-stopped-at-28nm/
hardware,3cnkxm,dylan522p,2,Fri Jul 10 04:30:14 2015 UTC,You mean 14/22nm
hardware,3cnkxm,eleitl,2,Thu Jul 9 13:03:10 2015 UTC,"That blog makes the case for 28 nm. Without detailed economics data, which are not publicly available and might vary across fabs and vendors it would be difficult to nail the node which broke Moore's neck.   It doesn't matter that much, the interesting information is that the days of the easy exponential is over. The doubling period is no longer constant, and further improvements would have to come from architecture (which is limited by the software development model) as well as minor ikebana (3D stacking with TSV, Si photonics, etc.)."
hardware,3cnkxm,dylan522p,2,Thu Jul 9 13:31:07 2015 UTC,28nm delays meant it missed the mark aswell for Moores law. 22nm was the last to be on it. 14nm was close but it only shipped in core m at that time. Economics data is irrelevant. You can see a nodes measurements and when it went into production and when it's products were shipping.
hardware,3cnkxm,eleitl,1 point,Thu Jul 9 16:44:02 2015 UTC,"Thanks, interesting information. I'm not keeping that close a tally, since some of it is just noise."
hardware,3cnkxm,dylan522p,1 point,Thu Jul 9 16:49:27 2015 UTC,https://www.semiwiki.com/forum/content/3884-who-will-lead-10nm.html
hardware,3cnkxm,eleitl,1 point,Thu Jul 9 16:53:54 2015 UTC,Very interesting. Thanks!
hardware,3cnkxm,zxcdw,4,Thu Jul 9 16:55:50 2015 UTC,"In theory, 7nm should roll around in 2017/2018, but we wouldn't be surprised if it misses that target by some margin.   So, read that ""we wouldn't be surprised"" as ""we expect that"".   but we expect that it misses that target by some margin.   There you go. So almost certainly not before 2020, and even then first by Intel, and likely only in low-power SoCs. Discrete PC parts come later once the process is actually mature, so with a 3-6 month delay, earliest.  RemindMe! 5 years ""Where are we?"""
hardware,3cnkxm,RemindMeBot,4,Thu Jul 9 17:12:26 2015 UTC,Messaging you on 2020-07-09 12:41:26 UTC to remind you of this comment.  CLICK THIS LINK to send a PM to also be reminded and to reduce spam.    [FAQs] | [Custom Reminder] | [Feedback] | [Code]
hardware,3cnkxm,tru3gam3r,2,Thu Jul 9 18:20:26 2015 UTC,Is this site even gonna be up in 2020?
hardware,3cnkxm,jinxnotit,1 point,Thu Jul 9 12:40:48 2015 UTC,Not if Chairman Pao has anything to say about it
hardware,3cnkxm,scobot,1 point,Thu Jul 9 12:41:37 2015 UTC,You are everything wrong with Reddits entitlement complex.
hardware,3cnkxm,imallin,1 point,Thu Jul 9 20:58:38 2015 UTC,"What is this, a chip for nants?!?"
hardware,3cnkxm,skilliard4,1 point,Fri Jul 10 07:05:34 2015 UTC,"I'll believe it when I see it.  We've been promised 20nm/16nm/14nm for years by TSMC/Glofo/Intel,  and none of them have really delivered yet.  20nm and 14nm smartphone SoC's are out there,  but they are not suitable for something like a GPU/CPU and have shown very little improvement in perf/watt.  Intel is also lagging with their 14nm,  and have only released low power versions that have not been very impressive.  On top of that, their 10nm process has also been delayed.  Making something in a lab,  and having a high-yield, cost effective manufacturing process, are not even close to the same thing."
hardware,3cpbgm,Scrabo,26,Thu Jul 9 18:08:31 2015 UTC,I would love to see the q6600 processor be added in this lineup as it was the first popular mainstream quadcore processor to see where we have come from.
hardware,3cpbgm,spellstrike,8,Thu Jul 9 18:44:37 2015 UTC,"My Q6600 ran for about 6 years. He went through 3 motherboards and got overclocked to 3.6GHZ. He was running everything I run now (a little slower but playable). Upgraded to a I5 4690K and even at it's stock settings it was noticeably faster than my Q6600 but I'll never forget the power of the Core 2 Quad Q6600 Kentsfield. (Newegg ratings have it a 5 out of 5 eggs (3,253 Reviews))"
hardware,3cpbgm,p71interceptor,9,Thu Jul 9 21:54:25 2015 UTC,I'm still using it with a 780ti. Skylake is coming.
hardware,3cpbgm,spellstrike,3,Thu Jul 9 22:21:34 2015 UTC,Respect... Do you have it oc'd?
hardware,3cpbgm,p71interceptor,3,Thu Jul 9 22:41:48 2015 UTC,had it beyond 3.2 with the stock cooler for a few months but ended up setting it back to 2.66 for stability as I didn't really need more more performance out of it.
hardware,3cpbgm,spellstrike,4,Fri Jul 10 00:59:06 2015 UTC,You definitely do with a 780ti in many games. You suffer in single threaded performance Quite heavily. IPC gains of over 80% and sizable clock increase
hardware,3cpbgm,dylan522p,3,Fri Jul 10 04:38:02 2015 UTC,I remember it was monster for the price. Low stock clock but overlooking got it to extreme levels of performance... Easily beat any core duo en in gaming because of this
hardware,3cpbgm,alainmagnan,2,Thu Jul 9 22:14:43 2015 UTC,https://youtu.be/Digugs4GdsI  Thats a video of my Q6600 @ 3.6ghz and a 7950WF running Crysis 3. It's amazing what that old girl could still do.
hardware,3cpbgm,p71interceptor,2,Thu Jul 9 22:35:46 2015 UTC,Mine ran for 7 years or so and had probably several tens of thousands of hours on the clock. I swear I could hear a sigh of relief when I pulled the plug a couple of weeks ago. xD
hardware,3cpbgm,Oinkidoinkidoink,2,Fri Jul 10 07:21:41 2015 UTC,I remember everyone I talked to telling me to just get an E6600 at the time. Nothing uses 4 cores. But that's the time I got more serious into editing and AE animation. And boy did I utilize the fuck out of those cores at that point.  I remember so many rumors and talks that the Q6600 wasn't a true quad but a dual E6600 under the hood. With some sort of interconnect. Any validity in that?
hardware,3cpbgm,SirCrest_YT,5,Fri Jul 10 04:48:59 2015 UTC,Yeah it was pretty much two E6600s connected together. All of Intel's early quad cores were like that. I remember AMD making a big deal about how their quads were 'true' quad core cpus and not two dual cores stuck together. The Intel chips were faster so no one cared about them not being proper quads.
hardware,3cpbgm,pineappleassortment,2,Fri Jul 10 09:04:21 2015 UTC,"Phenom was only really relevant in a specific subset of the HPC and server space, where having a native quadcore design actually did bring with it tangible performance benefits over Intel's MCM implementation. That didn't give AMD a win, it just kept the Opterons from losing as badly relative to their Xeon competitors.   Then again, AMD had to make a big deal out of Phenom (at least initially); no company in its right mind will outright say, ""Our next gen product is not at all competitive, even with lower prices. Come back in 3 years."""
hardware,3cpbgm,doneandtired2014,4,Fri Jul 10 16:09:28 2015 UTC,"That's exactly what it was, this guy took the heatspreader off so you can see, http://forums.extremeoverclocking.com/t289573.html"
hardware,3cpbgm,therealab,2,Fri Jul 10 05:41:51 2015 UTC,Yeah it was 2 core 2 duos stuck together... They shared something between them so it was handicapped in its design.  It was a a Frankenstein quad as opposed to a full blown quad but it still did the job.
hardware,3cpbgm,p71interceptor,4,Fri Jul 10 05:50:14 2015 UTC,"Still using my Phenom II :)  I'd like to see all the old multi core processors benched on the coming low level APIs, I'd find that super interesting personally."
hardware,3cpbgm,iMADEthis2post,3,Fri Jul 10 03:12:58 2015 UTC,Expecially when this is about when those specific people should be upgrading.
hardware,3cpbgm,spellstrike,1 point,Fri Jul 10 03:39:28 2015 UTC,"Yeah, I'm upgrading myself soon, If I could save on the CPU that would allow me to put money elsewhere. I would like something shiny and 1337 but if it isn't needed in gaming I have nothing else that would justify the need for a high end wallet raper."
hardware,3cpbgm,iMADEthis2post,7,Fri Jul 10 04:16:38 2015 UTC,Na an i7 920 would have been better.
hardware,3cpbgm,Vinnycabrini,6,Thu Jul 9 20:01:03 2015 UTC,...why not both?
hardware,3cpbgm,III-V,3,Thu Jul 9 20:40:00 2015 UTC,historically the q6600 makes more sense but the i7-920 is also at the hamstring of many.
hardware,3cpbgm,spellstrike,3,Thu Jul 9 22:19:01 2015 UTC,I'm biased as I have a 920 :) and it seems people are still having good results on the 1366 platform specially with Xeon CPUs.
hardware,3cpbgm,Vinnycabrini,3,Thu Jul 9 22:26:37 2015 UTC,i7 930 @ 3.9GHz here.  LGA 1366 for life!  (I also enjoy confusing younger builders by mentioning my 6x2GB RAM)
hardware,3cpbgm,christes,1 point,Fri Jul 10 01:34:35 2015 UTC,I'm still running my i7 950 at 4.2 from 2009. I did have send the msi x58 pro-e for an issue and they just sent me a brand new motherboard back in 2011. I really don't have a need to upgrade but skylake looks tempting with the new drive and usb support.
hardware,3cpbgm,LiberDeOpp,34,Thu Jul 9 22:14:05 2015 UTC,"All of these games are GPU intensive, none of them CPU intensive. Try running an MMORPG on anything but a high end i5 or better and watch it suffer to keep up in large player crowds."
hardware,3cpbgm,skilliard4,19,Thu Jul 9 20:43:51 2015 UTC,"Seriously, proper realistic benchmarks of ARMA 3 would actually be interesting to see."
hardware,3cpbgm,I-never-joke,13,Thu Jul 9 21:29:20 2015 UTC,"Eh, Civilization can be pretty CPU heavy, but only in when it's the AI's turn."
hardware,3cpbgm,AlchemicalDuckk,5,Thu Jul 9 22:47:32 2015 UTC,"Common misconception. All MMOs atm are built on shaky foundations (for today) and are limited by the API. It doesn't matter what CPU you use or how much you OC it, the software is too limiting. To ask more of hardware is to basically demand physical impossibilities."
hardware,3cpbgm,Trollatopoulous,2,Fri Jul 10 04:59:03 2015 UTC,It doesn't matter what CPU you use or how much you OC it.   Can you elaborate on this? I.e. do we have detailed information from an overclocked Haswell/DC user?
hardware,3cpbgm,terp02andrew,0,Fri Jul 10 13:16:27 2015 UTC,"You can usually find this stuff out by visiting the various MMO technical forums, most data is available for WoW, for obvious reasons. Alternatively you can test it out yourself. And no, I don't have any links saved."
hardware,3cpbgm,Trollatopoulous,4,Fri Jul 10 13:18:58 2015 UTC,"And find yourself chastised for recommending a modern i7 for online multiplayer titles + streaming. 3 years ago the i7 was ""unnecessary"" and ""overkill"" and ""pointless"" for gaming of any kind. Now hyperthreaded quad cores or better are pretty much the default processors for enthusiasts looking to push the limits of their rigs. You'll still get many downvotes around here from folks who think i5 is king in every department just because it's a price to performance beast. Makes no sense. Rant. Over."
hardware,3cpbgm,WillWorkForLTC,3,Fri Jul 10 04:53:24 2015 UTC,"That's because the old i5s tended to have comparable or better single-core performance and headroom for overclocking compared to contemporary and even new quad-core processors, combined with the fact that most modern games are optimized by lazy idiots and barely use two cores (1 for sound, 1 for literally everything else)."
hardware,3cpbgm,Kaghuros,1 point,Fri Jul 10 09:20:03 2015 UTC,"I know. Believe me the i5 K variants are great even today, but multi thread heavy multiplayer and single player titles were out years ago and it feel like nobody saw future multi-core optimisations coming, even though it's been obvious the whole time&--DX 12 will solidify this trend."
hardware,3cpbgm,WillWorkForLTC,4,Fri Jul 10 12:23:33 2015 UTC,"it feela like nobody saw future multi-core optimisations coming, even though it's been obvious the whole time.   errr... I think developers knows about multi core optimizations and they probably hated it.  Threads are not free and multi threaded algorithms tend to be inherently slower than single threaded algorithms   DX 12 will solidify this trend.   all dx 12 will do is solve a pressing issue that rendering should had been multi threaded in the first place since rendering is inherently embarrassing parallel.  Lots of game logic will continue to be single threaded."
hardware,3cpbgm,idoithere,5,Fri Jul 10 16:40:14 2015 UTC,"Well, ""inherently"" when you look at CPU time, but in wall time multi-threading usually comes in better.  To concur with your last, the problem is that the game loop is an inherently single-threaded conception.  You need to get input, update, draw, and repeat in that order.  And the problem with making it multi-threaded is that video frames are inherently discrete (displaying a partially-updated frame is wrong).  Therefore attempting to multi-thread these tasks implies a need for synchronization (signal all threads) or threadfencing (when you hit this point, stop and wait), which are both expensive.  One point of disagreement - rendering has always been multi-threaded, that's why you can use a 4096-core GPU to draw your image.  The problem is that the APIs aren't - for example, see OpenGL, where only one thread is allowed to touch OpenGL state at a time.  There are ways around this but they aren't pretty - if you offload preprocessing to the CPU you can thread that and keep a thread that only handles the OpenGL calls (but ideally you want to do as much of that as possible in the GPU), or you can decompose your display into multiple game-states/OpenGL contexts (like maybe skybox / environment / playermodel I guess?).  But those aren't easy nor all that effective really.  What DX12 will do is allow multiple threads to submit rendering calls into a buffer that the application manages, and then one of the threads pushes the big red button and submits the buffer to the API.  So you can have many state updates working in parallel at once."
hardware,3cpbgm,capn_hector,1 point,Fri Jul 10 17:43:37 2015 UTC,"To concur with your last, the problem is that the game loop is an inherently single-threaded conception. You need to get input, update, draw, and repeat in that order. And the problem with making it multi-threaded is that video frames are inherently discrete (displaying a partially-updated frame is wrong). Therefore attempting to multi-thread these tasks implies a need for synchronization (signal all threads) or threadfencing (when you hit this point, stop and wait), which are both expensive.   yep, this point is the main reason why I pessimistic about multi threading. Synchronization and data dependencies are the enemy of multi threading code. I am not exactly thrilled at the fact that multi threading code need overhead to make it multi threaded which why I said inherently slower. Threads and locks are not free.  However, good news is that data driven oriented programming might have a chance at making code faster.   One point of disagreement - rendering has always been multi-threaded, that's why you can use a 4096-core GPU to draw your image. The problem is that the APIs aren't - for example, see OpenGL, where only one thread is allowed to touch OpenGL state at a time. There are ways around this but they aren't pretty - if you offload preprocessing to the CPU you can thread that and keep a thread that only handles the OpenGL calls (but ideally you want to do as much of that as possible in the GPU), or you can decompose your display into multiple game-states/OpenGL contexts (like maybe skybox / environment / playermodel I guess?). But those aren't easy nor all that effective really.   I know its not. I said it should be mult-threaded. dx 12 is meant to fix that problem. I am just countering the parent point that the fact api calls are multi threaded that it will cascade down to other areas. I said no because plenty of software is not inherently multi threaded.  edit: mixed up data driven programming vs data oriented design. doh i am stupid"
hardware,3cpbgm,idoithere,0,Fri Jul 10 19:54:07 2015 UTC,Way over my head but I remember maybe 20% of this from reading about it. Thank you for the defense-- kind of defence. I'm not quite sure what it was but thanks.
hardware,3cpbgm,WillWorkForLTC,2,Fri Jul 10 17:59:50 2015 UTC,that post is hardly a defense. I think he misunderstood my position.
hardware,3cpbgm,idoithere,0,Fri Jul 10 19:55:45 2015 UTC,"A defence of these points: multithreaded optimisation scaling well and being future friendly, DX12 being more evidence of the former.  There's nothing wrong with your i5, but seriously wake up and smell the API buddy."
hardware,3cpbgm,WillWorkForLTC,2,Fri Jul 10 20:37:19 2015 UTC,"A defence of these points: multithreaded optimisation scaling well and being future friendly, DX12 being more evidence of the former.   I am saying dx12 multithreaded optimization are pretty natural.   The fact is that other multi threaded optimization that other areas will never come. However, I believe there will be hsa physic engines hopefully.  I think you are misunderstanding. The future is not multi threading. It will probably never be.   There's nothing wrong with your i5, but seriously wake up and smell the API buddy.    at this point, i realize that you are practically circlejerking.......  I am practically losing motivation to say anything about the future. whatever, i give up. The future will happen whether you will like it or not. I am just present things that will most likely happen."
hardware,3cpbgm,idoithere,1 point,Fri Jul 10 20:42:50 2015 UTC,"Yeah, I didn't listen to those guys. That's why I just went with what I wanted."
hardware,3cpbgm,Colorfag,1 point,Fri Jul 10 20:42:10 2015 UTC,"also i would like to see other benmark like madvr, flash, html5, then misc usage for e.g. streaming or when you are playing a gamer in the background while open up youtube tutorial video..."
hardware,3cpbgm,sterob,1 point,Fri Jul 10 12:04:13 2015 UTC,Bf4 on a closed online server would be better.
hardware,3cpbgm,veyron3003,3,Sat Jul 11 04:53:37 2015 UTC,My takeaway is that the higher you go with the GPU load (eg. resolution and quality) the more irrelevant the CPU becomes. It makes sense anyway but nice to see in a comparision like this. Would be interesting to see what kind of older CPU you can get away with on 4K which would be able to match a 4790K with the same GPU.
hardware,3cpbgm,barthw,0,Fri Jul 10 07:47:28 2015 UTC,"In theory you can go higher resolutions to eliminate the CPU bottleneck, but you will probably be microstuttering to Timbuktu and back even if you managed to maintain the same average frame rates."
hardware,3cpbgm,YennoX,3,Fri Jul 10 11:54:22 2015 UTC,These make no sense.  There is nothing in Bioshock Infinite that would run at 14 fps for any moment of time on an i5.
hardware,3cpbgm,imallin,5,Fri Jul 10 14:59:46 2015 UTC,ITT:    An i5 matches an i7: This is why you don't need an i7 for gaming! An i5 loses to an i7 by a significant margin: It's the game's fault for being terribly optimised!
hardware,3cpbgm,YennoX,4,Fri Jul 10 06:46:15 2015 UTC,"The latter is actually the reason why i5s are better for gaming (at least in my opinion). Practically no games I play utilize the cores of my i7 to their full potential, and I feel as though I'm getting a lot less performance than I paid for. It's good for a host of other non-gaming reasons, but a comparable i5 is often better if all you care about is stable performance. Most games barely use the second core at all, let alone the rest. Parallel programming is so rarely used where it ought to be."
hardware,3cpbgm,Kaghuros,2,Fri Jul 10 09:27:28 2015 UTC,do GPUs like the 290x or a GTX 980 really have drops to ~15 FPS on FHD resolution (coupled with an i5 4440)?  and CIV used as a benchmark?
hardware,3cpbgm,Klorel,1 point,Thu Jul 9 21:31:10 2015 UTC,Civ V is terribly optimised.
hardware,3cpbgm,jji7skyline,1 point,Fri Jul 10 05:14:06 2015 UTC,"They might be stutters, it looks like they're using the lowest recorded FPS which isn't very good for determining the actual minimums. Most useful would be 99th percentile which would be minimum excluding the occasional drop or stutter."
hardware,3cpbgm,AndreyATGB,2,Fri Jul 10 11:08:18 2015 UTC,"What's up with the i3 4130 having the best ""low score""?"
hardware,3cpbgm,Techdestro,12,Thu Jul 9 22:12:09 2015 UTC,Terrible testing methodology is what is up.
hardware,3cpbgm,dontnation,2,Thu Jul 9 23:14:00 2015 UTC,Arma 3 would have been nice to use. It can bring my 5ghz 4790k to it's knees.
hardware,3cpbgm,BlayneTX,2,Fri Jul 10 07:25:12 2015 UTC,That game really needs a dedicated physx card to run smoothly.  I notice a significant change in fps(about 20-30fps) when I offload the physx stuff to my k4200 from my 780ti on a scenario with about 180 ai.
hardware,3cpbgm,MandaloreZA,1 point,Fri Jul 10 08:34:35 2015 UTC,PhysX is handled by the CPU in Arma 3 though.
hardware,3cpbgm,BlayneTX,1 point,Fri Jul 10 08:58:12 2015 UTC,You can force it through the nvidia control panel.
hardware,3cpbgm,MandaloreZA,2,Fri Jul 10 23:58:28 2015 UTC,"Arma III only supports basic CPU Physx, there are no GPU accelerated Physx available."
hardware,3cpbgm,BlayneTX,1 point,Sat Jul 11 00:11:35 2015 UTC,Its because its ran one one core.
hardware,3cpbgm,veyron3003,2,Sat Jul 11 04:56:59 2015 UTC,Should have used Cities: Skyline. It actually uses all of my 8 threads.
hardware,3cpbgm,TheBloodEagleX,1 point,Fri Jul 10 09:59:05 2015 UTC,"The problem is that even when games use all the cores available, it generally won't improve how they perform vs a 4 core i5."
hardware,3cpbgm,LeMAD,1 point,Fri Jul 10 13:30:01 2015 UTC,"Dunno, I think C:S benefits with the hyper threading.  Unity 5 (the engine it was built on) actually does a pretty good job at allowing devs to properly utilize more threads, more efficiently. Many games are being made with Unity 5 also."
hardware,3cpbgm,TheBloodEagleX,1 point,Sat Jul 11 05:21:13 2015 UTC,There's tasks and games where an i3 with HT trades blows with an i5 at similar clocks.   HT can definitely make a difference anyone saying otherwise is basing their opinion on games that don't make effective use of more than 4 threads.
hardware,3cpbgm,TeutorixAleria,10,Sun Jul 12 12:40:49 2015 UTC,Here's some interesting takes from this.  860K + 290x(Mantle) beats a i7 4790K + 290x in 2 Mantle titles and not far behind in the 3rd. 75$ vs 339$.  I3 4130 beats or is close to the best AMD CPUs across most GPU/game setups. Sniper Elite 3 being the exception where the reverse is mostly true and most AMD CPUs come out on top.  I hope they come back to this benchmark once DX12 has arrived. Some older games might get ported and will give a good insight into the early potential as well as the AMD/Nvidia driver war. Would also like to see something a little more demanding that makes good use of multi-threading like The Witcher 3.
hardware,3cpbgm,Callum027,16,Thu Jul 9 18:37:34 2015 UTC,"Civilisation: Beyond Earth   860K + 290X - 58.3 fps 860K + 290X (Mantle) - 99.1 fps (+40.8 fps) i7-4790K + 290X - 93.2 fps i7-4790K + 290X (Mantle) - 130.1 fps (+36.9 fps)   Not sure what you're trying to saying about the 860K vs the i7-4790K. If you're trying to say you can pay a fraction of the price for the same performance, it's simply not true.  This just shows the untapped performance potential of modern APIs.  EDIT: grammar"
hardware,3cpbgm,Trollatopoulous,-6,Fri Jul 10 02:55:48 2015 UTC,"The i7 is clearly better, no doubt, but the question is, in terms of effective performance (i.e. stable 60fps is enough for 60hz) is it worth the price? The answer is in many cases: No."
hardware,3cpbgm,Randomoneh,3,Fri Jul 10 05:01:44 2015 UTC,Min. doesn't mean anything. What percentage of time has it spent there?
hardware,3cpbgm,Klorel,7,Thu Jul 9 20:20:50 2015 UTC,"isn't min the most important factor for gaming? high min --> no perceived stuttering.  i get your point, if it just occurs once during the test it may not be very representetive. but it's still an important number"
hardware,3cpbgm,Randomoneh,9,Thu Jul 9 21:36:51 2015 UTC,"If it occurs just once, it might be the case that something else went wrong. If it keeps repeating, it's about GPU & CPU. And this is what we want to know."
hardware,3cpbgm,aziridine86,5,Thu Jul 9 21:48:11 2015 UTC,"Something like ""percentage of frametimes over 50 msec"" or ""Bottom 1% average frametime"" might be more useful."
hardware,3cpbgm,anon1821,2,Thu Jul 9 23:50:39 2015 UTC,This article is why i subscribe to /r/hardware
hardware,3cpbgm,useful_idiot,1 point,Thu Jul 9 23:07:44 2015 UTC,And it makes a great space heater
hardware,3cpbgm,NarcolepticRage,1 point,Fri Jul 10 02:20:44 2015 UTC,I was looking for just this.  Now I have validation that my G1820 paired with a 380 is close to optimal.  No need to spend another $100-150 on an i3 for my htpc/gaming pc.
hardware,3cpbgm,Cravem,1 point,Fri Jul 10 02:24:49 2015 UTC,"My phenom 2 965 strugges extremely in most games, but I'll upgrade  in 1 year most likely and will have to wait until then.   Really hyped for mantle,dx12,vulkan though.Will make it easier to wait. In games like Thief I get 100% better performance with mantle (30~fps dx11, 65~fps mantle, HD7850) I guess the next cpu I get will be sufficient for about 10 years unless I do sth else than programming and gaming."
hardware,3cpbgm,gixxersixxer04,2,Thu Jul 9 18:57:55 2015 UTC,Are you overclocked?
hardware,3cpbgm,Cravem,1 point,Fri Jul 10 01:03:58 2015 UTC,"no, running on stock 3,4GHz since almost 6 years. My cooler is not the best one,nor has my case good airflow. Also I dont want to reduce its lifespan. I will gift my current pc to someone next year, so I would prefer that he/she can continue using it for several years without having problems."
hardware,3cpbgm,Cravem,1 point,Fri Jul 10 07:34:40 2015 UTC,My cpu is already 6 years old and was back then when I bought it already middle of the pack. The next CPU I get will most likely be top notch I7 or ZEN and with APIs as dx12 and vulcan it'll last me longer than my old cpu.
hardware,3cpbgm,terp02andrew,1 point,Fri Jul 10 12:12:35 2015 UTC,"Raw CPU performance has certainly slowed. Performance per watt - it's improved steadily at stock, less so once overclocked haha :p  A64, C2D, Bloomfield were all disruptive in their own ways; LF/SB/IB/Haswell/DC steadily iterated on performance...but I can't say it's been all that interesting. Just tick-tock progress, and Microcenter pricing certainly facilitated it.  I don't mention AMD cuz it's just been an uphill climb, particularly when an overclocked Lynnfield is still competitive to that entire lineup :/   If the Zen can at least approach IvyBridge IPC, then it's a good starting point. It will also need requisite headroom to be compelling for me to buy into."
hardware,3cpbgm,MilkyTones,-13,Fri Jul 10 12:31:39 2015 UTC,"One CPU is enough, thank you."
hardware,3cpyc5,willemdoom,9,Thu Jul 9 20:57:32 2015 UTC,"This card doesn't have a vapor chamber and doesn't have a lower tdp than the 290x. In short, another poorly researched and thought out review from hardwarecanucks."
hardware,3cpyc5,Exist50,6,Thu Jul 9 21:40:35 2015 UTC,"Maybe they will release a vapor chamber variant, but yeah I don't watch hardwarecanucks for their quality reviews."
hardware,3cpyc5,Exist50,4,Thu Jul 9 21:47:50 2015 UTC,They have the vapor-x naming for a reason. I expect to see one eventually.
hardware,3cpyc5,bobkitty,2,Thu Jul 9 22:40:55 2015 UTC,Can you explain what a vapor chamber is and does?
hardware,3cpyc5,Exist50,5,Thu Jul 9 22:45:11 2015 UTC,"https://www.youtube.com/watch?v=M4POFT3Ctek  This gives a good overview. The short of it is that it's basically a large, flat heat pipe."
hardware,3cpyc5,dumkopf604,1 point,Thu Jul 9 22:55:36 2015 UTC,"Dimitri is usually on the ball, not this guy, though."
hardware,3cpyc5,Exist50,9,Fri Jul 10 14:40:42 2015 UTC,It's just a very simple thing to research. That hardwarecanucks apparently didn't put the effort in to do so says a lot about the quality of their reviews.
hardware,3cpyc5,LiberDeOpp,1 point,Thu Jul 9 22:25:20 2015 UTC,OC card vs a stock 980. Why bother comparing the two? Is anyone buying reference 980s that aren't liquid cooling?
hardware,3cp88b,speckz,2,Thu Jul 9 17:45:47 2015 UTC,Makes me want to get one to run a BBS on. Ahh the good old days. :)
hardware,3cp88b,UnaClocker,1 point,Thu Jul 9 19:03:48 2015 UTC,Review of the early release hardware is available here >> Indie Retro News
hardware,3cp88b,WanderingAnachronism,1 point,Thu Jul 9 19:54:37 2015 UTC,"I fucking LOVED my Amiga 500.  I did not have the Harddrive addon unit or the updated chipset that my friend had so I could only run kickstart 1.3, and had to load that shit from the disk but I could custom design versions of the OS for specific purposes.  That said I was always pretty jealous of kickstart 2.04 and my friends ability to use higher density floppy disks with his Amiga 500."
hardware,3cp88b,zushiba,1 point,Fri Jul 10 04:16:45 2015 UTC,"A new way to commit piracy :D. Let's all do this. And let's all build hackintoshes, which is against the Mac OSX TOS.  But link to a banned subreddit? No that's against the Microsoft TOS, we can't have that."
hardware,3costq,kraakf,16,Thu Jul 9 15:55:25 2015 UTC,"Let me play devil's advocate here...  The big.LITTLE configuration sounds great in theory for battery life. And anyone that knows about this configuration knows it's really just about battery life, not performance. But what about the iPhone? iPhones use dual cores, and yet they achieve on par performance and battery life to most flagship Android phones."
hardware,3costq,Stingray88,9,Thu Jul 9 18:38:21 2015 UTC,"https://en.wikipedia.org/wiki/Poqet_PC  This was a single core computer, which through the use of very aggressive power management, which included idling the processor between keystrokes, the two AA batteries could last for several weeks in normal use."
hardware,3costq,conradsymes,3,Thu Jul 9 19:28:34 2015 UTC,"Poqet PC:       The Poqet PC is a very small, portable IBM PC compatible computer, introduced in 1989 by Poqet Computer Corporation with a price of $2000. The computer was discontinued after Fujitsu Ltd. bought Poqet Computer Corp. It was the first subnotebook form factor IBM-PC compatible computer that ran MS-DOS. The Poqet PC is powered by two AA-size batteries. Through the use of aggressive power management, which includes stopping the CPU between keystrokes, the batteries are able to power the computer for anywhere between a couple of weeks and a couple of months, depending on usage. The computer also uses an ""instant on"" feature, such that after powering it down, it can be used again immediately without having to go through a full booting sequence. The Poqet PC is comparable to the HP 95LX/HP 100LX/HP 200LX and the Atari Portfolio handheld computers.     Relevant: Palmtop PC | Handheld PC | Pocket computer | Pocket PC   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3costq,autowikibot,1 point,Thu Jul 9 19:30:10 2015 UTC,"which included idling the processor between keystrokes,    I'm pretty sure every modern computer does this. That's what interrupts are."
hardware,3costq,thoomfish,2,Thu Jul 9 20:12:46 2015 UTC,"I don't think interrupts are intended for power efficiency. As far as I know, they're used by hardware and software to basically ""interrupt"" what the processor is doing, and request a higher priority task to be completed."
hardware,3costq,skilliard4,5,Thu Jul 9 21:05:33 2015 UTC,I thought the idea was that the processor can idle and wait for an interrupt rather than repeatedly polling the keyboard/mouse for input.
hardware,3costq,thoomfish,4,Thu Jul 9 21:07:25 2015 UTC,"They are, but the original idea wasn't power efficiency. Moreso being able to go off and do other things while you wait."
hardware,3costq,CJKay93,1 point,Thu Jul 9 23:36:57 2015 UTC,Not as many interrupts I think. A complicating factor is that often instruction pipelines are waiting to fetch memory.
hardware,3costq,conradsymes,4,Thu Jul 9 20:49:03 2015 UTC,"Also Qualcomm's custom Krait cores provided on par performance and battery life with Samsung's big.LITTLE Exynos chips (and Apple's A_ chips)  This study is a little pointless with looking at battery life and not including non big.LITTLE setups, should have included an iPhone 6 and something with a Snapdragon 805  Also are Qualcomm are going back to custom cores for their 820, rumors are they aren't going to be using big.LITTLE  Samsung are also designing their own custom cores"
hardware,3costq,Vince789,2,Fri Jul 10 03:52:58 2015 UTC,"Apple makes very big cores compared to the other ARM manufacturers. They can only do this with the massive R&D they have available and the willingness to pay the fabs well for first dibs on the best tech. big.LITTLE is a cheap way of doing things, in part because it can use unmodified designs straight from ARM, but that's why companies like it. Only a relatively few companies (Samsung, Qualcomm, AMD, Apple, Nvidia, and a few others) have the license and R&D allowing them to make custom ARM cores. Samsung has the additional bonus of its own fabs.  I can't think of another company that makes a ""big ARM"" processor, except maybe AMD's K12 plans and possibly a few of the niche server chips."
hardware,3costq,Exist50,7,Thu Jul 9 18:52:22 2015 UTC,"IMO it's because Apple has better software than everyone else - because of their platform control, they can do things like coalescing wake timers and running native code for almost everything.  Their hardware is really top notch too but there's only so much you can do if your software is telling you to ramp up like 100 times an hour."
hardware,3costq,andromeduck,3,Thu Jul 9 19:13:51 2015 UTC,"Yeah, Android and Windows are getting better in that aspect, but they'll never be able to fine tune just as closely as Apple does. The Metal API is an example of this."
hardware,3costq,Exist50,1 point,Thu Jul 9 21:16:18 2015 UTC,"Nvidia makes a ""big ARM"", its Denver core.   Issue with Denver is that it only has in-order execution and several other flaws."
hardware,3costq,random_guy12,1 point,Fri Jul 10 21:58:35 2015 UTC,"Yeah, I remember that, but considering it's no longer in current products, I didn't really think it should be mentioned."
hardware,3costq,Exist50,0,Fri Jul 10 22:33:50 2015 UTC,>iPhone >on par performance and battery to flagship Androids >mfw
hardware,3costq,stereosteam,0,Sat Jul 11 13:20:56 2015 UTC,You missed a very important word.   on par performance and battery life to most flagship Android phones.   It's not as good as every android phone. But it absolutely is as good as most of them. It's in the middle of the pack. Be realistic and give it the credit it deserves.
hardware,3costq,Stingray88,5,Sat Jul 11 16:51:31 2015 UTC,"What is the point of his paper? 1000 words that can be summed up with '4 A57 and 4 A53 cores is worse than 8 A53 cores', which I'm pretty sure everyone remotely familiar with smartphone SoCs already knew."
hardware,3costq,Reporting4Booty,2,Thu Jul 9 18:49:46 2015 UTC,Only if you're dumb. The whole point of the A57 is for the burst of single thread. These socs just jump to the A57 cluster too soon to have higher benchmark scores.
hardware,3costq,dylan522p,1 point,Fri Jul 10 05:28:00 2015 UTC,"Also I reckon they should have included some other non big.LITTLE setups, e.g. iPhone 6+ (A8, dual Cyclone cores) and a Note 4/Nexus 6 (805, quad Krait 450 cores)"
hardware,3costq,Vince789,2,Fri Jul 10 03:47:20 2015 UTC,Interesting results. I would like to see a benchmark that runs for a long period of time in order to see the effect of disabling cores on thermal throttling.
hardware,3costq,BigJewFingers,1 point,Thu Jul 9 18:42:03 2015 UTC,"Answer: no, they do not."
hardware,3costq,Noobasdfjkl,1 point,Thu Jul 9 18:32:52 2015 UTC,Am I the only one who can't see the paper? I can only see the table of contents...
hardware,3costq,AlphaGavin,3,Thu Jul 9 17:28:59 2015 UTC,http://www.moorinsightsstrategy.com/wp-content/uploads/2015/07/Do-8-Cores-Really-Matter-in-Smartphones-Final-By-Moor-Insights-And-Strategy.pdf
hardware,3costq,Exist50,1 point,Thu Jul 9 17:42:48 2015 UTC,Oh I couldn't see it as I was on mobile... Thanks!
hardware,3costq,AlphaGavin,2,Fri Jul 10 04:21:45 2015 UTC,There's a link to the paper on the site.
hardware,3cqld9,Deathspawner,4,Thu Jul 9 23:57:23 2015 UTC,Average FPS graph failure.   You need Min-Average-Max and Frame-time.   The overall conclusion is easily reachable from other more in-depth reviews but the presentation here is lacking.
hardware,3cqld9,Seclorum,3,Fri Jul 10 00:43:18 2015 UTC,"Those charts are not nearly as pleasant to read as bar graphs, but god damn the gap in performance is enormous."
hardware,3cqld9,rightovahere,3,Fri Jul 10 00:57:06 2015 UTC,This basically shows why fury needs to be launched at 600. The Fury x can't compete against the non-reference 980ti. I feel bad for 3rd parties trying to make money on fury x cards and having to take back cards for pump noise. Amd you had plenty of time to fix this....sigh.
hardware,3cqld9,LiberDeOpp,2,Fri Jul 10 03:51:58 2015 UTC,"Wow, fantastic review! I'm so glad to finally see someone doing benchmarks of more than just the same damn 5-6 games all the time. Especially surprised to see TSW there, but very happy it is. Kudos!"
hardware,3cqld9,Trollatopoulous,1 point,Fri Jul 10 04:55:36 2015 UTC,this is a nice in depth examination of the products in several practical situations. This also shows that the Fury X falls well short of the 980 Ti
hardware,3cqld9,dfGobBluth,-5,Fri Jul 10 00:09:42 2015 UTC,"I don't consider it valid. It's not using Catalyst 15.7, and a lot of people are reporting big performance increases with 15.7. In addition, it's WHQL, so there's no excuse for somebody to not use it."
hardware,3cqld9,wagon153,10,Fri Jul 10 02:06:37 2015 UTC,15.7 was released a day after this dude posted his article. Let's cut him a little slack.
hardware,3cqld9,Cozmo85,4,Fri Jul 10 02:17:13 2015 UTC,Did amd announce a week beforehand that 15.7 was coming out today? Should nobody release benchmarks in anticipation of amds next next driver update?
hardware,3cqld9,LiberDeOpp,5,Fri Jul 10 02:33:20 2015 UTC,15.7 was mainly for older cards and for win10 support. I didn't see anything where fury x gained from 15.7.
hardware,3cl2gx,Rylock,55,Wed Jul 8 19:12:31 2015 UTC,New Frame Rate Target Control feature Freesync working on Crossfire VSR enabled on 200 series and above Lots of new Crossfire profiles Single GPU improvements for Far Cry 4 and Tomb Raider Windows 10 and WDDM 2.0 support
hardware,3cl2gx,logged_n_2_say,22,Wed Jul 8 19:18:13 2015 UTC,VSR enabled on 200 series and above   it looks like 7000 series is somewhat supported as well.
hardware,3cl2gx,wagon153,22,Wed Jul 8 19:57:50 2015 UTC,Everything all the way down to the HD 7790 is supported according to AMD.   http://support.amd.com/en-us/kb-articles/Pages/AMDCatalyst15-7WINReleaseNotes.aspx
hardware,3cl2gx,logged_n_2_say,10,Wed Jul 8 20:14:23 2015 UTC,"yup. looks like 3840 X 2160 is only on gcn 1.2, or the 285 and fury's."
hardware,3cl2gx,wagon153,12,Wed Jul 8 20:25:55 2015 UTC,I think that's a hardware limitation though.
hardware,3cl2gx,TheImmortalLS,5,Wed Jul 8 20:27:18 2015 UTC,"Yes, scaler limitation. Amd put their vsr on the display scaler, for more performance with hardware limitations. Nvidia shader-like approach, more resource intensive, no theoretical limit."
hardware,3cl2gx,logged_n_2_say,1 point,Thu Jul 9 00:54:12 2015 UTC,definitely.
hardware,3cl2gx,klove861,1 point,Wed Jul 8 20:28:52 2015 UTC,"I have a 7850 and I'm able to set my resolution to 2160p, and I haven't even installed the new drivers yet."
hardware,3cl2gx,logged_n_2_say,2,Thu Jul 9 16:35:17 2015 UTC,are you on a UHD monitor?
hardware,3cl2gx,klove861,1 point,Thu Jul 9 16:40:02 2015 UTC,"Yeah, I got it in anticipation of getting a Fury X. I was pretty surprised when I saw 2160p was available."
hardware,3cl2gx,logged_n_2_say,3,Thu Jul 9 16:42:02 2015 UTC,"gotcha, 7850 can still display UHD @ 60 hz with display port.  vsr is like downsampling where you render at higher resoltion and downscale to smooth out things.    so for instance, if you had a fury x and a 1080p, you would use vsr and set it to 2160 but your monitor is still 1080p.  your monitor is already at the awesome 2160p."
hardware,3cl2gx,Voxwork,5,Thu Jul 9 16:51:32 2015 UTC,"HD 7790   Ah dammit, as someone with a 7970 this made me a bit sad. He is getting old."
hardware,3cl2gx,wagon153,12,Wed Jul 8 20:41:33 2015 UTC,Why? Your card is practically a 280x.
hardware,3cl2gx,Voxwork,11,Wed Jul 8 20:42:57 2015 UTC,"Right, I'm sorry, apparently I can't read. Thanks for the correction, I'm leaving my original comment because I'm an idiot."
hardware,3cl2gx,xxfay6,3,Wed Jul 8 20:46:22 2015 UTC,"The 7790 is a newer architecture than the 7970, so his fears weren't that far off."
hardware,3cl2gx,WillWorkForLTC,2,Thu Jul 9 07:14:13 2015 UTC,LIES LIES LIES (tears falling on my 7870)!
hardware,3cl2gx,logged_n_2_say,2,Thu Jul 9 13:09:52 2015 UTC,be happy! pitcairn is the only gcn1.0 that made it to the 300 series.
hardware,3cl2gx,WillWorkForLTC,2,Thu Jul 9 16:43:36 2015 UTC,You and your inauthentic optimism. How am I supposed to 4k with this POS? The Canadian dollar is abysmal and a 980ti costs around $870 monopoly bucks us 14% tax. :-|
hardware,3cl2gx,logged_n_2_say,2,Fri Jul 10 20:41:28 2015 UTC,"luckily you requested all payment in LTC awhile ago, and are now sitting on the moon considering that last few weeks.  how's the view from up there?"
hardware,3cl2gx,xxfay6,1 point,Fri Jul 10 21:03:31 2015 UTC,Even though half of us didn't get VSR working apparently.
hardware,3cl2gx,n3x_,2,Wed Jul 15 08:00:45 2015 UTC,careful you don't short it
hardware,3cl2gx,WillWorkForLTC,1 point,Thu Jul 9 21:48:26 2015 UTC,It handled a cup of almond milk just fine a year ago so I think we're OK.
hardware,3cl2gx,logged_n_2_say,7,Thu Jul 9 22:10:59 2015 UTC,"i too have a 7970.  afaik, it's getting the same vsr support that a 390x is."
hardware,3cl2gx,Voxwork,5,Wed Jul 8 20:46:33 2015 UTC,"Yeah, I'm an idiot and didn't read it correctly. Thanks for the reply!"
hardware,3cl2gx,GodKingThoth,1 point,Wed Jul 8 20:51:25 2015 UTC,"7870 here, how many christmas' has it been..."
hardware,3cl2gx,bizude,6,Thu Jul 9 15:13:11 2015 UTC,Only for 16:9 & 16:10 users  21:9 monitors are out of luck :'(
hardware,3cl2gx,spikey341,5,Wed Jul 8 20:19:19 2015 UTC,khaaaaaan!
hardware,3cl2gx,Unoid,2,Thu Jul 9 00:28:49 2015 UTC,sherrrrrrlockkk!!!
hardware,3cl2gx,spikey341,1 point,Thu Jul 9 14:27:51 2015 UTC,smauuuuuuuuuug!
hardware,3cl2gx,tuirn,2,Fri Jul 10 15:41:53 2015 UTC,"And for those few of us that are still nursing a long older monitors (my DELL 2001FP's are still working), no 1600x1200 support either."
hardware,3cl2gx,melgibson666,10,Thu Jul 9 00:55:18 2015 UTC,Single GPU improvements for Far Cry 4 and Tomb Raider   Gotta improve the framerate for that 2 year old game.
hardware,3cl2gx,Put_It_All_On_Blck,17,Wed Jul 8 21:58:31 2015 UTC,It is still being used as a benchmark by many sites and the sequel is coming out on the same engine so some optimizations should carry over.
hardware,3cl2gx,monkeybannanna,-7,Wed Jul 8 22:13:39 2015 UTC,"Because many sites use these two games, it can trick users into how real performance would be on different games not specifically optimized as benchmarks"
hardware,3cl2gx,veyron3003,3,Wed Jul 8 23:55:27 2015 UTC,Or maybe that its an AMD title?
hardware,3cl2gx,Perunsan,2,Thu Jul 9 10:49:55 2015 UTC,"I'm really excited for FRTC, the Nvidia version makes path of exile playable (using v-sync makes it drop frames, not using it you get screen-tearing)"
hardware,3cl2gx,screwyou00,3,Wed Jul 8 19:49:22 2015 UTC,Is there a difference between FRTC and say BF4's FPS capping settings or ENB's FPS cap settings?
hardware,3cl2gx,kennai,2,Wed Jul 8 21:09:20 2015 UTC,It reduces power consumption by a proportion of the time it's spent not rendering frames.
hardware,3cl2gx,ThisFrickinSite,4,Wed Jul 8 21:51:36 2015 UTC,and fps cap doesn't do that?
hardware,3cl2gx,TaintedSquirrel,9,Wed Jul 8 22:22:08 2015 UTC,"AMD made it sound like the driver would dynamically adjust clocks/voltage to maintain the targeted framerate value.  I don't know exactly if FRTC is working the way they described it, but yes in theory it should be more power efficient than a regular fps cap."
hardware,3cl2gx,kennai,2,Wed Jul 8 22:48:36 2015 UTC,Not to this extent.
hardware,3cl2gx,svceon,2,Wed Jul 8 22:47:28 2015 UTC,"ideally it would, but this is driver-level capping, meaning less voltage meaning less temps. I get times where in witcher on ultra i get around 55C with 60FPS cap on CCC"
hardware,3cl2gx,CykaLogic,2,Thu Jul 9 08:29:03 2015 UTC,"Same here. To be honest, I'm mostly hoping my office doesn't heat up quite as much during marathon Witcher 3 sessions for the rest of the summer."
hardware,3cl2gx,Perunsan,2,Wed Jul 8 20:07:00 2015 UTC,"Unfortunately AMD cards are horribly optimized for PoE, as even a single patch of desecrated or shocked ground drops my FPS to 40-50(7870). Framerate drops below 20 with >4 players in a party using skills+auras."
hardware,3cl2gx,olavk2,1 point,Thu Jul 9 02:00:44 2015 UTC,Diablo had a similar problem but they had the resources to almost fix it (they will probably address these issues before next expansion ...)  it's not just amd cards it's works bad on their cpus to
hardware,3cl2gx,MrSapath,1 point,Thu Jul 9 02:04:21 2015 UTC,"it's not just amd cards it's works bad on their cpus to   well that is a distinct feature of MMO games, they relly heavily on single threaded performance, which AMD chips just dont have"
hardware,3cl2gx,Perunsan,2,Fri Jul 10 08:35:46 2015 UTC,FRTC is not a type of vsync.
hardware,3cl2gx,TaintedSquirrel,9,Wed Jul 8 20:05:12 2015 UTC,"that's why it's great because it limit the maximum amount of frames, btw gonna test it in about 5 min  with v-sync http://i.imgur.com/A767Wwt.jpg  with FRTC http://i.imgur.com/Y6mvqVO.jpg  not perfect but still a little better  ( this is the worse case scenario where you add a mod with more stuff on the ground)"
hardware,3cl2gx,MrSapath,8,Wed Jul 8 20:08:29 2015 UTC,You've been able to lock your framerate with RivaTuner and maybe a dozen other programs for years now.  FRTC is a bit more power efficient but if you need to limit your fps without Vsync then it's easily possible...
hardware,3cl2gx,spikey341,1 point,Wed Jul 8 22:47:05 2015 UTC,"Okay, I thought you mistook it for a form of vsync. Nevermind then."
hardware,3cl2gx,Perunsan,1 point,Wed Jul 8 20:16:28 2015 UTC,"yeah you'll have a better time with rivatuner or radeonpro. Been testing it for the last 30 min and the strictness of those two far surpasses frtc's, meaning no more frametime jumps :)"
hardware,3cl2gx,iMADEthis2post,1 point,Thu Jul 9 00:30:50 2015 UTC,"i've used radeonpro in the past but i forgot how to set it up with games that have their own launcher , having it in the CCC panel makes it really easy to use"
hardware,3cl2gx,logged_n_2_say,1 point,Thu Jul 9 01:51:31 2015 UTC,"Is FRTC available on the 7950? I'm unsure if it's a hardware or software thing. I'd love this though. I wonder what would happen if it's run with vsync also, would I need to disable that in the games I play or is it fine to leave it as?  EDIT: sorry I see ppl are saying yes to HD7xxx support :)"
hardware,3cl2gx,TaintedSquirrel,14,Thu Jul 9 02:44:35 2015 UTC,"driver here  it looks like we have a unified driver again, and FRTC works on 7000 series (as well as 200) as predicted.  looks like vsr is also on 7000 series, but not at highest resolution.  gotta say this came faster then i was thinking it would.  edit: also, someone needs to finally confirm the tessellation scores on a 290(x) for w3 too."
hardware,3cl2gx,logged_n_2_say,8,Wed Jul 8 19:54:05 2015 UTC,http://hardforum.com/showpost.php?p=1041718450&postcount=12
hardware,3cl2gx,Bonowski,4,Wed Jul 8 22:49:20 2015 UTC,"So they changed it for all tessellation, not just game specific. Interesting. Makes me wonder why they didn't do it awhile ago in the betas."
hardware,3cl2gx,Clearosys,16,Thu Jul 9 00:08:03 2015 UTC,"So I think this killed my QNIX monitor.  I updated the drivers and now I'm getting green lines and my screen and distortion.  I'm seeing this outside of Windows entirely when in POST, so that's awesome.  EDIT:  So I have  QX2710LED and AMD R9 290X.  For whatever reason, I now have green lines running vertically down my screen since I updated.  I can only assume my monitor got fried somehow as this is outside of Windows.  I see the lines during POST and in my BIOS, etc.  I am not very happy at the moment.  If anyone has any ideas, please let me know.  Also, I'm assuming it's a monitor issue, as I am not seeing any problems on my 2nd monitor (Samsung HDTV).  EDIT 2:  I just wanted to say thanks to everyone replying!  That's awesome to have people take the time to help a random person on the internet!  I reconnected every cable.  I tried different ports and even new cables.  I confirmed I was set at 60Hz and not overclocked.  I reverted ALL of my software / driver / config changes back to the settings before the issue triggered.  The problem persisted, however.  My friend has the same monitor and an AMD card.  He mentioned he had a similar issue after a driver update back in December.  His desktop burned into his screen right after the driver update and a few weird monitor flashes.  It was a bit more severe.  Reboots didn't fix it.  It was also visible during POST.  He said he powered everything down for a day, and it went back to normal.  Weird, right?  So I took his advice and did the old fashioned ""power down, unplug power cables, go for a run, and leave it alone"" approach.  I was annoyed and sick of swearing at my monitor.  Anyway, I powered it back up after 2 hours.  It's not perfect, but it is a lot better!  I can barely see the lines now.  I have to get about 6 inches from my screen to notice them, and even then, they're very dim now.  I'm really not sure why this is the case.  I'd love to hear thoughts anyone has, especially if it was my stupid fault.  I'm going to power it back down and leave it off overnight and see how it is in the morning.  I may ever try re-seating my video card.  That has gotten me out of similar situations in the past."
hardware,3cl2gx,TheImmortalLS,12,Wed Jul 8 22:51:09 2015 UTC,"If you have the QNIX overclocked, try setting it to 60hz and see if the problem persists."
hardware,3cl2gx,makar1,10,Thu Jul 9 00:47:15 2015 UTC,Try integrated graphics to see if it is the monitor or gpu?
hardware,3cl2gx,xxfay6,1 point,Thu Jul 9 00:56:58 2015 UTC,"Integrated graphics usually only has single link DVI, which won't drive the Qnix."
hardware,3cl2gx,makar1,1 point,Thu Jul 9 07:06:02 2015 UTC,"My Llano APU has dual-link, I would guess most recent chipsets would support it."
hardware,3cl2gx,TheImmortalLS,1 point,Thu Jul 9 07:17:04 2015 UTC,It's mostly determined by the motherboard rather than the CPU
hardware,3cl2gx,makar1,1 point,Thu Jul 9 08:28:28 2015 UTC,I'm not very experienced. Wouldn't it be native resolution at 30 Hz then?
hardware,3cl2gx,boredinballard,1 point,Thu Jul 9 08:54:20 2015 UTC,It won't work at all with single link.
hardware,3cl2gx,wallgomez,5,Sun Jul 12 09:33:47 2015 UTC,"Make sure the cables are plugged in all the way, or try another  cable."
hardware,3cl2gx,hdshatter,5,Thu Jul 9 00:33:11 2015 UTC,"I've got a qx2710 at 120hz, and as a piece of advice for future reference, always reset your refresh rate to default when you update drivers. On both AMD and nVidia cards."
hardware,3cl2gx,sterob,3,Thu Jul 9 03:52:31 2015 UTC,Check your cables and see if they are bad/loose before assuming its the GPU or display.
hardware,3cl2gx,Bonowski,2,Thu Jul 9 01:27:46 2015 UTC,"can you bold down the  ""I'm assuming it's a monitor issue, as I am not seeing any problems on my 2nd monitor (Samsung HDTV).""  to prevent witch hunt from green fan?"
hardware,3cl2gx,Maindric,1 point,Thu Jul 9 14:29:29 2015 UTC,Good call.  I just updated.
hardware,3cl2gx,Phantom_Absolute,6,Thu Jul 9 15:03:14 2015 UTC,"For those curious about performance differences, I got about a 1.7% increase in Firestrike Extreme score with the driver alone on my Fury X.  link  Nothing else changed but the driver."
hardware,3cl2gx,ExtraCunt,1 point,Thu Jul 9 07:15:53 2015 UTC,Yea but your physics score went down.
hardware,3cl2gx,Maindric,2,Thu Jul 9 13:02:17 2015 UTC,"His CPU's max turbo clock went from 3,397 MHz to 3,597 MHz..."
hardware,3cl2gx,Spacebotzero,1 point,Thu Jul 9 20:11:46 2015 UTC,"For some reason, my CPU randomly went to 1.1GHz during the physics test.  I have fixed it.  The physics is not impacted AT ALL by the GPU or drivers really, so there is not much point in that.  EDIT  In fact, since then I have OC'd the CPU to 4.2GHz stable, so now my physics score should be considerably higher."
hardware,3cl2gx,Exist50,7,Fri Jul 10 21:17:14 2015 UTC,"I updated.  Something weird is happening right now with my monitor.  I use an old Sharp 1080P TV as a monitor.  It has always been locked at a resolution of 1920x1080P @ 60hz. However,  right now I'm running it at 1440...@ 60hz. I don't get how this is happening."
hardware,3cl2gx,Spacebotzero,10,Wed Jul 8 21:21:21 2015 UTC,VSR?
hardware,3cl2gx,Exist50,3,Wed Jul 8 21:24:12 2015 UTC,Oh perhaps!  What exactly does VSR do?
hardware,3cl2gx,Spacebotzero,9,Wed Jul 8 21:30:03 2015 UTC,Basically renders at a higher resolution and downscales to your monitor's.
hardware,3cl2gx,Exist50,3,Wed Jul 8 21:31:07 2015 UTC,Hmm.... Is there much of a benefit to this?  Just the ability to fit more on your screen?
hardware,3cl2gx,Spacebotzero,7,Wed Jul 8 21:32:13 2015 UTC,It's typically only for games. Might be something else if you are seeing this for desktop use.
hardware,3cl2gx,wagon153,3,Wed Jul 8 21:33:55 2015 UTC,Yup.  I see it on desktop and in games.
hardware,3cl2gx,bphase,3,Wed Jul 8 21:38:03 2015 UTC,Yeah just turned on VSR. Am now able to crank it up to 2560x1440 with my 1080p monitor. Everything looks super blurry though. :/
hardware,3cl2gx,Teethpasta,5,Wed Jul 8 21:59:47 2015 UTC,"Yeah it's not for desktop, doesn't handle text so well, I suppose due to it not being 1:x ratio like 4K would be 1:2, which ends up messing it up. I suppose it happens with game text as well."
hardware,3cl2gx,Haxican,1 point,Wed Jul 8 23:17:33 2015 UTC,Yeah you need to use some that scales evenly like 4k
hardware,3cl2gx,bphase,2,Thu Jul 9 01:40:18 2015 UTC,If it works like the NVidia version it basically renders a higher res image then scales it down to fit on your screen which results in a less jaggy image w/o actually using anti-aliasing x2 x4 etc. From what I read it's easier for the GPU to render more pixels than actually doing the anti-aliasing. So the benefit is a better image with the GPU doing less work.
hardware,3cl2gx,forestcollector,8,Wed Jul 8 22:15:01 2015 UTC,"It's not easier than anti-aliasing, it is the same as SSAA or Super Sampling Anti-aliasing, an age-old very intensive technique that has been available at driver level every now and then back in the past. It's slower than MSAA, but produces better quality."
hardware,3cl2gx,CeeeeeJaaaaay,3,Wed Jul 8 23:45:52 2015 UTC,"Downsampling produces a visual blur from scaling the resolution down, so in that respect it's not exactly the same. That doesn't occur with SSAA as the 3D graphics are rendered at a higher resolution independent of the monitor resolution. SSAA is definitely superior in visual quality."
hardware,3cl2gx,Drenmar,6,Thu Jul 9 10:46:31 2015 UTC,"Rendering at a higher resolution is the best form of AA, but it's also the most expensive FPS wise."
hardware,3cl2gx,Anaron,4,Thu Jul 9 00:24:58 2015 UTC,"Finally VSR for my 7970 Ghz Ed, that card still has a lot of power so I'll be downscaling from 1440p to 1080p."
hardware,3cl2gx,Cossil,4,Wed Jul 8 23:11:57 2015 UTC,Yeah. I've been waiting for this feature to run Dota 2 at a higher resolution.
hardware,3cl2gx,Drenmar,1 point,Wed Jul 8 23:51:33 2015 UTC,I am running two 7970's cross fired and I want to know what this could mean for me. I also could not see where to enable it. Could you could help me understand?
hardware,3cl2gx,Cossil,2,Thu Jul 9 06:52:25 2015 UTC,"VSR allows you to play at higher resolutions than your monitor natively is able to display. With a 7970 you can play at 3200x1800 or 2560x1440 and downscale this resolution to your 1920x1080 monitor. This makes your games look MUCH better because higher resolution means more details and less jaggies and flickering, like going from 720p to 1080p. You can enable it under 'Digital Flat-Panel Properties' in your driver options (right click on desktop -> AMD Control Catalyst)"
hardware,3cl2gx,LintGrazOr8,1 point,Thu Jul 9 08:53:04 2015 UTC,"Hmm, I don't really have this option. Perhaps it is because my monitor is hooked up through DVI?"
hardware,3cl2gx,bizude,1 point,Thu Jul 9 12:18:32 2015 UTC,How do you enable VSR?
hardware,3cl2gx,LintGrazOr8,1 point,Fri Jul 10 00:49:15 2015 UTC,AMD Catalyst > My Digital Flat Panels > Properties.   http://imgur.com/PpwAE0F  It's only available for 16:9 & 16:10 resolutions
hardware,3cl2gx,bizude,1 point,Sat Jul 11 21:47:50 2015 UTC,"Oh hey, someone's been going through my comments. Thanks o/"
hardware,3cl2gx,iMADEthis2post,1 point,Sat Jul 11 21:49:26 2015 UTC,Hehe... I do that sometimes. Especially when I'm bored. Like right now... I'm very bored.
hardware,3cl2gx,KadenTau,3,Sat Jul 11 21:54:59 2015 UTC,Any parts of this available on win7?
hardware,3cl2gx,iMADEthis2post,2,Thu Jul 9 03:06:40 2015 UTC,"I was wondering myself. I used auto detect and it downloads a driver but it won't run saying I have the wrong OS...even though the filename has ""win7"" in it."
hardware,3cl2gx,KadenTau,1 point,Thu Jul 9 05:04:53 2015 UTC,"Mine worked okay, have Frame Rate Target Control and VSR working now.  I see no thermal difference with FRTC sadly but as I've also just upped the virtual resolution that could maybe nullify that. Everything seems smoother but I still have to run with vsync or it gets choppy, not sure how that works when running refresh rate, you would think that wouldn't be the case but apparently not.  My aim seems much better in FPS, just been hammering CS:GO.  Are you running 64bit win 7?"
hardware,3cl2gx,iMADEthis2post,1 point,Thu Jul 9 05:37:02 2015 UTC,I am. When I try to download manually and select the steps one by one the goddamned top field for selecting driver type resets when I hit display results. Kinda driving me nuts.
hardware,3cl2gx,KadenTau,1 point,Thu Jul 9 11:33:48 2015 UTC,"Try the auto detect hardware option, if you haven't already. That's what I used."
hardware,3cl2gx,iMADEthis2post,1 point,Thu Jul 9 17:47:56 2015 UTC,"I mentioned that in my original comment. It downloads the right file (at least according to the name) but then won't run for whatever reason. The downloader finishes the download and doesn't do anything else, and if I go find the exe in my temp folder it throws an error saying I have the wrong version of Windows and I need ""a 32-bit or 64-bit operating system"".  I'm like: as opposed to what? ;_;"
hardware,3cl2gx,KadenTau,1 point,Thu Jul 9 18:00:30 2015 UTC,"Sorry, not had a lot of sleep :). Could be something with your windows instal. Have you tried uninstalling all gfx drivers and trying to install on a naked machine?  Failing that I'd say try reinstalling windows, which is an arse."
hardware,3cl2gx,iMADEthis2post,2,Thu Jul 9 21:05:52 2015 UTC,Got it working. I just used the links in the side bar of the site. Herp.
hardware,3cl2gx,chapstickbomber,1 point,Thu Jul 9 22:54:20 2015 UTC,Ha cool :)
hardware,3cl2gx,bphase,8,Thu Jul 9 23:46:24 2015 UTC,"I think I have a few comments about how crossfire freesync support was incoming. feelsgoodman.jpg  I am super excited to see the performance boost that WDDM 2.0 is going to give AMD hardware. Driver overhead has been their Achilles heel for ages now. It would be a pretty big upset if AMD took the performance lead just because of Windows 10, between DX12 and WDDM 2.0  Also looking forward to WDDM 2.0 management of displays, versus the abortion that the current kernel's handling of mixed resolutions, refresh rates, and full screen switching.  Brothers, we may be entering the world of the split second alt-tab."
hardware,3cl2gx,chapstickbomber,7,Wed Jul 8 20:16:07 2015 UTC,"Brothers, we may be entering the world of the split second alt-tab.   Already the case in some games, even outside of windowed mode. But not enough of them. I wasn't aware that Win10 would help there as well, I hope that's the case."
hardware,3cl2gx,Hateless_,4,Thu Jul 9 00:14:45 2015 UTC,Now we just need freesync in windowed mode and a windows option to disable double buffered vsync on the desktop without hacks. Then I'll never use full screen again.
hardware,3cl2gx,chapstickbomber,1 point,Thu Jul 9 00:23:08 2015 UTC,"mixed resolutions, refresh rates, and full screen switching.   Care to elaborate? I've read nothing about this. And i'm intrigued because I use one 1080p 144hz monitor, and a 1680*1050 60hz one."
hardware,3cl2gx,pb7280,3,Thu Jul 9 01:30:44 2015 UTC,"It's quite display dependent, but wonky things happen on my setup at least, with a 4k and any other display attached.   A good one is that some windows will draw at 30hz even on my 144hz panel unless I reduce the size, drag it around for a sec, and then reset the position (max or snap left/right)."
hardware,3cl2gx,chapstickbomber,1 point,Thu Jul 9 02:17:57 2015 UTC,"I've had similar problems to this, I have a 60Hz monitor but vsync currently locks my framerate to 120Hz and I still get tearing. It only started with WDDM 2 drivers (I've had Windows 10 for months), but those were engineering samples provided by MS, these ones may fix the issue now that the OS is officially supported."
hardware,3cl2gx,dreiter,1 point,Thu Jul 9 20:57:06 2015 UTC,I would hope so.  Then again....
hardware,3cl2gx,paddytokey,3,Thu Jul 9 21:40:56 2015 UTC,It mentions VSR being supported on 1440p monitors at 60 Hz.  So what if the monitor is at 72 Hz or 90 Hz?
hardware,3cl2gx,__________0000000000,2,Wed Jul 8 21:27:06 2015 UTC,"Good question, let us know what happened if you have a monitor like that :)"
hardware,3cl2gx,dreiter,2,Thu Jul 9 04:29:03 2015 UTC,I have a U2515H clocked at 80Hz. If I run VSR at 3200x1800 it switches to 60Hz.
hardware,3cl2gx,steel86,1 point,Fri Jul 10 11:57:41 2015 UTC,"That's a bummer, but thanks for the info!"
hardware,3cl2gx,gryphonZA,2,Fri Jul 10 15:29:12 2015 UTC,Cant seem to enable FreeSync on my 7970/280X Crossfire with a BenQ XL2730Z.  Same when I disable my Crossfire and just use my 7970. Anyone else got lucky getting Freesync running on 7xxx series?  Edit: Okay looks like Freesync has not been enabled for lower cards. Only VSR and FRTC
hardware,3cl2gx,iMADEthis2post,7,Thu Jul 9 01:04:35 2015 UTC,Sorry but 280X does not support Freesync
hardware,3cl2gx,qwortz,0,Thu Jul 9 13:12:04 2015 UTC,"Yeah, from what I have been reading regarding freesync it's a hardware thing, 290x 280x don't support it 285 does support it. Hard to be disappointed with this mind given I'm getting FRTC and more toys to play with."
hardware,3cl2gx,iMADEthis2post,7,Thu Jul 9 03:01:15 2015 UTC,290x supports it iirc
hardware,3cl2gx,Kinaestheticsz,0,Thu Jul 9 05:42:11 2015 UTC,"Could be, I'm not 100% on it tbh."
hardware,3cl2gx,Phantom_Absolute,3,Thu Jul 9 05:59:46 2015 UTC,"Hawaii, Tonga, and Fiji are the ones that support FreeSync if I remember right."
hardware,3cl2gx,BlayneTX,1 point,Thu Jul 9 06:25:29 2015 UTC,Yup I'm running FreeSync on my 290
hardware,3cl2gx,Charwinger21,2,Thu Jul 9 13:00:42 2015 UTC,"""NOTE: To ensure stability, users should upgrade to the latest available Windows® 10 Technical Preview build provided by Microsoft before installing AMD Catalyst™ 15.7."""
hardware,3cl2gx,bunchajibbajabba,5,Thu Jul 9 01:56:35 2015 UTC,"""NOTE: To ensure stability, users should upgrade to the latest available Windows® 10 Technical Preview build provided by Microsoft before installing AMD Catalyst™ 15.7.""   If you're on W10.  If you're using W8.1 or W7 or Linux you are just recommended to update to the latest version of those OSes."
hardware,3cl2gx,LevLev,2,Thu Jul 9 05:52:09 2015 UTC,"Tried it on Windows 7, works just fine so far."
hardware,3cl2gx,Maldiavolo,1 point,Thu Jul 9 05:46:23 2015 UTC,"Does anyone know what ""ACP Application"" is on the Custom Install list?"
hardware,3cl2gx,luddist,1 point,Thu Jul 9 11:51:04 2015 UTC,It's for AMD's True Audio.  You will want to install it.
hardware,3cl2gx,sterob,1 point,Thu Jul 9 18:53:19 2015 UTC,"Side by side 290(X) vs 390(X) benchmarks at equal clock rates, anyone?"
hardware,3cl2gx,amnesiacgoldfish,1 point,Thu Jul 9 13:27:28 2015 UTC,can anyone do a VSR screenshoot comparison?
hardware,3cl2gx,TranquilMarmot,1 point,Thu Jul 9 14:33:55 2015 UTC,Seems like this link's broken now.
hardware,3cl2gx,DiHydro,2,Thu Jul 9 20:04:39 2015 UTC,Odd.  New link:  https://community.amd.com/community/amd-corporate/blog/2015/07/08/amd-catalyst8482-157-driver-enabling-a-premium-windows174-10-experience-and-so-much-more
hardware,3cl2gx,imallin,0,Thu Jul 9 20:07:49 2015 UTC,I wonder if this will fix Windows 10 seeing my R9 390 as a 2xx series card and installing the wrong drivers?
hardware,3cl2gx,DiHydro,-1,Thu Jul 9 06:12:15 2015 UTC,Nice! I was just thinking this weekend how it's been about 6 months since we saw a new driver!
hardware,3cl2gx,Charwinger21,15,Thu Jul 9 02:06:50 2015 UTC,They release beta's frequently.  Look to Nvidia's last few WHQL releases to see why the WHQL label doesn't mean jack shit regarding the quality of the driver.
hardware,3ckpul,winterblink,90,Wed Jul 8 17:47:19 2015 UTC,"I just noticed this, so pardon me if it's old, but it looks like they dumped that trampstamp logo for their gaming lines and unified everything under a slightly redesigned sails logo.  Much better. :)"
hardware,3ckpul,LuckyTtam,13,Wed Jul 8 17:47:51 2015 UTC,What will all those tramps do now?
hardware,3ckpul,Undeadhunter,15,Wed Jul 8 18:53:14 2015 UTC,You can do some amazing touch-up work on tattoos these days. :)
hardware,3ckpul,ProfitOfRegret,6,Wed Jul 8 18:57:07 2015 UTC,Wish I waited ... I got a K70 with a tramp stamp :(
hardware,3ckpul,LuckyTtam,4,Thu Jul 9 13:05:33 2015 UTC,There are ways to fix that
hardware,3ckpul,Undeadhunter,1 point,Thu Jul 9 17:03:04 2015 UTC,Some day it'll be an antique and you'll be signing autographs...........................................
hardware,3ckpul,LuckyTtam,1 point,Thu Jul 9 13:44:07 2015 UTC,I'll sell it to you. How does 500 sound?
hardware,3ckpul,RemindMeBot,2,Thu Jul 9 13:45:07 2015 UTC,"I'll get back to you in a few years if we have some hyperinflation!  RemindMe! 1095 Days ""Tramp Stamp"""
hardware,3ckpul,briguyd,1 point,Thu Jul 9 15:01:16 2015 UTC,Messaging you on 2018-07-08 15:01:49 UTC to remind you of this comment.  CLICK THIS LINK to send a PM to also be reminded and to reduce spam.    [FAQs] | [Custom Reminder] | [Feedback] | [Code]
hardware,3ckpul,thordsvin,1 point,Thu Jul 9 15:02:03 2015 UTC,I didn't wait and got one with the old sails.
hardware,3ckpul,TruGW2,1 point,Fri Jul 10 15:05:29 2015 UTC,They can still use Razer products.
hardware,3ckpul,Conpen,5,Thu Jul 9 16:51:55 2015 UTC,"Looks pretty good to me, certainly more aggressive.   I hope they still keep the old happier full sail logo for their non-gaming products though."
hardware,3ckpul,Kichigai,13,Wed Jul 8 19:58:16 2015 UTC,"This new branding will represent all of our divisions going forward, starting with the products we announced today at Computex.   Looks like everything is using the new logo now."
hardware,3ckpul,mikami677,10,Wed Jul 8 22:11:14 2015 UTC,I can deal with that. Could be worse.
hardware,3ckpul,zaures,8,Thu Jul 9 00:59:33 2015 UTC,"Yeah, at least it doesn't say ""GAMING"" in yellow letters."
hardware,3ckpul,ascii,47,Thu Jul 9 03:05:05 2015 UTC,"I think it looks good. WAAAAAAYYYY better than the tramp stamp logo. It kind of conveys the idea of moving forward as the sails look filled. Always kind of thought they could just invert the colors and make the main sail the Jolly Roger if they wanted an alternate ""hardcore"" logo."
hardware,3ckpul,Maysock,-1,Wed Jul 8 20:59:04 2015 UTC,+1 for jolly roger.
hardware,3ckpul,Slosh876,23,Wed Jul 8 21:38:34 2015 UTC,So can I buy a badge of this for my k65?
hardware,3ckpul,BBLF5112,2,Wed Jul 8 17:49:46 2015 UTC,This i would like...
hardware,3ckpul,theGentlemanInWhite,21,Wed Jul 8 23:26:31 2015 UTC,Better than Logitech's new logo/name.
hardware,3ckpul,Last_Jedi,13,Wed Jul 8 21:39:14 2015 UTC,Wait they changed their name? WTF why would they ruin all the good  brand recognition for no reason?
hardware,3ckpul,prodikl,5,Thu Jul 9 03:55:29 2015 UTC,"They didn't change their name.  They did change their logo, and announced a new brand called ""Logi"" for new product lines not as related to PCs. They are still keeping Logitech for their mice/keyboard/audio lines."
hardware,3ckpul,Pillowsmeller18,7,Thu Jul 9 22:18:35 2015 UTC,Blegh the new logo reads like loogie and looks childish  I agree the old logo was dated but the name still has potential.
hardware,3ckpul,24759625,1 point,Thu Jul 9 04:06:34 2015 UTC,"I keep thinking Loki, the really useless marvel villain."
hardware,3ckpul,pabloe168,2,Thu Jul 9 09:06:43 2015 UTC,https://www.reddit.com/r/respectthreads/comments/2n3fcu/respect_loki/
hardware,3ckpul,losermode,1 point,Thu Jul 9 15:22:07 2015 UTC,Logitech is associated with cheap half ass working electronics from the late 2000s. That is my guess. Nowadays you want to make pastel color electronics for the youth I guess.
hardware,3ckpul,bubbles_of_justice,1 point,Thu Jul 9 13:17:57 2015 UTC,damn kids and their pastels!
hardware,3ckpul,Fade_0,1 point,Thu Jul 9 13:36:31 2015 UTC,"I associate Logitech with decent mice, this pastel Logi thing is poo poo."
hardware,3ckpul,bubbles_of_justice,1 point,Thu Jul 9 15:20:06 2015 UTC,i thought of it as 'reliable shit that was cheap as hell but worked always'
hardware,3ckpul,mojorific,1 point,Sat Jul 11 03:42:10 2015 UTC,"Agreed.  But not all their gear is cheap.  Look at the mx master.  I literally can not think of a non flashy,  non ""gaming"" mouse as good as or even close to it. I cant imagine this logi logo on it."
hardware,3ckpul,Kaghuros,10,Sat Jul 11 06:22:58 2015 UTC,Thank goodness they decided to listen to the community.  That gaming logo was awful.
hardware,3ckpul,sunjay140,2,Thu Jul 9 02:54:37 2015 UTC,I guess it lost them enough money that they reconsidered. It's a shame we'll never know just how much that was.
hardware,3ckpul,WhatGravitas,1 point,Thu Jul 9 10:03:12 2015 UTC,What was so bad about it? Why would a logo make them lose money?
hardware,3ckpul,ezpzzz,3,Thu Jul 9 22:20:34 2015 UTC,"Because a lot of people do consider aesthetics especially when buying from a company like Corsair that definitely puts a lot of styling into their products.  So you get a sleek keyboard like the K70 that (with some keycap mod) would even fit in a minimal setup... and suddenly it has a super-ugly logo - and it doesn't fit their aesthetic that always tried to balance some flashy flair with enough restraint that it doesn't scream ""teenage boy"".  If Corsair wasn't selling products on their aesthetic, that wouldn't be a problem, but they are."
hardware,3ckpul,x3tripleace3x,11,Fri Jul 10 17:51:49 2015 UTC,This logo is much better than the old crab wielding dual scimitars logo.
hardware,3ckpul,Darkstryke,9,Wed Jul 8 21:07:52 2015 UTC,"Looks like a cleaner version of the old logo, I think it's definitely an improvement."
hardware,3ckpul,dmg36,4,Wed Jul 8 21:24:47 2015 UTC,I vote they go back in time to when the product envelope was limited and they weren't trying to chase every market possible. If people want cheap junk there are tons of other high profile vendors out there.
hardware,3ckpul,Import,7,Thu Jul 9 01:32:00 2015 UTC,"Thats funny, everybody said the Gaming-Logo sucks so now they claim they merged it somehow. They should have just gone back to the old one and call it a day."
hardware,3ckpul,marcoalexander,3,Thu Jul 9 01:55:17 2015 UTC,Reminds me of The Pirate Bay logo
hardware,3ckpul,ascii,3,Thu Jul 9 01:46:40 2015 UTC,Their old tramp stamp logo is actually why I wouldn't buy the old merchandise xD   I like this newish logo!
hardware,3ckpul,Bickell,6,Thu Jul 9 03:19:26 2015 UTC,"The sails look better. Not sure about the font changes, but still a net improvement."
hardware,3ckpul,PLD,7,Wed Jul 8 21:37:52 2015 UTC,Thank God.  Whoever approved the crab logo should be fired.
hardware,3ckpul,jsu718,3,Thu Jul 9 01:29:32 2015 UTC,So just like their old sails logo? Why did they change in the first place?
hardware,3ckpul,omega552003,5,Thu Jul 9 00:36:57 2015 UTC,"They wanted a separate logo for their ""gaming"" line of products. Their response to the hate at the time on their forum was pretty negative but I guess this means they came around."
hardware,3ckpul,JarJarBanksy,5,Thu Jul 9 03:46:51 2015 UTC,"Sales probably talked more than the forums, i really wanted the k70 but it has the tramp logo and its soo fucking ugly. Yeah I might be vain, but its like getting a Ferrari and the shifter is bedazzled in fake gems. Its just tacky and detracts from the overall aesthetic of the device severely."
hardware,3ckpul,hdshatter,2,Thu Jul 9 03:59:11 2015 UTC,I am happy with anything other than their bat'leth.
hardware,3ckpul,TaintedSquirrel,3,Thu Jul 9 04:00:35 2015 UTC,Why not just go back to the original? The original is the best of the 3.
hardware,3ckpul,ktechnician,2,Thu Jul 9 01:15:34 2015 UTC,I see 3 waving dudes.
hardware,3ckpul,king_of_blades,2,Wed Jul 8 18:15:16 2015 UTC,Omigosh cannot unsee
hardware,3ckpul,SocksForBreakfast,2,Wed Jul 8 23:17:10 2015 UTC,I see 3 guys doing Heil Hitler!
hardware,3ckpul,arikv2,2,Thu Jul 9 06:46:29 2015 UTC,"I like this a lot.  Aside from just better conveying speed with the full sails, the sharp corners give it much more of a striking, bold look.  Fits well for both gaming and standard consumer products.  Never really realized how outdated the old one looked until now."
hardware,3ckpul,prodikl,0,Wed Jul 8 23:34:13 2015 UTC,Looks good but still uglier than original.  Why not just keep original?
hardware,3ckpul,Badass_Norwegian,1 point,Wed Jul 8 22:08:51 2015 UTC,I'm looking the new type face. Dem R's
hardware,3ckpul,illuminarei,1 point,Thu Jul 9 04:08:02 2015 UTC,I like.
hardware,3ckpul,Sofaboy90,1 point,Thu Jul 9 05:04:42 2015 UTC,"This is way better than the old logo. I'm surprised they didn't redesign it when it came out. The feedback was almost all negative, but I guess for global businesses it takes a lot of effort to change something once it's out (reprinting, web design, etc)."
hardware,3ckpul,Darkerson,1 point,Wed Jul 8 21:42:20 2015 UTC,it seems to me that im the only one who liked that corsair gaming logo ._. hard to argue  against it if the majority wasnt of that opinion
hardware,3ckpul,Themightyoakwood,1 point,Wed Jul 8 21:50:35 2015 UTC,"I didnt care one bit about the logo, either. I was more worried about the actual product the logo was on."
hardware,3ckpul,omega552003,-1,Thu Jul 9 02:21:42 2015 UTC,Who cares? Its a fucking logo. Make good shit; I buy good shit.
hardware,3ckpul,satertek,6,Thu Jul 9 03:12:36 2015 UTC,except it looked like shit so it sold like shit.
hardware,3ckpul,ktechnician,-4,Thu Jul 9 04:00:38 2015 UTC,"I don't understand this.  'sair' is a meaningless term.  Instead of a new logo, they should have just renamed the company 'Cor'."
hardware,3ckpul,satertek,5,Wed Jul 8 22:10:04 2015 UTC,I'll upvote you just for attempting it.
hardware,3ckpul,Lordgeorge16,5,Wed Jul 8 23:28:40 2015 UTC,"Tried to make a joke about the Logitech news today and apparently failed, lol."
hardware,3ckpul,dreakon,-3,Wed Jul 8 23:17:01 2015 UTC,I actually kinda liked the tramp stamp for the gaming logo.  Looks wicked nice on my M65 in deep blue.
hardware,3ckpul,Lordgeorge16,1 point,Wed Jul 8 23:19:19 2015 UTC,"I guess they were targeting the few people that still used the word ""wicked"" in that context."
hardware,3ckpul,ktechnician,2,Thu Jul 9 01:34:38 2015 UTC,"Well, I do live in Massachusetts."
hardware,3ckpul,Stuck_on_Earth,0,Thu Jul 9 14:36:59 2015 UTC,I like it!
hardware,3ckpul,weltraumMonster,0,Thu Jul 9 14:51:18 2015 UTC,Classy with a bit of an aggressive demeanor.  Acceptable.
hardware,3ckpul,Brilliantrocket,0,Wed Jul 8 23:16:26 2015 UTC,looks kind of like the pirate bay logo
hardware,3ckpul,G65434-2,-10,Wed Jul 8 23:54:24 2015 UTC,"Does anyone really care about logos? There could be a company whose logo is a giant cock, and I'd still buy their stuff if it worked well."
hardware,3ckpul,flyingfences,12,Thu Jul 9 08:08:42 2015 UTC,There could be a company whose logo is a giant cock   Personally I wouldn't want the giant cock stamped on the side of my tower or workstation; especially when I present it to the CEO for consideration of a brand change.
hardware,3ckpul,feyenord,6,Wed Jul 8 17:50:00 2015 UTC,"If they were just switching from the old sails to the new sails, nobody would give a shit. However, when they introduced the ""tramp stamp"" crossed swords logo a few months ago, it was incredibly unpopular. This is sort of a ""thank the heavens we don't have to see that anymore"" announcement."
hardware,3ckpul,SwampFox4,-1,Wed Jul 8 18:37:04 2015 UTC,"I wish I could say nobody does, but when they added that old gaming division one there was a monstrous uproar -- even though the products were still the same.  shrug, but some people like their brands to have a certain look and feel."
hardware,3ckpul,Rasangone,7,Wed Jul 8 20:59:42 2015 UTC,"I don't know much about design, but that gaming logo looks absolutely hideous, like something a bored high-schooler would draw. The new logo looks very sleek and dynamic and all it took was some slight changes in curves from the original design."
hardware,3ckpul,Symix,-4,Wed Jul 8 17:52:38 2015 UTC,"Yeah the logo looks like shit, but strangely enough my k70 rgb that has that logo on it still looks great and performs great.   Some people NEED everything to be aesthetically perfect. I would say that most people just want great functionality."
hardware,3ckpul,Logun0,-4,Wed Jul 8 18:16:32 2015 UTC,This is why they should have never changed the logo. People don't want change as many people be OCD.
hardware,3ckpul,SPCGMR,-1,Wed Jul 8 18:34:05 2015 UTC,Wouldn't have noticed tbh
hardware,3cj48r,LongBowNL,58,Wed Jul 8 08:50:19 2015 UTC,The mod here should update the sidebar with /r/AdvancedMicroDevices
hardware,3cj48r,wasdzxc963,29,Wed Jul 8 09:52:37 2015 UTC,Done! Thanks for noticing that.
hardware,3cj48r,barthw,48,Wed Jul 8 11:58:44 2015 UTC,What a dick move by the old owner of /r/AMD :/
hardware,3cj48r,ornothumper,9,Wed Jul 8 13:40:35 2015 UTC,What happened?
hardware,3cj48r,Duane,2,Wed Jul 8 13:51:04 2015 UTC,He isn't a mod of /r/AMD anymore:  https://www.reddit.com/user/jecrois
hardware,3cj48r,Charwinger21,5,Wed Jul 8 13:53:23 2015 UTC,I think it doesn't show subs that are private on that list to other people.
hardware,3cj48r,spiral6,3,Thu Jul 9 00:08:55 2015 UTC,He still is.
hardware,3cj48r,barthw,7,Thu Jul 9 05:56:50 2015 UTC,Just click the link and read the post ;)  also some more detailed info: https://www.reddit.com/r/redditrequest/comments/3cbjqm/please_reconsider_your_decision_on_ramd_the/cstz6ed
hardware,3cj48r,HereWeGoHawks,6,Thu Jul 9 05:37:50 2015 UTC,https://www.reddit.com/r/AdvancedMicroDevices/comments/3cgjat/we_have_moved_tldr_inside/
hardware,3cj48r,CFGX,27,Wed Jul 8 14:21:17 2015 UTC,"Thanks for the heads up, now I can continue getting my regular news that AMD has no interesting products in sight for me to upgrade to >:("
hardware,3cj48r,CFGX,12,Wed Jul 8 13:54:14 2015 UTC,Fury not interesting?
hardware,3cj48r,Yearlaren,25,Wed Jul 8 12:39:50 2015 UTC,"CPU-wise. I'm set in my ways dammit, I still call their GPUs ATI."
hardware,3cj48r,tru3gam3r,14,Wed Jul 8 12:49:36 2015 UTC,"I still have an ATi GPU, the 5670. The 5000 series was the last to be called ATi.  If you ask me, they made a huge mistake by killing the ATi brand."
hardware,3cj48r,Yearlaren,3,Wed Jul 8 12:50:56 2015 UTC,Why?
hardware,3cj48r,dontnation,1 point,Wed Jul 8 15:25:40 2015 UTC,Because people liked it.
hardware,3cj48r,dontnation,6,Wed Jul 8 22:22:24 2015 UTC,"""Zen"" next year then. :P"
hardware,3cj48r,TehRoot,4,Wed Jul 8 22:43:30 2015 UTC,I've delayed upgrading for coming close to 2 years hoping for something attractive from AMD. I can't wait any longer.
hardware,3cj48r,CFGX,2,Wed Jul 8 12:52:27 2015 UTC,"Intel then, and depending on the performance Zen+ (2017-2018??) could be the next upgrade."
hardware,3cj48r,shroudedwolf51,4,Wed Jul 8 16:32:14 2015 UTC,"Yeah I've held out so long might as well go skylake since I'll need a new mobo anyway. Sadly, I won't be upgrading again for probably another 4 years.  Better luck next time AMD. Still sticking with their GPUs for the forseeable at least."
hardware,3cj48r,TheRealLHOswald,2,Wed Jul 8 16:40:03 2015 UTC,Zen is 2016..
hardware,3cj48r,skilliard4,1 point,Wed Jul 8 16:45:49 2015 UTC,I've heard that one before.
hardware,3cj48r,tenkay,8,Wed Jul 8 12:58:26 2015 UTC,"2016 is not over yet. :P (Heck, it hasn't even started.)"
hardware,3cj48r,Darkstryke,1 point,Wed Jul 8 13:01:11 2015 UTC,"I hate to be the optimist here, but... I'll believe it when I see the benchmarks.   Don't get me wrong. I would most certainly love to see AMD give Intel a surprise run for their money, as they did in the Athlon64 era. Or, hell. Even have a viable competitor product that doesn't have to make sacrifices, but... Their recent happenings haven't exactly been igniting my pubes."
hardware,3cj48r,TehRoot,2,Wed Jul 8 13:46:09 2015 UTC,Their recent happenings haven't exactly been igniting my pubes.    You should try the 9590 then. Guaranteed to set something on fire.
hardware,3cj48r,Darkstryke,0,Wed Jul 8 17:38:14 2015 UTC,"By the time Zen actually comes out Intel will probably have already came out with 10 NM CPUs :/  edit: why the downvotes? Intel is at 14 NM right now when AMD is at 28 NM, Cannonlake is already being developed, AMD is still working on Zen."
hardware,3cj48r,TehRoot,3,Wed Jul 8 22:43:41 2015 UTC,"Guessing, because they announced only a few days ago that they're delaying 10nm process products and instead releasing them as 14nm products. Under the new timer frame it would mean Zen would have to not come out for about 3 years."
hardware,3cj48r,naanplussed,1 point,Wed Jul 8 17:30:57 2015 UTC,"Barton, the hayday of AMD was a result of both AMD hitting stride and Intel fumbling horribly with an architecture. We know what happened when Conroe hit the streets, it's been rear view mirror ever since.  I still have my 2500m somewhere.."
hardware,3cj48r,TehRoot,6,Wed Jul 8 19:26:14 2015 UTC,"AMD didn't fumble with Bulldozer so much as they aimed for the wrong target.   Subsequent revisions did improve performance but there's only so much you can do to an architecture that's meant to be built around one type of application.   Zen is a new architecture, and I have good hope in AMD with Keller around."
hardware,3cj48r,wulfgar_beornegar,2,Wed Jul 8 14:49:01 2015 UTC,"Completely missing the mark with your intended market is the epitome of a fumble. I don't hold out much hope for Zen in so much as the same thing was said about Phenom/II, and they didn't turn out till it was too late."
hardware,3cj48r,Tizaki,5,Wed Jul 8 15:24:10 2015 UTC,"They didn't intend to hit the wrong target, multicore was becoming the premo-strategy, but it never took off as much as it was supposed too, and AMD was stuck with CMT.   You don't create new CPU architectures overnight mate. IIRC zen's been in development for the better part of the last 3 years, at least since 2012 since Keller immediately jumped on working on a new architecture in 2012."
hardware,3cj48r,del_rio,1 point,Wed Jul 8 16:05:33 2015 UTC,"And the 8320 (example) is good for streaming while playing? So their work in 2010, 2011 was paying off since that was more popular in the last two years than 2010. But buying was already slanted to Intel."
hardware,3cj48r,del_rio,1 point,Wed Jul 8 16:08:02 2015 UTC,"I had a 1090t until October of last year so that should be a testament to how long an architecture like K10 could last and bring good stuff to the table. I think I had a 945 previous to that too, so I was on K10 for the better part of 6 years or so before I felt like I needed to upgrade at all, and even then it was mostly for my overclocking/benchmarking pleasure.   Zen is a direct line back to K8 and 10 with Keller as lead engineer so I expect great things next year."
hardware,3cj48r,MerryLane,1 point,Thu Jul 9 17:14:56 2015 UTC,It's a really hard habit to break.
hardware,3cp128,cantbebothered67835,7,Thu Jul 9 16:55:43 2015 UTC,Any reason you don't want to buy a 1TB external and just copy to that?
hardware,3cp128,techdawg667,0,Thu Jul 9 17:02:51 2015 UTC,Too expensive...
hardware,3cp128,techdawg667,15,Thu Jul 9 17:12:01 2015 UTC,You can get a 1TB for 50$ and it will be a lot more useful than 40 DVDs
hardware,3cp128,fear865,6,Thu Jul 9 17:14:22 2015 UTC,$50 is too expensive? A Roll of DVDs is $25 alone.
hardware,3cp128,holydude02,3,Thu Jul 9 17:15:44 2015 UTC,A roll of 50 dvd's is about $10 in my country.
hardware,3cp128,Stingray88,15,Thu Jul 9 17:19:38 2015 UTC,Per GB a hard drive will probably outvalue the DVDs though. Plus it's easier. And faster. And rewritable. And safer.
hardware,3cp128,v8xd,-6,Thu Jul 9 17:39:53 2015 UTC,"Cheaper, easier, faster and rewritable yes.  Safer? No.   A hard drive is a lot less reliable than optical media. Especially if you can only afford one drive."
hardware,3cp128,autowikibot,4,Thu Jul 9 22:48:33 2015 UTC,Optical media are known for disc rot. Hard drives don't have that problem. https://en.wikipedia.org/wiki/Disc_rot
hardware,3cp128,andromeduck,1 point,Fri Jul 10 11:03:19 2015 UTC,"Disc rot:       Disc rot is a phrase describing the tendency of CD or DVD or other optical discs to become unreadable due to physical or chemical deterioration. The causes of this effect vary from oxidation of the reflective layer, to physical scuffing and abrasion of disc surfaces or edges, including visible scratches, to other kinds of reactions with contaminants, to ultra-violet light damage and de-bonding of the adhesive used to adhere the layers of the disc together.    Image i - Disc rot     Relevant: Compact disc | DVD-D | Compact Disc bronzing | RAR (file format)   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3cp128,Stingray88,1 point,Fri Jul 10 11:05:04 2015 UTC,Hard drives need to be spun up a few times a year.
hardware,3cp128,hitsujiTMO,1 point,Sun Jul 12 02:12:45 2015 UTC,"With proper storage, optical media lasts decades. Hard drives DO NOT. Disc rot doesn't just happen under any circumstances. Hell part of the description is about scratches... That is not something that happens to properly cared for discs. That's like saying hard drives are susceptible to failure from being dropped.   There's a good reason no company ever looks to hard drives for data archival. They use optical media and tape media. Never ever hard drives. Optical media for small scale buisness, less data. Tape media for larger business, lots of data."
hardware,3cp128,Stingray88,-5,Fri Jul 10 14:31:59 2015 UTC,"Exactly this. A typical writeable DVD will last for 2 years, with typically an upper limit of 5 years."
hardware,3cp128,capn_hector,2,Fri Jul 10 11:37:03 2015 UTC,"Lol no. You're thinking of early CDs. Newer optical media will last decades with proper storage. We all have optical discs at home that are 15+ years at home (DVDs), and they're rated to last much longer. How many people have HDDs that still spin up after 10 years? Not many. How many after 15 years? Almost none."
hardware,3cp128,You_Gullible_Sheep2,7,Fri Jul 10 14:32:36 2015 UTC,"The discs that most people are familiar with lasting so well aren't writable media however - rather than being written with a laser they're pressed, which yields much more durable pits/lands.  I can personally tell you that in my DVD backups from a decade ago I have a significant amount of bit-rot.  Some brands of media are worse than others, but when I last checked (a couple years ago) about 20% have severe bit-rot and another 30% have some minor degree of bit rot over.  They've sat in a cakebox spindle or in DVD binders on a shelf in my room, nothing extreme.  You're probably correct that it's better than a HDD would do over the same timeframe, but it's still not a trivial problem.  Now, if you really want it to last, try tape.  My father used to pull 10-year-old DLT tape backups from the family computer, no problem.  The one I'm kinda curious about nowadays is M-DISC Blu-Ray.  Way more expensive than pretty much anything else, though.  Also for the record DVDs only went on sale in Japan in 1995, the US in 1997 and in Europe in 1998, so it's strictly 20 years or less, with a strong probability of ""less"".  Most people probably only got them around the 2000 timeframe.  I can date ours to no earlier than May 1999 because the first movie we watched was Star Trek Insurrection, which was pretty new at the time.  And we were pretty early adopters."
hardware,3cp128,HavocInferno,1 point,Fri Jul 10 15:20:18 2015 UTC,I've got 10 year old 40GB IDE drives that work.
hardware,3cp128,MinecraftChrizz,2,Sun Jul 12 12:02:07 2015 UTC,so 10$ for ~235GB (assuming 4.7GB DVDs). How much is a 500G/1TB HDD in your country? And how much does saving 30$ mean to you compared to burning DVDs for three weeks?
hardware,3cp128,sterob,2,Thu Jul 9 18:06:49 2015 UTC,A 1tb would be around $65 - $70
hardware,3cp128,Randomoneh,2,Thu Jul 9 18:17:14 2015 UTC,"This or this is twice that, but will offer you a lot more convenience, because all your data is in one place. You don't have to search through 50 dvd's to find something."
hardware,3cp128,Stingray88,2,Thu Jul 9 21:42:56 2015 UTC,"cheap, fast, good you can only choose 2 of them."
hardware,3cp128,Drudicta,1 point,Fri Jul 10 12:58:26 2015 UTC,Then hard drive is not that much more expensive per GB. It is - however - much more faster and safer.
hardware,3cp128,Stingray88,-4,Thu Jul 9 17:49:20 2015 UTC,A hard drive is not safer than optical media if stored properly. Optical media is supposed to last on the shelves for 50+ years. Hard drives die much much sooner.
hardware,3cp128,Drudicta,2,Fri Jul 10 00:17:27 2015 UTC,"The same could be said if you unplugged and stored a HDD in the same conditions....  Dry, without light, and without vibration."
hardware,3cp128,Stingray88,2,Fri Jul 10 03:16:43 2015 UTC,Nope. Not at all. Hard drives are not rated to live that long on the shelf. They are absolutely not recommended for long term data archival. After 5-10 years of sitting on the shelf it's very common to grab a drive and find it won't spin up anymore. This is one part of why tape backups are still a thing.
hardware,3cp128,v8xd,1 point,Fri Jul 10 04:14:25 2015 UTC,I should probably be replacing my oldest drive then..... I have a 500GB that's been running since 2005.... Slowest in my machine but it still reads what's on it.
hardware,3cp128,Stingray88,1 point,Fri Jul 10 04:50:35 2015 UTC,"After 10 years you absolutely should. Don't get me wrong, you could be lucky and it'll run for another 5 years. But honestly and drive after 10 years is a ticking time bomb."
hardware,3cp128,Anarchyz11,1 point,Fri Jul 10 05:07:30 2015 UTC,"Citation needed. I don't believe one word you're saying. A hard disc is a mechanical drive, the bits will stay in the disc no matter how long you keep it. If you don't use it and store it properly, it will not magically fail after x years."
hardware,3cp128,willxcore,2,Fri Jul 10 11:00:16 2015 UTC,"First or all that's not true. Hard drives are designed for relatively short term data storage. Quite apart from the mechanical issues there's the fact that magnetic media, and I mean ALL magnetic media, does deteriorate with age, even under absolutely ideal conditions. This doesn't require any external influences as magnetic particles are affected by others near by and in modern drives the density is nothing short of astonishing.  A drive in use has the magnetic strength regularly refreshed, either by actual writing to disk or by the drive's own low level system, which periodically reads and rewrites sectors. A drive in storage or even one that's simply powered down, receives none of that refreshing of the data.  As for how long a drive can be stored without becoming at least partially unreadable is an ongoing topic of debate and will certainly vary even across drives from the same batch, let alone different models or manufacturers. I personally would never rely on a device specifically designed for short term data storage for any significant length of time. Even tapes, which are designed for relatively long term data storage, should be refreshed every few years.  But it's not the platters that are the main problem, it's the mechanical parts that fail. I said it doesn't spin up anymore, not that the platters magically wipe. Can you still recover the data? Sure! But that's incredibly expensive and you'd be doing it quite frequently.   Citation: Myself. Two decades in data archival of multiple petabytes of data working as an editor and IT consultant in Hollywood. Hard drives fail. Optical does to a lesser degree. Tape is the best. Take it or leave it."
hardware,3cp128,Blowmewhileiplaycod,3,Fri Jul 10 14:27:10 2015 UTC,Storage costs money dude.  I guess you could find a cheap 250GB drive for $20~ but it's not going to get easier than that.
hardware,3cp128,capn_hector,1 point,Thu Jul 9 17:18:09 2015 UTC,you can't spare 50-60 dollars off of your next paycheck to buy an external for your backups? the data must not be worth that much to you...
hardware,3cp128,Kichigai,5,Mon Jul 13 18:54:49 2015 UTC,Just get a 250gb HDD and call it a day
hardware,3cp128,dragontamer5788,3,Thu Jul 9 20:40:08 2015 UTC,"Amazon Glacier.  $0.01 per gigabyte per month and be done with it.  You have 200GB of data, would you pay $2 per month to make it Someone Else's Problem?  If you don't want an ongoing cost, if I remember Blueray is roughly equal or maybe a little more expensive per GB then DVD.  If you value having to master / write / store / search through 80% fewer discs then it's worth it.  The other way is hard disks.  If you have the data on multiple HDDs and the chance of failure is independent, it's unlikely they both spontaneously go corrupt at the same time.  Note that things that can make the chance of failure ""not independent"" includes buying multiple disks at the same time (they may be from the same batch, and they will follow roughly the same aging process), the same environmental conditions, etc.  The general rule of backups is 3-2-1 - 3 copies, 2 different mediums, one copy offsite.  If you do HDD or optical backups build parity files.  These will help you recover data if there's read errors.  I use a ""rolling"" scheme, the parity files for disc N are written to disc N+1.  Therefore even if an entire sequence of discs gets corrupted, I can recover them as long as I have one good parity file at the end.  I take the parity file from disc X, then use it to rebuild disc X-1, then use that to rebuild X-2, and keep rolling backwards until I hit the start of the corruption.  You can only recover as much corrupted data as you have parity - I typically do 20% parity, as I think anything more than that is probably catastrophic.  One more thing - if that's the fastest you can go, you have a problem with your drive.  700 KB/s is VERY low for a write rate.  If it's intentional, that's just fine.  I think it's good to burn optical discs slow - lower speed means each pit is exposed to the laser for a longer period of time, which translates into a more durable and lasting pit.  I always burn at 1x for my archival discs.  Also, I think you were talking about your drive, but if you were talking about media - DVD-RWs are NOT a good archival mechanism.  Use plain -Rs."
hardware,3cp128,Kichigai,4,Fri Jul 10 15:00:06 2015 UTC,"Tape is pretty fast, relatively speaking. It's just that the drives are expensive as hell and restoration can take a while because of the slow seek time.  Alternatively, a hard disk.  Edit: clarified my position on tape's pricing."
hardware,3cp128,Maysock,9,Thu Jul 9 17:01:46 2015 UTC,"Tape is actually very cheap, its the cheapest option per TB by far.  Too bad Tape Drives cost $2000+ USD. But 2.5TB LTO-6 tapes are only like... $30 USD."
hardware,3cp128,Maysock,5,Thu Jul 9 17:41:54 2015 UTC,"That's what I had meant to say. The initial investment really sets you back some. If we're only talking hundreds of gigs, though, LTO-3 might even be an option. Drive prices come down to a couple hundred, but you've still got to get into some SCSI hardware."
hardware,3cp128,Caw108,1 point,Thu Jul 9 17:48:42 2015 UTC,"So uh... What are you copying to DVDs? What about, like, Google drive or dropbox?"
hardware,3cp128,Maysock,1 point,Thu Jul 9 21:00:21 2015 UTC,Need over 200gb of storage
hardware,3cp128,PaulTheMerc,2,Thu Jul 9 21:56:56 2015 UTC,for???
hardware,3cp128,Maysock,3,Thu Jul 9 21:59:53 2015 UTC,I'm guessing for his porn collection.
hardware,3cp128,Caw108,2,Thu Jul 9 22:31:50 2015 UTC,that's my guess too since i've asked twice now. lel.
hardware,3cp128,Soytaco,2,Thu Jul 9 22:42:58 2015 UTC,It's for my gog games backlog.
hardware,3cp128,iMADEthis2post,1 point,Thu Jul 9 22:48:05 2015 UTC,explains the physical media requirement.
hardware,3cp128,TheBloodEagleX,-1,Thu Jul 9 23:18:10 2015 UTC,is gog an incorrect acronym for german goo girls?
hardware,3cp128,DJ-Foran,5,Thu Jul 9 23:41:50 2015 UTC,Gay on gay porn.
hardware,3cm4gj,shrewduser,9,Wed Jul 8 23:50:46 2015 UTC,"Anyone know why DX12 requires Windows 10, but Vulkan requires no such thing?"
hardware,3cm4gj,Jrix,20,Thu Jul 9 03:35:16 2015 UTC,"since the other commenter are being sideways and idk if your question is sarcastic.  DX is microsoft's project. so it is fairly natural they would use it as a migration point. i haven't heard but if they are reworking and still ahving a direct software store as part of win10, then that feature (which is lacking in win7) is their motivation.  one of the things they want is to reunite all the versions of windows. versions https://xkcd.com/927/  vulkan seems to be nearly an industry wide initiative, not even close to being majority owned by one contributor."
hardware,3cm4gj,fae_lai,2,Thu Jul 9 05:48:50 2015 UTC,"Image  Title: Standards  Title-text: Fortunately, the charging one has been solved now that we've all standardized on mini-USB. Or is it micro-USB? Shit.  Comic Explanation  Stats: This comic has been referenced 1722 times, representing 2.4036% of referenced xkcds.    xkcd.com | xkcd sub | Problems/Bugs? | Statistics | Stop Replying | Delete"
hardware,3cm4gj,xkcd_transcriber,14,Thu Jul 9 05:49:02 2015 UTC,Becuase Microsoft says so.
hardware,3cm4gj,Teethpasta,7,Thu Jul 9 04:01:27 2015 UTC,"Direct3D 12 requires WDDM 2.0, which is only available in Windows 10. I mean, I suppose you could say that's money related.  Windows Vista and Windows 7 both are in the extended support phase now, so they only do security patches. I suppose it would kind of suck for Windows 8/8.1 users to not get it since they're both in mainstream support still. However, with the free upgrade from 7/8/8.1 it won't be costing most people anything extra to upgrade."
hardware,3cm4gj,venom8599,6,Thu Jul 9 08:14:16 2015 UTC,"According to Microsoft, DX12 is dependent on WDDM 2.0, which is only on W10."
hardware,3cm4gj,LongBowNL,15,Thu Jul 9 07:26:35 2015 UTC,Money. Microsoft wants everyone to switch to Win10
hardware,3cm4gj,Idkidks,1 point,Thu Jul 9 03:49:05 2015 UTC,Well it is free though
hardware,3cm4gj,pb7280,2,Tue Jul 14 10:16:34 2015 UTC,Same reason that OpenGL 4 runs on Windows XP but not Direct X 11.  Because Microsoft said so.
hardware,3cm4gj,wrosecrans,1 point,Fri Jul 10 09:44:12 2015 UTC,does this mean we could see gaming on Linux and OS X be on the same level as windows in the future because Vulcan is an open standard
hardware,3cm4gj,a_simple_pie,1 point,Sun Jul 12 00:49:34 2015 UTC,as long as the drivers are up to par then it should be fairly comparable. more and more these days doing a game for a single platform / not making it portable and cross platform is just not viable.
hardware,3cm4gj,hdshatter,-18,Sun Jul 12 01:26:57 2015 UTC,The success of this API largely depends on the success of the Steam consoles.
hardware,3cm4gj,maybachsonbachs,14,Thu Jul 9 01:29:43 2015 UTC,"Completely disagree. Vulkan will succeed because engines and middleware will migrate to it, to say nothing of the better driver support. The people who matter will favor it.   I mean what's the alternative? Continuing to use monolithic APIs that everyone hates except ""hello world"" novices?"
hardware,3cm4gj,andromeduck,2,Thu Jul 9 02:00:42 2015 UTC,Engines and middleware are already on board. IMO what's going to be really make or break is Apple and Google because like it or not those two basically control the space outside of Windows/Consoles.
hardware,3cm4gj,pb7280,1 point,Thu Jul 9 19:03:04 2015 UTC,"Well Apple is using their own low level API ""Metal,"" it's coming to Macs soon too I think."
hardware,3cm4gj,andromeduck,1 point,Tue Jul 14 10:18:01 2015 UTC,Already arrived in the preview
hardware,3cm4gj,pb7280,1 point,Tue Jul 14 16:45:34 2015 UTC,"Yeah I have the preview, are there any games or apps that use it yet though?"
hardware,3cm4gj,Astrognome,13,Tue Jul 14 21:03:15 2015 UTC,"Entirely false.  What matters most is DX12 is only on two platforms, Windows and XBone.  Vulkan will work on anything that has Vulkan support, and nothing is stopping anyone from adding it to their system. I'd expect both consoles to get it, along with stuff like Chromebooks and newer mobile devices where the performance gains will be even more important."
hardware,3cm4gj,hdshatter,9,Thu Jul 9 03:23:54 2015 UTC,Microsoft never supported opengl on the Xbox.
hardware,3cm4gj,Astrognome,4,Thu Jul 9 03:30:35 2015 UTC,Good point.
hardware,3cm4gj,Exist50,1 point,Thu Jul 9 03:33:14 2015 UTC,No point in using a higher level API for fixed hardware.
hardware,3cm4gj,hdshatter,8,Fri Jul 10 00:48:13 2015 UTC,Vulcan is intended to supersede openGL in all use cases... therefore it will become the graphics API for most tablets / phones / embedded devices  it hardly seems to rest on the success of the steambox.
hardware,3cm4gj,Teethpasta,-1,Thu Jul 9 03:56:13 2015 UTC,I meant for PC games. Microsoft already secured their domination for PC gaming for the foreseeable future. Why should a developer spend their time to add Vulcan if 5% (made up number) of the market would be the only thing its useful for?
hardware,3cm4gj,hdshatter,9,Thu Jul 9 03:58:10 2015 UTC,Because you don't need to need to develop for dx12 at all and can just use vulkan
hardware,3cm4gj,MINIMAN10000,-1,Thu Jul 9 04:04:05 2015 UTC,"OpenGL was the same way, how many popular games used it before Steams big linux push? Close to none."
hardware,3cm4gj,Teethpasta,6,Thu Jul 9 04:11:49 2015 UTC,It seemed to me that there was a large negative connotation with opengl for a long while and people would always use directX. With vulkan being performance focused I believe it has the possibility to create a new image to really become popular.
hardware,3cm4gj,andromeduck,5,Thu Jul 9 05:09:53 2015 UTC,Opengl was not as good though vulkan should be much better
hardware,3cm4gj,TheCrimsonEscapade,1 point,Thu Jul 9 05:53:08 2015 UTC,"depends on what point in time, they've traded places regularly and in different areas - it's not quite clear cut  OGL drivers are pretty inconsistent though because there's not as much incentive for some vendors to go that extra step"
hardware,3cm4gj,hdshatter,3,Thu Jul 9 19:07:33 2015 UTC,"Quite a bit, actually. https://en.wikipedia.org/wiki/List_of_OpenGL_programs"
hardware,3cm4gj,Charwinger21,1 point,Thu Jul 9 15:09:04 2015 UTC,"Vulcan is actually a lot more similar to the api's the consoles use (vulcan is based on mantle which is written for the GCN architecture) so there's the fact that vulcan will make high performance porting easier though this is arguably true of dx12 anyway  but this point is rather moot, the world we live in now, most game developers don't worry about the underlying graphics api any more, that's usually abstracted away by an engine or set of libraries and those will target vulcan   vulcan will be performant and cross platform (mac / linux / windows (not just win 10) android / steambox etc) and again similar to the xbone and ps4 api's"
hardware,3cm4gj,Exist50,5,Thu Jul 9 05:27:53 2015 UTC,Has anyone else said they are supporting Vulcan besides Valve's Source 2.0? (Steamboxes is why)
hardware,3cm4gj,andromeduck,6,Thu Jul 9 05:36:01 2015 UTC,"Has anyone else said they are supporting Vulcan besides Valve's Source 2.0? (Steamboxes is why)   Unreal Engine and Frostbite's dev have shown extensive support for Vulkan, and presumably every engine that currently uses OpenGL will use Vulkan as well (so presumably CryEngine, Unity, Clausewitz, 4A, REDengine, id Tech, IW, etc.)."
hardware,3cm4gj,dudemanguy301,3,Thu Jul 9 06:20:34 2015 UTC,"Vulkan, being the child of Mantle, should be reasonably easy to get working with Frostbite, I'd think."
hardware,3ckkld,spiral6,25,Wed Jul 8 17:11:08 2015 UTC,But can it play Batman: Arkham Knight?   My sides
hardware,3ckkld,rationis,11,Wed Jul 8 20:11:45 2015 UTC,Sweet Jesus! I want one
hardware,3ckkld,andromeduck,-3,Wed Jul 8 17:38:33 2015 UTC,why?
hardware,3ckkld,NLWoody,36,Wed Jul 8 18:08:15 2015 UTC,His history says he has dual xeons and interest is Folding. So I'm guessing he does more on the productive side than entertainment.
hardware,3ckkld,SirGregorius,4,Wed Jul 8 19:01:13 2015 UTC,Folding as in protein folding?
hardware,3ckkld,Reapexx,17,Thu Jul 9 13:14:52 2015 UTC,"No, competetive origami. It's big in Japan."
hardware,3ckkld,BigJewFingers,4,Thu Jul 9 13:59:12 2015 UTC,"He might also make animals out of towels, but I can't be sure."
hardware,3ckkld,SirGregorius,3,Thu Jul 9 15:05:50 2015 UTC,Correct
hardware,3ckkld,SirGregorius,12,Thu Jul 9 15:08:24 2015 UTC,"scientific apps, data crunching/mining, medical imaging, VFX, the list goes on. Stuff that is way more complicated than just dicking around with video games"
hardware,3ckkld,poematik,11,Wed Jul 8 22:14:29 2015 UTC,"Same here whenever someone questions why I have 32GB of memory in my system, when the general consensus is that 8GB is all you need for gaming.  Well, it's probably because I run VMs and labs on my machine for work.  Which means I need all the memory I can get."
hardware,3ckkld,animeman59,6,Thu Jul 9 00:22:24 2015 UTC,"This. I'm surprised that people are surprised that you can use PC for anything but entertainment.  It gets even better when I say, that 64GB of RAM in my home PC isn't that much anyway."
hardware,3ckkld,dwe11er,6,Thu Jul 9 08:02:45 2015 UTC,"I'm an editor and do motion graphics. 32GB is almost the minimum for what I do. I'd love to move to x99 for 4x that. More ram, more cache, faster working in software.  That and I can say I have 128gb of ram... which is fun too."
hardware,3ckkld,SirCrest_YT,12,Thu Jul 9 13:54:55 2015 UTC,"This is a compute card, not even a workstation one. It can only really work in a server environment."
hardware,3ckkld,Exist50,7,Wed Jul 8 22:00:55 2015 UTC,"Yeah, it was build for enterprise purposes."
hardware,3ckkld,sk9592,1 point,Wed Jul 8 23:25:40 2015 UTC,"Yep, no video outputs on this card."
hardware,3ckkld,sob3k,7,Thu Jul 9 15:47:03 2015 UTC,Finally GPU with enough VRAM for GTA V on 4k
hardware,3ckkld,white-forest,2,Wed Jul 8 21:04:44 2015 UTC,This card would be amazing for VDI if AMD got a vGPU like driver out.
hardware,3ckkld,sterob,2,Thu Jul 9 14:22:15 2015 UTC,the question is how many hash can this card do?
hardware,3ckkld,krista_,2,Thu Jul 9 15:21:08 2015 UTC,All of them.
hardware,3ckkld,illuminarei,3,Fri Jul 10 00:40:14 2015 UTC,"Skyrim, with all the ENBS."
hardware,3ckkld,Farlo1,2,Wed Jul 8 21:43:05 2015 UTC,8k textures for every object!  Cup with 300 polygons? Fuck it!
hardware,3ckkld,Toxiguana,2,Thu Jul 9 09:10:58 2015 UTC,"I'm not very knowledgable when it comes to server grade graphics cards. But by the sound of this article, it seems like this one is kicking Nvidia's offerings in the can."
hardware,3ckkld,Exist50,7,Thu Jul 9 02:05:05 2015 UTC,"Nvidia's compute offerings are mostly based on Kepler, and GCN has been a very strong compute architecture from the beginning."
hardware,3ckkld,dylan522p,3,Thu Jul 9 05:46:14 2015 UTC,Problem is getting all that Compute performance to be put in use is way harder than doing the same for Nvidia cards with cuda
hardware,3ckkld,Exist50,2,Thu Jul 9 16:51:30 2015 UTC,"Yeah, software is another (huge) issue, but AMD is very compelling in the DP compute segment. One of the only areas that Maxwell hasn't hurt."
hardware,3ckkld,zarazek,1 point,Thu Jul 9 17:03:01 2015 UTC,"In terms of performance, sure. But many GPU compute apps are CUDA-only and this gives NVIDIA advantage of client lock-up."
hardware,3ckkld,sk9592,-1,Thu Jul 9 10:21:22 2015 UTC,"This is based on Hawaii/Grenada rather than Fiji. Makes sense since 32GB of GDDR5 is currently not possible with HBM.  Depending on what you're doing, if 12GB of GDDR5 is enough for you, a Quadro M6000 would be a better choice."
hardware,3cqups,Ethan_G,10,Fri Jul 10 01:16:14 2015 UTC,"Looks like there'll be a 950, no ti, as a 960 cut around $150."
hardware,3cqups,Exist50,2,Fri Jul 10 01:23:16 2015 UTC,A cut 960 for $150 is horrible performance per dollar.
hardware,3cqups,JarJarBanksy,1 point,Fri Jul 10 21:01:36 2015 UTC,That almost certainly means that we'll see a price drop for the GTX 750Ti and 750 right?
hardware,3cqups,Yearlaren,5,Fri Jul 10 01:34:45 2015 UTC,They will be discontinued. This is how they keep their profits up.
hardware,3cqups,arikv2,1 point,Fri Jul 10 03:05:21 2015 UTC,"Well, it's still rumors. And the card will still need a 6-pin connector."
hardware,3cqups,Exist50,1 point,Fri Jul 10 01:36:30 2015 UTC,I'd say this is very likely.
hardware,3cqups,TheImmortalLS,1 point,Fri Jul 10 01:39:23 2015 UTC,"This sums it up. There's gm200 (titan x and 980ti), 204 (980,970,980m,970m), 200 (960, lesser mobile, 950)"
hardware,3cqups,TheImmortalLS,3,Fri Jul 10 01:40:38 2015 UTC,"206 (960, lesser mobile, 950)   FTFY"
hardware,3cqups,everyZig,1 point,Fri Jul 10 05:45:22 2015 UTC,ty mate
hardware,3cqups,TheImmortalLS,2,Fri Jul 10 15:53:16 2015 UTC,Also GM107 (GTX 750Ti and GTX 750).
hardware,3cqups,Yearlaren,-3,Fri Jul 10 02:23:10 2015 UTC,there better be a ti or ill die.
hardware,3cqups,makar1,2,Fri Jul 10 02:39:55 2015 UTC,"The 960 is the ""Ti""."
hardware,3ckgw6,dengudomlige,10,Wed Jul 8 16:45:01 2015 UTC,"We found that from our NVIDIA test system when running at the full 3440 x 1440 resolution and 75Hz refresh rate, that the screen seemed to drop some frames. We verified this via the tests at BlurBusters.com, but you could also see the issue with the naked eye on moving content like PixPerAn. In those tests the image skipped and jumped a bit. It only did this at the maximum 75Hz refresh rate, not at anything lower like 60Hz.  We found no such issue with the screen from our AMD system, either using a proper FreeSync end to end setup, or breaking the FreeSync chain and using the card with an older driver or without DisplayPort. Perhaps this is an issue with NVIDIA cards. We've reported it back to Acer for further checks.   ಠ_ಠ  Otherwise it looks like a nice monitor, but holy shit at the €1200 price tag."
hardware,3ckgw6,OftenSarcastic,1 point,Wed Jul 8 18:07:00 2015 UTC,I ain't paying that much for a monitor that drops frames.
hardware,3ckgw6,bizude,6,Wed Jul 8 20:21:23 2015 UTC,Only a problem with their Nvidia test card though so might not be the monitor's fault. I'll update the post with the full quote.
hardware,3ckgw6,OftenSarcastic,5,Wed Jul 8 20:31:01 2015 UTC,Why would you even use a freesync monitor with an nvidia card?
hardware,3ckgw6,wanking_furiously,14,Wed Jul 8 21:18:18 2015 UTC,To see what would happen.
hardware,3ckgw6,Spacebotzero,3,Wed Jul 8 21:23:40 2015 UTC,For science!
hardware,3ckgw6,animeman59,1 point,Thu Jul 9 00:25:35 2015 UTC,I don't think there's any other 1440p that does 75hz
hardware,3ckgw6,bentan77,1 point,Thu Jul 9 00:24:26 2015 UTC,A Gsync version if this monitor is coming out soon.
hardware,3ckgw6,TrptJim,2,Thu Jul 9 01:13:42 2015 UTC,For probably $300 more right? :(
hardware,3ckgw6,Stingray88,1 point,Thu Jul 9 22:59:40 2015 UTC,"Because it's still a monitor? Should someone with no USB 3 capable devices not bother buying this either because it doesn't have legacy ports? The monitor is still an IPS, 75Hz @ WQHD device. It shouldn't be screwing up the way it has with an nVidia GPU but that's a defect and not by design.  Besides, I'm fairly confident that nVidia will enable support for Adaptive Sync before long."
hardware,3ckgw6,LazyGit,5,Thu Jul 9 12:01:17 2015 UTC,Great to see a Freesync monitor support a minimum of 30hz when above 1080p.
hardware,3ckgw6,OftenSarcastic,4,Wed Jul 8 16:46:30 2015 UTC,"Obviously the more important feature is this:     An LED strip of lights is located along most of the bottom edge of the screen which can be controlled via the OSD menu as shown above. [ . . . ] The ripple is quite nice, moving from end to end like the light on the front of KITT in Knight Rider (old school 80's reference!)"
hardware,3ckgw6,Spacebotzero,2,Wed Jul 8 18:15:56 2015 UTC,Huh?  Sounds gimmicky as fuck.  Why does this exist....along with built in speakers?
hardware,3ckgw6,OftenSarcastic,12,Wed Jul 8 19:33:40 2015 UTC,"Because it's part of their gamer brand and gamers love blinking lights of course. At least I'm assuming that's why both my keyboard and mouse have blinking light modes, it really adds to the experience and intensity of gaming!  Fuck knows why they added speakers. Everybody knows gamers wear headsets with more blinking lights."
hardware,3ckgw6,Foxton01,3,Wed Jul 8 19:49:05 2015 UTC,Everybody knows gamers wear headsets with more blinking lights.   You hurt me with this. Because now i wish my headphones had blinking lights.
hardware,3ckgw6,Spacebotzero,2,Thu Jul 9 04:55:15 2015 UTC,Speakers are just a free plus in the reviews. It costs them nothing. Reviewers always mention it though.  Cylon led strips are cool though! :)
hardware,3ckgw6,mmencius,2,Wed Jul 8 23:36:03 2015 UTC,"Well that fact that you can change the color, how the light works...like waves or pulse...or solid...is pretty neat. I'm okay with this, the review said the panel is very good so just as long as it can perform well, I'm okay with the addition of ambient lighting."
hardware,3ckgw6,Hariooo,0,Thu Jul 9 00:12:19 2015 UTC,But you're meant to hit blinking lights with your sword! How do gamers avoid breaking all their stuff?
hardware,3ckgw6,Spacebotzero,1 point,Thu Jul 9 00:44:51 2015 UTC,along with built in speakers   If someone uses headphones 100% of the time then maybe it'd be nice to have speakers on the monitor for when a friend comes over and you want to share a Youtube video or something without having had to buy an external set of speakers or squishing your heads together next to headphones?
hardware,3ckgw6,Hariooo,3,Thu Jul 9 14:22:01 2015 UTC,"I totally see your point.  I'm just questioning the user base that could afford a $1000+ monitor.  I'd assume that user can and does have an external set of speakers already that perform better than any kind of integrated set of speakers.  It is possible that those speakers had to be purchased along with the other parts as a sort of combo deal from the vender/supplier/manufacturer. Acer probably put it all together to create their product.  After all,  the panel is from LG."
hardware,3ckgw6,bigpappaflea,2,Thu Jul 9 14:33:30 2015 UTC,And I'd reject that assumption completely.
hardware,3ckgw6,OftenSarcastic,2,Thu Jul 9 14:38:11 2015 UTC,I am most interested to find out how long between the release of this one and the xr34cka.
hardware,3ckgw6,Deadband420,2,Wed Jul 8 17:21:30 2015 UTC,"When the info first appeared on them the Nvidia monitor was supposed to be out a month later in August IIRC, so they're probably still on track for that."
hardware,3ckgw6,adaminc,1 point,Wed Jul 8 18:10:35 2015 UTC,We'll probably see reviews for the cka version next month and see it go on sale late August or early September depending on where you live.
hardware,3ckgw6,GeckIRE,2,Wed Jul 8 18:23:40 2015 UTC,"No AdobeRGB support, boourns!"
hardware,3ckgw6,LazyGit,1 point,Wed Jul 8 20:24:19 2015 UTC,Absolutely love this monitor but the price of it is quite high :/
hardware,3ckgw6,Stiev0Kniev0,1 point,Wed Jul 8 17:25:55 2015 UTC,"No transitions reached down as low as the advertised 4ms G2G, but that's not really a surprise.   Seems pretty scandalous to me even if it isn't surprising."
hardware,3ckgw6,SummitTechEnterprise,1 point,Thu Jul 9 12:03:48 2015 UTC,I really wish this monitor was 90-100hz refresh rate. I wonder how well it overclocks.
hardware,3ckgw6,SummitTechEnterprise,1 point,Thu Jul 9 23:04:30 2015 UTC,Any idea when I can pick one of these up?
hardware,3cnat0,skilliard4,9,Thu Jul 9 06:01:47 2015 UTC,Talk about a hyperbolic title...  All this is is IBM partnering with a research college to create chips on the next node after 10nm. Namely they shrunk the process down to 7nm.
hardware,3cnat0,Seclorum,1 point,Thu Jul 9 18:46:16 2015 UTC,Hahah so if they are 1 percent more powerful than the leading machine THEY ARE THE MOST POWERFUL CHIP THAT HAS EVER BEEN CONCIEVED MUAHA AHAHAHAHAHAH
hardware,3cnat0,jdw101,1 point,Sat Jul 18 03:35:41 2015 UTC,Is this going to be for regular consumers or is this going to be marked up in price for Industrys?
hardware,3cnat0,seducingapotato,7,Thu Jul 9 06:17:34 2015 UTC,"This is pathfinding research done with electron beam lithography. It's practically impossible to mass-produce chips this way, or even make a single chip of the size modern CPUs are, but it allows you to ""look ahead"" of the current manufacturing processes and have an idea of what the new challenges for the next few nodes will be.  IBM will sell the results of this researchs to it's partners, Samsung and GF, which will someday use them for development of their new processes."
hardware,3cnat0,Tuna-Fish2,5,Thu Jul 9 10:29:40 2015 UTC,"This is all I could find that is even slightly relevant:   IBM also declined to speculate on when it might begin commercial manufacturing of this technology generation. This year, Taiwan Semiconductor Manufacturing Company said that it planned to begin pilot product of seven-nanometer chips in 2017. Unlike IBM, however, it has not demonstrated working chips to meet that goal.   Obviously ""commercial"" could just mean the server market, but perhaps there could be consumer products. Considering it uses a new material(silicon-germanium) instead of pure silicon, it may be initially expensive as it's a new manufacturing process."
hardware,3cnat0,Exist50,1 point,Thu Jul 9 06:23:09 2015 UTC,"IBM is pretty much just focused on the sever market, and they recently paid GlobalFoundries to take their fab business."
hardware,3cikro,Rasangone,8,Wed Jul 8 04:45:38 2015 UTC,"Good on them. Great for Zotac owners and for future buyers, 5 years is longer than the industry standard (3 years) and at the end of the warranty is about the right time to buy a new GPU. Good move."
hardware,3cikro,Csalbertcs,7,Wed Jul 8 04:55:47 2015 UTC,After 3 dead Zotac products... Never again...  They did rma all of them though.
hardware,3cikro,digitalcriminal,4,Wed Jul 8 14:45:21 2015 UTC,"I bought one Zotac product several years ago, a Mini-ITX HTPC motherboard.  Never again.  That thing felt so unbelievably cheap.  I thought the ports and RAM slots were going to break off when I was building it.    No other way to put it other than, it felt disposable."
hardware,3cikro,SocksForBreakfast,10,Wed Jul 8 23:55:27 2015 UTC,For >$200 cards only.
hardware,3cikro,Exist50,2,Wed Jul 8 06:45:37 2015 UTC,What determines if it's a >$200 card? MSRP? The price you pay at the store?(before or after rebate?)
hardware,3cikro,skilliard4,4,Wed Jul 8 18:43:50 2015 UTC,MSRP
hardware,3cikro,Exist50,15,Wed Jul 8 19:07:27 2015 UTC,For anyone reading this XFX has had a similar policy in place for years. So Zotac for lifetime warrantied nVidia products and XFX for AMD products ;)
hardware,3cikro,PeaInAPod,22,Wed Jul 8 05:13:32 2015 UTC,XFX just ended there's :(
hardware,3cikro,Exist50,9,Wed Jul 8 06:44:22 2015 UTC,Why would people buy them anymore now. The lifetime warranty was their main attraction seeing as the DD cooler is a mediocre cooler.
hardware,3cikro,lead12destroy,10,Wed Jul 8 13:47:48 2015 UTC,They will gain more money by not honoring lifetime warranties than they did from extra sales due to the lifetime warranty.  In short -- people abused it.
hardware,3cikro,TaintedSquirrel,7,Wed Jul 8 14:32:12 2015 UTC,not sure why downvoted. but there is a reason why xfx did this
hardware,3cikro,NLWoody,-2,Wed Jul 8 18:07:35 2015 UTC,"Because that's no longer possible, they had issues because people used AMD cards to mine like crazy. Now they're just gonna suffer brand image loss as well as sales & still take the hit from the previous warranties. In short, dumb business decision."
hardware,3cikro,Trollatopoulous,2,Wed Jul 8 18:55:23 2015 UTC,You have to start somewhere. What should they have done? Continue to >take the hit from previous warranties?
hardware,3cikro,TheImmortalLS,1 point,Thu Jul 9 01:06:11 2015 UTC,"They will have to take the hit from the previous warranties, that's the point, there's no way to escape that contract. The mining craze was an anomaly, it won't ever happen again. So putting safe-guards in place that protect against that but that also damage your brand and your product's value is simply bad business sense. In short, they shouldn't have done anything, because there was nothing they could've done against miners at that point & there's no benefit to cutting the LI now."
hardware,3cikro,Trollatopoulous,1 point,Thu Jul 9 11:45:25 2015 UTC,The AMD 200 series also had a somewhat high failure rate.
hardware,3cikro,makar1,1 point,Thu Jul 9 07:09:31 2015 UTC,The XFX Lifetime warranty still applies to their Double Dissipation cards and cards ending in R.
hardware,3cikro,PeaInAPod,5,Wed Jul 8 14:32:38 2015 UTC,All of the 300-series and Fury X models are 2 years only.  They effectively ended their lifetime warranty program starting with these new cards.
hardware,3cikro,TaintedSquirrel,-2,Wed Jul 8 14:37:51 2015 UTC,They haven't even released their cards yet. A XFX R9 390 Double Dissipation model was leaked recently and unless XFX issues a press statement to the contrary their is no reason to believe it won't carry a lifetime warranty.
hardware,3cikro,PeaInAPod,3,Wed Jul 8 14:46:32 2015 UTC,The cards launched like 2 weeks ago.  And their support rep already confirmed the lifetime warranty is gone.  The only ones that carry it are the Best Buy editions.
hardware,3cikro,TaintedSquirrel,0,Wed Jul 8 14:51:37 2015 UTC,lol fucking Newegg. I got to their site and search XFX R9 390 and nothing comes up so I assume they haven't released them yet which I felt pretty sure about since XFX was also late releasing their R9 290 cards.
hardware,3cikro,PeaInAPod,2,Wed Jul 8 14:56:30 2015 UTC,We're 3 weeks past 6/15 and 4 weeks past the 390 best buy leak
hardware,3cikro,TheImmortalLS,-1,Thu Jul 9 01:07:06 2015 UTC,Some people have more important things to follow than leaks from best buy and release dates for new products.
hardware,3cikro,PeaInAPod,2,Thu Jul 9 04:45:29 2015 UTC,"PNY still offers lifetime warranties too if you register the card. Before someone jumps on me and says it's a card life cycle warranty, they changed that years ago and it is, in fact, a lifetime warranty now."
hardware,3cikro,Nixflyn,1 point,Thu Jul 9 07:52:48 2015 UTC,Good to know. I'll consider them more seriously in the future.
hardware,3cikro,JarJarBanksy,1 point,Wed Jul 8 06:02:20 2015 UTC,Worldwide warranty for Zotac Nvidia cards and XFX AMD cards? Amazon here I come!
hardware,3cikro,adr007,7,Wed Jul 8 06:05:13 2015 UTC,"Unfortunately the XFX thing no longer applies, and the Zotac one is for >$200 cards."
hardware,3cikro,Exist50,0,Wed Jul 8 06:50:33 2015 UTC,The XFX Lifetime warranty still applies to their Double Dissipation cards and cards ending in R.
hardware,3cikro,PeaInAPod,-1,Wed Jul 8 14:31:26 2015 UTC,The XFX Lifetime warranty still applies to their Double Dissipation cards and card models ending in R.
hardware,3cikro,PeaInAPod,1 point,Wed Jul 8 14:39:15 2015 UTC,NA or Worldwide?
hardware,3cikro,adr007,1 point,Thu Jul 9 01:35:12 2015 UTC,"This is great, but I bought a zotac 960 3 or so months ago :/"
hardware,3cikro,NoozeHurley,1 point,Wed Jul 8 16:38:35 2015 UTC,Why do i have to register for this? Is this extra warranty still valid for used cards that may not be registered on my name?
hardware,3cikro,Klorel,1 point,Wed Jul 8 07:32:01 2015 UTC,"You have to register most graphics cards, and some have limited warranties if you don't register it within a month of purchase"
hardware,3cikro,Maysock,0,Wed Jul 8 10:09:53 2015 UTC,Are their cards any good? The reviews for the Zotac 970 say it's hot and loud.
hardware,3cikro,adhoc92,1 point,Thu Jul 9 02:14:32 2015 UTC,"My 970 is neither, but I've had several that had coil whine issues. My current one doesn't seem to have it anymore, now that I've broken it in."
hardware,3cikro,IncoherentOrange,-6,Thu Jul 9 21:08:45 2015 UTC,"Well, the warranty is void after you remove the cooler... so idk how useful this actually is.   After a few years the thermal paste will be worn out, and you'll have to void it to replace it."
hardware,3cikro,ReallyObvious,4,Wed Jul 8 06:23:27 2015 UTC,"or  Or  OR, you can just use the warranty they provide for another card."
hardware,3cikro,animeman59,2,Wed Jul 8 06:36:56 2015 UTC,They won't send a brand new card just for having dry paste   I'd be shocked if they did
hardware,3cikro,ReallyObvious,5,Wed Jul 8 07:19:31 2015 UTC,except when it kills itself for dry paste as I assume the temps go through the roof (or why would you change it?).
hardware,3cikro,n0Skillz,5,Wed Jul 8 09:39:17 2015 UTC,"Naw, it'll just throttle itself and run at low clockspeeds. Annoyingly low fps. It was happening to me before I swapped out the paste on one of my ancient cards."
hardware,3cikro,ReallyObvious,5,Wed Jul 8 09:41:50 2015 UTC,So I guess the question is: Will they do warranty claims on shit performance when that happens?
hardware,3cikro,n0Skillz,2,Wed Jul 8 09:52:48 2015 UTC,"That's a good question. They could plug it into a workstation, see it boot, then determine ""it booted just fine and displays an image without artifacts, it must be another part of your computer"", and send it back."
hardware,3cikro,skilliard4,1 point,Wed Jul 8 18:45:30 2015 UTC,Below factory stock speeds is a valid reason. Your overclock dropping from 1300 to only 1200 won't work though.
hardware,3cikro,TheImmortalLS,1 point,Thu Jul 9 01:08:47 2015 UTC,I think 98% don't change the cooler anyway. So this is a pretty good thing.
hardware,3cikro,Klorel,1 point,Wed Jul 8 07:33:44 2015 UTC,"Well, the warranty is void after you remove the cooler... so idk how useful this actually is.    That's like those little tags on your mattress that says ""DO NOT REMOVE"".   Yeah you're not supposed to remove the heatsink but nobody cares.  As long as you don't physically damage the card, they won't notice or care."
hardware,3cikro,TaintedSquirrel,2,Wed Jul 8 14:34:19 2015 UTC,There might be some sort of seal that breaks if you remove it or some other form of way to tell. I'm not sure if they do it but it wouldn't be too difficult or expensive to implement.
hardware,3cikro,skilliard4,1 point,Wed Jul 8 18:53:06 2015 UTC,That would be the warranty stickers usually placed on heat sink screws. My Zotac 980Ti doesn't have any.
hardware,3cil6y,dragontamer5788,7,Wed Jul 8 04:50:23 2015 UTC,"Very useful thanks, I'm excited as mine is arriving today.  Shame about the lack of DisplayPort cable though, as it seems to be surprisingly hard to find a quality cable in the UK (one that is VESA compliant)."
hardware,3cil6y,ICThat,7,Wed Jul 8 05:27:43 2015 UTC,"I didn't know input lag differs on different points on the screen, wouldn't this produce tearing? Or does the monitor use the slowest one all the time?"
hardware,3cil6y,AndreyATGB,5,Wed Jul 8 08:31:19 2015 UTC,"Lag differs on different parts of the screen because most screens are drawn from top-left to bottom right. Even a CRT will have more ""lag"" on the bottom than on the top of the screen.  After all, the cathode-ray-tube takes time to move the electron gun as it scans across the monitor. And CRTs are the fastest monitors, with what is considered 0 nominal lag. (But this particular tool will measure ~8.33ms of lag on a CRT at the mid-point, and ~16.66ms of lag at the bottom)  In movies where a CRT is filmed, you will notice that the TV flashes, because the camera can see the electron gun travel across the screen. The human brain however interprets the image and makes it whole again, so its all good."
hardware,3cil6y,AndreyATGB,3,Wed Jul 8 11:56:24 2015 UTC,I take it that's what low persistence/ULMB/global refresh deal with then?
hardware,3cil6y,bb999,3,Wed Jul 8 13:12:22 2015 UTC,"Those technologies try to emulate the CRT effect to handle a different problem: blur and response time. IPS screens have marginally worse response times than TN panels (4ms vs 1ms), and people can perceive that as blurriness.  Tearing is a totally different issue.  Display Lag is also a totally different issue. All serious gamers practically require a monitor with less than 20ms of lag (measured from the center). What I tested here is purely a display-lag issue."
hardware,3cil6y,dhds83,3,Wed Jul 8 13:42:11 2015 UTC,"IPS screens have marginally worse response times than TN panels (4ms vs 1ms), and people can perceive that as blurriness.   A fast response time, while necessary for motion blur reducing techniques, is not the cause of motion blur inherent to LCD monitors. The reason why LCDs look blurrier than CRTs has to do with the sample-and-hold effect. This link explains it pretty well: http://www.blurbusters.com/faq/oled-motion-blur/"
hardware,3cil6y,bphase,1 point,Wed Jul 8 15:58:55 2015 UTC,"Low persistence means the pixels are only lit and visible for a small fraction of a frame.  This can dramatically reduce the appearance of motion blur, but it has no effect on response time.  ULMB is an implementation of low persistence by Nvidia (if memory serves) that you'll commonly get access to on gsync monitors if you don't mind running them at a constant frame rate.  Global refresh means that all pixels are changed and lit simultaneously.  What u/dragontamer5788 is describing with the top left of the screen having a dramatically lower response time than the bottom right is what this deals with, but global refresh may or may not have a net positive effect on latency.  The current scan-out method, pixel by pixel, then row by row, is a holdover from the physical requirements of CRTs.  LCDs don't have any physical need to operate this way; they have just been run like CRTs for years in large part because of inertia.  Global refresh is orthogonal to latency and mostly unrelated to low persistence, except insofar as it may be slightly easier to do backlight-flash low persistence at high frame rates on a global refresh panel."
hardware,3cil6y,lild3an,1 point,Wed Jul 8 18:30:44 2015 UTC,Surely those values are halved on a 120Hz monitor though? And it sounds like this 29UM67 is 60Hz which means it can't be that fast compared to the 120/144Hz monitors.
hardware,3cil6y,lild3an,2,Wed Jul 8 13:53:06 2015 UTC,"Interesting thought experiment.  But completely inapplicable in my case unfortunately. The Lag Tester operates at 60Hz. The data for the bottom row of pixels simply isn't ""at the monitor"" until around 16ms into the frame.  The monitor tested in this topic has a FreeSync range up to 75Hz. But until someone makes a lag tester at 120 or 144Hz, the above results are only a few ms away from absolutely flawless."
hardware,3cil6y,bizude,3,Wed Jul 8 14:05:11 2015 UTC,"Please test freesync when you get a chance, would like to know if it causes any measurable lag in current implementations."
hardware,3cil6y,Stephenishere,2,Wed Jul 8 06:34:35 2015 UTC,"Unfortunately, the lag tester is HDMI only. I will not be able to test the lag with a displayport cable. Converters from HDMI-output to DP-input exist, but probably will add latency since it is an active operation. (DP to HDMI is passive and might not add any latency... but that's the ""wrong direction"")"
hardware,3cil6y,Stephenishere,1 point,Wed Jul 8 11:56:51 2015 UTC,"My luck, thanks anyways."
hardware,3cil6y,jingleberry512,2,Thu Jul 9 02:16:18 2015 UTC,You should post this in /r/Monitors
hardware,3cil6y,ringonewell,2,Wed Jul 8 14:46:17 2015 UTC,I'll make a cross-post.
hardware,3cil6y,ltbh,1 point,Wed Jul 8 15:30:25 2015 UTC,How much did you pay?
hardware,3cil6y,ltbh,3,Wed Jul 8 05:53:45 2015 UTC,"$342 USD from Newegg. Amazon has it cheaper though right now.  Obviously, prices fluctuate. But Amazon's current price is $328.14 USD with Free Shipping."
hardware,3cil6y,ltbh,2,Wed Jul 8 12:31:59 2015 UTC,Not bad.
hardware,3cil6y,logged_n_2_say,1 point,Wed Jul 8 14:09:33 2015 UTC,"I just paid £220 for the non free sync version, so I'm guessing around £260 or something"
hardware,3ch7ch,jman583,41,Tue Jul 7 21:54:00 2015 UTC,"Damn - their stock TANKED today (down over 15%). Pull through red team - love em' or hate em', competition is good."
hardware,3ch7ch,thejshep,-46,Tue Jul 7 23:26:18 2015 UTC,Team green 4 lief
hardware,3ch7ch,TruckChuck,-10,Wed Jul 8 04:32:36 2015 UTC,I hope a truck hits you
hardware,3ch7ch,internet_man_415,-20,Wed Jul 8 05:26:09 2015 UTC,I'd ragdoll like glorious nvidia physx.  Hope you don't watch the video of me dying on an amd card. They can't do physx lolol
hardware,3ch7ch,TruckChuck,38,Wed Jul 8 05:32:20 2015 UTC,It's like I'm on /r/pcmasterrace or something
hardware,3ch7ch,58592825866,-31,Wed Jul 8 05:38:00 2015 UTC,Nvidia 980ti master race reporting in.
hardware,3ch7ch,TruckChuck,-17,Wed Jul 8 05:57:31 2015 UTC,"960x2 SLI, it's sort of like a 980."
hardware,3ch7ch,M0b1us0ne,5,Wed Jul 8 06:11:53 2015 UTC,Really? The scaling must not be too good.
hardware,3ch7ch,Exist50,-4,Wed Jul 8 06:46:24 2015 UTC,It actually is pretty close in FPS to a 980ti under 4k/1440p from what benchmarks I've seen.  It has hiccoughs in some games with low minimum fps at high resolutions but overall it benchmarks pretty close.  EVGA GTX960 SSC.  I switched from a single AMD 7950 when it started black-screening a little too often.  I've been running Warframe at 1440p with the oversampling thing it does at 1:1.3x resolution and it averages mostly 60 with vsync and rarely drops.
hardware,3ch7ch,M0b1us0ne,-22,Wed Jul 8 07:50:08 2015 UTC,It's not filthy AMD. For that you should be commended.
hardware,3ch7ch,TruckChuck,14,Wed Jul 8 06:14:34 2015 UTC,Can you please find some other subreddit to troll? Surely one must exist.
hardware,3ch7ch,Exist50,1 point,Wed Jul 8 06:49:10 2015 UTC,"Hey, I've got an AMD CPU and will likely be buying AMD for my next card (FuryX too pretty), I just wanted these because I play warframe quite a bit (read most of the time) and the physx in it is quite pretty."
hardware,3ch7ch,M0b1us0ne,3,Wed Jul 8 07:52:04 2015 UTC,Get a load of this guy
hardware,3ch7ch,stevecosupply,0,Wed Jul 8 05:43:50 2015 UTC,"I'd ragdoll like glorious nvidia physx.  Hope you don't watch the video of me dying on an amd card. They can't do physx lolol   I actually would want to see you dying on an amd card. For one thing, ragdoll run on the cpu.   Those gpu eyecandy gets in the way of you falling silly and clipping the ground."
hardware,3ch7ch,idoithere,1 point,Wed Jul 8 06:51:45 2015 UTC,Just like Skyrim!
hardware,3ch7ch,wagon153,15,Wed Jul 8 20:44:03 2015 UTC,Don't die AMD. I love you (Q_Q)
hardware,3ch7ch,spoonsandswords,-26,Wed Jul 8 04:01:30 2015 UTC,Shh.  Let it happen.
hardware,3ch7ch,TruckChuck,0,Wed Jul 8 05:58:56 2015 UTC,I hope a truck hits you. /s
hardware,3ch7ch,Chilli_Axe,6,Wed Jul 8 14:16:46 2015 UTC,"Two things I'd like to see AMD do:   Chromebooks/boxes! I've owned several and enjoy them thoroughly. The Intel offerings in these are good but the graphics performance has left me with a lot to be desired when AMD's A4 APU offerings could compete quite well with what their putting in the low end, Intel powered devices. (I guess we could also throw the sub $250, 11-12"" Windows laptop market in here also) Integrated solutions. The J18/19/29/30/3700 Intel solutions are good but, again, the graphics leave a lot to be desired and AMD really doesn't have the market penetration that Intel has here. This is by no means a large market and there are a few offerings in A4 and A6 mobile solutions but not with the availability and price range that Intel does. They're almost all, 2013 jaguar-based A4-5000's. I don't think this is necessarily an issue of Intel's clout and resources but a lack of... well, trying. That's conjecture of course but it really seems that way to me. I'd like to be able to buy a $50-75 AMD A4 or A6 mobo/cpu with a sub-20w TDP and have a few choices in mobo brands. It's not like all that inventory sitting in warehouses is doing them any good!"
hardware,3ch7ch,zakats,3,Wed Jul 8 06:49:39 2015 UTC,Intel's contra-revenue will keep AMD out of the low end for now.
hardware,3ch7ch,Exist50,1 point,Wed Jul 8 07:47:17 2015 UTC,"Why would you worry about graphics performance on a Chromebook?  They can't run exe's and can't run more than browser games.  You can install Linux, but they are not made to do that and a company is not going to build a product for the purpose of doing something they don't intend."
hardware,3ch7ch,Thunder_Bastard,1 point,Wed Jul 8 17:31:28 2015 UTC,"Sure, I can see how you'd have that perspective but it's my opinion that using the Chromebook as a chrome OS device would still benefit greatly from better graphics performance.   An extreme example of this is full HD video playback being a bit less reliable than I'd like. It's fairly decent but there's room for improvement to be sure. I want smooth 1080 @ 60fps"
hardware,3ch7ch,zakats,26,Wed Jul 8 20:09:59 2015 UTC,"Eh, the blame is placed in the wrong place at least from my perspective.  I used to buy solely AMD/ATI CPU's and GPU's but have since crossed over due to AMD completely abandoning the enthusiast CPU market, and the GPU situation has been a hot mess between power consumption, heat, and driver issues.  Thinking about this has also led me to remember how much Abit motherboards used to kick ass and how disappointed I was when they exited the market."
hardware,3ch7ch,snowlovesnow,4,Tue Jul 7 22:20:38 2015 UTC,Abit motherboards were such shit when it came to reliability in the end.
hardware,3ch7ch,Cozmo85,5,Wed Jul 8 01:45:51 2015 UTC,"A symptom of what caused the company to fold: Embezzlement and inflated accounting, and their leading engineer defecting to DFI to lead their LanParty range.  The last motherboard I cared to purchase from them was the NF7-S around 2003.  Best motherboard of that time."
hardware,3ch7ch,snowlovesnow,3,Wed Jul 8 01:53:22 2015 UTC,Haha and then DFI following suit.
hardware,3ch7ch,theinfiniti,2,Wed Jul 8 05:54:10 2015 UTC,"I still remember how jelly I was, cuz I bought the A7N8X instead (rev1.06). Of course later on, a BIOS update allowed me to easily push 200FSB, but latencies were relaxed to the point that my 192Mhz FSB results were better pre-BIOS update :P  Consequently my last AMD board: DFI NF4 Ultra-D.  First Intel board: DFI LT P35.  [Incredibles]Coincidence? I think not :p[/Incredibles]"
hardware,3ch7ch,terp02andrew,1 point,Wed Jul 8 14:26:15 2015 UTC,"i crossed over once other OS support like linux and even windows was non existent for legacy cards. even Nivdia is still cranking out or was cranking out software updates for for even the 8800.  I found out out just how exclusive hackintosh and linux us with nvidia hardware since there are drivers proprietary drivers. open source drivers cost a a ton performance since they lack the ""secret sauce"" the proprietary drivers have.  also my still very usable 1gb HD4850s and HD2600s throw up all sorts of errors with a clean install of windows 7 and the latest legacy drivers from AMD.  my dual 4850s can play bioshock infinite at medium settings at 35-45 fps at 1050p."
hardware,3ch7ch,cdoublejj,13,Tue Jul 7 23:25:35 2015 UTC,"Weird, my 4870 works just fine after a clean install."
hardware,3ch7ch,BolognaTugboat,7,Tue Jul 7 23:43:54 2015 UTC,same. 4870 that went from Vista - > win7 -> win7 enterprise -> win 8. Don't recall having any driver issues.
hardware,3ch7ch,weez09,0,Wed Jul 8 01:34:14 2015 UTC,"kbdsync error, the forums are full of complaints and fixes that don't work for me. for both my 4850s and hd2600xt. also open cl is non existent and does not function for both card series either. gpu-z craps out with an error stating open cl is missing."
hardware,3ch7ch,cdoublejj,1 point,Wed Jul 8 00:10:27 2015 UTC,AMD abandoned the enthusiast and the high end CPU market. They don't compete with i7s nor with i5s. If at least they competed with i5s they wouldn't be in the bad situation they are now.
hardware,3ch7ch,Yearlaren,14,Wed Jul 8 15:11:47 2015 UTC,This sort of scares me. Its getting lower and lower. What if I buy an r9 290 and the drivers are discontinued or something.
hardware,3ch7ch,norraptor,18,Wed Jul 8 02:20:48 2015 UTC,"They aren't going out of business anytime soon, and realistically Intel and/or Nvidia would probably not let that happen - it would bring up anti-trust issues for them."
hardware,3ch7ch,Meanest_Phlebotomist,10,Wed Jul 8 02:39:20 2015 UTC,If their share price drops low enough they could be at risk for getting delisted from the NASDAQ. I wouldn't be surprised if we saw a 1:10 reverse split in the near future.
hardware,3ch7ch,Logical_Phallusy,3,Wed Jul 8 04:20:21 2015 UTC,"not only that but there are other companies who would love to get their hands on amd's patents, and most of them would not want to play nearly as nicely with intel and nvidia as AMD does."
hardware,3ch7ch,runningfromlions,3,Wed Jul 8 11:46:40 2015 UTC,"If you're referring to the IP  AMD cross-licenses with Intel (the ""x86 license"" in particular), those have some mighty fine print.  They're poison-pilled, if an entity acquires a controlling stake (or even a large minority stake) in AMD, or if they enter into certain types of close business relationships, their license goes bye-bye.  In the past Intel has threatened them just for working with GloFo.  Their patent portfolio is probably pretty decent too, but even with them it would not be easy for most companies to jump into competition with Intel or NVIDIA.  Both of those companies have a dominant position in their markets."
hardware,3ch7ch,capn_hector,7,Thu Jul 9 00:14:59 2015 UTC,"for linux, the r9 290 community drivers are nice. mac os x has kexts for it you can find online. the main problem would be driver optimizations for AAA games in addition to crossfire profiles."
hardware,3ch7ch,TheImmortalLS,1 point,Wed Jul 8 03:32:04 2015 UTC,"luckily the only tripple A titles I think im waiting for are dark souls 3, need for speed, and the new doom."
hardware,3ch7ch,norraptor,0,Wed Jul 8 16:51:21 2015 UTC,"If AMD went under, both Intel and Nvidia would get broken up for holding monopolies."
hardware,3ch7ch,AR-47,-21,Wed Jul 8 17:15:32 2015 UTC,Don't buy AMD. Simple.
hardware,3ch7ch,TruckChuck,1 point,Wed Jul 8 06:01:42 2015 UTC,but.. but.. amd is team red. its like saying don't push the little red button. sometimes you have to.
hardware,3ch7ch,norraptor,-7,Wed Jul 8 16:51:45 2015 UTC,what if no drivers   Well...
hardware,3ch7ch,BlayneTX,4,Wed Jul 8 07:27:15 2015 UTC,AMD's APU battery life is horrendous. It's completely unusable.
hardware,3ch7ch,Monkeyfeng,7,Wed Jul 8 03:40:34 2015 UTC,"That's mostly due to high idle power consumption, which was a huge focus of Carrizo: http://cdn3.wccftech.com/wp-content/uploads/2015/02/AMD-Carrizo-APU_Low-Power-Standby-State.jpg  That and power consumption while streaming/video watching."
hardware,3ch7ch,Exist50,3,Wed Jul 8 04:31:02 2015 UTC,I love the idea of APU as a low cost entry into PC gaming but I struggle to actually recommend one.  In the desktop market it is hard to recommend an APU to a prospective new gamer because the FM2 socket is a major barrier to upgradeability. You can add an external graphics card but you will still be bottlenecked by the weak range of CPUs available for FM2.  In the laptop market it is hard to recommend an APU because laptop APUs are graphically weak and not much better than Intel I3 offerings.  Another difficulty with APUs is that their gaming  performance is highly dependent on fast dual  channel RAM but potential customers for an APU don't understand this so system builders save money sticking a single sick of slow RAM in a budget desktop or laptop.
hardware,3ch7ch,Liambp,10,Wed Jul 8 06:44:14 2015 UTC,"I think Carrizo actually changes things for mobile, but I concur with your desktop analysis."
hardware,3ch7ch,Exist50,2,Wed Jul 8 06:48:06 2015 UTC,I haven't seen any release version benchmarks of Carrizo but I hope you are right and I hope it is a success for AMD. A huge number of people want to play games on their laptop these days  but are fooled into thinking that you need an outrageously expensive i7 with mediocre built in graphics to do it. If AMD produce a part that will allow laptop gamers to play the latest releases at moderate graphical quality without spending a fortune then I think they have a winner.
hardware,3ch7ch,Liambp,4,Wed Jul 8 08:02:35 2015 UTC,"Carrizo has Tonga's delta color compression, which should help a bit with bandwidth, but not too much: http://www.notebookcheck.net/fileadmin/_processed_/csm_carrizo_gcn_color_compression_3840d469ba.jpg  Unfortunately, Carrizo seems like a good product, but one that's still failing to sell. I'm not sure whether to blame the OEMs, marketing, or something else.   I've seen a bit of Carrizo data, but I'm not sure how much of it's from a 15w vs 35w SKU. Also, I have yet to see a full writeup by any of the major tech sites."
hardware,3ch7ch,Exist50,2,Wed Jul 8 08:10:57 2015 UTC,"Edit: I assume Carizzo will also need good memory so AMD should probably look at ways to discourage single sick of RAM laptops. Perhaps an ""AMD approved"" sticker or something similar for those who pair Carizzo with appropriate RAM."
hardware,3ch7ch,Liambp,3,Wed Jul 8 08:05:55 2015 UTC,I think the best thing for the PC market would be if Samsung or a similar company bought AMD. They don't have enough money to spend on R&D now so  they're just getting further and further behind and won't be able to stay competitive. Only a huge company would be able to restore competition to the market.
hardware,3ch7ch,Noremac28-1,7,Wed Jul 8 09:58:06 2015 UTC,I hope to see AMD APUs on the next gen Windows 10 budget laptops.  Maybe then things will pick up.
hardware,3ch7ch,BolognaTugboat,6,Tue Jul 7 23:45:18 2015 UTC,"The problem is, broadwell or skylake chips have nearly as good of performance in graphics, way better performance in applications, and way better power and thermal profiles.   The AMD APU concept really already exists with Intel's IGPU's."
hardware,3ch7ch,Seclorum,14,Wed Jul 8 02:22:13 2015 UTC,"You really have to pay to get the extra graphics performance, though, and Carrizo is pretty efficient, all things considered. They focused on low power particularly for video and idle, two main battery drains for a laptop. IMO, the CPU performance difference is not noticeable to most users."
hardware,3ch7ch,Exist50,6,Wed Jul 8 02:34:51 2015 UTC,"Oh certainly. But so much else in a laptop is being cut down or shrunk.   People respond more to Intel's offerings because they see they get great performance, and still very good battery life, and very good graphics performance.   It wouldn't surprise me if come cannonlake, Intel just says, 'Fuck it' and throws everything on die to make a full SOC for a laptop.   Then all you need is a big battery space."
hardware,3ch7ch,Seclorum,6,Wed Jul 8 02:51:15 2015 UTC,I can't say I know a single person (myself notwithstanding) that would check benchmarks for a laptop before buying.
hardware,3ch7ch,Exist50,2,Wed Jul 8 02:58:29 2015 UTC,Pretty much everyone I know does.   Too many have been burned by a flashy laptop that just cant perform and they spent way too much on something that doesn't do what they wanted it to do.
hardware,3ch7ch,Seclorum,1 point,Wed Jul 8 03:02:36 2015 UTC,"It seems most people's problems can be resolved with more RAM and an SSD. That usually suffices unless the laptop is used for more strenuous activities than the usual Office suite.   Btw, you probably know, but Carrizo is a SOC."
hardware,3ch7ch,Exist50,0,Wed Jul 8 03:09:53 2015 UTC,Yeah. Everyone is pretty much going that direction.   But for a full SOC I'm thinking more like they also have HBM like ram stacks under the heat spreader's or right on the interposer substrate. Then you dont need space for seperate Ram modules either.
hardware,3ch7ch,Seclorum,2,Wed Jul 8 03:35:24 2015 UTC,"AMD's basically committed to HBM in APUs, but starting with a custom server/compute one first. I think costs will need to be brought down a lot before we get mainstream chips with HBM as system RAM."
hardware,3ch7ch,Exist50,0,Wed Jul 8 03:56:09 2015 UTC,"In a lot of ways having some onboard makes sense. Reduces trace length and if they tune the layers right they are just as fast as regular ram dimms.   But I dont see people at home using them, I see system integrators and manufacturers using them in their products. Hell, tablets are nearly there.   Imagine a tablet/transformer that the keyboard section is just an extra huge battery pack for the main tablet unit."
hardware,3ch7ch,Seclorum,1 point,Wed Jul 8 04:03:57 2015 UTC,every computer that a family member is looking to purchase since I bought an atom netbook. Never again.
hardware,3ch7ch,PaulTheMerc,1 point,Wed Jul 8 05:25:36 2015 UTC,"an atom netbook   Considering you're using the term netbook to describe it, I'm getting a very bad feeling about that that atom was."
hardware,3ch7ch,Exist50,3,Wed Jul 8 05:26:42 2015 UTC,"Huh?  Carrizo is going to be pretty low watt have low temps so I don't see how Broadwell could be considered ""way better.""  And since we're talking about budget laptops (Celeron) I don't expect performance to be that big of a factor.  Only time will tell but I expect, for the price, the budget laptops with Carrizo are going to perform much better with graphics intensive jobs like gaming."
hardware,3ch7ch,BolognaTugboat,1 point,Wed Jul 8 03:04:52 2015 UTC,"The only thing I see that Broadwell lacks is hardware HVEC decode.   It's IGPU uses less than a single watt of power when the system is idle, while the processor itself uses just 5 watts total idle.  Even graphics intensive games, like say GTA5 can be played on Broadwell IGPU's with good success.   So no, not even graphics intensive tasks does Carrizo strictly perform much better. They are very comparable offerings but most manufacturers will look at power usage in most situations and see the Intel as more attractive because of battery life."
hardware,3ch7ch,Seclorum,5,Wed Jul 8 03:44:33 2015 UTC,"AMD claims that the Carrizo system manages a decrease from 4.5 to just 2.7 Watts compared to the Kaveri reference design, so Carrizo should be able to close the gap to Broadwell systems.   Even if it doesn't actually use 2.7w idle, 4.5w alone is great.  Yes, Broadwell CPUs can play GTA5 very well (at low settings) -- like the i5-5675c and i7-5775c, but why even bring that up when we're talking about budget laptop cpus?   You realize how expensive those CPUs are?  Since there are no benchmarks for Carrizo yet we just have to assume they will be a little better than Kaveri which can also play GTV 5.   The A10-7870k does just fine at low settings and costs a hell of a lot less than the Broadwell CPUs I mentioned.   If you weigh this AMD CPU against Broadwell CPUs that cost even remotely the same price, it performs quite a bit better.  Judging on the performance per price that we're seeing right now, and considering Carrizo will be an improvement, then I expect AMD APUs to continue beating Intel at comparable price ranges.   There's no reason to assume it will be different.  And the few negatives there are currently will (supposedly) be removed by new generation using much less power and running cooler."
hardware,3ch7ch,BolognaTugboat,1 point,Wed Jul 8 04:31:52 2015 UTC,"Can't really compare desktop to mobile chips, though."
hardware,3ch7ch,Exist50,1 point,Wed Jul 8 04:35:53 2015 UTC,Since there are no benchmarks for Carrizo yet   They had them on anandtech the other month. On this very sub even.
hardware,3ch7ch,Seclorum,2,Wed Jul 8 04:45:22 2015 UTC,"http://www.anandtech.com/tag/carrizo  Where?  Sorry but there's no benchmarks out yet, just details on the architecture.    Edit:  It seems I was wrong.  I found this link on linustechtips.   I'm not sure how credible this site is but if it's accurate then wow, the iGPU is almost double the performance of a 7850k."
hardware,3ch7ch,BolognaTugboat,3,Wed Jul 8 05:20:38 2015 UTC,"But... That's already been done... Countless times   Low end APUs are genuinely so shit they can't even compare to a Z3735 found in modern tablets. Seriously. An A4-5000 doesn't compare in anything but low end gaming, while producing more heat. Intel is kicking ass with atoms and rebadged celeron and pebtium counterparts as they are able to make them cheaper and have much higher demand (due to the tablet and mobile market that AMD completely lost now).  I hate to say this but AMD is completely screwed. I used to believe. I still praise the days and glory of AM2 and X4 and X6 Athlons and Phenoms that beat out Core 2s, along with the price to performance ratios of the 4850/4870 cards and subsequent generations. But it's coming to a point where keeping competitive involves cutting barely existent profit margins. No new development, no power optimization, no improvements in performance other than higher clocks with better thermal solutions.  AMD can't keep rolling with what they have and hoping for the best because they're already years behind nvidia and Intel's technologies."
hardware,3ch7ch,theinfiniti,1 point,Wed Jul 8 06:04:00 2015 UTC,"Intel is kicking ass with atoms and rebadged celeron and pebtium counterparts as they are able to make them cheaper and have much higher demand    This isn't quite true. Intel looses over $1 billion a quarter on ""contra-revenue"" for its Atom chips. It's the only thing allowing them to compete in that low-cost area. AMD could never afford such a hit. And that a4 does compare decently against low end atom chips. They are quite similar in IPC, with AMD even winning by some benchmarks. This is where the process gap hurts the most.   AMD can't keep rolling with what they have and hoping for the best because they're already years behind nvidia and Intel's technologies.   For Intel, I would agree, but on the other hand, they're hitting a process wall hard. Samsung/GloFo's 14nm isn't equal to Intel's, but it's much better than the current 28nm. As for Nvidia, I don't think there's much of a tech gap at all. Fiji's more or less closed the efficiency gap (or at least brought it back to reasonable levels), and AMD definitely has the lead in HBM. I suspect, should they live, AMD will have a temporary leg up next gen, and I can explain why if you want."
hardware,3ch7ch,Exist50,4,Wed Jul 8 06:59:16 2015 UTC,"In terms of atoms I meant that kicking ass as in popularity and integration. Due to the OEMs adopting them in their current manner and end users being satisfied with the performance, Intel can jack up the price for the next generation to generate profit easily. And due to their general continuing success on the stock market, and single product line of losses doesn't matter much. Point still remains, in the coming future AMD will have nearly no presence in the mobile/tablet market, regardless of the efficiency of their coming SoCs. And considering its still a quickly growing market, while PC and console sales are diminishing, AMD can't rely on those solely as sources of revenue.  As for Fiji, fair enough, I haven't read far enough to be aware of the performance (and a few reviewers that were supposed to present the numbers got DOA cards). If you could explain that would be great."
hardware,3ch7ch,theinfiniti,1 point,Wed Jul 8 07:13:21 2015 UTC,"Well the only reason OEMs are using them is because Intel's eating a lot of the cost. The current breed of atom chips were designed for high end tablets, but that never really panned out. Intel will probably try to bring down costs without budging prices much until they match, but loses on that scale don't make shareholders happy.   Carrizo seems to have some mild design win successes, but given this post, clearly not enough. It's actually quite advantageous to OEMs as (like Jaguar) it is an SoC, unlike the pentiums/celerons/i3s it's competing against.   As for why I think AMD will have a leg up in 2016 (should current plans remain), it more has to do with how ambitious Nvidia is being. In 2016, Nvidia plans on having a new architecture, new fab process, and new memory tech. All three at once is very ambitious for one gen, and historically not gone well for them. Also, Nvidia needs to add back significant compute capabilities to replace the Kepler Tesla cards.   AMD, meanwhile, has HBM down already, and while it does have to make both an architectural change and a fab shift, AMD's in the relatively unique position of having priority at GlobalFoundries, which is already making 14nm finfet products. It's not the process AMD'll use, but it's better than what TSMC has. Nvidia will almost certainly go with TSMC again, but has to wait for 16nm FF+, and TSMC hasn't exactly been reliable of late.   Now, of course AMD has much less R&D money for graphics (supposedly the majority is being spent on Zen), but everything else seems to be putting them in a much easier position to execute.   Btw,   while PC and console sales are diminishing   I knew about PC sales, but I was at least under the impression that the consoles were doing fine. Anything suggest otherwise? AMD might even get a deal with Nintendo's NX system, but I'm remaining skeptical about it for now."
hardware,3ch7ch,Exist50,0,Wed Jul 8 07:41:28 2015 UTC,"Low end APUs are genuinely so shit they can't even compare to a Z3735 found in modern tablets.   100% lie, please prove me wrong.    Why are you even comparing tablet CPUs to AMD APUs?  Their APUs are not even used in tablets.   Atoms ARE getting much better but they still have a ways to go.  I've had a Venue 8 and found the CPU lacking for anything more than basic tasks.   On the other hand I was surprised by the performance of my A4-3400 temp desktop.   (due to the tablet and mobile market that AMD completely lost now).   Because they're not even trying to compete there.   Conversely, where is Intel in the console market?   I hate to say this but AMD is completely screwed. I used to believe. I still praise the days and glory of AM2 and X4 and X6 Athlons and Phenoms that beat out Core 2s, along with the price to performance ratios of the 4850/4870 cards and subsequent generations.    ... The hell, are you trying to say that AMD doesn't still have a better price to performance ratio than Intel?   The AMD CPUs perform great on newer games that actually utilize their cores AND they're cheaper AND they multitask better than Intel.  I can buy a FX-8320 for $136 right now on Amazon.  What does Intel offer?  A dual core i3?   The only thing Intel has is the very high end but fuck spending that much on a CPU.  I'd rather spend less on an 8 core, or even the 6 core, and buy the best GPU I can buy, where it will matter more.  Edit:  Btw, did you even read that source I linked?  Weird you didn't comment on that since if true it shows the Carrizo iGPU is going to absolutely shit on Intel iGPUs.    Which is what the actual discussion was about, upcoming AMD APU performance in low end laptops."
hardware,3ch7ch,BolognaTugboat,1 point,Wed Jul 8 18:53:12 2015 UTC,I'm going to make my sister get one.
hardware,3ch7ch,tedlasman,1 point,Tue Jul 7 23:53:18 2015 UTC,"ehh, for a laptop, a mobile intel + nvidia mobile may be better for power consumption and power. if you have a desktop, an amd apu is very viable in the budget range."
hardware,3ch7ch,TheImmortalLS,6,Wed Jul 8 03:31:00 2015 UTC,"If anything, it's the opposite. Carrizo is the best bet for good graphics at a relatively cheap price (if you can find a decent system). Adding a discrete GPU would increase both power consumption and costs. Meanwhile on the desktop, there's much more headroom in terms of power consumption, and the cards can be far, far better than integrated."
hardware,3ch7ch,Exist50,0,Wed Jul 8 04:33:55 2015 UTC,"How long have you been paying attention to AMD's laptop market availability?  Kaveri 35W/high end didn't surface at all until over eight months after the chip was released. They basically never made it to North America, and almost never had anything above a 768P machine.  Even long, long into Kaveri's lifespan, retailers were still full of stock with parts such as the A10-5750M, etc. AMD's Richland sat stagnant on the shell too often for too long.  It doesn't matter if the silicon is worthwhile if the predecessors sat on the shelf so long that few companies are willing to invest in anything beyond having a token AMD laptop in their line-up."
hardware,3ch7ch,LFKhael,6,Wed Jul 8 14:54:07 2015 UTC,"Just built my dad a computer with a 7700K. They're great price/performance, it's just that people don't buy them :("
hardware,3ch7ch,Sremylop,3,Wed Jul 8 02:20:21 2015 UTC,"Same here, A8-7600 for my mother's PC. It's a good, cheap and versatile chip for people who don't need to push pixels like crazy.   I guess it's that those people tend to buy prebuilt when they don't have someone in the family around to build them a PC. Or they only have a laptop. AMD barely exist in those markets."
hardware,3ch7ch,Rylock,1 point,Wed Jul 8 17:53:04 2015 UTC,And to some extent I don't even think they should be. People just aren't educated buyers :(
hardware,3ch7ch,Sremylop,2,Wed Jul 8 18:27:23 2015 UTC,One thing that also affected their result was their late decision to skip 20nm and go for finfet. That cost them extra 33 millions.
hardware,3ch7ch,Rentta,1 point,Wed Jul 8 08:26:47 2015 UTC,Love it....  AMD says it's revenues are falling and blames it on a general lack of sales in PC's and not their own shortcomings.   So what happens?  Intel stock tanks too.
hardware,3ch7ch,Thunder_Bastard,1 point,Wed Jul 8 17:26:37 2015 UTC,Maybe they should take that as a sign that APUs aren't the best use of their R&D time and money.
hardware,3ch7ch,iconic2125,0,Wed Jul 8 18:01:57 2015 UTC,So soon after a flop product launch too. They'll probably be sub 2 per share tomorrow. Nothing new really on the horizon that's better than what they currently have till next year.
hardware,3ch7ch,DHFearnot,7,Wed Jul 8 04:23:05 2015 UTC,"The Nano's something, but it'll be niche. The Fury will sell better, but doesn't appear to be a game-changer. This has nothing to do with Fiji's success or lack thereof, though."
hardware,3ch7ch,Exist50,1 point,Wed Jul 8 04:37:13 2015 UTC,You're right it has to with the multiple flops before this one.
hardware,3ch7ch,DHFearnot,2,Wed Jul 8 06:25:50 2015 UTC,"Not really anything in particular. The Bulldozer linage has already made its mark, and it's not like anything else of note has come out."
hardware,3cjal4,kaost,1 point,Wed Jul 8 10:21:27 2015 UTC,/r/Micro_Bit/ subreddit
hardware,3cg5nx,Helicase21,18,Tue Jul 7 17:31:46 2015 UTC,"Silverstone are still championing the mATX form factor. IIRC they announced three mATX cases based on the TJ08 at Computex this year. I love my TJ08-E - it's got excellent airflow and space to hold a ton of stuff including full sized GPUs and tall CPU coolers, all while being no bigger than the Phanteks Evolv ITX (which I would probably get if I ever changed cases right now)."
hardware,3cg5nx,dexter311,6,Tue Jul 7 21:29:34 2015 UTC,"I'll echo the support for the TJ-08.  Silverstone has some nice options as well as Phanteks.  I'm sure Lian Li has some stuff, but they will break the bank."
hardware,3cg5nx,noobas4urus,4,Wed Jul 8 01:31:52 2015 UTC,I also love my PS07B. Great build quality and the interior is roomy considering the compact nature.
hardware,3cg5nx,JonF1,2,Wed Jul 8 04:02:54 2015 UTC,"Yep, although cramped you can fit quite a lot in there: http://i.imgur.com/OvZK8v0.jpg  I've also added a 120mm intake into the top blowing onto the gfx card."
hardware,3cg5nx,sclnd,4,Wed Jul 8 14:34:19 2015 UTC,TJ08/PS07 recommendation seconded
hardware,3cg5nx,pet_the_puppy,1 point,Wed Jul 8 01:45:45 2015 UTC,no kidding I love my little Sugo SG10B  this is a gaming beast and tiny as fuq  still able to hold my 980ti with no issues  A lot of us aren't kids who need LED's and shit everywhere
hardware,3cg5nx,drogean3,32,Wed Jul 8 21:06:03 2015 UTC,"They don't seem to be as popular as they should. Anyone that prefers single GPU builds can do pretty much all they need with mATX.   My previous PC was a fridge-like full tower and took up an obscene amount of room for the number of components in there. Seems like only someone investing around $3K or more in a PC should bother with something that size.  The Phanteks Evolv mATX to me is a perfect case that allows for plenty of components (full size video card, 2 HDDs + 2 SSDs, most PSUs under 1KW) while being smaller than a mid-tower.  Anything truly compact is obviously better served with mITX."
hardware,3cg5nx,Rylock,11,Tue Jul 7 19:27:23 2015 UTC,"I still have an ATX midi-tower for two reasons:   Abundant space for experimental hardware (eg. RasPi), especially after all four 5.25"" slots are free. No burglar would steal something that looks like a piece of junk from the nineties."
hardware,3cg5nx,tin_dog,5,Tue Jul 7 20:45:35 2015 UTC,Curious about how and why you are mounting your raspberry pi in your ATX chassis?    Thanks
hardware,3cg5nx,xbillybobx,1 point,Wed Jul 8 02:51:43 2015 UTC,"How depends on what it's going to do. Some network stuff, or playback device maybe.  Why? Because science!"
hardware,3cg5nx,tin_dog,14,Wed Jul 8 18:48:52 2015 UTC,"I love ""sleeper"" cases. (Really just simple, functional designs) My friends all have weird tacky neon alienware knockoffs and i'm just sitting here with an R4. No windows, no leds, just the parts themselves in a black box.  I like to think any robber who sees it wouldn't bother, but I know for a fact if I had my old NZXT Apollo case it'd be a prime target, even if it had the old parts to match. :D"
hardware,3cg5nx,flyafar,7,Tue Jul 7 23:09:08 2015 UTC,"More like this one but more yellow-ish and full of ""funny"" stickers."
hardware,3cg5nx,tin_dog,4,Tue Jul 7 23:46:19 2015 UTC,I wouldn't mind one of those if the build quality/building convenience was up to par and I didn't have to worry about yellowing...
hardware,3cg5nx,flyafar,6,Wed Jul 8 00:05:26 2015 UTC,"I love older cases, once they start to look tacky on the outside they get character imo. The problem is most cheap garbage starts to develop rattles and fall apart internally prior to that point.  With modern processors producing so little heat you really don't need 6 fans like you did many years ago."
hardware,3cg5nx,Theodoros9,1 point,Wed Jul 8 03:44:26 2015 UTC,You may be able to un-yellow it.
hardware,3cg5nx,SayNoToAdwareFirefox,1 point,Wed Jul 8 18:19:00 2015 UTC,"Retr0bright:       Retr0bright is a chemical mixture used to remove yellowing from ABS plastic computer and electronics cases, including computers that were manufactured by Commodore and Apple in the 1980s and 1990s, and various video game consoles and cartridges.  Yellowing is caused by both bromine and exposure to ultraviolet (UV) light. Bromine is added to ABS plastic as a fire retardant. [clarification needed]  There is still some debate over the long-term effectiveness of this technique. Some have discovered the yellowing reappears, and there is discussion of factors that may result in this happening. There are also some concerns that the process weakens the plastic.       Image i - A TRS-80 Color Computer 2 case showing significant yellowing     Relevant: Amiga 500 | Commodore 64 | Amiga   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3cg5nx,autowikibot,4,Wed Jul 8 18:20:03 2015 UTC,That and I've never found ATX cases really take up that much room. Most of the space PC cases take up is vertical and vertical space is hardly lacking for most users. In terms of footprint a micro atx case really won't take up significantly less desk space and they still are a lot harder to work in imo.
hardware,3cg5nx,Theodoros9,3,Wed Jul 8 03:43:02 2015 UTC,Can you ELI5 how and why you would plug a RaspPi into an X86 system?
hardware,3cg5nx,Rain_Coast,1 point,Wed Jul 8 04:29:51 2015 UTC,"DIY fan and lighting controller, most likely."
hardware,3cg5nx,ryno9o,1 point,Wed Jul 8 17:06:36 2015 UTC,https://www.reddit.com/r/hardware/comments/3cg5nx/what_happened_to_matx_cases/cswhm9r
hardware,3cg5nx,tin_dog,3,Wed Jul 8 18:49:18 2015 UTC,i am still looking for mATX case with 10 hdd support.
hardware,3cg5nx,sterob,2,Wed Jul 8 03:44:02 2015 UTC,I think the fractal node 804 has that much space
hardware,3cg5nx,jingleberry512,1 point,Wed Jul 8 12:38:30 2015 UTC,Some of the Lian Li cases should have the space at that size IIRC from when I was looking to downsize
hardware,3cg5nx,TheRealGecko,2,Thu Jul 9 20:57:13 2015 UTC,"My Current Mainboard has two PCIe x16 Slots for Crossfire goodness, and it's mATX.  I think any chassis that takes a standard ATX Power Supply would make a great gaming system!"
hardware,3cg5nx,ZarK-eh,2,Tue Jul 7 21:45:14 2015 UTC,"If you really want to it can fit an xfire/sli build provided you watercool.  Also managed to fit a 1kw seasonic in the bottom by taking out the HDD cage.  Great case but that's about the max it could ever offer, certainly a 1gpu setup in practice."
hardware,3cg5nx,innerknowing,1 point,Tue Jul 7 19:37:26 2015 UTC,"I thought about mATX but:    The power button (on the top of the case) would be further down (PC sits on the floor) and I'd have to lean down more. Nah, I'm a lazy bastard, I rather get the bigger case with the button more easily reachable. Also, the bigger case has better airflow. In a mATX, my modded GPU (with aftermerket air cooling) would block airflow from the bottom to the CPU. Lastly, a bigger case has more room for my clumsy hands when I tinker with it.   There's so many reasons for a full tower now that I think about it!"
hardware,3cg5nx,hibbel,1 point,Wed Jul 8 10:50:41 2015 UTC,"I always seem to do single GPU builds but I always want the option of dual GPU should I need it.  I haven't upgraded anything in ages and I'll be doing around £3,000 in upgrades over the next year, tri screen eyefinity with freesync. If this even works, haven't seen confirmation as yet as only reviews I've seen for freesync are for single monitor setups. This could potentially take me up to a crossfire setup, but I have heard that there are issues with freesync and crossfire, so again who knows.  I'd honestly really like to play around with it at some point and DX12 seems like a good jumping on point with not only better CPU utilisation, but native multi GPU support (including mixed vendors, which would be interesting if any developers play around with that)  Interestingly btw Phanteks Enthoo Evolv ATX Mid Tower will be my next case, beautiful design and not an overly gigantic monster."
hardware,3cg5nx,iMADEthis2post,1 point,Wed Jul 8 15:22:01 2015 UTC,"Yeah that scenario is definitely above the average user's requirements. ATX makes much more sense there.   The new Evolv ATX is a ridiculously nice case, all the flaws of the original are fixed and there are more options. If I had an unlimited budget I'd build a crazy PC in one of those."
hardware,3cg5nx,Rylock,1 point,Wed Jul 8 15:38:18 2015 UTC,"Base systems last me a long time, so I can justify the expense. Technically I could have a crossfire setup with an mATX but I heavily doubt I'd get one with an M.2 PCIe x4 port which is also on my wants list. Waiting for Zen personally, failing that an intel x99 setup.  I have had a few backup Shuttle systems over the years but the PSUs always brake on the damn things. Great at the time though."
hardware,3cg5nx,iMADEthis2post,-1,Wed Jul 8 16:23:25 2015 UTC,"Anyone that prefers single GPU builds can do pretty much all they need with mATX.   Except GPUs are fucking enormous. My R9 280X is literally over 11"" long. That is hard to fit in a mATX case.  Edit: I'm super happy a few of you found cases to fit big cards. That doesn't change the fact that your options for cases are pretty limited if you are going for a mATX case and want a good GPU. That is a perfectly valid reason for them being scarce and on point with OP's question. Your few instances of finding cases to fit them are meaningless."
hardware,3cg5nx,bfodder,9,Tue Jul 7 21:13:45 2015 UTC,"They fit fine in a proper case designed to house them. My Silverstone TJ08-E is one of the most compact mATX case out there, but can easily house my Sapphire R9 290X Tri-X."
hardware,3cg5nx,dexter311,-14,Tue Jul 7 21:28:04 2015 UTC,Congrats on having the one mATX case that can fit it. That doesn't make this statement untrue.   That is hard to fit in a mATX case.
hardware,3cg5nx,bfodder,3,Tue Jul 7 21:59:58 2015 UTC,"A lot of mATX cases will fit that card.  I went to pcpartpicker and used their compatibility filter.  Assuming I want a mATX case that has:   front panel USB 3.0 a side panel window has a PCPP rating of 4 or 5 stars   I have a couple dozen cases available that will fit. Some might require removing a drive cage, but all will fit that card."
hardware,3cg5nx,bfodder,-4,Tue Jul 7 22:59:32 2015 UTC,If by some you mean all but 1 or 2.
hardware,3cg5nx,bfodder,2,Tue Jul 7 23:36:22 2015 UTC,"sure, but unless you have a need for drives that is significantly above average, you wouldn't need that cage anyways."
hardware,3cg5nx,bfodder,1 point,Tue Jul 7 23:40:23 2015 UTC,Can you link your search you mentioned earlier?
hardware,3cg5nx,MCJeeba,1 point,Tue Jul 7 23:52:20 2015 UTC,"here  I have an empty parts list up with that 290x tri-x, so the compatibility filter works."
hardware,3cg5nx,djfakey,0,Tue Jul 7 23:54:29 2015 UTC,I notice you ticked mid-tower. Aren't those basically ATX cases that also have the mounts for a mATX mobo? Doing only mATX cases really drops the number down.
hardware,3cg5nx,bfodder,3,Wed Jul 8 00:13:20 2015 UTC,"350D, Prodigy-M, Lian Li PCv359, Phanteks Evolv, Fractal Node 804, Air 240 is good for length but isn't compatible with tall coolers.  These are just off the top of my head."
hardware,3cg5nx,djfakey,4,Wed Jul 8 03:57:37 2015 UTC,"Hard as in what?  Working around it? Sure I agree, but like with any proper build, pre-planning is important.  There are plenty of mATX cases and mITX cases that can fit 11""+ cards.  I looked up a ton when I was looking for a case to house my R9 295x2"
hardware,3cg5nx,bfodder,1 point,Wed Jul 8 02:39:26 2015 UTC,"Or maybe you just want to upgrade the GPU and don't want to have to buy a new case. You'll notice that I said it is greater than 11"" as well. Read some reviews on the ASUS R9 280X. I bet you only need to go about 3 before somebody warns you that your mid tower even has a good chance of not fitting it.  Some cases existing that fit it also doesn't mean that your options aren't severely limited, which is a perfectly valid reason that you don't see many of them anymore which is what OP's question was."
hardware,3cg5nx,fraseyboy,1 point,Wed Jul 8 02:40:31 2015 UTC,"Sure case options are limited, but that's why there are options for GPU's as well.  If it doesn't fit, then choose a different manufacturer, an ITX sized one or whatever.  If you want to upgrade the GPU and don't want to buy a new case, then keep shopping for a GPU that will fit?"
hardware,3cg5nx,bfodder,1 point,Wed Jul 8 02:56:45 2015 UTC,"Ugh, options are still limited. It is a factor in the scarcity of mATX cases."
hardware,3cg5nx,hobowithabazooka,2,Wed Jul 8 03:00:41 2015 UTC,"Hell, even fitting a R9 280 in my Antec Solo ATX involved cutting a hole in one of the drive cages."
hardware,3cg5nx,bfodder,3,Wed Jul 8 04:30:15 2015 UTC,"Well, you see, that doesn't actually matter because a very small number of mATX cases exist that it would fit in.  /s"
hardware,3cg5nx,kht120,3,Wed Jul 8 05:04:49 2015 UTC,I had a power color pcs+ 290x  and then a 295x2 in an n200 and a node 804. Those are huge cards > 12 inches and managed to fit
hardware,3cg5nx,random_guy12,-2,Wed Jul 8 02:00:16 2015 UTC,"I said hard, not impossible. You few outliers don't make the statement incorrect."
hardware,3cg5nx,bfodder,2,Wed Jul 8 02:13:54 2015 UTC,A lot of mITX cases can fit that nowadays.
hardware,3cg5nx,jinxnotit,2,Wed Jul 8 22:33:26 2015 UTC,"Your options aren't limited at all. The vast majority of mATX cases will support any full-size size GPU, unless it's absurdly long (13-14"").  It's a little harder with mITX cases, but many will still handle 11"" GPUs.   There's just no reason your average builder with a standard K series CPU and single GPU needs an ATX machine."
hardware,3cg5nx,FSKFitzgerald,-2,Thu Jul 9 02:40:10 2015 UTC,That simply isn't true.
hardware,3cg5nx,joshruffdotcom,18,Thu Jul 9 02:55:13 2015 UTC,You answered your own question.  ITX popularity supplanted MATX. And if you need bigger than ITX you have more room and convenience with ATX.
hardware,3cg5nx,Raptor5150,8,Tue Jul 7 19:00:22 2015 UTC,"I recently moved down to mATX, because I couldn't find anything in mini-ITX that would allow me to continue to have my sound card and my GPU, that was also a horizontal-mount motherboard.  I ended up buying an Aerocool DS Cube for ~$50, which is a killer deal for the ease of working within it. There is a noted lack of intake filters, and there's only a windowed panel -- no mesh grate to allow the GPU to really breathe -- but it's still a great case that offers a smaller size than my Air 540 and prevents GPU sag."
hardware,3cg5nx,BlayneTX,8,Wed Jul 8 01:15:01 2015 UTC,"In my experience, any great mATX board is pricey compared to their ATX counterparts. However, I do love my Air 240 mATX build. Small footprint but enough room for a powerful SLI/crossfire setup."
hardware,3cg5nx,Raptor5150,2,Wed Jul 8 00:23:30 2015 UTC,Right there with you. I love my Air 240 cant wait to pick up another 980 for SLI when they come down in used price.
hardware,3cg5nx,BlayneTX,1 point,Wed Jul 8 02:54:58 2015 UTC,I have that setup and the top card cooks. They are reference cards too. Kind of wish I just went full ATX.
hardware,3cg5nx,dexter311,1 point,Wed Jul 8 06:43:33 2015 UTC,Must be the Texas summer were having. What's the ambient temp in the room?
hardware,3cg5nx,hughJ-,1 point,Wed Jul 8 13:33:55 2015 UTC,21 degrees c
hardware,3cg5nx,alabrand,2,Wed Jul 8 21:41:00 2015 UTC,The only thing that kept me from getting an Air 240 is the poor support for large air coolers. Which isn't surprising given Corsair's product line features CLCs.
hardware,3cg5nx,ClearedFram3,3,Wed Jul 8 04:58:55 2015 UTC,"Systems are lasting longer than they used to and systems that last long tend to leverage expandability more as the needs of the user tend to change over time.  My current system is an mATX x58, coming up on 6 years old, and while fine as a gaming and development system, I have been bumping into the limits of SATA, PCIe, and USB in the past couple years.  The reason I'm now facing a new build is not because of performance (my 4GHz 6core Xeon is plenty), but because of an inability to squeeze in a second video card and still have room for a USB 3.0 card.  Had I bought ATX instead of mATX this system could easily have lasted me another 2 years.  On top of that we also have the harsh reality that Moore's Law may currently be in its death throes and my next build will be the last before something dramatic has to change in the semiconductor industry to progress further.  If my next system lasts as long as this one has then that'll take me all the way up to 2024.  Saving 3 inches of motherboard for the sake of aesthetics seem like a poor trade when it drastically limits your options down the road."
hardware,3cg5nx,Dave_the_Pigeon,3,Wed Jul 8 02:20:47 2015 UTC,"I'm still waiting for a fully aluminium, cooling-oriented mATX case from either In Win or Streacom.  But it will probably never happen."
hardware,3cg5nx,Dave_the_Pigeon,2,Wed Jul 8 10:03:08 2015 UTC,http://www.kimeraindustries.com/#introduction
hardware,3cg5nx,dexter311,4,Wed Jul 8 02:28:42 2015 UTC,"mATX, although I no longer use the form-factor always seemed like the best, most useable form factor to me.  Most people are just going to need one PCIe card (graphics card), most people are only going to need the bare minimum of IO (a few USB, ethernet, audio etc).  mATX offers essentially all of the functionality of consumer chipsets like z97 without the extra price of ports / functionalities most people will never use.  mITX is idiotic, it's more expensive than ATX and offers significantly less features.  The only reason I stopped using mATX was when I switched to an x99 platform.  The only reason to use x99 is more memory and more PCIe hence, why buy anything except an ATX or eATX x99 board?"
hardware,3cg5nx,feyenord,4,Tue Jul 7 20:49:40 2015 UTC,"I wouldn't say that's the only reason to use an x99, you might just want to use a Haswell-E processor for some reason, but it does make sense.   Other than that, you've pretty much summarised my reasons for wishing mATX had more attention."
hardware,3cg5nx,atomicthumbs,1 point,Tue Jul 7 20:52:47 2015 UTC,"Ok, so x99 does have more uses than quad-channel memory and PCIe channels.  However, buying an x99 board with less than 4 dimm slots or less than two PCIe slots is slightly idiotic.  I do understand some compute applications that use the PCIe bandwidth say for PCIe SSD's or compute graphics cards in conjunction with super-nice haswell-E Xeon CPU's.  But really, if you're using a nice Xeon server CPU for compute applications having a shit ton of ram to use with that fancy CPU will always make more sense."
hardware,3cg5nx,JustaPassanger,1 point,Tue Jul 7 21:07:00 2015 UTC,"All the mATX X99 boards I've seen have 4 DIMM slots. 64GB of RAM is nothing to shake a stick at! But yeah, if you're doing heavy compute work or whatever then X99 mATX makes no sense anyway. You'll more than likely be throwing it in a rack case so there's no reason for using mATX."
hardware,3cg5nx,Cptn_Awesome,1 point,Wed Jul 8 04:57:16 2015 UTC,"I'm content with a Zalman T4, it's cheap and does the job."
hardware,3cg5nx,hineybush,1 point,Tue Jul 7 19:21:51 2015 UTC,"I like my Aerocool DS Cube. Sideways motherboard with the drives and PSU underneath, with the PSU's cooling air entirely out of the way, and plenty of room for watercoolers and big air coolers."
hardware,3cg5nx,headband,1 point,Thu Jul 9 01:17:10 2015 UTC,"I think there are several reasons why matx isn't as popular as it should be.   There are more ATX motherboards to choose from. People eat what they know, ATX has been a standard for so long. Buyers take a quick look at the specs (max RAM, PCI slots etc.) and decide that more is better even though they're never going to need it. Same goes for cases, ATX has more fan, HDD slots so it looks better on paper. Height/size. I think most users place the PC on the floor. Reaching the buttons and USB slots can be a bit inconvenient with shorter matx cases.  Some matx cases are still a bit oversized so putting them on the desk might not be an option.   I think matx will get more popular but it's going to take a long time. There are more matx cases than we think but you kinda have to look for them whereas ATX cases are everywhere."
hardware,3cg5nx,headband,1 point,Thu Jul 9 13:26:44 2015 UTC,"I've built all three sizes (ATX, mATX, ITX) in the past. ITX is great for having the smallest footprint but WOW what a pain to build in when compared to ATX or mATX.   What I've noticed, at least here in Canada, is that the prices for mATX boards and Cases are usually the same or higher when compared to equivalent ATX boards and larger cases. And more often then not, when I've helped friends decide on parts, form factor becomes less of an issue when they feel like they're not getting as much as they can get with the money they're spending. Even tough I tell them that they're not going to be using those extra PCI slots anyways.   If we extrapolate this out to the mass, maybe companies aren't seeing a big enough demand for mATX because those of us that do care about form factor are going straight to ITX."
hardware,3cg5nx,random_guy12,1 point,Thu Jul 9 14:12:48 2015 UTC,Just finished my build in a Corsair Air 240. It's a sexy little cube.
hardware,3cg5nx,headband,-5,Thu Jul 9 16:01:34 2015 UTC,"I haven't messed with matx in years, but traditionally atx had better power distribution and cleaner signals resulting in better over locking headroom.  And really they aren't that much smaller."
hardware,3cg5nx,random_guy12,3,Tue Jul 7 20:41:44 2015 UTC,"A define mini is 19.29"" x 8.27"" x 15.55"", for a total volume of 2480 cubic inches  A define R5 is 18.19"" x 9.13"" x 20.51"", for a total volume of 3406 cubic inches  I wouldn't say a 37% volume decrease is ""that much smaller""  (source is specs page on newegg)"
hardware,3cg5nx,Seclorum,2,Tue Jul 7 20:48:22 2015 UTC,It's more about the footprint than the volume
hardware,3cg5nx,Exist50,2,Tue Jul 7 21:07:46 2015 UTC,"And the footprints of the more compact cases are significantly smaller. The Define Mini he used in his example is absurdly large for an mATX case.   My CM N200 (7.9 x 14.9 x 17.5) (2050 in3) is much easier to travel with than any mid tower case, and I have to move it between home and school reasonably often."
hardware,3cg5nx,BrockYXE,1 point,Thu Jul 9 02:49:12 2015 UTC,"No, 90% of Marx cases are just shorter. You can also get goofy smaller atx cases if you dig arround"
hardware,3cg5nx,bphase,1 point,Thu Jul 9 03:09:31 2015 UTC,"That's even a poor comparison, because the Define Mini is absurdly large for an mATX case. They can easily slash 2 inches off the length and one inch from the height.   That would bring the volume down to 2080 in3. A massive reduction."
hardware,3cg5nx,bphase,1 point,Thu Jul 9 02:46:03 2015 UTC,"yeah, I was trying to use cases that had a really obvious comparison in the ATX world, which pretty much restricts things to the Define Mini/Define R3/4/5; the 350D/650D; and the Air 540/Air 240."
hardware,3cg5nx,itssmacks_,-5,Thu Jul 9 03:11:54 2015 UTC,I've tended to notice mATX boards are more expensive than straight up ATX boards. Usually around 20-30 dollars more.
hardware,3cg5nx,Zarknox,10,Tue Jul 7 17:42:41 2015 UTC,"quick example, using some Gigabyte mobos (prices from pcpartpicker):  ATX GA-Z97-D3H: $115  mATX GA-Z97M-D3H: $87  and MSI:  ATX Z97-G43: $132  mATX Z97M-G43: $105"
hardware,3cg5nx,BrockYXE,10,Tue Jul 7 17:48:03 2015 UTC,Only applies for very high end ones. Cheap boards are usually matx because there just isn't much space needed.
hardware,3cg5nx,Zarknox,2,Tue Jul 7 18:39:40 2015 UTC,That's the cost of putting the same tech on a smaller object.
hardware,3cf1m9,speckz,5,Tue Jul 7 12:13:37 2015 UTC,Any estimates on how long it will be till SSDs are cheap enough to beat HDDs in $/GB? I have heard 2-3 years but that seems wrong seeing that a 2TB SSD is $800 and a 5TB HDD is less than $150.
hardware,3cf1m9,hdshatter,10,Tue Jul 7 17:10:39 2015 UTC,2-3 years is too optimistic. It'll take till the end of the decade at least.
hardware,3cf1m9,Exist50,1 point,Tue Jul 7 18:36:23 2015 UTC,"Yeah, SSDs are at 40 cents a GB vs HDDs 3 cents a GB.  They'll have to half the price 4 times before they are neck and neck.  How long will it take them to go from 40 cents to 20 cents per GB?  I'm guessing 18 to 24 months"
hardware,3cf1m9,daubertMotion,3,Tue Jul 7 21:06:33 2015 UTC,And that assumes HDD prices don't change
hardware,3cf1m9,Exist50,1 point,Tue Jul 7 21:52:04 2015 UTC,"Which of course they will, but I think at every milestone they reach (halfing the price per gig) they'll become much more viable at certain market shares."
hardware,3cf1m9,daubertMotion,2,Tue Jul 7 22:18:34 2015 UTC,No one knows. It really depends on possible problems that either side can encounter when going forwards on smaller processes.
hardware,3cf1m9,Schmich,2,Tue Jul 7 18:00:57 2015 UTC,"SSD are getting cheaper. but so do HDD , so probably never"
hardware,3cf1m9,NLWoody,-1,Tue Jul 7 21:42:04 2015 UTC,HDD prices have been stagnant.
hardware,3cf1m9,bfodder,1 point,Tue Jul 7 22:39:00 2015 UTC,"Actually, that's not really that relevant. Let me explain.  Per byte, tapes are still much, much cheaper than HDDs. Still, nobody uses them anymore (at home). A quarter of a century ago, that was different. You could buy tape drives for backups for PCs.  It's not that HHDs are cheaper per byte, it's that they are more than cheap enough for your (non-datacentre) backup needs and the amount saved by switching to tapes isn't worth the hassle.  In a couple of years from now, SSDs will be cheap enough for many of us to buy for their main data drives. Why? Size rises and prizes drop way faster than your average user's size-requirements.  My main data drive is still 1TB, holds all my pictures, music and what few videos I actually don't stream. Same size I got with a PC 5 years ago. OK, today 1TB is the lower limit but 2TB would be more than I conceivably need during the lifetime of a drive I'd buy today.  So, all SSD has to achieve is to be close enough to HDD at the 2TB size point two years from now. Close enough so I start to consider HDD to be too much of a hassle (those damn long loading times!) compared to a more expensive SSD that I can still easily afford.  Today, any requirement that can be met with .5TB or less will be moved to SSD. Two years down the road, I expect SSD to be financially viable and cheap enough to replace my main drive, pushing HDD to the backup-drive niche."
hardware,3cf1m9,hibbel,1 point,Wed Jul 8 10:35:37 2015 UTC,"Last year Seagate said we'd reach 100TB HDDs by 2025. If they're so sure about that, I'd be skeptical of SSDs becoming cheaper than HDDs anytime soon."
hardware,3cf1m9,Stingray88,1 point,Tue Jul 7 20:09:05 2015 UTC,Wasn't that using new technology that's better than a SSD?
hardware,3cf1m9,hdshatter,3,Tue Jul 7 20:33:54 2015 UTC,Hmm... I don't recall hearing anything like that.  But I'd hope so... writing 100TB at the speed of present day HDDs would be a bitch.
hardware,3cf1m9,Stingray88,1 point,Tue Jul 7 20:50:20 2015 UTC,"""oops my RAID5 had a drive go bad... Better rebuild.""  See you in the next ice-age."
hardware,3cf1m9,SirCrest_YT,3,Wed Jul 8 04:18:07 2015 UTC,"I'll be taking a 512gb M.2 drive, like the Samsung SM951 rather than a Sata 3 drive for my upgrade. Hopefully Zen will Justify Mobo vendors including enthusiast level options like this on AMD systems. At Least a year off, so I'm fairly optimistic at this point."
hardware,3cf1m9,iMADEthis2post,4,Wed Jul 8 16:10:18 2015 UTC,"Given their bad track record for the last 840s and 850s, ill pass."
hardware,3cf1m9,totallyworkingATM,15,Tue Jul 7 22:26:05 2015 UTC,"Wasn't that just for some ridiculously obscure use?  Like, random linux distro (so <10% of users), with certain features enabled (<10% of those users) and for enterprise use (<1% of those users)."
hardware,3cf1m9,BezierPatch,2,Tue Jul 7 22:53:17 2015 UTC,Does Windows 10 use asynchronus TRIM? Or OS X 10.11?  That problem is only one support package for your OS away from hurting you. So why settle for a drive that'll presumably not randomly delete your valid data?
hardware,3cf1m9,hibbel,1 point,Wed Jul 8 10:41:28 2015 UTC,"Enterprise use, eh? Those 70% of the server market?"
hardware,3cf1m9,dwe11er,5,Wed Jul 8 07:25:18 2015 UTC,Using Consumer hardware? Lol  You want to get sued?
hardware,3cf1m9,BezierPatch,2,Wed Jul 8 09:23:56 2015 UTC,<$0.30/GB and I'll buy one of Samsung's 1TB drives. Using a varied collection of 7200 RPM platter drives and a 250gb SSD for the important stuff. 1TB of fast storage for most of my games would be great - it is a bit annoying micromanaging 250gb of space.
hardware,3cf1m9,jinatsuko,2,Wed Jul 8 00:43:26 2015 UTC,yay
hardware,3cf1m9,TruckChuck,-10,Tue Jul 7 15:35:05 2015 UTC,Reminder: don't buy these if you run Linux or you'll lose your data to an errant TRIM command
hardware,3cf1m9,XSSpants,13,Tue Jul 7 17:30:35 2015 UTC,Quit spewing fud. Just make sure your risers had an updated kernel or has backported the blacklist update to their current kernel.
hardware,3cf1m9,ZeDestructor,-13,Tue Jul 7 17:57:35 2015 UTC,"It's not FUD, linux destroys data on these drives. The kernel workaround is a hack and leaves you high and dry on TRIM support."
hardware,3cf1m9,XSSpants,12,Tue Jul 7 18:00:59 2015 UTC,"And if you'd bothered to read the full worklog of the company tracking it down, you'd see that at the bottom, there is a nice link to a change to the Linux kernel that blacklists queued TRIM for the 850 Pro (line 4238-4239, to be exact). Note that the non-queued TRIM works properly, else Windows (which uses non-queued TRIM exclusively) would also have the same bug.  This change has been merged into kernel 4.1 (iirc, could be as early as 4.0.x, or planned for the upcoming 4.2 series), and backported to many older kernels by various maintainers.  So please, stop spreading spread the FUD, and provide a useful warning in the future.  EDIT: also, downvotes? for calling you out? really? that mature?"
hardware,3cf1m9,ZeDestructor,-4,Tue Jul 7 18:15:13 2015 UTC,The warning is useful. It should be considered by anybody not running 4.1 or backports (hasn't made it into my Fedora install yet.)
hardware,3cf1m9,XSSpants,1 point,Tue Jul 7 18:17:21 2015 UTC,And your reminder about it is crap. It's a blanket statement with no fix or workaround implied or stated.
hardware,3cf1m9,ZeDestructor,0,Wed Jul 8 21:51:54 2015 UTC,People are capable of doing their own research.
hardware,3cf1m9,XSSpants,2,Thu Jul 9 12:58:32 2015 UTC,"By that measure, your original comment has no value and should never have been posted."
hardware,3cf1m9,ZeDestructor,1 point,Thu Jul 9 18:59:36 2015 UTC,"It's still a valid warning, yo. There's nothing invalid about my original statements."
hardware,3cf1m9,XSSpants,1 point,Thu Jul 9 19:21:20 2015 UTC,"First you give a crappy warning  with no sources or workaround (both of which are readily available), then you claim that ""people are capable of doing their own research."" Oddly enough, if people did their due research, they'd find out about the TRIM issue, which would make your original comment completely useless.  You like moving the goalposts don't you?  By the way, I never claimed your warning was invalid, just that it was crap and FUD - yes, you can spread FUD using 100% true  and legitimate arguments."
hardware,3cf1m9,ZeDestructor,-1,Fri Jul 10 13:09:19 2015 UTC,You seem quite knowledgable on this subject. You wouldn't happen to know if OS X uses queued trim do you?
hardware,3cf1m9,Stingray88,3,Tue Jul 7 20:10:36 2015 UTC,"Not a clue. I tend to ignore everything OSX-related since I don't use it, and nobody I know that I provide any support/advice to uses it either."
hardware,3cji98,wanking_furiously,6,Wed Jul 8 11:57:14 2015 UTC,source is from videocardz.com which was linked yesterday and is still frontpage r/hardware
hardware,3cji98,ijnokmp,1 point,Wed Jul 8 12:38:06 2015 UTC,"Bugger, must have just missed it when I checked."
hardware,3cji98,ryno9o,1 point,Wed Jul 8 21:29:51 2015 UTC,Nice that it comes with the DP to DVI cable for those with the Korean monitors.
hardware,3cji98,PippinMuffinHead,1 point,Wed Jul 8 12:43:25 2015 UTC,"Beside that in brackets it says ""FR Only"". Do you have an idea what that means? France only?"
hardware,3cji98,Kobayakawamiyuki,0,Wed Jul 8 16:37:29 2015 UTC,"Which is near useless for 2560x1440.  Edit for the people that have downvoted me: You need an active adapter for DP to DVI for any resolution above 1920x1200. Downvote me all you want, but if you have a 2560x1440 monitor with only DVI inputs, then don't be disappointed when you learn the hard way."
hardware,3cji98,CeeeeeJaaaaay,1 point,Wed Jul 8 13:23:43 2015 UTC,Can yo do 1080p120Hz with the bundled adapter?
hardware,3cji98,Kobayakawamiyuki,1 point,Thu Jul 9 15:06:07 2015 UTC,Nope. You need an active adapter that supports dual link for 120Hz.
hardware,3cji98,CeeeeeJaaaaay,1 point,Thu Jul 9 16:51:08 2015 UTC,Cheers
hardware,3cevfk,Andrej_ID,8,Tue Jul 7 10:58:31 2015 UTC,"I love how small these cards are. Hopefully it becomes the norm.  If half height became a standard thing too... oh man, it would be beautiful."
hardware,3cevfk,b0yd07,6,Tue Jul 7 14:35:10 2015 UTC,"Well both AMD and Nvidia are going with HBM 2 next gen, so expect greater Vram capacities in about this footprint.   The big driver of board size, fitting all the GDDR5 chips and their traces, is going bye bye."
hardware,3cevfk,Seclorum,7,Tue Jul 7 16:45:07 2015 UTC,This must be your wet dream (no pun intended) then.  It's my favorite card ever.
hardware,3cevfk,vitallity,3,Tue Jul 7 16:21:42 2015 UTC,How is that even possible??
hardware,3cevfk,flyafar,2,Tue Jul 7 22:57:57 2015 UTC,They just replaced the back plate. The 580 just had a vent there.
hardware,3cevfk,Cozmo85,1 point,Wed Jul 8 01:49:42 2015 UTC,I'm hoping 2016 will be the rise of the mini-dtx form factor with half height cards becoming the norm.
hardware,3cevfk,ryno9o,14,Tue Jul 7 18:39:28 2015 UTC,Looks like an overclocker's dream.
hardware,3cevfk,BlayneTX,9,Tue Jul 7 13:43:00 2015 UTC,"Meh, Fury X never was cooling limited."
hardware,3cevfk,bphase,-4,Tue Jul 7 14:27:48 2015 UTC,This should deal with the fan whine issues though!
hardware,3cevfk,reynardtfox,3,Tue Jul 7 15:16:32 2015 UTC,"Its not the fan that whines, but the pump."
hardware,3cevfk,Seclorum,3,Tue Jul 7 16:43:43 2015 UTC,"Good catch.  The EK release should solve that though, shouldn't it?"
hardware,3cevfk,reynardtfox,7,Tue Jul 7 16:52:50 2015 UTC,"Yep.   Now we just need software devs to figure out how to interface with the voltage controller AMD used and get voltage control's so you can see how high if at all, the card can be pushed."
hardware,3cevfk,Seclorum,1 point,Tue Jul 7 17:11:53 2015 UTC,"Cool.  I've been waiting on this info before I make my next build, but at this rate I might just wait for HBM gen 2 given my lack of a need for an upgrade atm."
hardware,3cevfk,reynardtfox,2,Tue Jul 7 17:20:20 2015 UTC,Yeah. For the most part I personally would wait till the next round of offerings. HBM 2 has the potential for stupid high Vram totals on boards as well as all the same space saving and efficiency gains.
hardware,3cevfk,Seclorum,1 point,Tue Jul 7 17:38:32 2015 UTC,Yeah. When will this be? I am very disappointed about the lame over clock at stock volts and I really hope all it takes is a voltage bump to make the fury sing.
hardware,3cevfk,lancypancy,2,Wed Jul 8 03:10:34 2015 UTC,"No idea. Last I heard, over a week ago, they were still playing with registry values and still hadn't found how to interface with it yet."
hardware,3cevfk,Seclorum,1 point,Wed Jul 8 03:33:33 2015 UTC,I don't understand why AMD doesn't help them.... It is directly in their interest to help them fix this ASAP.
hardware,3cevfk,ShoutingDani,1 point,Wed Jul 8 18:45:11 2015 UTC,You would think so. No idea why there isn't more documentation on how to interface with it.
hardware,3cevfk,Seclorum,3,Wed Jul 8 20:27:44 2015 UTC,"Have they already sold out, or have they not started selling them yet?"
hardware,3cevfk,R403Q,4,Tue Jul 7 13:15:33 2015 UTC,Should be in the stock tomorrow.
hardware,3cevfk,R403Q,1 point,Tue Jul 7 14:20:34 2015 UTC,Sweet! Thanks.
hardware,3chegu,Pat-Roner,1 point,Tue Jul 7 22:45:54 2015 UTC,"I love Zotac, for the price this card is awesome, only $650? Thats the price for stock is it not?"
hardware,3chegu,Abipolarbears,0,Wed Jul 8 15:43:24 2015 UTC,I wish Fury X wasn't so underwhelming. I don't want to spend and extra $200 on a gsync monitor.
hardware,3chegu,maybachsonbachs,3,Wed Jul 8 02:13:18 2015 UTC,"Me neither, but I also don't want a variable refresh rate monitor that doesn't work below 40Hz."
hardware,3chegu,Yearlaren,3,Wed Jul 8 04:41:28 2015 UTC,"Yeah that's why I wanted the Acer XG270HU, but I'd rather not compromise on the gpu just to make a statement about nvidia's market practices."
hardware,3chegu,maybachsonbachs,0,Wed Jul 8 13:06:06 2015 UTC,I also don't want to believe in a world where <40fps is ever acceptable.
hardware,3chegu,Hariooo,2,Wed Jul 8 16:01:45 2015 UTC,Your frame rate will from time to time drop below 40. I don't want to get tearing every time it happens.
hardware,3chegu,Yearlaren,-1,Wed Jul 8 16:28:58 2015 UTC,No it really doesn't lol  Think of it this way you can spend the extra $200 on a better AMD card and not get shitty 40fps.
hardware,3chegu,Hariooo,5,Wed Jul 8 17:26:03 2015 UTC,But there is no current AMD offering that is $200 more than a 980 ti...
hardware,3chegu,JeffroGymnast,0,Wed Jul 8 17:56:18 2015 UTC,That's... a good point.
hardware,3chegu,Yearlaren,2,Wed Jul 8 17:30:27 2015 UTC,"Then don't.  Why you would waste money on a proprietary piece of technology that locks you into one company's product?  Wait it out until the monitor manufacturers get their heads out of their butts, and give us the open standard that allows up variable refresh rates without requiring specific video cards.  Your monitor should not determine your video card, and your video card should not determine your monitor.  It's antithetical to PC gaming."
hardware,3chegu,animeman59,2,Wed Jul 8 06:55:55 2015 UTC,"My issue is this is a complete build. I'm not just upgrading. I have a trash 5 yr old laptop, so I'm not waiting for some standards body to get its act together. I just want to build my machine sometime in the next 2 months."
hardware,3chegu,maybachsonbachs,0,Wed Jul 8 13:04:00 2015 UTC,Is a 5% performance increase worth 200 dollars?
hardware,3chegu,merolis,7,Wed Jul 8 02:32:56 2015 UTC,But it's not 5% at 1440p when you OC the 980 Ti. I'm really just waiting for the Fury OC situation to get clearer. People here keep saying something to the effect of just wait for board makers to update their software.
hardware,3chegu,maybachsonbachs,1 point,Wed Jul 8 13:03:32 2015 UTC,"As I understand it, Unwinder is still waiting to receive a Fury X he can personally test and use. The situation will be clearer once that happens."
hardware,3chegu,terp02andrew,2,Wed Jul 8 13:48:55 2015 UTC,http://forums.guru3d.com/showthread.php?p=5117322#post5117322  Things aren't looking great. The card apparently downclocks when voltage is increased.
hardware,3cghck,qda,20,Tue Jul 7 18:51:39 2015 UTC,"Higher end drives usually have some shaking protection, I don't think they really care nowadays because SSDs can be thrown off a building and still work."
hardware,3cghck,hdshatter,12,Tue Jul 7 19:44:00 2015 UTC,Post/username synergy.
hardware,3cghck,foxtrot1_1,6,Wed Jul 8 15:28:26 2015 UTC,"usually the protection is done via the mainboard with the use of an accelerometer.  the driver/software then tells the drive to move it's head to the park position.  one thing to note is that newer drives use platters made of glass, so the could shatter more easily."
hardware,3cghck,t_Lancer,2,Tue Jul 7 19:35:02 2015 UTC,I thought the accelerometer was in the HDD?  Hence this: http://www.wired.com/2010/09/disk-drives-earthquake-detection/
hardware,3cghck,Schmich,1 point,Tue Jul 7 23:42:06 2015 UTC,"most laptops are fairly decent with this anymore, as stated they use the accelerometer to detect when the laptop is moving and shut down the hard drives.  This will protect your hard drive if you drop your laptop.  This does not protect your HDD from dropping something on or bumping into the laptop (as it doesn't have a chance to park the read heads due to no pre-movement).    Source I just had to replace dad's laptop hdd due to dropping something and having it hit the laptop.  I had a spare SSDD(solid state disk drive) and I threw that in there so he wouldn't have to worry about that happening again (and a bonus faster loading times).  TLDR; if the laptop is actually moving, there is usually enough time to park the read write heads so they don't hit the platter when it stops moving (hits the floor).  However, if the object moving isn't the laptop and instead another object that makes contact with the laptop, there usually isn't enough time to park the heads and save the HDD."
hardware,3cghck,fakename5,1 point,Thu Jul 9 22:04:34 2015 UTC,i wasnt aware of the ablity of point of using glass HDD disks.....why would you do that when metal is stronger?
hardware,3cghck,CommanderArcher,5,Tue Jul 7 20:54:24 2015 UTC,Here's the reasons for glass:   glass is smoother so the head is lower and allows higher spindle speed glass is more rigid so the platters can be smaller/thinner/lighter glass is more thermal consistent so it does not expand as much when heated up   Glass/ceramic composites are also usable to counter the fragility issue.
hardware,3cghck,lucun,2,Tue Jul 7 21:24:54 2015 UTC,Stronger can have a lot of different meanings in material science.   https://en.wikipedia.org/wiki/List_of_materials_properties#Mechanical_properties
hardware,3cghck,Sisaroth,6,Wed Jul 8 06:44:14 2015 UTC,Ssd is the best shock protection you are going to get. Hdd has a running head like a record player and even the best protection will only do so much once they are knocked off course. If the disk gets damaged you're fucked.
hardware,3cghck,LiberDeOpp,3,Tue Jul 7 20:39:37 2015 UTC,"Unless it's an SSD, no they're not more resilient than they used to be.  I work in a computer repair shop and laptops come in a few times a year with shattered drives.  They are made of a ceramic composite resembling glass.    SSDs are cheap these days, so my recommendation is to get one & throw the drive it came with into a case & use it as your backup."
hardware,3cghck,lexpython,2,Tue Jul 7 22:18:42 2015 UTC,"yes most drives can lock themselves if they detect that they are falling. the mobo tells the drive to lock in place and the needle swings back to its resting position. SSDs are always better than HDD, and you can find large ones for fairly cheap if you look around."
hardware,3cghck,CommanderArcher,1 point,Tue Jul 7 20:56:50 2015 UTC,Interesting. Using a laptop with a hdd in space may not work then.
hardware,3cghck,TheImmortalLS,1 point,Thu Jul 9 08:17:55 2015 UTC,Presumably you can turn off that feature in BIOS.
hardware,3cghck,Kaghuros,1 point,Thu Jul 9 10:17:14 2015 UTC,"Actually hdds in space need to be protected from cosmic rays, otherwise they barely work at all, same with cpus, computers meant for space a have alot of shielding that ours don't need. But they do work"
hardware,3cghck,CommanderArcher,3,Thu Jul 9 15:46:59 2015 UTC,/r/SSDMasterRace
hardware,3cghck,WYLD_STALLYNS,1 point,Wed Jul 8 00:51:51 2015 UTC,"I know HP includes drive protection software on their laptops, although I'm not sure about other OEMs."
hardware,3cghck,Inaspectuss,1 point,Tue Jul 7 21:47:48 2015 UTC,"Nope, just a general question about HDD standards these days.."
hardware,3cghck,exo67,1 point,Tue Jul 7 18:51:39 2015 UTC,it's a bot.  edit: if you were expecting a reply.
hardware,3cghck,john_dune,3,Tue Jul 7 18:53:52 2015 UTC,"I know, but some bots are monitored by humans.. Looks like someone deleted the bot's reply"
hardware,3cghck,TheImmortalLS,-7,Tue Jul 7 19:01:08 2015 UTC,"What's the capacity of the laptop's hdd? If it's a multiple of 2 it's an ssd and super shock resistant. If it's an even number, it's a traditional drive and fairly shock resistant (drops of upto 4feet)"
hardware,3cghck,john_dune,2,Tue Jul 7 19:29:17 2015 UTC,"You mean 128 GB vs 1 TB? Those are just capacities, which don't correspond to whether it's a ssd or not."
hardware,3cghck,TheImmortalLS,0,Tue Jul 7 21:46:45 2015 UTC,"Yes and no.  Most traditional laptops have mechanical hard drives in the following sizes: 250, 320, 500, 750, and 1000gb. Most ssds are listed in the following sizes: 128, 256, 512gb.  So in most cases, those sizes will never be the same between the ssds and hdds (though with some cases, the SSD that's 256gb may be overprovisioned and knocked down to 250gb).  So it's a simple rule of thumb that can generally indicate what kind of hard drive is in a computer."
hardware,3cghck,john_dune,2,Thu Jul 9 08:19:03 2015 UTC,"I agree on the hdds, but ssds can have varying capacities, which is why I personally don't rely on it as a first measure. I just see if it's advertised as ssd or hdd"
hardware,3ccvjb,nth_derivative,68,Mon Jul 6 22:43:27 2015 UTC,290s and some 290Xs are cheaper. Get em while they last.
hardware,3ccvjb,luddist,20,Mon Jul 6 23:15:56 2015 UTC,Since there are like 5 billion of them out in the wild it will be fine. There's a 290 or 290x posted almost daily on swap.
hardware,3ccvjb,darrenphillipjones,8,Tue Jul 7 00:53:20 2015 UTC,"And get the 300 series hacked drivers while you're at it.  http://forums.guru3d.com/showthread.php?t=400078   It's the next obvious comparison point, but the problem is that AMD has forked its drivers - the 15.15 300-series driver won't work with the older card while the existing 15.5 200-series driver doesn't work on the 390X. This presents a problem as downclocking the 390X to match 290X, we noted some notable differences in performance: Assassin's Creed Unity, for example. The solution? A hacked 200-series compatible 15.15 driver showed no performance drops on our test titles with the 290X, and suddenly downclocked 390X performance was a match on our test subjects. We're going to assume that those optimisations will roll out to all AMD users in due course, and that's the like-for-like comparison we're going to go with for this piece.   http://www.eurogamer.net/articles/digitalfoundry-2015-radeon-r9-390x-review"
hardware,3ccvjb,namae_nanka,7,Tue Jul 7 04:46:11 2015 UTC,Or be lazy like me and just wait until 15.20 is released. I guess its going to be the release driver for win10.
hardware,3ccvjb,jakobx,1 point,Tue Jul 7 06:32:38 2015 UTC,wheres the link for the hacked driver?
hardware,3ccvjb,jforce321,2,Tue Jul 7 11:21:27 2015 UTC,http://forums.guru3d.com/showthread.php?t=399956
hardware,3ccvjb,_Captain_Redbeard_,1 point,Tue Jul 7 14:08:54 2015 UTC,ty :)
hardware,3ccvjb,jforce321,1 point,Tue Jul 7 14:29:20 2015 UTC,"Quite a few people have tried to reproduct this (myself included) and the drivers made no difference whatsoever.  You are better off getting the most up to date Windows 10 drivers, which right now is 15.200.1023.7."
hardware,3ccvjb,glr123,1 point,Tue Jul 7 22:04:03 2015 UTC,Strange that eurogamer would be reporting this then. And no tessellation improvement either? Even with the flashing?
hardware,3ccvjb,namae_nanka,1 point,Tue Jul 7 22:32:31 2015 UTC,"Even the big guys can be wrong. They're having to experiment to get their data, they're not getting it from amd directly, and sometimes experimenting fucks up"
hardware,3ccvjb,AssCrackBanditHunter,1 point,Wed Jul 8 00:08:14 2015 UTC,"They're actually following the small guys' lead, the guys over at guru3d forum where they got the hacked driver from."
hardware,3ccvjb,namae_nanka,1 point,Wed Jul 8 02:06:49 2015 UTC,"I wonder how they're getting slightly better performance out of the same chips. I read they ""updated the microcode"" for the 390(X) chips and supposedly that was partially responsible for the small performance gains seen. I'd assume that would be found in the BIOS but could it be in the drivers?  Another possibility is that the new drivers cheat by lowering image quality, but Nvidia got busted for that a while ago and weren't treated lightly for it."
hardware,3ccvjb,luddist,1 point,Wed Jul 8 02:35:17 2015 UTC,"Apparently there are tessellation improvements, a 290X flashed to 290X basically doubling the score at higher factors, and draw calls improvement which hint towards a cpu overhead reduction."
hardware,3ccvjb,namae_nanka,6,Wed Jul 8 03:06:21 2015 UTC,This is only thing that makes sense because the 8gb of ram is worthless in a card incapable of running 4k. The 290 series was never bad and I wish amd repeated that success this generation.
hardware,3ccvjb,LiberDeOpp,14,Tue Jul 7 02:55:17 2015 UTC,"I wouldn't call 8GB ram worthless yet. Years ago there were many people who would argue endlessly that 2GB on a 680 and 670 would be plenty for 1080p. GTA V can crush that easily, despite those cards still being plenty powerful enough to push good settings you'll hit the VRAM wall before you run out of power.  As ""next gen"" games are finally starting to be built from the ground up to utilize all that console VRAM, it could be pretty important going forward to have around 6 or so if you want to stretch your purchase out a little longer."
hardware,3ccvjb,niioan,3,Tue Jul 7 04:09:09 2015 UTC,Do you think that the R9 390 is the best thing to get if you can't stand to wait for another card to come out at the ~350 price range?  I have a 1440p monitor and was debating picking up this card because my 270x(2gb) isn't doing so well these days.
hardware,3ccvjb,Domooo,2,Tue Jul 7 04:57:37 2015 UTC,"280x is a pretty good value card for 1080p or so, not sure on availability though."
hardware,3ccvjb,Omnislip,2,Tue Jul 7 09:18:50 2015 UTC,"short answer yes, especially at 1440p, typically the higher the rez the better AMD looks.  but as an alternative I would highly recommend The Sapphire 290 and OC it (hopefully) to 1100.  http://www.newegg.com/Product/Product.aspx?Item=N82E16814202143&cm_re=r9_290-_-14-202-143-_-Product  Save as much money as possible now and invest next year when GPUs get, what should be a substantial upgrade in late 2016, also hoping that doesn't get delayed any further.... The 290 should do good for a decent amount of time  For the record that is my personal plan, but I'm also going to wait till that particular 290 goes back down to 230 (after rebate) where it had been many times before and I will use that card till the 2016 16/14nm cards drop to a price:performance I can live with, which will probably be well into 2017."
hardware,3ccvjb,niioan,1 point,Tue Jul 7 05:10:44 2015 UTC,"Part of the reason I was thinking about getting the 390 is semi future proofing, because whatever I get I'll end up putting in whatever computer I build my SO once I upgrade in the new generation of cards. I'll have to compare them."
hardware,3ccvjb,Domooo,3,Tue Jul 7 05:16:56 2015 UTC,"Once you OC a 290 to the same speeds of a 390 it gives almost exactly the same performance, and the 390 also runs on newer drivers, which should come to the 290 series as well at some point, they are probably being held back so the 390 looks slightly better IMO.   The 390 is just a bit more refined but it is basically the same level of performance as a 290. The 8GB and faster stock VRAM speed is nice and should probably OC on average a bit better due to 28nm being a refined process by now, but I'm not sure thats worth the $80 difference, it's not for me, if I had to make a choice ASAP.   On the other hand the 390 still isn't a bad purchase because there is really nothing else to buy in that price bracket besides a 970, but I really don't like the VRAM problems it has and the 390 seems to be equal or better anyway."
hardware,3ccvjb,niioan,1 point,Tue Jul 7 23:33:24 2015 UTC,"Yeah I know I could get very similar results from a 290, though I've read good things about some of the coolers on the 300 series.   I'll probably hold off on getting anything regardless and see if anything good comes up on Prime Day next week.  Definitely trying to about the 970 due to the VRAM issue though."
hardware,3ccvjb,Domooo,1 point,Tue Jul 7 23:38:52 2015 UTC,Anything with 4gb will be plenty. If 8gb was a big issue amd wouldn't have their top of the line cards only having 4gb. You can look online and you'll see very few games come to using more than 4gb. More importantly the actual gpu is going to be the problem way before that 8gb gets half used up.
hardware,3ccvjb,LiberDeOpp,2,Tue Jul 7 07:13:13 2015 UTC,Anything with 4gb will be plenty. If 8gb was a big issue amd wouldn't have their top of the line cards only having 4gb.    That's not correct at all. The reason their Fury cards only have 4gb is because at the moment it is literally impossible for them to put more than that.
hardware,3ccvjb,atriax,2,Wed Jul 8 20:10:48 2015 UTC,Any coming bethesda game + texture pack.  I'll take 8gb.
hardware,3ccvjb,barberstrisand,0,Tue Jul 7 10:53:16 2015 UTC,You realize witcher 3 uses around 2gb of vram at 4k? By the time 4gb of ram becomes obsolete there will be a new generation of video cards out from both companies. Game companies build for what people have and very few people will more than 4gb of ram for a long time.
hardware,3ccvjb,LiberDeOpp,1 point,Tue Jul 7 07:10:18 2015 UTC,You realize shadow of mordor uses more than 4gb of vram at 1440p at ultra right? Don't say one game as if that's the end all be all. GTA5 also uses quite a lot of vram.
hardware,3ccvjb,atriax,1 point,Wed Jul 8 20:11:26 2015 UTC,"Depends on what you do... If you're a student or an artist without the cash for a TitanX, Quadro or even a 6gb 980ti, having 8gb of vram open up plenty of possibility for some hardcore 3D scene gpu rendering since your whole model must fit in vram or the rendering will fail."
hardware,3ccvjb,amenard,1 point,Tue Jul 7 13:12:51 2015 UTC,How much cheaper?they seem that they have the same price
hardware,3ccvjb,myhtconex,1 point,Tue Jul 7 04:21:03 2015 UTC,Not in where I live.
hardware,3ccvjb,Pik16,1 point,Tue Jul 7 06:41:24 2015 UTC,So if I have a reference 290x would it be better to pickup another 290x to crossfire them or hold out for a non reference 390?
hardware,3ccvjb,ferriolom,1 point,Tue Jul 7 14:18:40 2015 UTC,I just picked up an R9 290 and it is a beast of a card for the price. OC'd it to 1175/1350 with no voltage increment. Huge step up from my old 6770.
hardware,3ccvjb,Mr_Enduring,25,Tue Jul 7 04:12:21 2015 UTC,"ive been waiting for this review. The 970 is such an amazing card, I had a hard time choosing between the 970 and the 390. If there was a minor price difference I still wouldve chosen the 390 simply because of the extra vram. Since price is a non issue, both cards are so close in performance you really can choose either one. I look forward to more driver updates and DX12 games."
hardware,3ccvjb,MP9,-46,Tue Jul 7 01:24:33 2015 UTC,The 970 is faster at stock. Also has a lot more room for overclocking. Its better to compare it to a 290x
hardware,3ccvjb,wwbulk,43,Tue Jul 7 01:41:11 2015 UTC,This is literally the opposite of what the review said.
hardware,3ccvjb,Zepherith,-23,Tue Jul 7 02:00:22 2015 UTC,Then the review is literally the opposite of all other reviews
hardware,3ccvjb,headband,20,Tue Jul 7 02:06:18 2015 UTC,"... I don't think you've mastered the definition of the word literally yet.   The Sapphire R9 390 Nitro 8GB is a more cost effective solution – this graphics card is released to go head to head against the Nvidia GTX970. As our tests have shown today, it is clearly able to outperform the Nvidia card – both are close to reference speeds according to manufacturer specifications. Not only that, but when manually overclocked to the limit, the R9 390 can trade blows with the more expensive R9 390X.   http://www.kitguru.net/components/graphic-cards/zardon/sapphire-r9-390-nitro-8gb-review/23/"
hardware,3ccvjb,BKachur,10,Tue Jul 7 03:35:28 2015 UTC,... did you not watch the review posted because its contrary to what your claiming here. The 390 is faster at stock then faster at OC. Verify your facts before making a claim.
hardware,3ccvjb,BKachur,4,Tue Jul 7 01:57:15 2015 UTC,"In the review you posted, the 970 only beats the 390x in GTAV and Unity at 1080p and only Unity at 1440p. The other 4/5 games they tested had the 390x perform better."
hardware,3ccvjb,Helvegr,2,Tue Jul 7 03:15:06 2015 UTC,"Dude... you link didn't even review the 390... If your gonna argue your point about a topic at lease make sure your talking about the right thing. Here is first google link on the 8GB 390.  http://www.kitguru.net/components/graphic-cards/zardon/sapphire-r9-390-nitro-8gb-review/23/   The Sapphire R9 390 Nitro 8GB is a more cost effective solution – this graphics card is released to go head to head against the Nvidia GTX970. As our tests have shown today, it is clearly able to outperform the Nvidia card – both are close to reference speeds according to manufacturer specifications. Not only that, but when manually overclocked to the limit, the R9 390 can trade blows with the more expensive R9 390X."
hardware,3ccvjb,BKachur,-3,Tue Jul 7 03:34:01 2015 UTC,I have a an EVGA 970 SC 2974 edition. It has a stable oc at 1506 on the core at 1.2 mv.  I don't know about stock speeds but this card easily competes with the 390 and my temps are 56c with 70% under load at about 3 hours.
hardware,3ccvjb,LiberDeOpp,125,Tue Jul 7 03:32:31 2015 UTC,"I wonder if there's anything anyone could say that would stop 95% of the builds on /r/buildapc from being GTX 970s?  The 390 could literally cure cancer and people would be like, ""Yeah but... It consumes 300W and can't use PhysX.""  AMD could completely solve their power usage problems and people would jump ship to another excuse, ""Yeah but... The PCB is like 0.5 inches longer, it would take two hands to put into my chassis.  Nvidia only requires 1 hand.""  /rant"
hardware,3ccvjb,TaintedSquirrel,29,Tue Jul 7 02:53:43 2015 UTC,"I've been buying AMD cards since the 4000 series. I just wish these new cards impressed me, but they don't. An 8GB R290X, the Fury X, and the GTX 980 Ti are the only upgrades I could bother getting. And they are WAY too expensive for me. =/"
hardware,3ccvjb,Drudicta,-13,Tue Jul 7 02:10:37 2015 UTC,8GB R290X   I assume you meant to say 8GB R9 390x
hardware,3ccvjb,TokyoRock,21,Tue Jul 7 02:49:02 2015 UTC,There are 8 GB 290X's for around $350.
hardware,3ccvjb,TaintedSquirrel,6,Tue Jul 7 03:20:02 2015 UTC,But...but...  Did you watch the video?
hardware,3ccvjb,TokyoRock,9,Tue Jul 7 03:20:49 2015 UTC,Why would I spend $370 on an 8GB 290X(which is the lowest I can find them for) instead of $330 on an 8GB 390 which is clearly as good/better?
hardware,3ccvjb,C4ples,-9,Tue Jul 7 03:34:01 2015 UTC,"It's not ""as good or better"" it's an overclocked 290.  It will be as good or better than a 290X at stock, sure.  The difference between the two is very small, maybe like 5%.  I would buy the 390 just for the new cooler/PCB designs."
hardware,3ccvjb,TaintedSquirrel,4,Tue Jul 7 03:46:18 2015 UTC,"It's not ""as good or better""   It pretty clearly is. Go actually read some reviews and let me know what you think then.   it's an overclocked 290   How is it simply an overclocked 290? The 390 shares the exact same base clocks as partners' 290s. Furthermore with power delivery and consumption improvements as well as the standard 8GB VRAM it's absolutely not simply an OC'd 290.   The difference between the two is very small, maybe like 5%.   Know what that's called? ""As good or better""."
hardware,3ccvjb,C4ples,1 point,Tue Jul 7 03:48:47 2015 UTC,"http://hardocp.com/article/2015/06/18/msi_r9_390x_gaming_8g_video_card_review/9#.VZtRR_kVjCM   Clock-for-clock the performance is identical in Witcher 3 between the R9 390X and 290X, despite the R9 390X having twice the VRAM.  Same here, exactly the same performance.  Here as well, performance is right on par with each other.  This proves it, at the same clocks the AMD Radeon R9 390X performs exactly the same as the AMD Radeon R9 290X."
hardware,3ccvjb,TaintedSquirrel,0,Tue Jul 7 04:09:09 2015 UTC,So you would intentionally spend more money just for the sake of getting an older generation of card?
hardware,3ccvjb,Soytaco,4,Tue Jul 7 04:12:21 2015 UTC,290X is over $100 cheaper than the 390X.
hardware,3ccvjb,TaintedSquirrel,7,Tue Jul 7 05:18:55 2015 UTC,"especially because    at the same clocks they perform the same   the r9 390x has higher clocks, so it is better. so basically, less power consumption ,less heat, higher clocks, cheaper = not ""As good or better"" .   Some people just want to down talk stuff."
hardware,3ccvjb,soidboerk,-1,Tue Jul 7 14:06:43 2015 UTC,"Because I can go on Ebay and haggle for an outdated card. Possibly down to 200 dollars if I'm really lucky.  I'm poor, I don't care if it's used."
hardware,3ccvjb,Drudicta,-7,Tue Jul 7 09:23:26 2015 UTC,Because the 290x has more transistors and processors.
hardware,3ccvjb,veyron3003,1 point,Tue Jul 7 03:58:18 2015 UTC,Or on Ebay for cheaper. I wouldn't spend more than 250 dollars though. I'm a cheap bastard. A poor.... cheap bastard.
hardware,3ccvjb,Drudicta,18,Tue Jul 7 06:26:19 2015 UTC,The 390 isn't better I would they fairly trade blows right now. It really depends on what you would rather own.  I think nvidia should still drop the 970 price just out of selfishness.
hardware,3ccvjb,LiberDeOpp,13,Tue Jul 7 03:57:20 2015 UTC,If amd solved their power consumption issue I'd jump ship and never look back.
hardware,3ccvjb,Maysock,12,Tue Jul 7 02:58:40 2015 UTC,Maybe you but others wont. Nvidia used to be the one with higher power consumption but still their market share was bigger. What AMD needs is better marketing.
hardware,3ccvjb,jakobx,22,Tue Jul 7 02:21:30 2015 UTC,"Their new frame rate thing apparently helps a lot.  Frame Rate Target Control, or something.  It stops the card from doing more than it has to in games that it can handle without working at 100%."
hardware,3ccvjb,Valridagan,3,Tue Jul 7 06:36:00 2015 UTC,"I thought it was already mostly solved though.  TDP for comparable cards on either side are very similar, iirc"
hardware,3ccvjb,Dragonsong,4,Tue Jul 7 03:07:01 2015 UTC,"On the high end, kinda.  The 390 has a 275w tdp, whereas the 970 is 145w. That means SLI is a lot less demanding on my powerbill. Bums me out, because I like that 390x."
hardware,3ccvjb,Maysock,31,Tue Jul 7 02:41:48 2015 UTC,"TDP is an initialism for Thermal Design Power, and it doesn't correlate directly with power consumption. Under load, the GTX 970 and the R9 390 are actually much closer, but the 970 still wins. Also, the 970 runs a lot cooler on less cooling, so you'd also be losing the fanless mode on most 970 models.  Realistically, you have to ask yourself if those features are worth losing 1-3 FPS, which seems to be the difference between the two, and well within the margin of error."
hardware,3ccvjb,Oafah,1 point,Tue Jul 7 02:51:31 2015 UTC,"While TPD in theory sure is something else than power consumption in practice it's almost always interchangeable with power consumption. The actual power consumption usually is within 10% of the TPD. And about the GTX 970 vs R9 390:  A factory OC'd GTX 970 at 168w.  A factory OC'd R9 390 at 282w.  So no, they're not very close to each other ""under load""."
hardware,3ccvjb,MaloWlol,10,Tue Jul 7 03:08:16 2015 UTC,"I think nVidia's specification/definition for TDP is a bit different from AMD's.  If you look at reviews for the GTX 970 they draw around 220 W on full load.    http://hexus.net/tech/reviews/graphics/81985-evga-geforce-gtx-970-super-superclocked/?page=10  I suppose the 390's TDP is still noticeably higher though, seems to be around 320 W  http://www.techpowerup.com/reviews/Powercolor/R9_390_PCS_Plus/28.html"
hardware,3ccvjb,Dragonsong,6,Tue Jul 7 11:37:05 2015 UTC,"You're comparing an OEM to Nvidia's spec for their reference card. Of course an overclocked card is gonna draw more power, not to mention they can change the VRMs as well. 145W doesn't apply to anything that's not an Nvidia branded reference card, it has nothing to do with the companies involved."
hardware,3ccvjb,azn_dude1,4,Tue Jul 7 03:02:41 2015 UTC,"You do realize your first link is SYSTEM power consumption right? So no, 970 by it self does not draw 220w on load. Did you even look at your 2nd link? It clearly shows 970 has peak draw of 181w and an average 163w, which is in line with all the other 970 reviews. Probably mention a number of times, TDP is not the same as power draw.   Did people even look at his links...."
hardware,3ccvjb,Sayburirum,1 point,Tue Jul 7 05:27:59 2015 UTC,Why is this guy getting upvoted? The 220w GTX 970 he linked to is total system draw at the wall before PSU conversion. And the 320W on the 390 is a maximum peak draw and not average maximum load draw.  Here's a correct power consumption of a factory OC'd GTX 970 at 168w.  Here's a correct power consumption of the same 390 you linked at 282w.
hardware,3ccvjb,MaloWlol,2,Tue Jul 7 13:30:27 2015 UTC,my bad
hardware,3ccvjb,Dragonsong,1 point,Tue Jul 7 11:33:20 2015 UTC,you have no idea what you are talking about. Run the numbers right now. compare the amount of money your bill will change between them and tell me it costs more to run this card than the price difference between the nvidia alternative.
hardware,3ccvjb,imoblivioustothis,5,Tue Jul 7 19:03:43 2015 UTC,People need to learn TDP is not the same thing as power consumption...  Let us look at how much wattage difference there is measured from the wall not Nvidia's TDP they trick people into thinking TDP = Wattage used.  Here is normal gaming power consumption:   https://tpucdn.com/reviews/Powercolor/R9_390_PCS_Plus/images/power_average.gif
hardware,3ccvjb,hdshatter,-3,Tue Jul 7 06:54:51 2015 UTC,"Not really, history and tests has showed us that actual power consumption usually is within 10%, and often less, of the TDP. Using the TDP to figure out the power consumption is a valid method. Even in the image you linked this can be shown, both cards are within 10-ish% of their TDPs."
hardware,3ccvjb,MaloWlol,-2,Tue Jul 7 07:26:44 2015 UTC,"Actual consumption difference in a lot less AMD measures tdp conservatively, while intel and nvidia do some sort of weird stuff.   In the end, nvidia and amd overclocked should be within 50 W for similar performance."
hardware,3ccvjb,TheImmortalLS,11,Tue Jul 7 11:42:39 2015 UTC,"If there weren't dozens of tests that said otherwise, I would believe you."
hardware,3ccvjb,thelordpresident,2,Tue Jul 7 03:26:32 2015 UTC,Comparing TDP between GPUs from different brands is like comparing TV contract ratios between different brands. They all measure it differently. Some companies 1000000:1 is the same thing as others 1000:1.  The real wattage is figured out by plugging it into a kill a watt meter and measuring power draw.
hardware,3ccvjb,hdshatter,2,Tue Jul 7 06:55:30 2015 UTC,kill a watt meter   Hopefully something a lot more accurate than a $20 Kill-a-Watt.
hardware,3ccvjb,aziridine86,-2,Tue Jul 7 07:28:01 2015 UTC,"Not really, history and tests has showed us that actual power consumption usually is within 10%, and often less, of the TDP. Using the TDP to figure out the power consumption is a valid method."
hardware,3ccvjb,MaloWlol,-2,Tue Jul 7 07:46:29 2015 UTC,A lot closer to 100
hardware,3ccvjb,dylan522p,-4,Tue Jul 7 11:38:44 2015 UTC,"Not really, history and tests has showed us that actual power consumption usually is within 10%, and often less, of the TDP. Using the TDP to figure out the power consumption is a valid method.  And in the end Nvidia and AMD overclocked has a 114w difference for similar performance.  A factory OC'd GTX 970 at 168w.  A factory OC'd R9 390 at 282w."
hardware,3ccvjb,MaloWlol,2,Tue Jul 7 05:15:47 2015 UTC,"tbh I don't find their testing methodology kosher. different graphics cards will put different stresses on the cpu. how, if they measure system peak with a gpu + cpu running furmark or something, do they end up with just a peak gpu. they need to subtract a cpu running furmark, which doesn't work without a gpu.  do you have a site measuring at the slot? i know techpowerup does.  edit: found two msi techpower up articles. older 290x and the 970. Although the 290x doesn't have the power saving features the 390x has, it does appear there is more of a difference (average) than I thought."
hardware,3ccvjb,TheImmortalLS,1 point,Tue Jul 7 11:40:44 2015 UTC,"390x isn't any better, it consumes much more for that extra performance, 370w peak and 344w average. http://www.techpowerup.com/reviews/MSI/R9_390X_Gaming/28.html"
hardware,3ccvjb,Sayburirum,18,Tue Jul 7 14:48:50 2015 UTC,"I think the stupidest argument nowadays is Nvidia has better drivers, that comment is pure cancer. If anything Nvidia drivers are worse than AMD. Just lookup how many people are complaining about Nvidia drivers compared to AMD.   GPU not being used at full speed? Doesn't happen on AMD.  TDRs? Not an issue on AMD, only time I ever have seen one was because of unstable overclocks.  Chrome Glitches/doesn't work? Not an issue on AMD either. (My laptop experiences this randomly and its annoying, has a 960m)  Drivers released that have killed/bricked GPUs? Never on AMD.  When I used to use Nvidia on my desktop with a 670 and updated it once the driver made my PC display no image after windows loads and I was unable to restore my restore points so I had to copy all the data to another drive and format it + reinstall windows."
hardware,3ccvjb,hdshatter,15,Tue Jul 7 15:10:03 2015 UTC,Numbers don't mean anything. More people own nvidia so of course more people will complain about their drivers.
hardware,3ccvjb,TruckChuck,3,Tue Jul 7 07:02:44 2015 UTC,The steam survey is pretty shocking to see it's 3 to 1 in favor of nvidia and AMD is almost passed by integrated video but I don't know if that counts the AMD all in one chip brand or just the intel one.
hardware,3ccvjb,RiffyDivine2,3,Tue Jul 7 07:57:44 2015 UTC,AMD is almost passed by integrated video   Most likely because every i series processor has an integrated GPU. This is going to highly inflate stats of users.
hardware,3ccvjb,Scurro,1 point,Tue Jul 7 13:30:55 2015 UTC,"Hopefully, we will see. I want them to shake up nvidia but this card is sounding like it won't be the one to do that."
hardware,3ccvjb,RiffyDivine2,3,Wed Jul 8 17:31:27 2015 UTC,"I can only speak from my own experience using both, and I've had absolutely 0 problems for the past 4 years running SLI with Nvidia, not ever have I had a driver related issue. The only issue I've had is that in some titles only 1 of my cards will be loaded as there's no SLI profile for it, but I've never had any other issues.  When I used AMD before that I had 5'ish titles or so that I simply couldn't run due to problems with the AMD drivers. And most beta and alpha games I played ran horribly for months due to unoptimized drivers, something I haven't seen at all the past 4 years using Nvidia.  While I don't usually go around saying ""Avoid AMD, they drivers are shit"", do I however share my experience with the two if anyone asks for advice on what to do, but I also make sure to let them know that there are other people who has had the opposite experience and that it's up to them to decide which side to go with."
hardware,3ccvjb,MaloWlol,1 point,Fri Jul 10 13:16:43 2015 UTC,"Same as you, been using nvidia in SLi since the 670's and never looked back and never had any issue I didn't cause myself. Like when I first setup SLi and everything had a green tint to it, turns out I had to just flip the bridge around and problem solved. Maybe I didn't have issues with my 670's cause I only get EVGA brand cards."
hardware,3ccvjb,RiffyDivine2,1 point,Tue Jul 7 11:47:47 2015 UTC,"I wish you were right, but that hardly seems to be the reality. I'm getting so sick of small issues from AMD drivers that im just considering jumping ship.  For example, theres a cursor corruption issue that happens when using a multimonitor setup. Here's a thread about a guy complaining about this issue on his 4850 in 2009. Now, here's a thread in 2015 about someone complaining about the same exact issue on his R9 290.  I'm still suffering from this issue for years and years. Its not the only issue either, just the longest standing one i can think of. Im getting so tired of AMD driver quality issues."
hardware,3ccvjb,XaosII,1 point,Tue Jul 7 13:29:42 2015 UTC,"My build had at most 9"" for gpu so the 8"" zotac 970 was a godsend. Its a pretty sweet card"
hardware,3ccvjb,Abipolarbears,1 point,Tue Jul 7 20:26:03 2015 UTC,"For me it comes down to OpenCL. Dollar for dollar support and performance for CUDA is still beating out OpenCL, and if AMD can get people to work on solving that (which may simply be a political thing among devs at this point) then it'll become viable for me."
hardware,3ccvjb,Kichigai,2,Tue Jul 7 14:30:54 2015 UTC,"Long time amd user. Just bought my first nvidia card. A 970.  2 reasons.   I installed Linux last year and ran into too many problems.  I play a lot of Sims, dcs and project cars. Both have issues with amd..  If they could have just fixed those things, which they didn't in the last 5 years, I would have stuck with amd. I have no brand loyalty, I buy on price performance. But when it just didn't run the things it should, time to find the best price performance that does, which in this case was a 970."
hardware,3ccvjb,gengis,-3,Tue Jul 7 15:46:23 2015 UTC,"If AMD did something about power consumption I would jump ship in a second. And if they ever made any kind of interesting software, I might too. But they don't, so I won't."
hardware,3ccvjb,thelordpresident,1 point,Tue Jul 7 16:01:37 2015 UTC,FRTC is pretty interesting.
hardware,3ccvjb,wagon153,1 point,Tue Jul 7 06:53:59 2015 UTC,For games
hardware,3ccvjb,thelordpresident,-10,Wed Jul 8 20:23:30 2015 UTC,"That's the result of Nvidia putting huge ammounts of $ into advertising.  Also, you don't have to wait for them to solve their power issues, they already did that with the Furyx and people are still buying the 980ti. It doesn't even matter if the Furyx comes with a ""free"" water cooler. Their argument is that the ti is faster, we're talking about 5-10% difference. I've even seen someone say that he went with the ti because he didn't want to risk buying new 1'st gen technology (HBM memory)."
hardware,3ccvjb,JustaPassanger,5,Thu Jul 9 02:23:48 2015 UTC,"First of all, the Fury X still draws more juice than the 980Ti (275 vs 250 TDP). While it's a lot better than the other cards, it's still kind of a bummer that the slower card uses more power.   It doesn't even matter if the Furyx comes with a ""free"" water cooler.   And now you actually have to find a place to put the radiator. In my case, I just don't have any more room for that. I literally can't fit this card inside without removing existing components. But the lower temperatures and slightly lower noise levels are definitely nice.   Their argument is that the ti is faster, we're talking about 5-10% difference.   5-10% is nothing to scoff at. And while the average may be 5-10%, in some games and some resolutions the difference is significantly larger.  But that's not all. Look through Techreport's review. In almost every game the Fury X has significantly higher frame time fluctuations than the 980Ti. Here's their conclusion as a graphic.  So, objectively: if we're only taking the card into consideration, why shouldn't I buy a 980Ti that draws less power, reaches higher fps, has a more stable frame rate, and comes with 6GB VRAM?  AMD have built an impressive card with the Fury X. But since most reviews give the 980Ti a slight edge over it, the people who don't care about specifically supporting either company will probably just go for the slightly better alternative."
hardware,3ccvjb,Mr_s3rius,-4,Tue Jul 7 05:04:03 2015 UTC,"Even if the cards were a tie, the software Nvidia came up with and more games are using should count for something. I like the gameworks stuff and was surprised how much I liked seeing the hairworks stuff in the witcher."
hardware,3ccvjb,RiffyDivine2,-15,Tue Jul 7 08:23:07 2015 UTC,I don't enjoy rebrands
hardware,3ccvjb,TruckChuck,0,Tue Jul 7 13:33:25 2015 UTC,let me know when 4gb of extra Vram a rebrand.
hardware,3ccvjb,veyron3003,-10,Tue Jul 7 07:57:00 2015 UTC,Lulz enjoy your 8gb on a card that can't run 4k.
hardware,3ccvjb,TruckChuck,5,Tue Jul 7 08:35:34 2015 UTC,"As the proud owner of a 980 (non-ti, I like my card to be low-power) I must say - you're a troll.  4GB is well worth it for Skyrim if you apply the right mods. Just you wait for Fallout 4 and texture packs. If anything, we'll start wondering weather we should have spent those $1k for the 12GB a TitanX features after all. Processing power is not the problem, modding everything for 4k or even 8k textures will be what'll hold me back (at 4GB only)."
hardware,3ccvjb,hibbel,-6,Tue Jul 7 08:39:46 2015 UTC,12gb vram for fallout 4...and I'm the troll
hardware,3ccvjb,TruckChuck,3,Tue Jul 7 11:14:34 2015 UTC,"I'm simply extrapolating from Skyrim, that is listed on steam with 1GB VRAM as recommended. Yet, when I go all-out on mods, I fill my 4GB easily on 1080p.  Now, the latest games like GTAV already come close to being bottlenecked with 4GB VRAM at 2160p. Tomshardware's review of the Fury X showed that you can push the card to a point where it starts swapping and framerates tank – compared to a 980ti with 6GB VRAM.  So, current games already can use up to 4GB VRAM. Modded Bethesda games can be pushed to use several times the VRAM that is recommended (not required!) for the base game. Put the two together and you're looking at modded Fallout 4 potentially eating up more VRAM not only than that provided by a Fury X but also more than provided by a 980ti.  However, until mods push it that far, I hope I'll be sporting a 16nm Pascal or Fury X successor that has 8GB HBM."
hardware,3ccvjb,hibbel,-8,Tue Jul 7 11:48:01 2015 UTC,"It can actually. Just at 30fps. Tell me, how well can a 970 run after 3.5gb?"
hardware,3ccvjb,veyron3003,-5,Tue Jul 7 12:43:41 2015 UTC,Oh 30fps. Sounds cinematic.  Don't know I never went above 3gb at 1440p.
hardware,3ccvjb,TruckChuck,-5,Tue Jul 7 08:48:43 2015 UTC,Then you should keep your mouth shut.
hardware,3ccvjb,veyron3003,-6,Tue Jul 7 08:58:32 2015 UTC,https://www.psychologytoday.com/blog/what-your-child-needs-know-about-sex-and-when/201109/the-super-sexualization-children-time-take  https://www.psychologytoday.com/blog/real-healing/201208/overexposed-and-under-prepared-the-effects-early-exposure-sexual-content  http://www.protectkids.com/effects/harms.htm  http://www.rand.org/pubs/research_briefs/RB9068.html
hardware,3ccvjb,TruckChuck,-7,Tue Jul 7 12:15:24 2015 UTC,"OK, so this is a weird and sad one, but the reason that I won't be buying a Fury, or any other AMD card in the next year, is simply because they're unpopular.  This is sort of a market confidence type argument, just that, if AMD is unpopular enough, and it looks like they're going that way, then in a year there might not be an AMD anymore.  If that happens, no driver updates ever again.  People complain about the drivers now, wait until they haven't been updated in 6 month (or have only been updated by open-source community efforts).  I had a Voodoo 3, and when nVidia came around and ended the reign of my favorite graphics card maker, things got ugly pretty fast."
hardware,3ccvjb,StellaTerra,3,Tue Jul 7 15:09:15 2015 UTC,Argumentum ad populum is a logical fallacy.  Nvidia didn't end 3DFX.  They ended their own company by bringing all card production in house instead allowing AIBs to continue to buy chips to build their own cards.
hardware,3ccvjb,Maldiavolo,-4,Tue Jul 7 16:33:57 2015 UTC,"It's not an argumentum ad populum.  No more than a stock market analyst saying ""I don't want to buy this stock because other people won't and then it will be worth less"".  I mean, you can attack the logic that it will happen that AMD folds, but if it folds, the cards' performance will suffer greatly.  Same goes with 3DFX, it doesn't really matter that it was their mistake, nVidia out competed them with a better strategy.  It has no bearing on the fact that their folding hurt the performance of their cards."
hardware,3ccvjb,StellaTerra,2,Tue Jul 7 20:10:33 2015 UTC,Well stock and product offering are two very different things.
hardware,3ccvjb,glr123,2,Tue Jul 7 21:41:22 2015 UTC,"OK, so this is a weird and sad one, but the reason that I won't be buying a Fury, or any other AMD card in the next year, is simply because they're unpopular.   Ahh you even used the word popular in your reasoning.....   Same goes with 3DFX, it doesn't really matter that it was their mistake, nVidia out competed them with a better strategy. It has no bearing on the fact that their folding hurt the performance of their cards.   Wait so if it was 3DFX's mistake how can Nvidia have outcompete them?  Competition implies an action or series of actions that a company takes to win.  If the other entity screws up and torpedos their own business it has nothing to do with competition.  It more analogous to forfeiting a match than it is playing and losing.  Aside from that, it is a fact that 3DFX was the bearer of their own demise.  It is not up for debate because it's historically accepted.        Edit:wording"
hardware,3ccvjb,Maldiavolo,2,Tue Jul 7 22:01:38 2015 UTC,"You're misunderstanding argumentum ad populum. He's not saying ""more people are buying nVidia, thus nVidia is the better option inherently"". The argument is that AMD might not be around in the foreseeable future because their products aren't popular.  Now, I disagree with the conclusion that AMD might not be around (their ""death"" has been predicted for decades now), and there's some pretty good debate about the consumer ethics of avoiding a brand because they're the little guy, but it's not an appeal to popularity."
hardware,3ccvjb,SPOOFE,1 point,Tue Jul 7 22:11:48 2015 UTC,I'm not misunderstanding anything.  You don't get that the inverse of argumentum ad populum is also a fallacy.  It just doesn't have a phrase associated to it.  You cannot claim that because something is popular it's right and you also cannot claim that because something is unpopular it's wrong.  That's the way logic works.
hardware,3ccvjb,Maldiavolo,1 point,Tue Jul 7 22:41:10 2015 UTC,"I'm not misunderstanding anything. You don't get that the inverse of argumentum ad populum is also a fallacy.   What you're misunderstanding is that it's not a fallacy at all. It may not be the soundest of logic, but not all poor conclusions are the result of a fallacy."
hardware,3ccvjb,SPOOFE,0,Wed Jul 8 00:57:36 2015 UTC,"""It is logically fallacious because the mere fact that a belief is widely held is not necessarily a guarantee that the belief is correct"" I'm not saying AMD cards are bad because most people believe they are bad, I'm saying that that belief has the power to effect the continued existence of the company, which really very definitely would have an impact on the performance of their cards.  I mean, are you arguing that if the company didn't exist, that if no more drivers were put out it wouldn't affect performance? Or are you saying that that the unpopularity of the products has no bearing on the viability of the company?  My logic goes like this, sans red herrings:   AMD cards (and CPUs) are unpopular. That's the premise of this whole thread. If the products are unpopular enough, the company won't turn a profit, and might fold. If it folds, they won't produce any more drivers. If they don't produce drivers, the cards' performances will suffer greatly. Therefor, there is some non-trivial danger that AMD's GPUs' performance will suffer in the near future.   The argumentum ad populum argument looks more like this:   Video cards that perform well are popular. AMD cards aren't popular. Therefor AMD cards don't perform well.   Not at all the argument I was making."
hardware,3ccvjb,StellaTerra,1 point,Wed Jul 8 07:02:01 2015 UTC,"Hey SPOOFE. Thanks for disagreeing and making sense, all in a post that isn't just a downvote. I definitely agree that the ethics are questionable, and maybe this is a 'be the change' kind of thing, I just have concerns about the viability for the reason stated. Honestly my concerns about the future of the company have less to do with their trailing position in the GPU race than they do with their seemingly insurmountable deficit in the CPU one. But that's all based on reading articles that were, especially in light of your thoughts, probably alarmist click-bait. Like I said, my concerns are somewhat time-boxed: if I see things pick up I'd be happy to jump ship for the right card.  Also... I'm female, not 'He'. ^_^"
hardware,3ccvjb,StellaTerra,8,Wed Jul 8 04:24:07 2015 UTC,"finally, I wondered what people meant by active vrm cooling... I mean normally it means that fans blows on something as opposed to passive...  In this context they mean that vrm heatsinks have direct contact with the main heatsink and the heatpipes"
hardware,3ccvjb,DoTheEvoIution,3,Wed Jul 8 04:08:57 2015 UTC,"Active doesn't necessarily mean it's connected to its own heat sink, but the term is sometimes used differently for GPUs"
hardware,3ccvjb,Exist50,2,Tue Jul 7 00:11:52 2015 UTC,Active is heatsink and fan passive is heatsink only.  Passive can also mean no heatsink but that would get hot very fast.
hardware,3ccvjb,LiberDeOpp,2,Tue Jul 7 02:30:07 2015 UTC,"VRM cooling isn't that important currently though. In many cases, the fan(s) are enough. I do believe last gen 290/290x's had some issues with VRM temperatures though."
hardware,3ccvjb,Kobayakawamiyuki,1 point,Tue Jul 7 03:00:35 2015 UTC,"yeah, my asus 290 DC2's vrm heats up like a bitch: http://phpvenus.com/voz/upload/f79349b17747744e92efb9a6cd16a4baffb2a090.png  However 390x DC3 has active vrm cooling now so I hope it's better.  https://c1.staticflickr.com/1/562/19350489112_534880a72d_o.jpg"
hardware,3ccvjb,amusha,2,Tue Jul 7 05:13:26 2015 UTC,Let's be fair. Asus had the absolute worst cooler of the 290/290x GPU's.
hardware,3ccvjb,kennai,11,Tue Jul 7 11:28:32 2015 UTC,"Hopefully Nvidia drops their prices, and soon. I would jump on this but I need a Cuda capable card for python with pycuda and whatnot, but I am only a student for 1 more month and could really use a good cuda card before I finish."
hardware,3ccvjb,hak8or,9,Wed Jul 8 11:10:26 2015 UTC,Any advantage using pycuda vs pyopencl?
hardware,3ccvjb,Esyir,16,Tue Jul 7 01:25:36 2015 UTC,"Cuda is simpler (if you are actually coding in cuda, not pycuda) than OpenCL, and allows you to use an amazing amount of object oriented design and C++ concepts and code when writing it, you also don't have to worry about JIT and converting your code to binaries for your kernels.  OpenCL requires a whole lot more hardware specific knowledge and relies heavily on C concepts more than C++, IE no object oriented-ness (Bolt might have changed that, but I'm not sure) the advantage of OpenCL is that it can run on anything that has the bindings written for it, which is virtually every dedicated processing device on the market and works with AMD cards (accept the godamn Raspberry Pi, fuck you Broadcom you lazy pieces of shit, and on top of that your GPU assembly doesn't even work properly).  A specific hardware feature that makes AMD cards much better than Nvidia cards in some areas in terms of compute performance is the fact that AMD APUs, CPUs and GPUs are capable of taking a ""vector"", or a group of primitives (IE ints doubles floats etc) usually in powers of two (2 4 8 16 etc) and on one compute core equivalent on AMD can compute all 16 uint8 or 8 shorts or 4 floating point operations at once, not even using multiple stream processors, just one.  You might hear people talk about Nvidia cards being better at scalar computation than vector computation when compared to AMD cards, this is where that idea comes from, in addition to Nvidia cards managing the computational power of their processors differently (hence cuda cores) making computational units smarter than an AMD unit, but can only function on one scalar (AKA primitive type) at one time"
hardware,3ccvjb,Plazmatic,6,Tue Jul 7 02:00:01 2015 UTC,"Huh. TIL. I'm working exclusively on CPU atm, so the details on Cuda/OpenCL unclear to me. Thanks."
hardware,3ccvjb,Esyir,3,Tue Jul 7 04:25:03 2015 UTC,"I was under the impression that PyCuda was more popular and therefore I had a large community I could lean upon for when I need help. But, after some googling, it seems both are pretty active, so never mind then."
hardware,3ccvjb,hak8or,4,Tue Jul 7 05:42:58 2015 UTC,just use pyopencl breh
hardware,3ccvjb,nnotserPx,9,Tue Jul 7 04:12:20 2015 UTC,Counterpoint:  https://www.techpowerup.com/reviews/Powercolor/R9_390_PCS_Plus/32.html
hardware,3ccvjb,jakobx,2,Tue Jul 7 02:05:02 2015 UTC,So buy r9 290 or r9 270x?
hardware,3ccvjb,Randomoneh,5,Tue Jul 7 06:08:48 2015 UTC,I want to see this thing in Crossfire.
hardware,3ccvjb,vivalafica,1 point,Tue Jul 7 06:39:50 2015 UTC,My friend just got two of them in crossfire. Plays Metro Last Light at 1440p 60fps with ease.
hardware,3ccvjb,iktnl,6,Tue Jul 7 02:28:00 2015 UTC,"Wooh, I chose the right card! Almost went for the 290X or the 970."
hardware,3ccvjb,_fmm,10,Tue Jul 7 16:26:38 2015 UTC,"The 390 is the same price as the 970 in Australia (both at $500, in fact 970s are on average $30 cheaper).    I don't know why anyone would buy a 390 here."
hardware,3ccvjb,IC_Pandemonium,17,Tue Jul 7 09:33:27 2015 UTC,"More performance at higher resolutions. 390 is a fantastic sweetspot for 1440p or multimonitor, and trades blows/wins at 1080p."
hardware,3ccvjb,X-Ecutioner,1 point,Tue Jul 7 03:05:24 2015 UTC,"My 290 oc'd like a mofo and was one of the best price/performance cards I have ever had while my 970 is completely unable to clock anywhere. As a matter of fact, my first 970 died on the first day after just installing drivers (shit happens but still) and this second one is OC nightmare. Also these overly praised nvidia drivers have been one big pile of shit. First GTA V drivers fucked up every other game and then gpu resets started happening which are still somewhat here after their ""hotfix"". I have boatload of reasons to consider moving to 300 series."
hardware,3ccvjb,builtitapc,2,Tue Jul 7 06:34:41 2015 UTC,Just got this card today and my 130's cover doesn't fit on without the fans hitting it. But Witcher 3 4k makes 1080p look like child's play.
hardware,3ccvjb,LiberDeOpp,7,Tue Jul 7 07:59:09 2015 UTC,How do you run witcher 3 at 4k with this card? Is everything turned down to low?
hardware,3ccvjb,builtitapc,8,Tue Jul 7 02:50:25 2015 UTC,"Everything ultra gets me about 22fps stock. A few harmless tweaks and you're at 30fps. Depends what is playable to you, but resolution is big to me.  I'm still working out getting the settings tweaked even further for more frame gains.  Edit: typo"
hardware,3ccvjb,LiberDeOpp,4,Tue Jul 7 03:11:20 2015 UTC,http://www.gamersnexus.net/game-bench/1952-complete-witcher-3-graphics-optimization-guide-and-performance Hope this helps with some of your tweaks. Would be nice to get 4k going in witcher 3 and I'm loving the game even if I'm on 1080p.
hardware,3ccvjb,builtitapc,3,Tue Jul 7 03:29:23 2015 UTC,Thanks! I will try these out and report back.
hardware,3ccvjb,watchme3,3,Tue Jul 7 03:41:41 2015 UTC,when i made the change from a 1080p to 1440p playing witcher. My jaw dropped. I can t believe i haven t upgraded my monitor for 7 years. also avging 55fps with a 970 on ultra settings without aa and no hairworks. Also some stuff like grass turned down to high. Should mention i oced the card to do +210 mhz stable.
hardware,3ccvjb,LiberDeOpp,1 point,Tue Jul 7 03:45:19 2015 UTC,Yeah my 970 is average 75 on ultra in 1080p with a 1506 oc probably wont go 1440p since 4k should be dropping in the next year. I can only imagine what that upgrade will look like.
hardware,3ccvjb,watchme3,1 point,Tue Jul 7 20:23:06 2015 UTC,"man 4k seems like a bit of an overkill for now. I wouldn't use anyting bigger than 27"" for a monitor, but it d be cool to have a 4k tv hooked up to the pc."
hardware,3ccvjb,builtitapc,2,Tue Jul 7 20:35:09 2015 UTC,"Extremely unscientific benchmarking: Using steam's in game FPS counter, running around on Roach.  With everything cranked and AO at SSAO and tessellation at 16x 18-21 FPS  Hairworks off, 22-23 FPS  Foliage Visibility Range High and Medium, 24-25 FPS  Ambient Occlusion off, 24-31 FPS  Shadows, Grass, & Texture quality set to high, 28-31 FPS"
hardware,3ccvjb,LiberDeOpp,1 point,Tue Jul 7 22:16:33 2015 UTC,Good deal I'll probably wait to get sli 970 or 980ti before i jump to 4k. The smart thing would be to wait for the gen since they will hbm 2.0 and die shrinks
hardware,3ccvjb,builtitapc,1 point,Tue Jul 7 19:30:13 2015 UTC,"Yeah, tough to justify at this point. Depends on the games you play, really."
hardware,3ccvjb,LiberDeOpp,3,Tue Jul 7 20:29:45 2015 UTC,I really doubt you're a solid 30fps unless you have large OC or med-low settings.
hardware,3ccvjb,ritz_are_the_shitz,5,Tue Jul 7 21:05:26 2015 UTC,"Witcher 3 is surprisingly not all that demanding if you know what to tweak in the ini. There are some things that drastically impact performance (like distant shadows and lod quality) that don't have as much a say in image quality. Plus, if you're not stupid and run tessellation at 16x instead of 64x, then the perf hit is minimal. I run witcher 3 at 1440p~45fps, maxed past ultra (further render distance, etc) on a 290x. I don't doubt this card could run witcher 3 at near ultra settings on 4k."
hardware,3ccvjb,RoboTrigger,2,Tue Jul 7 03:33:48 2015 UTC,Does anyone know of any tests comparing OC'd 970s and OC'd R9 390s?
hardware,3ccvjb,parasemic,9,Tue Jul 7 03:24:19 2015 UTC,The video in OP??
hardware,3ccvjb,RoboTrigger,1 point,Tue Jul 7 07:35:54 2015 UTC,I obviously had a brain fart. Thanks :)
hardware,3ccvjb,MaloWlol,2,Tue Jul 7 09:13:23 2015 UTC,"It's a 13 minute long video and I don't have time to watch it all. Judging by the title I'm guessing they're saying that the R9 390 is better performance / dollar than the GTX 970, anyone can summarize why their conclusion is different than Techpowerups?  Is it some regional pricing or has AMD done a price drop on the R9 390 since TPU's review?"
hardware,3ccvjb,RiffyDivine2,-8,Tue Jul 7 09:35:46 2015 UTC,"Don't rule out the whole thing being supported by AMD, not sure if videos are required to have it labeled on them about it yet."
hardware,3ccvjb,Mikodebiko12,2,Tue Jul 7 12:34:42 2015 UTC,The real king is the 290/x
hardware,3ccvjb,TexSC,3,Tue Jul 7 13:12:11 2015 UTC,Great to see some viable competition!
hardware,3ccvjb,rationis,1 point,Wed Jul 8 13:01:33 2015 UTC,"The comments in here are a plethora of childish responses primarily from the Nvidia fanbase. There is a lot of disinformation going on.   C'mon guys, people don't behave this badly on r/nvidia and r/advancedmicrodevices"
hardware,3ccvjb,RiffyDivine2,2,Mon Jul 6 23:08:43 2015 UTC,What did you expect given the topic and that there are other reviews that disagree with this statement. It would be impossible to really have a discussion on the topic of video cards without it turning into this.
hardware,3ccvjb,DJ-Foran,1 point,Tue Jul 7 09:41:26 2015 UTC,"mmhm, and I think the level of discussion is fine for sorting out misconceptions about drivers, TDP, playable resolutions and so forth as we enter another product cycle."
hardware,3ccvjb,RiffyDivine2,1 point,Tue Jul 7 13:14:52 2015 UTC,Yeah but it's annoying having to unhide posts that got -30 downvotes for putting the correct information out there. I wonder if coke and pepsi get as angry as we do?
hardware,3ccvjb,willxcore,1 point,Tue Jul 7 16:03:09 2015 UTC,Really? Let's see xfire performance and scaling.
hardware,3ccvjb,antemon,1 point,Tue Jul 7 17:54:49 2015 UTC,"For whatever its worth, I think this is good news.  Why?  Higher end cards not performing as well as the competetion should mean manufacturers will be forced to drop their prices. Which will in turn push lower end cards into even lower prices. Which in turn will mean o can afford a mid-low end range card because im poor."
hardware,3ccvjb,retolx,1 point,Tue Jul 7 16:42:16 2015 UTC,"Unless they won't lower prices... Otherwise we would soon have almost free graphics cards if your logic always applied.  Also nvidia currently owns the market (sadly) they don't really need to drop the prices. Some people don't even know about AMD being alternative, especially some newbies just entering PC building world don't realize there is something else than NVIDIA Geforce."
hardware,3ccvjb,iamstephen1128,2,Tue Jul 7 08:14:12 2015 UTC,Unless they won't lower prices... Otherwise we would soon have almost free graphics cards if your logic always applied.   Just gonna leave this here:  https://en.m.wikipedia.org/wiki/Reductio_ad_absurdum
hardware,3cemyq,Dora_LIU,5,Tue Jul 7 08:59:21 2015 UTC,US Only :(
hardware,3cemyq,VagSmoothie,5,Tue Jul 7 13:22:13 2015 UTC,just send them to linus
hardware,3cemyq,spellstrike,2,Tue Jul 7 16:18:29 2015 UTC,"I already have the dual arm mount from Amazon, its fantastic for the price!  Might apply for the laptop/monitor mount so I can finally have all three monitors + laptop mounted :P"
hardware,3cemyq,silver_tongue,2,Tue Jul 7 14:53:41 2015 UTC,"""They are open to anywhere"" OP please learn to read. It's clearly US only as soon as you open the application form."
hardware,3cemyq,tekwarfare,2,Tue Jul 7 20:43:03 2015 UTC,"While misleading, I think he means that the company is open to you posting the reviews anywhere. So I think the ""learning to read"" falls back in your court.  Edited for too many commas"
hardware,3cfyi5,vivalafica,6,Tue Jul 7 16:42:45 2015 UTC,"For non Bootable you are looking at a RAM disk, it allocates a portion of system memory for use as storage.   Benchmark of some of the softwares that allows this. https://www.raymond.cc/blog/12-ram-disk-software-benchmarked-for-fastest-read-and-write-speed/"
hardware,3cfyi5,rainbrodash666,5,Tue Jul 7 16:49:31 2015 UTC,"Currently using one for caching with ddr3 2133mhz Ram. It is indeed insanely fast.   Take a look at the PCIe 750 Intel SSDs with NVMe as well. If you want really fast storage and not spend thousands of dollars for an lsi controller to raid multiple sata SSDs, the Intel 750 is the way to go. But please do not buy the ahci version if you are already spending that amount of money.  As for non bootable storage (from consumer hardware), enterprise grade fibre channel storage is probably the way to go. They essentially raid multiple sas harddisks using the zfs file system."
hardware,3cfyi5,DavidRappl,1 point,Tue Jul 7 17:10:49 2015 UTC,What do you think storage in the next 10-20 years would be like? Flash based or something more exotic?
hardware,3cfyi5,dylan522p,2,Tue Jul 7 20:26:43 2015 UTC,"Reram nvram memristor, God knows what else."
hardware,3cfyi5,DavidRappl,2,Tue Jul 7 22:19:46 2015 UTC,"Since biological progress has been faster then technological progress in recent years, we might see DNA based storage f.e. But it will be a long way till something like this makes its way to consumers.   As for current technologies, faster and cheaper flash is always a safe bet ;-)"
hardware,3cfyi5,krista_,6,Wed Jul 8 06:48:31 2015 UTC,"Use a second machine as a HD!  Buy Infiniband equipment.  Make one computer a SSD backed RAM disk, and have it setup to host a PXE Boot Environmet  Have your FastBoot Computer PXE Boot off your Infiniband Connection.  FDR Infiniband is 56gbps per port, and two port cards are available.  (I'm going to play around with this this weekend.  I'm testing before upgrading my storage server with a new Xeon 1650v2 32GB Adaptec 81605zq -> 5x4tb SAS + 4 256GB 850 Pro.   While it's on my bench, I'll test the storage setup on the 2695v3 128GB Ram MB/CPU that is also on my bench at the moment)  E:  mb/gb.  I'm still in the 90's, apparently."
hardware,3cfyi5,dylan522p,2,Tue Jul 7 23:54:49 2015 UTC,So the second machine is always on?
hardware,3cfyi5,krista_,5,Wed Jul 8 01:00:46 2015 UTC,"Yes.  Also, the RAMDISK is backed with SSD, so upon idle or shutdown, the ram is written to SSD.  Obviously, upon boot, the RAMDISK is reloaded from SSD.  OR, what I plan to do, is tier my storage with RAM being tier 0 (essentially a cache), the 4x256GB 850 pros being tier 1, and the 5x4tb HDD being tier 2"
hardware,3cfyi5,dylan522p,2,Wed Jul 8 01:04:59 2015 UTC,A full shutdown to boot would take a while. why not just make a ramdisk on the same machine?
hardware,3cfyi5,krista_,3,Wed Jul 8 03:36:02 2015 UTC,"Can't boot off of it.  Besides, I build quiet computers that are quite energy efficient, so I leave them on.... especially the servers."
hardware,3cfyi5,krista_,3,Wed Jul 8 04:34:08 2015 UTC,"It's not for the faint of heart, but I'm expecting > 2.6GBps (GigaBYTES per second) of sustained read.  More if I add drives.   The theoretical max is around 7GBps.  Realistically, if I add more drives in a balanced manor, I'll get around 5.5Gbps."
hardware,3cfyi5,bb999,1 point,Wed Jul 8 01:09:58 2015 UTC,"You could go a lot faster with a RAID controller with a large number of ports on the local machine (like your Adaptec). A 24 port card could host 24 SSDs. If each SSD can do 500MB/s or so, you can theoretically get 12GB/s out of that card. And you can run as many cards as you have PCI slots.   Accessing a RAMDISK over an external interface seems like you would lose a lot of the benefits, such as ultra-low latency.   Btw, I'm running a 5x4TB array (RAID 5) on an Adaptec controller as well. Depending on the benchmarking tool, I get anywhere from 550MB/s to nearly of 900MB/s (have to run what is basically a low QD sequential read benchmark. 900MB/s is essentially the theoretical limit of 5 drives). The controller is a 71605 and the drives are WD 4TB SE drives."
hardware,3cfyi5,krista_,1 point,Thu Jul 9 02:38:28 2015 UTC,"Infiniband won't affect your latency much.  You can share graphics cards over it....to an extent, it can be thought of as a PCIE bridge/extension.  I'm running a 81605zq with 5x4TB HGS SAS and a MaxCache of 4x 256gb 850 pro ssds.  This is my primary SAN server.  I have a backup that has an 81605zq and 7x 2tb HGS coolspin sata and 2x 120gb corsair force 3 ssds.  All of these guys are on an FDR Infiniband network on an SX6512 switch."
hardware,3cfyi5,baskinmygreatness,2,Thu Jul 9 06:43:49 2015 UTC,SM 951 nvme if you can find it. SM951 ahci of you cant.   The intel 750 takes longer to boot and is more expensive for minimal  gains
hardware,3cfyi5,hdshatter,2,Wed Jul 8 11:05:36 2015 UTC,Fastest bootable for a normal PC would be a PCI SSD
hardware,3cfyi5,SmashedSqwurl,1 point,Tue Jul 7 18:11:42 2015 UTC,"A PCIe RAMdisk like this one is probably the fastest ""bootable"" storage you can get.  I put bootable in quotation marks because you'll need to make sure your computer never loses power in most cases (this one being an exception because it has solid state backup)."
hardware,3cfyi5,krista_,2,Tue Jul 7 18:29:48 2015 UTC,Cheaper to use a second machine as a SAN RAMDisk... And faster
hardware,3cfyi5,SmashedSqwurl,1 point,Thu Jul 9 00:41:58 2015 UTC,"Cheaper, probably, but a PCIe 3.0 x16 RAMdisk would be much faster and have better latency."
hardware,3cfyi5,krista_,3,Thu Jul 9 00:51:09 2015 UTC,"Maybe... But they don't make them.  The card referenced is $5k+ and is PCIE v1 x4.  They do make PCIE v3 16x Infiniband cards that have more network bandwidth than the PCIE slot.  PCIE v3 x16 is 128gbps  Infiniband FDR is 56gbps per port, with two ports per card for a total of 112gbps.  Infiniband EDR is 100gbps per port, with two ports per card for a total of 200gbps...which will max your slot out."
hardware,3cfyi5,SmashedSqwurl,1 point,Thu Jul 9 00:57:59 2015 UTC,"Well, my way would be simpler if it existed ;)  Infiniband EDR is nuts. Did they do that in anticipation of PCIe 4.0 or do they plan on CPUs having Infiniband controllers hooked directly to the QPI/HyperTransport bus?"
hardware,3cfyi5,krista_,3,Thu Jul 9 01:07:07 2015 UTC,"Probably a bit of both.  EDR is approaching ram speed of current generation devices, and although you can essentially map one machine's ram into another's, it's not NUMA compliant like a multisocket board.  Eventually this may change.  IB is used heavily in HPC.  For fun, you can pick up a pair of single port FDR cards on eBay for around $100 each, or $250 for dual port...although they are PCIE v3 8x, that's still 64gbps on the slot.  Cables are inexpensive on eBay as well, and with two computers, you don't need a switch, either!  I've got this working right now in my home lab, although I recently added a switch for more nodes, I'm not booting from it right now.  I'll probably try a Win 8.1 pxe boot from a remote ramdisk via dual port FDR in the coming week of two."
hardware,3cdp0f,Fatigue-Error,8,Tue Jul 7 02:43:28 2015 UTC,"If you've been champing at the bit (or chomping, if you want to get loosey-goosey with the original idiom) for Intel's Skylake-S launch, you can rest easy knowing that you're in the homestretch. Well, probably. While nothing is yet official, we've been hearing for some time now that Skylake-S chips will be announced at Gamescom on August 5, 2015, and that still appears to be the case.   That would be perfect timing for my new build this fall."
hardware,3cdp0f,NPCwars,6,Tue Jul 7 02:44:15 2015 UTC,Same here. Can't wait to finally build.
hardware,3cdp0f,mongees,5,Tue Jul 7 03:27:39 2015 UTC,I have an original i7 860 with stock dell mb. Is it worth upgrading?
hardware,3cdp0f,americosg,3,Tue Jul 7 15:05:16 2015 UTC,"Only if you're planning on building a new pc altogether. Since you would need a new motherboard, some new ram(Since it supports only ddr4 and ddr3l and from what I've heard this type of ddr3 is diferent from mainstream ones.), the processor itself. This would put you at least at the 600 dolars range. And to get the most out of the X170 chipset some new storage options like m.2 or PCI-E ssd would be advisable. Now you would have reached 800 dolars at the bare minimum, and it would be easy to top 1000 dolars.  But if you already plan on upgrading anyway, the jump from Z97 to x99 or z170 is worth it in my opinion since you get that extra 10-15% extra power and the already mentioned new storage options.  For me personally ddr4 is a most since I'm planning on build an overkill mITX build with a corsair bulldog or simillar and 32 gb ram. And from what I know this is only posible to do on an ITX motherboard on ddr4. I'm currently aiming at those sweet Corsair Vegeance 16gb sticks."
hardware,3cdp0f,yuhong,2,Tue Jul 7 17:22:04 2015 UTC,I think 1.5V DDR3 will probably work.
hardware,3cdp0f,americosg,1 point,Tue Jul 7 21:22:19 2015 UTC,"The upcoming “Skylake-S” CPUs in LGA1151 packaging for desktops will only officially support DDR4 and DDR3L memory with 1.2V or 1.35V voltages, according to sources with knowledge of the matter. The new microprocessors will not support standard DDR3 memory with 1.5V or 1.65V voltages, which are used by today’s high-end desktop personal computers."
hardware,3cdp0f,yuhong,1 point,Tue Jul 7 21:26:21 2015 UTC,I wonder why it is technically not supported.
hardware,3cdp0f,americosg,2,Tue Jul 7 21:28:37 2015 UTC,Maybe it's easier for the controller to manage this different type of ddr3 since it seems to have a power design closer to ddr4. Making it cheaper to implement the new controller or something... That would be my guess.
hardware,3cdp0f,IntelKabylake,1 point,Tue Jul 7 21:33:29 2015 UTC,Hoping to upgrade when the prices come down a bit. Currently running a Sandylake i5 setup and its showing its age a little bit.
hardware,3cdp0f,mongees,1 point,Wed Jul 8 07:17:12 2015 UTC,"So ram isn't backwards compatible? I know im going to need a new motherboard. Ill probably buy a new case for this upgrade. I've been waiting for this cpu, didn't know if I should wait for another."
hardware,3cdp0f,americosg,1 point,Thu Jul 9 15:08:32 2015 UTC,Yeah ram isn't backwards compatible. Try selling your old build since you seem to need to buy almost everything new anyway.
hardware,3cdp0f,random_guy12,2,Thu Jul 9 21:34:17 2015 UTC,Only if your current i7 isn't cutting it anymore.
hardware,3cdp0f,Teethpasta,1 point,Tue Jul 7 17:02:54 2015 UTC,Well you might as well unless you are ready to wait at least two more years at the very least for a new architecture update.
hardware,3cdp0f,Seclorum,1 point,Wed Jul 8 01:42:42 2015 UTC,"i7 920 here, and I would say it's nice to use a more modern chip for some things, but by and large even my 920 clocked stock is pretty fast for my workload.   I do notice a difference between this chip and my laptop with a i5 4000 something chip. The Laptop is wicked fast for some things.   Overall I would wait for Skylake before thinking of upgrading anyway. As the socket should be compatible with Skylake refresh and whatever else they put in between that and the delayed Cannonlake launch."
hardware,3cdp0f,XorFish,1 point,Wed Jul 8 02:42:53 2015 UTC,X5650/60 + overclock= i7 4790k stock performance.
hardware,3cdp0f,CykaLogic,1 point,Wed Jul 8 07:22:56 2015 UTC,You're not hitting 4GHz Haswell performance with 4.5GHz Nehalem.
hardware,3cdp0f,XorFish,1 point,Thu Jul 9 02:51:51 2015 UTC,of corse I'm not talking sbout single core performance.
hardware,3cdp0f,Warmo161,2,Thu Jul 9 06:43:11 2015 UTC,"I've been planning a new PC build for a few months, how would these compete with a 5820k? (Or are benchmarks even out yet?)"
hardware,3cdp0f,JustFinishedBSG,10,Tue Jul 7 07:36:33 2015 UTC,"Skylake should be barely if not at all better IPC wise than a 5820k.   The real important changes are:   MUCH better GPU ( you don't care I guess, good for laptops ) New chipset with much improved connectivity."
hardware,3cdp0f,Sremylop,1 point,Tue Jul 7 08:12:28 2015 UTC,Yeah an X99 system isn't going to be outdated probably any time soon
hardware,3cdp0f,hdshatter,2,Tue Jul 7 11:59:31 2015 UTC,"The X99 replacement are supposed to come out in 2016 and its going to be Broadwell not Skylake, so even less gains.  X99 already was more advanced than Z97 and it got an X99A refresht hat added USB 3.1 support. I don't see anything major coming out with Broadwell since PCI 4.0 won't be done till 2017."
hardware,3cdp0f,Seclorum,1 point,Tue Jul 7 17:05:08 2015 UTC,They canned Broadwell-E. Skylake-E is next for the enthusiast platform with a new socket.
hardware,3cdp0f,hdshatter,1 point,Wed Jul 8 02:45:58 2015 UTC,That was a rumor and the latest Broadwell-E news says its coming out Q1 2016. Only major change looks like Broadwell instead of Haswell so not worth waiting for...
hardware,3cdp0f,Seclorum,1 point,Wed Jul 8 08:23:29 2015 UTC,"Only information on Broadwell-e I can find is on WTFtech, not a reliable source."
hardware,3cdp0f,skilliard4,1 point,Wed Jul 8 15:38:28 2015 UTC,That's quite a bold statement when we haven't even seen benchmarks of the new Skylake CPUs yet
hardware,3cdp0f,Le_rebbit_account,1 point,Tue Jul 7 18:04:18 2015 UTC,"Well, we've seen the reported clock speeds which seem reasonably accurate. You can extrapolate from there assuming 10%+ ipc improvement."
hardware,3cdp0f,idoithere,1 point,Tue Jul 7 19:30:03 2015 UTC,really? source? I do not know how base clock speed relate to overclocking ability.  I thought intel/amd are lowering clock rates to save power
hardware,3cdp0f,Seclorum,1 point,Tue Jul 7 19:41:56 2015 UTC,On low end chips yeah. But Skylake-S is their high power desktop part.
hardware,3cdp0f,idoithere,0,Wed Jul 8 02:44:28 2015 UTC,"usually, the most important part about skylake s is that it is overclockable.  I dont think we can extrapolate overclocking potential based on base clock speeds."
hardware,3cdp0f,Seclorum,1 point,Wed Jul 8 07:03:12 2015 UTC,"MUCH better GPU ( you don't care I guess, good for laptops )   DX12 will allow the IGPU in supported titles to work in concert with discrete graphics cards.   It may not be much, but it's something."
hardware,3cdp0f,TeutorixAleria,1 point,Wed Jul 8 02:45:18 2015 UTC,If it's anything like hybrid crossfire it's going to be very hit and miss
hardware,3cdp0f,Seclorum,1 point,Wed Jul 8 11:55:27 2015 UTC,"It all depends on how well the Software and Engine devs can code their system, and what tasks they throw at it.   One task I'dd love to see them have by default, is the IGPU keeps the desktop rendered, so when you Alt-Tab out of a game, it doesn't have to sit there and unload the game and load the desktop, but instead it can just switch the output stream to the IGPU."
hardware,3cdp0f,maybe_just_one,2,Wed Jul 8 15:37:05 2015 UTC,"Specs seem underwhelming, the 6700K has a lower boost clock and a higher TDP than the 4790k.   Are there some architectural changes that will have speed improvements, or are they just dedicating the more resources to the iGPU I won't use?"
hardware,3cdp0f,Seclorum,1 point,Tue Jul 7 20:29:12 2015 UTC,"Supposedly some architecture improvements somewhere as well as they dropped the FIVR from the die due to heat concerns.   But a large portion of the die, more than half of it I believe, should be the IGPU part.   With DX12 it might see some use if devs code things correctly, because multiple mismatched parts can contribute to rendering in DX12 instead of requiring full mirroring for every part."
hardware,3cbz1n,TaintedSquirrel,11,Mon Jul 6 18:53:13 2015 UTC,"Had no idea pcie riser cards were a thing, where do you get them from? Do they come with the board? The case?"
hardware,3cbz1n,troublegoats,16,Mon Jul 6 19:58:56 2015 UTC,It comes with the case in this instance.
hardware,3cbz1n,Seclorum,8,Mon Jul 6 20:42:24 2015 UTC,Had no idea pcie riser cards were a thing   It seems you are not up to date on how bitcoin and litecoin and dogecoin mining used to be two years ago. Almost everyone used risers or extensions so they can fit more GPU's in a pc.
hardware,3cbz1n,hak8or,1 point,Tue Jul 7 00:46:49 2015 UTC,How many GPUs are we talking here?
hardware,3cbz1n,troublegoats,9,Tue Jul 7 02:56:47 2015 UTC,Lots.   Anywhere from 2 all the way to the stratosphere.   https://3s1shj19s0di2dovmhkw15h1-wpengine.netdna-ssl.com/wp-content/uploads/2013/06/The-Biggen-Super-Litecoin-Rig-mod-577x1024.jpg
hardware,3cbz1n,formerlybamftopus,6,Tue Jul 7 03:04:24 2015 UTC,"One per every PCI-E slot, so if your motherboard supports 9 PCI slots (PCI-E 16x and PCI-E 4x, etc, as long as it has PCI-E) then you can have 9 7950's running under one system."
hardware,3cbz1n,hak8or,3,Tue Jul 7 03:29:35 2015 UTC,"Though 8 GPUs used to be the maximum at least, not sure if that's been lifted yet. For example, I had one rig with 4x 5970s, which had 2 GPUs each for 8 total GPUs, which was the maximum."
hardware,3cbz1n,bphase,5,Tue Jul 7 11:35:23 2015 UTC,Risers are very much a thing on rackmount systems. They're just not terribly common on consumer systems.
hardware,3cbz1n,wewd,8,Mon Jul 6 20:52:02 2015 UTC,Anybody happen to know a release date for this case?
hardware,3cbz1n,Wusiji_Doctor,3,Mon Jul 6 19:48:05 2015 UTC,I wrote to Fractal a bit more than a week back. They replied that the cases were just about to leave from production in China and were expected to hit retail in Europe in August.
hardware,3cbz1n,nitpickr,1 point,Sun Jul 12 17:59:22 2015 UTC,Awesome! Thanks buddy!
hardware,3cbz1n,Wusiji_Doctor,3,Mon Jul 13 01:02:18 2015 UTC,"If you don't want to wait, Lian-Li has been making these ""console killer"" cases for years, starting with the PC-Q12 and now with the PC-Q19."
hardware,3cbz1n,Y0tsuya,3,Mon Jul 6 21:14:36 2015 UTC,"Yep, and there are several offerings from Silverstone too. The RVZ01 gets a lot of attention, and the ML07B looks solid too. I just prefer how the Fractal looks, so if it's coming out around/before the GTX 950 launch (inevitably dropping the price of used 750ti's, which is what I want for my HTPC build), I'd love to get my hands on it. Otherwise, yeah, the PC-Q19 or ML07B would be the way to go"
hardware,3cbz1n,Wusiji_Doctor,6,Mon Jul 6 21:24:04 2015 UTC,"Don't forget the FTZ01! Don't see it around very often but I have one myself and love it. Very similar to this case, albeit pricier."
hardware,3cbz1n,deyam,2,Mon Jul 6 21:51:53 2015 UTC,"FTZ01 also supports a slimline slot load dvd/bluray drive, which may be something people are interested in for an HTPC. I think the FTZ01 has similar dimensions, but is a little taller."
hardware,3cbz1n,jagilbertvt,1 point,Wed Jul 8 02:48:39 2015 UTC,Dont forget the RVZ02!
hardware,3cbz1n,OyabunRyo,12,Tue Jul 7 15:36:27 2015 UTC,I was wondering when they'd take mini-ITX seriously.  Nice job FD.
hardware,3cbz1n,user3404,9,Mon Jul 6 20:19:09 2015 UTC,They do have the node 304
hardware,3cbz1n,Isaactsg,3,Tue Jul 7 01:54:25 2015 UTC,Also the Core 500.
hardware,3cbz1n,Seclorum,3,Tue Jul 7 06:14:12 2015 UTC,I like that they aren't doing reviews in front of the big window any longer.  I was interested in this case when it was announced earlier. But it's bigger than I thought. I'm looking for a replacement for a mini-box.com m350 for almost pure vanity.
hardware,3cbz1n,unsaltedbutter,6,Mon Jul 6 19:14:45 2015 UTC,But it's really nice during the winter season with the snow in the background.  http://www.youtube.com/watch?v=X_YioNlc298&t=0m18s
hardware,3cbz1n,bl1nds1ght,1 point,Mon Jul 6 19:15:57 2015 UTC,psssst... Ncase M1 v4
hardware,3cbz1n,Have_you_tried_this,3,Mon Jul 6 21:27:20 2015 UTC,The ncase? Is it still about double the cost of every other small case?
hardware,3cbz1n,NCASEdesign,9,Mon Jul 6 22:48:31 2015 UTC,"It's still the same price. Low volume production + high quality construction is expensive. We're a very small operation (two people), and we don't have the resources a larger company would have to reduce cost through volume manufacturing techniques. Basically, we either do it at this price or not at all. People can decide if they feel it's worth it.  A note on quality: watch HardwareCanucks' review of the M1, and note the lack of chassis flex. Then watch any of Dmitry's other case reviews where he checks for flex. It's not the be-all and end-all of quality, but it is an indicator. And if you're happy with thin steel and molded plastic cases, then by all means, don't let myself or anyone else tell you you shouldn't be. We're just offering something a cut above the usual mass-market stuff, for those who are interested in it."
hardware,3cbz1n,Seclorum,1 point,Mon Jul 6 23:20:49 2015 UTC,Man that looks like a sweet case!
hardware,3cbz1n,bl1nds1ght,1 point,Tue Jul 7 06:22:58 2015 UTC,"Corsair has mATX and ITX cases in the $180 region, as do a few other mainstream manufacturers.  There's nothing particularly extraordinary about the M1's price, especially considering its rarity.  If you're looking for a unique case that totally fits the bill, that's it."
hardware,3cbz1n,sdrykidtkdrj,3,Mon Jul 6 23:06:36 2015 UTC,Also competes with the $70 Silverstone ML07B in addition to the mentioned $85 Silverstone RVZ01B.
hardware,3cbz1n,americosg,3,Mon Jul 6 22:22:54 2015 UTC,What is the best cooler that fits this case? It has only 56mm CPU cooler clearence...
hardware,3cbz1n,kerradeph,2,Tue Jul 7 14:26:12 2015 UTC,The Noctua NH-L9i/NH-L9a would be a decent option. but there might be some better options that are closer to the max size that will fit in this case. I was looking at that one in particular as it will fit nicely on the Maximus VII Impact.
hardware,3cbz1n,americosg,1 point,Sat Jul 11 09:30:18 2015 UTC,"After some research I didn't found any cooler that made me feel confortable. Right now I'm thinking about getting a Silverstone FTZ01 wich is pretty classy in aluminum. The case has 83mm of cooler clearence, so I plan on getting Noctua NH-L12 wich is 93mm with both fans. I will remove the top one and put a  Prolimatech Ultra Sleek Vortex 12. That way the height will be 82-83mm in total.  I hope to archieve enough cooling power to cool a 95W processor a bit overclocked without getting too much noise."
hardware,3cbz1n,Juheebus,1 point,Sat Jul 11 12:08:19 2015 UTC,The recently announced cryorig c7 seems like it's going to be a great option.
hardware,3cbz1n,Seclorum,6,Wed Jul 8 15:56:50 2015 UTC,"Only things I'd like to see changed is to make the magnetic filters accessible from outside, without needing to disassemble the case.  And to add some kind of 3.5 inch drive space."
hardware,3cbz1n,stabsthedrama,2,Mon Jul 6 21:46:30 2015 UTC,For a micro itx build you cant really have a 3.5 bay in mind. If anything you go external.   With the price of ssd's now at under $100 for 250gb within the next few years very few new cases will even utilize 3.5.   Crazy to think bit 3tb ssds will soon enough be the price that current platter 3tbs are.
hardware,3cbz1n,Seclorum,6,Tue Jul 7 03:39:11 2015 UTC,"Crazy to think bit 3tb ssds will soon enough be the price that current platter 3tbs are.    The problem comes with even bigger platter drives keep coming out at cheap prices.   I definitely think the sensible option for today is still an SSD paired with at least one large data storage drive.   But those 1tb SSD's are already hitting the point where I would consider them a viable option for mass storage.   But as you said, if you want a big storage platter drive, get an external. Hell, for a couple years my Steam folder lived off one while I gamed on a laptop. Loading times were not that bad even on USB2.0."
hardware,3cbz1n,dexter311,1 point,Tue Jul 7 04:54:21 2015 UTC,"2.5"" spinners are also an option."
hardware,3cbz1n,bhyndman,2,Tue Jul 7 08:07:11 2015 UTC,When will this be available in Canada?
hardware,3cbz1n,kerradeph,1 point,Tue Jul 7 01:53:13 2015 UTC,Yeah. I've gotten most of the parts sorted for a build I'd like to make. I just want to know how much the entire thing would be as without a case having a computer like this is a little disappointing.
hardware,3cbz1n,Clob,2,Sun Jul 12 09:47:53 2015 UTC,How would a Fury X fit with it's water cooler?
hardware,3cbz1n,Wusiji_Doctor,6,Mon Jul 6 19:11:25 2015 UTC,I don't think the water cooled card would. You'd likely have to wait until they release the air-cooled models
hardware,3cbz1n,svceon,2,Mon Jul 6 19:47:17 2015 UTC,"I'm speaking out of my mouth, but the fury x is very small compared to the gpu in the video, also, there is a lot of space for two fans between the GPU and the case side. The radiator would fit in there no problem."
hardware,3cbz1n,NCASEdesign,3,Mon Jul 6 21:09:03 2015 UTC,"The Fury X with its radiator is definitely significantly larger than the reference nvidia card used in the video, and there is in reality very little chance of it fitting."
hardware,3cbz1n,Seclorum,1 point,Mon Jul 6 22:47:54 2015 UTC,"The Fan+Radiator unit is 65mm+ thick, and the FuryX itself is 40mm thick. There just isn't room next to it to fit side by side like that, the forward mount doesn't have the clearance on the card either.   You 'might' be able to fit it, if you install a custom loop just for the gfx card and take the card down to a single slot size, but it will be tight as hell."
hardware,3cbz1n,Seclorum,2,Tue Jul 7 00:34:57 2015 UTC,"You might be able to squeeze the cooler into one of the 120mm fan mounts right in that chamber, but the tubes would get in the way and the rad/fan are pretty thick.   You would probably be able to squeeze a straight Fury or the Nano in there just fine.   The Dual Fury might also fit depending on just what kind of crazy cooling solution they mate it with."
hardware,3cbz1n,NCASEdesign,2,Mon Jul 6 20:46:41 2015 UTC,"I wouldn't count on it fitting. The Fury X radiator is 65mm thick, while the card itself is the standard ~40mm dual-slot thickness, so there's no way you could have the rad and card side-by-side in the Node 202, which is only 82mm thick.  There's also not enough space in front of the card, since it's 194mm long and the rad is at least 120mm, and that doesn't take into account the tubing. It might seem like it would just barely fit on paper with the Node 202 coming in at 330mm deep, but that includes the PCI tab at the back of the case (~10mm) and the chassis material thickness itself. The front I/O PCB also extends 20-25mm into the case, which would interfere with the radiator. Add tubing into the mix and there's no way it's going to work.  Edit: Image for illustration"
hardware,3cbz1n,Yummerzzzz,5,Mon Jul 6 22:46:12 2015 UTC,"I think this is my perfect Mini-ITX case, is that the maximum PSU supported? I'll have to go Nvidia if so, I think with my 2500K instead of a R9 390 :("
hardware,3cbz1n,gundamsudoku003,10,Mon Jul 6 19:28:03 2015 UTC,"Supports sfx and sfx-l, Silverstone has a 650w sfx psu out now and a 750w one releasing soon."
hardware,3cbz1n,Seclorum,2,Mon Jul 6 20:30:12 2015 UTC,"That was one of the only issues I would have with a case like this!  I didn't know if the PSU could be swapped with something a bit bigger than 450w.   Given that there isn't much you can put in there, I wouldn't really worry about anything more than 650w just to have some good headroom if you wanted to overclock."
hardware,3cbz1n,bl1nds1ght,5,Mon Jul 6 20:44:25 2015 UTC,"Bro, 450W will run the vast, vast majority of systems out there, including i7/Titan (no oc).  A sfx 500w or 600w from Silverstone will be more than enough, even with light to moderate oc'ing.  See the 600w (here)."
hardware,3cbz1n,Seclorum,2,Mon Jul 6 21:30:53 2015 UTC,"Oh I know, but looking at some GFX cards drawing almost 300 watts themselves and more when they are overclocked, and then you have 50-80w for the CPU/Memory and a couple watts for the drives.   I like to have a good margin on my PSU, which is why the 600-650w range is where I like to build for. 450w just feels way too close to the edge for my tastes but it could work for a lot of builds.   The fact that there are options nowadays for PSU's in this form factor and at the capacity makes this kind of build attractive to me."
hardware,3cbz1n,bl1nds1ght,2,Mon Jul 6 21:44:12 2015 UTC,"The fact that there are options nowadays for PSU's in this form factor and at the capacity makes this kind of build attractive to me.   Oh definitely.  I was looking to build an ITX rig in 2013, but it was still a bit expensive then.  Things have come down in price since that time."
hardware,3cbz1n,formerlybamftopus,0,Mon Jul 6 21:50:49 2015 UTC,"Without watercooling,  overclocking and mITX dont exactly go together. Not enough space to evacuate all the heat produced."
hardware,3cbz1n,Seclorum,4,Tue Jul 7 03:07:49 2015 UTC,Extreme overclocking maybe. But it looks like more than enough airflow for some modest clocks.
hardware,3cbz1n,Yummerzzzz,1 point,Tue Jul 7 03:10:21 2015 UTC,"Yeah just read into that, I'm looking at £67 for a 500W SFX, the minimum I'd say for a 390, or keep the 450W and have a 970....I think the Mini-ITX investment is too large for me right now."
hardware,3ccyus,TaintedSquirrel,15,Mon Jul 6 23:08:49 2015 UTC,"We have received feedback that during open bench testing a small number of Fury X cards emit a sound from the high speed liquid cooling pump that, while not loud, is bothersome to some users. While the vast majority of initial Fury X owners report remarkably quiet operation, we take this feedback seriously, as AMD’s mission is to always deliver the best possible experience to our Radeon customers.  AMD Radeon R9 Fury X customers demand and deserve the best, so adjustments in the sound baffling adhesive compound were applied in the assembly of the high speed cooling pump to address the specific sound a few end users experienced as problematic. This improved the acoustic profile of the pump, and repeat testing shows the specific pitch/sound in question was largely reduced through adjustments to the sound-baffling adhesive compound in the pump.  AMD will work with its graphic card partners to ensure the satisfaction of the small number of initial customers who observed this specific sound and experienced it as bothersome. AMD is confident that on-going production of Radeon R9 Fury X product reduces the specific sound in question, but this is also a highly subjective matter with wide differences in PC case builds and room acoustics.  The AMD Radeon™ R9 Fury X radiator fan is near silent, and this makes any sound from the high-speed pump more noticeable to some end users, especially during open bench testing. Thus although the overall sound levels are remarkably low for an enthusiast product, AMD has worked to reduce the specific sound that some customers report as bothersome."
hardware,3ccyus,defiance158,32,Mon Jul 6 23:09:37 2015 UTC,"The AMD Radeon™ R9 Fury X radiator fan is near silent, and this makes any sound from the high-speed pump more noticeable to some end users   That's a good point .....  Take any high end GPU off the shelf and it'll likely have 2-3 fans generating an insane amount of noise, which we are accustomed to.  Now take the Fury and remove all the fans from the card, and move the single low noise fan away from the GPU and put it on the radiator away from the card....... suddenly the pump is going to sound unreasonably loud, because that's the only damn thing you're gonna hear.   It's like having a snowblower and not giving a damn about the noise the engine and blower made, but you traded the snowblower for a shovel and now you are complaining about the noise the shovel is making instead of giving the shovel credit for being quieter."
hardware,3ccyus,ZandalarZandali,23,Tue Jul 7 01:04:25 2015 UTC,I sound-proofed my rig last year and now the audible noise of air moving through my case is more annoying than the fans themselves.  Getting rid of fan noise itself introduces a whole new set of noises you have to deal with.  So I understand where they are coming from.
hardware,3ccyus,TheImmortalLS,11,Tue Jul 7 02:07:11 2015 UTC,And those who have made systems with no fans and SSDs find themselves annoyed by electronics making high pitched whines...
hardware,3ccyus,ExogenBreach,4,Tue Jul 7 04:56:57 2015 UTC,"And then there's me, sad because of ambient noise (rip dorms)"
hardware,3ccyus,JustaPassanger,8,Tue Jul 7 05:49:27 2015 UTC,All that money you guys could have spent on headphones.
hardware,3ccyus,teuast,5,Tue Jul 7 06:39:02 2015 UTC,"That's not quite right though, there's noise and then there's noise. E.g. two fans show the same level of noise but one can be more annoying because it's clicking or scraping etc... The audio samples of the Furyx pump noise I've listened to were pretty annoying. There are mixed reports though, not all of them have this noise issue."
hardware,3ccyus,One_Mikey,1 point,Tue Jul 7 05:16:43 2015 UTC,"Everybody says the recorded samples were annoying, and I legit couldn't hear anything. I don't get it, I guess."
hardware,3ccyus,teuast,2,Tue Jul 7 17:47:01 2015 UTC,"It could be of such a high pitch that your ears can't pick it up, though I haven't tried listening myself."
hardware,3ccyus,longshot2025,1 point,Tue Jul 7 19:15:17 2015 UTC,"Doubtful. I'm 20 and have pretty good hearing. Maybe I'm just used to higher noise levels or don't care about noise levels because I wear headphones most of the time. In any case, I do feel like people are making a mountain out of a molehill here."
hardware,3ccyus,MaloWlol,1 point,Tue Jul 7 19:21:54 2015 UTC,"Some people aren't as sensitive to high pitch as others, regardless of age. I couldn't last a minute listening to some of the clips, but I know from experience that kind of noise bothers me more than others."
hardware,3ccyus,Seclorum,2,Wed Jul 8 16:54:45 2015 UTC,"While there's some truth to what you're saying the actual problem is that the pump noise is so loud that it's louder than the fans on a normal air-cooled card would be.  Sweclockers got a version of the Fury X with pump noise and made some recordings of it so you can see for yourself: http://www.sweclockers.com/test/20730-amd-radeon-r9-fury-x/18#content  If you don't understand Swedish they're in this order: Fury X idle, GTX 980ti idle, Fury X load, GTX 980ti load. Or just use google-translate to read their thoughts about it as well."
hardware,3ccyus,darrenphillipjones,5,Tue Jul 7 11:57:27 2015 UTC,"So basically, they changed the glue in the pump?"
hardware,3ccyus,TexasJefferson,8,Tue Jul 7 00:19:53 2015 UTC,I'm always conflicted by this stuff. A few users? Like every other review I read said the pump noise was annoying.
hardware,3ccyus,Idkidks,4,Tue Jul 7 01:05:35 2015 UTC,"Meh, marketing department demands it. As long as they are taking the appropriate steps to fix it, don't get caught up in how they try to minimize it."
hardware,3ccyus,Nixflyn,7,Tue Jul 7 04:00:31 2015 UTC,Review samples were one of the first off the lines.
hardware,3ccyus,Idkidks,4,Tue Jul 7 01:22:28 2015 UTC,Even when reviewers bought retail card after the fact they found that the noise had increased compared to review samples.
hardware,3ccyus,glr123,7,Tue Jul 7 01:48:24 2015 UTC,"Those were made along with the review samples, again, first off the lines."
hardware,3ccyus,nbiscuitz,0,Tue Jul 7 01:56:14 2015 UTC,And most of those do open bench testing. They dont' put them into cases.
hardware,3ccyus,darrenphillipjones,1 point,Tue Jul 7 16:07:06 2015 UTC,"I can still hear mine through the case, it's very annoying sound to me. Currently under RMA, will see what happens."
hardware,3ccyus,glr123,1 point,Wed Jul 8 11:12:15 2015 UTC,"Do t know why people keep defending AMD. You know big review sites are going to test noise, and you completely disregard it as an issue. Really sloppy."
hardware,3ccyus,TheImmortalLS,4,Tue Jul 7 16:22:48 2015 UTC,"I hardly disregarded it. Open bench testing is a fair point, and AMD is coming out in front of this. I don't see how I'm defending them blindly, it's better than a 3.5gb cover-up."
hardware,3ccyus,jinxnotit,8,Tue Jul 7 16:37:06 2015 UTC,"I'm reading the comments and this one guy comments after a long thread of amd hate   ""I haven't owned an amd card since the 4XXX, but some people seem very eager to suck the 980 ti's dick """
hardware,3ccyus,TheImmortalLS,6,Tue Jul 7 05:48:27 2015 UTC,"My favorite is the deeper you read, the bigger the performance differential between the Fury X and the 980ti.  ""It's 5-10% slower on average of mostly Gameworks games!"" ""Why but that Fail X when it's 10-15% slower!""  My eyes strained a muscle and I had to stop reading."
hardware,3ccyus,Themightyoakwood,4,Tue Jul 7 07:38:02 2015 UTC,"If you read comments for their entertainment value, you must remember to be able to vent stress like your gpu vents heat  I personally avoid the problem altogether by not even getting mad. ""why are these people so stupid?"" is my alternative to taking it personally.  also, oh god differential. it's like they're trying to be smart. it's like me saying, the amd card is lowerly."
hardware,3ccyus,jinxnotit,10,Tue Jul 7 14:57:00 2015 UTC,"I love how the biggest problem with the Fury X is the pump noise. Like, fuck that, I'm buying a 980ti."
hardware,3ccyus,MaloWlol,8,Tue Jul 7 04:21:22 2015 UTC,It's no 3.5GB but it's something!
hardware,3ccyus,glr123,2,Tue Jul 7 07:31:53 2015 UTC,"I don't think the biggest problem with the Fury X is the pump noise, I think it is that it didn't manage to beat the 980ti in performance, not even at the marketed 4k.  But the pump noise is a legit concern when it makes the card louder than the GTX 980ti purely because of it. Sweclockers got a version of the Fury X with pump noise and made some recordings of it so you can see for yourself: http://www.sweclockers.com/test/20730-amd-radeon-r9-fury-x/18#content  If you don't understand Swedish they're in this order: Fury X idle, GTX 980ti idle, Fury X load, GTX 980ti load. Or just use google-translate to read their thoughts about it as well."
hardware,3ccyus,OftenSarcastic,2,Tue Jul 7 11:55:55 2015 UTC,"I at least approve of AMD coming out and being upfront with it. The Nvidia thing still makes me angry, getting less than you purchased and performance deterioration as well. AMD has done stupid things too, no doubt about that, but they could have just swept it under the rug."
hardware,3ccyus,kennai,2,Tue Jul 7 16:06:39 2015 UTC,Does anyone have a link to a video of the noise with a decent sound setup? The ones I've seen have stuffed the microphone/camera right into the case to get it to pick up the noise and I'm having a hard time taking them seriously.
hardware,3ccyus,OftenSarcastic,2,Tue Jul 7 08:44:51 2015 UTC,"What do you mean by decent sound setup? If you mean a surround speaker system, then you'll hear it as long as the case is less than 4 feet from you. Once you start moving the case to around 5-10 feet the noise it emits starts to be almost impossible to hear over any noise generating device. In a close back headphone sound system you'll next to never hear the device unless it's very close to you. In an open back headphone system, you will almost never hear the computer over any other sound.  This is from my personal experience, so your mileage my vary. My hearing isn't the best, but it's not bad or at least I like to think that and I haven't been proven otherwise so far."
hardware,3ccha3,olavk2,14,Mon Jul 6 20:57:34 2015 UTC,What 20nm products are we talking about?
hardware,3ccha3,TaintedSquirrel,11,Mon Jul 6 22:01:53 2015 UTC,I honestly dont know.
hardware,3ccha3,daubertMotion,1 point,Mon Jul 6 22:09:20 2015 UTC,Probably die shrinks for the console APUs
hardware,3ccha3,spyder25000,1 point,Tue Jul 7 20:17:33 2015 UTC,They were going to make some APU's on 20nm that's it. Still using bulldozer arch so nothing special
hardware,3ccha3,Exist50,3,Tue Jul 7 02:41:41 2015 UTC,I don't think any Bulldozer products were scheduled for 20nm
hardware,3ccha3,jinxnotit,-11,Tue Jul 7 02:43:10 2015 UTC,Come oooonnn 14nm 300X and APU's!
hardware,3ccha3,jinxnotit,9,Mon Jul 6 21:03:56 2015 UTC,14nm isnt coming before next year sadly ;-;
hardware,3ccha3,Teethpasta,-12,Mon Jul 6 21:11:11 2015 UTC,Maybe. But the fact they're ahead of the curve is great news.  Now we just need to execution from AMD.
hardware,3ccha3,jinxnotit,14,Mon Jul 6 21:12:36 2015 UTC,Uh they are far behind? Finfet is already out on 14nm.....
hardware,3ccha3,Teethpasta,2,Mon Jul 6 23:17:19 2015 UTC,"I don't know about far behind, as much as it is Intel is that far ahead of everyone else."
hardware,3ccha3,internetosaurus,5,Mon Jul 6 23:50:58 2015 UTC,Samsung is there as well. It's split 50/50 as to what is behind or ahead.
hardware,3ccha3,Teethpasta,9,Tue Jul 7 00:44:17 2015 UTC,"Samsung's 14 nm process is behind Intel's. The various nm measurements are actually somewhat of arbitrary names.  This article from SemiAccurate gives a pretty good explanation of the difference, and this article from ExtremeTech has a helpful graphic to show what the difference would be."
hardware,3ccha3,jinxnotit,3,Tue Jul 7 01:09:05 2015 UTC,Yes yes I'm aware. I knew I should have included that in there so someone wouldn't post that. I'm sure most people are well aware by now. Cool graphic though
hardware,3ccha3,makar1,-2,Tue Jul 7 13:44:09 2015 UTC,"Samsung, Glofo, TSMC don't have the money to put in to node shrink that Intel does. Samsung may have the better 14nm process though."
hardware,3ccha3,SPOOFE,7,Tue Jul 7 01:14:26 2015 UTC,Intel have had 14nm APUs on the market for several months.
hardware,3ccha3,jinxnotit,2,Mon Jul 6 23:35:42 2015 UTC,"Technically, Intel doesn't make APU's."
hardware,3ccha3,cuddlefucker,-5,Tue Jul 7 18:33:09 2015 UTC,"And Nvidia is still talking about Pascal.  If it's applied to GPU's that puts them ahead of the curve.  And before you mention Iris Pro, it's not a dedicated GPU."
hardware,3ccha3,jinxnotit,9,Mon Jul 6 23:50:27 2015 UTC,"I think you are making a fatal error in your logic here. AMD and Nvidia don't fab their own chips. They rely on other companies fabs. Usually the same companies. The only way for one of them to jump ahead of the other would be for one of them to get a contract with samsung at this point, which seems unlikely. Pascal is very likely going to be on TSMCs 16nm FinFET process. 20 nm was too slow to fruition, and didn't provide the performance gains that people wanted.   I'm all for progress, but rushing a product to a new node isn't going to do anyone any favors."
hardware,3ccha3,CPU-Architecture,1 point,Tue Jul 7 00:26:17 2015 UTC,What makes you think it's rushed?  Samsung has been cranking 14nm chips out for a while now. So it's not like it's brand new tech that hasn't matured.
hardware,3ccha3,jinxnotit,5,Tue Jul 7 01:15:55 2015 UTC,Samsung have been cranking 14nm for low power SoC. Nothing of interest of a high performance dGPU.
hardware,3ccha3,SPOOFE,1 point,Tue Jul 7 11:42:53 2015 UTC,"Once you shift to 14nm they are all going to be low power. It's why anyone expecting huge leaps in performance from the process change over is going to be greatly disappointed.  And GloFo has two variants who collaborated with Samsung on 14nm production with 14nm LPP for high performance low power. And early production, low power 14nm LPE.  It's not unreasonable to expect Samsung to have similar variances in their node as well."
hardware,3ccha3,CPU-Architecture,1 point,Tue Jul 7 16:06:40 2015 UTC,"Once you shift to 14nm they are all going to be low power.   A given process can be tuned to have either a high performance at the expense of needing more power, or low-wattage for mobile battery-powered devices with the trade-off being that they can't be ramped up.  20nm processes came out on a low-power process, but no high-performance."
hardware,3ccha3,cuddlefucker,1 point,Tue Jul 7 18:31:48 2015 UTC,"I am talking about the lithography characteristics. Some will be optimize for low power, some for high density and other for high performance.  They will not all be low-power. I think you got something wrong about low power process."
hardware,3ccha3,jinxnotit,1 point,Wed Jul 8 10:57:12 2015 UTC,Just because the fab is mature doesn't mean that the architecture of Nvidia or AMDs chips is optimized for it. Making an end product is a lot more complicated than just pairing their tech to a new fab. The results would be completely underwhelming if that's all they did. They'll likely have to come up with a whole new architecture for the migration to FinFETs.
hardware,3ccha3,cuddlefucker,-2,Tue Jul 7 01:19:17 2015 UTC,"Of course they will have to make a new architecture. Do you always talk down to people?  If they made the announcement for their investors, that they are moving from 20nm to 14nm FinFets how long do you think they have been working on it planning for 14nm?  If they launch 14nm and offer competitive performance to Nvidias 900's while consuming MUCH less power how on earth could that possibly be considered a ""rushed"" product?  If anything it's just smart planning for Arctic Islands. Especially if the rumor is true that Pascal silicon has already been taped out."
hardware,3ccha3,dylan522p,1 point,Tue Jul 7 07:29:13 2015 UTC,"If anything it's just smart planning for Arctic Islands. Especially if the rumor is true that Pascal silicon has already been taped out.   It would be smart for them to have planned their next architecture for  smaller FinFET processes. That doesn't mean that they are even close to on the same page as Nvidia. Nvidia has a R&D budget greater than AMDs in spite of the fact that they are only developing video cards, and not the whole host of products that AMD works on."
hardware,3ccst1,random_digital,4,Mon Jul 6 22:22:49 2015 UTC,TSMC pls.
hardware,3ccst1,Perunsan,2,Mon Jul 6 22:28:58 2015 UTC,"Second, the company is taking a charge of $33 million associated with a change in its product plans. Several of its chips currently in development were initially targeted to use TSMC's 20-nm fabrication process, but the company has decided to move those chips over to a more advanced FinFET-based process technology instead."
hardware,3ccst1,logged_n_2_say,3,Tue Jul 7 00:03:44 2015 UTC,"AMD closed after hours trading down 13.8% to $2.13, creating new 52-week lows along the way."
hardware,3ccst1,zmeul,3,Tue Jul 7 06:31:12 2015 UTC,This is going to really hurt. I doubt they were optimistic to begin with.  Edit: Just check AMD's stock... Brutal.
hardware,3ccst1,Exist50,1 point,Tue Jul 7 03:58:52 2015 UTC,"Consumer demand falls, and changes to the process increase cost. Nothing to see here.  AMD has enough cash and ABL to survive until 20nm and Zen."
hardware,3cb2cn,varuntis1993,10,Mon Jul 6 14:51:16 2015 UTC,"This'll be very exciting once pricing becomes more accessible. I'm not going to drop a grand on an SSD, but I know it's only a matter of time"
hardware,3cb2cn,Wusiji_Doctor,3,Mon Jul 6 19:51:56 2015 UTC,Any word on pricing for these?
hardware,3cb2cn,ColsonIRL,8,Mon Jul 6 16:11:06 2015 UTC,"http://www.engadget.com/2015/07/06/samsung-2tb-ssd/   the 'standard' 2TB 850 Evo is officially priced at $800, while the slightly faster, longer-lasting 850 Pro could easily break the bank at $1,000."
hardware,3cb2cn,igacek,4,Mon Jul 6 16:20:36 2015 UTC,I'll take 20
hardware,3cb2cn,bdt13334,2,Mon Jul 6 17:22:44 2015 UTC,"damn, my 1TB pci-e ssd was around 800. Now you can get double the storage for that price."
hardware,3cb2cn,poematik,1 point,Tue Jul 7 01:10:28 2015 UTC,"1TB SSD's used to cost around 3-4k a few years ago, so 1k for 2TB ain't all that bad."
hardware,3cb2cn,Net_Bastard,1 point,Mon Jul 6 20:24:29 2015 UTC,Yep. See here:  link
hardware,3cb2cn,schwiing,16,Tue Jul 7 10:54:19 2015 UTC,"Waste of money at this point. Better off waiting for the 1TB drives to drop to under $200, which probably won't take very long. I'd love a big SSD for games but even 500GB isn't enough with game installs reaching 30-50+ GB now."
hardware,3cb2cn,Bickell,9,Mon Jul 6 16:19:24 2015 UTC,That would be a 50% reduction in price. Which would likely take over 12 months.
hardware,3cb2cn,wanking_furiously,2,Mon Jul 6 17:12:09 2015 UTC,"Do you need to have 10 games installed at once? I'm getting by just fine with a 128gb drive dedicated to games. And I don't think 1 TB drives are going to drop that low for a long time, especially with the M.2 switch."
hardware,3cb2cn,foxtrot1_1,11,Mon Jul 6 19:23:51 2015 UTC,"Need, no. But are you going to delete 30GB games to download them later? Or manually manage files? Not the end of the world, but I'd rather be gaming. Just a question of budget vs time vs size of games"
hardware,3cb2cn,fliphopanonymous,3,Mon Jul 6 17:24:46 2015 UTC,"It would be nice if you could set up Steam to use a set amount of space in one drive as a sort of cache for another drive in order to solve the manually managing files problem. e.g. 128 GB SSD and 1 TB HDD, set your Steam library to the HDD but give it like 60 GB on the SSD to use as cache.   They already have data on what you play the most, it would be relatively trivial to implement."
hardware,3cb2cn,bunchajibbajabba,2,Mon Jul 6 18:00:20 2015 UTC,I think that'd require Steam to constantly monitor your HDD space. The tinfoilers would go nuts if Steam didn't directly tell them it would.
hardware,3cb2cn,fliphopanonymous,0,Mon Jul 6 18:12:07 2015 UTC,"Not really. Think of it this way: you select a location on that drive like you normally would when you make a new Steam library location, except this time it gives you a little slider and asks ""how much space would you like the cache to use?"" Steam then creates a file of that size and stores the cache in that internally."
hardware,3cb2cn,gaggra,2,Mon Jul 6 20:35:12 2015 UTC,"What you're talking about is SSD caching, and it's readily available on Linux through bcache or LVM or the ZFS l2arc. Given that Steam is available on Linux, this sort of setup isn't out of reach."
hardware,3cb2cn,fliphopanonymous,1 point,Mon Jul 6 22:19:42 2015 UTC,It's actually the set up I have (using bcache)
hardware,3cb2cn,Darkerson,1 point,Tue Jul 7 06:59:45 2015 UTC,"Well the good news is you dont need steam to be able to do that. Its called Intel Smart Response Technology. The bad news is its not on every motherboard, but it is out there and has been for a while. I have it on my AsRock Extreme3 Gen3 and it really did make a difference, back when I only had a 64GB SSD and a 2TB Hard drive."
hardware,3cb2cn,fliphopanonymous,3,Tue Jul 7 14:32:57 2015 UTC,"Well first of all is only for Intel chipsets, second it uses a whole drive.   What I'm taking about is more like an application specific, and wouldn't necessarily use a whole drive, like bcache"
hardware,3cb2cn,Darkerson,1 point,Mon Jul 6 22:02:23 2015 UTC,"I know its not exactly what you wanted, I was just pointing out an alternative option. I wish it was supported on the OS level instead of through only certain motherboards, but considering how much SSD prices have fallen, I doubt anyone would bother.   I think another close alternative would be DIMMDrive."
hardware,3cb2cn,ManlyPoop,4,Tue Jul 7 00:22:57 2015 UTC,"You can copy paste a steam game from one place to another.  Steam doesn't care if you swap games in or out, as long as Steam knows where the game is. Or you can download a program to do it for you. SteamMover comes to mind.  SSD for the games you play now, HDD for games you're storing for later. Takes no more than a few seconds to swap manually (with sys links or copy pasting) or with SteamMover if you're lazy. All the while, you still have streamlined access to both HDD and SSD games."
hardware,3cb2cn,bl0odredsandman,2,Tue Jul 7 03:22:09 2015 UTC,"I use Steammover and it works just fine. I only have a 250GB ssd so when I'm done playing it, I just move it over to my 2TB HDD with another steam library in it."
hardware,3cb2cn,Modna,4,Mon Jul 6 19:04:18 2015 UTC,I have a weird obsession with keeping all my steam games installed at once just in case I get the itch. 256 gig SSD + 1.5TB HDD works nicely. BUT It's getting to the point that a 5400 RPM drive is noticeably slower for my games.
hardware,3cb2cn,willxcore,5,Mon Jul 6 20:17:52 2015 UTC,"I thought this way when I had slow internet, now it takes 15 minutes tops to get a game downloaded and installed so I only keep games I play at least once or twice a week installed."
hardware,3cb2cn,cyberd0rk,8,Mon Jul 6 18:01:57 2015 UTC,It's funny how fast internet totally changes your perspective on your hardware. I went from 3mbit DSL that would average a gig an hour to 105mbit cable. Now long term game storage is a thing of the past for me.
hardware,3cb2cn,Modna,1 point,Mon Jul 6 18:04:48 2015 UTC,I am on 200 down 20 up. I got this habit when I had an extremely unstable 6 mbit connection. I keep it now because hoarding is fun
hardware,3cb2cn,TheImmortalLS,1 point,Mon Jul 6 18:09:28 2015 UTC,I'm still on 3 Mbit down sighs but the ping ain't bad.
hardware,3cb2cn,cyberd0rk,1 point,Mon Jul 6 18:16:02 2015 UTC,True. I played FPS online all the time without struggles. Streaming anything above 360p on YouTube? FahgedAbouded
hardware,3cb2cn,TheImmortalLS,1 point,Tue Jul 7 05:51:23 2015 UTC,"720p 30 is easy if no one else is on the internet. If anything is even remotely touching it, down to 480p for me :("
hardware,3cb2cn,Modna,1 point,Tue Jul 7 13:08:23 2015 UTC,I have gone up and down on internet. I got this habit when I had super crappy unstable internet and now that I have 200 down 20 up I still do it.
hardware,3cb2cn,Bickell,1 point,Tue Jul 7 14:43:42 2015 UTC,"I have 125mbps. I'd still rather keep games installed so I have instant access to them. The more installed, the better. Not to mention that there's some ISPs that are implementing data caps so those people that are affected are basically fucked if they have to download a game like GTA5 that is 50 gigs."
hardware,3cb2cn,fliphopanonymous,2,Mon Jul 6 18:13:20 2015 UTC,Can't you just move them back to the SSD?
hardware,3cb2cn,Modna,1 point,Mon Jul 6 21:36:32 2015 UTC,"If I wanted to, yes. I only keep programs that I need speed or use frequently on my SSD. Work programs like Solidworks go on there. And when I used to play League I kept it on there as well."
hardware,3cb2cn,foxtrot1_1,1 point,Mon Jul 6 18:03:32 2015 UTC,"Every time you install a game on Steam it asks you which directory you want to use. Or you can just re-download a game, yeah. If I'm going to re-play it I keep it around, but that's nothing like 500gb of games."
hardware,3cb2cn,hoyfkd,1 point,Mon Jul 6 18:12:38 2015 UTC,I just keep the installer files on another drive in case I want to reinstall.  Easy peasy.
hardware,3cb2cn,LiberDeOpp,1 point,Mon Jul 6 18:29:04 2015 UTC,Yeah I have a 256ssd I use a main drive and then my 3 tb stroage holds all the games. With storage prices on the verge of dropping while capacity increases there is no reason to buy right now.  This all makes me wonder what would happen if samsung bought amd or at least the graphics divison.
hardware,3cb2cn,bphase,1 point,Mon Jul 6 22:41:12 2015 UTC,"I've a 160GB SSD and it's not really comfortable even with 3TB of HDD space backing it up. It can only hold 2-3 big games at a time which can be enough with fast internet, but lately I've had GTA V hogging the space for a long time, which leaves little else room.  Plenty of reason to buy now for us that have a small SSD or no SSD at all, prices are quite affordable for the reasonably sized 250-500GB SSDs. Also, I'm seriously considering dropping mechanical storage completely on my next build, and only getting a 1TB SSD."
hardware,3cb2cn,LiberDeOpp,1 point,Mon Jul 6 19:25:46 2015 UTC,"I would strongly recommend one of the lower priced offerings in the 256 to 500gb range. If I were you I would wait try to wait for the back to school and holiday sales events.    The biggest reason I went to an ssd at all is for boot up times since my main system is loud and power hungry. I have just a 2tb hdd in my ""always on"" system."
hardware,3cb2cn,DoTheEvoIution,0,Mon Jul 6 19:47:59 2015 UTC,Whats up with upvoting this ethernal waiting the futureproofing attitude?  Yokel sticking to his HDDs or 64GB ssd because just around the horizon theres something better... thats smart?
hardware,3cb2cn,Bickell,-1,Mon Jul 6 20:26:47 2015 UTC,"There's obviously no such thing as future proofing but spending $500-1,000 on a SSD is idiotic. My 3TB Barracuda was dirt cheap and loads my games fast enough for now. Spending insane amounts of money to shave off 15-45 seconds on load times isn't worth it."
hardware,3cb2cn,Seclorum,2,Mon Jul 6 21:18:39 2015 UTC,"Especially when 2, 1tb drives cost less than 1, 2tb drive.   By almost 200 bucks even!   1tb drives are anywhere from 340-500 right now depending on brand."
hardware,3cb2cn,poematik,2,Mon Jul 6 21:34:05 2015 UTC,Would love to build a RAID box with a few of these
hardware,3cb2cn,marcoalexander,3,Tue Jul 7 02:18:50 2015 UTC,Why wouldn't you buy 2 EVOs on sale for like 200 bucks less? 800 bucks seems a bit much :/
hardware,3cb2cn,masturbateAndSwitch,7,Mon Jul 6 20:25:42 2015 UTC,The 2TB drive comes with $200 worth of e-Viagra
hardware,3cb2cn,ivo09,1 point,Mon Jul 6 20:14:07 2015 UTC,"After the initial sellout, retailers will probably be selling these under MSRP."
hardware,3cb2cn,marcoalexander,1 point,Mon Jul 6 23:29:27 2015 UTC,I hope so xD
hardware,3cb2cn,Dragonsong,5,Tue Jul 7 01:27:39 2015 UTC,Samsung seems like the most unreliable brand right now.  If they couldn't get their previous SSDs to work properly I can't see why they would for these.
hardware,3cb2cn,techno_babble_,3,Tue Jul 7 01:39:14 2015 UTC,No idea why you've been downvoted. Samsung SSDs have been plagued with reliability issues lately.
hardware,3cb2cn,Seclorum,2,Mon Jul 6 20:42:49 2015 UTC,"The 840 issues only really cropped up with people sitting there and obsessively running benchmarks.   The newest issues is more a problem with the drives never being designed for Queued Trim commands, of which only certain Linux distros even use.   Every MFG has had serious issues with their drives in the past few years, except for Intel."
hardware,3cb2cn,Killmeplsok,2,Mon Jul 6 21:49:24 2015 UTC,Crucial at least had been pretty quiet in the this department too.
hardware,3cb2cn,Supercow12,2,Mon Jul 6 22:40:46 2015 UTC,"Crucial had the same problem as Samsung with queued trim: https://github.com/torvalds/linux/blob/master/drivers/ata/libata-core.c#L4228 There is the blacklist for queued trim in the linux kernel. As you can see, the MX100, M500, and M550 are all on that list.  Samsung issues just get more publicized."
hardware,3cb2cn,Seclorum,1 point,Tue Jul 7 02:07:54 2015 UTC,Ahem   M4 glitch that forces machines into unrecoverable BSOD loops... or the drive just disappears and never ever connects to a device again...
hardware,3cb2cn,Killmeplsok,1 point,Tue Jul 7 12:19:43 2015 UTC,"That was very long ago, they have been very good for the past 3 years at least, and they do fix those problems, unlike the 840 evo ones which still have to rely on workarounds.  If we have to go back to that far of the history, the Intel 8MB bug was pretty big too."
hardware,3cb2cn,Seclorum,0,Tue Jul 7 02:16:28 2015 UTC,"The 840 evo's work just fine. It's not like the drive completely packs up and ceases to exist anymore. And in real world usage you dont notice the slowdown. You have to specifically test your drive with benchmarks to check because you cant tell with normal usage.  Even then, what workaround? You download the Magician software, run the tool, Bam your fine until you get paranoid again and start the process again.   The 850's are out now, competitively priced, and the 'glitch' for Queued Trim get's fixed by the Linux Distro's themselves because the problem existed with them in the first place."
hardware,3cb2cn,dwe11er,6,Tue Jul 7 02:26:08 2015 UTC,"No it didn't. Linux developers just disabled queued TRIM for Samsung devices.  Also, Samsung SSDs shouldn't declare queued TRIM support in the first place, they just don't have it."
hardware,3cb2cn,Unoid,1 point,Tue Jul 7 03:17:06 2015 UTC,How did they handle the 840series TLC hardware defects?
hardware,3cb2cn,Unoid,1 point,Tue Jul 7 07:29:11 2015 UTC,They didn't!!!
hardware,3cb2cn,jdw101,-9,Tue Jul 7 02:25:02 2015 UTC,Will they truly be no compromise? Seems like the performance of these says nope.  Would you rather have the 1.2tb Intel 750 with less space and more performance?  That is the route I took.
hardware,3cb2cn,Seclorum,23,Tue Jul 7 02:25:14 2015 UTC,"Most people wont sit there and obsessively run benchmarks on their drives to reach the highest possible numbers.   These are consumer drives, and most consumers will not notice the difference between the intel drive or the samsung drive in real world usage.  What they will notice is the intel has less space and the samsung more."
hardware,3cb2cn,ChuckVader,13,Mon Jul 6 16:29:42 2015 UTC,"Unless your pc is moonlighting as a google server, you will never see the benefits of an intel 750 over a Samsung 850 pro.  I mean the tech is great and the throughput is incredible, but unless you have 20 sources trying to access the data simultaneously you're just not going to see it in use outside of benchmarks."
hardware,3ccecm,JC713,-1,Mon Jul 6 20:37:06 2015 UTC,Already posted: https://www.reddit.com/r/hardware/comments/3c2mn8/asus_strix_r9_fury_listed/
hardware,3cay26,Steelheaded,-3,Mon Jul 6 14:15:34 2015 UTC,"Can you read those graphs, because I sure as fuck can't?"
hardware,3cay26,kennai,2,Mon Jul 6 18:17:37 2015 UTC,It's not impossible... just slightly difficult from a normal viewing distance.
hardware,3cdjmm,conradsymes,6,Tue Jul 7 01:58:02 2015 UTC,"At least with Intel processors, they add new instructions on the CPU that combine what would normally take multiple software steps into one single step:   AESENC. This instruction performs a single round of encryption. The instruction combines the four steps of the AES algorithm - ShiftRows, SubBytes, MixColumns & AddRoundKey into a single instruction. AESENCLAST. Instruction for the last round of encryption. Combines the ShiftRows, SubBytes, & AddRoundKey steps into one instruction. AESDEC. Instruction for a single round of decryption. This combines the four steps of AES - InvShiftRows, InvSubBytes, InvMixColumns, AddRoundKey into a single instruction AESDECLAST. Performs last round of decryption. It combines InvShiftRows, InvSubBytes, AddRoundKey into one instruction. AESKEYGENASSIST is used for generating the round keys used for encryption. AESIMC is used for converting the encryption round keys to a form usable for decryption using the Equivalent Inverse Cipher."
hardware,3cc7hn,DrFunkenstyne,30,Mon Jul 6 19:50:30 2015 UTC,april 21st 2017 08:48 UTC
hardware,3cc7hn,vorrishnikov,6,Mon Jul 6 22:51:12 2015 UTC,"Thanks, I kind of had a feeling but wanted to be sure."
hardware,3cc7hn,Seclorum,8,Tue Jul 7 09:00:12 2015 UTC,"To start with, you need more PCIe lanes available on the motherboard for use, then you need controllers with NVME to catch on, then you need to make it all cheap enough to compete with mature SATA technologies.  Because you sure as hell cant tell the difference for most end user tasks right now, so the only way they will catch on is if you make them cheap enough."
hardware,3cc7hn,Roph,19,Mon Jul 6 20:31:47 2015 UTC,"Probably never, they're pointless for the regular consumer and I'd argue, even the enthusiast.   The ""Speed"" you feel from an SSD is not so much from raw throughput but from ridiculously high IOPS and basically zero seek/latency. The fact that we're up against the SATAIII wall doesn't really matter. The fact that you can have 6-digit per second IOPs does."
hardware,3cc7hn,Stingray88,4,Mon Jul 6 19:55:42 2015 UTC,"Current consumer oriented software does not utilize the raw speed available on a PCIe SSD. But if these speeds were commonly available, do you think developers might make use of it in future software?"
hardware,3cc7hn,MEaster,1 point,Mon Jul 6 21:22:13 2015 UTC,Surely a high transfer rate is only really utilised by transfer of large files. What consumer software could really take advantage of it?  Some games can use it; perhaps high resolution video editing too.
hardware,3cc7hn,Stingray88,3,Mon Jul 6 21:47:57 2015 UTC,"I'm not sure to be honest, I'm mostly just playing devils advocate haha.   Video editing could certainly take advantage of the speeds, but at that level it wouldn't be consumer."
hardware,3cc7hn,conradsymes,2,Mon Jul 6 22:13:05 2015 UTC,"The high transfer rate is well utilized by Paradox games. Loading one game usually involves loading a thousand ten kilobyte text files and graphics, which takes longer than it should for hard drives."
hardware,3cc7hn,TheImmortalLS,3,Tue Jul 7 05:39:17 2015 UTC,"Semantics, but   1000 small files are loaded faster by the ssd because of the zero seek vs hdd physical seek. The transfer speed doesn't play such a big role for such small files."
hardware,3cc7hn,conradsymes,2,Tue Jul 7 06:03:10 2015 UTC,"Oh, now I understand. Whoops, sorry."
hardware,3cc7hn,dylan522p,3,Tue Jul 7 06:04:20 2015 UTC,And nvme greatly increases that area and makes latency even lower
hardware,3cc7hn,Thunder_Bastard,2,Tue Jul 7 05:23:59 2015 UTC,"This is basically it.  A SSD will cut my startup time from about 4-5 minutes (Including Outlook which has about 30,000 emails so it takes a few minutes by itself) to under 25 seconds total.  Plus at the end of those 25 seconds I am 100% ready to work, a HDD is still chewing on some stuff.  A PCIE SSD would take that down to about 20 seconds for twice the price.  Plus there is the PITA factor that the PCIE card has to be set up to boot properly, and can't be used externally if I need to clone it or the PC dies.  SATA is easy and common, and still provides a huge improvement moving to a regular SSD."
hardware,3cc7hn,maybachsonbachs,4,Tue Jul 7 03:34:02 2015 UTC,From what I understand typical desktop usage is best estimated by random r/w performance. Flashy sequential numbers get headlines but won't affect subjective performance.  Having pcie3 x4 bandwidth won't make a difference to the typical user. So why pay >twice the amount per GB?
hardware,3cc7hn,Yearlaren,2,Mon Jul 6 20:04:07 2015 UTC,"I don't care about higher speeds than SATA III 6Gbps, but I'd really like to have a PCIe SSD because you don't need to plug them to the motherboard nor to the PSU."
hardware,3cc7hn,catify,3,Mon Jul 6 21:16:56 2015 UTC,"They are already IMO.  They are already used in almost all current Macs, and in the M.2 form factor among many other branded notebook PC's.  There is no point of 2.5"" SSD's anymore when they look like this inside: Samsung 850 PRO 250gb"
hardware,3cc7hn,hdshatter,1 point,Tue Jul 7 21:23:38 2015 UTC,"I doubt they will stop making 2.5"" any time soon, they are better for desktops and not every motherboard/laptop supports m.2.  I have both, my desktop is a 250GB 850 Pro and my laptop has an M.2 850 Evo. I notice no difference between the 2 devices in terms of speed for normal PC. If I benchmark the M.2 drive is 25MBps faster read, but the write is like 80MBps slower. About the same Read IOPs, way less Write IOPs."
hardware,3cc7hn,Darius510,5,Wed Jul 8 08:26:16 2015 UTC,"When they're cheap enough to compete. Subjectively they're only a hair faster than SATA drives for consumer workloads. So they can justify a 20-30% price premium, but not 400%."
hardware,3cc7hn,xtothemess,1 point,Mon Jul 6 22:32:51 2015 UTC,M.2 is basically a PCIE SSD
hardware,3cb39x,Bingoose,11,Mon Jul 6 14:58:30 2015 UTC,"Aug 5th for Skylake-K, Aug 30th-Sept 5th for Skylake-S  http://benchlife.info/intel-skylake-s-platform-will-release-z170-and-6700k-6600k-in-aug-5th-07062015/"
hardware,3cb39x,Begoru,3,Mon Jul 6 15:14:10 2015 UTC,"Broadwell for desktop came out a while ago. The C series processors are all that we're getting. Unless you're talking about Broadwell-E, which is coming out in 2016 or 2017 (can't remember)."
hardware,3cb39x,III-V,4,Mon Jul 6 16:42:27 2015 UTC,I can't find Broadwell available for purchase anywhere. Are you sure it's already out?
hardware,3cb39x,aziridine86,8,Mon Jul 6 16:49:58 2015 UTC,Broadwell seems to be missing in action as far as anyone knows despite being 'released'.   See this comment sourcing Legit Reviews:  https://www.reddit.com/r/hardware/comments/3bn6p0/edram_overclocking_on_the_intel_core_i75775c/csnrqax
hardware,3cb39x,flamers1500,3,Mon Jul 6 17:00:10 2015 UTC,"Wow, that's fucking terrible lol. Thanks for the link though."
hardware,3cb39x,01111010100,1 point,Mon Jul 6 17:01:47 2015 UTC,For server based Broadwell chips it is expected to be next year according to a roadmap that I saw last month
hardware,3cb39x,Seclorum,1 point,Tue Jul 7 08:05:05 2015 UTC,I can get Broadwell through EPP.
hardware,3cb39x,Sremylop,2,Tue Jul 7 21:34:42 2015 UTC,Broadwell is already out.   They released 2 terrible chips for the desktop and the rest are laptop and tablet chips.   Its functionally slower than Haswell refresh because it's only real redeeming feature beyond the die getting smaller was that it has doubled the IGPU footprint on the die.
hardware,3cb39x,reynardtfox,7,Mon Jul 6 16:52:03 2015 UTC,I wouldn't say they're terrible. They're fine. Just terribly awkward and overpriced.
hardware,3cb39x,Sremylop,11,Mon Jul 6 19:48:08 2015 UTC,I wouldn't say they're terrible. They're fine. Just terribly awkward and overpriced.   ಠ_ಠ
hardware,3cb39x,reynardtfox,6,Mon Jul 6 20:01:06 2015 UTC,"Haha yeah that may sound funny but I'll stand by it. Terrible means it's a piece of trash. Terribly awkward and overpriced means the product itself isn't terrible. It does what it is designed to do. It just has a stupid design. Broadwell should have had i3s, not i5s and i7s."
hardware,3cb39x,lucun,3,Mon Jul 6 20:32:14 2015 UTC,"Haha, yeah I totally agree with you.  I just thought it was funny that you said they weren't terrible but then used the adverb terribly to describe them lol"
hardware,3c9jq2,Fatigue-Error,10,Mon Jul 6 03:54:50 2015 UTC,"Damn, for such a newcomer to the chassis industry, Phanteks seems to be doing everything right. Lian Li could learn a thing or two from Phanteks about internal layout.   Though I do find it kind of funny that a company known previously only for its air coolers and fans would make a chassis with such attention to watercooling compatibility and ease."
hardware,3c9jq2,Exist50,1 point,Mon Jul 6 05:02:11 2015 UTC,"I kinda get a similar chuckle when I commonly see NF-F12s on radiators. They are used so often in water cooling, but Noctua exclusively does air cooling."
hardware,3c9jq2,sk9592,1 point,Mon Jul 6 14:04:23 2015 UTC,"Well to be fair, in terms of fans, air cooling and water cooling are more or less identical. In both, the fan is trying to push as much air through a constricted set of fins. All that changes is where it does it."
hardware,3c9jq2,myu42996,1 point,Mon Jul 6 14:34:44 2015 UTC,"That explains Noctua, but for Phanteks, the radiator, pump, and reservoir support is top notch."
hardware,3c9jq2,Exist50,3,Mon Jul 6 16:37:36 2015 UTC,"Given the current trend towards zero external-facing bays, someone needs to offer a small ""auxillary chassis""-- a USB box designed to be a little less ""flimsy crap"" than the typical external CD-ROM or external hard disc dock.  I could picture one with two 5.25 bays, each containing a 3.5 adapter, two USB-SATA adapters connected via a hub, and also a USB header for things like card readers and break-out boxes, with a quality internal PSU rather than a stupid brick."
hardware,3c9jq2,Bounty1Berry,4,Mon Jul 6 06:19:46 2015 UTC,"Well, the whole reason for the trend is that few enthusiasts use the front panel, or if they do, it's usually only for an optical drive."
hardware,3c9jq2,Exist50,3,Mon Jul 6 07:06:25 2015 UTC,"I'm so happy we've finally started dropping 5.25"" bays."
hardware,3c9jq2,Penderyn,2,Mon Jul 6 07:28:08 2015 UTC,I mean... why would you WANT an external CD-Drive you actually have to pay decent money for? It's not like it really matters or is used a bunch.
hardware,3c9jq2,C4ples,2,Mon Jul 6 08:47:33 2015 UTC,Or just get a nice USB optical drive and set it on your desk.
hardware,3c9jq2,NoAirBanding,1 point,Mon Jul 6 19:58:56 2015 UTC,This is exactly what I break out when I need one.  It's cheap and can be unplugged in a hurry and put away somewhere out of sight when not needed.
hardware,3c9jq2,bl1nds1ght,1 point,Mon Jul 6 21:35:58 2015 UTC,"The conventional external optical drive comes in a flimsy case, usually needs an external power brick (both low quality and bulky), and usually only comes in ""optical drive with drive preinstalled"" or ""hard drive with no open side"" flavours.  If you have something else (card reader, LS-120, tape drive, optical jukebox, etc.), you're out of luck.  I'm thinking, honestly, of the old SCSI external drive cases you used to see used with old Macs, Sun machines, etc.  They looked like serious business and you could probably use one to prop up a car in an emergency."
hardware,3c9jq2,Bounty1Berry,1 point,Wed Jul 8 06:45:23 2015 UTC,"I'm not sure what external drives you've been looking at!  I had to borrow my gf's external drive the other day, and it's a nice looking slot loader with just a USB cable. No external power. It's been made to look similar to a macbook pro - all sandblasted aluminium - but it's definitely not hugely robust. Still does the job and certainly can't be called ugly."
hardware,3c9jq2,tomllm,2,Thu Jul 9 14:23:57 2015 UTC,Are there any other cases with the Gold Enthusiasts award?
hardware,3c9jq2,AlphaGavin,2,Mon Jul 6 05:28:31 2015 UTC,"Love this case, I believe it will be my next. Have my basket all ready on overclockers.  Some links stolen from DBT85 on the forums   http://proclockers.com/reviews/computer-cases/phanteks-enthoo-evolv-atx-review?page=0%2C0[1] http://proclockers.com/reviews/computer-cases/phanteks-enthoo-evolv-atx-review?page=0%2C0[2] http://www.kitguru.net/components/cases/leo-waldock/phanteks-enthoo-evolv-atx-review/[3] http://www.hardwarecanucks.com/forum/hardware-canucks-reviews/69524-phanteks-enthoo-evolv-atx-review.html   http://forums.overclockers.co.uk/showthread.php?t=18674194  EDIT: Links fixed."
hardware,3c9jq2,iMADEthis2post,1 point,Mon Jul 6 07:59:55 2015 UTC,Those thanks should go to DBT85 of the OCUK forums ;)
hardware,3c9jq2,iMADEthis2post,1 point,Mon Jul 6 09:19:53 2015 UTC,"Yeah, have been using them at least twice as long as I have reddit and will continue to long after this place has sank beneath the waves."
hardware,3c9jq2,iMADEthis2post,2,Mon Jul 6 09:41:18 2015 UTC,all links dead except the oc one.
hardware,3c9jq2,Hidden__Troll,1 point,Mon Jul 6 10:41:52 2015 UTC,That's bizarre. If you go directly to overclockers they work from there  http://proclockers.com/reviews/computer-cases/phanteks-enthoo-evolv-atx-review?page=0%2C0  http://proclockers.com/reviews/computer-cases/phanteks-enthoo-evolv-atx-review?page=0%2C0  http://www.kitguru.net/components/cases/leo-waldock/phanteks-enthoo-evolv-atx-review/  http://www.hardwarecanucks.com/forum/hardware-canucks-reviews/69524-phanteks-enthoo-evolv-atx-review.html  they should work will edit my original post
hardware,3c9jq2,iMADEthis2post,1 point,Mon Jul 6 11:42:13 2015 UTC,"IS this thing even out? I watched a review of it a while ago, but all I can find is the mATX one.  NVM end of July."
hardware,3c9jq2,kaasmi,1 point,Tue Jul 7 01:13:03 2015 UTC,i ordered one from a german case seller and they told me it is going to ship on 17.7.15
hardware,3c9jq2,Dek0rati0n,1 point,Tue Jul 7 01:50:27 2015 UTC,About god damn time its coming out!
hardware,3c9jq2,iPlayRealDotA,1 point,Mon Jul 6 07:31:09 2015 UTC,Been drooling over this case since I first heard about it. Definitely a buy for me.
hardware,3c9jq2,FitzDaBastard,1 point,Sat Jul 11 11:29:33 2015 UTC,"If only it wasn't bloody expensive. I really like the aesthetics, but I'm having a really hard time justifying the purchase when I could put the money towards a performance upgrade."
hardware,3c9jq2,TheR3VO,1 point,Mon Jul 6 08:16:58 2015 UTC,"I got a sonata iii for by current build which is a few years old. The only regret from that one. I've upgraded the ram, GPU and added an ssd. But, replacing a case is virtually a rebuild.  For my next build this fall, I'm starting off with the right case, and then speccing out the rest of the machine."
hardware,3c9jq2,TripleBrass,1 point,Mon Jul 6 08:19:14 2015 UTC,"I was looking into getting a Primo until I saw this.  I'm struggling to understand the differences other than full vs mid tower and about $50.  I have an 800D and would ""like"" to downsize, but the Primo looks amazing."
hardware,3c9jq2,BlayneTX,1 point,Mon Jul 6 10:43:27 2015 UTC,That seems like a really great case but I just can't go for that concave front panel. Looking forward to seeing what else they release though because these are top notch.
hardware,3c9jq2,dagmx,1 point,Mon Jul 6 11:32:56 2015 UTC,What would be a good alternative to this case that has a 5.25 external bay? Unfortunately can't give up on a disk drive just yet
hardware,3c9jq2,Kubi74,1 point,Mon Jul 6 13:18:02 2015 UTC,"I really like my phanteks case, it is just perfect. would buy again."
hardware,3c9sss,Exist50,12,Mon Jul 6 05:33:00 2015 UTC,"One question, Why is the radiator below the test bench rather than mounting the radiator above the card like the instructions tell you?"
hardware,3c9sss,Seclorum,-4,Mon Jul 6 05:46:48 2015 UTC,"Good point, but then again, I doubt that many people will take that part of the instructions seriously. That aside, the power and other info is still valid."
hardware,3c9sss,glr123,13,Mon Jul 6 05:50:33 2015 UTC,"Why not? If it matters, lower temps and load mean less power consumption and the results are totally different.  This test is inherently flawed, since the proper protocols for the card werent followed."
hardware,3c9sss,TaintedSquirrel,1 point,Mon Jul 6 08:26:30 2015 UTC,If it matters   Key point here. All watercooling solutions have the same 'requirement' - it's a matter of physics rather than individual design.  The fact that some no longer even mention it in documentation is demonstration that it's highly unlikely to cause any issues.
hardware,3c9sss,aziridine86,5,Mon Jul 6 18:21:36 2015 UTC,"A shocking number of people will read that as:  ""Other CLC manufacturers don't even mention it, but AMD put it in their user manual as a requirement.  Thus, AMD's CLC uses an inferior design""."
hardware,3c9sss,veyron3003,1 point,Mon Jul 6 19:14:24 2015 UTC,"Sadly, you're likely right.  To be honest, I still think it should be in every CLC's documentation.  It's an issue that does crop up from time to time."
hardware,3c9sss,itoldyouiwouldeatyou,3,Mon Jul 6 19:16:09 2015 UTC,"Good article, but why not give ""efficiency"" in 'FPS per watt' instead of 'Watts per FPS'?  That way you would get a proper 'higher is better' number as you would expect for an efficiency rating.  It would turn the numbers into decimals, but that could be fixed (e.g. FPS per 100 watts)."
hardware,3c9sss,Nixflyn,1 point,Mon Jul 6 08:15:21 2015 UTC,Some really good info in here about the companies involved and the state of the problem as it exists.
hardware,3c9sss,Nixflyn,0,Mon Jul 6 05:33:54 2015 UTC,I really dont see why so many people complain about fan noise. my system is about 52db from about 3.5ft away (ear level). That's with 9 fans.
hardware,3c8t0j,Randomoneh,9,Sun Jul 5 23:43:58 2015 UTC,"There may be a correlation due to surface area and number of pieces per master slab but there are several other factors affecting yield besides just PPI. For example, the complexity of the technology (full color is more complex than monochrome) and complexity of the manufacturing process are much more important than dot pitch. Also factor in the maturity of the technologies. TN has been around for ages while OLEDs are the new kids on the block. They've found all the things that can go wrong with TN while they are still working out the tricks with OLEDs."
hardware,3c8t0j,KillAllTheThings,2,Mon Jul 6 01:38:16 2015 UTC,"I believe those are very important  AFAIK JDI is one of Apple's supplier of displays for iPhones and iPads, while Apple only uses LG's P-OLED displays in the Apple Watch  Also LG only use their P-OLEDs for their smartwatches as well, their flagship phone uses LCD, except some of their highend TVs which are also P-OLED  The Apple Watch (and LG's own smartwatches ) has a far smaller displays in comparison with the displays of the iPhones and iPads (or LG's G4)"
hardware,3c8t0j,Vince789,1 point,Mon Jul 6 05:16:29 2015 UTC,The LG G Flex / G Flex 2 both use P-OLED (that said the G Flex is a very low volume device)
hardware,3c8t0j,pcman2000,2,Mon Jul 6 11:45:04 2015 UTC,"Yep, but I believe their flagship is the G4, which is LCD"
hardware,3c8t0j,Vince789,4,Mon Jul 6 12:54:24 2015 UTC,Don't forget that these two panels are based on different tech (OLED vs. AMOLED?). Different companies also have different approaches. I don't think those can be directly compared unless you're comparing two different product lines manufactured by the same company using the same process.
hardware,3c8t0j,viperabyss,5,Mon Jul 6 01:33:30 2015 UTC,"Apple screens are LCD afaik. Also to keep in mind is that Apple may have much more stringent standards than LG, or maybe there's something else going on there. Or maybe it's as simple as the different PPI :P"
hardware,3c8t0j,ocshoes,5,Mon Jul 6 01:47:16 2015 UTC,"I'm fairly certain it isn't just about different PPI. That's like arguing AMD / Globalfoundries' fabrication process is crappy compared to Intel's fabrication process, even though there are millions of different factors between them. Heck, even humidity, weather, and climate are factors to fabrications. That's why Intel introduced, at high operating cost, ""copy exact"" to their fabrication plants."
hardware,3c8t0j,viperabyss,1 point,Mon Jul 6 01:59:31 2015 UTC,"Excellent point. I was mostly kidding anyway, but I actually hadn't thought of it like that."
hardware,3c8t0j,ocshoes,2,Mon Jul 6 07:14:00 2015 UTC,"Apple sources their displays from many suppliers and they are all LCD except the Apple Watch, which is P-OLED (and made by LG)  Its probably because Apple has stringent standards they set for their suppliers  LG sells whatever meets Apple's standards to Apple, then sells some that are ok but not up to Apple's stringent standards to another buyers, which makes their yield seem higher"
hardware,3c8t0j,Vince789,1 point,Mon Jul 6 05:05:58 2015 UTC,"not up to Apple's stringent standards to another buyers, which makes their yield seem higher   Yeah, I hadn't thought about that. That's probably true. In fact the only reason I mentioned apple's high standards is iirc a lot of the Thunderbolt display panels had no visible defects but were still rejected and sold as the off brand Korean monitors."
hardware,3c8t0j,ocshoes,2,Mon Jul 6 07:11:53 2015 UTC,Can someone explain why it's said that manufacturing small high density panels are easier to avoid defects than large low density ones? I'd figure that the tiny tiny pixels on my phone screen are easier to mess up than the big ones on my monitor.
hardware,3c8t0j,myu42996,1 point,Mon Jul 6 14:33:31 2015 UTC,"Exactly. There are quotes out there from LG executives, I think, where they say it's much easier making 1080p panels compared to 4K panels of the same size."
hardware,3c8t0j,TeutorixAleria,1 point,Mon Jul 6 18:00:19 2015 UTC,It's not quite that it's easier just more cost effective.   The difficulty is in making a high resolution screen of the same size as another low resolution screen.   For example a large 8k screen with one physical defect is ruined.   When you can produce a large number of 1080p screens for phones in the same area one defect only ruins one screen so your profit is higher.
hardware,3c8t0j,epsys,1 point,Wed Jul 8 12:10:57 2015 UTC,"So let's say we've got 3 panels, each one of them 10-inch. Only difference is pixel pitch. 50, 200 and 800 PPI. What would be your guess on yield rate on those?    I guess what I want to know is which one has a bigger yield rate - 5-inch 1080p or 32-inch 1080p?"
hardware,3c8t0j,unanun,2,Mon Jul 6 04:26:02 2015 UTC,"there's a science to yield, not just a dreidel. yield is probably fantastic for both of those, because profits depend on the yield on both. It's not like we're talking about 14nm lithography here"
hardware,3c6u4s,generalgranko,54,Sun Jul 5 11:56:41 2015 UTC,"The only connector able to push that right now would be DisplayPort 1.3, which isn't available in consumer products yet. Besides that, I dont know if 120hz 4K panels are viable to produce yet, or if they would be thousands of dollars like the first 4K panels.  I'd expect first, expensive models popping up next year."
hardware,3c6u4s,HavocInferno,16,Sun Jul 5 12:14:26 2015 UTC,"Well, they could do it like the Dell 5K monitor and many early 4K ones where you have two inputs each driving half the screen."
hardware,3c6u4s,Exist50,13,Sun Jul 5 15:49:29 2015 UTC,"How does that even work? Surely the two halves would go out of sync or something during fast motion and gaming. There's a huge lcd display at a shop nearby, made of 25 tv screens all showing portions of a single video and they were 5 frames ahead or behind each other, it was jarring."
hardware,3c6u4s,blitzhex,13,Sun Jul 5 18:15:35 2015 UTC,"2 displays isn't as demanding as 25. I would have questions of frame buffer sizes and numbers. But two displays should be easy.  Plus, our consumer cards are made to drive 2-6 displays out of the box. And I know some AMD cards get around the limitation of driving more than 2 displays off of only 2 clock sources: they sync two of the displays to one clock. I'm sure that would resolve your sync theory."
hardware,3c6u4s,Hypoglybetic,7,Sun Jul 5 18:33:09 2015 UTC,How do you think SLI works? This is a solved problem.
hardware,3c6u4s,ahugenerd,2,Mon Jul 6 01:28:29 2015 UTC,I'm pretty sure that SLI doesn't work that way. It's not like each GPU is rendering half of frame.
hardware,3c6u4s,gumol,5,Mon Jul 6 06:44:42 2015 UTC,When it's using Split Frame Rendering that's exactly how it works.
hardware,3c6u4s,Stingray88,1 point,Mon Jul 6 08:00:08 2015 UTC,"When is in the future, unless you have this magical api that already does that in the present for sli"
hardware,3c6u4s,TheImmortalLS,1 point,Tue Jul 7 06:08:40 2015 UTC,Huh? Its been an option for forever.
hardware,3c6u4s,Stingray88,2,Tue Jul 7 06:12:18 2015 UTC,"Can you point me towards an example? As far as I know, everyone uses alternate frame rendering which is why everyone has multi-GPU latency and microstutter issues."
hardware,3c6u4s,TheImmortalLS,3,Tue Jul 7 15:01:40 2015 UTC,"If they're different displays, it could be their individual response time of the displays you're seeing. I know if I were to stetch a video across my two monitors my TN is noticeably quicker than my IPS."
hardware,3c6u4s,LegatusIgnatius,1 point,Sun Jul 5 19:23:04 2015 UTC,Take a look at nvidia mosaic.
hardware,3c6u4s,MandaloreZA,9,Mon Jul 6 05:21:56 2015 UTC,"expecting something similiar aswell, kinda saddens me that the reign of 1080p was so long, can't wait to make the jump to 4k hardware and 4k monitor along with it, but that will be even several more years until it's trully affordable :/"
hardware,3c6u4s,CrazedToCraze,54,Sun Jul 5 12:18:12 2015 UTC,"The vast majority of monitors in the wild are still 1080p, the reign won't be over for quite a while.  Hell, there is still a disgusting amount of laptops being sold at 1366x768. Meanwhile, in the mobile phone world..."
hardware,3c6u4s,TeutorixAleria,20,Sun Jul 5 12:23:14 2015 UTC,Small screens aren't as susceptible to low yeilds which is why we get so many phones with crazy high resolution screens
hardware,3c6u4s,_Ganon,10,Sun Jul 5 12:54:05 2015 UTC,Could you possibly go into a little more detail on what this means?
hardware,3c6u4s,Lee1138,22,Sun Jul 5 15:40:47 2015 UTC,"The bigger the surface area of the screen you want, the more chance there is that that area has a defect, rendering the panel unusable for that size. When the panel you need is only 6'' compared to a regular 24'' computer monitor, the probability of defects on the mobile screen is that much smaller. Which means there is less waste and that in turn means lower prices for the 6'' high pixel density screens than for a 24'' screen of the same pixel density. I assume right now, the production cost/waste of a 24'' panel with the same pixel density as a 6'' panel would be so high that it would only appeal to a very small market segment, meaning that it becomes infeasible to sell at the current price point."
hardware,3c6u4s,Randomoneh,2,Sun Jul 5 15:49:48 2015 UTC,Don't you know that you get lower yields with lower pixel pitch?
hardware,3c6u4s,crshbndct,1 point,Sun Jul 5 23:25:32 2015 UTC,"I just liken it to die sizes. 400mm² at 28nm is easier to get a good yield on than 500mm² at 40nm, even though it has many more transistors."
hardware,3c6u4s,MrPoletski,-7,Mon Jul 6 11:16:46 2015 UTC,the tech for manufacturing panels in tiles then seamlessly joining them together is LONG overdue.
hardware,3c6u4s,dylan522p,7,Sun Jul 5 17:21:13 2015 UTC,And impossible with the way scree s are made
hardware,3c6u4s,MrPoletski,-1,Sun Jul 5 20:21:43 2015 UTC,"So find another way to make displays so that it is possible?  Thanks for the downvotes, Team CantBeDone.  Like I said, the tech is long overdue."
hardware,3c6u4s,TeutorixAleria,2,Mon Jul 6 05:47:36 2015 UTC,"There's a big difference between theorised technology that hasn't been invented yet and some random guy on the Internet saying ""why can't we do x yet?"""
hardware,3c6u4s,FartingBob,3,Mon Jul 6 09:33:03 2015 UTC,"The larger a panel (applies to both physcial dimensions and pixel count) the higher the chance there will be a defect on the panel which may make it unsellable. With smaller screens the physical dimensions mean much less defects, which is why high DPI panels tend to start with smaller screens (phones), then start appearing in tablets and eventually monitors and TV's once the process is more refined and the manufacturing yield is better."
hardware,3c6u4s,kenHar,4,Sun Jul 5 16:28:10 2015 UTC,"the only problem i have with the low resolution on my netbook (very old one 1024x600 10"") is that a lot of programs arent optimized for it (especially eclipse)."
hardware,3c6u4s,rePAN6517,6,Sun Jul 5 14:09:50 2015 UTC,A netbook seems very underpowered for doing development on
hardware,3c6u4s,im-a-koala,8,Sun Jul 5 14:20:15 2015 UTC,"That depends entirely on what kind of development you're doing.  If you're developing small-scale utilities or embedded stuff, a netbook is probably fine, in terms of computing power.  Of course I'd hate programming on such a tiny screen."
hardware,3c6u4s,kenHar,3,Sun Jul 5 14:23:12 2015 UTC,with a few tweaks its usable enough and im just using it because i already had it and its just for a small webapp project in university.
hardware,3c6u4s,narutoninjakid,2,Sun Jul 5 14:26:10 2015 UTC,This is true. Shopping for a laptop for parents and a good majority of 15.6 inch laptops are this resolution. Its disgusting lol.
hardware,3c6u4s,mmencius,1 point,Mon Jul 6 02:48:15 2015 UTC,Those phone resolutions are a total waste. They're all retina unless people hold their phones way closer to their face than I do at least.
hardware,3c6u4s,hdshatter,4,Mon Jul 6 04:00:06 2015 UTC,1440p master race.
hardware,3c6u4s,Stingray88,2,Sun Jul 5 23:57:56 2015 UTC,1440p ultrawide master race.
hardware,3c6u4s,Stingray88,2,Mon Jul 6 00:55:04 2015 UTC,"not enough 144hz, you don't qualify, sorry!"
hardware,3c6u4s,iconic2125,2,Mon Jul 6 08:05:42 2015 UTC,"Hah I've only got a single 980 Ti! With settings on max at 3440x1440, I couldn't even make 120Hz worth it.  But seriously, the 21:9 aspect ratio is the way to go. My next monitor will have higher resolution than 3440x1440, or higher refresh rate than 60... or both... but it'll still definitely be 21:9"
hardware,3c6u4s,chapstickbomber,1 point,Mon Jul 6 08:28:07 2015 UTC,"tbh 16:9 seems perfect for me for majority of games, about FPS, 4:3 due to my competitive aspect and focus on middle of screen at all the time is my way to go, the only games I would prefer 21:9 would probably be RPGs like Skyrim  welp, you can always choose 16:9/4:3 resolution on 21:9 monitor after all  definitely make your monitor higher hz.... I'd sacrifice any resolution for higher hz, even if I had to play 800x600 for next 10y, serious."
hardware,3c6u4s,cheekynakedoompaloom,1 point,Mon Jul 6 08:32:00 2015 UTC,Can't wait for the Asus and Acer curved ultrawide 1440 G-Sync panels to come out.
hardware,3c6u4s,Pufflekun,3,Mon Jul 6 08:43:29 2015 UTC,"Even the shitty Seiki 4k 30hz displays have a 120hz input signal driving the panel itself.  AFAIK, it isn't the panels themselves that are the limitation, but getting a TCON to pipe at 1.2GHz pixel clock, faster than 5k, and with a smaller market of buyers."
hardware,3c6u4s,Sapiogram,2,Mon Jul 6 21:52:03 2015 UTC,"can confirm, i can oc my panel to a true 123hz at 1080p and 72hz at 1440p. bandwidth permitting 2160p 120hz is already possible on retail panels. just... no standard around that can do 2160p@120."
hardware,3c6u4s,Pufflekun,8,Sun Jul 5 21:17:52 2015 UTC,"Plus, you'd have to have a heel of a rig to run modern games at 4K 120HZ. I'd guess something like quad 980 Ti's."
hardware,3c6u4s,BuildYourComputer,10,Sun Jul 5 22:48:30 2015 UTC,Only if you insist on having the highest settings.
hardware,3c6u4s,gizza,15,Sun Jul 5 15:21:56 2015 UTC,"Sure, but if someone wants to buy a monitor that's going to cost what, about $2,000? then you've gotta assume that they're going to want to max out their settings."
hardware,3c6u4s,BuildYourComputer,0,Sun Jul 5 15:55:36 2015 UTC,"Or, you're like me, and understand that just because I throw money at something doesn't mean it's going to give me an even return, but you pay the price to have the newest stuff."
hardware,3c6u4s,atsu333,4,Sun Jul 5 16:09:15 2015 UTC,"But what's the point of a 4K 120Hz monitors if you have to turn all the graphics to minimum? You'll benefit from the high frame rate, but the 4K would kinda be wasted."
hardware,3c6u4s,JOSHPIG3000,3,Mon Jul 6 01:33:36 2015 UTC,"Not really. Every other resolution looks blurry now, and a lot of the older games I play have no problems hitting above 100fps on ultra, or unreal tournament or one of my main games, cs go. People forget that the textures in 4k are also increased, even on lowered settings."
hardware,3c6u4s,IndigoMoss,6,Mon Jul 6 05:15:39 2015 UTC,If you don't have it on the highest settings then what's the point of going 4k? It's all about making the image as pretty as possible.
hardware,3c6u4s,mmencius,13,Tue Jul 7 15:46:55 2015 UTC,Running 4K is often a good replacement for some game effects like anti-aliasing.
hardware,3c6u4s,IndigoMoss,5,Sun Jul 5 16:21:44 2015 UTC,Can confirm. Running a GTX 980ti on a 50 inch 4k TV. Very few games actually need anti aliasing at 5 feet viewing distance.
hardware,3c6u4s,lordx3n0saeon,1 point,Sun Jul 5 16:38:32 2015 UTC,"50 inch 4K is retina at 5 feet, so no games should."
hardware,3c6u4s,HavocInferno,1 point,Sun Jul 5 21:27:10 2015 UTC,"A few still do, most notably Sleeping Dogs and Hitman: Absolution. That being said, both of those games are light enough on the card that the GTX 980 Ti can run them at 60 FPS with anti-aliasing enabled."
hardware,3c6u4s,sleepertime,7,Mon Jul 6 04:01:51 2015 UTC,Clarity. I'll gladly trade some geometric complexity and lighting to minimize the infuriating jagged edges everywhere.
hardware,3c6u4s,DemiDualism,5,Mon Jul 6 12:17:57 2015 UTC,Some value resolution over object detail. One reason being that 4K vs FHD/WQHD gives you more detail up close on textures and more detail far away (since distant objects have more pixels than on FHD/WQHD).
hardware,3c6u4s,elevul,3,Sun Jul 5 16:46:48 2015 UTC,"I can agree here, I run AC:BF at full native 2160p,  but have AA off and most settings on medium-high,  I love the crispness of the image,  rather than a low-res ambient occlusion festival."
hardware,3c6u4s,Miltrivd,2,Sun Jul 5 17:41:23 2015 UTC,Not in counterstrike
hardware,3c6u4s,FPSNige,1 point,Sun Jul 5 23:14:57 2015 UTC,Noscope sniping people miles away.
hardware,3c6u4s,myodved,0,Sun Jul 5 19:29:50 2015 UTC,The insane price plus lots of indie/emulated games that don't/can't support 4K. That's why I'm out of the 4K hype.  I'll stay 1080p for as long as the majority of games have it as standard and the price barrier isn't as crazy as it is right now. Getting a 100-144Hz monitor and a system capable of sustaining those framerates across all games is my next investment.
hardware,3c6u4s,thejshep,-1,Sun Jul 5 21:46:09 2015 UTC,Considering the AMD Fury X excels at 4k. We would start with perhaps tri crossfire.
hardware,3c6u4s,maybachsonbachs,1 point,Sun Jul 5 22:16:46 2015 UTC,"Are there any Freesync/Gsync 4k models, maybe with lightbox? I swear I remember one. It doesn't completely make up the difference, but it would make for a pretty good 4k60 experience in the meantime, with the added bonus of being able to run it with a really good card or two."
hardware,3c6u4s,NamenIos,2,Sun Jul 5 22:10:44 2015 UTC,Acer XB280HK
hardware,3c6u4s,musef1,1 point,Sun Jul 5 17:14:04 2015 UTC,1.2 could do 2160@120 if 4:2:2
hardware,3c6u4s,Stingray88,8,Sun Jul 5 20:58:52 2015 UTC,Who wants that when reading text? 4:2:2 works fine in video but is absolutely not suited for a pc monitor.
hardware,3c6u4s,iAnonymousGuy,2,Sun Jul 5 14:03:56 2015 UTC,Can anyone ELI5 what 4:2:2 is?
hardware,3c6u4s,NamenIos,3,Sun Jul 5 14:13:24 2015 UTC,"This is my goto video for explaining the difference between 4:4:4, 4:2:2 and 4:2:0."
hardware,3c6u4s,musef1,1 point,Sun Jul 5 21:27:31 2015 UTC,"in case no one comes by to explain it better... 4:2:2 is the chromatic range of the display which dictates color accuracy. 4:4:4 is the gold standard, 4:2:2 is standard, I believe."
hardware,3c6u4s,maybachsonbachs,2,Mon Jul 6 08:08:21 2015 UTC,"422 is used nowhere in computer hardware, just internally in software like video compression etc."
hardware,3c6u4s,HavocInferno,1 point,Sun Jul 5 21:59:21 2015 UTC,"Ah I see. I think your explanation is good enough for me, thanks!"
hardware,3c6u4s,maybachsonbachs,0,Mon Jul 6 04:04:36 2015 UTC,"just run 60hz when doing desktop and 120hz when gaming/video. ideal no, viable, maybe?"
hardware,3c6u4s,HavocInferno,2,Sun Jul 5 22:08:53 2015 UTC,"thing is, when you game at 120hz, dont you do so because you need crisp vision and very smooth gameplay? Idk how well a competitive shooter plays with 4:2:2...illegible text being just one issue."
hardware,3c6u4s,robertotomas,1 point,Sun Jul 5 14:16:00 2015 UTC,"text wouldn't be illegible, and only the smallest text would be difficult to make out  if you can read text on a jpeg (most commonly lossy 4:2:0) then you certainly could read text lossless 4:2:2. It's only half the color information lost. I'd probably guess most people wouldn't be able to differentiate under typical usage."
hardware,3c6u4s,HavocInferno,1 point,Sun Jul 5 17:40:11 2015 UTC,half the color information lost might be a no-go for some competitive users
hardware,3c6u4s,robertotomas,1 point,Sun Jul 5 18:25:44 2015 UTC,"wow, usually they finalize the standard within the same year that the products come out.. I thought Sony already had a plausable 1.4 on high end video equipment"
hardware,3c6u4s,elevul,1 point,Sun Jul 5 18:30:10 2015 UTC,"Well neither Maxwell 2.0 nor R300/Fury series have DP1.3, so it'll take at least until the next GPU gen for us to get proper 1.3 on GPUs. For displays I know not a single available model that has 1.3.  It's finalized, but depending on how much has changed from older drafts and stages, it's inevitable that support for the connector will only see light a year or two after finalization"
hardware,3c6u4s,sifnt,1 point,Sun Jul 5 18:30:29 2015 UTC,"it actually is so simple a thing that it would never take 1-2 years without legal issues being in the way. Similar things have come out within weeks of finalization, not years (SATA III, HDMI 1.4 both come to mind.. the first HDMI 1.4 TVs were available months before the standard was finalized -- to the point it was almost a problem)  I bet Sony is pushing its IP again in courts somewhere, and that is the issue.."
hardware,3c6u4s,sifnt,0,Mon Jul 6 04:47:37 2015 UTC,They can definitely do them cheaply. It's just that they won't because money.
hardware,3c6u4s,random_guy12,14,Mon Jul 6 11:39:20 2015 UTC,I'm with you on that. Thinking of a good 27` 1440p once it becomes affordable with anti blur/144hz/good colour (ips/VA) and freesync/gsync.  On a related note I'd love to hear if there is any serious work on high quality hardware upscalers... Something like 'edge directed interpolation' could probably make 1080p look much better on a native 4k screen.
hardware,3c6u4s,sifnt,1 point,Sun Jul 5 21:43:49 2015 UTC,"my dream is a 24"" 2160p 1ms 144hz IPS freesync/gsync monitor  if the 1ms - IPS will be forever impossible, I'm not that picky about colors, VA will be still prefered to TN, but TN would do anyway  I know 4k on 24"" won't be that noticeable like on bigger screenes, but I have no interest in getting a bigger monitor, especially since the main games I play are competitive FPS where big monitors don't seem practical at ALL, 24"" is the perfect size"
hardware,3c6u4s,random_guy12,3,Sun Jul 5 14:03:54 2015 UTC,"Fair points, running a 23.6(?) incher Dell IPS atm and it feels a bit small for work tasks so its all about compromises. Plan will be one centre 27/30with 1 portrait mode 24 on either side.   Want the colours to look nice and consistent (and will calibrate them) on all monitors, and the centre monitor needs to kick ass in Starcraft."
hardware,3c6u4s,sifnt,1 point,Sun Jul 5 14:54:17 2015 UTC,"Are you running without any scaling? Otherwise, using default 200% scaling, it shouldn't feel any different from a 1080p monitor, just sharp as hell."
hardware,3c6u4s,chaddledee,1 point,Sun Jul 5 17:12:25 2015 UTC,"Not running 4k yet, other monitor features higher priority.  I'm aware 200% 'nearest neighbour' scaling should look better on a 4k monitor than the same size screen at 1080p, but there are much better scaling algorithms out there in academia that while they wouldn't compete with rendering at native 4k they might get up to half way there. If they could be done in real time we'd have a really good compromise."
hardware,3c6u4s,Juheebus,2,Mon Jul 6 06:20:50 2015 UTC,"That's not really what I'm talking about. I'm talking about having OSX or Windows scale the UI elements so everything is as large as it would be at 1080p, while natively rendering at 4k."
hardware,3c6u4s,dylan522p,1 point,Mon Jul 6 15:29:13 2015 UTC,"Fair enough, point taken. I run linux on my workstation and need triple monitors to align well (scale of objects etc) so I'm not so sure the UI frameworks are mature enough yet, or performance is up to it (average 20+ windows open, with 100+ tabs in browsers).  144hz low motion blur screens would be nice (and be awesome on gaming PC as well), while 4k is too much trouble currently, hence if the monitor had a high quality scalar to 4k without compromising any of the other features that would be ideal for the next 3~ years."
hardware,3c6u4s,PhilipK_Dick,1 point,Mon Jul 6 16:54:15 2015 UTC,"tbh triple 27'' for casual gaming on games like Skyrim would be great for me, keeping a single 24'' for games where I want to excell competitively like CSGO is a must however"
hardware,3c6u4s,Sandwich247,3,Mon Jul 6 17:13:44 2015 UTC,"Just something interesting to note: big monitor could potentially be better for competitive FPS games. The periphery of your eye has a lower response time and higher critical flicker frequency (suggesting a better temporal ""resolution"") than the centre of your eye. You may react faster if your monitor filled more of your field of view."
hardware,3c6u4s,mmencius,2,Mon Jul 6 08:50:56 2015 UTC,And I've heard the opposite of that from some competitive gamers. In the end it's your preference that matters.
hardware,3c6u4s,elevul,1 point,Sun Jul 5 18:21:45 2015 UTC,"not completely neglecting that, but theory doesn't always equal reality, even 24'' was way too hard to get used to after making a very small jump from 23'' which seemed perfect before, I believe 27'' would somewhat still be doable for me, but 24'' just seems perfect, talking about purely the theory, playing on something like 50'' would then be much better, I don't see that being any true in reality tho"
hardware,3c6u4s,bossrabbit,1 point,Mon Jul 6 21:58:13 2015 UTC,Same but oled
hardware,3c6u4s,lordx3n0saeon,1 point,Mon Jul 6 08:56:04 2015 UTC,OLED 4k 144hz is future tech stuff for now.  At least 3 or 4 years away from being reasonably priced.
hardware,3c6u4s,mmencius,1 point,Sun Jul 5 20:26:31 2015 UTC,"Would love a ~24"", high rez monitor. Pixel density is all that really matters to me. I prefer my 1280x1024, ~17"" monitor to me 1920x1080 32"" one."
hardware,3c6u4s,mmencius,2,Mon Jul 6 03:17:21 2015 UTC,"Can't imagine playing CSGO on anything even bit bigger than 24'', considering most 4k monitors aren't even 27'' but 30""+ makes me a bit sad."
hardware,3c6u4s,ICanHazTehCookie,1 point,Mon Jul 6 00:25:19 2015 UTC,A plasma looks better than all those things. My dream is a 30 inch 2880p 144 Hz OLED VRR monitor.
hardware,3c6u4s,lordx3n0saeon,1 point,Mon Jul 6 08:27:03 2015 UTC,ok
hardware,3c6u4s,mmencius,-1,Mon Jul 6 04:03:19 2015 UTC,Lol 24
hardware,3c6u4s,Exist50,7,Mon Jul 6 08:20:57 2015 UTC,Somewhat related question: what hardware would one need to game at these settings? Would it be feasible?
hardware,3c6u4s,InappropriateThought,7,Sun Jul 5 22:05:46 2015 UTC,Dual fury X's do crisis 3 @4k 90FPS.  Expect next year's Titan to do the same (the 16nm jump will be huge I think)
hardware,3c6u4s,Exist50,1 point,Sun Jul 5 14:57:48 2015 UTC,And expect next year's 1080 Ti to be better than that Titan at 2/3 the price.
hardware,3c6u4s,InappropriateThought,1 point,Sun Jul 5 16:51:13 2015 UTC,what's funny is that aftermarket cooled 980 Ti's ARE actually better than Titan X
hardware,3c6u4s,Sandwich247,1 point,Mon Jul 6 04:04:17 2015 UTC,Yes...
hardware,3c6u4s,wolfcry0,0,Mon Jul 6 08:32:13 2015 UTC,"Because they're factory overclocked, not because they are more powerful."
hardware,3c6u4s,kidwuf,1 point,Mon Jul 6 09:14:15 2015 UTC,"Heads up:  maxwell -> Pascal-> Volta  Volta Titan is 2 generations from now.  Sure the 1180Ti will probably beat it 6 months later, but that's not the point.  No more shoestring budgets by that point."
hardware,3c6u4s,robertotomas,1 point,Tue Jul 7 08:35:30 2015 UTC,"Hmm my mistake, in a different comment I correctly called it the 1180 Ti. And this year it was three months, and you got a better card. I don't see why enthusiasts would get a card without aftermarket designs."
hardware,3c6u4s,plagues138,6,Mon Jul 6 12:58:29 2015 UTC,At least dual 980ti's/Fury X's.
hardware,3c6u4s,lordx3n0saeon,5,Mon Jul 6 19:54:03 2015 UTC,"Not if you're running at max or close to max, depending on the game of course. I'm running 2-way sli with 980 ti's and on AAA titles like witcher 3 I still fail to hit constant 60fps even with some settings turned down and no AA."
hardware,3c6u4s,plagues138,3,Sun Jul 5 15:48:47 2015 UTC,Hairworks?
hardware,3c6u4s,lordx3n0saeon,1 point,Sun Jul 5 16:43:40 2015 UTC,"Mmm I remember having to turn that down as well to get to reasonable frame rates, can't remember if I had it disabled or set to self only, but definitely not set to everyone. Basically with everything maxed and no AA, I was getting just over 30fps. Had to turn down quite a few settings (grass density is a monster), including hairworks to get it to roughly 45 to 50 fps, just don't remember how far I turned hairworks down."
hardware,3c6u4s,plagues138,2,Sun Jul 5 16:48:56 2015 UTC,"The Titan X was supposed to do it but, no. Two 980Ti cards or 2 FuryXs may do it but I havn't seen any benchmarks yet."
hardware,3c6u4s,capn_hector,1 point,Sun Jul 5 16:55:27 2015 UTC,Dual 980ti or Fury X would work if you're not expecting max/ultra settings
hardware,3c6u4s,mmencius,1 point,Mon Jul 6 00:26:42 2015 UTC,"a GTX 970/980, if one wants to just play games like CS:GO/LoL/DotA like I do."
hardware,3c6u4s,lordx3n0saeon,-2,Mon Jul 6 00:45:04 2015 UTC,An AMD R9 295x2
hardware,3c6u4s,Darkstryke,6,Mon Jul 6 08:30:52 2015 UTC,"the international standards for display tech have targets of 8k@120hz in high color (>10bit) by 2020, and 8k@60hz and 4k@120hz (at 10bit color) as early as 2016. Whether it will happen or not is a different story."
hardware,3c6u4s,Sandwich247,7,Sun Jul 5 16:31:46 2015 UTC,"Once there is a single gpu that can Max out games at over 60fps, that is a reasonable price.  Right now the market for such a monitor would be so small that its not worth producing."
hardware,3c6u4s,Jakedubbleya,4,Sun Jul 5 18:41:27 2015 UTC,"I don't know about you, but my monitors tend to last 5-6 years."
hardware,3c6u4s,elevul,7,Sun Jul 5 15:42:44 2015 UTC,"Same, but the amount or people who would go out, spend a ton of money on a monitor they won't be able to fully support for a few years is going to be pretty damn small."
hardware,3c6u4s,j5v,1 point,Sun Jul 5 16:52:10 2015 UTC,I would buy a 4k/144hz today.  Why? My rig is 100% capable of silky smooth 144FPS web browsing at 4k. Gaming? Maybe 20-65 FPS depending on title (980).   When I get myself a dual-Titan rig as a reward to myself for paying off my student loans you better believe It'll do 4k/144! (Hint: that'll probably be a Volta Titan :/)
hardware,3c6u4s,Charwinger21,5,Sun Jul 5 16:54:12 2015 UTC,"you would be part of a very small user base.  Companies like to make money, they make what they know will sell. it doesnt matter if theres a few thousand people out there with 2x/3x titan x set ups, because they wont make a product to sell a few thousand units."
hardware,3c6u4s,ycnz,2,Sun Jul 5 18:44:10 2015 UTC,"Steam Hardware Survey puts a significant chunk (1 in 3) of the 4K users as running dual-4K or quad-4K.  Could you convince a meaningful chunk of those users to buy a single 144hz 4K screen instead at a similar cost?  Dual-4K users are probably dropping close to a grand on their screens.  They're also not driving them with a $150 card either, those are the users who are running Fury Xs and 980 Tis and Titan Xs.  They have the money to blow on a second flagship card to SLI with their first one.  That puts average FPS at 120-130 and minimum FPS around 60, which is within the performance envelope of either G-Sync/Freesync or FPS rate targeting.  If the hardware showed up today at near-flagship level pricing (say less than $1000) it would sell."
hardware,3c6u4s,MINIMAN10000,0,Sun Jul 5 18:50:38 2015 UTC,Please don't get Volta Titan. Get a Volta 1180 Ti which beats Volta Titan at 2/3 the price. I hate Nvidia's pricing on the Titans.
hardware,3c6u4s,capn_hector,1 point,Mon Jul 6 02:39:28 2015 UTC,But I need my 32GB of VRAM for my 3x 5k setup!
hardware,3c6u4s,MINIMAN10000,-1,Mon Jul 6 04:05:25 2015 UTC,Never underestimate the power of marketing on clueless hardware buyers.
hardware,3c6u4s,capn_hector,0,Mon Jul 6 12:56:53 2015 UTC,Modern games would be able to hit a solid 120 FPS on 2K if they were optimised properly.
hardware,3c6u4s,MINIMAN10000,2,Sun Jul 5 17:11:42 2015 UTC,"We'd need better cables, but mostly better GPUs"
hardware,3c6u4s,capn_hector,2,Mon Jul 6 00:21:56 2015 UTC,As soon as we have DP1.3 capable video cards and scalers.
hardware,3c6u4s,MINIMAN10000,2,Sun Jul 5 19:29:26 2015 UTC,"Signal bandwidth is the limitation. I think 4K@120Hz is not the format that would motivate availability of a HDMI or DisplayPort standard. Rather, adoption of 8K@30Hz would produce the connectivity that would enable enough bandwidth for 4K@120Hz.  I also expect different methods of signal compression (and game rendering) by then, which loosens the coupling of connector bandwidth to the traditional measures of resolution and refresh rate."
hardware,3c6u4s,ycnz,1 point,Sun Jul 5 21:43:25 2015 UTC,"Signal bandwidth is the limitation. I think 4K@120Hz is not the format that would motivate availability of a HDMI or DisplayPort standard. Rather, adoption of 8K@30Hz would produce the connectivity that would enable enough bandwidth for 4K@120Hz.   DisplayPort 1.3 supports both of those."
hardware,3c6u4s,MINIMAN10000,1 point,Mon Jul 6 11:17:58 2015 UTC,"Right now, the two adaptive sync technologies are not at all interoperable - this is probably a bigger problem than anything else - you'll generally hang on to a monitor for many years.  I use 4k at work (32"" Sharp), and it's incredible for general use. But playing games at native res isn't feasible. :)"
hardware,3c6u4s,ycnz,2,Wed Jul 15 04:51:07 2015 UTC,I dono for me the choice is simple. Free sync is created by a standards association whereas g-sync is created by a graphics card company. I'm following the standards association.
hardware,3c6u4s,MINIMAN10000,2,Sun Jul 5 19:05:13 2015 UTC,"So how do you feel about Mantle?  That was a proprietary standard too...   ...come to think of it, so is DX12.  Microsoft isn't a standards association either.  I agree that open is better but right now the simple fact is that G-Sync is a better standard that supports adaptive-sync over a consistent range that is wider than any Freesync product on the market at 1440p.  And the hardware that implements G-sync is better than the hardware that implements Freesync.  Largely because G-SYNC isn't a loose ""protocol implementation"" but a hard and firm set of standards set by the standard authority.  In the abstract I would prefer the open standard too, but it's not like the company with a 76% market share is a nobody that's going to disappear overnight.  Nobody ever got fired for buying IBM equipment."
hardware,3c6u4s,ycnz,1 point,Sun Jul 5 19:51:01 2015 UTC,Mantle was proprietary for the time being they had plans to make it so that nvidia could use mantle but then they decided to drop mantle and help develop the Khronos Group Vulkan API again working to develop industry standards.  The adaptive sync for Freesync has no requirements for the range and leaves it entirely up to those who implement it. The XL2730Z can vary from 40Hz to 144Hz for example. Here is a post  describing how each of them handle it. Below 40 fps nvidia just re displays the frame again rather than waiting for a new frame causing the game and monitor refresh rates to not be in sync It can do this because the G-sync module itself has a place to store the previous frame. Whereas below 40 fps Freesync no longer synchronizes the refresh rate to the game and just locks the monitor refresh rate again causing the game and monitor refresh rates to not be in sync. Neither of which is ideal. But well below 40 fps isn't ideal either when your trying to hit 144 fps.   According to This Rather than locking to 40 fps AMD defaults to the monitor's highest refresh rate once it drops down below 40 fps.
hardware,3c6u4s,Sandwich247,-1,Mon Jul 6 03:43:48 2015 UTC,"That's not technically correct though - Mobile G-Sync can refresh the frame without an on-card memory store or scalar chip.  And G-Sync's limit is 30fps which vs 35fps (eg MG279Q) means your requirement to keep a 1/6th safety margin over your stutter limit is 35fps vs 41fps.  In other words you need 20% more FPS minimum on your Freesync monitor vs what the G-sync monitor would require.  The problem is exactly that Freesync imposes no requirements and allows the manufacturers to stick the label on whatever crap they want.  NVIDIA keeps tighter control over the label - since it's a brand that's exclusive to NVIDIA products - and that means G-Sync ends up on the premium products and Freesync ends up on the ""budget"" products.  You can stick the ""Freesync"" label on your 45fps adaptive-sync product, while NVIDIA isn't going to let you use their brand unless you meet the 30fps spec limit."
hardware,3c6u4s,MINIMAN10000,0,Mon Jul 6 04:45:13 2015 UTC,"The OP was about monitors and in monitors the memory will be in the gsync module to refresh the frame.   G-Sync's limit is 30fps vs 45fps   I don't understand what you meant by this   which in terms of keeping a 1/6th margin over your stutter limit is 52.5fps vs 35fps.   I continue to not understand what you are talking about could you reference articles better explaining this because your next point continues off it   In other words you need 50% more FPS minimum on your Freesync monitor vs what the G-sync monitor would require.   Again I have never heard of any of these things.  Yes freesync doesn't impose limits but you know what else doesn't impose limits? Monitors themselves. Tons of monitors are sold for being 1080p 60 fps monitors on a daily basis while looking absolutely terrible. What I'm getting at is as far as I'm aware nothing stops the panels themselves from having bad color, refresh rates, and contrast all which can be budget monitor features while still bearing the G-Sync label because it refreshes at the required rates.  As far as I'm aware the only thing your guaranteed with G-sync is the range of FPS consistency. So in other words like my earlier post. Just make sure the range is within acceptable levels and boom your done.  If you care about all the other features being up to snuff in a monitor this is just one more thing to tack on. In the end what it allows is future monitors can begin implementing lower framerates competitively."
hardware,3c6u4s,Sandwich247,0,Mon Jul 6 05:37:55 2015 UTC,"It's not an obscure subject - G-sync imposes standards on adaptive-sync performance, Freesync does not.  So when your LG monitor dips below 48 fps then Freesync kicks out, and you will have to either deal with V-sync lag or tearing.  Whereas G-sync keeps ticking down to 30fps.  Depending on the monitors that are compared, factoring in a reasonable margin of safety, this may be significantly more than a 50% difference in necessary performance between the G-sync and Freesync standards.  ""It's a monitor, therefore it cannot be measured according to a standard of performance"" is a bullshit answer, since monitors have been measured since forever and nobody wants to buy a monitor that doesn't meet relevant standards (32 bit color, 1080p resolution, 60 hertz refresh rate).  G-sync is an adaptive-sync standard, not a color standard, and pretty much every monitor that costs less than $15,000 (the buy-in for 10-bit color) has the same limitations in terms of color (they're 8-bit per channel).  The problem is that many Freesync monitors do not fall within the ""acceptable"" limit in terms of minimum and maximum FPS (48hz is not an acceptable minimum, 75hz is not an acceptable maximum), so the label doesn't mean much.  While G-sync does mean something, because NVIDIA imposes some standards.  So anything with the ""G-sync"" label falls within the acceptable range.  Sure, when some future Freesync standard means that monitors can handle a wider adaptive-sync range than the equivalent G-sync standard, then it'll mean something.  Right now G-sync imposes some minimum standards while Freesync does not, so Freesync is more of a ""technology specification"" rather than a ""standard"".  Also, any comment on the monitors that the Freesync label is getting slapped on right now rather than in some hypothetical future?  How many of them are superior to the equivalent G-sync implementations?  Care to remark on how the XB270H (G-sync) compares to the XG270H (Freesync)?  (apart from being the most backwards naming scheme ever)    That's part of the problem, that the premium monitors (144hz 1440p IPS with wide sync range) are getting G-sync and the crappy monitors (144hz 1440p TN with narrow sync range) are getting Freesync. The people with the flagship/halo card(s) aren't buying 2nd tier monitors and they will spend extra for 144hz when it becomes available."
hardware,3c6u4s,ICanFindAnything,2,Mon Jul 6 05:51:23 2015 UTC,"Alright I don't know why you didn't say your concern was with Freesync having issues with ghosting. Those are issues with the monitor itself not freesync in this thread BenQ is shipping with the Advanced Motion Accelerator defaulted too high causing the ghosting effect.  My point is no one wants to buy a garbage monitor so they do research. On things like contrast, brightness, color reproduction, freesync fps range. The freesync label simply means it has adaptive sync but to know the range your going to have to look for yourself just like any other spec."
hardware,3c6u4s,Thebman712,1 point,Mon Jul 6 06:18:45 2015 UTC,"Technically, that's true, but being limited to AMD isn't ideal - if nothing else, it eliminates choice."
hardware,3c6u4s,XorFish,2,Mon Jul 6 06:36:50 2015 UTC,It isn't AMD's fault that it is limited. While not ideal Nvidia carries the blame for not supporting the standards and I will purchase AMD over Nvidia over this matter. Well This matter combined with all other proprietary technologies they have. Physx and gameworks. Along with their poor linux track record. They have shown they will abuse their market position and create proprietary things to sway customers and If the difference between price and performance between Nvidia and AMD isn't to great I am willing to choose AMD over this.
hardware,3c6u4s,sidthejar99,1 point,Sun Jul 5 22:10:51 2015 UTC,"Oh, philosophically, AMD are marginally better - but I would wait. Both techs are still pretty early days."
hardware,3c6u4s,Yatlol,1 point,Mon Jul 6 00:12:42 2015 UTC,As long as the specs and price are within your range I don't see any reason not to get freesync. Monitor specific freesync implementations can do things like 40-144 hz or 100-144 hz so be sure to pay attention to that. Then you have blur reduction technologies like ULMB. TN and IPS. Resolution and aspect ratios. Response times and input lag. Usually that covers most common desires for monitors
hardware,3c6u4s,fulthrottlejazzhands,1 point,Mon Jul 6 00:33:10 2015 UTC,"My assumption is that they'll fight it out, and agree on a common standard within the next year, simply because the current situation is so stupid. There seem to be actual technical limitations with freesync relative to g-sync, so I have high hopes for the next gen."
hardware,3c6u4s,n3x_,1 point,Mon Jul 6 02:32:06 2015 UTC,If only both were supported on all cards... :c
hardware,3c6u4s,__Cyber_Dildonics__,1 point,Mon Jul 6 02:58:54 2015 UTC,Nvidia chooses to lock out AMD. Nvidia refuses to support freesync. You can try bugging them to get both supported on all cards but given their track record they'll ignore you.
hardware,3c6u4s,tbonanno,1 point,Mon Jul 6 00:28:10 2015 UTC,"Maybe Q2 next year if we're lucky but theyy will be the first itterations and, as such, wont be as great as the 1440p 120-144Hz monitors will be. If you want a good one, try Q4 2016 or Q1 2017 is my prediction."
hardware,3c6u4s,bphase,-1,Mon Jul 6 02:21:41 2015 UTC,"Inb4 ghaphics cards can't power a monitor like this, so you shouldn't be asking anyway.  For real though, I'd expect them to show up about this time next year. They'll be crazy expensive and probably not worth investing in for a while like 1440p had been until ~6 months ago.  Edit: looks like I was way late. OP never asked if it was feasible... Only when they might be released."
hardware,3c6u4s,RX10,3,Mon Jul 6 00:17:03 2015 UTC,Even though i have a 280x i would be interested in getting one at a $1000 price point because it would be used for 7+ years and i would go through many gpu upgrades. And right now i could play with settings down a bit at 1440 and play csgo at 1080p 144hz
hardware,3c6u4s,JJSCHMITTY,2,Sun Jul 5 13:29:58 2015 UTC,980 ti SLI can power 4k on over 60 fps in a lot of games.
hardware,3c6u4s,RX10,1 point,Sun Jul 5 14:43:35 2015 UTC,"There's Asus MG279Q, which is 1440p, 120hz and freesynch capable, and besides that there's a whole bunch of 4k/2k IPS panels with a high refresh rates coming. So hopefully prices would go down"
hardware,3c6u4s,JJSCHMITTY,8,Sun Jul 5 19:08:26 2015 UTC,That monitor can go up to 144hz
hardware,3c6u4s,RX10,4,Sun Jul 5 14:10:28 2015 UTC,"Fantastic monitor, have had it for a couple weeks.  I was hoping for a 4k 144hz Freesync or gsync display, but figured a) they won't be available for 2-3 years (although there may be some extremely expensive models coming in 1-2 years) and b) there is no card out there that can run 4k anywhere near the FPS that would really take advantage of a 144hz refresh, probably won't be until 16nm.  In summary, it will be 2-3 years until 4k 144hz is practical."
hardware,3c6u4s,leo_blue,2,Sun Jul 5 16:26:56 2015 UTC,freesynch
hardware,3c6u4s,Berzerker7,12,Sun Jul 5 16:12:38 2015 UTC,"Not everyone is playing games, and of the people that are, not everyone is playing the latest, most demanding games, and of the people that are playing those, some know how to turn their settings down."
hardware,3c6u4s,RX10,1 point,Sun Jul 5 16:51:16 2015 UTC,"Can confirm, have a 4k monitor. And it is painful to be turning the settings down from ultra like I'm used to. You learn to deal"
hardware,3c6u4s,JJSCHMITTY,3,Sun Jul 5 12:49:36 2015 UTC,"Some games are easier than others, and you can sacrifice graphics setting for fps. Although 1440p @ 120+ Hz makes more sense atm, as going 4K to sacrifice quality settings is kind of counterproductive.  But still, 4K@120+ is interesting. That'd be something that should last for many upgrades to come with nothing much better in sight. A couple of next-gen GPUs should be able to handle it OK."
hardware,3c6u4s,RX10,-6,Sun Jul 5 13:39:14 2015 UTC,My uncle has a 120 000$ 4k 120hz oled display.....but it is not available for  consumers...you can expect them next year for sure
hardware,3c6u4s,Pringlecks,1 point,Sun Jul 5 15:41:49 2015 UTC,"Could you provide details and pictures, especially of the connectors? I would like to know how it is driven."
hardware,3c6u4s,Pringlecks,2,Sun Jul 5 13:01:25 2015 UTC,http://www.amazon.com/Samsung-UN105S9-Curved-105-Inch-Ultra/dp/B00L403O8U/ref=sr_1_1?s=electronics&ie=UTF8&qid=1436116210&sr=1-1&keywords=4k+120hz&refinements=p_n_size_browse-bin%3A3578043011%2Cp_36%3A8000000-15000000
hardware,3c6u4s,Gregorymendel,4,Sun Jul 5 16:19:28 2015 UTC,"We're (collectively) referring to 4k 120 hz native, not interpolated."
hardware,3c6u4s,Raising,1 point,Sun Jul 5 17:02:17 2015 UTC,it says 120hz native  1440 clear motion rate
hardware,3c6u4s,RX10,2,Sun Jul 5 17:25:14 2015 UTC,"That's a TV... We're talking about monitors. Plus the specs say nothing about it being OLED. And the 120Hz is just the backlight strobing, not the true refresh rate. And I'm not even talking about color accuracy, and image persistance."
hardware,3c6u4s,SiSkEr,7,Sun Jul 5 17:51:38 2015 UTC,"...no, no one here is jealous, you're just an idiot."
hardware,3c6u4s,SiSkEr,-1,Sun Jul 5 20:55:25 2015 UTC,I do not live with him sorry
hardware,3c6u4s,csacc,3,Sun Jul 5 17:52:25 2015 UTC,"Get pictures or its bullshit, because there's no way to drive such a screen as of today, 7/5/2015."
hardware,3c6u4s,XorFish,-4,Sun Jul 5 18:03:57 2015 UTC,"I couldn´t give less fucks of what you think honestly....  Mass people thinks there´s not superior tech available for richer people.  Well there is, you can get technology ahead of its time but you need a lot of money.  I remember in 2004 assisting to a conference where they had 4k screens , consumers barely were on 1024x768"
hardware,3c6u4s,csacc,6,Sun Jul 5 18:46:04 2015 UTC,Wow this fucking guy just has dubious verbal diarrhea.
hardware,3c6u4s,Harb67,2,Sun Jul 5 17:08:40 2015 UTC,Wow this fucking guy just has dubious verbal diarrhea.
hardware,3c6u4s,Pringlecks,1 point,Sun Jul 5 17:25:52 2015 UTC,Kek
hardware,3c6u4s,ryemigie,1 point,Sun Jul 5 21:05:23 2015 UTC,$120k for a tv.. is this a joke ?
hardware,3c6u4s,dannybates,1 point,Sun Jul 5 22:43:58 2015 UTC,a tenth of a million
hardware,3c6u4s,InappropriateThought,-1,Mon Jul 6 01:11:52 2015 UTC,Here is a 4k 144hz g-sync. http://www.amazon.co.uk/Acer-Predator-XB280HKbprz-Displayport-380x2160/dp/B00MB5O5OY/ref=sr_1_1?tag=144hmoni0e-21&ie=UTF8&qid=1436074854&sr=8-1&keywords=Acer+XB280HK
hardware,3c6u4s,iPlayRealDotA,12,Mon Jul 6 08:08:26 2015 UTC,"no, wrong amazon description, 60hz 4k, there is no >60hz 4k yet"
hardware,3c6u4s,mmencius,3,Mon Jul 6 01:57:45 2015 UTC,"Ah, what a shame :/"
hardware,3c6u4s,iPlayRealDotA,2,Mon Jul 6 02:18:08 2015 UTC,we will get them soon enough  #hope #believe
hardware,3c6u4s,ScepticMatt,0,Sun Jul 5 13:13:11 2015 UTC,Around the time we get 8k @ 60hz being reasonably priced?
hardware,3c6u4s,dannybates,2,Sun Jul 5 13:16:46 2015 UTC,Isn't 8k@60hz twice as much information as 4k@120hz?
hardware,3c6u4s,ScepticMatt,1 point,Sun Jul 5 13:38:46 2015 UTC,Maybe 30hz was more appropriate.  We just didn't get 144hz for 1080 until 4k was reasonably priced so I figured I'd make the same comparison to 8k.
hardware,3c6u4s,mmencius,1 point,Sun Jul 5 14:50:42 2015 UTC,"we got 120hz 1080p back in 2010/2011 if I'm not mistaken, which isn't really MUCH different from 144hz"
hardware,3c6u4s,dannybates,0,Sun Jul 5 17:54:41 2015 UTC,I might just be testing one right meow. Maybe. Possibly. It could be a [REDACTED] and it might just be siiiick!
hardware,3c6u4s,Pringlecks,-9,Sun Jul 5 19:07:09 2015 UTC,About a month before 8k/288hz diplays pop up and 4k becomes the resolution of the proletariat.
hardware,3c6u4s,Sandwich247,3,Sun Jul 5 20:21:31 2015 UTC,Hurr durr every six months you gotta replace everything cuz it's already outdated
hardware,3c6u4s,j5v,4,Mon Jul 6 08:52:42 2015 UTC,No.
hardware,3c6u4s,nonameowns,-4,Mon Jul 6 16:22:17 2015 UTC,I want a true 1080p 240hz screen for CSGO first :)
hardware,3c6u4s,bdt13334,3,Sun Jul 5 14:31:00 2015 UTC,For what? 120hz 3d? Going to 240hz is beyond pointless.
hardware,3c6u4s,nonameowns,0,Sun Jul 5 22:39:58 2015 UTC,but the e-peen and karma yo. xD
hardware,3c1l9u,TaintedSquirrel,279,Fri Jul 3 22:36:38 2015 UTC,"All the other stuff going on with Reddit admins was supposed to distract us from the real facts, Ellen Pao is an NVIDIA fanboy confirmed  but really this is really odd, especially considering mods of NVIDIA/AMD are the same NVIDIA is up and they don't have a clue"
hardware,3c1l9u,Zarknox,28,Fri Jul 3 23:00:09 2015 UTC,Looks like to me the top mod just closed shop and gave all the other mods the boot.
hardware,3c1l9u,Gaget,11,Sat Jul 4 04:03:48 2015 UTC,Some times I wish I was an old mod for a lot of subreddits so I can stop this stupid mod drama.
hardware,3c1l9u,bad-r0bot,19,Sat Jul 4 08:14:24 2015 UTC,"The thing is, there is not mod drama with the mods of r/AMD and r/nvidia. The old mod is the issue in this case."
hardware,3c1l9u,sk9592,3,Sat Jul 4 12:29:59 2015 UTC,"Yeah, why the hell is that mod booting other people? Why is /r/amd still private? Is there no mod higher up in charge that can stop this nonsense and clear things up?"
hardware,3c1l9u,bad-r0bot,2,Sat Jul 4 15:04:08 2015 UTC,"First of all, no one as any idea whether it was a mod or not. So stop jumping to conclusions for the time being.  Second, the say it works is that the oldest mod on a sub has seniority  to boot anyone else off. In the case of /r/amd, the oldest mod has not been active with the sub in a long time. If he did boot everyone off, he did so with no apparent reason, out of the blue, and with no communication with other mods.   Either way, it was not because of ""drama"" between mods."
hardware,3c1l9u,sk9592,2,Sat Jul 4 20:50:54 2015 UTC,Thank you for clearing this up.
hardware,3c1l9u,bad-r0bot,2,Sat Jul 4 21:15:00 2015 UTC,"Well, there was some drama. Like... we tried to make our own score about graphics cards. Our test script caught fire because /u/renegadeAI put it behind a reference 290x"
hardware,3c1l9u,ZandalarZandali,7,Sun Jul 5 05:40:17 2015 UTC,SHHH THAT NEVER HAPPENED
hardware,3c1l9u,RenegadeAI,-6,Sun Jul 5 05:41:55 2015 UTC,Paging /u/GallowBoob - your thoughts on this?
hardware,3c1l9u,fieldofsnowkiller,1 point,Sat Jul 4 10:56:01 2015 UTC,I've seen this happen before and I never understand it when it does.
hardware,3c1l9u,PeregrineFury,1 point,Sun Jul 5 01:59:29 2015 UTC,What an asshole. Is there anything we can do?
hardware,3c1l9u,iBoMbY,28,Mon Jul 6 08:24:05 2015 UTC,"Oh man, I've been reading that thread all day for weeks on fiji news and was becoming quite fond of it.   Its an AMD silence!"
hardware,3c1l9u,FPSNige,54,Sat Jul 4 05:35:07 2015 UTC,"Not sure if it is related but /r/watercooling is now private.  edit: Untill proven otherwise, I would assume it is just part of the protest.  edit2; seems like the same thing happened to /r/watercooling as /r/amd."
hardware,3c1l9u,set111,17,Fri Jul 3 22:51:05 2015 UTC,First sub I actually read that's been made private. Great.
hardware,3c1l9u,techno_babble_,4,Sat Jul 4 01:26:24 2015 UTC,Seriously!
hardware,3c1l9u,reddit_reaper,4,Sat Jul 4 06:16:27 2015 UTC,"Wasn't the intention of the mod team unfortunately.  The founder stepped in, removed everybody and set it to private.  We're trying to get it back under our control via a reddit request, but it may take some time."
hardware,3c1l9u,Makirole,3,Sun Jul 5 10:01:50 2015 UTC,"Thanks mate. This really highlights major flaws in the way Reddit works, when someone can go rogue and damage a sub."
hardware,3c1l9u,techno_babble_,3,Sun Jul 5 11:48:33 2015 UTC,"Yeah it's not great unfortunately.  If you want to lend your support to the request, here's a link to the thread.  We're getting hammered over there atm so any support would be much appreciated."
hardware,3c1l9u,Makirole,2,Sun Jul 5 12:01:49 2015 UTC,If the sub never gets back under control what will the followup sub be?
hardware,3c1l9u,Vawqer,3,Sun Jul 5 17:44:06 2015 UTC,Probably /r/liquidcooling.  That place is private atm as it was taken from another user who had been scamming people.  We'd rather keep the old sub rather that migrate though as so many of the resources were sub specific.
hardware,3c1l9u,Makirole,2,Sun Jul 5 18:12:00 2015 UTC,"Yes, it would be rather sad to see those resources vanish all together ..."
hardware,3c1l9u,Vawqer,1 point,Sun Jul 5 18:22:28 2015 UTC,"Wait were you a mod? Damn you're really active in the reddit community. Quick question: what do you think is a major player in winning a pc building contest? (For example abstract case, original color scheme, etc."
hardware,3c1l9u,stereosteam,1 point,Mon Jul 6 17:54:27 2015 UTC,"I was/am indeed :)  I think the biggest factor in winning a contest is working out what that community values.  For instance, OCN values OTT watercooling and colour coordination, Coolermaster loves cheap and tacky greebles and neon crap from 2005, Bit-Tech values being a bit different and using alternative techniques with precision.  That may sound a bit harsh on the Coolermaster front, but I'm fed up with them after the results of this year's competition.  Having an original idea goes a long way to having success, the next step is executing it to the best of your ability.  If it's hosted online, taking good pictures is paramount."
hardware,3c1l9u,Makirole,24,Mon Jul 6 18:34:10 2015 UTC,That's part of the protesting.
hardware,3c1l9u,BrockYXE,3,Fri Jul 3 23:33:04 2015 UTC,"Just want to point out that we did not agree to be part of this protest.  The head mod/founder set it to private, removed all the other mods (myself included) and shut shop."
hardware,3c1l9u,Makirole,1 point,Sun Jul 5 09:58:27 2015 UTC,That's horseshit.
hardware,3c1l9u,BrockYXE,2,Sun Jul 5 17:37:20 2015 UTC,"Not sure if you mean ""I don't believe you"" or ""That's really awful"""
hardware,3c1l9u,Makirole,1 point,Sun Jul 5 18:15:06 2015 UTC,It's shitty.
hardware,3c1l9u,BrockYXE,3,Sun Jul 5 18:19:36 2015 UTC,"Yeah it really was, was mortified to see that this morning we'd been removed as mods.  Luckily many parts I have archived around, the crucial bits can't be saved though if we move subs as they're subreddit specific."
hardware,3c1l9u,Makirole,5,Sun Jul 5 18:21:35 2015 UTC,what's the protest over?
hardware,3c1l9u,epsys,24,Sat Jul 4 03:52:10 2015 UTC,"The admin team left a bunch of mods in the dark about an important issue, which was a final nail in the straw of mod/admin disconnect coffin."
hardware,3c1l9u,Medic-chan,5,Sat Jul 4 04:53:13 2015 UTC,Someone very important who manages a lot of stuff has been let go apparently (/r/outoftheloop).   edit: list of subs that are private when this comment was posted  edit2: big post explaining everything or imgur link
hardware,3c1l9u,bad-r0bot,-18,Sat Jul 4 08:15:31 2015 UTC,"Some chick who worked for reddit and was the main relations between big names and /r/IamA got fired, for no real reason or something.  I really don't know much on the topic, and I care even less than that. It should still be the top post on /r/OutOfTheLoop."
hardware,3c1l9u,BrockYXE,-31,Sat Jul 4 03:55:59 2015 UTC,"Reddit fired an employee, like all companies do. Reddit threw a fit."
hardware,3c1l9u,sterffff,35,Sat Jul 4 04:25:16 2015 UTC,"The problem lies with the fact that her firing occurred after months of inadequate communication between Reddit admins and the subreddit moderators who do most of the heavy lifting to keep the site ticking. Just about every AMA was disrupted due to Victoria's sudden dismissal, and there was no prior warning or attempt to replace her duties by the Reddit staff. Her firing is just the most recent offense that broke the camel's back."
hardware,3c1l9u,Conpen,6,Sat Jul 4 04:33:22 2015 UTC,"No, when all other companies do it they make sure they have a replacement lined up.  They don't have complete fucking radio silence and leave the volunteer crew standing around wondering wtf happened with no coordination, and suddenly also perform the paid worker's duties as well."
hardware,3c1l9u,willyolio,6,Sat Jul 4 13:21:10 2015 UTC,Only if they intend to actually hire a replacement.
hardware,3c1l9u,ethraax,-7,Sat Jul 4 15:12:45 2015 UTC,"Lol, iama worked for years without Victoria just fine. It will continue to work exactly like it did before she was hired."
hardware,3c1l9u,sterffff,2,Sat Jul 4 15:08:03 2015 UTC,So shitty quality with no celebrities and little to no confirmation of identity?  That stuff had gone to /r/shittyiama now.
hardware,3c1l9u,willyolio,0,Sat Jul 4 15:35:35 2015 UTC,"oh no, whatever will we do without our blatant marketing amas!!!!"
hardware,3c1l9u,sterffff,1 point,Sat Jul 4 15:38:41 2015 UTC,"Has this been confirmed? If it is, they went ""dark"" awful late and everything else has come back long ago. It seems to be completely out of step with the other subs."
hardware,3c1l9u,phigo50,4,Sat Jul 4 11:06:11 2015 UTC,You're spot on.  The founder of the sub stepped in an removed all the other mods and set it to private.  We're trying to rectify the situation but it's looking to be a long slog.
hardware,3c1l9u,Makirole,1 point,Sun Jul 5 10:00:03 2015 UTC,"Dammit, I feared that was the case. Thanks for clarifying, is it just a weird coincidence or is it the same ""original mod"" in both /r/amd and /r/watercooling?  It's just typical that these 2 go down, my two most read subs in the past few weeks. Just as I'm about to a) buy a Fury (X) and b) start my first water cooling install! I know of other forums for guidance if I need it but I like the way subreddits work and voat doesn't have established communities yet."
hardware,3c1l9u,phigo50,2,Sun Jul 5 10:29:58 2015 UTC,"Looks to be just a coincidence, seems this has happened to a number of subs around the place.  Yeah reddit as a platform seems to work well for troubleshooting and advice type queries, more so than traditional forums.  Rubbish for work in progress threads though.  If you'd like any WC tips/info feel free to float my way, I'll do my best to answer any questions."
hardware,3c1l9u,Makirole,2,Sun Jul 5 18:19:30 2015 UTC,"Thanks, I appreciate it. The internet is a wonderful place though, I think I'm covered. Hopefully /r/watercooling will be back under control soon enough. There really should be some sort of fail safe once a sub gets to a certain number of subscribers so that one mod, even if he did found the sub, can't just do this.   I digress but back in my IRC days there was usually an opped bot in the bigger channel that would demote another op if he started de-opping or kicking other ops. I guess if it's the founder of the sub it's a different thing but I can't imagine it would be that difficult to implement some sort of fail-safe."
hardware,3c1l9u,phigo50,4,Sun Jul 5 18:42:54 2015 UTC,"Exactly the same happened to us, we've been closed and booted by the oldest mod/founder.  Just great.  Trying to get a reddit request take over atm."
hardware,3c1l9u,Makirole,10,Sun Jul 5 09:57:26 2015 UTC,I bet all 2 subscribers are disappointed.
hardware,3c1l9u,bunchajibbajabba,43,Sat Jul 4 00:02:13 2015 UTC,7250 but who's counting?
hardware,3c1l9u,redditgoogle,12,Sat Jul 4 00:48:41 2015 UTC,"I think it actually hit 7970 recently, as someone on there pointed out."
hardware,3c1l9u,phigo50,5,Sat Jul 4 10:13:42 2015 UTC,"That was /r/amd (where the number 7970 is actually relevant), we're talking about /r/watercooling here."
hardware,3c1l9u,cvance10,14,Sat Jul 4 11:07:18 2015 UTC,Love watercooling.  If you like PC hardware you will like it too.
hardware,3c1l9u,TeutorixAleria,38,Sat Jul 4 00:25:45 2015 UTC,If you haven't watercooled your Wi-Fi router you've still got work to do
hardware,3c1l9u,bohemian_wombat,44,Sat Jul 4 00:33:12 2015 UTC,Knocked a glass of coke onto it the other day.   Does that count?
hardware,3c1l9u,PologizeForThat,29,Sat Jul 4 00:34:37 2015 UTC,It does!  Congratulations!
hardware,3c1l9u,darrenphillipjones,3,Sat Jul 4 00:53:37 2015 UTC,I wonder how I fix my water cooler router... Let me check /r/watercooling! Nope.
hardware,3c1l9u,bunchajibbajabba,2,Sat Jul 4 15:30:43 2015 UTC,"Go to the googles out of curiosity. Yep, someone already did that."
hardware,3c1l9u,ShortySim101,1 point,Sat Jul 4 01:13:39 2015 UTC,I'm not surprised.   I've seen watercooling HDD's and water cooled PSU's before.   Anything that makes the slightest amount of the needs to be water cooled.
hardware,3c1l9u,fuccboi9000,1 point,Sun Jul 5 21:43:40 2015 UTC,my modem needs to be watercooled.
hardware,3c1l9u,hdshatter,1 point,Sat Jul 4 02:19:50 2015 UTC,What about your monitor?
hardware,3c1l9u,rationis,15,Sat Jul 4 09:01:43 2015 UTC,Some of us are going to /r/AdvancedMicroDevices
hardware,3c1l9u,Put_It_All_On_Blck,2,Sat Jul 4 20:19:07 2015 UTC,"Which is cool and all, but the sub is a lot harder to find if you dont know it exists. Hopefully they get the /r/AMD issue solved, as that url is a lot easier to find via google or guessing."
hardware,3c1l9u,rationis,3,Sun Jul 5 03:00:38 2015 UTC,"Whats funny is the number of AMD users that didn't even know what ""AMD"" stood for."
hardware,3c1l9u,Put_It_All_On_Blck,6,Sun Jul 5 03:04:00 2015 UTC,"You think that's bad? There was a time where Asus was releasing marketing videos that contradicted each others pronunciation of the company, and it wasnt just one video.."
hardware,3c1l9u,alainmagnan,25,Sun Jul 5 03:12:39 2015 UTC,Wondering the same. I particularly liked the sub
hardware,3c1l9u,deadhand-,18,Fri Jul 3 23:01:26 2015 UTC,"Yes. There were a few AMD employees who posted there as well, and would clarify things occasionally and provide information that's difficult to find elsewhere. Unfortunately, those posts are effectively gone. Can't even search through their comment history if it ever needs to be referenced. :("
hardware,3c1l9u,iktnl,21,Sat Jul 4 08:33:04 2015 UTC,Who's the only remaining mod and can you contact them? Oddly strange. /r/Nvidia also doesn't seem to know what's happening.
hardware,3c1l9u,snowlovesnow,29,Fri Jul 3 22:47:23 2015 UTC,"According to former mods talking in the same thread on /r/Nvidia, the only mod who might be left is the original mod who has been MIA for a very long time.  They don't know if that original mod removed them all, or if the admins dropped everybody."
hardware,3c1l9u,meowffins,21,Fri Jul 3 23:00:19 2015 UTC,"From what i understand, a mod can only remove a mod under them. So the last person left is probably the one to do it. Account compromised? Possibly."
hardware,3c1l9u,RealParity,4,Fri Jul 3 23:38:39 2015 UTC,It is not confirmed that this one guy is still a mod. He is inactive and all other active mods are confirmed to be removed. The inactive guy may or may not be the last remaining mod of /r/amd
hardware,3c1l9u,RealParity,3,Sat Jul 4 13:22:28 2015 UTC,It is not confirmed that this one guy is still a mod. He is inactive and all other active mods are confirmed to be removed. The inactive guy may or may not be the last remaining mod of /r/amd
hardware,3c1l9u,76assassin3,31,Sat Jul 4 13:27:13 2015 UTC,It looks like the creator doesn't even have mod access: http://i.imgur.com/wOI2zeJ.png
hardware,3c1l9u,OftenSarcastic,18,Fri Jul 3 23:11:07 2015 UTC,Does the subreddit still show up in their profile if it's set to private?  I know I can't see the posts I made to /r/amd so reddit is setup to hide some info for private subs at least.
hardware,3c1l9u,iktnl,3,Fri Jul 3 23:28:10 2015 UTC,"I just tested this with mine and it does appear, so he's no mod any more.  Makes me wonder what the hell is going on even more. What does Reddit have to gain by removing /r/AMD mods? I don't see why."
hardware,3c1l9u,OftenSarcastic,21,Fri Jul 3 23:32:41 2015 UTC,"Eh, did you look at your own profile while logged out? I don't see you as a moderator of anything."
hardware,3c1l9u,iktnl,22,Fri Jul 3 23:34:36 2015 UTC,"Oops, I refreshed too early, it's gone too, now. My conclusion is wrong then."
hardware,3c1l9u,logged_n_2_say,9,Fri Jul 3 23:39:20 2015 UTC,"I pm'd /u/zandalarzandali earlier today about it, and he said basically what was posted in the pic.   I didn't press him any further."
hardware,3c1l9u,I_Love_Opera,6,Sat Jul 4 00:55:56 2015 UTC,"So they are selling subreddits moderation to companies and highest bidders, thats my theory and I am sticking with it."
hardware,3c1l9u,ShitBabyPiss,2,Sat Jul 4 16:12:55 2015 UTC,get the other mods on board and just make r/A.M.D
hardware,3c1l9u,plagues138,-8,Sun Jul 5 16:21:20 2015 UTC,"Maybe the top mod was one of those ""FUCK.REDDIT, GO TO VOAT.COM!!"" Idiots"
hardware,3c1l9u,tedlasman,2,Sat Jul 4 00:10:25 2015 UTC,"Looking at how things are going, they may not be that stupid."
hardware,3c1l9u,BlayneTX,-44,Sat Jul 4 01:06:03 2015 UTC,It wasn't a great sub imo. The top voted submissions were things like online shop payment confirmation for buying a Fury X .
hardware,3c1l9u,ritz_are_the_shitz,24,Sat Jul 4 16:50:47 2015 UTC,"There's so little voting going on there's basically no difference between new and top. Plus, every sub is gonna have shitposts. The key is the community - if I didn't care about the community Id go to the forums at toms or ltt"
hardware,3c1l9u,headband,-70,Sun Jul 5 15:46:38 2015 UTC,/r/hardware was already hijacked by Amd so they don't even need a subreddit really
hardware,3c1l9u,RX10,-68,Sat Jul 4 02:09:49 2015 UTC,AMD closed business and social media communications. RIP
hardware,3c1l9u,Darkstryke,-4,Sat Jul 4 02:30:14 2015 UTC,Grrr nVidia! /s
hardware,3c1l9u,joeyxl,-163,Sat Jul 4 00:07:38 2015 UTC,"It has to do with a bunch of redditors crying over an admin who got fired. Apparently because I don't give a fuck I'm labeled as a bad person.   Edit: wow 42 downvotes, in flattered."
hardware,3c1l9u,igacek,79,Fri Jul 3 22:54:06 2015 UTC,Did you even read the fucking post?   /r/AMD has been blacked out all of today however it doesn't seem to have anything to do with the current protesting on other subreddits. All of the mods have been removed and the sub is locked.
hardware,3c1l9u,Reapexx,39,Sat Jul 4 19:28:22 2015 UTC,"wow 42 downvotes, in flattered.   Don't be flattered:  You're just wrong."
hardware,3c1l9u,snowlovesnow,30,Fri Jul 3 23:08:31 2015 UTC,All of the active mods got removed.  There's more to it than just going private.
hardware,3c1l9u,CFGX,44,Fri Jul 3 23:22:05 2015 UTC,"If you're intentionally trying to look like a dumb fuck, you're killing it!"
hardware,3c1l9u,Supreme_Beef,20,Sat Jul 4 00:38:05 2015 UTC,"Be careful not to cut yourself on that edge, bro."
hardware,3c1l9u,tru3gam3r,31,Fri Jul 3 23:22:24 2015 UTC,If you don't give a fuck then why are you here?
hardware,3c1l9u,58592825866,15,Fri Jul 3 23:40:52 2015 UTC,Don't forget to breathe when you're on your way to pick up your disability cheque tomorrow
hardware,3c1l9u,masturbateAndSwitch,-20,Sat Jul 4 00:07:38 2015 UTC,"^That this shitty comment was upvoted is more evidence to support my theory: you can say literally anything after a comment that's highly downvoted and it will be upvoted, as long as you don't agree with the downvoted comment"
hardware,3c1l9u,58592825866,11,Fri Jul 3 23:16:12 2015 UTC,Literally? Or do you mean figuratively? I require clarification.
hardware,3c1l9u,masturbateAndSwitch,-22,Sat Jul 4 00:23:41 2015 UTC,"One of the accepted meanings of ""literally"" is ""figuratively""-- the usage is so common it's been added to the OED. But I meant ""literally"" literally (but hyperbolically), not figuratively."
hardware,3c1l9u,Fade_0,2,Sat Jul 4 03:14:01 2015 UTC,"what the fuck are you talking about  ""I mean literally. wait no, not literally literally, like figuratively literally."""
hardware,3c1l9u,masturbateAndSwitch,-5,Sat Jul 4 03:30:55 2015 UTC,"I was making a subtle distinction, but never mind."
hardware,3c1l9u,namae_nanka,-41,Sat Jul 4 03:49:57 2015 UTC,All that HBM overclocking corrupted their sub.
hardware,3c1l9u,Le_rebbit_account,-24,Sat Jul 4 15:32:11 2015 UTC,IT'S OVER AMD IS FINISHED!
hardware,3c2mn8,Exist50,14,Sat Jul 4 04:36:51 2015 UTC,That's a very competitive price. If the benchmarks hit close enough to the 980ti I migut buy one.
hardware,3c2mn8,thoosequa,6,Sat Jul 4 05:58:51 2015 UTC,It IS a very competitive price. That could be a game changer actually. I would have a reason to choose Fury over a Ti.
hardware,3c2mn8,Zimith,4,Sat Jul 4 09:11:05 2015 UTC,not really it's overpriced like always in EU
hardware,3c2mn8,Perunsan,16,Sat Jul 4 11:22:22 2015 UTC,"Overpriced here or underpriced in the US, that's irrelevant. In the EU the price tag on that card will make it able to compete with the Ti in the EU, which is what matters."
hardware,3c2mn8,Zimith,6,Sat Jul 4 12:41:07 2015 UTC,"Gets downvoted for basically explaining how comparing prices in different markets work, some quality voting right there."
hardware,3c2mn8,thoosequa,5,Sat Jul 4 14:57:37 2015 UTC,"Well but compared to the 980ti euro prices it's competitive. A local retailer sells it for 860 euros, if this stays in the 650 region we have a top seller easily."
hardware,3c2mn8,thoosequa,2,Sat Jul 4 11:48:57 2015 UTC,wow that 980ti price is crazy 641* € in here :)
hardware,3c2mn8,Perunsan,1 point,Sat Jul 4 12:04:02 2015 UTC,And that's just for the reference cooler superclocked version. Imagine what aftermarket coolers cost. The Gigabyte G1 980ti costs a whopping 900 euros on amazon
hardware,3c2mn8,thoosequa,2,Sat Jul 4 16:30:43 2015 UTC,980ti prices in Germany right now so if the fury comes in at ~600€ it is a viable choice
hardware,3c2mn8,random352486,4,Sat Jul 4 17:27:23 2015 UTC,"Bear in mind that Fury is going to be ~87,5% of Fury X performance (3584 stream processors vs 4096), and the 980 Ti beats or outperforms the Fury X in all cases (0-5% at 4K, 10% at 1440p, 15% at 1080p).  And then you overclock them and the 980 Ti overclocks 15-20% against the Fury's 5%.  87,5% of the 680 EUR you list there for the 980 Ti is EUR 595.  In other words even if you assume that the Fury X could match up to a 980 Ti, the 980 Ti would offer more performance at the same cost-to-performance ratio.  And the Fury X doesn't match up to a 980 Ti, so it's actually a worse cost-to-performance.  At EUR 600 the Fury is a bad buy.  If you're not going to offer better performance, you have to offer better cost-to-performance.  Because at the end of the day FPS is how flagship cards compete, not pricing - it's a halo-product pricing model."
hardware,3c2mn8,capn_hector,1 point,Mon Jul 6 01:19:57 2015 UTC,87.5% of the Fury X's cores doesn't necissarily relate to 87.5% of its performance.  The 980ti proves that much.
hardware,3c2mn8,Anarchyz11,1 point,Mon Jul 6 14:45:52 2015 UTC,"The FuryX is liquid cooled and still loses to the 980 TI, I don't think this one will get close on air."
hardware,3c2mn8,ClockworkOnion,-1,Mon Jul 6 03:24:06 2015 UTC,"It doesn't lose though. Depending on the game and resolution both cards are trading blows. The Fury X loses against the Titan X which was its original competitor, but it definite doesn't lose against the 980ti. So a Fury that hits slightly below a Fury X for 550 dollars and with aftermarket coolers will be a heck of card."
hardware,3c2mn8,thoosequa,2,Mon Jul 6 05:15:55 2015 UTC,"You're joking right? The 980 TI is 20% faster on average at 1080p, 10-15% at 1440p, and 5-10% at 4K. They only trade blows in games that AMD have traditionally done well in at that."
hardware,3c2mn8,ClockworkOnion,-4,Mon Jul 6 14:10:13 2015 UTC,"I'm anticipating near-Fury X performance at sub-4K, and trailing more as the resolution is increased."
hardware,3c2mn8,BlayneTX,8,Sat Jul 4 06:06:03 2015 UTC,I'll guess trading blows with the equivalently priced Nvidia card (custom 980?) at stock clocks and blown out of the water when overclocked.
hardware,3c2mn8,TaintedSquirrel,11,Sat Jul 4 12:29:28 2015 UTC,It has to be faster than the 980 otherwise AMD is wasting their time.  The 390X already trades blows with the 980.
hardware,3c2mn8,xandergod,2,Sat Jul 4 14:42:34 2015 UTC,At stock.
hardware,3c2mn8,Sirilius,2,Sat Jul 4 15:56:40 2015 UTC,"I think that's a bit pessimistic. We see that the Fury X doesn't scale well compared to the 390x at anything sub-4K. That seems to imply the extra shaders aren't being fully utilized, in which case the cut won't hurt much."
hardware,3c2mn8,StealthGhost,11,Sat Jul 4 14:31:06 2015 UTC,"Seems like the observations of the Strix 980ti's cooler might just turn out to be right. If they use the same power delivery and get OC utilities working, this could be a very interesting card."
hardware,3c2mn8,logged_n_2_say,6,Sat Jul 4 04:38:31 2015 UTC,could you clarify what was the observation on the 980ti cooler? I thought that cooler wasn't great?
hardware,3c2mn8,dylan522p,21,Sat Jul 4 05:33:42 2015 UTC,"GM200 and Fiji are within 1% of each other's die size. People noticed that two of the heatpipes for the DCUIII cooler do not even make contact with the GPU. http://i.imgur.com/as6NYoa.jpg  However, those two heatpipes might line up very well with the HBM stacks: http://www.techpowerup.com/gpudb/images/c774_Fiji_XT.jpg  Which would explain the unbalanced design."
hardware,3c2mn8,dylan522p,10,Sat Jul 4 05:43:19 2015 UTC,"Fury X and dual Fury are both rotated the wrong way for that to work though.  Unless Asus makes it work with their own pcb I guess, and they have their own pcb streamlined now from what I've read, fully automated.  So it just couldn't be reference.  They usually don't put as much effort into AMD as they do nvidia though."
hardware,3c2mn8,innerknowing,9,Sat Jul 4 10:04:34 2015 UTC,"Reference fury cards may be different, but I assume a strix fury will be a custom pcb."
hardware,3c2mn8,innerknowing,6,Sat Jul 4 12:18:35 2015 UTC,"I have to wonder if AMD will even bother releasing a reference Fury. After all, we haven't seen any leaks on it whatsoever (beyond ""triple fan"") and the AiB partners can probably do a better job."
hardware,3c2mn8,dylan522p,1 point,Sat Jul 4 16:28:31 2015 UTC,I can't wait for a super small card with fury like the size of Nano or less because I'm sure they could get the power delivery even smaller than amd did. Maybe a single fan for gpu and another for vrm low clocks. It'd be a crazy HPC. Or maybe a single fan blower design.
hardware,3c2mn8,innerknowing,2,Sun Jul 5 01:21:41 2015 UTC,"Sadly, it doesn't look like the board partners are as creative as we might like. They could do pretty awesome things with existing cards if they were willing to downclock, but I have yet to see a single card slower than reference. It's a shame, because you could probably get a pretty efficient mitx sized Hawaii card if you throttled it in.   Btw, you may find it interesting that in an interview, an AMD spokesman said they could make the Nano PCB even smaller than they did, but they were limited by the size of the PCIe slot itself. I'd love to see what they could do if they had no such limitations."
hardware,3c2mn8,dylan522p,2,Sun Jul 5 02:18:40 2015 UTC,I'd love to see fury on as small of a pub possible and clocked reallllly low like 700-800mhz. Sacrificing a few hundred mhz cuts power significantly for Tahiti. Fury might be even better.
hardware,3c2mn8,SeriousDan,1 point,Sun Jul 5 02:52:57 2015 UTC,"I mean, that basically sounds like the Nano, but with a cut down GPU."
hardware,3c2mn8,Mr_s3rius,5,Sun Jul 5 02:54:32 2015 UTC,"Its designed (allegedly)for the relatively much larger die size of the fury,so it doesn't make a lot of contact with a 980ti using all of the heatpipes it has.  Mediocre cooler for a 980ti, would be a great cooler for a fury."
hardware,3c2mn8,logged_n_2_say,6,Sat Jul 4 05:39:38 2015 UTC,"Fiji is basically the same size as GM200, but the interposed and HBM add to the footprint."
hardware,3c2ba9,zmeul,11,Sat Jul 4 02:41:03 2015 UTC,"All of cruicials memory is made by micron, since they are the same company. The others you'll have to check the chips since they can buy from either of the three"
hardware,3c2ba9,alainmagnan,4,Sat Jul 4 16:42:52 2015 UTC,Anything over 2666mhz is most likely Hynix and performs really badly.
hardware,3c2ba9,buildzoid,1 point,Sun Jul 5 01:06:11 2015 UTC,"Hynix RAM in general, or just Hynix RAM over 2.6GHz? Because Hynix DDR3 is of exceptional quality IMO, although I have only used 1066."
hardware,3c2ba9,stapler8,1 point,Sun Jul 5 03:32:42 2015 UTC,Any of the mfr and cfr ICs at any clock are god awful.
hardware,3c2ba9,buildzoid,1 point,Sun Jul 5 10:21:20 2015 UTC,Ah. I generally deal with low-speed ECCR memory from them.
hardware,3c2ba9,stapler8,1 point,Sun Jul 5 16:52:44 2015 UTC,I think it is only single rank that is awful.
hardware,3c2ba9,yuhong,1 point,Sun Jul 5 20:43:23 2015 UTC,the double rank stuff needs really loose timings so it's not much better. It's not like gamers need to care but anyone into competitive benching should avoid mfr and cfr kits like the plague.
hardware,3c2ba9,buildzoid,12,Sun Jul 5 20:46:50 2015 UTC,"Aren't all RAM modules made by the same 3-5 companies and then just branded as G Skill, Corsair, Kingston, etc?  Is there anyway to know beforehand who actually made the RAM that one may be interested in? I just purchased an 8GB kit of G Skill Ripjaws and am totally satisfied with it. I mixed those two sticks with two other 2 GB sticks (2x2GB NO-name@1333/9-9-9-24 and 2x4GB G Skill Ripjaws@1600/9-9-9-24=12GB total) and it has been working fine for the last month. No hiccups, problems or complaints from hard- or software.   Plus, they do give a lifetime warranty on their sticks, so that is definitely saying something about their products."
hardware,3c2ba9,CaptSkunk,12,Sat Jul 4 07:07:53 2015 UTC,"Since Elpida was purchased by Micron Technology in 2013, only three companies (Samsung, SK Hynix, and Micron) control the vast majority of DRAM production as far as I know:  http://images.bidnessetc.com/content/uploads/images/source/7.[40].jpg  http://static.cdn-seekingalpha.com/uploads/2015/5/71188_14319886868552_rId7.png  So aside from Nanya (<5% DRAM market share) and a few other small players, almost all DDR3 modules should be coming from the big three."
hardware,3c2ba9,aziridine86,3,Sat Jul 4 07:18:42 2015 UTC,With G.skill the 2400 CL9 and 2666 CL10 4GB per stick kits are high bin samsungs that do up to 2800 CL 10 on 1.9-2V. Anything over at or above 2800mhz is most likely Hynix and while it clocks high it doesn't perform better than much lower clock Samsung kits. Most Crucial RAM is Micron and the 1600 8-8-8-24 and 1866 9-9-9-27 kits can hit 2000 8-9-8-25 1T at 1.65V with ease however they do not really go above that.
hardware,3c2ba9,buildzoid,2,Sun Jul 5 01:10:44 2015 UTC,that is definitely saying something about their products.   Not that much really.
hardware,3c2ba9,Randomoneh,1 point,Sat Jul 4 17:45:56 2015 UTC,"Well, I'd rather have a warranty that will be good 30 years from now (yes, DDR3 will be antique by then) than one that is only good for a few years. Plus, that shows that they are willing to stand by their products.   Look at craftsmen tools or Snap On. They both after lifetime warranties and they both make damn good tools, that you have to want to destroy, to break them."
hardware,3c2ba9,CaptSkunk,2,Sat Jul 4 18:02:06 2015 UTC,I think his point is that DRAM stick manufacturers with lifetime warranties are a dime a dozen.  It's very common.
hardware,3c2ba9,Evidence_Of_Absence,3,Sat Jul 4 20:03:55 2015 UTC,"I've not yet had Kingston modules go bad on me, nor had a DOA stick."
hardware,3c2ba9,Seclorum,12,Sat Jul 4 03:06:53 2015 UTC,"Isn't almost all RAM made by Hynix, Micron, and Samsung?  I'm pretty sure the only variances between RAM you get from different brands is the heatspreaders. Everything else, including quality should be more or less identical (except for EEC-R RAM).  Edit: Want to clarify that I'm saying that RAM from companies like Kingston, Corsair, etc, is rebranded from one of those OEM's."
hardware,3c2ba9,stapler8,1 point,Sat Jul 4 05:35:56 2015 UTC,"Would not surprise me.   I've had terrible luck with Crucial and Corsair sticks over the years. Almost half of them eventually went bad, not even overclocking them. They just started spitting out nothing but errors in any machine I stuck them in.   I've got two machines I built around the same time with Kingston in them and they are still trucking. So either they fixed the issues at the original point of manufacture for the modules or they fixed whatever was wrong at the 3rd party mfg's."
hardware,3c2ba9,Seclorum,1 point,Sat Jul 4 05:45:01 2015 UTC,Could be due to the fires at the Hynix plant? They may not have initially fab'd modules up to previous spec when they reopened.
hardware,3c2ba9,stapler8,1 point,Sat Jul 4 05:46:30 2015 UTC,It was back in the 2004-2009 time period where I was getting lots of bad ram.
hardware,3c2ba9,Seclorum,1 point,Sat Jul 4 05:51:05 2015 UTC,"Ah, okay. DDR2 then? If so, I had a bad run-in myself with some when I had the Core2."
hardware,3c2ba9,stapler8,1 point,Sat Jul 4 05:57:35 2015 UTC,At the time mostly DDR2 yeah. My Kingston's are both DDR3 sets.
hardware,3c2ba9,Seclorum,1 point,Sat Jul 4 06:06:39 2015 UTC,"I haven't bought much DDR3. Bought 4GB back in 2013 when I moved to Haswell from Bearlake, and another 4GB of no name ram for $15 two months later."
hardware,3c2ba9,stapler8,1 point,Sat Jul 4 06:09:21 2015 UTC,Me neither. I got my i7 920's (Both machines are identical) a while ago and haven't felt the need to upgrade them until recently.
hardware,3c2ba9,Seclorum,1 point,Sat Jul 4 06:13:07 2015 UTC,Definitely. In fact while I was in Korea the cheapest RAM you could get was good old green pcb samsung branded memory that a vendor'd get in bulk. It worked great and never died (then again it seems like ram doesn't really die much anyway). Always made me wonder what you're getting by paying 20% more for XTREME PWNER 1337MHZ ram
hardware,3c2ba9,prodikl,2,Wed Jul 8 18:38:06 2015 UTC,"Seems to be well supported: https://www.pugetsystems.com/labs/articles/Most-Reliable-Hardware-of-2014-616/#RAM(Memory)  Then again, they did mostly test Kingston RAM, and ECC at that."
hardware,3c2ba9,Exist50,2,Sat Jul 4 04:22:59 2015 UTC,I did but I sent it in and they sent me two. Sticks back with. Same quantity. I went from.  2x8gb to 3x8gb.
hardware,3c2ba9,dylan522p,1 point,Sat Jul 4 05:40:34 2015 UTC,Isn't Kingston usually the cheapest with heat spreaders?
hardware,3c2ba9,hdshatter,-1,Sat Jul 4 09:18:07 2015 UTC,Friendly reminder of Kingston's foul play around 18 months ago.
hardware,3c2ba9,antisomething,1 point,Sun Jul 5 00:38:27 2015 UTC,"But that's an SSD, and hardly the only RAM manufacturer that has a history of foul business cough coughSamsungcough cough"
hardware,3c2ba9,continous,1 point,Mon Jul 6 03:34:57 2015 UTC,I'd have taken a shot at Samsung if their name was the first word of the post.
hardware,3c2ba9,antisomething,1 point,Mon Jul 6 23:57:15 2015 UTC,Point is; they're not evil because of one wrong doing.
hardware,3c2ba9,continous,1 point,Tue Jul 7 05:15:07 2015 UTC,Did I say that? You're putting words in my mouth.  Businesses will be businesses and all. When they do something unsavoury - remember it. Otherwise they do it again.
hardware,3c2ba9,antisomething,1 point,Fri Jul 10 00:14:12 2015 UTC,"You're heavily insinuating it; you're acting as if Kingston has a history of it, but a single instance does not equate to a history."
hardware,3c2ba9,continous,1 point,Fri Jul 10 04:05:13 2015 UTC,"You're assuming what you inferred is what I was trying to insinuate. You're reading too much into a single sentence.  I've explained that's not what I was getting at. You're the one who brought in the ""history of foul business"" concept. If I really wanted to bust Kingston/Samsung/anyone's chops, I'd use more than a throwaway line."
hardware,3c2ba9,antisomething,1 point,Fri Jul 10 23:50:24 2015 UTC,You're assuming what you inferred is what I was trying to insinuate.   Then why would you say that? There is literally no other context where it makes sense.
hardware,3c2ba9,continous,1 point,Sat Jul 11 04:03:00 2015 UTC,"They took advantage of reviewers and their clients; It's worth noting.  END  Not 'Kingston is the worst HD HW manufacturer company ever', not 'they keep doing this again and again', not 'Samsung is better by comparison'.  Just 'Remember that one time.'  You don't have to think a company is evil to want to hold it accountable for wrongdoing."
hardware,3c2ba9,antisomething,0,Sat Jul 11 14:12:15 2015 UTC,But my point though is that every company has done some wrong or another; nobody is perfect. To hold them accountable for it forever is silly; and to not buy their product purely because of it is even sillier.
hardware,3c07g7,j5v,8,Fri Jul 3 16:08:29 2015 UTC,"If you don't mind my asking what sort of performance did you get from your 970 with that LG 31""? I'm upgrading from a 24"" to a 28"", and I'm curious to know what sort of performance drop I can expect for gaming.  edit: the monitor I'm considering is 3840 x 2160, not 1080."
hardware,3c07g7,ManofToast,28,Fri Jul 3 18:25:39 2015 UTC,you do realise that physical size doesn't matter right? a 5'' 1080p screen is going to give you same performance as a 100'' 1080p screen because it's the same amount of pixels
hardware,3c07g7,FearrMe,3,Fri Jul 3 18:27:28 2015 UTC,"I do, I forgot to mention the monitor I'm looking at is 3840 x 2160, not just a regular upsized 1920x1080."
hardware,3c07g7,ManofToast,10,Fri Jul 3 18:33:36 2015 UTC,"Moving from 1920x1080 to 3840x2160 should cut your framerate down to 1/3 to 1/4th so long as you have enough VRAM, if you don't then Expect performance to plummet. You can expect VRAM requirements to increase by about 50% or more but typicaly less than 100% more.  The 970 can play older games at this resolution but is not recommended for modern titles at this kind of resolution as you will be lacking in raw proccessing power and will run into problems with tight memory constraints."
hardware,3c07g7,dudemanguy301,1 point,Fri Jul 3 19:36:43 2015 UTC,"Damn, maybe I'll hold off."
hardware,3c07g7,ManofToast,4,Fri Jul 3 19:53:57 2015 UTC,It's too early for 4k if you don't have two 980ti's/titans/980s
hardware,3c07g7,3HunnaBurritos,6,Fri Jul 3 20:51:53 2015 UTC,Or two Fury x's.
hardware,3c07g7,WillWorkForLTC,2,Fri Jul 3 21:27:54 2015 UTC,Yup :)
hardware,3c07g7,3HunnaBurritos,1 point,Fri Jul 3 21:36:22 2015 UTC,Indeed. Plenty of benchmarks around at various quality levels of various games. I would want at least one 980ti with plans for a second if gaming at that res.
hardware,3c07g7,meowffins,2,Fri Jul 3 20:03:48 2015 UTC,"No problem asking. I'm not sure exactly which aspect you're asking about: the latency of the monitor, or whether the 970 can handle 4K gaming.  Latency: It feels slightly sluggish, but I cannot say whether this is just my misguided perception of it. Since I replaced my wireless mouse, it feels better, but I haven't run any side-by-side latency comparisons with a 'gaming' monitor, nor have I attempted to quantify the latency, so I'm reluctant to suggest that this monitor is actually laggy. Also, I tend to play RPGs, rather than fast shooters, so that lessens the need for a snappy monitor.  4K gaming: Depends entirely on the title. Without turning down the settings, Witcher 3 plays well on the 970 at 2560x1440, and excellently at 1920x1080. Racing sims are really good at 4K, because you can see the track far ahead, e.g. Project Cars goes around 50-60 FPS. Colours are really good, especially for games with good lighting effects. There's usually plenty of OC room, depending on the variety of 970 you buy.    the monitor I'm considering is 3840 x 2160, not 1080 Not sure if you've spotted, the LG I have is 4096x2160, rather than 3840x2160, so I'm hedging my bets this is a general 4K gaming question."
hardware,3c07g7,mysql101,1 point,Fri Jul 3 22:29:46 2015 UTC,My 970 has noticeable frame decrease from 1080 to 1440 with the new car driving game at max settings. I bet you'll see issues too.
hardware,3c07g7,BrainSlurper,2,Fri Jul 3 19:33:09 2015 UTC,"Haha what is the new car driving game? For anyone wondering though, 1080 to 1440 should be a 33% decrease in FPS provided VRAM isn't a bottleneck."
hardware,3c07g7,mysql101,1 point,Fri Jul 3 20:59:23 2015 UTC,Project cars.   In 1080p I haven't found anything yet that slows it.
hardware,3c07g7,ManofToast,1 point,Fri Jul 3 21:39:53 2015 UTC,For sure. Maybe I'll hold off or get something a little smaller.
hardware,3c07g7,nooeh,1 point,Fri Jul 3 19:54:33 2015 UTC,I would like the link to your review thanks!
hardware,3c1nse,Fatigue-Error,3,Fri Jul 3 22:58:17 2015 UTC,Strange that they release a cooler so close to Skylake and yet don't advertise socket 1151 compatibility (though all 115X coolers will be compatible)
hardware,3c1nse,MyDreamName,2,Sat Jul 4 23:32:23 2015 UTC,what is the fpi?
hardware,3c1nse,tedlasman,1 point,Sat Jul 4 01:43:30 2015 UTC,Fins per inch.
hardware,3c1nse,BlayneTX,2,Sat Jul 4 02:00:27 2015 UTC,For this radiator.
hardware,3c1nse,tedlasman,1 point,Sat Jul 4 02:12:51 2015 UTC,Slimmer versions of their excellent TD02 and TD03 coolers.
hardware,3c1nse,ok_backbay,2,Fri Jul 3 22:58:40 2015 UTC,"I had the TD03 on my 4570, with two Noctua fans, got around 40C.  After 2 years the pump crapped out on me, rather than RMA it I bought a Hyper 212 EVO, and my CPU is at 30C idle.  Just my experience, but I'd rather stick with air cooling now, than these close loop water systems."
hardware,3c1nse,jinxnotit,3,Sun Jul 5 00:45:50 2015 UTC,And if you took temperatures measured over time at load the water cooled unit would be more consistent where as the air cooler would prone to heat spikes and surges and just not very controlled.  I think the biggest weakness on most of these AIO's at this point is their water block and self contained nature.
hardware,3c1nse,iPlayRealDotA,1 point,Sun Jul 5 05:06:17 2015 UTC,But then you won't be able to tell people that your computer is liquid cooled though!
hardware,3bxxn5,Exist50,23,Fri Jul 3 01:56:49 2015 UTC,"How low would the clock have to be to go from 275w to 175w?  edit - Opps, it's in the article. Speculation though.   because clockspeeds, voltages, and power consumption have a non-linear effect, at this point in time it is reasonable to assume that AMD is going to be able to hit and sustain relatively high clockspeeds even at 175W just by backing off on load voltage. AMD is not giving us any expectations for clockspeeds at this time, though on a personal note based on the kind of clockspeed scaling we see on other 28nm GPUs, I would be surprised if a 175W Fiji could not sustain 800MHz or better in games at 175W, assuming the cooler is capable of dissipating that much heat."
hardware,3bxxn5,Scrabo,18,Fri Jul 3 01:59:58 2015 UTC,"Depends how high AMD clocked it compared to where it can go. Hawaii underclocks beautifully because AMD clocked it a bit beyond its thermally optimal point, while Maxwell is much more conservative (and thus overclocks well).   Edit: Hijacking (what was the) top comment to post an interesting video: https://www.youtube.com/watch?v=KIiNFXf00_U&feature=youtu.be&t=7m55s  Also, from Ryan Smith himself: http://forums.anandtech.com/showpost.php?p=37531438&postcount=16"
hardware,3bxxn5,makar1,4,Fri Jul 3 02:04:24 2015 UTC,"That would also suggest that the Fury X doesn't have much overclocking headroom, even with unlocked voltage."
hardware,3bxxn5,slapdashbr,3,Fri Jul 3 12:56:28 2015 UTC,"well, it isn't going to be great, but the integrated water cooler will help. Stability is usually worse when you are pushing the thermal envelope. Not going to get you more than about an extra 20MHz or so though"
hardware,3bxxn5,TehRoot,1 point,Fri Jul 3 13:25:34 2015 UTC,"Hawaii overclocked great. You could get another 120-145 core, and good cards could get from 1050 to 1200+."
hardware,3bxxn5,dylan522p,2,Fri Jul 3 15:06:46 2015 UTC,That's really meh. 10% is not close to what we got with the rest of the Radeon lineup for past 3 years or Kepler or Maxwell.
hardware,3bxxn5,innerknowing,1 point,Fri Jul 3 19:16:14 2015 UTC,Tahiti was not really any better than Hawaii if you are looking at the GHz editions oc potential
hardware,3bxxn5,dylan522p,1 point,Sat Jul 4 05:43:57 2015 UTC,Stock Tahiti was amazing ocer. Look at the clocks it was stock and look at was a ok of was. Tahiti wasnt that crazy compared to Pitcairn and the other smaller chips though
hardware,3bxxn5,continous,1 point,Sat Jul 4 05:55:34 2015 UTC,Pitcairn   Still relevant cause free performance upgrades.
hardware,3bxxn5,bizude,1 point,Sat Jul 4 17:15:50 2015 UTC,I'm able to hit 25% OC's on my 290x.
hardware,3bxxn5,wanking_furiously,2,Thu Jul 9 15:32:27 2015 UTC,Using an exponent of 2.6 gives 84%. This is just a number that a professor gave me once and ignores differences in cooling or temperature tolerance.
hardware,3bxxn5,Seclorum,37,Sun Jul 5 13:21:30 2015 UTC,If anything I believe the Fury will be fully unlocked while the Nano is the cut down.
hardware,3bxxn5,TRD099,13,Fri Jul 3 02:51:31 2015 UTC,"That seemed to be the leading opinion, right next to Fury X= Fiji XT, Fury= Fiji Pro, and Nano= Fiji LE. I'd think that given the die size, the latter would be true."
hardware,3bxxn5,bphase,3,Fri Jul 3 04:31:25 2015 UTC,"Pretty sure it  was going to be Fury X = Fiji XT, Fury = Fiji XT non WC and Nano = Fiji (PRO/LE or w/e they are going to call it.)."
hardware,3bxxn5,redditedc,3,Fri Jul 3 17:41:25 2015 UTC,You mean board size?
hardware,3bxxn5,redditedc,3,Fri Jul 3 08:34:37 2015 UTC,"Die size. With such a large chip, you have both a greater chance of there being a defect and more room to cut it down. Tahiti even at one point had two cut models, though no longer."
hardware,3bxxn5,hans_ober,3,Fri Jul 3 15:05:55 2015 UTC,I also hope it's just some miscommunication. There is a thought in the back of my mind though... AMD might actually be planning on doing this. They could be putting the only air-cooled full-Fiji on a compact form-factor card to again charge a premium over a standard dual-fan card.
hardware,3bxxn5,Pimptastic_Brad,6,Fri Jul 3 06:31:38 2015 UTC,Apparently it's from an AMD rep: http://forums.anandtech.com/showpost.php?p=37531438&postcount=16
hardware,3bxxn5,rationis,6,Fri Jul 3 07:11:38 2015 UTC,Doesn't mean it was mixed up internally at AMD though. We saw how much trouble Nvidia had with inter-office communication with the 970 specifications.
hardware,3bxxn5,sabot00,12,Fri Jul 3 08:24:53 2015 UTC,"Yeah right, inter-office communication.  I'm not a fan of either company (I use Intel HD), but that's a horrible excuse."
hardware,3bxxn5,redditedc,1 point,Fri Jul 3 11:13:33 2015 UTC,Why?
hardware,3bxxn5,phigo50,5,Fri Jul 3 16:11:25 2015 UTC,I use Intel HD   lol
hardware,3bxxn5,ritz_are_the_shitz,1 point,Sat Jul 4 01:24:55 2015 UTC,Well Intel's drivers are pretty good.
hardware,3bxxn5,GthrowawayG,2,Sat Jul 4 01:23:06 2015 UTC,"Unlike the 970 thing, though, it is very easy to test whether this is a full chip. Any misinformation would be dispelled the second reviews are up, so there's no point to it."
hardware,3bxxn5,Seclorum,2,Fri Jul 3 15:09:45 2015 UTC,"I'm not saying this is some sort of screwup/coverup that will last past launch, rather that it's possible that a datasheet handed to the marketing team had the names Fury and Nano transposed."
hardware,3bxxn5,elevul,1 point,Fri Jul 3 17:30:24 2015 UTC,"I would think this too, I seem to remember them saying at the announcement on June 16th that the Nano has similar performance to a 290x."
hardware,3bxxn5,-RYknow,2,Fri Jul 3 12:39:22 2015 UTC,"I thought they said significantly less power for significantly more performance?  if this is an underclocked fully-fledged fiji, that could be true. clockspeed increases don't increase linearly with voltage, as we know. so if the nano comes out with a .9v or .8v @750-800mhz I could see it having a 175-200w tdp and ~fury (non-x) performance.  of course, such performance would make the fury worthless  and it would mean as soon as you blocked it it would be as good as the fury X.  this is gonna be interesting."
hardware,3bxxn5,Maimakterion,1 point,Sat Jul 4 15:55:15 2015 UTC,I think whether or not the Fury is going to be fully unlocked is going to be dependent on yield
hardware,3bxxn5,headband,2,Fri Jul 3 18:09:52 2015 UTC,It always does.
hardware,3bxxn5,epsys,6,Fri Jul 3 18:29:44 2015 UTC,"Probably their best chips will go into the Nano, the ones capable of undervolting the most, while the rest will go into the Fury X because with that watercooler the voltage doesn't matter."
hardware,3bxxn5,kr239,5,Fri Jul 3 12:02:32 2015 UTC,The first part is almost exactly what the article says. Seems to also imply that the Nano will be a good overclocker with a better cooler.
hardware,3bxxn5,epsys,4,Fri Jul 3 15:13:27 2015 UTC,"I, for one would love to see this start a trend. I've become more fond of smaller builds. Having more then capable cards in this size range would be excellent IMO. Sure, you can get fantastic cases that fit the huge cards, but I think my system could overall look much better with a shorter card like this. I build systems as a hobby. So big power and eye candy are equally as important to me.  Just my $.02."
hardware,3bxxn5,dylan522p,13,Fri Jul 3 19:25:38 2015 UTC,"Personally, I don't believe this. Yields would have to be superb to have two fully enabled SKUs. While we haven't heard of any issues, with such a big die, I'm not so confident."
hardware,3bxxn5,dylan522p,19,Fri Jul 3 01:58:03 2015 UTC,It's entirely possible they're in oversupply of full Fiji die because of low HBM+interposer yields.
hardware,3bxxn5,dylan522p,9,Fri Jul 3 02:57:16 2015 UTC,"The interposer should have near perfect yields. I think it's 65nm, and has basically nothing that can go wrong given how little it does.  Also, since the Nano will also come with 4GB of HBM, that similarly shouldn't be a factor."
hardware,3bxxn5,dylan522p,9,Fri Jul 3 03:00:25 2015 UTC,The tsvs are the tricky part
hardware,3bxxn5,TehRoot,4,Fri Jul 3 03:20:20 2015 UTC,that's why  sometimes you make two for twice the price
hardware,3bxxn5,dylan522p,2,Fri Jul 3 07:36:09 2015 UTC,"Is that you, Mr Hadden?"
hardware,3bxxn5,Anaron,2,Sun Jul 5 11:25:30 2015 UTC,Thank you for getting that reference. I did not want to have to search for his name and link a picture.
hardware,3bxxn5,logged_n_2_say,4,Sun Jul 5 18:51:26 2015 UTC,"Yes, but the Hbm is low yield and laying the Hbm on the interposed requires ridiculously accurate placement."
hardware,3bxxn5,jinxnotit,4,Fri Jul 3 03:42:42 2015 UTC,"Any source on the HBM yields? Poor yields have been thrown around a lot, but I've yet to see anything authoritative. And this entire industry is based on really fine tolerances. One would think if AMD had such an excess of chips they'd try to make it much cheaper. After all, they do no good sitting around."
hardware,3bxxn5,slapdashbr,5,Fri Jul 3 04:07:04 2015 UTC,"All tsv based stacked technologies are. Nand is a bit more simple and has been worked on more, but stacking dram has a more complicated technology. Asking for proof of low yields is insane cause they obviously don't release that information. Any new semiconductor technology is going to have low yields. There's a reason amd is the only one using it and only in their halo products. Look at pricing of Hbm. Fiji is a smaller die and less memory than gm200 yet it still is the same price and amd take lesser margins aswell as evidenced by the lesser performance but still being forced to put an equivalent price."
hardware,3bxxn5,ritz_are_the_shitz,4,Fri Jul 3 05:32:20 2015 UTC,"It's new and in low production, but that may be because there are few products to use it. I thought it was quite clear that AMD did not have the resources to redesign its product stack, which would look a lot more competitive with parts as efficient as Fiji. Also, all public statements have been that yields are fine, so it's just meaningless to assume otherwise. As for the reason AMD is the only one using it, well, they co-designed it. That seemed obvious.  Also, GM200 and Fiji are within about 5mm2 of each other. That's negligible. And by the fact that two other Fiji products are coming, it's clear that AMD does not need to sell the Fury X for its price. I do suspect there are driver and optimization issues, though. Fury X's performance does not scale as one would expect compared to the 390x.  Look, I'm not saying HBM yields are necessarily good, but that it certainly can't be assumed to be limiting AMD's chip production. You can't have 4 SKUs in reasonable quantities for a tech that is severely limited. And the Fury and Nano will likely sell better than the Fury X."
hardware,3bxxn5,BlayneTX,1 point,Fri Jul 3 05:45:06 2015 UTC,"They have 1 skus until, essentially none because that one is sold out. They will come out with others later but for now they only have one"
hardware,3bxxn5,alainmagnan,1 point,Fri Jul 3 08:24:14 2015 UTC,"The Fury is coming the 14th, so it's not like there is going to be some significant delay in availability. The time difference is too small to have any significant effect on HBM availability."
hardware,3bxxn5,alainmagnan,1 point,Fri Jul 3 15:08:34 2015 UTC,And I guarantee it will be limited supply and sold out
hardware,3bvfgh,ayy-bee,35,Thu Jul 2 13:54:23 2015 UTC,"The one thing I'm not seeing anyone talk about: minimum FPS.  If you have FRTC enabled and the temps are lower, that gives the GPU headroom to boost voltage/clocks for highly taxing frames and reduce the top few % of frame times.  AMD's power subsystem is extremely complex and this gives them an opportunity to take advantage of it.  Is there any article/review showing frame time analysis with FRTC?"
hardware,3bvfgh,spiker611,8,Thu Jul 2 15:01:32 2015 UTC,"Really good point, I'd like to see benchmarks with it on and off."
hardware,3bvfgh,Slyons89,2,Thu Jul 2 16:10:53 2015 UTC,I think this is already going on with NVidia and their boost technology if you enabled vsync and left triple buffering off.  The weird part to me is how this is slowly changing the black and white performance of the gpu. If you let this boost business stuff get out of hand you could have a card that runs today's games fantastically and next year's horribly
hardware,3bvfgh,epsys,2,Fri Jul 3 10:03:33 2015 UTC,"I've used the one in Radeon Pro for a while, and I have to say there is some stuttering when using Vsync in certain games. But that's probably an engine issue.  Also yes it seems more likely to turbo as much as it needs to render a frame."
hardware,3bvfgh,jorgp2,2,Thu Jul 2 17:45:45 2015 UTC,"Am I missing something? FRTC kicks in when the the graphics card is outputting a lot of frames, i.e., on situations that aren't demanding.  Minimum framerates happen during demanding situations.  So FRTC shouldn't affect minimum framerates."
hardware,3bvfgh,Yearlaren,8,Thu Jul 2 22:18:24 2015 UTC,"A game has demanding parts and not-so-demanding parts.  Say, for an exaggerated example, without FRTC your max/min rates are 100 and 30 fps.  Your GPU is running at 100% the whole time and getting pretty hot.  With FRTC at 60 fps, your GPU is running at considerably less than 100% for the majority of the time, allowing it to cool down.  Then, when the demanding scene is being rendered, the GPU has more thermal headroom to boost voltage and clocks to render the frame faster.  This would theoretically boost the minimum FPS.  This is a theory, but given AMD's power architecture it is not impossible.  I'm wondering if they have implemented something like this."
hardware,3bvfgh,spiker611,7,Thu Jul 2 22:37:54 2015 UTC,I guess that makes sense. You could be playing a game where in just a second the graphics card goes from 25% to 100%.  But how many graphics cards throttle because of temperature? This definitely won't help the Fury X because it is liquid cooled.
hardware,3bvfgh,Yearlaren,1 point,Thu Jul 2 22:47:53 2015 UTC,I know at least the R9 290X does.
hardware,3bvfgh,chaddledee,3,Fri Jul 3 08:57:25 2015 UTC,"Only the reference model, and a handful of poorly designed aftermarket coolers."
hardware,3bvfgh,makar1,0,Fri Jul 3 12:52:00 2015 UTC,", it's not only about temperatures but tdp, too. If you divide each second's TDP into 90 frames you have to render 50% more within that TDP than if you're only rendering 60 frames. Those 60 frames could then be 1.5x more complicated within the same TDP  Wait. I'm not sure how that's any different from a more complex game that just used all the GPU  Also keep in mind once we get overclocking up and running, things should change"
hardware,3bvfgh,epsys,12,Fri Jul 3 10:06:38 2015 UTC,"Does anyone know if this only available for 3XX-series and Fury-series cards, or if it's usable on the 2XX-series? I.E.: Is this entirely driver-based, or is it a driver-controlled hardware function?"
hardware,3bvfgh,Zaev,9,Thu Jul 2 15:17:26 2015 UTC,Since its available for 3xx which is more or less the same as 2xx its driver based. I hope the next driver includes it cause i want this for my 290. No point in using more power than is needed. My monitor maxes at 75hz.
hardware,3bvfgh,jakobx,2,Thu Jul 2 15:27:46 2015 UTC,"Those were my thoughts, but I figured there might be a subtle hardware change in the 3XX series that wasn't immediately noticed. I'm running 2x 270x on a monitor OC'd to 72hz, myself."
hardware,3bvfgh,Zaev,4,Thu Jul 2 15:31:16 2015 UTC,"i'll copy what i posted in another thread who asked if it would work with the 200 series   There shouldn't be a hardware limitation on gcn 1.1 and 1.2 cards if it works with a 390x and fury x.  And he says it works on 300 series so that means it should work on the gcn 1.0 370, which is essentially a higher binned 7850 and 265.  That means I don't see any hardware limitations (for 7000 and 200), but who knows if it will come to those cards. knowing amd, it should but if it does it may be down the line."
hardware,3bvfgh,logged_n_2_say,4,Thu Jul 2 15:43:19 2015 UTC,"Well there's this: https://twitter.com/Thracks/status/616643025297649665  ‏@Technos_Eric:     @amd_roy @killyourfm @AMDRadeon @pcworld is FRTC only on the 300 series and Fury?   ‏@Thracks:   @amd_roy @killyourfm @Technos_Eric @AMDRadeon @pcworld Yes it is.      ‏@Ramzinho:     @Thracks @killyourfm Can We consider RFTC a Variable Controllable Vsync. Also Can we have it for the 200 Series..? ""Technically Speaking""   @Thracks:     @Ramzinho @killyourfm No, FRTC is not variable v-sync. It is a feature we built specifically for the 300 Series.   Edit:  http://www.forbes.com/sites/jasonevangelho/2015/07/03/amd-bringing-frame-rate-target-control-feature-to-radeon-200-series-cards-this-month/   “FRTC is supported on Fiji, Grenada, Antigua, Trinidad, Tobago, Hawaii, Tahiti, Curacao, Bonaire, Tonga.”          After a chat with AMD to iron out the details, I can confirm that Radeon 200 series users will get FRTC later this month.      Robert Hallock (‏@Thracks):   @hpstg84 @Ramzinho The cutoff will be 200 Series, but I'm glad 3rd party tools like RTSS exist for savvy users.          @hpstg84 Those are the GPUs we want to test and officially support. We leave the door open for the other GPUs. That's pretty much it."
hardware,3bvfgh,OftenSarcastic,1 point,Fri Jul 3 02:12:19 2015 UTC,Now that's a clusterfuck of varying information.
hardware,3bvfgh,christes,1 point,Sat Jul 4 02:47:55 2015 UTC,Yeah nobody can ever accuse AMD's PR guys of doing their job properly.
hardware,3bvfgh,OftenSarcastic,3,Sat Jul 4 08:31:20 2015 UTC,My guess is that we'll see a big driver update for the Windows 10 launch and we'll see this enabled then on the 2XX cards.
hardware,3bvfgh,blackeyedcheese,94,Thu Jul 2 21:20:19 2015 UTC,"When I come to /r/hardware, I expect a greater degree of understanding of the subject matter from the audience, but the comments here are just atrocious. Let me help clarify things a bit without replying to each of you.  The frame rate that a graphics card outputs is distinctly different and entirely separate from the rate at which your display panel refreshes. Vsync was the first successful attempt at resolving this issue by fixing the frame rate not only to the refresh rate of the display, but by actually syncing the two together to eliminate screen-tearing.  Frame capping does not achieve what Vsync does. You can set a frame rate cap to 60, set your display to 60Hz, and still get screen tearing because the refresh isn't synced with the output. On the plus side, frame capping doesn't result in the same input lag that Vsync does, as Vsync literally has to ""wait"" for your graphics card to output a frame before it can display it completely, thus resulting in that signature mouse drag Vsync creates.  VRR, ARR, or whatever branding you want to give it (Freesync/Gsync) eliminates the problem of input lag while syncing altogether by allowing the refresh rate of the display to change. Until you buy a monitor capable of one of these proprietary functions, you either have to suffer with Vsync or frame capping.  So for those of you wondering why someone would even want to cap their frame rate, that's precisely why. Having your GPU pump out 115FPS on your favourite title won't matter if your display can only refresh 60 times per second, essentially grabbing a new frame half of the time. In other words, your shitty monitor is already limiting you, while your graphics card is running at full power trying to squeeze a camel through a pisshole."
hardware,3bvfgh,Oafah,7,Thu Jul 2 14:43:29 2015 UTC,"So for those of you wondering why someone would even want to cap their frame rate, that's precisely why. Having your GPU pump out 115FPS on your favourite title won't matter if your display can only refresh 60 times per second, essentially grabbing a new frame half of the time. In other words, your shitty monitor is already limiting you, while your graphics card is running at full power trying to squeeze a camel through a pisshole.   Well, there are some cases where running more than 60 fps can be advantageous, I don't know of any recent cases but for instance in the original CoD, running at like 240fps allowed you to jump a little higher."
hardware,3bvfgh,C0R4x,6,Thu Jul 2 15:44:23 2015 UTC,"That's...interesting.  And to be fair, yes there are some advantages; some of them directly measurable.  For example, a higher frame rate will result is ""smaller"" tearing than a slower one, because the panel actually grabs lesser portions of each frame to compile one complete, unsynced frame.  In other words, think of it like a deck of cards with a picture painted on the side of the deck. If you've got a normal sized deck of 52 cards, and you slightly slide the deck so that the stack is ""parallelogramed"", the picture is still whole, even if a bit distorted. Now instead, try taking just two over-thick cards with a single picture painted between them and do the same. You'll get one giant slice down the middle that mismatched the two halves in a glaring obviously way.  While this analogy isn't exactly applicable, it does give you an idea of how a panel with a fixed refresh rate can ""sample"" multiple frames to create (unintentionally) a smoother one."
hardware,3bvfgh,Oafah,-10,Thu Jul 2 15:51:45 2015 UTC,"For example, a higher frame rate will result is ""smaller"" tearing than a slower one, because the panel actually grabs lesser portions of each frame to compile one complete, unsynced frame.   That's utter crap.  There's no smaller or larger tearing, there's just tearing.  It's a horizontal mismatch of framedata, nothing more - sometimes it's closer to the top, sometimes closer to the middle and framerate only has impact in a randomised sense based on the nature of renderer in question (ie how ir draws the screen in context to what frame count it outputs and how that framecount/sec divides into the hz refresh rate of the display).  There is no blanket rule.   That's...interesting.   No it's not, it's basic Quake4Arena stuff - the physics is tied to the game's internal refresh rate.  You increase the refresh rate beyond the accustomed limit of 60f/s and you start to monkey with the physics.  This is basic stuff, nothing interesting about it."
hardware,3bvfgh,steak4take,10,Thu Jul 2 16:58:40 2015 UTC,That's utter crap.   Actually it's just math. Simple math.
hardware,3bvfgh,Oafah,-9,Thu Jul 2 17:00:20 2015 UTC,"No, it's utter crap.  The framerate needs to divide into the hz rate of the display for full frame display.  When it can't you get horizontal mismatches (what people often refer to a tearing).  No matter how high the framerate, if it doesn't divide evenly into the hz rate those mismatches can occur anywhere."
hardware,3bvfgh,steak4take,9,Thu Jul 2 17:05:56 2015 UTC,"Yes, and that has nothing to do with what I said.  If you've got 3, 4, or 5 times the frame rate when compared to your refresh rate, your display with display partial frames that are more similar and over a wider sample, thus reducing the ""size"" of the tear."
hardware,3bvfgh,Oafah,3,Thu Jul 2 17:16:54 2015 UTC,"Yep.  The higher your frame rate is relative to the refresh rate, the smaller and more frequent the discrepancies(tears) will be during each scanout.  In other words if you had a frame rate on a 1080p monitor that was 1080x faster than the refresh rate, then you could effectively be rendering a unique frame for every scan line.  This would remove the sharp visible tears by distributing them across the height of the display and would instead appear as a progressive shear.  More specifically*, that shearing would only be visible for content that's in motion relative to the retina, while content that is stationary relative to the retina (a moving object that you're tracking with your eye) would actually appear geometrically correct.  *This is assuming a traditional progressive scan display, which is why the upcoming VR displays are using a global update and strobe."
hardware,3bvfgh,hughJ-,1 point,Thu Jul 2 22:38:43 2015 UTC,Thank you! :)
hardware,3bvfgh,C0R4x,-11,Fri Jul 3 07:35:21 2015 UTC,"Please.     a higher frame rate will result is ""smaller"" tearing than a slower one   That's what you said and it's bullshit.  The number is irrelevant, what matters is how that number divides into the hz rate of the display.  That's it. that's all.  Never heard of AD stepping down?  Don't know how a MODEM works?  No wonder you find basic shit like breaking the physics in Q3A engine game ""interesting""."
hardware,3bvfgh,steak4take,7,Thu Jul 2 17:20:06 2015 UTC,"Well, I'm done talking to you. You clearly have far too much anger inside of you, and not enough evidence to have a healthy discussion with."
hardware,3bvfgh,Oafah,-14,Thu Jul 2 17:22:50 2015 UTC,"""I'm taking my ball and going home.  You were mean to me."""
hardware,3bvfgh,steak4take,6,Thu Jul 2 17:24:02 2015 UTC,What an angry misinterpretation of what they said.
hardware,3bvfgh,electricheat,30,Thu Jul 2 18:01:14 2015 UTC,"On the plus side, frame capping doesn't result in the same input lag that Vsync does, as Vsync literally has to ""wait"" for your graphics card to output a frame before it can display it completely, thus resulting in that signature mouse drag Vsync creates.   This is incorrect, frame capping will result in the same input lag as Vsync, however it won't result in the frame-delay that you get with Vsync on if a frames rendering time exceeds 1000/hz ms which causes frame-rate drops (which obviously also increases input delay, so input delay will be worse with Vsync than frame capping, but only because a loss in FPS).   Having your GPU pump out 115FPS on your favourite title won't matter if your display can only refresh 60 times per second, essentially grabbing a new frame half of the time.   Yes it will, 115FPS will lead to less input lag. Comparison of how old each frame is when displayed with 115FPS vs 60FPS on a 60hz monitor:  0.00ms Rendering starts  16.6ms First time monitor grabs frame: 16.6ms old with both.  33.3ms Second time monitor grabs frame: 16.6ms old with 60FPS, with 115FPS the first frame finished after 8.69ms and a new frame was started, however the second frame didn't finish in time to be ready for when the monitor grabbed a frame for the first time. The second frame was finished at 17.38ms and the third frame was started. The third frame finished at 26.07. So when the monitor grabs a frame for the second time it actually grabs the third frame rendered by the graphics card, a frame which is now 7.23ms old, which means on this frame there's a 9.37ms lower input delay with 115FPS compared to 60FPS."
hardware,3bvfgh,MaloWlol,7,Thu Jul 2 15:59:11 2015 UTC,"0.00ms Rendering starts  16.6ms First time monitor grabs frame: 16.6ms old with both.  33.3ms Second time monitor grabs frame: 16.6ms old with 60FPS, with 115FPS the first frame finished after 8.69ms and a new frame was started, however the second frame didn't finish in time to be ready for when the monitor grabbed a frame for the first time. The second frame was finished at 17.38ms and the third frame was started. The third frame finished at 26.07. So when the monitor grabs a frame for the second time it actually grabs the third frame rendered by the graphics card, a frame which is now 7.23ms old, which means on this frame there's a 9.37ms lower input delay with 115FPS compared to 60FPS.   Note that because tearing isn't mentioned, this is really a description of the difference between vsync and triple buffering, not vsync and unsynchronized uncapped rendering. Without some form of synchronization, you will get tearing and different parts of the displayed frame will have different age and thus there isn't one single input latency number."
hardware,3bvfgh,wtallis,0,Thu Jul 2 16:33:42 2015 UTC,"It is true that the example used is very simple and in reality you'll have varying frame-render times and occasional screen tearing.   and thus there isn't one single input latency number.   This is somewhat incorrect, my example assumes double-buffered rendering (the most common kind) with Vsync off. That means that there are 2 buffers much like Vsync, but the graphics card doesn't wait for a refresh from the monitor to swap between them and start the next frame. What this means is that tearing will not occur when the monitor grabs a frame while the graphics card is rendering a frame, but rather tearing will only occur when the monitor grabs a frame while the graphics card just finished a frame and is currently copying it over to the back-buffer. The time it takes to copy over a frame to another buffer is much smaller than the time it takes to render the frame so therefor the chance of getting a tear in the frame is somewhat low, and that's why we're not getting screen tearing in every frame rendered when playing games with Vsync off. So while it is true that my example doesn't account for screen tearing and that sometimes only half of the screen will have a lower input latency, the majority of them will be frames without tearing and will therefor have a single input latency number."
hardware,3bvfgh,MaloWlol,4,Thu Jul 2 16:50:54 2015 UTC,"I don't think you understand how double buffering is done. There's no copying between buffers, just a pointer swap. The pointer for where the display controller should read data to send over the wire is changed to point to the newly completed frame, thus making it the ""front buffer"", and the other buffer is wiped so that the next frame can be drawn into it (treating it as the back buffer).  If the GPU finishes drawing a frame and updates the front buffer pointer at any time other than during the vertical blanking period (less than 4% of the total time for 1080p60), then you get tearing. If there's no synchronization but the rendering is averaging about the same frame rate as the display's refresh rate, then you will have an average of just under one tear per frame. If the rendering is running about twice as fast as the display, then you get a minimum of two tears per frame.  You may be thinking that sending a frame to the monitor is an atomic and fast operation. It isn't. Frames aren't sent in a burst; they're streamed to the monitor over almost the entire duration of the refresh interval. HDMI and DVI clock the data link only as fast as necessary to be able to transmit the whole frame in 1/60th of a second (or whatever the refresh rate is). DisplayPort operates at one of a handful of fixed data rates and the vertical blanking period is sized to provide the appropriate amount of padding—this is why all the variable refresh rate systems are implemented on top of DisplayPort. In general, the display is almost always in the process of receiving a frame."
hardware,3bvfgh,wtallis,5,Thu Jul 2 18:16:02 2015 UTC,"This is incorrect, frame capping will result in the same input lag as Vsync, however it won't result in the frame-delay that you get with Vsync on if a frames rendering time exceeds 1000/hz ms which causes frame-rate drops (which obviously also increases input delay, so input delay will be worse with Vsync than frame capping, but only because a loss in FPS).   Actually framerate capping can reduce latency.  Here's some data which makes this clear. (source)  Simply enabling V-Sync (double buffering) without a framerate cap has 66ms latency.  Reducing the Flip Queue size to 1 reduces latency by one frame. (in most games it will actually reduce latency by two frames or more)  Adding a framerate cap via the RivaTuner Statistics Server (RTSS) removes yet another two frames of latency!  And finally, enabling a framerate cap in the game engine reduces latency by another frame.  All of that with double-buffered V-Sync enabled.  Disabling V-Sync would shave off one last frame of latency, but you would then have screen tearing.  So that's four frames of latency eliminated by using a framerate cap.     Now I do find that performance can be a bit more uneven in many games when you do this - eliminating those buffered frames means that you're far more likely to miss a v-sync and skip a frame - so what I normally do is just set the Flip Queue size to 1.  Doing this will shave off two frames of latency in most games for free, since the default Flip Queue size is 3 unless the application specifies otherwise.  I'm not playing games competitively online, and that's the better balance between latency and smoothness in my opinion.      Yes it will, 115FPS will lead to less input lag.    Correct. It will be ugly, but you will have lower input lag."
hardware,3bvfgh,163941,1 point,Thu Jul 2 19:41:45 2015 UTC,"If only RTSS could alter flip queue size. Radeonpro is the only program I know of that does it, but RP is outdated now and bugs out with a lot of newer games. AMD poached the developer then never bothered implementing that feature into their drivers. Thanks AMD!"
hardware,3bvfgh,58592825866,1 point,Thu Jul 2 23:46:29 2015 UTC,"If only RTSS could alter flip queue size. Radeonpro is the only program I know of that does it, but RP is outdated now and bugs out with a lot of newer games. AMD poached the developer then never bothered implementing that feature into their drivers. Thanks AMD!   Is the Flip Queue size not stored in a game's profile on AMD?  With NVIDIA it's just a regular driver setting (listed as ""Maximum Pre-Rendered Frames"") so you only have to set it once in the game profile and never have to change it again."
hardware,3bvfgh,163941,1 point,Thu Jul 2 23:53:04 2015 UTC,What?
hardware,3bvfgh,58592825866,1 point,Thu Jul 2 23:59:33 2015 UTC,"What?   Sorry if that was unclear before.  Does changing the Flip Queue setting on AMD require you to keep RadeonPro running all the time?  I thought it was like NVIDIA Inspector which is an unofficial program that sets hidden/advanced options in the NVIDIA Driver, rather than a tool which needs to stay open to have an effect."
hardware,3bvfgh,163941,1 point,Fri Jul 3 00:15:36 2015 UTC,"I don't know, but GTA V always crashes when RP is open, so I could only limit the framerate with RTSS. But if the flip queue setting remains active even when RP is closed, I guess I should install it again and use it along with RTSS."
hardware,3bvfgh,58592825866,1 point,Fri Jul 3 00:19:03 2015 UTC,"RP actually works with gta5 you just have to set some compatibility settings, here is how  http://forums.guru3d.com/showpost.php?p=5052190&postcount=27  for me however it seems like not all of the settings work ... and you can't run RP and msi afterburner at the same time"
hardware,3bvfgh,avrion,1 point,Sat Jul 4 19:39:20 2015 UTC,"Somewhere in the catalyst control center, there's an option for number of pre-rendered frames, or at least there was when I was using a 5850."
hardware,3bvfgh,therealab,-1,Fri Jul 3 00:23:13 2015 UTC,"I don't understand why adding a frame-cap while having Vsync on would be beneficial. Obviously my previous comment assumed Vsync off when a frame-cap was added, since in theory I think adding a frame cap while Vsync is on should do absolutely nothing. Are you sure that Vsync is actually on during the tests with frame-cap, and that the frame cap doesn't override the Vsync setting to be turned off?  Also the data you provided shows that triple-buffering has a higher input delay than double-buffering, which it shouldn't have. The benefits of triple-buffering is basically exactly like V-sync off but without tearing.  So while I can agree with you that frame-capping can reduce latency, I would reserve that can to only being scenarios where something is horribly wrong, either because the developers who made the game-engine were were incredibly incompetent or by there being some OS/Driver issue. There is no reason any game should see an input-lag increase with triple-buffering or an input-lag decrease with a framerate-cap."
hardware,3bvfgh,MaloWlol,2,Thu Jul 2 21:14:51 2015 UTC,"I don't understand why adding a frame-cap while having Vsync on would be beneficial.   OK, here is one scenario:  Say you can render a game at 600 FPS uncapped.  If you enable V-Sync at 60Hz, your frame will be rendered in 1/10th of a frame and the rest of that time is spent waiting for the display to update before it can work on the next frame.  So you have almost a whole frame of latency there.  If you delay rendering at the right stage in the pipeline so that it only renders that frame in the last 10% before the display refreshes, the latency between when your input was sampled and when it was displayed is only 1/10th of a frame rather than sitting there waiting 90% of the frame after rendering it.  To put it another way; adding the frame limit reduces the latency from 15ms to 1.67ms.     Now I don't pretend to know all of the reasons, but my loose understanding is that if you implement a frame cap in the right stage of the pipeline (and RTSS gets it right) it can basically prevent a number of frames from being buffered in advance, since the engine only has the frame that it just worked on available to present the next time the display refreshes, rather than queuing up several frames in advance.  Being external to the engine means that it adds its own latency, but the latency which it adds at this stage often ends up considerably lower than allowing the framerate to be uncapped.  That's why, if the option is available and implemented correctly, capping in the engine can reduce latency even further than using RTSS.  So that's another reason why framerate limiting can reduce latency.      Also the data you provided shows that triple-buffering has a higher input delay than double-buffering, which it shouldn't have. The benefits of triple-buffering is basically exactly like V-sync off but without tearing.   Only if triple-buffering is implemented correctly.  With double-buffering (V-Sync) you only have two buffers: the ""display"" buffer (which is used to display the image) and the ""render"" buffer, which queues up the next frame to be displayed.  Triple buffering adds a second ""render buffer"", so instead of only being able to render one frame in advance, you have two buffers that you could theoretically render to continuously.   Hold the image being displayed in the ""display"" buffer Store the next frame in ""render buffer 1"" Render another frame in ""render buffer 2""   What you could theoretically do, is that every time you fill up the second render buffer, you empty out the first one and render a third frame if you have time. And do it again if you still have more time before the display has to be updated.  Because you have two render buffers, you always have a completed frame ready to present to the display, so you can keep dumping out the oldest one and try to render a new one if you have the time to do so. Then when the display is due to refresh, you update it with the latest complete frame.  This way you could potentially render 600 FPS at 60Hz with very low latency, and no screen tearing.     But the way that triple buffering works in every game that I've ever played, is that it caps the framerate at 60 FPS on a 60Hz display.  So it's not dumping out the oldest frame and trying to render a new one in an attempt to reduce latency, it's just queuing up two frames in advance of displaying them, to ensure that you don't get screen tearing if the framerate drops below 60 FPS, rather than using the method described above.  This means that you have an additional frame of latency, instead of it reducing latency.     The only time I've seen triple-buffering work in the ideal way is when you are playing a game in Borderless Windowed Mode with Aero (desktop composition) enabled, and V-Sync off in the game.  V-Sync being disabled in the game allows the framerate to go above 60 FPS (often several hundred FPS in older games) while the desktop compositor manages triple-buffering the right way, so that latency can be greatly reduced.  But while this lets you use an unlocked framerate without any screen tearing, the game will stutter because the frame presentation is no longer synchronized with the refresh rate.  Assuming you can render quite a bit more than 60 FPS, it should be a good option for reducing latency without screen tearing.  But it won't be as low latency as V-Sync Off, and it won't be as smooth as having V-Sync On.  Variable Refresh Rate technologies like G-Sync and - to a lesser degree - Adaptive-Sync, are the best options today for low latency, tear-free, and stutter-free gaming as you don't have to compromise.  Without a VRR display, you have to pick two:   V-Sync On: High Latency. No tearing or stuttering if you can maintain the framerate. V-Sync Off: Minimum Latency. Always tears and stutters. Triple-Buffering: Never tears. Latency is low if windowed mode is used, though it stutters."
hardware,3bvfgh,163941,4,Thu Jul 2 22:14:32 2015 UTC,"But the way that triple buffering works in every game that I've ever played, is that it caps the framerate at 60 FPS on a 60Hz display.   That's because Microsoft lies. They implement a 3-frame blocking queue and call it triple buffering, but it isn't unless you jump through some extra hoops when setting up the swap chain and when presenting frames. Microsoft's documentation largely pretends that the difference between FIFO and most-recent ordering doesn't exist and assumes that everyone is trying to decode and display a movie rather than render interactively."
hardware,3bvfgh,wtallis,1 point,Fri Jul 3 00:14:16 2015 UTC,Wtf?
hardware,3bvfgh,epsys,1 point,Fri Jul 3 10:14:47 2015 UTC,"Your scenario of Vsync with frame-cap does hold some truth in theory, but shouldn't play out anything like that in practice. Because of varying frame-times you can't predict exactly how long you can delay rendering to time it to be done right before the monitor-refresh. So instead you'd often not be able to render the frame in time for the monitor-refresh and therefor your frame would be a full refresh behind and you would instead add input delay. What most frame-cap does is basically look at how long time the previous frame took to render and adds a delay before rendering the next frame to make sure that previous frame actually ""took"" as long as your cap permits. Vsync already does exactly this, meaning an FPS-cap of anything other than exactly the same as your monitors refresh rate will lead to more input lag. A very advanced frame-cap where you set it to match your monitors refresh rate AND you're able to set how much % of the delay should instead be added before any rendering is done and how much should % of the delay should be added after the rendering as usual, you could possibly tune it to see an advantage in input delay if your frame-times are very stable. But I've never heard of an advanced frame-cap like this, nor a game that has stable enough frame-times for it to be really beneficial.    Only if triple-buffering is implemented correctly.   Yes obviously I'm assuming that triple-buffering is implemented correctly, otherwise it isn't really triple-buffering and it's not worth talking about. Every game I've played that has triple-buffering (very few games today sadly) has had it implemented correctly.   But the way that triple buffering works in every game that I've ever played, is that it caps the framerate at 60 FPS on a 60Hz display.   The way you describe triple-buffering working in games you've played I've never seen in any game. What you explain is an incredibly horrible implementation and it shouldn't even be called triple-buffering. Also do mind that FPS-counters often has trouble recording correct frame-rates when triple-buffering is enabled, making it look like you're only getting the same FPS as your monitors refresh-rate. This is because FPS-meters often (always?) measures the amount Device->Present calls, and not the amount of SwapChain swaps."
hardware,3bvfgh,MaloWlol,1 point,Fri Jul 3 07:33:29 2015 UTC,"Your scenario of Vsync with frame-cap does hold some truth in theory, but shouldn't play out anything like that in practice.    Well those are measurements showing the effects in an actual game.   Because of varying frame-times you can't predict exactly how long you can delay rendering to time it to be done right before the monitor-refresh. So instead you'd often not be able to render the frame in time for the monitor-refresh and therefor your frame would be a full refresh behind and you would instead add input delay.   That's true.  However it doesn't result in a frame delay, but it can result in a dropped frame (stutter) if you cannot guarantee that you won't miss the render time - which would apply to older games like CS:GO on high-end hardware.  That's why I prefer to drop the flip-queue to 1, but not implement a frame cap unless it's an older game where I can guarantee the performance.  Shaving off those two frames (the default is a flip-queue of 3 for most games) is already a huge reduction in latency, and using regular V-Sync means that it won't stutter as long as I can keep the framerate at 60.   A very advanced frame-cap where you set it to match your monitors refresh rate AND you're able to set how much % of the delay should instead be added before any rendering is done and how much should % of the delay should be added after the rendering as usual, you could possibly tune it to see an advantage in input delay if your frame-times are very stable. But I've never heard of an advanced frame-cap like this, nor a game that has stable enough frame-times for it to be really beneficial.    GeDoSaTo can do this, however it has proven to generally be less effective than RTSS' framerate capping for reducing latency.   The way you describe triple-buffering working in games you've played I've never seen in any game. What you explain is an incredibly horrible implementation and it shouldn't even be called triple-buffering.   If you have desktop composition (Aero) enabled and run a game in windowed mode with V-Sync off, that is the behavior you get.  And many people consider that to be the ideal situation, since you have the low latency of being able to render hundreds of frames per second, but avoid the screen tearing that disabling v-sync normally entails. (since the desktop compositor applies triple-buffering)   Also do mind that FPS-counters often has trouble recording correct frame-rates when triple-buffering is enabled, making it look like you're only getting the same FPS as your monitors refresh-rate. This is because FPS-meters often (always?) measures the amount Device->Present calls, and not the amount of SwapChain swaps.   Well you can always monitor GPU load as well.  If it's being capped at the monitor's refresh rate and not continuing to render new frames, best-case scenario you have latency equivalent to V-Sync, worst-case scenario you have an additional frame of latency over V-Sync."
hardware,3bvfgh,163941,1 point,Tue Jul 7 02:59:41 2015 UTC,"Having your GPU pump out 115FPS on your favorite title won't matter if your display can only refresh 60 times per second.   This is true for VISUAL fidelity, however a lot of people just don't care about tearing and want the fastest response as possible. You can definitely still FEEL a difference, especially in games like Quake, CS and Call of Duty. This is because certain client-side actions (like the position of the mouse) are updated once per frame. More frames = more updates = more accurate representation of mouse movement. For a lot of people, having better feel for mouse movement is way more important that having the game LOOK perfectly in sync and smooth."
hardware,3bvfgh,willxcore,1 point,Mon Jul 6 19:02:47 2015 UTC,"Triple buffered vsync mostly eliminates the latency, though. The latency is basically capped at the time it takes to render a frame (no relation to the screen refresh)."
hardware,3bvfgh,ethraax,3,Thu Jul 2 15:41:06 2015 UTC,"Well, it's all a matter of perception really, as I've never seen anyone tackle the issue empirically through testing, but I still ""feel"" a difference between triple-buffered Vsync and a nice G-sync setup, which I currently do not own."
hardware,3bvfgh,Oafah,2,Thu Jul 2 15:44:21 2015 UTC,"Outside the range of refresh rates that Gsync/Freesync supports, they fall back to triple buffering. Within that range they do have a quantifiable advantage over triple buffering, but it's usually not as big as the difference between vsync and triple buffering.  It's worth pointing out that a lot of Direct3d stuff lies about supporting triple buffering, so a lot of your experience with ""triple buffering"" may have not actually been triple buffering, just a 3-frame render queue, which is strictly worse for interactive tasks than even vsync."
hardware,3bvfgh,wtallis,2,Thu Jul 2 16:37:08 2015 UTC,"Outside the range of refresh rates that Gsync/Freesync supports, they fall back to triple buffering.    They fall back to a V-Sync on or off state, not triple buffering."
hardware,3bvfgh,163941,2,Thu Jul 2 20:05:41 2015 UTC,"You may be right; I'm having trouble finding a conclusive source. Anyways, there's no difference at low frame rates, only when the GPU can outpace the display."
hardware,3bvfgh,wtallis,1 point,Thu Jul 2 21:41:48 2015 UTC,"You may be right; I'm having trouble finding a conclusive source. Anyways, there's no difference at low frame rates, only when the GPU can outpace the display.    Correct.  Many people actually recommend that you cap the framerate to something around 140 FPS with a 144Hz G-Sync monitor to prevent it from entering this V-Sync On state where it starts buffering several frames and adding latency, or the V-Sync Off state where the latency remains low as the framerate gets above 144, but it starts tearing.  With Adaptive-Sync it's a bit more of a problem, since all the current Adaptive-Sync displays have a minimum refresh rate that they can work at, and below that you either have to turn V-Sync On at say 40Hz (very high latency!) or V-Sync Off at 40Hz (horrific screen tearing).  G-Sync certainly handles the low end of the Variable Refresh Rate window a lot better than Adaptive-Sync does right now. At the upper end of the window, they're very similar."
hardware,3bvfgh,163941,2,Thu Jul 2 22:25:38 2015 UTC,"Many people actually recommend that you cap the framerate to something around 140 FPS with a 144Hz G-Sync monitor to prevent it from entering this V-Sync On state where it starts buffering several frames and adding latency,    Wow. You'd think the drivers would just plain refuse to create really long swap chains in situations like this, but that's DirectX for you...  I don't understand what you're saying about about how the variable refresh rate systems handle low frame rates. The logical way to do it would be to wait the maximum amount of time for the software to submit a new frame (eg. 1/40th of a second) and if no frame arrives, re-display the last frame (which takes 1/90th to 1/144th of a second depending on the maximum refresh rate), and if a new frame arrives in that time, it can be displayed as soon as that re-display is complete. Are you saying that some systems will delay the new frame for more than their shortest possible frame time? Or are you just saying that G-Sync displays will tend to support waiting longer for a new frame before they have to fall back to refreshing the stale image?"
hardware,3bvfgh,wtallis,1 point,Thu Jul 2 22:36:51 2015 UTC,"I don't understand what you're saying about about how the variable refresh rate systems handle low frame rates. The logical way to do it would be to wait the maximum amount of time for the software to submit a new frame (eg. 1/40th of a second) and if no frame arrives, re-display the last frame (which takes 1/90th to 1/144th of a second depending on the maximum refresh rate), and if a new frame arrives in that time, it can be displayed as soon as that re-display is complete. Are you saying that some systems will delay the new frame for more than their shortest possible frame time? Or are you just saying that G-Sync displays will tend to support waiting longer for a new frame before they have to fall back to refreshing the stale image?   G-Sync could theoretically support any framerate on the low end because it starts to repeat frames once it gets below the minimum refresh rate that the panel can handle.  Adaptive-Sync does not do this. It currently has to stop at the minimum refresh rate of the panel and then reverts to a standard V-Sync On (high latency) or V-Sync Off (lots of tearing) state.  I really don't know the technicalities of why they differ once they get below the minimum refresh rate that the panel supports.  My loose understanding is that NVIDIA are able to do this with G-Sync because there is a hardware module inside the display which stores the current frame indefinitely, and polls the GPU for new frames at 1000Hz.  Adaptive-Sync is more of a software-based solution, and there is some limitation there which is preventing it from ""simply"" repeating frames like NVIDIA do with G-Sync.  I think it's something to do with the fact that the display doesn't store the current frame, and once they repeat a frame it affects the window they have available to present the next frame, and so it's likely to stutter.  But as I said, I don't have a very good understanding of the problem. I haven't found a good source of information for that."
hardware,3bvfgh,163941,2,Thu Jul 2 23:33:55 2015 UTC,"Adaptive-Sync does not do this. It currently has to stop at the minimum refresh rate of the panel and then reverts to a standard V-Sync On (high latency) or V-Sync Off (lots of tearing) state.   What does this mean? That the adaptive sync stays off for some duration of time that's longer than one frame? I can't see how there are more than two options. NVidia's got to be doing one of the two things that you describe as the alternatives for Adaptive Sync.  I don't think there's any difference in practice, and the VESA standard method is more likely to use Panel Self-Refresh than NVidia's solution. In either system, if an overdue frame is completed while the display is not busy refreshing, the overdue frame will be displayed immediately. If the display is busy refreshing, the only two options are to either tear and show part of the new frame, or to wait until a new refresh can be started and show the new frame intact. In the latter case, the latency penalty is not what you get when using vsync at 40FPS, it's the latency penalty you get when using vsync at the maximum refresh rate supported by the display—the average delay will be less than 1/120th of a second.  Whether the display has a framebuffer to self-refresh or it relies on the GPU to re-send stale frames does not matter at all for what the latency is or what the options for degrading are when the frame rate falls outside the range of supported refresh rates."
hardware,3bvfgh,wtallis,1 point,Fri Jul 3 00:08:03 2015 UTC,"Was this difference on the same machine? Either way, there is a slight difference, mostly when your graphics card isn't that good, but it's still much better than the old double buffered vsync. For someone with a setup capable of rendering 150+ fps, the difference should be pretty minute."
hardware,3bvfgh,ethraax,-1,Thu Jul 2 15:51:05 2015 UTC,"Triple buffered vsync mostly eliminates the latency, though. The latency is basically capped at the time it takes to render a frame (no relation to the screen refresh).   Actually, most implementations of triple-buffering will increase latency by a frame on top of what double-buffering (regular v-sync) was.  Idealized triple-buffering would render hundreds of frames (or at least more than your refresh rate) swapping between the free buffers and just as your screen needs an update it will present the latest complete frame.  If you enable triple buffering on a 60Hz monitor, and your framerate is still capped at 60 FPS, latency is probably worse.     Gaming in Borderless Windowed Mode with Aero (Desktop Composition) enabled is the only time I've ever seen a proper implementation that allows the framerate to get higher than your refresh rate, while still avoiding tearing.  And though you won't get screen tearing, and it may reduce input lag, it's never perfectly smooth the way that V-Sync is when you are hitting the framerate cap and never dropping frames.  It always stutters because the frame presentation is not synchronized with the refresh rate."
hardware,3bvfgh,163941,-1,Thu Jul 2 20:04:23 2015 UTC,Wow half your comment was incorrect. I expect a better understanding of the subject matter. Your comment was largely atrocious.
hardware,3bvfgh,Marechal64,2,Thu Jul 2 20:55:31 2015 UTC,Can this not be set for individual games? Did I miss that part?
hardware,3bvfgh,Nixflyn,1 point,Thu Jul 2 14:28:20 2015 UTC,"Not through Catalyst Control Center. Although I don't see the point in limiting it in one DX11 application but not another... Unless you switch to a monitor with a higher refresh rate for one game, and then switch to a different monitor with a lower refresh rate for a different game. I don't see the use case for setting it individually."
hardware,3bvfgh,Slyons89,1 point,Thu Jul 2 16:10:02 2015 UTC,"Any game that uses the Bethesda gamebryo engine (Skyrim, all the Fallouts) will break the physics engine when they go above 60 FPS. If you have a high refresh rate monitor, vsync will sync you to your monitor's refresh rate and there's no way to reduce the frame rate other than manually setting your monitor to 60Hz...or using Nvidia inspector to set a frame rate cap on specific games at the diver level."
hardware,3bvfgh,Nixflyn,1 point,Thu Jul 2 23:08:10 2015 UTC,"That's a really good point. I only have a 60hz monitor so I've never considered needing to have vsync at something like 144hz, or about that bug in the Bethesda engine."
hardware,3bvfgh,Slyons89,1 point,Fri Jul 3 01:38:11 2015 UTC,"Yeah, there's still quite a few games that break above 60 FPS. Without Nvidia Inspector it'd be too much of a hassle."
hardware,3bvfgh,Nixflyn,1 point,Fri Jul 3 01:48:01 2015 UTC,You can use Radeon Pro for that.
hardware,3bvfgh,jorgp2,1 point,Thu Jul 2 17:46:21 2015 UTC,"Radeon Pro   I know RadeonPro isn't being supported anymore, and I don't think it supports anything newer than the 290x/290..."
hardware,3bvfgh,JDAndChocolatebear,1 point,Thu Jul 2 18:31:04 2015 UTC,It's a software feature supports all GPUs.
hardware,3bvfgh,jorgp2,1 point,Thu Jul 2 20:41:42 2015 UTC,"But from what I understand, things sometimes need to be updated to work with the newer GPUs."
hardware,3bvfgh,JDAndChocolatebear,1 point,Thu Jul 2 23:29:04 2015 UTC,"Well it works fine with my 290, which was released after development ceased."
hardware,3bvfgh,jorgp2,1 point,Thu Jul 2 23:34:16 2015 UTC,No it wasn't. http://www.radeonpro.info/2013/11/new-build-with-support-for-amd-radeon-r-series/
hardware,3bvfgh,JDAndChocolatebear,2,Thu Jul 2 23:44:34 2015 UTC,Just noticed your Username.
hardware,3bvfgh,jorgp2,1 point,Fri Jul 3 00:04:43 2015 UTC,It doesn't work with everything though. Development stopped years ago on it.
hardware,3bvfgh,Nixflyn,1 point,Thu Jul 2 23:09:34 2015 UTC,"It can be set in some games and can't be set on some other games. In addition, it's always done by tinkering with the game files; it's never an option in the graphics settings. I think it's good to have a setting that when turned on limits framerates on all games."
hardware,3bvfgh,Yearlaren,11,Thu Jul 2 22:22:33 2015 UTC,I don't understand the big deal. It's an FPS cap function available in so many other forms of software. Is it a big deal because it's built into CCC now? They approached this article like it was some big new form of AA or something.
hardware,3bvfgh,SirCrest_YT,28,Thu Jul 2 14:20:52 2015 UTC,"As far as I gather, normal fps cap functions usually ""block"" the GPU and force it to wait x milliseconds before starting the next frame. That's how it's usually done. However, they also block the game's update loop.  AMD's FRTC on the other hand, doesn't block at all. But the AMD driver will disregard any rendering commands issued during that ""blocking time frame"". That way, the game itself still runs at 100+ fps which means you get the usual benefits of lower input delay, etc."
hardware,3bvfgh,Mr_s3rius,5,Thu Jul 2 14:38:49 2015 UTC,"So rather than acting like a forced wait time, it just ignores render commands and lets the game still run uncapped.  Interesting. That does make it seem much different than I imagined."
hardware,3bvfgh,SirCrest_YT,6,Thu Jul 2 17:59:31 2015 UTC,Will this make it possible to eliminate tearing without the performance hit that traditional v-sync causes?
hardware,3bvfgh,BCMM,5,Thu Jul 2 15:15:23 2015 UTC,"Not really. It's only deciding whether to discard drawing commands or to execute them; it's never deciding to delay them or to redirect them to a third buffer. Triple buffering is still largely an OpenGL thing, though it's also a necessary part of the variable refresh rate systems."
hardware,3bvfgh,wtallis,2,Thu Jul 2 15:30:30 2015 UTC,"Does that mean that software like FRAPS would still display a number over the cap? I enabled the cap at 60 FPS on my 390X, but Heroes of the Storm still shows 100+ FPS all the time, reported by FRAPS."
hardware,3bvfgh,Slyons89,4,Thu Jul 2 16:08:16 2015 UTC,"In the article they mentioned that Blizzard games currently don't work with FRTC for whatever reason.  With a working game, FRAPS should only show you the capped fps."
hardware,3bvfgh,Mr_s3rius,1 point,Thu Jul 2 16:25:26 2015 UTC,"Good to know, thank you!"
hardware,3bvfgh,Slyons89,1 point,Thu Jul 2 16:26:05 2015 UTC,Is that any different than nvidia drivers do it ? Because I've been using the fps cap for a long time. I can't tell any input lag compared to uncapped.
hardware,3bvfgh,Symix,2,Thu Jul 2 16:14:02 2015 UTC,I don't know.   But input lag is not very noticeable. The difference in input delay between running capped 60fps and uncapped 100fps is less than 7ms.
hardware,3bvfgh,Mr_s3rius,1 point,Thu Jul 2 16:24:01 2015 UTC,"Depends on the person. Side by side I can easily pick the difference between 60fps and 100 fps... even on a 60Hz monitor I find it quite noticeable. I also find the input lag from V-Sync unbearable on my 120Hz monitor, even with tripple buffering. I also had terrible framepacing on my r6850 which was bad enough that I just binned it and bought a GTX 760. Was barely an upgrade (compared to what I'd normally do anyway) but the +- 5-10ms constant hitching made many games all but unplayable below 60fps.  I'm still waiting for Freesync to mature to a reasonable point before I jump on the bandwagon but a 144Hz Freesync/Gsync monitor with <2ms input lag (across a decent range of transitions, not the silly G2G rating many use) and fuck all input lag from the scaler and using something like this FRTC to cap games at 125fps is my idea of heaven."
hardware,3bvfgh,jojotmagnifficent,-7,Fri Jul 3 06:29:15 2015 UTC,"Hahaha, AMD put effort into new development? Why do that when you can just blame your short comings on your competition?"
hardware,3bvfgh,ClockworkOnion,1 point,Thu Jul 2 16:58:39 2015 UTC,id this is for AMD 3XX cards and the fury........why dont i have it? i have a R9 390......
hardware,3bvfgh,CommanderArcher,1 point,Thu Jul 2 17:03:21 2015 UTC,You should have it...
hardware,3bvfgh,Exist50,2,Thu Jul 2 17:51:57 2015 UTC,i updated to the right version of catylest and i have it.
hardware,3bvfgh,CommanderArcher,1 point,Thu Jul 2 18:37:25 2015 UTC,"so if i have a 60hz monitor should i turn this on to save on heat,noise and power consumption?"
hardware,3bvfgh,troublegoats,4,Thu Jul 2 17:19:29 2015 UTC,Yes.
hardware,3bvfgh,jinxnotit,1 point,Thu Jul 2 18:27:30 2015 UTC,"nvidia has an unofficial version of this kinda. you can set a frame cap through nvidia inspector. i have mine set at 120fps. although i only have a 60hz monitor, i personally notice much less screen tearing when running 60+ fps, i only use it to keep things like game menu's from pulling 2000fps and getting crazy coil whine like the article above showcased. nice little failsafe because even though i have read that coil whine isn't necessarily bad for the card, i don't want to risk it (plus its pretty damn annoying)  edit: for those curious, i run a evga gtx 780 sc with the acx cooler. card never pushes past 71c ever"
hardware,3bvfgh,PhxkinMassacre,1 point,Fri Jul 3 01:05:01 2015 UTC,"hmm nice, even for menus alone with lazy devs who never put frame limiters on this is worth it."
hardware,3bvfgh,1usernamelater,1 point,Fri Jul 3 04:14:59 2015 UTC,The poor man's Freesync.
hardware,3bvfgh,hdshatter,1 point,Sun Jul 5 00:58:39 2015 UTC,"Couldn't you use FRTC with a freesync monitor to keep the fps from going over the upper limit of the monitors freesync range and therefore pretty much eliminate tearing altogether.  I know you can't prevent it from going under the lower limit in detailed scenes but with a powerful GPU like the Fury X that risk should be reduced especially on lower resolution freesync monitors.  In this example I would be talking about the LG 29UM67 (resolution = 2560x1080) which has a freesync range of 48-75, set FRTC at 70 and enjoy.  Thoughts?"
hardware,3bvfgh,zasy99,1 point,Sun Jul 5 19:12:23 2015 UTC,"I enabled the limit to 60 FPS on my 390X using this setting, and it is not effective. Both Witcher 3 and Heroes of the Storm still go over 60 FPS. Heroes displays 100+ FPS in FRAPS.  Color me disappointed. Or maybe I just don't understand how this works, please inform me.  EDIT - It actually seems to work fine in Witcher 3."
hardware,3bvfgh,Slyons89,4,Thu Jul 2 16:06:02 2015 UTC,Ive read that it doesnt work on blizzard games. Probably some bug that its going to be fixed in the future.
hardware,3bvfgh,jakobx,1 point,Thu Jul 2 16:17:35 2015 UTC,Thanks for the info!
hardware,3bvfgh,Slyons89,1 point,Thu Jul 2 16:26:10 2015 UTC,I've read  it doesnt work in OpenGL and DX9 games
hardware,3bvfgh,jigssaw,5,Thu Jul 2 16:45:52 2015 UTC,RivaTuner comes with Afterburner not CCC.
hardware,3bvfgh,punknurface,-17,Thu Jul 2 14:34:27 2015 UTC,"If I wanted to play video games at 45fps, I would have bought a lesser video card in the first place. Why would I want to limit my video card?  This is no different than throttling the darn thing."
hardware,3bvfgh,Imidazole0,6,Thu Jul 2 14:59:55 2015 UTC,"I think it can be quite useful, some new freesync monitors for example, only have a freesync max of like 90Hz. so when a game goes above 90fps you get a big box come up that says Freesync is now disabled. This can be good for those kind of situations."
hardware,3bvfgh,GeckIRE,4,Thu Jul 2 14:03:53 2015 UTC,why is the maximum 95fps? Why can't we lock it to 120 or 144 to match our high refresh rate monitor?
hardware,3bvfgh,AP_RAMMUS_OK,2,Thu Jul 2 14:22:28 2015 UTC,"If your monitor supports those refresh rates, you're better off just enabling vsync."
hardware,3bvfgh,wtallis,1 point,Thu Jul 2 14:09:58 2015 UTC,"IMHO, if you have a 120 or 144 hz monitor vsync is a waste. You just simply won't be able to perceive any screen tearing at those high of refresh rates, even if the game isn't being run at that high of an FPS."
hardware,3bvfgh,milo09885,1 point,Thu Jul 2 15:32:23 2015 UTC,"If a game's rendering at 40FPS without vsync, it doesn't matter whether the monitor is refreshing at 60Hz, 120Hz, or 144Hz, the tearing will be identical. The severity of tearing depends only on the rendering rate because the decisions about what pixels to send to the monitor don't take into account the monitor's refresh cycle at all."
hardware,3bvfgh,wtallis,3,Thu Jul 2 21:14:11 2015 UTC,"Assuming a 60 fps monitor, how could this be a bad thing?"
hardware,3bvfgh,Sremylop,3,Thu Jul 2 21:30:52 2015 UTC,I have deleted all my content out of protest. Reddit's only value comes from it's content. Delete all your content and Reddit becomes worthless.
hardware,3bvfgh,EpicNoob1983,-4,Thu Jul 2 14:21:51 2015 UTC,"If a persons display is capped at 60fps, what's the point in going any higher than that.   Reduced input-lag.   but vsync doesn't prevent your card from rendering faster than 60fps, it just throws those extra frames away wasting GPU time and power.   Wrong, Vsync does prevent your card from rendering faster than your monitors refresh rate. The program will stall at the Device->Present() call until the monitor refreshes and grabs the frame."
hardware,3bvfgh,MaloWlol,2,Thu Jul 2 14:19:32 2015 UTC,"Your last paragraph only applies to double buffered vsync, not triple buffered vsync. Triple buffered vsync keeps rendering frames until the monitor finishes displaying a frame and needs the next one, at which point it grabs the most recent fully rendered frame."
hardware,3bvfgh,ethraax,0,Thu Jul 2 14:22:20 2015 UTC,"Yes, sadly tripple-buffering is incredibly rare, I don't know of any modern game that supports it. So double-buffered vsync is assumed when talking about vsync. Tripple-buffering really is the best of both worlds at the expense of some VRAM."
hardware,3bvfgh,MaloWlol,2,Thu Jul 2 15:45:27 2015 UTC,"Eh, I spend most of my gaming time playing L4D2 which supports it. I thought CS:GO did as well, but I don't have it installed at the moment. I thought you could also force it in your drivers, although I've never needed to."
hardware,3bvfgh,ethraax,0,Thu Jul 2 16:02:01 2015 UTC,"I don't think any game I've played for the past 5 years has had support for tripple-buffering, with possibly the exception of CS:GO, a game where I don't really care about it as I'll gladly take half a frame quicker (tearing) to reduce input lag.  Iirc with OpenGL games forcing it in the drivers works pretty fine. However with DirectX games (the majority of them) it's not that simple, and often when forced you get visual glitches and other problems."
hardware,3bvfgh,MaloWlol,1 point,Thu Jul 2 16:14:58 2015 UTC,I have deleted all my content out of protest. Reddit's only value comes from it's content. Delete all your content and Reddit becomes worthless.
hardware,3bvfgh,EpicNoob1983,0,Thu Jul 2 16:21:15 2015 UTC,"The last part of your previous comment is the point, reduced power consumption and therefor also temperatures. It's mostly only beneficial for people on laptops etc to increase battery time and help with overheating. Also limiting frames can be useful in certain rare scenarios as to for example help combat coil whine or specific games that suffer from too fast rendering (only very old games basically)."
hardware,3bvfgh,MaloWlol,2,Thu Jul 2 14:29:43 2015 UTC,I have deleted all my content out of protest. Reddit's only value comes from it's content. Delete all your content and Reddit becomes worthless.
hardware,3bvfgh,EpicNoob1983,2,Thu Jul 2 14:34:23 2015 UTC,"Yes, but Vsync is locked to only working with your monitors refresh-rate. If you have a 60hz monitor you can't limit the game to run at 80 or 40 fps. If you're playing a heavy game and is not able to push 60fps but rather 50, putting Vsync on wont save any battery, while a frame cap at 30 will."
hardware,3bvfgh,MaloWlol,3,Thu Jul 2 15:42:05 2015 UTC,smoothness of vsync without the input lag.
hardware,3bvfgh,namae_nanka,2,Thu Jul 2 16:00:48 2015 UTC,"Depending on the type of gameplay, generally people will notice and be annoyed by an unstable framerate rather than a lower but consistent framerate, even if the unstable one hits a high framerate."
hardware,3bvfgh,plank_,-5,Thu Jul 2 14:30:39 2015 UTC,"Limiting frame-rates will not help making frames more consistent, unless you mean simply putting the frame-limit at the lowest FPS you'd get during a drop anyway in which case yes, it'd be more consistent, but I doubt anyone would think that is an improvement. Every frame rendered is an individual job, and with a frame-limiter the graphics card will be put into idle after every finished frame if it is finished quicker than 1000/FPS-TARGET ms and will therefor not be able to help mitigating future frames that render slowly."
hardware,3bvfgh,MaloWlol,3,Thu Jul 2 14:13:11 2015 UTC,"You forgot to take into account boost clocks. If your GPU isn't working 100% rendering more easy frames than needed, then it will have more thermal headroom to accelerate the rendering of the occasional harder or delayed frame."
hardware,3bvfgh,wtallis,1 point,Thu Jul 2 14:19:01 2015 UTC,"Supposedly this method has the card reject draw calls until the needed frame is up for rendering, which should save on GPU resources."
hardware,3bvfgh,Kaghuros,-2,Thu Jul 2 15:35:25 2015 UTC,"been limiting fps for over a decade now..  wish there was a way to tune fps more precisely, in battlefield 4 for example, I can limit into decimals which makes me have virtually no input lag with vsync on, most monitors vary a little from the 60hz mark.."
hardware,3bvfgh,Raising,3,Thu Jul 2 18:57:58 2015 UTC,No. You might want to did a bit more research.
hardware,3bvq03,speckz,29,Thu Jul 2 15:20:40 2015 UTC,"As for the R9 290X comparison, the results end up being very interesting. The R9 Fury X sees some very impressive gains here, improving over the R9 290X by 47% at 1440p and an amazing 60% at 4K. Given that the latter is outright outside our theoretical performance window for a shader-bound scenario, I suspect there’s more at play here than just GPU improvements. And sure enough, running the modified Catalyst 15.15 drivers on the R9 290X finds that performance improves by 21% at 4K, to 43.6fps, so it looks like AMD has been doing some optimizing for this game.   http://www.anandtech.com/show/9390/the-amd-radeon-r9-fury-x-review/18  Hmm, optimizing for an indie game sounds a bit improbable. Maybe the found a common bottleneck in the drivers? Syntetic texel fill rate (3dmark) is 47% higher for Fury X, and pixel fill rate is 139% higher. I wouldn't be surprised if there still exist some massive bottlenecks..."
hardware,3bvq03,kwahoo2,2,Thu Jul 2 15:58:35 2015 UTC,"decreasing tessellation, similar to this? that's my first thought, but i don't know the game."
hardware,3bvq03,logged_n_2_say,7,Thu Jul 2 17:25:29 2015 UTC,TTP does not use tessellation.
hardware,3bvq03,kwahoo2,1 point,Thu Jul 2 18:23:46 2015 UTC,back to the drawing board.
hardware,3bvq03,logged_n_2_say,3,Thu Jul 2 18:56:24 2015 UTC,I tried to reproduce this result and I never could.  https://www.reddit.com/r/hardware/comments/3aetyn/390x_tessellation_is_not_improved_performance/csby37k
hardware,3bvq03,glr123,1 point,Fri Jul 3 06:25:49 2015 UTC,"I remember! To me, you have pretty much proven it's a game specific driver optimization."
hardware,3bvq03,logged_n_2_say,0,Fri Jul 3 12:01:42 2015 UTC,"Wait, didn't another site test the different driver and find no difference in performance?"
hardware,3bvq03,OftenSarcastic,2,Thu Jul 2 18:32:59 2015 UTC,"for fury, not for 290x."
hardware,3bvq03,logged_n_2_say,2,Thu Jul 2 18:55:05 2015 UTC,Yeah looks like I remembered wrong: tesselation difference (source).
hardware,3bvq03,OftenSarcastic,1 point,Thu Jul 2 19:08:40 2015 UTC,"ha, i thought you meant the fury x ""wrong driver"" rumor that going around.  http://www.gamersnexus.net/guides/1996-amd-r9-fury-x-driver-benchmark"
hardware,3bvq03,logged_n_2_say,1 point,Thu Jul 2 19:11:14 2015 UTC,Yeah I saw those too. I like that the gamers nexus article shows the odd file names for the press drivers. That likely explains why Tech Report listed their driver version as 15.6 as well.
hardware,3bvq03,OftenSarcastic,6,Thu Jul 2 19:17:51 2015 UTC,"skimmied through it, good review and a lot more discussion on hardware, fiji, and the future.  compute scores are somewhat meh considering the processors, but not bad.  sound doesn't seem to be an issue for them and is in fact a high point under load.  temperature looks amazing (water cooled), power consumption isn't majorly improved with hbm and the rest we already knew from the bench.  thought this was interesting:   I also believe there’s more than just VRAM bottlenecking occurring here. The GTX 980 sees at least marginally better framerates with the same size VRAM pool (and a lot less of almost everything else), which leads me to believe that AMD’s drivers may be holding them back here. Certainly the R9 290X comparison lends some possible credit to that, as the 99th percentile gains are under 20%. http://www.anandtech.com/show/9390/the-amd-radeon-r9-fury-x-review/22   i was excited for this too:   Fiji is the first AMD discrete GPU to support full hardware HEVC decoding,   tight titties, but where's a benchmark?"
hardware,3bvq03,logged_n_2_say,5,Thu Jul 2 17:24:35 2015 UTC,http://www.computerbase.de/2015-06/amd-radeon-r9-fury-x-test/8/#diagramm-videos-abspielen-h265-lav-filters
hardware,3bvq03,namae_nanka,1 point,Fri Jul 3 00:16:48 2015 UTC,Awesome. Thanks.
hardware,3bvq03,logged_n_2_say,1 point,Fri Jul 3 03:13:42 2015 UTC,"Can you English tl:dr this,  because stupid."
hardware,3bvq03,ofcsu1,29,Mon Jul 6 06:51:42 2015 UTC,it's weird. you should basically buy a 980Ti but if the Fury X didn't exist we'd still be stuck with only 980's and Titan X's (ie 2 bad choices). have to thank AMD for making Nvidia even a little bit competitive  gonna be sad if AMD flops
hardware,3bvq03,Hariooo,19,Thu Jul 2 18:41:59 2015 UTC,"People have been saying this since Intel released the Core ix architecture. AMD has been chugging along since. Their Zen CPU architecture looks promising and the Fury, while not the current crown holder, is a solid card on which they can modify to suit the requirements of different price points in the market. Don't forget the dual GPU Fury is due out later this year, to which I have yet to see a rival card from NVidia.  *edit - Grammar"
hardware,3bvq03,Apocolypse007,14,Thu Jul 2 20:18:00 2015 UTC,"And aside from i5 750 to i5 2500k, we haven't really seen a Core CPU demonstrate a great value proposition since. So no, the Intel-AMD competition is basically a worst case scenario outside of AMD folding (not at home kappa)."
hardware,3bvq03,Hariooo,1 point,Thu Jul 2 20:57:41 2015 UTC,that's ignoring the tech trends underlying all of this  even if AMD volumes were twice as high we'd be seeing the same trends
hardware,3bvq03,andromeduck,1 point,Fri Jul 3 02:52:31 2015 UTC,Isn't the titan Z a dual GPU Titan X?
hardware,3bvq03,Medic-chan,9,Fri Jul 3 04:28:52 2015 UTC,"The Titan Z is $1,600. While it may be powerful, without seeing a significant price drop, I don't see it competing with the dual fury.  Edit: Also, the Titan Z is two Titan black cores I believe, not Titan X."
hardware,3bvq03,Apocolypse007,1 point,Fri Jul 3 05:35:58 2015 UTC,Titan Z may win in DP performance.
hardware,3bvq03,dylan522p,1 point,Sat Jul 4 02:08:04 2015 UTC,The gimped the Titan Xs DP performance because people were buying titans over quadro that didn't need the ecc memory or quadro features.
hardware,3bvq03,hdshatter,1 point,Sat Jul 4 09:25:35 2015 UTC,No the fp64 hardware is literally gutted out of Maxwell. They made a revised GK210 called GK200 and its slightly more fp64 performance and that is the best they will have for tesla Quadra or anything FP64 until pascal.
hardware,3bvq03,dylan522p,5,Sat Jul 4 10:46:11 2015 UTC,"Still, looking at AMD’s situation I can’t help but wonder what happens from here, as it seems like AMD badly needed a win they won’t quite get.   That's what I said a week ago, although at that time I wasn't sure whether it was a tie or not, while Ryan straight up said that AMD lost."
hardware,3bvq03,Yearlaren,0,Thu Jul 2 20:58:07 2015 UTC,You're looking at a difference of a margin of error.  Windows 10 swoops in with optimized drivers and I wouldn't be surprised if it's faster by 4% as opposed to slower.
hardware,3bvq03,jinxnotit,6,Thu Jul 2 23:26:35 2015 UTC,"I would be because that's not how software works.  Sure AMD has more to gain because of how bad their drivers are currently in terms of comparative overhead but most of that is API level. If you specifically select for those things then sure, but in real world applications won't change architecture overnight and there's still a ton of resource management and HW specific infra that will remain the same across both API's.  Just think about how long it took for previous DX features to get picked up. Tessellation took about 5-10 years between OGL and common usage in OGL/DX, compute shaders took about 2-3 years in DX.  I'd say we're still 2-3 years from common adoption and real gains from major studios so it's really not worth considering if you're purchasing this year.   If anything the 4 GB RAM limitation should be far more concerning if you're looking at 4k IMO."
hardware,3bvq03,andromeduck,2,Fri Jul 3 03:06:51 2015 UTC,It is how drivers work on the other hand.  Through driver development AMD has prolonged the life of their cards where in certain testing environments and conditions they are punching well above its weight.  By not having great optimization for windows 8.1 and instead Windows 10 with another big driver update and a catalyst control center update soon after there is a lot left on the table. Even Anandtech's reviewer states that the hardware seems to be more limited by the drivers and software then the hardware it's self.  The hardware is beastly. The software is crap in a pile waiting to be polished and shined and organized.
hardware,3bvq03,jinxnotit,3,Fri Jul 3 04:49:10 2015 UTC,I mean if that were the case we'd easily find out via synthetics but all that's showing is it doesn't have a very good texture/geometry balance relative to shader power.  AMD also has 3 driver frontends -- in house DX/GL + open source GL -- which haven't really been sharing code/development much so it's going to be quite spotty for a long while yet.
hardware,3bvq03,andromeduck,2,Fri Jul 3 05:22:35 2015 UTC,What else is new with AMD cards though? They've been worse at tesselation since they launched GCN cores.  And their driver teams may be feeling overwhelmed. But I think that's just as much Gameworks doing as it is their staff size. Having to constantly divert attention to Gameworks games that make last minute changes before release and completely alters their way of writing drivers is just as much to blame.
hardware,3bvq03,jinxnotit,2,Fri Jul 3 06:12:48 2015 UTC,Gameworks still uses the vanilla API as intended so there's very little excuse there IMO. If you try any of the graphical debug/profile tools like razor/nsight and whatever amd's was you'd know source is almost never required. If you need it it's probably easier to just ask someone more familiar.  I'm really surprised people are just taking Huddy's shit slinging at face value here. He's just like any other PR guy.
hardware,3bvq03,andromeduck,5,Fri Jul 3 06:21:21 2015 UTC,"It's not just Huddy, It's Joe Macri too. Read what he had to say about Witcher 3.  It's also other developers here and elsewhere who have called out Nvidia on it as well.   And obviously AMD doesn't ""need"" to use the source code, otherwise they wouldn't be putting out drivers for improved performance in gameworks games. But examining the source code gives AMD a lot more flexibility and let's them keep a small driver team. It fundamentally changed how AMD drivers team operates.  It's just Nvidia trying to step on their neck. If AMD kept Nvidia from HBM you can bet Nvidia would be bitching even louder."
hardware,3bvq03,jinxnotit,-1,Fri Jul 3 06:32:26 2015 UTC,"If AMD prevented anyone from HBM they'd also have prevented themselves, same with freesync and tressfx and their open source driver. The reason they went open with all those things is because they're too weak to do it in house but whose fault is that?   It was only a few years ago when both AMD and ATI were individually worth more than Nvidia but now Nvidia has spent more on CUDA than AMD has hemorrhaged in the last 2-3 years.   They could have invested in tools/research a decade ago but nope. Same with frame pacing and GPGPU and pretty much everything else last ~5 years. They could have done it but instead they're eternally playing catch-up.  Same business with Intel, they sent too much on fabs and drained their capital then went balls deep with ATI which only worsened the problems.  Also like to other devs?"
hardware,3bvq03,andromeduck,5,Fri Jul 3 06:57:14 2015 UTC,Lol.   HSA Llano Freesync Eyefinity Mantle LiquidVR XDMA TruAudio HBM   And that's just stuff that's just happened recently! Boy they sure have done nothing...
hardware,3bvq03,jinxnotit,1 point,Fri Jul 3 12:42:07 2015 UTC,I really do wonder if AMD is going to destroy Nvidia in DX12 performance. From what I have heard drivers don't really matter in DX12 because of the low level access.
hardware,3bvq03,hdshatter,2,Sat Jul 4 09:20:50 2015 UTC,"Yes and no.  Drivers are still the instructions that tells the OS what to do with the hardware. So, while driver overhead and efficiency will improve. It still needs efficient ""instructions"" on what to do with the hardware.  As for whether or not AMD will destroy Nvidia in DX 12 doubtful. It will be fairly even. Nvidia has some smart people that know what they're doing."
hardware,3bvq03,jinxnotit,11,Sat Jul 4 14:43:46 2015 UTC,Finally!   Ryan you toyed with my emotions long enough.
hardware,3bvq03,innerknowing,18,Thu Jul 2 15:42:11 2015 UTC,"His conclusion how they're not single 4k gaming cards just yet, every time I post this I get down voted saying if you can turn settings down to play it then it's fine.   No, this isn't what a 4k capable card is.   It's one that may require very minor changes or none at all to image quality.   Not having to run the game at medium to low for solid framerate.   A couple other reviews said similar things, but I'm somehow the asshole when I point it out lol."
hardware,3bvq03,w00t692,5,Thu Jul 2 16:22:24 2015 UTC,"A lot of people are just butt hurt when they can't ""max everything"". Even though at 4k your graphics will look better at lower settings. It especially doesn't need AA like 1080 does but people won't call it max if it's not on."
hardware,3bvq03,jamie1414,2,Thu Jul 2 21:44:00 2015 UTC,Making low textures ultra sharp doesn't make anything better.  And i've seen MANY people argue 2x aa is at the very least noticeable at 4k.
hardware,3bvq03,w00t692,2,Fri Jul 3 02:30:59 2015 UTC,Depends on the game. I think Diablo 3 scales beautifully with 4k. And yeah 2x aa is noticable on 4k most likely but you have to do like 8x aa to be comparable...possibly.
hardware,3bvq03,jamie1414,1 point,Fri Jul 3 02:33:17 2015 UTC,"Diablo 3 can be played at 4k on even a midrange card, so if that's all you play, by all means buy like a 960 gtx or an r9 380.  If you're like me, and you play diablo 3.... plus a shitton of other games including newer games and lots of AAA titles, you're probably going to want something more substantial haha."
hardware,3bvq03,w00t692,1 point,Fri Jul 3 02:41:19 2015 UTC,"Textures quality only eats VRAM. Sharp edges eats performance, and edges are already sharp at 4k"
hardware,3bvq03,TheImmortalLS,1 point,Fri Jul 3 20:19:42 2015 UTC,"That depends on the size of the display, though. At 24"" it's definitely not needed."
hardware,3bvq03,ZeM3D,1 point,Tue Jul 7 07:44:56 2015 UTC,At 24 inches it has to be closer than the typical desktop monitor to actually visually appreciate the resolution as well lol.
hardware,3bvq03,w00t692,1 point,Tue Jul 7 11:51:45 2015 UTC,I play at 1440p and I can barley tell the difference between 4x ad 8x msaa but at 1080p its easy for me to tell.  SMAA is good enough for me at 1440p most of the time. I imagine it looks awesome at 4k especially with the low FPS hit it has.
hardware,3bvq03,hdshatter,2,Sat Jul 4 09:30:32 2015 UTC,"we've been saying this since UHD monitors started popping up. Turn off AA, get a UHD monitor smaller than 28"" an you'll not notice it. Nobody has posted any evidence that there is noticeable aliasing like this. They just spout off about it and don't actually use UHD monitors.   These cards work fine if you turn off AA because you don't need it in modern titles. Shovel Knight doesn't count, jackasses."
hardware,3bvq03,imoblivioustothis,1 point,Fri Jul 3 05:30:22 2015 UTC,The problem is DPI scaling sucks on desktops. It's not like it is on Android where they can just pop on a better resolution screen and it scales pretty damn well.
hardware,3bvq03,hdshatter,8,Sat Jul 4 09:28:38 2015 UTC,"No, this isn't what a 4k capable card is.   That is subject to opinion.  Strictly speaking, a 4k gaming capable card is one that is capable of running games at 4k, no matter what settings.  Who are you (or I, or Anandtech) to decide what exactly that phrase means, especially since everyone already has a different opinion on the topic."
hardware,3bvq03,Mr_s3rius,9,Thu Jul 2 16:30:13 2015 UTC,"As with a lot of things that can be subjective, it's more important that we're all on the same page when we compare things. What /u/w00t692 describes as the standard has more or less been the standard for a long time.  When someone says a card is 4K capable for a specific game within the gaming community, it implies that it can run that game at a minimum of 60 FPS at max settings. It's the automatic assumption, because we need an automatic assumption when not specified further or we won't be able to effectively convey information. This is just how it's always been and it's not a matter of opinions, its just a matter of consistency so we can understand each other.   Further specifications are more than welcome of course... if you consider 120 FPS your minimum that a game should be able to run at in order to qualify a GPU, then you can and should specify as such. Otherwise the default is 60. You'll notice most benchmarks also list when specific graphics settings are turned off to be perfectly clear as well, such as Witcher 3 benchmarks that might list results with Hairworks off as it's known to drastically decrease performance. This is also very often done with various forms of AA... benchmarks will specify if they're using AA, and which form of AA they're using, or if they're not using any at all.  We can be pedantic over opinions all day, and it'll get us no where. This is the whole reason we have common language assumptions within a given community."
hardware,3bvq03,Stingray88,3,Thu Jul 2 18:22:17 2015 UTC,"It's the automatic assumption   Well, not for me. I don't say that to be stand-offish, I just never considered 4k capable to mean ""runs max settings 4k 60fps"". My association was more like ""runs most stuff decently on 4k""  ""4k capable"" is a term that's usually used without mentioning specific games. At that point, the automatic assumption falls apart anyways. Is it 4k capable with regards to Arma 3? Or Minecraft? Who knows, it's usually not mentioned. Case in point: AMD. Their 300er reveal had ""4k"" written all over it, without actually mentioning any titles specifically.  And I think this is the more important usage of the term. It's not ""can I play X game in 4k"", it's ""is that card good for my 4k screen?"". And at that point, pointing out specific games isn't very helpful. When talking about general 4k capabilities, more leeway is needed.  PS: People got to stop using downvote to show silent disagreement... I'ma upvote you back to 1 :)"
hardware,3bvq03,Mr_s3rius,1 point,Thu Jul 2 22:49:41 2015 UTC,"Well, not for me. I don't say that to be stand-offish, I just never considered 4k capable to mean ""runs max settings 4k 60fps"". My association was more like ""runs most stuff decently on 4k""   I don't think you sound stand-offish, and I don't intend to be either! Just good discussion.  I don't think you're seeing the point I'm trying to make here though...  The problem is that ""decently"" is subjective. Where as X settings with Y FPS isn't subjective, they're objective values. We can't discuss benchmarks without consistency, which is why benchmarks using video games are usually always run on max settings unless specificed otherwise... and always specified if otherwise.   ""4k capable"" is a term that's usually used without mentioning specific games. At that point, the automatic assumption falls apart anyways. Is it 4k capable with regards to Arma 3? Or Minecraft? Who knows, it's usually not mentioned. Case in point: AMD. Their 300er reveal had ""4k"" written all over it, without actually mentioning any titles specifically.   AMD and nVidia... sure. They're going to market things how they're going to market them even if not accurate. However benchmarks are really what customers should be paying attention to, and benchmarks always list the games/settings used at give resolutions.  We need to be clear, and its not possible to be clear without consistency. Thus saying a GPU can run a game at a given resolution implies max settings 60 FPS unless otherwise specified. That's clear and consistent. You're always more than welcome to specify your other criteria, such as a frame rate lower than 60 FPS, but if you don't specify that we need a default.  This is the default for the gaming community. For you, and everyone else. There's nothing wrong running a game at lower than 60 FPS, and at lower settings. If you're cool with that, that's cool. But if you say your GPU runs Arma 3 at 4K... unless you specify settings and FPS, the automatic assumption is max settings, 60 FPS+.  Discussing if a GPU can run well up to Jeff's standards, or Kyle's standards... isn't very useful. Objective values are a better standard."
hardware,3bvq03,Stingray88,1 point,Thu Jul 2 23:12:04 2015 UTC,"A 4k capable card to me is also one that can run games at 4k with playable settings at playable fps. But when I come to hardware forums/hardware news I'm aware that 4k capable means, 60 fps min at max settings on the latest hardware pushing games. I think it's just something you come to accept as a standard if you want to talk about 4k capable at all."
hardware,3bvq03,weez09,2,Thu Jul 2 23:32:25 2015 UTC,"60 fps min at max settings on the latest hardware pushing games   That is pretty high standard.   There are certainly games where a GTX 970 can't maintain 60 fps minimum at 1080p on max settings (e.g. Witcher 3, GTA V) and new games are constantly coming out.  I wouldn't say that means that the GTX 970 is not a ""1080p capable card""."
hardware,3bvq03,aziridine86,13,Fri Jul 3 23:22:39 2015 UTC,If 4k capable means it can display 4k and that's it then we've had 4k capable cards for a while.   Now me? People call the 970 gtx the perfect 1080p card. Is this because it plays 1080p with a variety of settings turned to medium or low? Nope! It plays basically any game at 1080p at max settings.   This is what I go by and what most people should go by.
hardware,3bvq03,w00t692,3,Thu Jul 2 16:51:17 2015 UTC,"If 4k capable means it can display 4k and that's it then we've had 4k capable cards for a while.   Yes, people have been gaming at 4K for a while."
hardware,3bvq03,SPOOFE,1 point,Thu Jul 2 22:47:12 2015 UTC,"Sure.   Dual card setups and low settings.  If the only stipulation is ""well it runs""  then why should hardware advance?"
hardware,3bvq03,w00t692,-1,Thu Jul 2 23:08:49 2015 UTC,"If the only stipulation is ""well it runs"" then why should hardware advance?   Where on earth did you get ""only stipulation""? There's all sorts of stipulations for all sorts of terms. The term 4K capable means ""capable of 4K""."
hardware,3bvq03,SPOOFE,2,Fri Jul 3 01:44:58 2015 UTC,"I don't need to argue my point anymore, it's well and clearly stated many times.  I don't need to listen to shit about ""4k capable"" when i'd rather hear it listed as ""4k usable""."
hardware,3bvq03,w00t692,1 point,Fri Jul 3 02:00:44 2015 UTC,"You didn't need to argue the point at all, is what I'm getting at. It's ridiculous that you'd vehemently insist that ""4K capable"" can't mean ""capable of running a game at 4K resolution"".   If you'd rather it be called something else then why even bother arguing what this means?"
hardware,3bvq03,SPOOFE,1 point,Fri Jul 3 02:10:29 2015 UTC,"I guess capable can mean that.  I dunno in my head when someone says 4k capable i think, well yeah it's ""capable"" of 4k but that doesn't mean it's good at 4k.  Either way the crux of my argument isn't about capable or not.  It's fury x and 980 ti's not being single 4k cards.  No matter how much anyone argues the point, every benchmark and every game has shown otherwise."
hardware,3bvq03,w00t692,4,Fri Jul 3 02:24:51 2015 UTC,"I haven't been able to read all of this yet but I'm finding it to be more thorough, concise, and intriguing than other reviews that we've seen thus far. Overall it looks like the Fury X averages just 10 FPS less than the 980ti (no OC) but is also a bit cooler and quieter than its competitor under heavy loads. Compute performance is a bit lackluster if that concerns you at all."
hardware,3bvq03,KamikazeRusher,2,Thu Jul 2 16:33:11 2015 UTC,Seems to still be very good at compute. Usually anandtech tests a bit more than that. I wonder if the writer's illness has something to do with it.
hardware,3bvq03,Exist50,5,Thu Jul 2 17:44:25 2015 UTC,Perhaps. You never know with a Cold. They tend to drain me completely of energy for two weeks.  I do wonder though how HBM will compare to GDDR5 when DX12 and Vulkan are released
hardware,3bvq03,KamikazeRusher,-2,Thu Jul 2 18:56:48 2015 UTC,"Uh... same as it does now... VK and D3D12 are graphics API's, an interface to the hardware of which GDDR5/HBM is a part of, it won't fundamentally change anyting.  It's like if your company got a new CEO, how would your typing speed with keyboard A would compare to keyboard B."
hardware,3bvq03,andromeduck,1 point,Fri Jul 3 03:11:10 2015 UTC,"Not really true.  Vulkan and DX12 are much lower-level and much more strict about proper usage than their predecessors, which (amongst plenty of other efficiency improvements) means they're better about handling RAM.  Page 7:   Longer term, AMD is looking at the launch of Windows 10 and DirectX 12 to change the situation for the better. The low-level API will allow careful developers to avoid duplicate assets in the first place, and WDDM 2.0 overall is said to be a bit nicer about how it handles VRAM consumption."
hardware,3bvq03,Diosjenin,1 point,Fri Jul 3 06:13:14 2015 UTC,"They've been saying the same thing every year at SIGGRAPH, Microsoft and Khronos. It's going to be a relatively big jump but don't expect anything ground breaking for the next year or two.   This whole trend is more of a CPU thing than GPU."
hardware,3bvq03,andromeduck,3,Fri Jul 3 07:02:35 2015 UTC,Pump noise isn't that big of a deal after all?
hardware,3bvq03,Randomoneh,5,Thu Jul 2 16:31:15 2015 UTC,On this unit. On other units it is.
hardware,3bvq03,jinxnotit,2,Thu Jul 2 16:57:25 2015 UTC,Pump noise was a defect and its apparently fixed in the newest batch which is using a new cooler.
hardware,3bwjcl,zmeul,9,Thu Jul 2 19:03:17 2015 UTC,"may cause abdominal bleeding. If you have suicidal thoughts please contact a medical professional.   Anyway, let's wait a bit for the does. I don't like maybes.   It's funny we had a bunch of maybes before the card was released. And we still have a bunch of maybes. I guess I'll wait for the non x to come out. Hopefully we will know what is actually coming int he mail when we buy this card."
hardware,3bwjcl,darrenphillipjones,2,Fri Jul 3 04:16:39 2015 UTC,"Why not just  turn the rpms  down. Most pumps have a half speed  option, and it shouldn't  affect temps too much, or at all maybe"
hardware,3bwjcl,Dewrito,3,Fri Jul 3 12:44:47 2015 UTC,This would be a sensible option if Cooler Master included it. As Far as anyone can tell there is no option or switch anywhere to turn it down.
hardware,3bwjcl,Seclorum,4,Fri Jul 3 23:27:12 2015 UTC,AMD will be lucky to not keep tumbling with stupid stuff like this. Rumors also indicate that the 980 ti was finished long before release while it seems as tho the fury x was being pushed along as fast as they could manage.
hardware,3bwjcl,bigpappaflea,3,Fri Jul 3 12:44:29 2015 UTC,"It was basically 'finished' at the same time as the Titan X, since they are identical aside from disabled cores."
hardware,3bwjcl,makar1,6,Fri Jul 3 12:53:45 2015 UTC,That makes amd very behind in my eyes because it took them longer to release a card that doesn't even outright be the 980 ti which nvidia finished months ago. Inb4 HBM is new tech blah blah blah
hardware,3bwjcl,bigpappaflea,0,Fri Jul 3 12:58:32 2015 UTC,1.HBM is no tech 2.AMD is a smaller company than NVIDIA.
hardware,3bwjcl,TheMW28,3,Fri Jul 3 19:33:28 2015 UTC,AMD has more employees and revenue than nvidia actually
hardware,3bwjcl,batmatt,12,Fri Jul 3 20:33:34 2015 UTC,In their GPU department?
hardware,3bwjcl,TheMW28,1 point,Fri Jul 3 20:34:49 2015 UTC,Except AMD isn't just a GPU company they manufacturer CPUs as well so the fact that they are a) bringing in about the same revenue and b) losing a lot more money than their competitor which is only a GPU company is not at all a positive thing whatsoever.
hardware,3bwjcl,reynardtfox,-6,Mon Jul 6 20:10:34 2015 UTC,Kudos to AMD for fixing a problem that solely exists in the first place because they rushed Fury X to market.
hardware,3bwjcl,TaintedSquirrel,27,Thu Jul 2 20:09:07 2015 UTC,rushed!?  this card has been in development for how many years?! and only at launch they discover the pump has issues - allow me not give AMD any credit; they went with the cheaper solution and 1st adopters are paying for it
hardware,3bwjcl,TaintedSquirrel,16,Thu Jul 2 20:25:38 2015 UTC,"The GPU, card, and cooler are 3 distinctly separate things designed by entirely different teams.  Even moreso in this case since the CLC itself is made by CM."
hardware,3bwjcl,TaintedSquirrel,-8,Thu Jul 2 21:18:56 2015 UTC,"and again, the CLC by CM wasn't just manufactured  you don't design the card and at the end discover that there's no cooling for it - the whole assembly was designed and manufactured with all of it's components specs already known  R9 295X2 has two pumps, but I don't recall anyone owning one of these cars complaining about hi-pitched pump noises"
hardware,3bwjcl,TaintedSquirrel,13,Thu Jul 2 21:29:03 2015 UTC,"We're talking about QA. The card was rushed to market and therefore the final production samples weren't properly tested.  On the other hand, the 295 X2 probably was... AMD had much less pressure from Nvidia at the time.  The fact that they fixed the pump whine so prompty indicates that it's not a problem with the design itself.  Either AMD didn't give CM enough time to test their CLC or AMD didn't give themselves enough time to test the final units.  The severe lack of review samples and low store supply reinforces the issue.  In fact I would say there's overwhelming evidence from the entire launch that shows it was rushed."
hardware,3bwjcl,Seclorum,-13,Thu Jul 2 21:34:15 2015 UTC,"cut the crap mate .. the card was in development for years and the leaks just prove that  it was rumored the card will have CM cooling since ""forever"""
hardware,3bwjcl,TaintedSquirrel,-14,Thu Jul 2 21:51:18 2015 UTC,"cut the crap mate .. the card was in development for years and the leaks just prove that  it was rumored the card will have CM cooling since ""forever"""
hardware,3bwjcl,jinxnotit,11,Thu Jul 2 21:39:28 2015 UTC,"I don't know how else to say it without sounding like a dick but this is a cut-and-dry case of poor QA.  Start to finish.  I don't see why you feel the need to be disagreeable about everything.  If nothing else, it's Cooler Master's fault not AMD's.  This is what you said:   and only at launch they discover the pump has issues   Definition of QA issue."
hardware,3bwjcl,Seclorum,9,Thu Jul 2 21:40:54 2015 UTC,"Definition of QA issue.   A lot depends on what testing parameters Cooler Master had in place on their end before shipping the CLC's off to AMD for final assembly.   Another aspect is that not every sample reviewers were getting, exhibited the issue.  Further, in some tests you had people directly sampling the noise off an open air test bench from the distance of 6 inches or less off the card.   AMD has a QA issue for not testing and ensuring the pumps they recieved from Cooler Master functioned properly before they assembled the final cards, in addition you also have Cooler Master for shipping pumps with defects.   At some point I expect that AMD made the choice to just say, ""Fuck it"" and they sent out the earliest cards without new pumps so they could hit their target date for launch."
hardware,3bwjcl,jinxnotit,-8,Thu Jul 2 21:52:07 2015 UTC,"QA - is done way way before shipping the finished product, and this is not the case; AMD very well knew what they were putting in - you're telling me that every person working of the Fury X was deaf or wearing ear-plugs?!  and yet, you bluntly chose to ignore that    and please don't quote me out of context to further your agenda"
hardware,3bwjcl,0Ninth9Night0,8,Thu Jul 2 22:01:42 2015 UTC,"you're telling me that every person working of the Fury X was deaf or wearing ear-plugs?!   I'm telling you they didn't find the issue in the first place because neither the completed products nor the CLC itself were tested properly.  This was most likely due to time constraints (based on all the other issues) or AMD's/CM's incompetence.  Either way it wasn't done.  We're both talking about the same problem but for some reason you're acting like we're disagreeing.  We are agreeing with each other, in different terms."
hardware,3bwjcl,Seclorum,-11,Thu Jul 2 22:03:20 2015 UTC,"not even a car makes it out of the production line before ""the car"", as a complete assembly, is tested - only after that it's allowed to be shipped for sale  are you telling me that AMD just ignored that ... wow!"
hardware,3bwjcl,jinxnotit,-7,Thu Jul 2 22:12:04 2015 UTC,"If nothing else, it's Cooler Master's fault not AMD's.   And yet you still feel it necessary to point the finger at AMD."
hardware,3bwvmx,zmeul,3,Thu Jul 2 20:33:50 2015 UTC,"That second board looks to have quite an overkill power delivery for a non overclocking model. Come to think of it, so do many of the skylake boards shown off."
hardware,3bwvmx,Exist50,1 point,Thu Jul 2 22:13:44 2015 UTC,Since there is no more fivr you should see more phases than haswell
hardware,3bwvmx,xtothemess,1 point,Fri Jul 3 00:49:04 2015 UTC,Aren't they adding it back with Cannonlake or something?
hardware,3bwvmx,Exist50,1 point,Fri Jul 3 01:00:12 2015 UTC,"no idea, maybe."
hardware,3bwvmx,xtothemess,1 point,Fri Jul 3 03:56:52 2015 UTC,Two different design teams see it two different ways.
hardware,3bvwgb,LiLThuG,1 point,Thu Jul 2 16:10:14 2015 UTC,Wanttt.....
hardware,3btych,Fatigue-Error,39,Thu Jul 2 03:14:09 2015 UTC,That's an....interesting title.
hardware,3btych,shriek,8,Thu Jul 2 05:51:36 2015 UTC,I see it grabbed your attention.
hardware,3btych,rambi2222,4,Thu Jul 2 07:30:23 2015 UTC,But did it bait his click?
hardware,3btych,mlkelty,1 point,Thu Jul 2 11:50:35 2015 UTC,Oh it definitely did. I think OP is a baiting master.
hardware,3btych,jamie1414,2,Thu Jul 2 14:16:31 2015 UTC,A master baiter?
hardware,3btych,alex_3799,1 point,Thu Jul 2 17:32:06 2015 UTC,"Hey, that's just the Ars Technica title. Sigh."
hardware,3btych,frogger5687,6,Thu Jul 2 22:57:37 2015 UTC,That's impressive.
hardware,3btych,IC_Pandemonium,7,Thu Jul 2 04:23:46 2015 UTC,Seems a bit weird on a point and shoot. But I guess they want to hit up that casual stalker type that doesn't have the cash for a proper SLR set up.
hardware,3btych,C0R4x,5,Thu Jul 2 11:25:30 2015 UTC,"A 2000mm lens will cost you a leg and an arm, and really isn't very useful in a lot of situations. So I'm thinking this is more of a gimmick feature than very useful. All to get the biggest number in front of the x."
hardware,3btych,avgxp,7,Thu Jul 2 12:07:24 2015 UTC,"Definitely a gimmick, just look at the image quality when it fully extended.  I don't know that much about photography but I know that picture would look like shit."
hardware,3btych,sterob,2,Thu Jul 2 13:20:04 2015 UTC,i would say pretty good for that price.   https://www.youtube.com/watch?v=mfshAzV0FN4
hardware,3btych,makar1,2,Fri Jul 3 16:58:25 2015 UTC,The focal length of the lens is only 357mm. They aren't that expensive.
hardware,3btych,C0R4x,8,Thu Jul 2 13:42:01 2015 UTC,"Yes, this lens is. The camera has got a fairly small sensor though, and the post I'm reacting on is talking about a ""proper"" SLR setup. To get an equivalent FoV on a full-frame DSLR, you'd need a 2000mm lens. In the original ars-article, they mention a 1300mm canon lens, which goes for 90000 new. It's not made anymore though, but you can get one 2nd hand for 180000 USD."
hardware,3btych,makar1,4,Thu Jul 2 14:02:49 2015 UTC,"You can easily crop a full frame image to obtain the awful quality ""2000mm"" FoV from this camera."
hardware,3btych,SteffenMoewe,1 point,Thu Jul 2 14:59:09 2015 UTC,But cropping is basically just a digital zoom
hardware,3btych,C0R4x,1 point,Sat Jul 4 08:57:50 2015 UTC,Yes.
hardware,3btych,DCanon,8,Sat Jul 4 13:05:47 2015 UTC,A perverts dream
hardware,3btych,continous,-3,Thu Jul 2 07:13:38 2015 UTC,"zooms in on ass Oh yeah, that's it. I can see you up there on the penthou-person turns around, dick in hand...I can make this work..."
hardware,3btych,SirCrest_YT,8,Thu Jul 2 10:43:06 2015 UTC,Alright.
hardware,3btych,epsys,0,Thu Jul 2 14:23:26 2015 UTC,How would you know?
hardware,3btych,TheIdesOfMay,7,Fri Jul 3 10:25:59 2015 UTC,Nikon are having a fucking field day with this free marketing lately.
hardware,3btych,liltbrockie,15,Thu Jul 2 07:46:59 2015 UTC,Yes they are.... if you make something cool people talk about it!
hardware,3btych,nazzo,0,Thu Jul 2 09:49:46 2015 UTC,"As awesome of a lens that camera has, this might be the wrong sub reddit for it."
hardware,3btych,lucun,35,Thu Jul 2 03:40:27 2015 UTC,"Well, it's technology hardware, so I'd say it fits. It's just most of us here are into computer tech so it gets posted more."
hardware,3btych,nazzo,-12,Thu Jul 2 03:43:14 2015 UTC,Wouldn't r/technology/ be more proper?
hardware,3btych,Copperhe4d,17,Thu Jul 2 03:48:08 2015 UTC,no
hardware,3btych,aziridine86,9,Thu Jul 2 04:02:40 2015 UTC,"Almost everything in this sub relates to either computers or mobile computing (smartphones).   Look at the list of 'related subreddits'. Almost none of them (except /r/tech) have anything to do with any non-computing related technology  This isn't a general technology sub. It has traditionally been a sub for technology related to computers and mobile computing.   Look at the top posts for the last month:   SSD prices Thunderbolt 3 unveiled Fury X SSD capacity and pricing versus HDDs Samsung SSD TRIM bug Galax 980 Ti LN2 edition The death of <$130 graphics cards (integrated GPU discussion) DDR4 RAM pricing AMD picks up Jim Anderson from Intel GTX 980 Ti launched   EDIT: I'm not saying this post needs to be deleted or anything, but camera lens technology doesnt fit the typical mold of what gets posted here and what gets upvoted here."
hardware,3btych,aziridine86,1 point,Thu Jul 2 06:24:34 2015 UTC,I suppose that's true.  Certainly it doesn't seem like the mods have made a strong effort to lock down the sub in terms of what content does or doesn't belong here (except for questions that belong in BAPC that is).
hardware,3btych,continous,1 point,Thu Jul 2 08:39:42 2015 UTC,"No, this is less tech than it is hardware. It isn't necessarily new, but rather improved."
hardware,3btych,mynewaccount5,2,Thu Jul 2 08:40:49 2015 UTC,why?
hardware,3btych,fzed1199,-2,Thu Jul 2 10:44:31 2015 UTC,/r/watchingchildrenfromadistance?
hardware,3btych,transmitthis,1 point,Thu Jul 2 05:52:52 2015 UTC,They really should have been using This background track for the video.
hardware,3btych,xtothemess,1 point,Thu Jul 2 13:39:46 2015 UTC,"I have the 42x optical version, I bet this is insane. i saw a guy video the moon with it too."
hardware,3btych,Raising,1 point,Thu Jul 2 14:46:42 2015 UTC,"isn't a telescope more ""appropriate"" for long range stalking ?"
hardware,3btych,dghelprat,1 point,Thu Jul 2 14:55:37 2015 UTC,"It's not creeping, it's technical prowess."
hardware,3buv1b,narwi,11,Thu Jul 2 09:56:17 2015 UTC,"I wonder if it's possible to cut down GM206 to the point that it doesn't need an additional power connector, but still make it meaningfully better than the GTX 750 Ti? That seems like a pretty narrow margin to hit."
hardware,3buv1b,sk9592,5,Thu Jul 2 13:32:36 2015 UTC,That would be a pretty massive cut to get below 75W.
hardware,3buv1b,makar1,5,Thu Jul 2 13:44:10 2015 UTC,"The power target for a 960 is 120W, for a PCIe only would need to cut that by 45W. A 1GB cut down and underclocked GTX950 that used only PCIe power might be possible, but I am not sure if it really pays off. Then again, I don't really have a power budget for it, so hard to say.   And really, many GTX 750 do come with a pcie power connector for extra juice."
hardware,3buv1b,arikv2,3,Thu Jul 2 14:46:20 2015 UTC,Going to replace 750ti on market to bring up prices.
hardware,3buv1b,quadrahelix,2,Thu Jul 2 18:01:25 2015 UTC,"Yep, there is only so much you can with 75 Watts. Maxwell 2.0 didn't increase efficiency all that much. The PCIe bus power limit puts a hard cap on the possible improvement."
hardware,3buv1b,58592825866,1 point,Thu Jul 2 13:52:34 2015 UTC,There's room for improvement somewhere considering the 750ti uses an outdated version of Maxwell
hardware,3buv1b,Yearlaren,9,Thu Jul 2 23:58:27 2015 UTC,As a budget gamer I'm excited. We don't get a lot of price wars below the $200.
hardware,3buv1b,gamerguy247,4,Thu Jul 2 22:02:47 2015 UTC,All nvidia needs to do is beat/compete with the 270X at the $150 pricepoint then you have the most bang for the buck gpu under 200- I hope 950 ti is just that while 950 competes with r7 370
hardware,3buv1b,fuzzycuffs,3,Thu Jul 2 20:01:12 2015 UTC,I'd like a low profile one with displayport please.
hardware,3buv1b,Myrang3r,3,Fri Jul 3 00:48:27 2015 UTC,"Gigabyte already has a low profle 750ti 2gb, which has displayport, 2x hdmi and dvi-i."
hardware,3buv1b,fuzzycuffs,3,Fri Jul 3 09:32:35 2015 UTC,Yup. It's the only one as well. I'm hoping for the 950Ti equivalent and more of them. The Gigabyte card is always way more expensive than the others because it's the only one.
hardware,3buv1b,Kagemand,1 point,Fri Jul 3 09:49:00 2015 UTC,Yes please.
hardware,3buv1b,The_Lord_of_Moisture,1 point,Sat Jul 4 00:40:34 2015 UTC,Excited to see how much these beat my old 7770 when they're released. Hopefully it'll be a worthy successor considering how surprising the performance the 7770 had back when it was released for the price.
hardware,3buv1b,Klorel,1 point,Fri Jul 3 01:30:40 2015 UTC,i guess i can't expect a 950 ti to render a 960 useless as the 980 ti did it with the titan x?
hardware,3buv1b,heratic666,3,Fri Jul 3 11:06:05 2015 UTC,"No, not likely."
hardware,3buv1b,wagon153,-11,Fri Jul 3 13:14:53 2015 UTC,Just what we need! Another gutless discreet gpu. Why would you bother? May as well save your money and just use onboard.
hardware,3buv1b,wywywywy,15,Thu Jul 2 17:38:26 2015 UTC,"The 750 and 750 TI is still significantly stronger then any integrated graphics out there, including AMD's APUs(the 7850k has the equivalent to a 250x). These cards have a place in the market."
hardware,3buv1b,Phlawless_Phallus,3,Thu Jul 2 18:07:21 2015 UTC,This card is perfect for the MOBA crowd
hardware,3buv1b,BrockYXE,7,Fri Jul 3 11:29:25 2015 UTC,"Not sure what you're getting at here. The 750ti is enough to play most games at 1080p with medium settings & 30-60fps. Integrated graphics still can't quite do that, and they especially can't do that with the level of cpu you'd buy to go with a mid tier card like this, like an FX-6300 or the Pentium"
hardware,3buv1b,w00t692,1 point,Thu Jul 2 18:08:33 2015 UTC,"The only integrated gpu that get anywhere near a 750, and this card will most likely be a hefty percentage faster than that, is the top range i7. The only case where you might want to combine a top of the line cpu with a low end graphics card is if you are getting a professional card to run Autodesk etc software on, which does not apply here.   In all other cases you are much better off with adding a discrete card, starting off with a 750 Ti (or whichever the amd analogue is).  Provided you care about graphics performance at all."
hardware,3bv27w,FitnessRegiment,15,Thu Jul 2 11:37:06 2015 UTC,"For ""just"" new parts, I usually use notebookcheck.com. They dont have all new laptops, but they keep close track of CPUs and GPUs for laptops.  For actual laptop models, I think it's better to just look up which brands are currently considered reputable and good enough in quality, then check for each what their current offerings are, and once you find a model that interests you, check reviews on that."
hardware,3bv27w,HavocInferno,-3,Thu Jul 2 12:34:44 2015 UTC,"Thanks! I know a lot of people hate on notebooks/laptops, but I really think the future is having tiny systems and not huge ones with separate GPUs etc...  I can't wait until iGPUs inside of Intel CPUs become as powerful as modern day GPUs"
hardware,3bv27w,maybachsonbachs,13,Thu Jul 2 12:41:57 2015 UTC,discrete components will always be more powerful. the different form factors address different needs.
hardware,3bv27w,HavocInferno,-6,Thu Jul 2 13:07:07 2015 UTC,"I understand what you mean but, the 980m is already getting fairly strong. If the next generation is as powerful jump as the 880m to 980m we will be seeing some fairly stable Triple A gameplay.  I don't really think discrete components will be the future, they may be more powerful and useful for now. But putting power into a small form factor is always amazing. If we can fit the GPU, RAM and CPU onto 1 chip like integrated ram/memory. That would be revolutionary in terms of speeds and cooling.  Discrete components means a lot of problems that integrated components don't have."
hardware,3bv27w,virtush,8,Thu Jul 2 13:18:18 2015 UTC,"While you're right, the 980M is fairly strong, it's significantly more expensive than the same performance in a discrete desktop GPU, and discrete desktop GPUs in the same price range are significantly more powerful.  It's in the nature of hardware. For the same price, you can always have higher performance in a larger form factor. Because producing larger is cheaper than producing small.  There will always be a market for enthusiast hardware, and always a market for budget hardware. And both needs can be satisfied cheaper and in terms of performance better with discrete parts.  On that note though, discrete vs integrated isnt much different in terms of problems."
hardware,3bv27w,HavocInferno,2,Thu Jul 2 15:22:35 2015 UTC,Dude.. the 980m is a discrete GPU.
hardware,3bv27w,Teethpasta,1 point,Sat Jul 4 16:55:56 2015 UTC,I know. But I think it's clear what I'm talking about. I'll edit it.
hardware,3bv27w,Quaddlewap,1 point,Sat Jul 4 18:13:55 2015 UTC,Yeah the opposite is the case. You can't replace or upgrade a completely integrated system very easily or at all.
hardware,3bv27w,mmencius,3,Thu Jul 2 14:05:32 2015 UTC,"There are other disadvantages as well, which may easily be overlooked:  Noise. (load and idle). Its always a trade off between noise and size. And its harder to clean the fan/heatsinks.  Drivers. I still use GPU drivers from 2011, because there was exactly one driver update and the generic one from AMD doesn't work. Neither did an unofficial one that I tried.  Powerful laptops are certainly nice,  but my next machine will be a desktop."
hardware,3bv27w,mmencius,2,Thu Jul 2 20:48:28 2015 UTC,"Fair enough, especially since GPUs are becoming out-dated at a much faster pace than CPUs, and MUCH faster rate than RAM."
hardware,3bv27w,mmencius,-2,Thu Jul 2 14:17:49 2015 UTC,Strong but extraordinarily overpriced. Get a 3DS.
hardware,3bv27w,read_the_article_,1 point,Thu Jul 2 20:28:24 2015 UTC,"what? I don't think a 3DS plays Skyrim with Mods or renders videos using Premiere, or that it can compile data."
hardware,3bv27w,kennai,-5,Thu Jul 2 20:52:28 2015 UTC,You can play Skyrim with mods or render videos on a brilliant 27-34 inch monitor or even 50-80 inch TV at home with a powerful desktop. When you're on a flight you can do something else with that time. Play Fire Emblem Awakening.
hardware,3btktq,Exist50,25,Thu Jul 2 01:09:03 2015 UTC,Is there any indication that this isn't just an esthetic change?
hardware,3btktq,Randomoneh,22,Thu Jul 2 01:59:00 2015 UTC,"The connector on the bottom of the copper pipe is different. It is originally all copper, it looks like it is brass now. While they is not indicative of anything in the pump, at all, it does imply some changes have been made."
hardware,3btktq,glr123,7,Thu Jul 2 03:42:14 2015 UTC,Good catch.
hardware,3btktq,Seclorum,7,Thu Jul 2 04:16:25 2015 UTC,It could still be copper actually. The angle of the shot is terrible and all your seeing is distorted reflected light. Look at the section of straight copper over the VRM's and how the reflected light makes it look like brass right there too. And the top edge of the fitting is only getting a black reflection on it.   It's possible they changed the fitting but without a better picture it's too early to be definitive.
hardware,3btktq,Teresi2Finger,7,Thu Jul 2 06:10:12 2015 UTC,"I don't know. Even with the section of the ""brass"" shadowed by the pump, it still looks very different from the copper."
hardware,3btktq,UrbanToiletShrimp,5,Thu Jul 2 06:20:02 2015 UTC,"There's some anecdotal evidence, but nothing concrete."
hardware,3btktq,SeaJayCJ,9,Thu Jul 2 02:00:24 2015 UTC,Did you get the new one? I have one coming in the mail now so hopefully it isn't fucky.
hardware,3btktq,melgibson666,16,Thu Jul 2 01:25:50 2015 UTC,http://i.imgur.com/IgU4Y.gif
hardware,3btktq,willi_werkel,12,Thu Jul 2 04:07:23 2015 UTC,"No, sorry, I took this pic from the anandtech forums. http://forums.anandtech.com/showthread.php?t=2437161&page=2"
hardware,3btktq,RRGeneral,18,Thu Jul 2 01:28:28 2015 UTC,Anti-noise embossed logo :p
hardware,3btktq,willi_werkel,15,Thu Jul 2 02:58:41 2015 UTC,Stickers make too much noise.
hardware,3btktq,Slyons89,-3,Thu Jul 2 03:01:10 2015 UTC,How does a sticker make noise? Oo
hardware,3btktq,Retell,10,Thu Jul 2 09:02:18 2015 UTC,He was joking...
hardware,3btktq,Retell,10,Thu Jul 2 11:06:55 2015 UTC,"Oh, I'm sorry :("
hardware,3btktq,logged_n_2_say,9,Thu Jul 2 11:51:19 2015 UTC,"The coil whine issue would bother me. I guess problems should be expected in the first shipment of a brand new card with a new water cooler setup. Hopefully AMD will do the right thing and allow early buyers to swap their pumps out later or get a replacement card if they have that problem. They could fix those returned cards and sell them refurbished. Shit, I'd buy a factory refurbished Fury X for $500."
hardware,3btktq,Ellimis,6,Thu Jul 2 03:09:01 2015 UTC,Looks like at least Newegg is doing RMAs.
hardware,3btktq,psikeiro,6,Thu Jul 2 03:25:12 2015 UTC,"There's no way a revision would be in the hands of anyone but AMD or Coolermaster at this time.  Elaboration, There wouldn't be a revision based on the recent findings this quickly. It could be a revision from the reviewer's cards.  -Cooling PM and Liquid Cooling Sales Director"
hardware,3btktq,Retell,3,Thu Jul 2 04:27:03 2015 UTC,I wouldn't be surprised if they knew about this issue and just sent out a few pre-production type units to reviewers. Bad move if true.
hardware,3btktq,jinxnotit,3,Thu Jul 2 04:38:34 2015 UTC,I have 2 retail Fury's and one preproduction. The preproduction is blank. I'll pull the cover off one of the retail's tomorrow.
hardware,3btktq,jinxnotit,1 point,Thu Jul 2 07:07:56 2015 UTC,"Maybe amd can freeze time with a third party time freezing machine?  In all seriousness the only way I could see it being true is if the pump maker brought it to amd after a small production was already sent out and sold. As in they have the model being sold in other kits, but that still puts the impossible time turnaround on the maker :. But that seems unlikely and it still feels like amd could pull them before that happened."
hardware,3btktq,Retell,3,Thu Jul 2 12:14:45 2015 UTC,"AMD's good at heating things up a lot, not freezing them."
hardware,3btktq,jinxnotit,2,Thu Jul 2 13:02:23 2015 UTC,m'sir
hardware,3btktq,III-V,0,Thu Jul 2 05:03:19 2015 UTC,m'sikeiro
hardware,3btktq,w00t692,-3,Thu Jul 2 16:53:32 2015 UTC,"Or, you could read what AMD has said in many different reviews about pump whine when the embargo lifted and how they knew about the issue and had already corrected the problem.  Or you could go full blown PCPer and reject reality to substitute your own."
hardware,3btsp0,BlueberryGland,28,Thu Jul 2 02:20:46 2015 UTC,/r/hardware
hardware,3btsp0,Reapexx,-2,Thu Jul 2 02:26:38 2015 UTC,"hardware news, reviews and intelligent discussion.   I'd call it relevant. you can always choose not to post (and let the mods decide)"
hardware,3btsp0,Reapexx,22,Thu Jul 2 02:32:20 2015 UTC,I think you're misunderstanding my comment
hardware,3btsp0,defiance158,16,Thu Jul 2 02:34:43 2015 UTC,that's hilarious what just happened here.
hardware,3btsp0,redditedc,5,Thu Jul 2 02:41:29 2015 UTC,OP wants video/youtube channels as indicated in post.
hardware,3btsp0,maybachsonbachs,2,Thu Jul 2 02:41:50 2015 UTC,indeed I am. I assumed you were just being a backseat mod like many redditors like to do. derp  still looking for video content anyway
hardware,3btsp0,Sremylop,3,Thu Jul 2 02:48:17 2015 UTC,Video content is low bandwidth
hardware,3btsp0,legitimateusername4,48,Thu Jul 2 10:33:15 2015 UTC,"Eh, can't stand videos myself. I don't like listening to people talk. I read mostly tom's and anand"
hardware,3btsp0,maybachsonbachs,20,Thu Jul 2 03:02:29 2015 UTC,"I have  no idea why anyone likes videos in place of articles.  It's the same information, but at one quarter the speed."
hardware,3btsp0,shadowofashadow,19,Thu Jul 2 10:12:49 2015 UTC,And not searchable or indexed
hardware,3btsp0,elevul,8,Thu Jul 2 10:34:03 2015 UTC,Well one reason I can give is that I can't read an article while I'm cleaning up my house or cooking dinner.  I like to have the videos on in the background or while I'm doing something else.
hardware,3btsp0,bphase,4,Thu Jul 2 13:36:55 2015 UTC,this is the key for me. always doing something else. Also i like the entertainment factor
hardware,3btsp0,continous,2,Thu Jul 2 17:45:34 2015 UTC,I like them because I can just have them on my second monitor in the morning while I do other stuff.  Or I can rip them from youtube as audio only and listen to them on my morning commute.
hardware,3btsp0,olavk2,23,Thu Jul 2 23:07:27 2015 UTC,"Tech Report  AnandTech  Techpowerup  Not much of a youtube guy. Though the video form has its benefits, maybe I should try to get more into them. Just feels like those traditional written reviews are more professional still."
hardware,3btsp0,shadowofashadow,3,Thu Jul 2 03:12:28 2015 UTC,"Well it really does depend on what you want out of a review. Written reviews will almost always be more in-depth and professional because the format lends better to that, but the video format is extremely good at conveying the general information quickly."
hardware,3btsp0,Penderyn,2,Thu Jul 2 10:48:11 2015 UTC,"Written reviews are more professional and i prefer them when i want to get to know the performance and characteristics of a device, but there is something oddly relaxing about watching a video review."
hardware,3btsp0,DuoRunner,2,Thu Jul 2 09:54:32 2015 UTC,I find the video reviews are  like a surface look and if I want to go deeper I hit the articles.
hardware,3btsp0,Juheebus,2,Thu Jul 2 13:35:01 2015 UTC,Tech report is the one!
hardware,3btsp0,Bresdin,6,Thu Jul 2 22:50:33 2015 UTC,I love 3Dgameman for his very informative case reviews.
hardware,3btsp0,signet111,3,Thu Jul 2 04:20:09 2015 UTC,I like his cooler reviews too. ThinkComputers is also pretty decent.
hardware,3btsp0,LendlGlobal,14,Thu Jul 2 05:24:53 2015 UTC,"Pcper is great for some products, goes above and beyond everyone else on the list"
hardware,3btsp0,zmeul,2,Thu Jul 2 03:03:42 2015 UTC,Agreed.  They have some wonderfully in depth and technical articles on their site as well. Their reviews are extremely thorough and they try very very hard to not take sides on opposing competitors.
hardware,3btsp0,Elessun,2,Thu Jul 2 06:21:00 2015 UTC,"They have a pretty good weekly podcast, too."
hardware,3btsp0,epsys,13,Thu Jul 2 09:23:05 2015 UTC,AnandTech - primary  the rest .. meh
hardware,3btsp0,Elessun,0,Thu Jul 2 04:25:33 2015 UTC,AnandTech isn't what they are used to being anymore :(
hardware,3btsp0,decoy11,4,Fri Jul 3 07:14:11 2015 UTC,"They're fine, haven't noticed. They're like the only site with journalists that actually know what they're talking about. Last Tom hardware article I looked at yesterday the guy was worried about 80c VRMs and didn't know they don't break until 125c == 80c completely ok. To which I had to say the following:   Sigh Ugh What is the world coming to"
hardware,3btsp0,pabloe168,0,Fri Jul 3 10:29:39 2015 UTC,"Since Anand (the guy) sold Anand (his website), funny enough, Tom's Hardware and AnandTech are owned by the same company.  I have been noticing some sort of noticeable decline in the overall quality of their articles since the switch, though they are still one of the best in depth coverage of course."
hardware,3btsp0,Exist50,9,Fri Jul 3 10:54:28 2015 UTC,TechReport  Anand  Guru3d  HardOCP  Hardware Canucks
hardware,3btsp0,GeckIRE,3,Thu Jul 2 04:33:56 2015 UTC,I learned to stay away from Linus and Jay as well as barnacles. Reviews are not impressions. Much less gift driven content.
hardware,3btsp0,elevul,5,Sun Jul 5 06:32:33 2015 UTC,"Mostly Anandtech and Tomshardware. The latter isn't great at times, but Anandtech is consistently good, and best of all, seemingly non-biased."
hardware,3btsp0,TRD099,5,Thu Jul 2 04:43:12 2015 UTC,I compiled a list of all the Sites that have been mentioned in this thread   Linus Tech Tips HardwareCanucks Tek Syndicate Paul's Hardware JayzTwoCents Tech Report AnandTech Techpowerup Guru3D Pcper 3Dgameman /r/hardware HardOCP NCIX  Tomshardware
hardware,3btsp0,spiral6,2,Thu Jul 2 08:48:38 2015 UTC,"To go is Anandtech.  But since I'm subscribed to Linus and pcper's youtube channels, their reviews come to me, so those are the ones I watch the most."
hardware,3btsp0,Andrej_ID,2,Thu Jul 2 23:06:47 2015 UTC,"Obviously not JayZ and stuff like that.  Anand, SilentPCReview, Jonnyguru and Hardocp and sometimes TechPowerUp."
hardware,3btsp0,Elessun,2,Fri Jul 3 17:50:07 2015 UTC,NCIX is cool.
hardware,3btsp0,logged_n_2_say,3,Thu Jul 2 05:31:12 2015 UTC,Guru3D. They use proper 1920x1200 resolution and not 1080p :D
hardware,3btsp0,QWieke,3,Thu Jul 2 06:29:58 2015 UTC,Not much love for us 16:10 lovers
hardware,3btsp0,Elessun,1 point,Fri Jul 3 07:17:25 2015 UTC,"the only youtube reviews i watch consistently are about amps, speakers, and headphones and he has a subreddit that he posts them too /r/zeos . (it's all POV, all the time)  with that said, i find youtube channels about hardware to be rife with errors and best for opinions.  also the discussion section on youtube channels is abysmal, and that's one area in other text articles that you can see some good discussion and rebuttal.  i stick to articles for computer hardware, but occasional watch reviews of cases."
hardware,3btsp0,cuicuit,1 point,Thu Jul 2 13:16:15 2015 UTC,"Tweakers.net is an insanely good website,  it's Dutch though."
hardware,3btsp0,Fremdschaemer,1 point,Thu Jul 2 23:12:18 2015 UTC,"Say what you want of Tom, but I love having this friend that talks to me like he's chatting with me about the product instead of the cliches...  OC3D is my go to website for all that they've got available"
hardware,3btsp0,AttackOfTheThumbs,1 point,Fri Jul 3 07:15:45 2015 UTC,"Best one i know is hardware.fr but it's in french, for english review I go to anandtech."
hardware,3btsp0,Sofaboy90,1 point,Fri Jul 3 11:23:06 2015 UTC,ht4u.net is pretty nice if you understand German.
hardware,3btsp0,jinxnotit,1 point,Fri Jul 3 22:13:09 2015 UTC,"Personally I think youtube reviews to be good for cases, certain peripherals, but most of the hardware, you are much better off with text. Superior in every way I can imagine. I can sit on a chart as long as I want without having to pause and that bullshit. I can search for keywords I am interested in or skip to specific sections.  Youtube reviews are years behind, fuck, youtube needs to catch up to pornhub and added the skip to content at x markers or whatever you'd call them. That could drastically improve it.  TechPowerUp has IMO the best GPU reviews. Good writing, great charts, easy comparisons. I only wish they had a bench similar to Anand, but maybe a bit more comprehensive with GPU models, because every year Anand makes a new one and every year it becomes harder and harder to compare certain setups. For example my broken 670 SLI to a Fury X."
hardware,3btsp0,Yearlaren,1 point,Sun Jul 5 03:26:01 2015 UTC,"depends, if im just somewhat interested in somethingm linus and hardwarecanucks are enough for me but if i get seriously interested in buying something i do as much research as possible and thats what everybody should do if you consider seriously buying something."
hardware,3btsp0,AndreyATGB,1 point,Sun Jul 5 13:00:41 2015 UTC,All of the above plus some. Never rely on one source.  Get it from multiple sources so that you get a better feel for how things were reviewed and what they did differently from each other.  Understanding the hardware is really essential as well.
hardware,3btsp0,SirCrest_YT,1 point,Thu Jul 2 06:22:43 2015 UTC,"Anandtech, Tom's Hardware and TechPowerUP. PCper and Guru 3D also seem to be good but I don't frequent them."
hardware,3btsp0,defiance158,1 point,Thu Jul 2 07:20:30 2015 UTC,AnandTech for the content of the review (benches and detailed explanations) and I watch other youtubers (Linus mostly) mostly because I like his videos. I definitely trust Anand the most when it comes to numbers.
hardware,3btsp0,patycy,1 point,Thu Jul 2 09:16:39 2015 UTC,LTT and jayZ are my video go-to's.. But Anandtech and HardOCP are my written sources usually.
hardware,3btsp0,Schmich,0,Thu Jul 2 14:23:04 2015 UTC,"I don't even have a dedicated source for tech reviews.  I mainly come here to /r/hardware and just read everything that's available.   I watch every Linus Tech Tips video, but I mainly watch that for entertainment. I just feel like video reviews are never as thorough as written reviews with accompanying graphs and benchmarks that you can browse through at your own pace."
hardware,3btsp0,patycy,0,Thu Jul 2 02:43:46 2015 UTC,that's definitely true that articles and such are much more thorough..I guess in the end I do watch it for entertainment factor a lot more than anything else.  that's why I'm here...i've exhausted like every linus tech tips video ever :P I could have just tried random youtube reviews but I feel Like Linus and some of the others i mentioned are leagues above some of the others....and I'm trying to find more youtubers of that caliber if possible
hardware,3btsp0,Th3Hitman,0,Thu Jul 2 02:46:55 2015 UTC,Primarily Hardocp for hardware reviews. Anandtech as review comparisons against hardocp. These two sites are probably the more consistent in their testing compare to the other sites around the web.
hardware,3btsp0,DoTheEvoIution,1 point,Thu Jul 2 04:54:43 2015 UTC,Too biased towards Nvidia.
hardware,3btsp0,Unoid,1 point,Thu Jul 2 18:37:39 2015 UTC,Hardocp or Anandtech or both?
hardware,3bv8yd,swinders,29,Thu Jul 2 12:51:29 2015 UTC,Plug it in to my Gibson and hack the planet.  What the fuck is this post?
hardware,3bv8yd,jinxnotit,16,Thu Jul 2 16:49:36 2015 UTC,"USB drives aren't allowed on the premise. If one gets plugged in to a company asset, it will notify CSC and my machine will be temporarily decommissioned until thoroughly cleaned .. for like, no reason.  So if a usb drive got past security, and found its way on my desk, pretty sure I'd just throw it in the garbage. Or take it home with me.   Yeah, I'd probably take it home."
hardware,3bv8yd,Jesse36m3,5,Thu Jul 2 16:51:09 2015 UTC,"That's a security risk, what the fuck do you do that they have those procedures in place?"
hardware,3bv8yd,Tenzarin,12,Thu Jul 2 16:52:43 2015 UTC,"All I can say is that I'm a Sr. Draftsman who works for a large corporation that primarily does business with the U.S. Navy.  Ridiculous, isn't it?"
hardware,3bv8yd,Jesse36m3,8,Thu Jul 2 17:09:58 2015 UTC,Not entirely.
hardware,3bv8yd,Tenzarin,7,Thu Jul 2 17:11:40 2015 UTC,"I was only joking about that because it's happened here before.   Seeing kids trying to plug their phone charger into the computer is hysterical. No, wait Jeff, don't plu .. ahherhghhgmmm . um. Nothing. Carry on."
hardware,3bv8yd,Jesse36m3,3,Thu Jul 2 17:16:07 2015 UTC,And that's why I keep a wall charger with me.
hardware,3bv8yd,Charwinger21,1 point,Fri Jul 3 20:23:06 2015 UTC,"Free usb condom, anybody?"
hardware,3bv8yd,fieldofsnowkiller,3,Sat Jul 4 10:58:01 2015 UTC,"Wasn't it the Navy that threw away a bunch of mice because the systems they had been used with had been compromised?  It was some federal agency, if it wasn't the Navy."
hardware,3bv8yd,Slamdunkdink,2,Fri Jul 3 20:34:13 2015 UTC,"I'm not sure, I've never heard of that one before. They are crazy however, so it doesn't seem too far-fetched."
hardware,3bv8yd,Jesse36m3,1 point,Sat Jul 4 18:12:15 2015 UTC,Found a link - http://arstechnica.com/information-technology/2013/07/us-agency-baffled-by-modern-technology-destroys-mice-to-get-rid-of-viruses/
hardware,3bv8yd,Slamdunkdink,1 point,Sun Jul 5 19:10:07 2015 UTC,Tell us your line of business
hardware,3bv8yd,Pat-Roner,1 point,Thu Jul 2 17:02:28 2015 UTC,"That seems like a very make-work solution compared to just having the drive not work  ""Oh yeah totally need to clean everything if a drive has been plugged in BTW I need more people and a raise for managing said people."""
hardware,3bv8yd,hisroyalnastiness,13,Thu Jul 2 18:56:02 2015 UTC,"As someone who works in the DOD industry, we have to watch shitty security training webinars on this.   Comes complete with a ""bad guy"" wearing a ski mask ""hacking"" on a computer."
hardware,3bv8yd,TransverseMercator,3,Thu Jul 2 20:13:07 2015 UTC,AND HE ITERATES OVER ALL THE KEYS AND KILLS EVERYBODY IN THE COUNTRY
hardware,3bv8yd,fieldofsnowkiller,1 point,Sat Jul 4 10:58:31 2015 UTC,"well it didn't work did it, OPM got hacked and basically everyone with a security clearance and their families and friends just became targets of China. I bet next time you get some electronics it will come with a mic in it to record what you do at home to get blackmail on your to force you to spy on our government. OPM idiots totoally dropped the ball, all people who had their SSNs and names hacked should get new SSNs and new names and relocate."
hardware,3bv8yd,xtothemess,1 point,Sat Jul 4 17:55:41 2015 UTC,Idiots just think they are charging their cellphones is basically what happens.   Source: US ARMY 7 years
hardware,3bv8yd,Bogus1989,12,Mon Jul 6 12:10:27 2015 UTC,Go home and plug it in a spare laptop/raspberry pi
hardware,3bv8yd,acdop100,12,Thu Jul 2 15:37:30 2015 UTC,Plug it into someone else's computer and walk away
hardware,3bv8yd,jankenpwn,7,Thu Jul 2 16:29:55 2015 UTC,Most of these commenters didn't read the attached article.   Who ever built that contraption is kind of a dick! Now I can never trust any USB device l device ever again.
hardware,3bv8yd,335is,4,Fri Jul 3 13:48:04 2015 UTC,My office workstation is shitty so i would not care and plug it in.
hardware,3bv8yd,Pat-Roner,2,Thu Jul 2 17:03:09 2015 UTC,"Who knows, you might end up getting a better computer."
hardware,3bv8yd,Slamdunkdink,4,Fri Jul 3 20:35:26 2015 UTC,One of my jobs is located at my University. So i'd plug it into one of these shitty thin clients around here and hope it somehow destroys them and we get better not as shitty thin clients ( or actual computers)....fuck thin clients.
hardware,3bv8yd,T-Wrecks559,4,Thu Jul 2 17:07:36 2015 UTC,That's happening here too...CAD over VNC sounds great if you're the budget guy not the people that actually have to use it
hardware,3bv8yd,hisroyalnastiness,3,Thu Jul 2 18:58:41 2015 UTC,"Computer/electrical engineer major here. No cad yet, not sure if I'll even take a class. However, I do enjoy programming on a linux server that I have remote access to. If for whatever reason I don't have my laptop, the damn thin clients are so slow it lags from typing alone. Hell, even moving a window around too fast causes it to freeze for a while.  I learned from one of my csci teachers that linux > microsoft visual, so i don't do much coding on my actual computer, unless i really need the extra ram and cpu power, but I haven't needed yet."
hardware,3bv8yd,T-Wrecks559,4,Thu Jul 2 20:47:02 2015 UTC,"I find it hilarious all this paranoid answers. 99% of the people on the planet would just go straight to the computer.  However I'd ask good o'l Bernie, the coworker that generally doesn't pay attention, to plug it in his PC and report back."
hardware,3bv8yd,pabloe168,3,Thu Jul 2 19:09:33 2015 UTC,"I'm in the military, so I'd immediately plug it into a computer I'm logged in on."
hardware,3bv8yd,decemberwolf,2,Sat Jul 4 02:31:03 2015 UTC,"I'd build a VM on an un-networked device and see what's on it, then format the whole VM, rebuild the machine and set the USB on fire."
hardware,3bv8yd,elevul,2,Thu Jul 2 21:29:41 2015 UTC,Boot my Pentium 4 Linux server and connect it there.
hardware,3bv8yd,0x60,1 point,Thu Jul 2 22:58:07 2015 UTC,Be very very careful with it
hardware,3bv8yd,bebopfan,1 point,Thu Jul 2 15:43:26 2015 UTC,Plug it into my coworker's laptop who always leaves his computer unlocked.
hardware,3bv8yd,xandergod,1 point,Thu Jul 2 16:54:51 2015 UTC,Plug it into my chromebook under the guest login.
hardware,3bv8yd,Maysock,1 point,Fri Jul 3 00:23:30 2015 UTC,Probably throw it into my empty hackbook running tails.
hardware,3bv8yd,sterob,1 point,Fri Jul 3 02:32:32 2015 UTC,my ghetto computer doesnt even have USB port.
hardware,3bv8yd,Slamdunkdink,1 point,Fri Jul 3 08:24:13 2015 UTC,"Sure wouldn't use it at work.  What if it loaded child porn to your C: drive root.  Then someone could alert HR.  Might be a way to get someone out of the way so you could get promoted, or just get even with someone.  Or maybe some sadistic sob just getting his jollies."
hardware,3bv8yd,xtothemess,1 point,Fri Jul 3 20:40:12 2015 UTC,Modern motherboards we have today have specific guards in line for this. Cheap OEMs that make most of the laptops and desktops don't integrated defenses like aftermarket motherboard vendors have.
hardware,3bv8yd,skilliard4,1 point,Sat Jul 4 17:53:00 2015 UTC,"Report it to IT, as someone is likely trying to hijack security with social engineering. I guess I should read the article, brb.  Edit: was close, it was a device intended to output an ESD to destroy a workstation. Still a successful choice, unless IT thinks they're witty by disabling autorun and running it on a Virtual machine, to which the esd could still occur."
hardware,3bv8yd,IronWolve,1 point,Sun Jul 5 04:16:47 2015 UTC,"Not give 2 shits, flash it and use it."
hardware,3brsu8,SHEADYguy,28,Wed Jul 1 17:04:16 2015 UTC,This is what I'd have to imagine an overclocked refrigerator would sound like.
hardware,3brsu8,selfpostnokarma,18,Wed Jul 1 19:24:11 2015 UTC,"i do think it sounds bad, but i dont know how much of an issue it would be not sitting on a test bench listening right next to it.  here is the unaltered audio (which is likely more muted than real world at that distance) from the article  but wouldn't a more typical use be alright?  my define mini is under my desk.  with all that said, the biggest potential issue with it is:   it is worth noting that the high frequency sound from the Fury X cards is constant, even when idle and in Windows.   if it's audible in a case, that would drive me crazy.  when gaming you have sound from somewhere (typically) but sitting idle and regular use would be my problem."
hardware,3brsu8,logged_n_2_say,9,Wed Jul 1 20:12:54 2015 UTC,Apparently a huge issue based on the amount of complaints I've personally seen so far.
hardware,3brsu8,TaintedSquirrel,4,Wed Jul 1 20:52:15 2015 UTC,"i know you have more of a pulse on the situation than i do.    i didn't even realize it was an issue at all, and saw this post a day ago and most on there seemed to pooh-pooh it."
hardware,3brsu8,logged_n_2_say,4,Wed Jul 1 21:03:46 2015 UTC,AMD says they fixed this in production units. Can anyone confirm?
hardware,3brsu8,a_monkie,2,Thu Jul 2 01:50:19 2015 UTC,What? Lol.   Can you read this thread where AMD obviously is using a different pump?
hardware,3brsu8,jinxnotit,1 point,Thu Jul 2 03:48:44 2015 UTC,Doesn't change the fact you can buy faulty cards at retailers.
hardware,3brsu8,GIassHalfFull,1 point,Thu Jul 2 04:20:00 2015 UTC,"Or faulty first runs of a brand new product.  But this ""article"" needs to address the new pump, not just deny the PR response and make up non-sense that supports their opinion. PCPer doesn't want the rep of being Fox News, I don't think."
hardware,3brsu8,jinxnotit,1 point,Thu Jul 2 04:56:40 2015 UTC,I don't think they care what some random idiot on the internet thinks.
hardware,3brsu8,GIassHalfFull,1 point,Thu Jul 2 05:01:12 2015 UTC,They should. Since that appears to be their reader base.
hardware,3brsu8,jinxnotit,25,Thu Jul 2 05:11:55 2015 UTC,"There are few things to take away from this testing. First, the currently selling and shipping AMD Radeon R9 Fury X cards do not include any kind of fix for the pump whine or sound levels of the cooler. Though AMD tells me that we just had a miscommunication or misinterpretation of the comments they shared with us prior to our NDA, I think it is at the very least fair to say that AMD was hoping to deflect the issue on launch day. Now that retail cards are out and end users (not just us) are getting their hands on them, I think its obvious that the sound issues are more of a problem than AMD would like to admit.   I was expecting this result    I have looked over /r/amd and it seems quite few people returned their cards for pump noise and / or coil wine"
hardware,3brsu8,zmeul,5,Thu Jul 2 06:05:57 2015 UTC,As it should be. If there are problems they should be dealt with.
hardware,3brsu8,jinxnotit,0,Wed Jul 1 17:40:48 2015 UTC,"So apparently PC Perspective doesn't know much about watercooling?  In regards to AMD saying pump whine is normal PCP respond with the following:   I would disagree that this is ""normal"" but even so   Pump whine is a common concern with pumps and great care is spend to avoid it.  There's a lot of isolation and damping built into pump mounting and that doesn't always solve the problem.  You're recommended to avoid certain types of common pumps if you can afford to because they're known for being worse for this sort of thing.  Also, their graphs are terrible.  They use a linear scale on a frequency response.  Who in their right mind does that?  Their analysis ""spikiness=worse"" is amateur at best.  On top of this, they do their testing in an open bed setup with the sensor 6"" from the card.  First, putting the card in a closed case will significantly change how it responds at the case will attenuate noise better or worse depending on the frequency.  Second, the positional relationship between the sound and the listener is pretty critical, and 6"" away is hardly representative of how a user might experience the sound.  All in all, there may very well be some pump whine issues, but this is not the article to base that on."
hardware,3brsu8,tarheel91,20,Wed Jul 1 22:50:15 2015 UTC,R9 295X2 has 2 pumps and doesn't sound as Fury X
hardware,3brsu8,zmeul,12,Wed Jul 1 17:47:43 2015 UTC,Fuck. I can't even hear my 295x2s pump. This is BS and they need to get their asses on it.
hardware,3brsu8,Teresi2Finger,-18,Wed Jul 1 18:52:38 2015 UTC,"The R9 295X2 uses 2 pumps that are completely different from the one used in the Fury X.  Hell the entire solution is made by Asetek vs. CM for the Fury X.  They're not comparable in any way.  I'd also point out that 2 pumps are more likely to be quieter than 1 pump, as each pump doesn't have to run as hard.  However, space constraints likely makes that infeasible on the Fury X."
hardware,3brsu8,tarheel91,28,Wed Jul 1 19:12:24 2015 UTC,What is comparable are results:  One has much worse pump whine/noise than the other.
hardware,3brsu8,meowffins,19,Wed Jul 1 19:09:28 2015 UTC,"How are they not comparable? They're two pumps designed for this exact purpose, theyre direct competitors and the cooler master pump is obviously the lesser one. Thats like saying that an msi custom card is uncomparable to an evga one because theyre not the same manufacturer, ridiculous."
hardware,3brsu8,ZeM3D,-8,Wed Jul 1 19:20:20 2015 UTC,"They have different load cases and space constraints.  Two pumps don't have to run as hard as a single pump.  The 295X2 cooling solution covers only the GPUs.  The Fury X cooling solution has to cool the VRMs, GPUs, and memory.  Should I go into more differences or is that sufficient?"
hardware,3brsu8,tarheel91,3,Wed Jul 1 20:25:26 2015 UTC,"You're failing to understand a key point here; we don't care about the differences, only the results. One is annoying 100% of the time (idle and load), and one isn't. That's all the end user is going to care about. No one is going to say ""this annoys the shit out of me, but it's a single pump so it's perfectly fine."""
hardware,3brsu8,Nixflyn,12,Wed Jul 1 20:28:58 2015 UTC,comparable?! like in what? AMD could've used the Asetek pumps instead of the cheapo CoolerMaster ...  then yeah! I will compare them
hardware,3brsu8,zmeul,-13,Thu Jul 2 01:15:59 2015 UTC,1)  Asetek designed the entire cooling solution for the 295X2.  CM designed the entire cooling solution for the Fury X.  2) What does any of this have to do with my point that the analysis by PCP was poorly done and reported?
hardware,3brsu8,tarheel91,8,Wed Jul 1 19:48:16 2015 UTC,"everything ! since they done the same ""poor analysis"" on both specimens"
hardware,3brsu8,zmeul,-8,Wed Jul 1 20:07:23 2015 UTC,"There's a saying in regards to testing (and programming) ""Garbage in, garbage out.""  Comparing two piles of garbage doesn't give you much value."
hardware,3brsu8,tarheel91,24,Wed Jul 1 20:18:48 2015 UTC,"I owned many water cooling setups and none of them had that sound so don't try to say that this is ""normal""   Even the mpc655 pump at maximum doesn't produce that sound and probably moves 10 times more water.  Also that high pitch noise can be heard meters away"
hardware,3brsu8,Raising,7,Wed Jul 1 20:23:38 2015 UTC,"Lemme quote the product description on FrozenCPU for the MPC655:    Considerably reduced audible operating noise: the high pitch frequency of the MCP65[0] is now eliminated thanks to a new chipset controller.   High frequency whine is more common with cheaper pumps.  You find it a lot of times in pump/res combos for bay drives and the like.  Also, again, I'm not saying there's no pump whine issue.  I'm saying the testing done in this article was poorly conducted and reported and not grounds for making a conclusion one way or the other."
hardware,3brsu8,tarheel91,-8,Wed Jul 1 18:38:27 2015 UTC,dude.. there's a bunch of mpc655 pumps mine has none of that high pitch
hardware,3brsu8,Raising,9,Wed Jul 1 18:45:45 2015 UTC,"Reread that, buddy.  The MPC655 improved on the MPC650 which had a high pitch whine issue.  Not saying there's anything wrong with the MPC655.  I'm saying there are documented examples of high pitch whine on pumps."
hardware,3brsu8,tarheel91,1 point,Wed Jul 1 19:25:32 2015 UTC,Sounds like my bloody fish tank when it needs cleaning out
hardware,3brsu8,Pieloi,7,Wed Jul 1 20:03:44 2015 UTC,They use a linear scale on a frequency response. Who in their right mind does that?    Someone who's never seen the inside of an engineering dept.
hardware,3brsu8,lordx3n0saeon,-1,Thu Jul 2 02:03:38 2015 UTC,Perfectly said.
hardware,3brsu8,jinxnotit,-8,Wed Jul 1 20:24:26 2015 UTC,"It's kinda like synthetic benchmarks. A majority of the time, they are relatively useless. Is there an issue here? Well ya, maybe. But the testing methodology is both flawed, and unrealistic to a real world scenario. Systematic testing of the noise, done properly and in different cases would have been much more illuminating. But, PcPer had always been slanted pretty hard against AMD so I take their reviews with a grain of salt."
hardware,3brsu8,glr123,4,Wed Jul 1 22:47:35 2015 UTC,It is pretty much a known fact that the pump whine is an issue with or without this article.  So it's actually the opposite of synthetic benchmarks. Reviewers have experienced the pump whine first hand.
hardware,3brsu8,meowffins,0,Wed Jul 1 18:47:00 2015 UTC,"Similarly, it's pretty much a known fact that Nvidia outperforms AMD in 3DMark benchmarks. But, in real world applications, competing cards are much more even.  I didn't say that the pump whine doesn't happen. What I did say is that their synthetic testing methodolgy falls short of real world practicality. Similar to synthetic benchmarks, what may be there is something that is likely to not influence the majority of users in real world applications which is not an open mobo on your desk with no case."
hardware,3brsu8,glr123,2,Wed Jul 1 19:22:27 2015 UTC,AMD just keeps looking like the B-team at this point. I don't even know what to say anymore.
hardware,3brsu8,XaeroR35,3,Wed Jul 1 21:40:36 2015 UTC,Damn them for outsourcing their water cooler!  If only it performed as good as Nvidias. Oh... Wait...
hardware,3brsu8,jinxnotit,4,Wed Jul 1 19:50:55 2015 UTC,Maybe nvidia has to right. Let 3rd parties do water cooling.
hardware,3brsu8,ken27238,0,Wed Jul 1 22:45:38 2015 UTC,"AMD did.  Initial batch has issues, so it's revised and upgraded for the next shipment and it becomes a non-issue.  Just more clickbait for PCPer to publish."
hardware,3brsu8,jinxnotit,3,Wed Jul 1 23:05:27 2015 UTC,Nvidia should start producing hotter cards so they can use watercooling. C'mon guys.
hardware,3brsu8,darrenphillipjones,0,Wed Jul 1 23:09:11 2015 UTC,"Yeah, no one wants water cooled Nvidia cards.  Troll harder."
hardware,3brsu8,jinxnotit,0,Thu Jul 2 00:11:54 2015 UTC,"Was just a joke? Watercooled cards are massive in size, are more expensive, and have excess tubing to deal with. a few years ago it'd be awesome! But everyone has mitx cases now. So it's a hassle dealing with all that stuff."
hardware,3brsu8,darrenphillipjones,1 point,Thu Jul 2 00:17:16 2015 UTC,My Ncase M1 is ready.
hardware,3brsu8,dstew74,1 point,Thu Jul 2 00:21:41 2015 UTC,"Oh man, I am about to order some custom sleeving. hnnng"
hardware,3brsu8,darrenphillipjones,1 point,Thu Jul 2 01:57:14 2015 UTC,"I have the EVGA Hybrid on a 980Ti in my M1. Load temps are around 45C, or 52C with the fan at 7V."
hardware,3brsu8,makar1,0,Thu Jul 2 03:27:49 2015 UTC,"Sorry, This article just pisses me off at how unprofessional and slanted it is and he still wants to pretend he's some unbiased reporter.  I just bought the Cougar QBX, comes with a 120mm mount right at the side. I could name at least five other ITX cases from different manufacturers that are AIO water cooling compatible."
hardware,3brsu8,jinxnotit,2,Thu Jul 2 14:05:27 2015 UTC,"Yea, but you have to agree it adds a lot of bulk to the card. Even the ""small"" fury x is now huge when you look at the foot print it creates."
hardware,3brsu8,darrenphillipjones,2,Thu Jul 2 00:29:31 2015 UTC,It does. But I would rather it come water cooled than not.  Just because of the overclock capability it's going to afford.
hardware,3brsu8,jinxnotit,2,Thu Jul 2 00:34:53 2015 UTC,What overclocking ability?
hardware,3brsu8,makar1,1 point,Thu Jul 2 00:48:21 2015 UTC,You think it's going to stay locked? Really?
hardware,3brsu8,jinxnotit,0,Thu Jul 2 15:01:56 2015 UTC,If you want a water cooled nVidia card there are plenty of 3rd party and after market options. The problem AMD have is that their flagship card runs so hot water cooling is basically a necessity.
hardware,3brsu8,gizza,0,Thu Jul 2 16:27:41 2015 UTC,"Thanks for posting and revealing you know jack shit.  Fury X under load - 50C 980ti under load - 83C  If Fury were so hot it needed water, its core temps would be as hot as the 980ti reference temps under air."
hardware,3brsu8,jinxnotit,2,Thu Jul 2 03:40:11 2015 UTC,The fuck are you on about? It only runs as cool as 50C because it's water cooled.
hardware,3brsu8,gizza,-1,Thu Jul 2 04:17:06 2015 UTC,"You know nothing about what you are talking about is what ""I'm on about"".  If it ""needed"" water cooling its core temperatures wouldn't be a damn near frosty 50 degrees.  980ti core temperatures are 83 degrees. That's a massive thermal difference to the Fury."
hardware,3brsu8,jinxnotit,1 point,Thu Jul 2 04:27:26 2015 UTC,My 980Ti with AIO cooling runs at 45C under load
hardware,3brsu8,makar1,0,Thu Jul 2 04:30:08 2015 UTC,"Neat. Your anecdotal evidence contributes greatly as to why Fury ""needs"" a water cooler."
hardware,3brsu8,jinxnotit,0,Thu Jul 2 14:05:59 2015 UTC,You can't compare the temp of water cooling to air cooling. My CPU when overclocking topped 90C until I got a water cooler and now at an even higher OC it never tops 50C.
hardware,3brsu8,gizza,3,Thu Jul 2 14:31:56 2015 UTC,"And 80% of the PC gaming community breathes a sigh of relief... ""At least I didn't buy one of those!""  The one thing AMD consistently does right is make Nvidia look good."
hardware,3brsu8,TaintedSquirrel,10,Thu Jul 2 04:48:33 2015 UTC,Good thing Nvidia is even better at making themselves look like assholes.
hardware,3brsu8,atriax,1 point,Wed Jul 1 20:45:20 2015 UTC,Obviously not that good at it seeing as how their market share continues to increase.
hardware,3brsu8,gizza,-1,Wed Jul 1 21:27:59 2015 UTC,"With shit articles like this, and gullible trolls like you, it's not so hard to see."
hardware,3brsu8,jinxnotit,-3,Thu Jul 2 03:41:44 2015 UTC,B-Team with A-Team pricing. So in my book the fury is a C-Team sort of card. The 290 is still the champion of the B-Team though.
hardware,3brsu8,darrenphillipjones,2,Thu Jul 2 04:22:28 2015 UTC,It's just more PCPer clickbait.  The production pumps are in the wild.  Anything with the sticker is the pre-production model regardless of it being sold in retail packaging.
hardware,3brsu8,Maldiavolo,2,Thu Jul 2 00:12:49 2015 UTC,Can you share anything supporting that the new models are no stickered?
hardware,3brsu8,darrenphillipjones,1 point,Wed Jul 1 23:50:16 2015 UTC,Ahh I just did.  See: link to the picture.
hardware,3brsu8,Maldiavolo,4,Thu Jul 2 00:14:10 2015 UTC,"I see the photo, I have read. But you show a photo of an open card, and claim something about a sticker. What sticker? I see several. Also do you have a photo of every manufactures cards not having it?"
hardware,3brsu8,darrenphillipjones,1 point,Thu Jul 2 00:29:09 2015 UTC,Last picture on the bottom.  Have you read no reviews of the Fury X?  http://www.pcper.com/reviews/Graphics-Cards/AMD-Radeon-R9-Fury-X-4GB-Review-Fiji-Finally-Tested/AMD-Fury-X-Graphics-Card-
hardware,3brsu8,Maldiavolo,-14,Thu Jul 2 00:33:46 2015 UTC,"Suck that green cock harder! Lol.  PCPer now officially team green.  Edit for my ""fans""."
hardware,3brsu8,jinxnotit,1 point,Thu Jul 2 18:28:17 2015 UTC,"How about actually refuting what PCPer said and not making a stupid comment that adds no content to this discussion? ""hurr durr they $ayd $omting wrong that meens they h8 AMD hurr durr"""
hardware,3brsu8,Idkidks,1 point,Wed Jul 1 22:44:47 2015 UTC,"I guess pointing out bias by ignoring what AMD actually said, then supplanting their own ideas of what they think things are isn't enough.  Where would you like me to start?   The fact that they ignored AMD's statement and drew their own false conclusions? The fact that they think all pumps have zero noise? The fact that they think a 980ti reference cooler despite being an order of magnitude louder at load is actually ""more pleasant"" as fact instead of letting their reader draw their own conclusion? The fact they tested on an open bench instead of in a case to magnify and amplify the sound? The fact that they can't even establish a legitimate testing methodology using sensitive instruments? The fact that they STILL haven't added a correction to the story pointing out that there are in fact changes to the pump in newer retail batches as AMD claimed?   You pick the starting point. And we'll go from there."
hardware,3brsu8,jinxnotit,1 point,Fri Jul 3 04:11:40 2015 UTC,"Good job, you finally brought some intelligent thought! By the way, I love AMD just as much as you do, I just don't make noncontributory comments on threads."
hardware,3brsu8,Idkidks,0,Fri Jul 3 04:37:55 2015 UTC,"I'm not really suprised the pump is whining. All the stupid right angle bends they've got, they're not exactly doing wonders for the amount of work the pump needs to do to get the damn fluid moving.  Lots of right angles and that stupid flattened section over the VRMs. Moronic appreciation of fluid flow."
hardware,3byrta,kidwuf,42,Fri Jul 3 06:43:11 2015 UTC,"It would be one thing if these were raw images, but comparing frames of a Youtube video is inane."
hardware,3byrta,redditedc,-15,Fri Jul 3 06:57:43 2015 UTC,"See my response below, this post is not about graphic or texture quality. Decals and clutter are missing on the Titan X. The lines, arrows, and clutter are lessened and cut out sooner (lower draw distance) on the Titan X than the Fury X."
hardware,3byrta,redditedc,16,Fri Jul 3 08:12:20 2015 UTC,The screencaps of the video are ghosted as hell making it hard to make out details in the first place and the shots are not from the same location.   I'll wait for some real stills.
hardware,3byrta,Kiloutre,10,Fri Jul 3 08:43:21 2015 UTC,"The screencaps of the video are ghosted as hell making it hard to make out details in the first place and the shots are not from the same location.   Yeah it's pretty hard to check anything, but if you look at the roads you clearly see that there's more details on the Fury side than on the Titan X side. Dunno if the game is responsible for that (since in BF4 you can destroy things it's highly possible) or if it's just different settings, though."
hardware,3byrta,TRD099,2,Fri Jul 3 11:18:29 2015 UTC,"You should watch the videos, you can notice the difference.   I'm not saying it was different settings but who knows."
hardware,3byrta,jimanjr,6,Fri Jul 3 17:47:02 2015 UTC,Looks like a FOV difference to me. And that changes a lot of things like filtering and so on...
hardware,3byrta,Elessun,19,Fri Jul 3 13:50:22 2015 UTC,"The images in comparison aren't even from the same version of the game, this is silly.  For example, check the <D> marks to the right of the blue sign in the first picture, in the up right corner of the black area in the second picture, and on the fuse boxes in the last image; they are rendered completely different.  Different graphics cards do not cause differences in objects rendered, they might render it better or worse indeed, but they do not alter the overall output that much, after all it is the same set of draw calls given to them in a case of apples to apples comparisons.  This is pure BS, also I agree with /u/redditedc's post on the ridiculousness of doing comparisons over YouTube videos. Those two videos might even be taken with different capture software at different settings, yet alone what I have mentioned above."
hardware,3byrta,elusive_cat,-10,Fri Jul 3 07:09:10 2015 UTC,The post is not talking about graphic or detail quality. It is talking about draw and decal view distance. The Titan X has on the street fewer clutter and the street lines and arrows stop much closer to the player than on the Fury X. Compressed or not that stuff is still missing.
hardware,3byrta,parasemic,6,Fri Jul 3 08:10:37 2015 UTC,There is no point in comparing two different builds of a game. One could be optimized better which then allowed devs to enhance draw distance or add more details etc.  As an example you could compare Witcher 3 2013 E3 demo running on Titan X and the actual 2015 game running on Fury X and claim how one card is better than the other.
hardware,3byrta,dylan522p,-3,Fri Jul 3 09:21:45 2015 UTC,What fucking two different builds of the game? The OP is running same game version with both cards right after one and another with exact same settings. TX looks like shit compared to FX for some reason.
hardware,3byrta,parasemic,4,Fri Jul 3 18:36:00 2015 UTC,Compression that youtube does destroys any ability to make am actual comparison
hardware,3byrta,dylan522p,-1,Fri Jul 3 19:11:52 2015 UTC,"Yes, but it should and would affect both videos identically.  http://i.imgur.com/ooRehGg.png  That much of a difference cannot be caused by compression. Why would it magically just affect other half of the video?"
hardware,3byrta,grannyte,1 point,Fri Jul 3 19:15:04 2015 UTC,Because it was recorded in a different way or compressed in a different way....
hardware,3byrta,parasemic,5,Fri Jul 3 20:15:39 2015 UTC,Only in the first video the op redid it ensuring both drivers were not modifying any setting and that his capture card was setp identicaly and there still are somme differences between the fury X and the 980TI.  He even filmed an other game swapping the fury X and th 980 Ti in case youtube would be compressing one side of the video more yet again the Fury X looked better.  So now there are sill some issue with the methodology the next step is to record internaly instead of through an external capture card to ensure it's not some compatibility issue betweent he capture card and the nvidia GPU. Next will be still screenshots in case the frame rendered by the 980 Ti is compressed more in video becaus compressing codec hate nviia.
hardware,3byrta,dylan522p,2,Fri Jul 3 20:27:19 2015 UTC,Apparently that didn't happen
hardware,3byrta,parasemic,1 point,Fri Jul 3 20:18:38 2015 UTC,Yup because that's the only person who has ever noticed the difference between the two when the two cards are performing the same calculations and all the people out there that benchmark all these cards never noticed a difference.
hardware,3byrta,CallMePyro,1 point,Fri Jul 3 20:37:47 2015 UTC,You do understand a huge majority of reviewers do not actually test all cards all over again for every new piece of hardware but compare the result to already done tests? Not many people actually own Titan X and Fury X to test both after one another
hardware,3byrta,CommanderArcher,15,Fri Jul 3 20:50:02 2015 UTC,These aren't raw imagines. These are compressed screenshots of a compressed youtube video. Pointless.
hardware,3byrta,CallMePyro,2,Fri Jul 3 07:07:23 2015 UTC,"the compression wouldnt get rid of items at random and make one video look shittier than the other. they are producing different outputs which is why one is worse, the compression wouldnt magically decrease the draw distance and LOD distance on one but not the other. this is either on Nvidia or the game version."
hardware,3byrta,dylan522p,-8,Fri Jul 3 23:21:41 2015 UTC,"Compressed or not, if something is missing, like decals on the street (puddle reflections and lane lines and arrows) it will still be missing in the compressed format."
hardware,3byrta,CallMePyro,2,Fri Jul 3 08:05:54 2015 UTC,"All I see is two side by side screen shots. The post claims both the games were at the same settings but I find that quite hard to believe, and there certainly is no proof that these are actually a 980ti and a fury X being compared.  All I'm saying is that I'd much rather stay skeptical and investigate more instead of just rallying the pitchforks like you seem to be attempting to do."
hardware,3byrta,Shahnewaz,-5,Fri Jul 3 17:56:21 2015 UTC,Compression literally destroys any sort of detail that you were comparing
hardware,3byrta,imallin,-8,Fri Jul 3 08:50:47 2015 UTC,"PNG is lossless compression, moron."
hardware,3byrta,sifnt,2,Fri Jul 3 17:14:26 2015 UTC,"Uh. Not really sure what the hostility is for, but whatever.   Fair enough, the picture compression probably doesn't matter. The youtube compression, however, could easily account for the differences you see.  edit: Also as other posters above me have mentioned, these aren't even the same versions of the game."
hardware,3byrta,domiran,3,Fri Jul 3 17:54:02 2015 UTC,The root of the problem is the image quality setting in Nvidia control panel. Check the difference (and proof) here: http://hardforum.com/showpost.php?p=1041709168&postcount=84
hardware,3byrta,hdshatter,2,Sat Jul 4 20:24:42 2015 UTC,"Theres some in depth discussion of this over at anandtech.  Apparently the Nvidia default CC options have crap filtering,  and changing the settings to match AMD results in an 8-10% performance drop.  There are multiple games that this has been observed in."
hardware,3byrta,godsayshi,3,Sat Jul 4 13:52:52 2015 UTC,Certainly wouldn't be the first time drivers have cheated in benchmarks (even if the original comparison attempted to be genuine with same settings/game build)...  I seem to remember NVIDIA cheating in 3d mark 2003 (or 2006?) some years ago. Hopefully someone else will recall the specifics.
hardware,3byrta,namae_nanka,3,Fri Jul 3 10:42:20 2015 UTC,"If I remember right, both NVIDIA and AMD were caught with their hands in the cookie jar at one point on this. Thems were some dark days."
hardware,3byrta,lucun,0,Sat Jul 4 10:10:48 2015 UTC,"Pretty sure Nvidia still cheats in those, their scores just don't line up right with AMDs if you start playing real games."
hardware,3byrta,MichaelDeucalion,1 point,Sat Jul 4 21:43:49 2015 UTC,"Anecdotally I upgraded from a Kyro II to a GeForce a decade ago and I still remember today that the rendering quality really seemed very different. The Kyro II was slower but I felt it had much more detailed and colourful scenes. I did not go through the trouble to test this properly however. It may have been subjective or more likely down to different defaults for the cards.  Although a couple of snaps from a scene that appears to be in motion from youtube are essentially useless, I still think it's a topic the could create some useful discussion if tests and research were done properly."
hardware,3byrta,Aggressorot,2,Sat Jul 4 21:17:13 2015 UTC,All it looks like is lower anisotropic filtering on the Titan which wouldn't effect FPS anyway.   A 10% difference in his later video.
hardware,3byrta,sk9592,1 point,Fri Jul 3 18:18:31 2015 UTC,"He clicked no to save settings on the Titan X video. The game didn't ask him to save settings in the Fury X video...  EDIT: also, doesn't the NVidia control panel allow overriding 3d application settings such as AA if the user changes the default settings?"
hardware,3byrta,I-never-joke,1 point,Sun Jul 5 04:08:14 2015 UTC,op come on m8 you're just diggin the hole deeper in these comments
hardware,3byrta,thejshep,-6,Sat Jul 4 03:14:25 2015 UTC,"ITT butthurt people negating stuff of the bat... I don't say that the data shown is legit, but instead of downvoting and calling bullshit you might wanna make a little research which would benefit us all."
hardware,3byrta,BlayneTX,0,Sat Jul 4 03:37:44 2015 UTC,"I don't say that the data shown is legit, but instead of downvoting and calling bullshit   Uhm, no. Calling bullshit is exactly the correct response. Anyone who wants to make claims about rendering differences between the Titan X and Fury X needs to first put forward legitimate evidence. Then we can discuss its implications.    you might wanna make a little research which would benefit us all.   Putting forward bullshit doesn't require everyone else to go off looking for ways to confirm it. That's on the person making the claim."
hardware,3brzcp,catch878,13,Wed Jul 1 17:51:35 2015 UTC,Looks like this is the phone for everyone saying they'd sacrifice other things for battery life.
hardware,3brzcp,Exist50,6,Wed Jul 1 18:14:49 2015 UTC,"I bought a portable 16000mAh battery that i used to carry when i traveled. Costed Cost about 40€. It's so useful that i carry it with me inside my bag where ever i go.  The s3 is a power drain, but never had problems since"
hardware,3brzcp,his_penis,5,Wed Jul 1 18:28:33 2015 UTC,When I got my latest phone I simply bought a couple extra batteries and an external charger. ( A wall charger that a battery can be placed into. )
hardware,3brzcp,Hysterics7787,1 point,Thu Jul 2 01:10:25 2015 UTC,"I still rather my portable battery. It's no bigger than my phone, i can charge it from a wall charger as well and i can charge my phone 6-7 times just as fast as a wall charger :-)  It takes half a day to recharge that big boy completely though"
hardware,3brzcp,his_penis,2,Thu Jul 2 12:05:40 2015 UTC,costed   Just cost. A friendly FYI for future use :)
hardware,3brzcp,AttackOfTheThumbs,1 point,Sun Jul 5 03:31:27 2015 UTC,Thanks :-)
hardware,3brzcp,his_penis,2,Sun Jul 5 08:26:24 2015 UTC,Now kiss. ;)
hardware,3brzcp,melgibson666,1 point,Wed Jul 8 00:39:03 2015 UTC,I just bought a secondary battery for my Note4 and an external charger.
hardware,3brzcp,elevul,4,Thu Jul 2 23:09:09 2015 UTC,"If it were also waterproof, it would be basically the perfect phone for me. 720p is fine, though 5.5"" is a bit larger than ideal for that resolution. Going all the way up to 1080p would be a clear case of diminishing returns. It's not really any larger than my current phone (Moto X first-gen) when it's in its case. 10Ah is about the total battery capacity I carry with me on my weekend trips anyways, and getting it in one package would be a worthwhile convenience."
hardware,3brzcp,wtallis,0,Wed Jul 1 19:28:29 2015 UTC,"The speaker holes scream ""Not waterproof"" to me."
hardware,3brzcp,MINIMAN10000,1 point,Thu Jul 2 02:02:28 2015 UTC,"Also the exposed charging port.     But yeah, a super high capacity (1 week charge) waterproof phone with ~4.5"", 720p screen would be my dream phone.  A slightly thicker Z3 compact would be perfect (I have the tablet version of the Z3C already)."
hardware,3brzcp,DEADB33F,1 point,Thu Jul 2 04:42:27 2015 UTC,Droid turbo ia good for me and I watch hours of video from youtube frequently.
hardware,3brzcp,JarJarBanksy,1 point,Thu Jul 2 00:36:49 2015 UTC,5.5 inch is a bit large but so long as it isn't 6 or above it's fine with me. 720p would prefer 1080p but whatever it's a battery phone it works just fine. 16 gb is pretty standard. 2 gb ram should be plenty for mobile for the phones life I imagine. My concern is looking into the CPU they used all the articles stated the mediatek had poor power efficiency and high drain whether the network is idle or not. As well as all the stories I hear about chinese phones having extremely high battery drain. I feel I need someone to actually create a benchmark of the battery life and compare it to top phones battery life like the one plus which if you only include 5.X inch phones excludes the 6.1 inch huawei mate 2 making it the longest battery life of the 5 inch phones Chart. With the One Plus Two coming out soon it will have to compete with that phone on battery life.
hardware,3brzcp,MINIMAN10000,1 point,Thu Jul 2 01:16:16 2015 UTC,If only it ran Windows Phone...
hardware,3brzcp,Bounty1Berry,10,Thu Jul 2 02:39:29 2015 UTC,"5.5"" 720p 16gb slow cpu? 2gb ram  This is a battery pack with a low end phone smart phone attached.   Eh."
hardware,3brzcp,Clob,15,Wed Jul 1 20:58:23 2015 UTC,Low specs mean even better battery life.
hardware,3brzcp,Yearlaren,3,Wed Jul 1 22:14:34 2015 UTC,"If you're kind of nomadic but still want the functionality of a smart phone it's really not a bad value proposition, and more importantly one that hasn't really been supplied to the market yet. I still carry my 5210 around with me on treks and rides because I know a single charge carries me through the entire trip. If I could do this with a phone that then allows me to run Strava, digital maps and a camera, I'd be tempted."
hardware,3brzcp,IC_Pandemonium,1 point,Thu Jul 2 01:14:32 2015 UTC,720 and 2gb ram isn't really low end mid range imo
hardware,3brzcp,dylan522p,1 point,Thu Jul 2 03:54:23 2015 UTC,"Who are these people who need supercomputers in their phones?   I just need it to make calls, instant messaging and Web browsing.   I'm not trying to play intensive games that's what my tablet does."
hardware,3brzcp,TeutorixAleria,2,Fri Jul 3 12:36:21 2015 UTC,Good thing there is grass to compare the size.
hardware,3brzcp,darrenphillipjones,1 point,Thu Jul 2 00:44:24 2015 UTC,I wonder how much it weighs.
hardware,3brzcp,IByrdl,1 point,Wed Jul 1 22:00:56 2015 UTC,I feel like you should be using units of Ah when dealing with a battery that large.
hardware,3brzcp,SeaJayCJ,1 point,Thu Jul 2 01:59:39 2015 UTC,Needs banana for scale!
hardware,3brzcp,Fatigue-Error,1 point,Thu Jul 2 03:11:06 2015 UTC,"Sounded OK until I read the screensize. Can I just get something decent below 5 inches with good specs and Android?  My M8 is too large, but I love pretty much everything else about it."
hardware,3brzcp,AttackOfTheThumbs,1 point,Sun Jul 5 03:30:56 2015 UTC,"After seeing the injuries people sustain from those freak battery accidents, i cant help but think that if it happens to anyone with this phone they're going to die."
hardware,3brzcp,sh1dLOng,4,Wed Jul 1 20:00:55 2015 UTC,"Which freak accidents in particular?  Laptops have always used high watt hour (Wh) battery packs. Tablet batteries are almost approaching laptop range. If thr fact that our cars contain gallons of highly flammable gasoline doesn't concern many, neither should 40Wh batteries."
hardware,3brzcp,amfjani,1 point,Wed Jul 1 23:36:27 2015 UTC,I think it typically happens to phones (although very rarely) because they are stored in areas where they aren't allowed to dissipate any heat and/or the battery can easily be punctured/damaged
hardware,3brzcp,sh1dLOng,2,Thu Jul 2 01:28:23 2015 UTC,"The last specific incident I read about was the Sony laptop battery recall. But those were due to severe manufacturing defects, not environmental factors. Lithium-ion cells can fail with flames, but that's a very unlikely situation. Lithium colbalt batteries are more susceptible to this failure mode compared with lithium iron phosphate but it also allows for higher specific energy. The safety circuitry in battery packs and their chargers protect us. Storing larger amounts of energy is inherently riskier, but that's what safety design and testing is for."
hardware,3brzcp,amfjani,3,Thu Jul 2 05:37:11 2015 UTC,"On the other hand, by having a battery that's actually big enough, this phone won't have to stress the battery and may have a lower risk of anything going catastrophically wrong in the first place."
hardware,3brzcp,wtallis,1 point,Wed Jul 1 21:49:19 2015 UTC,Yeah supposedly it will have two separate cells which it will use independently to better control heat dissipation.
hardware,3brzcp,Ubel,1 point,Wed Jul 1 23:19:46 2015 UTC,"However this does also mean that there's twice as much chance of something going wrong due to a manufacturing defect or something. Still, better than having one larger cell if it does go wrong."
hardware,3brzcp,Anon123212321,1 point,Wed Jul 1 23:34:29 2015 UTC,Although I would like to point out the large cells in airplanes caused significant problems while Tesla uses a ton of small batteries and there are no problems that I know of. But I would prefer a single large battery.
hardware,3brzcp,MINIMAN10000,-5,Thu Jul 2 01:20:25 2015 UTC,"The only scenario I can think of where something like that is practical, is if you go hiking and don't have access to a charger for several days. Under any other circumstances, a smaller battery with higher phone specks would serve you better."
hardware,3brzcp,Sylanthra,2,Wed Jul 1 18:31:10 2015 UTC,"I disagree. My smaller phone with higher specs does not serve me well at all.   Good CPU doesn't mean much when I have to keep it in power saving mode all the damn time, good screen doesn't mean anything when I have to keep the brightness down to save battery. The camera's great! ...But if I use it, it chews through both battery life and the pathetic storage capacity.  All that makes it a bit pointless. The phones are great, until you go out somewhere away from a charger, attempt to utilize all the bells and whistles it has, and end up with a dead battery. It defeats the point because in the end you try not to use the phone too much due to battery anxiety."
hardware,3brzcp,musef1,1 point,Wed Jul 1 21:05:42 2015 UTC,Just get a battery pack. This phone is just as big as one.
hardware,3brzcp,X2isHere,1 point,Wed Jul 1 21:54:14 2015 UTC,"A problem with that for me is that I don't like large phones. I'm planning on getting a portable battery packs in the near future for things like days trips out, but I'm also hoping to upgrade to a Z3 compact (or z3+/z4 compact) soon as apparently they have good battery life.  Edit: My first sentence in this comment is a bit contradictory to the first comment I made. What I meant is I'd prefer a phone with lower specs for better battery life and would take a thicker phone in depth, I don't like larger phones in terms of length and width."
hardware,3brzcp,musef1,1 point,Wed Jul 1 22:13:58 2015 UTC,"I use my tablet for email and internet on the go so my phone is a small Nokia thing I bought for £4 (new).  Having a three week battery life is awesome, I can go weeks without charging and sometimes even forget that the thing needs topping up from time to time.  I  can't even imagine going back to having to remember to put my phone on charge one or more times a day."
hardware,3brzcp,DEADB33F,1 point,Thu Jul 2 04:29:03 2015 UTC,"I currently use an iPhone 5s. I am perfectly happy with its speed, but battery life is an issue. That phone has a faster cpu (0.2ghz faster) and double the ram. If I enjoyed android as much as I do iOS, I would be all over that phone. My 8500 mAh battery pack will charge my iPhone four or more times. I would love to have that battery built in. Edit: mAh, not mWh"
hardware,3brzcp,heavenly_blade101,9,Mon Jul 6 13:05:53 2015 UTC,I thought the exact opposite. Matter of taste.
hardware,3brzcp,Mr_s3rius,1 point,Wed Jul 1 19:56:06 2015 UTC,"Me too. It kind of looks like it has flavours of 90s' cool, with a modern smartphone format."
hardware,3bqkwt,bondiben,80,Wed Jul 1 10:14:36 2015 UTC,"Comparing to Intel's pricing is just silly, it should be compared to the same shop's pricing.  Preorder pricing for the 6700K is $393.53, they sell the 4790K for $394.82."
hardware,3bqkwt,bentan77,10,Wed Jul 1 11:35:05 2015 UTC,Then those prices are actually quite promising.
hardware,3bqkwt,schneemensch,20,Wed Jul 1 14:42:40 2015 UTC,"All it says is that Intel's holding prices constant, i.e. exactly what they've been doing for years."
hardware,3bqkwt,Exist50,6,Wed Jul 1 17:04:50 2015 UTC,Im happy if the prices stay the same.
hardware,3bqkwt,schneemensch,5,Wed Jul 1 17:16:58 2015 UTC,What could be argued though is that IF AMD was competing in the high-end CPU market the prices could come down. Or at least more cpu power per dollar. Whether that would happen is up in the air but it is a possibility.  As it is now Intel pretty much gets to dictate the prices for high end cpus
hardware,3bqkwt,kuuttis,5,Wed Jul 1 19:11:20 2015 UTC,Of course Intel would have to adjust to AMD prices. But if the prices stay the same we are getting more CPU power per dollar. And that a good thing because like you said Intel could charge almost whatever they want.
hardware,3bqkwt,schneemensch,29,Wed Jul 1 19:36:56 2015 UTC,Speculations on preorder price by some random webshop. Come on...
hardware,3bqkwt,o_x,-12,Wed Jul 1 11:21:39 2015 UTC,In line with these prices too: http://wccftech.com/intel-broadwell-core-i75775c-core-i55675c-skylake-core-i76700-core-i56500-listed-preorders/
hardware,3bqkwt,darrenphillipjones,17,Wed Jul 1 11:50:31 2015 UTC,"So cpu-world reads the wccftech website and makes preorder prices. See the issue here? 60 sites could have the same price, it just means it starts with one bad seed. If you've ever worked with marketing, prices can change the day of release, let alone weeks in advance.  I'm sure Intel laughs at stuff like this because they probably haven't even set a firm price. Electronic market changes way too fast. They never set prices in stone this far out.  Let alone ""2 months ago by Hassan Mujtaba."" So 3 months advance on a firm price for a cpu? C'mon man. You know better than that if you've been around any product release ever."
hardware,3bqkwt,Teethpasta,1 point,Wed Jul 1 12:45:08 2015 UTC,Intel never really changes their prices
hardware,3bqkwt,Tea_Bag,2,Wed Jul 1 16:07:56 2015 UTC,Ah the i7K has been sneaking up about $10 each gen since Sandy tbf
hardware,3bqkwt,darrenphillipjones,0,Wed Jul 1 20:26:07 2015 UTC,So you work for intel's marketing department? I see no evidence anywhere of Intel setting a price months in advance and not changing it.
hardware,3bqkwt,Teethpasta,1 point,Wed Jul 1 16:33:44 2015 UTC,Haswell CPUs have been the same price for the entirety of their existence
hardware,3bqkwt,darrenphillipjones,1 point,Wed Jul 1 21:26:20 2015 UTC,Once they were released. Skylake isn't released.
hardware,3bqkwt,longlive289s,10,Wed Jul 1 21:39:55 2015 UTC,Just had a buddy who works at a disti look up pricing on a couple of these  BX80662I56600K - $234.00  BX80662I76700K - $335.50  Also mentioned embargo is on till 8/5
hardware,3bqkwt,pertoosis,6,Wed Jul 1 22:24:48 2015 UTC,I still run a 750.
hardware,3bqkwt,DeezRobotNuts,2,Thu Jul 2 01:49:06 2015 UTC,"Me too, been 6ish years. It's been running 6-10 hours a day everyday."
hardware,3bqkwt,Metsuki,1 point,Mon Jul 6 02:39:22 2015 UTC,"Same, definitely upgrading this time."
hardware,3bqkwt,Techdestro,16,Thu Jul 2 19:52:09 2015 UTC,Hope they are worth the higher price. Zen! We need you
hardware,3bqkwt,III-V,19,Wed Jul 1 11:07:46 2015 UTC,"They're not a higher price. The author of the article is just ignorant.   Plus, pre-order pricing is always inflated."
hardware,3bqkwt,Techdestro,1 point,Wed Jul 1 13:37:29 2015 UTC,"That may be true, here's to hoping!"
hardware,3bqkwt,Klorel,-6,Wed Jul 1 13:57:28 2015 UTC,"zen will compete with the next tock. not with skylake. late 2016, sadly not early 2016."
hardware,3bqkwt,Sapiogram,11,Wed Jul 1 12:04:44 2015 UTC,"The next tock is not out for a looong time. By Intel's schedule it will be out in H2 2017, but with the recent delays in their 10nm process, it might be even later. Zen is scheduled for Q4 2016, giving it 6-12 months together with Skylake if both companies' plans go as intended. (That's a big if on both sides, of course)"
hardware,3bqkwt,bphase,4,Wed Jul 1 12:25:14 2015 UTC,"Next tick actually, which is Cannonlake. Though that seems to be delayed to Q1/17 I think, so it'll probably be Zen & Skylake or its refresh Koby(?) Lake battling it out for a bit."
hardware,3bqkwt,Exist50,1 point,Wed Jul 1 17:23:20 2015 UTC,Did AMD ever confirm Q4? I heard some talk about Q2.
hardware,3bqkwt,dontnation,1 point,Wed Jul 1 15:50:48 2015 UTC,"With the 10nm delay Intel is now planning for another tick release between skylake and cannonlake. THAT will be AMD's competition.  Wish I could support Zen, but I've held out as long as I could for something attractive from AMD. Going blue at the end of the year."
hardware,3bqkwt,ndr2h,7,Mon Jul 6 21:20:20 2015 UTC,"Before we all grab our pitchforks the article says : ""It is important to remember that that the market prices are usually higher than the official ones, and that pre-order prices may go down a bit once the processors are released"""
hardware,3bqkwt,Yearlaren,2,Wed Jul 1 11:35:03 2015 UTC,They're only releasing i5 and i7 Skylake CPUs?
hardware,3bqkwt,FlyingFortress17,3,Wed Jul 1 22:09:43 2015 UTC,I'm guessing the i3s will come a bit later.
hardware,3bqkwt,dylan522p,5,Wed Jul 1 22:16:46 2015 UTC,They always do. I3 a quarter later then pentiums n celeron the quarter after that
hardware,3bqkwt,marrio91,1 point,Wed Jul 1 22:18:03 2015 UTC,Except with Broadwell where everything went to fuck.
hardware,3bqkwt,W1k0,2,Tue Jul 14 09:31:11 2015 UTC,Can someone explain the difference between a 6600k and my 4690k?
hardware,3bqkwt,schneemensch,3,Wed Jul 1 21:35:36 2015 UTC,You don't have to worry the difference in performance will not be more than 10%.   But it comes with a new chipset and that will have exciting things like nvm ssd and usb3.1 support.
hardware,3bqkwt,MaloWlol,3,Thu Jul 2 06:12:40 2015 UTC,"6600k uses 1151 socket instead of the 1150 for your current 4690k. With that comes new chipsets in the 100-series like the Z170, I'm not quite sure of the benefits of the new chipsets but probably the usual in form of more USB 3.0's and Sata 6's, more PCI-E lanes for M2 connections etc. A big thing though is that these chipsets supports both DDR4 and DDR3, meaning depending on which motherboard you buy you'll be able to use either. The benefits of DDR4 are very slim though for most usage.  Apart from that we can't know any other benefits until we read some reviews and benchmarks of the 6600k, which should be due in next month I think, so we don't know how much better they perform per clock than Haswell. But we can assume that it'll be the usual 5-10%. IIRC the actual clock frequency of the 6600k were very similar to the 4690k, so no benefits there at least."
hardware,3bqkwt,dontnation,1 point,Thu Jul 2 08:39:31 2015 UTC,It only supports DDR3L (likely to appease laptop makers).  I don't think desktop mobo makers will be able to leverage that into DDR3 support. But maybe.
hardware,3bqkwt,MaloWlol,1 point,Mon Jul 6 21:24:39 2015 UTC,You're correct. Is there any difference between DDR3 and DDR3L except for the voltage? Do they have the same notch and physical form?
hardware,3bqkwt,dontnation,1 point,Mon Jul 6 23:18:14 2015 UTC,I take it back. I was under the assumption that DDR3L was only made in So-DIMM formfactor. But apparently desktop modules do exist.  The only difference is the voltage.  So if you have 1.35V ddr3l then they may have a skylake motherboard that can handle it.  Also considering the only difference is voltage I wouldn't be surprised to see Mobo makers put out a board that supports regular DDR3.  It just won't officially be supported by Intel.
hardware,3bqkwt,e6600,1 point,Mon Jul 6 23:36:40 2015 UTC,i paid $525 for my 5820k/x99 ud4 at launch at microcenter. hopefully they do the same for the 6700k.  maybe around $450 with a z170 mobo
hardware,3bqkwt,RhombusAcheron,2,Wed Jul 1 13:03:00 2015 UTC,5820 is haswell-E right? My 3820 was pretty pricey when it launched compared to the main sandy bridge lineup.
hardware,3bqkwt,nekura_,1 point,Wed Jul 1 14:34:36 2015 UTC,I bought my 3820 from Microcenter. They have stupid all-the-time door buster deals on CPUs. I think it was over $100 cheaper than anything online at the time.
hardware,3bqkwt,Jakeattack77,1 point,Wed Jul 1 15:04:09 2015 UTC,"""stupid"" lol i got my 4790k from there for 250, even lower than what they have it for now at 270. plus a mobo deal."
hardware,3bqkwt,Teethpasta,-1,Wed Jul 1 20:53:24 2015 UTC,You need 40 pcie lanes that bad?
hardware,3bqkwt,Jakeattack77,1 point,Thu Jul 2 14:15:50 2015 UTC,"Wait for the 6700k im so confused. it has a lower turbo boost, yet its tdp is higher, despite it being 14nm. other than the ddr4 support, whats the difference going to be??"
hardware,3bqkwt,MaloWlol,2,Wed Jul 1 20:54:31 2015 UTC,"New socket and new chipsets with that, support for DDR4, probably a 5-10% IPC increase. The increase in TDP is probably partly from the improved graphics in it. Since the Intel stock-cooler no longer will be shipped with the k-CPU's in Skylake the TDP has also probably been bumped a bit to allow the CPU to more often push to it's turbo-clocks, something that wasn't possible with the old stock coolers, but Intel can safely assume that people will be using coolers with better cooling capabilities now."
hardware,3bqkwt,Jakeattack77,2,Thu Jul 2 08:46:53 2015 UTC,"kinda dumb they have improved graphics for an unlocked card, which will not be using its graphics much.   However, I think i have a theory for why they do this. I believe that perhaps many of the models are the same chips, just binned higher. so if it preforms well then its an i7, and lower preforming chips are i5s etc. It would make sense because yields some times suck, so if they get imperfect cards they sell them as lower performance ones. so this means they coudnt just have a redesigned i7 with no igpu, it would have to be a whole separate line like the Xeon."
hardware,3bqkwt,rePAN6517,-1,Thu Jul 2 18:17:31 2015 UTC,"Well, no competition from AMD means Intel can charge more.  It sucks but it's reality.  I really hope AMD returns to their old Athlon glory days with Zen."
hardware,3bqkwt,III-V,16,Wed Jul 1 13:06:07 2015 UTC,"The ""reality"" is that you took the title at face value, and didn't do any critical thinking.   Seriously, it's like this with every CPU launch ever. Pre-order prices are inflated, people go nuts (because they pay zero attention), and launch prices are actually in line with where they should be.   Shame on the author for spreading misinformation, and shame on you for believing it."
hardware,3bqkwt,rhart6,1 point,Wed Jul 1 13:45:52 2015 UTC,Anyone have a prediction on what will happen to the z97 prices? I'm holding of on building till I have a little more money and am wondering what kind of price drops would be reasonable to expect (if any) when the new chips come out.
hardware,3bqkwt,LFKhael,3,Wed Jul 1 14:08:28 2015 UTC,"Intel doesn't do price drops on their parts. Retailers are going to keep that stock around at the same price while that inventory slowly dwindles as people decide to upgrade or replace parts. The only time you're going to find Intel parts at a significant discount is when they're used, or if you're near a Fry's/Microcenter."
hardware,3bqkwt,rockycrab,2,Wed Jul 1 15:44:20 2015 UTC,"The prices will stay the same or go up. In fact, new 4670K/4770K CPUs sold by Newegg are currently more expensive than new 4690K/4790K CPUs by the same seller.  If you're only talking about motherboard prices, then yes it's possible but it wouldn't be that much less than a Z170."
hardware,3bqkwt,schneemensch,1 point,Wed Jul 1 20:34:40 2015 UTC,"I do not know about the motherboard prices, but I heard Intel prices stay almost the same when a new generation comes out. I would not expect a big price drop for Haswell."
hardware,3bqkwt,fuzzycuffs,-8,Wed Jul 1 15:20:31 2015 UTC,Prices go up and Intel won't give you a heat sink and fan?  They're really milking i7 buyers.   (And I'm going to be one of them)
hardware,3bqkwt,III-V,7,Wed Jul 1 12:40:00 2015 UTC,"They're not going up. Read the top comment, for one. Two, you shouldn't be taking titles at face value."
hardware,3bqkwt,DoTheEvoIution,2,Wed Jul 1 13:40:51 2015 UTC,"haswell already went up and stayed up compared to the ivy bridge  and reports are, not just this one with the pre-order prices, that they are going up again with skylake AND they are not including stock heatsink with the ""k"" models   inb4 durr hurr dont need it   well some do, and its just less value you get"
hardware,3bqkwt,RhombusAcheron,-2,Wed Jul 1 14:02:08 2015 UTC,"The people buying k processors tend to be doing it for overclocking, they're not using the stock heatsink anyway."
hardware,3bqkwt,epictuna,2,Wed Jul 1 14:36:08 2015 UTC,"Still a reduction in the value of the product. Besides, many people do use them (a) to test the system (b) because they do work, just not as well (c) because they all use the same mounting and so can be backup coolers"
hardware,3bqkwt,doneandtired2014,-2,Wed Jul 1 15:36:25 2015 UTC,"No one buys K suffix processors and uses the stock heatsink long term; the people who do use stock Intel heatsinks do so only as stop gap solutions until their aftermarket air and water cooling solutions arrive/are bought. Those stock coolers are worth low double digit hours at best.  For the rest of us, a stock cooler on a K processor is just one more thing to clutter the house with."
hardware,3bqkwt,epictuna,4,Wed Jul 1 16:32:54 2015 UTC,Still a reduction in the value of the product   It's not like they dropped the price as a result of not including the cooler
hardware,3bqkwt,doneandtired2014,1 point,Wed Jul 1 18:07:16 2015 UTC,"They stand to save millions by not including something 95% of the enthusiast market does not use or even want, money that could be used for more R&D, marketing, etc. The money Intel saved on one unit wouldn't even be enough to buy you a bottle of Pepsi at a 7-11 if passed onto you at the time of purchase. Save millions on not making and bundling something the overwhelming majority of that demographic has said, ""We don't care if a stock cooler is included because we NEVER use them"" and reinvest it into something beneficial long term....or lower the price so someone can buy a drink to go with their CPU. That isn't a hard choice to make. If you're th that hung up over an included OEM heatsink, you're buying the wrong SKU from the get go."
hardware,3bqkwt,tenthkarma,1 point,Wed Jul 1 19:44:12 2015 UTC,The K processors are a significantly higher TDP and clock rate compared to the non-K processors. They are not just an overclocker's chip this time around.
hardware,3bqkwt,reynardtfox,0,Wed Jul 1 17:23:31 2015 UTC,I thought the no heatsink or fan were only for the enthusiast line which isn't being released just yet?
hardware,3bqkwt,skilliard4,-6,Wed Jul 1 13:28:20 2015 UTC,"Man, if this is true, they're really jacking up the prices on their high end CPUs due to no competition."
hardware,3brmi0,bdt13334,3,Wed Jul 1 16:18:56 2015 UTC,Am I crazy for thinking about upgrading my GTX 980s to 980 Tis? One 980 TI non-reference like this or the MSI or Gigabyte ones are like 4-5 frames less than my 980s.. crazy.
hardware,3brmi0,DrexelDragon93,7,Wed Jul 1 20:02:25 2015 UTC,"An over clocked 980ti performs almost aswell as two 980s, which is pretty impressive. You would be crazy to buy one tho lol"
hardware,3brmi0,hish911,6,Wed Jul 1 20:04:24 2015 UTC,"I'm thinking of buying two and selling my 980s.. The other reason for doing this is after this summer I won't have the money to upgrade my GPUs again for awhile and by the time pascal comes out I won't really have much value to sell my 980s for. I'm also going to moving alot over the next year (school, military) so it would help if I just had two GPUs at max power without watercooling (which I have now).  Idk.. maybe I'm crazy"
hardware,3brmi0,DrexelDragon93,4,Wed Jul 1 20:12:39 2015 UTC,"Nah you can probably sell ur 980s for a good price, if you got the funds go for it"
hardware,3brmi0,hish911,4,Wed Jul 1 20:37:37 2015 UTC,"I do, I hope I can get around $1000 for my 980s and their waterblocks ($125 each) and then two 980 tis would be like $1400."
hardware,3brmi0,DrexelDragon93,2,Wed Jul 1 20:40:11 2015 UTC,"It's not 4-5 less, more like 10-20 less.   Do you have reference 980s? If not, you could OC them. They might not OC as well considering they're in SLI and there is more heat though."
hardware,3brmi0,DrexelDragon93,2,Wed Jul 1 20:06:27 2015 UTC,I have ref 980s and they are overclocked to 1500core and 7900 mem.  They are watercooled too lol
hardware,3brmi0,continous,3,Wed Jul 1 20:10:53 2015 UTC,Then yes you are crazy lol
hardware,3brmi0,hish911,3,Wed Jul 1 21:31:20 2015 UTC,"Not necessarily, if he doesn't plan on upgrading when pascal comes around, as he said, then it would be in his best interest to obtain the best performing card(s) on the market while he has the cash."
hardware,3brmi0,deltaak,1 point,Thu Jul 2 07:49:14 2015 UTC,Does anyone know when the msi 980ti or g1 980ti will be available again? I can't find them in stock
hardware,3brmi0,hish911,1 point,Wed Jul 1 19:57:09 2015 UTC,http://www.newegg.com/Product/Product.aspx?Item=N82E16814125787
hardware,3brmi0,deltaak,1 point,Wed Jul 1 19:59:20 2015 UTC,"Nice last time I checked they were out of stock, how does the g1 cooler compare to msi?"
hardware,3brmi0,ShoutingDani,1 point,Wed Jul 1 20:02:52 2015 UTC,"I don't know man, I'm sure there are plenty of comparisons out there."
hardware,3brmi0,melgibson666,1 point,Wed Jul 1 20:45:56 2015 UTC,"tl:dr is msi is more quiet, g1 seems to be overlocking better??(in the review i read...keep silicon lottery in mind, the reviewer might have been lucky with his G1/unlucky with his msi)."
hardware,3brmi0,hish911,1 point,Thu Jul 2 05:07:35 2015 UTC,I just ordered my msi gtx 980 ti today. But it sold out in like 20 minutes.
hardware,3brmi0,melgibson666,1 point,Wed Jul 1 20:02:09 2015 UTC,From newegg?
hardware,3brmi0,eXXaXion,1 point,Wed Jul 1 20:04:51 2015 UTC,Yes. Ordered around 5 am I think. They were gone by 5:30am PDT.
hardware,3bqnot,reveil,72,Wed Jul 1 10:54:25 2015 UTC,"Guru3d is getting different results:   http://www.guru3d.com/articles_pages/asus_radeon_r9_380_strix_review,5.html  http://www.guru3d.com/articles_pages/msi_geforce_gtx_960_gaming_oc_review,7.html  As you can see in their test the actual power consumption of the cards comes very close to their TDP's. And I tend to trust Guru3d more than some random youtube video."
hardware,3bqnot,MaloWlol,5,Wed Jul 1 14:40:20 2015 UTC,"Yep, all the reviews I've seen contradicts the title, its somewhere between +38-95w.  +59w: http://www.gamersnexus.net/hwreviews/1984-amd-r9-390-380-benchmark-review/Page-2  +40-50w: http://www.techspot.com/review/1019-radeon-r9-390x-390-380/page7.html  +95w: http://www.overclockersclub.com/reviews/msi_r9_380_gaming_2g/15.htm  +53-74w: http://www.ocaholic.co.uk/modules/smartsection/item.php?itemid=1630&amp;page=16  +38-53w: http://lanoc.org/review/video-cards/7097-powercolor-pcs-r9-380?showall=&amp;start=7  +49-64w: http://lanoc.org/review/video-cards/7096-msi-r9-380-gaming-2g?showall=&amp;start=7  +76w: http://www.eteknix.com/sapphire-nitro-r9-380-4gb-graphics-card-review/14/"
hardware,3bqnot,Sayburirum,5,Thu Jul 2 06:43:42 2015 UTC,"Agreed, thanks for the links!"
hardware,3bqnot,Potss,43,Wed Jul 1 18:35:41 2015 UTC,"Techpowerup would seem to disagree strongly  http://www.techpowerup.com/reviews/ASUS/GTX_960_STRIX_OC/27.html  http://www.techpowerup.com/reviews/Sapphire/R9_285_Dual-X_OC/23.html  As would Guru3d  http://www.guru3d.com/articles_pages/asus_radeon_r9_380_strix_review,5.html  http://www.guru3d.com/articles_pages/msi_geforce_gtx_960_gaming_oc_review,7.html"
hardware,3bqnot,makar1,10,Wed Jul 1 14:42:08 2015 UTC,more comparisons-  http://www.hardocp.com/article/2015/03/23/asus_strix_gtx_960_directcu_ii_oc_video_card_review/9  http://www.eteknix.com/sapphire-nitro-r9-380-4gb-graphics-card-review/14/  http://media.gamersnexus.net/images/media/2015/amd/amd-r93-watts.png
hardware,3bqnot,keeif,14,Wed Jul 1 15:19:43 2015 UTC,TDP does not equal Power Use. Don't use power viruses like Furmark.
hardware,3bqnot,hans_ober,0,Wed Jul 1 14:40:24 2015 UTC,"Well, the listed number might not, since it's just a reference for usable heatsinks...  Realistically though, the amount of heat output by a chip should exactly equal the power consumption, simply due to conservation of energy.   Besides the slight loss from performing the work of pushing electrons through the chip, any electrical energy that goes in should match the heat energy that comes out."
hardware,3bqnot,random_guy12,31,Thu Jul 2 23:48:22 2015 UTC,"Testing power consumption with Furmark isn't indicative of real world use. That program is a torture test similar to IBT or Prime95. Furmark can give you a decent sense of the maximum power draw, but that's because it prevents GPUs from using their power management schemes. So unless you like looking at fuzzy donuts all day, it's a pretty meaningless test."
hardware,3bqnot,AlchemicalDuckk,28,Wed Jul 1 12:25:15 2015 UTC,"Furmark does not prevent the cards from doing their normal power management schemes.   It's an unrealistic test because it attempts to separately load every aspect of the card to the max simultaneously, while real graphics workloads are typically only max out one thing at a time."
hardware,3bqnot,Tuna-Fish2,12,Wed Jul 1 12:39:24 2015 UTC,Usually furmark was considered a poor  tool because the driver would throttle the card once it did detect the application. I do not know however whether this still holds true.
hardware,3bqnot,deusextelevision,4,Wed Jul 1 13:55:45 2015 UTC,It is still true. Nvidia and AMD drivers autodetect if Furmark or a variant is being ran and throttles the card so you don't kill it.
hardware,3bqnot,Kinaestheticsz,7,Wed Jul 1 19:38:39 2015 UTC,"Exactly, loading everything on the GPU to the max means that the GPU can't use dynamic frequency or power gating to get consumption down."
hardware,3bqnot,AlchemicalDuckk,7,Wed Jul 1 12:45:06 2015 UTC,"Which, for those who do not know, is a huge reason why Maxwell and Fiji as as efficient as they are."
hardware,3bqnot,Exist50,9,Wed Jul 1 15:49:22 2015 UTC,"Well a 290 has a TDP of 300 watts, but the VRM on mine says it peaks around 240 watts. When overclocked, otherwise it refuses to go past 210.  Not the most accurate  measurement, but lines up to what reviewers have said."
hardware,3bqnot,jorgp2,18,Wed Jul 1 11:56:56 2015 UTC,"This is because, ""thermal design power (TDP), sometimes called thermal design point, is the maximum amount of heat generated by the CPU that the cooling system in a computer is required to dissipate in typical operation"". The amount of power that a card actually draws can vary based on other components such as the PSU. So when something says TDP then it's informing you of what sort of cooling solution you need to operate it nominally. It's a common mistake to think that TDP = power consumption because it's in Watts :D"
hardware,3bqnot,downeverythingvote_i,-1,Wed Jul 1 13:36:39 2015 UTC,"e: I'm wrong, see responses below :)  No, the card's power consumption is not influenced by the PSU and the TDP is equal to the maximum card power consumption because thermodynamics. A card operating at below TDP just means it not using all of its resources, typically because real-world loads will cause the card to bottleneck somewhere without stressing everything to 100 %."
hardware,3bqnot,PolyWit,8,Wed Jul 1 14:02:50 2015 UTC,"TDP was never intended to be the actual power consumption, and instead is a manufacturer's recommendation on what to expect on a card. Try running Furmark on a card. If TDP were indeed the max power consumption on a card, then Furmark wouldn't cause power usage to spike that high. Hell, even a reference 290X was drawing insane amounts of power. AMD is known for aiming their TDP high, while Nvidia typically aims low. Neither are wrong, but they are estimating usage scenarios.  And yes, a PSU can influence the power draw and TDP through a component just as easily as a motherboard can. Although voltage regulation is becoming better, factors such as ripple can generate excess heat and change the amount of power on to components."
hardware,3bqnot,greenlepricon,4,Wed Jul 1 14:55:58 2015 UTC,"Ah, fair enough, that actually solves some puzzlement I had over certain extremely high power consumption results. So the issue is that GPU designers have started quoting non-maximum ""typical"" values as the TDP, which makes it all a bit qualitative."
hardware,3bqnot,PolyWit,4,Wed Jul 1 15:30:29 2015 UTC,"Exactly! I don't think anyone other than the manufacturer knows exactly how TDP is determined, but it's usually a reasonable amount off from what the power consumption is. Another problem is that chips behave differently, and so TDP is calculated as an average based on where chips are binned.  Intel has even started using terms such as SDP (scenario design power). They are all marketing terms, and just give the manufacturers and OEMs something to go off of and also to advertise. To give your original post some credit though, Nvidia in particular is getting pretty good at applying TDP limits to its cards. Love it or hate it, I think TDP is much more indicative of maximum power draw than it used to be."
hardware,3bqnot,greenlepricon,1 point,Wed Jul 1 16:24:43 2015 UTC,"Intel stopped sdp, they were using it for a bit to try and make chips that were almost mobile seem mobile"
hardware,3bqnot,dylan522p,1 point,Wed Jul 1 21:46:11 2015 UTC,Ah I hadn't heard that they stopped using SDP. Good riddance if it's gone since it was practically an arbitrary metric.
hardware,3bqnot,greenlepricon,2,Wed Jul 1 21:50:59 2015 UTC,"Ehhhhhh, idk it seemed like it makes sense for mobile devices. It's like making the tdp but instead of full load it's what your typical workload is and what amount of heat is produced then."
hardware,3bqnot,dylan522p,1 point,Wed Jul 1 22:05:47 2015 UTC,"That's true, but the issue that I have is when metrics aren't specified very well. I get annoyed enough when AMD and Nvidia fudge their results, but I'm not as concerned since those parts go in my desktop, which is sufficiently cooled to begin with. For portable electronics, TDP is a must for me, and if SDP is published then I still want to know the TDP. Temperature is much more noticeable on my hand or lap than in a computer case. At the very least I would like to know what a ""typical"" workload means to Intel. If they published a percent CPU usage with a timeframe or anything useful like that, then I would be much more open to the idea."
hardware,3bqnot,greenlepricon,-2,Wed Jul 1 22:18:08 2015 UTC,"Nope. TDP just tells you the power that will be dissipated as heat.  Read a complicated article on thermal management of chips a while back. The TDP is power dissipated as heat. Two cards might use the same power, but due to efficiency / whatever reasons... one might dissipate more energy as heat, but the other uses that power (energy) to do useful work, and does not waste it as heat."
hardware,3bqnot,hans_ober,9,Wed Jul 1 14:43:26 2015 UTC,"If the energy is being consumed but not dissipated as heat, what form do you think it has as it leaves the GPU? It's not like the GPUs are supplying a large amount of power through the display cables, and they're certainly not giving off multiple watts of microwaves."
hardware,3bqnot,wtallis,1 point,Wed Jul 1 15:19:17 2015 UTC,Fans bruh
hardware,3bqnot,aquaknox,4,Wed Jul 1 15:40:02 2015 UTC,Fans moving away hot air?
hardware,3bqnot,wtallis,1 point,Wed Jul 1 15:54:17 2015 UTC,"Sure, but pushing air around is work, work requires energy. Most of the energy given off from a gpu is heat, but different fan setups could lead to more or less of the input energy being put out as rotational energy instead of just heat."
hardware,3bqnot,aquaknox,2,Wed Jul 1 16:40:05 2015 UTC,Which also is quickly converted to heat via friction in the air. And fans are only <1% of the power budget anyway.
hardware,3bqnot,bphase,1 point,Wed Jul 1 17:42:24 2015 UTC,"Yeah, but the fans on a GPU draw less than a watt each, so the amount of that energy that isn't heat by the time it leaves the case is negligible compared to the total power consumption of the card."
hardware,3bqnot,wtallis,1 point,Wed Jul 1 18:07:49 2015 UTC,"You're probably trolling, but remember EVGA? There was something about them having more efficient fans, which allows the GPU to use the power the fans would originally use.   :D"
hardware,3bqnot,hans_ober,0,Wed Jul 1 16:57:53 2015 UTC,"Energy gets converted from electrical to heat = waste. It probably comes down to how the chip manufacturer has designed their chip to reduce wastage of energy as heat.. make the energy supplied do useful work.. calculate polygons/numbers..   I'm no engineer, but the way I see it, it comes down to Physical and Architectural differences. Physical is basically the hardware issues: Transistors are leaky, VRMs and inefficient, and very importantly power gating...  Do not need a section of transistors? Turn them off. Do not need your transistors working @ high clocks? Slow them down! Transistors use too much power? Get better process tech. Use a certain type of transistors (high perf, high power) for certain sections of your chip which will be rarely needed, other parts can use lower pef, lower power. Arrange your layout so that you're not wasting cycles on error correction.  How fast you do this in response to load determines how much time you waste keeping them on (guzzling power).  Architectural: Find a way of making your chip do c=a+b; d=c+e in 1 step/ clock instead of keeping it running for 2 clocks. You've just halved power usage. Find a way to stop your chip from accessing memory every cycle, predicting what goes into the cache, you've saved power which would have been used o flush memory + wasted cycles keeping chip @ high perf.  These are probably some reasons. Now, the process node is the same (28nm), so you can count that out. There will be minute differences in the sub process/ layout which could make a difference though.  Kepler vs Maxwell. The hardware is probably the same, with a few improvements.  Perf/Watt?  Huge difference. Maxwell changed quite a few things under the hood. From larger cache to redesigned units; Nvidia probably found a way to do things in a more efficient manner."
hardware,3bqnot,hans_ober,2,Wed Jul 1 16:56:15 2015 UTC,"No, just no.  Power in = Power out.  Nvidia cards require less voltage for higher clocks, so they consume less power."
hardware,3bqnot,jorgp2,1 point,Wed Jul 1 17:32:48 2015 UTC,"You're correct; and refering to TDP.  I was talking about perf/watt in my post. Lower voltages, binning, power/frequency scaling and power gating might be better on Nvidia GPUs.  But, look at Maxwell. With perf/watt doubled, they managed to halve the power required to provide little more than the same performance (GTX 750 vs GTX 650)  My mistake actually, shouldn't have started about perf/watt in a TDP thread, but that also matters."
hardware,3bqnot,hans_ober,4,Thu Jul 2 02:28:50 2015 UTC,"Fans on a GPU typically draw less than a watt each, and some of that still gets converted to heat before it leaves the case. All told, the total power dissipated by a video card under stress test in forms other than heat (eg. sound, RF leakage, driving display connections, and exhaust momentum) is at most a few watts. For the typical benchmarking setup, completely ignoring all of that will not be the biggest source of error."
hardware,3bqnot,wtallis,4,Wed Jul 1 17:34:49 2015 UTC,"however the variance will be very very small, and the vast majority of the energy used (read: bascially all of it) will still be converted to heat even in the more ""efficient"" card.  but /u/PolyWit is missing that both amd and nvidia define tdp as ""maximum in typical use case."" that doesnt mean it's maximum possible.   NVIDIA’s definition of TDP is the maximum power draw in “real world applications”.   http://www.anandtech.com/show/4239/nvidias-geforce-gtx-590-duking-it-out-for-the-single-card-king   [AMD's definition] is the maximum power a processor can draw for a thermally significant period while running commercially useful software.   https://web.archive.org/web/20111007083713/http://support.amd.com/us/Processor_TechDocs/43374.pdf"
hardware,3bqnot,logged_n_2_say,0,Wed Jul 1 18:13:43 2015 UTC,"Yeah, but look at perf/watt figures. Maxwell comes out on top, Fiji is worse than Kepler iirc; Hawaii is...  You must be right about TDP; but perf/watt also matters and is interesting because both are on the same process node. Architecture is the only significant difference; transistor binning/ chip design might help, but it's not going to make a big difference."
hardware,3bqnot,hans_ober,1 point,Wed Jul 1 15:22:51 2015 UTC,"it's true. maxwell is damn near a miracle for performance/watt.  even fiji with the major power savings of hbm doesnt beat it.  nvidia does well with their massive r&d budget.  edit: but kepler gets beat by fiji (with hbm), from a pure gpu stance i'm not sure who would win."
hardware,3bqnot,logged_n_2_say,2,Wed Jul 1 16:32:16 2015 UTC,"HBM supposedly saved on GDDR5 power usage, which is something in the neighborhood of ~40W. Makes me wonder why AMD still can't get perf/watt up while using HBM. They saved power, but a guzzling gpu (core) is using it up. Need to under-clock  HBM to mimic GDDR5 bandwidth. Lose any performance? If not, AMD adopted HBM too early for it to make a difference, memory was NOT a bottleneck. Work on the GPU!"
hardware,3bqnot,hans_ober,2,Wed Jul 1 16:53:00 2015 UTC,"Need to under-clock HBM to mimic GDDR5 bandwidth. Lose any performance?   i'd love to see this.  one thing that's flabbergasting to me, is that the bugged catalyst that allowed overclocking the memory, showed massive improvements on 3dmark extreme by overclocking hbm.  cant wait for real world test, and an unlocked fiji with voltage control."
hardware,3bqnot,logged_n_2_say,2,Wed Jul 1 17:01:01 2015 UTC,Someone needs to release a modded BIOS.  Need to see memory controller load + real time memory bandwidth/latency.  There should be a way to measure power by isolating the power rails too.
hardware,3bqnot,hans_ober,3,Wed Jul 1 17:04:27 2015 UTC,"Interesting, don't suppose you have a source? If you input a certain amount of electrical energy, and get a certain amount of heat output, as far as I can see they must exactly equal each other. Where would the rest of the energy go? Electronics isn't my background at all so that's a genuine question, I'd be interested to know :)"
hardware,3bqnot,PolyWit,-2,Wed Jul 1 17:14:45 2015 UTC,"Spent half an hour searching for possible places where I read that, but I can't seem to find it.  Energy gets converted from electrical to heat = waste. It probably comes down to how the chip manufacturer has designed their chip to reduce wastage of energy as heat.. make the energy supplied do useful work.. calculate polygons/numbers..   I'm no engineer, but the way I see it, it comes down to Physical and Architectural differences. Physical is basically the hardware issues: Transistors are leaky, VRMs and inefficient, and very importantly power gating...  Do not need a section of transistors? Turn them off. Do not need your transistors working @ high clocks? Slow them down! Transistors use too much power? Get better process tech. Use a certain type of transistors (high perf, high power) for certain sections of your chip which will be rarely needed, other parts can use lower pef, lower power. Arrange your layout so that you're not wasting cycles on error correction.  How fast you do this in response to load determines how much time you waste keeping them on (guzzling power).  Architectural: Find a way of making your chip do c=a+b; d=c+e in 1 step/ clock instead of keeping it running for 2 clocks. You've just halved power usage. Find a way to stop your chip from accessing memory every cycle, predicting what goes into the cache, you've saved power which would have been used o flush memory + wasted cycles keeping chip @ high perf.  These are probably some reasons. Now, the process node is the same (28nm), so you can count that out. There will be minute differences in the sub process/ layout which could make a difference though.  Kepler vs Maxwell. The hardware is probably the same, with a few improvements.  Perf/Watt?  Huge difference. Maxwell changed quite a few things under the hood. From larger cache to redesigned units; Nvidia probably found a way to do things in a more efficient manner  EDIT: Adding. Clocking. Clocking your chip at a certain region will be more efficient in the perf/watt curve. Balance clocks vs number of units."
hardware,3bqnot,hans_ober,5,Wed Jul 1 15:57:58 2015 UTC,"So are you saying a 200W card could only output 100W of heat?  That is completely wrong. a 200W card outputs ~200W of heat. Always. It doesn't matter how ""efficient"" your card is, all it means is you get more done with the same power i.e. your performance/watt is higher.   This is a direct result from conservation of energy. You can not input 200W of power and only get 100W out. It must be converted into heat/kinetic/potential/etc. energy, and only heat applies in the  case of electronics, besides for a small amount for fans. Even the noise and movement of the fans eventually turns into heat as though."
hardware,3bqnot,bphase,2,Wed Jul 1 16:56:21 2015 UTC,"The analogy to a mechanical system works OK, but it breaks down when you consider that the output of, say, our engine is shaft work which is a measurable useful output. Our output shaft power is equal to our input energy (say, heat of combustion of diesel) minus inefficiencies; friction on the cylinders, gears, whatever.  With electronics, you have input electrical power and we know you have heat generation. Unless there is another place for the energy to go, all the electrical power has to be realised as heat. I'm not aware of an electronic equivalent of a spinning metal shaft turning a wheel or whatever; electronics just output signals. ""Ideally, energy goes towards your intended goal"" doesn't really cut it in thermodynamics, I'm pretty sure our intended goal/output of making frames has a minuscule energy cost to transmit that data to a monitor.  This is getting kinda theoretical and splitting hairs but hopefully a guy with a background in electronics will tell us what's up."
hardware,3bqnot,PolyWit,6,Wed Jul 1 17:12:25 2015 UTC,"If I were to design a space heater on the same amount of power, I could create a LOT more heat than what modern gpu's put out. As space heaters, even AMD cards are pretty ineffecient.   You're completely wrong there. GPUs are more expensive than space heaters, but for the same electrical input a GPU will dissipate at most a few watts less heat than a space heater."
hardware,3bqnot,wtallis,4,Wed Jul 1 17:58:09 2015 UTC,"Something doesn't add up for me. For a start, benchmarks of CPUs typically find that the measured electrical power consumption is at, or close to, their TDP. That says to me that the designers are, in fact, stating that the entire power input is realised as heat.  Secondly, if the energy is somehow consumed by the transistors in the process of flipping states and energising the memory, where does that energy go? It can't be destroyed, it has to go somewhere. The only ""somewhere"" that I can see is heat."
hardware,3bqnot,PolyWit,-2,Wed Jul 1 18:25:42 2015 UTC,"For Nvidia, listed tdp now basically equals average gaming power consumption."
hardware,3bqnot,Exist50,2,Wed Jul 1 18:48:05 2015 UTC,"got 290 trix here and a wattmetter on a phenom x4 (not a most power friendly cpu)   skyrim, bioshock infinite - ~250W BF4 - ~360W"
hardware,3bqnot,DoTheEvoIution,1 point,Wed Jul 1 19:01:11 2015 UTC,Are you overclocked?
hardware,3bqnot,jorgp2,1 point,Wed Jul 1 19:17:50 2015 UTC,nothing was not when I measured it
hardware,3bqnot,DoTheEvoIution,1 point,Wed Jul 1 19:16:50 2015 UTC,"Are those your values from the wall, or calculated for efficiency?"
hardware,3bqnot,bl1nds1ght,1 point,Wed Jul 1 19:59:21 2015 UTC,"wall, no efficiency calculated in, psu is seasonic X 650W"
hardware,3bqnot,DoTheEvoIution,1 point,Wed Jul 1 15:45:15 2015 UTC,"Is that GPU core power only, or including VRAM power?"
hardware,3bqnot,JeffroGymnast,1 point,Wed Jul 1 13:58:17 2015 UTC,I'm not sure but it might be both since it has only one VRM for both.
hardware,3bqnot,jorgp2,7,Wed Jul 1 17:38:04 2015 UTC,"For the record, TDP is a measure of heat output, not power draw. The two are often closely linked, as a higher TDP typically results from a bigger draw and more juice flowing through the fucking thing, but you shouldn't take the TDP numbers as an effective measure of power efficiency."
hardware,3bqnot,Oafah,5,Wed Jul 1 23:15:16 2015 UTC,The two are directly linked in everything that computes something. What else than heat should be generated instead?
hardware,3bqnot,NamenIos,4,Wed Jul 1 21:46:32 2015 UTC,"As I said, they are often closely linked, but they are not necessarily proportionate with one another. You can lower the TDP of a card without reducing its power draw."
hardware,3bqnot,Oafah,3,Wed Jul 1 23:14:57 2015 UTC,How?
hardware,3bqnot,bphase,5,Wed Jul 1 18:26:23 2015 UTC,"Arbitrarily changing the number, as TDP is not clearly defined. Otherwise there is no way (law of physics and such ...)."
hardware,3bqnot,NamenIos,2,Wed Jul 1 20:36:28 2015 UTC,"Largely, yes. That's why I said ""the two are not proportionate to one another"". TDP is not a measure of power draw. It's a target in heat output that a cooler should be designed to dissipate."
hardware,3bqnot,Oafah,1 point,Wed Jul 1 14:18:36 2015 UTC,"The wattage really doesnt seem to b e messured aprpiatly, but the 380 really seems to be the better performing card."
hardware,3bqnot,1st_veteran,1 point,Wed Jul 1 17:17:52 2015 UTC,MW Technology? You better check more reliable sources.
hardware,3bqnot,Sicarius_,1 point,Wed Jul 1 17:21:11 2015 UTC,"Youtube video has wildly differing results to every professional test  it's in favor of AMD though, I can get free internet points if I post this"
hardware,3bqnot,nawoanor,-18,Wed Jul 1 18:06:07 2015 UTC,Is Nvidia's famed Maxwell efficiency a bit exaggerated by misleading TDP figures?
hardware,3bqnot,bentan77,9,Wed Jul 1 18:15:27 2015 UTC,"They used Furmark (which isn't accurate for games) and they compare it to the a card that on average uses a fair bit more than a ""regular"" GTX 960"
hardware,3bqnot,fzed1199,2,Wed Jul 1 18:37:31 2015 UTC,Did you check their future mark test? 1080x1024.
hardware,3bqnot,fzed1199,3,Fri Jul 3 21:16:46 2015 UTC,Nvidia use average tdp whereas Amd uses max tdp
hardware,3bqnot,dylan522p,1 point,Wed Jul 1 13:09:59 2015 UTC,Tell that to Hawaii where people give it an unofficial of 290W
hardware,3bqnot,VanayadGaming,13,Wed Jul 1 19:09:09 2015 UTC,TDP doesn't mean power draw but maximum power the card is capable of taking in. Thermal Design Power. One can have 1kw TDP and the card could use only 100W ....
hardware,3bqnot,Ellimis,5,Wed Jul 1 11:04:39 2015 UTC,"No, it doesn't. https://en.wikipedia.org/wiki/Thermal_design_power"
hardware,3bqnot,III-V,14,Wed Jul 1 11:29:59 2015 UTC,"Unfortunately, each company has their own ""definition."" AMD, Intel and Nvidia all report/measure TDP differently."
hardware,3bqnot,VanayadGaming,-1,Wed Jul 1 16:39:56 2015 UTC,Close enough :P
hardware,3bqnot,Ellimis,6,Wed Jul 1 13:18:01 2015 UTC,"Unfortunately the misconception you stated is exactly the kind of things that leads to OP's question, based on totally false pretenses. It's this, nothing more or less:    the maximum amount of heat generated by the CPU (GPU in this case) that the cooling system in a computer is required to dissipate in typical operation."
hardware,3bqnot,VanayadGaming,-1,Thu Jul 2 04:41:52 2015 UTC,focus on maximum... TDP is stating the maximum.... not under normal load what wattage you will have :)
hardware,3bqnot,Ellimis,2,Wed Jul 1 11:15:37 2015 UTC,A synthetic benchmark taxes the GPU in ways that even max-settings gaming cannot.
hardware,3bqnot,coldpassion,4,Wed Jul 1 12:57:30 2015 UTC,"maaaaaan, nobody can understand it correctly... thank god someone else explains it..."
hardware,3bqnot,VanayadGaming,2,Wed Jul 1 13:34:47 2015 UTC,"Well, again.. TDP is misleading. Power under load is more relevant."
hardware,3bqnot,coldpassion,1 point,Wed Jul 1 13:01:51 2015 UTC,"of course, but some people care about the amount of power (heat) that a cooler must dissipate in order to keep a silicon chip within its operating temperatures.. so tdp is good to know.. it's just that we have to know what exactly it is :)"
hardware,3bqnot,Exist50,1 point,Wed Jul 1 13:15:17 2015 UTC,"Well no, tdp refers to the needs of the cooling solution, not necessarily the chip itself."
hardware,3bqnot,coldpassion,1 point,Wed Jul 1 13:56:20 2015 UTC,tdp is a value for the chip.. the cooler has to be made according to this..
hardware,3bqnot,Exist50,1 point,Wed Jul 1 15:22:58 2015 UTC,"Yes, but it's not a spec for the chip's max power consumption, just how much you should design the cooler to dissipate. With something like Core M, which needs to be thermally throttled to keep within its power envelope, this is an important distinction."
hardware,3bqnot,coldpassion,1 point,Wed Jul 1 11:36:00 2015 UTC,i never said anything about power consumption.. we don't disagree :)
hardware,3bqnot,Exist50,1 point,Wed Jul 1 13:02:17 2015 UTC,"Not trying to disagree, just pointing it out."
hardware,3bqnot,wulfgar_beornegar,1 point,Wed Jul 1 16:29:33 2015 UTC,"This is still a somewhat conservative assessment of the limit though, right? Lots of overclockers go over the TDP, some way more than others (chalk that up to the silicon lottery). So TDP is the manufacturer's way of saying, ""this is the limit. go over this, and you just might see some serious shit"". Right?"
hardware,3bqnot,VanayadGaming,2,Wed Jul 1 15:47:24 2015 UTC,Right.
hardware,3bqnot,olavk2,2,Wed Jul 1 16:32:26 2015 UTC,"One thing to note about TDP though is that it only counts for stock clocks, you start overclocking and the TDP doesnt apply anymore. TDP is basically so that OEMs can design coolers for processors(cpu and gpu)"
hardware,3bqnot,Exist50,1 point,Wed Jul 1 17:09:16 2015 UTC,"For Nvidia, tdp=average gaming power consumption, certainly not max power draw."
hardware,3bqnot,dexter311,1 point,Wed Jul 1 17:15:27 2015 UTC,"TDP doesn't have anything to do with power input. It is concerned with power OUTPUT, specifically thermal power, and is the maximum thermal loading which the card is expected to dissipate in normal operation."
hardware,3bqnot,hans_ober,-1,Wed Jul 1 17:17:59 2015 UTC,Nvidia's TDP's are low;  BUT  do a test where you measure performance & power.  You'll get perf/watt = efficiency. Maxwell cards tend to come out on top. Fiji on the other hand is another story. Hawaii? Don't even..
hardware,3bqgtp,redditedc,17,Wed Jul 1 09:13:26 2015 UTC,"It has been heavily criticized for not having all the heatpipes in contact with the GPU   I think this is a stupid criticism. You can't magically fit five heatpipes into a single GM200 footprint. What would people rather have Asus do, exactly? I'd rather have two heatpipes not touching than not have those heatpipes at all.     also not have anything cooling the memory chips.   This is a legitimate criticism. That bit of thermal tape is the difference between naked memory chips and ones that are well-cooled.    how did they miss it?!?   Beats me, honestly. Maybe the RAM doesn't get that hot when naked and just having the fans blowing on them under stock settings. Not good for overclocking, that's for sure."
hardware,3bqgtp,SeaJayCJ,16,Wed Jul 1 09:42:21 2015 UTC,"What would people rather have Asus do, exactly?   Use a heat-spreader like the competition?"
hardware,3bqgtp,makar1,11,Wed Jul 1 09:47:25 2015 UTC,"If it performs well, who cares? The heatpipes are made of copper already, and crushed up against each other. I don't think throwing a copper plate on top would do a hell of a lot.  The RAM thing is a legitimate concern, I doubt the heatpipe thing is.   This bullshit, on the other hand..."
hardware,3bqgtp,SeaJayCJ,7,Wed Jul 1 09:52:46 2015 UTC,"Actually, now that I think about it, isn't the entire point of heatpipes that they conduct heat significantly faster than copper alone? So putting a copper plate between would slow down heat transfer?"
hardware,3bqgtp,SeaJayCJ,8,Wed Jul 1 10:01:56 2015 UTC,"It impedes heat transfer to the pipes that would otherwise be directly touching, but should improve transfer to the ones that wouldn't."
hardware,3bqgtp,ijnokmp,2,Wed Jul 1 10:39:16 2015 UTC,"And when you consider that the G1 has an extra pipe, allowing more heat transfer taken away from the copper plate, combined with (further down the line) more surface area of those pipes in contact with a larger quantity of cooling fins.. it all makes sense.  Each G1 pipe not only seem to be in contact to more cooling fins (over their length), but overall they look well spaced away from each other, and seem to all fairly equally be arriving at the fins much more directly than the Asus cooler pipes.  Overall you would want to be sharing the heat out as quickly and equally as possible to every cooling fin right? -To maximise the effect of the cooling fans. Look at the Asus cooling pipes, and each cooling pipes job surrounded by however many cooling fins, it's like they've tried to balance out the workload of each pipe, but each pipe is transferring too different an amount of heat over too different a distance to a completely different amount of cooling fins...  Honestly gigabytes cooler is a work of art in comparison, and the others look bashed and botched up."
hardware,3bqgtp,Nixflyn,1 point,Wed Jul 1 14:29:28 2015 UTC,"Someone gets it! If I turned in this design (the Asus one) at work, I'd be fired. The heat distribution is just so lopsided."
hardware,3bqgtp,makar1,0,Thu Jul 2 10:18:35 2015 UTC,"Only perhaps 10% of the heatpipe is in any sort of contact with the die without a heat-spreader. With the heatspreader, it's 50% or more.  The heatpipes are clearly an issue based on Asus' poor performing Hawaii coolers."
hardware,3bqgtp,SeaJayCJ,2,Wed Jul 1 09:59:38 2015 UTC,"I thought that those Hawaii coolers sucked because they were recycled from the GTX 780, and thus designed for a GTX 780's TDP.   I think we should just wait for actual data before we pass any judgements. Will it suck? Maybe. If they neglected to properly cool the damn RAM, there's a fair chance they fucked up the actual cooler as well..."
hardware,3bqgtp,Kaldskryke,2,Wed Jul 1 10:38:03 2015 UTC,"The whole reason why they call it ""Direct CU"" is because there isn't a heatspreader. The heatspreader is a source of thermal resistance, and getting rid of it can improve performance a little bit, at least for the heatpipes that are directly touching."
hardware,3bqgtp,mermaliens,5,Wed Jul 1 20:47:24 2015 UTC,"I don't understand. My old ASUS strix 970 didn't have active cooling on the memory chips either, but I could get a solid 8000Mhz overclock on them. So do you mean if they had a piece of thermal tape on them it would go even higher? Not trying to be rude btw, I'm just curious if this memory cooling is as big as an issue as what I've heard on reddit and online in general."
hardware,3bqgtp,Maysock,1 point,Wed Jul 1 12:20:24 2015 UTC,The strix 970 consumes significantly less power even heavily overclocked. That's the real difference. I get your point though.
hardware,3bqgtp,CEKPETHO,2,Wed Jul 1 16:42:09 2015 UTC,"Maybe the GPU chip uses less power, but memory chips use the same."
hardware,3bqgtp,Maysock,1 point,Wed Jul 1 21:13:53 2015 UTC,"So then it shouldn't be a problem, as it wasn't on the strix 970."
hardware,3bqgtp,CEKPETHO,1 point,Wed Jul 1 22:39:55 2015 UTC,Yep
hardware,3bqgtp,continous,1 point,Wed Jul 1 22:46:02 2015 UTC,"Not good for overclocking, that's for sure.   Idk, I've been finding that more often than not the Maxwell cards are silicon bound rather than thermal bound."
hardware,3bqgtp,SeaJayCJ,1 point,Thu Jul 2 07:45:30 2015 UTC,"The VRAM on the back of the Titan X reaches 100C. Granted, that is the back, so it's not getting any second-hand airflow from the cooler (only case airflow), but it still shows that VRAM needs decent cooling."
hardware,3bqgtp,continous,1 point,Thu Jul 2 07:54:56 2015 UTC,"I'm not saying this situation is good, but rather, thermal headroom has become less of a big deal lately."
hardware,3bqgtp,faderprime,0,Thu Jul 2 07:57:01 2015 UTC,The memory cooling is where I'm confused. I don't see a difference in that regard between the two coolers.
hardware,3bqgtp,CEKPETHO,2,Wed Jul 1 13:37:04 2015 UTC,"Well, the Titan X has memory chips on the back of the PCB that are in no way actively cooled. They do not even have a backplate passively cooling them. Maybe the memory used in Titan X and 980TI does not require any active cooling? Maybe air blowing from the fans through the heatsinks onto the memory chips is enough?"
hardware,3bqgtp,Hiryougan,2,Wed Jul 1 21:19:42 2015 UTC,Could it be that DCU3 cooler was made with Fiji and HBM in mind since it has larger die size? Those two heatpipes would fit HBM perfectly
hardware,3bqgtp,substrate80,1 point,Mon Jul 6 11:12:48 2015 UTC,"I've been a little curious and confused about some heatpipe based designs lately.  My understanding is that the section of the heatpipe in contact with the heat-generating component should be at the lowest elevation, to allow the condensate to flow by gravity back down to the heat-generating component.  In a standard ATX case, the video card printed circuit board is on top and the GPU is on the bottomside, and thus the heatpipes contact the GPU at the highest elevation and the heatsink fins are at the lowest elevation.  Seems upside down to me."
hardware,3bqgtp,Kaldskryke,4,Wed Jul 1 18:32:39 2015 UTC,I'm fairly certain they would use capillary action rather than gravity specifically for that reason. But I'm not a hardware engineer at one of these companies so I can't speak to whether that's the case or not.
hardware,3bqgtp,BlayneTX,7,Wed Jul 1 19:19:21 2015 UTC,"Not all heatpipes are created equal. There's a wide variety of wicking materials, and there are sometimes tradeoffs between the strength of the capillary action, thermal conductivity, heat capacity, and of course manufacturing cost.  Sometimes you'll see heatpipes advertised as containing a sintered powder wick, which is a marketable feature because it performs well but it has a high manufacturing cost. It's somewhat more common for computer heatpipes to be made with layered metal mesh, which is a reasonable compromise between cost and effectiveness. The cheapest heatpipes might not have any wicking features at all, or just a simple grooved interior.  tl:dr; high-quality heatpipes are less prone to orientation than cheap-crap heatpipes"
hardware,3bqgww,Dark-X,20,Wed Jul 1 09:14:43 2015 UTC,I expected a custom loop and 2 980Tis or Titan Xs TBH.
hardware,3bqgww,CeeeeeJaaaaay,5,Wed Jul 1 11:26:32 2015 UTC,"""I don't really play games""  Edit: Just meant, he said he didn't play games but still went with dual 980s."
hardware,3bqgww,SirCrest_YT,7,Wed Jul 1 15:53:37 2015 UTC,"Pretty sure that PC is made to last. He probably spent $1000 or so on the custom building alone, if he has that much to spend may as well add $300 for 2 cards that go 60% faster and have more VRAM. They also perform better in CUDA accelerated renders, so there's that."
hardware,3bqgww,CeeeeeJaaaaay,21,Wed Jul 1 16:01:59 2015 UTC,I'm gonna make a guess and say he didn't spend a penny on the PC and is making a few videos to advertise them instead.
hardware,3bqgww,JustaPassanger,7,Wed Jul 1 16:35:59 2015 UTC,It was graciously donated to him.
hardware,3bqgww,leviwhite9,4,Wed Jul 1 16:58:01 2015 UTC,"In this case I think it was ""donated"", but he's proudly said he buys most of his hardware he reviews in some interviews hes done."
hardware,3bqgww,SirCrest_YT,2,Wed Jul 1 17:39:21 2015 UTC,Or it gets sent to him to review and he sends it back.
hardware,3bqgww,BroomSIR,1 point,Wed Jul 1 18:01:28 2015 UTC,"That's probably true, I only watch his vids occasionally. Nothing wrong with taking hardware or getting payed btw. I just think that all content creators should at least mention it in the description. I know that Total Biscuit does this, not sure about MKBHD or others."
hardware,3bqgww,JustaPassanger,2,Sun Jul 5 06:24:57 2015 UTC,I suppose he could have added the difference if they didn't want to pay for it.
hardware,3bqgww,CeeeeeJaaaaay,5,Wed Jul 1 16:38:21 2015 UTC,"Cuda is useful in Premiere and CC but it's not magic. It also doesn't use two GPU's.  Also my comment wasn't that he shouldn't have gotten two titan's because he doesn't game, but that he got two 980's but is self-proclaimed non-gamer. Versus a single 970 or a Titan Black which would arguably be a better workstation card than a 980/Ti/TitanX."
hardware,3bqgww,SirCrest_YT,3,Wed Jul 1 17:38:46 2015 UTC,"Cuda is useful in Premiere and CC but it's not magic. It also doesn't use two GPU's.   I'm aware of that I use the Adobe suite myself.   Also my comment wasn't that he shouldn't have gotten two titan's because he doesn't game, but that he got two 980's but is self-proclaimed non-gamer. Versus a single 970 or a Titan Black which would arguably be a better workstation card tha na 980/Ti/TitanX.   I see what you mean now, and I agree. I don't know why he went with 2 cards if he doesn't play games, but maybe he wants to start gaming? IDK. Not sure why you got downvoted anyway..."
hardware,3bqgww,CeeeeeJaaaaay,2,Wed Jul 1 17:48:39 2015 UTC,"I re-read my comment and I can see why people thought I was just doing a dumb quote, I was more so just confirming the question of why not going with two more modern cards.  He surely could start since he has dual 4k monitors. He has the machine for it now."
hardware,3bqgww,SirCrest_YT,2,Wed Jul 1 17:53:58 2015 UTC,The Titan X does not have dramatically increased CUDA cores vs. a 980Ti like the older ones did. It was even more cut down/disabled by nVidia because they wanted semi-pro users to buy more expensive pro cards.
hardware,3bqgww,Kaghuros,2,Wed Jul 1 17:46:52 2015 UTC,I never said Titan X has better CUDA performance than 980Ti.
hardware,3bqgww,CeeeeeJaaaaay,1 point,Wed Jul 1 18:19:44 2015 UTC,And adobe can't use the 900 series at all yet...
hardware,3bqgww,ElectronicsWizardry,24,Tue Jul 7 04:04:38 2015 UTC,I think it's funny how people hated on him for using a mac pro instead of a PC and now they are hating on him for buying pre built vs building it all himself. Yet they fail to realize that building himself would actually be a waste of money in this case because if he spends the same time on making a video instead he would probably earn more than what he saves by building himself. The guy has >$100k worth of camera gear for christ's sake.
hardware,3bqgww,barthw,19,Wed Jul 1 09:35:57 2015 UTC,People forget that he built a hackintosh before the Mac pro came out
hardware,3bqgww,IcFram,1 point,Wed Jul 1 14:53:12 2015 UTC,"Exactly. It's not like he doesn't know how to build a computer, he's done it before and could have easily done this for less. Maybe they gave it to him for free or discounted, but either way, he could afford it on his dime. It's a specifically customized  PC that's very powerful with almost zero effort. I'd personally save the money and do it all myself regardless of how much money I make, but that's because I really like the building part."
hardware,3bqgww,mbop,3,Tue Jul 7 05:25:14 2015 UTC,The guy has >$100k worth of camera gear    Do Youtuber's really pull in this much cash?
hardware,3bqgww,roro_mush,14,Wed Jul 1 16:28:16 2015 UTC,"Linus has several employees and is moving into a new office. That's with 1.5 mio subs, MKBHD has 2.5 mio subs... Vitaly 8 mio, is driving a Jag and we all know that pewdiepie is a multi millionaire with 30 mio subs."
hardware,3bqgww,JustaPassanger,7,Wed Jul 1 16:40:28 2015 UTC,"And have you seen Linus' house? It's fucking massive, and houses like that aren't cheap in the GVA."
hardware,3bqgww,BrockYXE,2,Wed Jul 1 17:31:18 2015 UTC,plus linus puts his stuff on vessel and does alot of sponsor spots
hardware,3bqgww,k0fi96,1 point,Wed Jul 1 21:41:25 2015 UTC,"That's true, I didn't mention it because it's almost normal for youtubers now to have several youtube accounts plus whatever they have on Vessel, Patreon, Twitch etc... A lot of them are also getting payed for reviews or for playing certain games (if they don't talk shit about it)."
hardware,3bqgww,JustaPassanger,2,Sun Jul 5 06:10:45 2015 UTC,"Not sure what it is now, but back in 2010 if you had 100k subscribers and pushed out bi weekly videos you could expect to be making around 50k USD/year."
hardware,3bqgww,willxcore,1 point,Wed Jul 1 19:45:37 2015 UTC,"2 or 3 years after youtube launched and I realised that people are going to be making a lot of money with it, I unsuccessfully tried to launch channels.  It takes a lot more start-up capital than people imagine to get this going. People want high quality videos, constant updates, newer and better hardware. At the time I estimated you would need about 10k to get a decent enough ball rolling that you could actually dedicate enough time to it.  Otherwise your channel will end up burning out too fast."
hardware,3bqgww,AttackOfTheThumbs,1 point,Sun Jul 5 03:35:43 2015 UTC,The people I knew that were bringing in a decent amount of money started off just uploading regular videos of themselves and friends playing WoW and Halo. One eventually started making EDM and started a label and has gotten pretty popular from it. I think it takes dedication as well as creative talent more than anything.
hardware,3bqgww,willxcore,4,Sun Jul 5 03:40:38 2015 UTC,I think it looks cool. They did a nice paint job on it.
hardware,3bqgww,i_mormon_stuff,6,Wed Jul 1 14:21:38 2015 UTC,"I really don't understand why it's being called a supercomputer, this is NOT a supercomputer it's just a beefy rig.  That being said, I understand why you would buy pre-built vs building yourself if the price isn't that much better. The price you would pay for a system like that.. just.. wow."
hardware,3bqgww,DexRogue,4,Wed Jul 1 16:55:30 2015 UTC,because mkbhd exaggerates and/or does not really know what he is talking about ... what else
hardware,3bqgww,technologia,1 point,Wed Jul 1 17:48:22 2015 UTC,"Out of curiosity what constitutes as a supercomputer? I perfectly understand this computer is not one, but where is the border?"
hardware,3bqgww,Vawqer,4,Wed Jul 1 22:29:14 2015 UTC,"What is a ""proper"" race car? It's high performance, of course, but first and foremost it's built for racing, which means it doesn't contain such luxuries as air-conditioning or particularly comfortable seats and its generally ill advised to try to use it as a daily driver. In fact to really use it to its full potential you probably need a team of people for maintenance/pit stops.  The sort of technologies and performance metrics that super computers are defined and differentiated from their normal brethren by changes from year to year, but if a machine doesn't sit near the limit of practicality afforded by the level of technology at the time of its construction then its probably not a supercomputer.  A supercomputer is the result of throwing more and more hardware at a problem until the overhead associated with the interconnect, or storage, or cooling or what have you gets in the way of further scaling.  This means that if you don't need a team of people to build it, maintain it, and run it, its probably not a supercomputer. This means that if the cooling system, interconnect, data storage, etc, etc aren't complex systems in and of themselves then its probably not a supercomputer."
hardware,3bqgww,Dr_professional,1 point,Thu Jul 2 00:16:45 2015 UTC,"Supercomputers are usually large processor clusters housed in warehouses with a team of IT professionals to maintain the hardware and networking equipment. Often times, details about Supercomputers are either classified by a government, or kept as trade secrets by the companies who own them."
hardware,3bqgww,cuddlefucker,1 point,Thu Jul 2 14:39:19 2015 UTC,It's all relative.  We carry the supercomputers of the 60's around in our pockets today.  In 10 years we just might have the equivalent of University or corporate level supercomputers on our desktops.
hardware,3bqgww,Slamdunkdink,4,Fri Jul 3 21:34:11 2015 UTC,Yea this one was like $8500+ out of the box will all the custom he got.
hardware,3bqgww,justastarvingartist,1 point,Wed Jul 1 15:46:18 2015 UTC,Am I the only one that really isn't impressed by this thing? I've seen much cooler builds on /r/battlestations and /r/cablemanagement.
hardware,3bqgww,Bickell,2,Wed Jul 1 22:54:20 2015 UTC,No love for /r/buildapc?
hardware,3bqgww,cuddlefucker,0,Thu Jul 2 14:41:10 2015 UTC,"i downvoted this video on youtube.. it was just a way to make an advertisement for the company, because obviously he didn't pay anything... there's nothing important or worth telling about this ""rig"".."
hardware,3bqgww,coldpassion,-4,Wed Jul 1 18:28:52 2015 UTC,Can we not post trash?
hardware,3bqi4h,LiLThuG,11,Wed Jul 1 09:33:41 2015 UTC,It's definitely a beast. I was pretty happy to see I could get 60fps in Witcher 3 with max settings at 3440x1440 with a little OC. Hairworks off of course.
hardware,3bqi4h,Stingray88,3,Wed Jul 1 15:43:02 2015 UTC,It is so hard for me to find 3440x1440 benchmarks. I was wondering if a single 980ti could drive W3 at that res.
hardware,3bqi4h,Stingray88,6,Wed Jul 1 16:19:45 2015 UTC,"With the factory OC it hovers around 52fps. 42fps with hairworks. I boosted it to 1500MHz and it's a clean 60fps. No huge spikes in the 1-2 hours I've played.   Also I agree, 3440x1440 is quickly becoming very popular with gamers. Benchmarkers should be including it. Might be an issue with certain games not supporting it."
hardware,3bqi4h,diggs747,1 point,Wed Jul 1 16:40:30 2015 UTC,"I have msi gaming g6 OC'd to 1430 and can barely get ~60fps on 1440p. I have a 4770k, not sure where the hell I'm bottlenecking."
hardware,3bqi4h,SirCrest_YT,1 point,Wed Jul 1 17:51:17 2015 UTC,With or without hairworks? And have you tests with SLI off and on? Maybe there is a driver with SLI. For a while I had to force AFR2 for Witcher3.
hardware,3bqi4h,diggs747,1 point,Wed Jul 1 17:57:01 2015 UTC,"I'm not using SLI. With hairworks my fps goes down to ~55. All my settings are maxed. I'm on whatever the current drivers are. Whats AFR2, and how do I force it?"
hardware,3bqi4h,SirCrest_YT,1 point,Wed Jul 1 18:11:41 2015 UTC,"Oh my bad, I thought you said you had a pair of them running.   Stingray was vague whether his 1440p60 was with or without hairworks, so i'd guess off + the 1.5ghz OC."
hardware,3bqi4h,diggs747,1 point,Wed Jul 1 18:35:59 2015 UTC,"Ok, that'd be in line with my personal benchmarks too. I've noticed a lot of other review sites are getting even higher marks, but they are running higher end processors/ram then most people."
hardware,3bqi4h,Stingray88,1 point,Wed Jul 1 18:49:33 2015 UTC,"I've got a 3770K @ 4.5GHz, but the GPU is definitely the bottleneck with W3."
hardware,3bqi4h,diggs747,1 point,Wed Jul 1 18:23:49 2015 UTC,"Oh ok, read your post wrong. I thought you were getting 60fps on 4k."
hardware,3bqi4h,djfakey,1 point,Wed Jul 1 18:26:33 2015 UTC,"One could just estimate the performance by looking at 2560x1440 and 4k and just figure right in between, it should actualy be closer to 2560x1440."
hardware,3bqi4h,DexRogue,2,Wed Jul 1 18:05:34 2015 UTC,That's what I've been doing. I've just been splitting the difference.
hardware,3bosvn,AssCrackBanditHunter,71,Tue Jun 30 22:50:36 2015 UTC,"GDDR5 SGRAM is based on DDR3 SDRAM. They are basically the same chips with some functional differences.  The memory is arranged differently on a graphics card, and is accessed in a different method. Graphics cards have memory laid out in parallel, with chunks of memory dedicated to specific ALUs (Arithmetic Logic Units). This means that often times, a specific ALU can only access memory in a specific chunk of the physical memory chip. This specific allocation is what allows for much greater speeds.  GPUs are excellent at performing parallel tasks, which this memory configuration is beneficial for. This does not work well with a CPU, which needs to do many sequential tasks. The CPU needs to be able to access arbitrary data regardless of it's physical location in the SDRAM.  In summary; It's more about the arrangement and configuration of the memory chips, and how they are accessed by the processor, instead of what the chips actually are made of. GDDR is specialized and arranged in a way to allow for high-bandwidth transfers in order to complete parallel tasks, and this allows it to operate at a higher frequency. CPUs require a different arrangement of memory so it can access information anywhere in the system memory at any time. It is more difficult to make SDRAM memory operate as quickly as it does in GDDR configuration.  I have a basic understanding, I'm sure this is not 100% correct, so maybe someone can correct me or add to this answer.  EDIT: I think the answer by /u/Qzrx in this thread complements my answer nicely. The configuration differences I detailed provide the end result he describes: GDDR5 has higher latency and more throughput, while DDR3 has lower latency and less throughput. It is designed this way due to the different requirements of GPU tasks vs CPU tasks."
hardware,3bosvn,Slyons89,7,Tue Jun 30 23:05:19 2015 UTC,"But then, how do systems like the ps4 work, which has a unified vram and ram with GDDR5?"
hardware,3bosvn,AlchemicalDuckk,17,Tue Jun 30 23:31:31 2015 UTC,"While GDDR5 is a bit higher latency, it's not exactly end-of-the-world bad for a CPU. Modern CPUs have a tiered memory hierarchy. At a high level, programs and data is stored on a drive for long term storage, then loaded into RAM when needed. But the CPU doesn't need all that data at once - it only works on chunks at a time. So there is a cache memory subsystem on the CPU for only loading what it needs. Cache is very small, on the order of kilobytes to a few megabytes, but is very fast compared to system RAM. The CPU has a prefetcher, which predicts what data the CPU needs next, and loads that into cache. By doing that, CPUs mitigate a good chunk of latency. Tests by places like Anandtech and Tom's Hardware have shown that the difference between high and low latency sets of DDR3 is on the order of a couple percent at most, and oftentimes the difference is effectively zero."
hardware,3bosvn,andromeduck,12,Tue Jun 30 23:56:04 2015 UTC,That still doesn't answer the question because cold misses will always be missed and because of how expensive LLC misses are.  Real answer is that 2x the clock latency combined with 2x the clocks result in very little net change in real latency.
hardware,3bosvn,andromeduck,2,Wed Jul 1 01:57:34 2015 UTC,Higher latency in terms of clocks. 2x the clock latency + 2x the clock means negligible differences in real latency.
hardware,3bosvn,Slyons89,1 point,Wed Jul 1 01:52:05 2015 UTC,"I don't know too much about the PS4. I do know that it uses an APU, which is a combined CPU and GPU. Similar APUs are available for PCs, but use DDR3, and the graphics component is limited by the lower bandwidth of the DDR3.  Since the PS4 is a custom system that does not conform to all of the same programming and architecture standards as a regular x86 style PC, they may be using custom low-level or operating system functions that allow the CPU to work OK with GDDR5.  It's one of the things that allow consoles to wring more performance out of the same hardware vs PCs. They are a bit more specialized. Regular PCs have to meet specific standards that allow them to perform tons of different tasks, instead of being specialized in mostly games and media playback. PCs are a 'jack of all trades' kind of thing."
hardware,3bosvn,andromeduck,6,Wed Jul 1 00:00:19 2015 UTC,No. Memory type is handled exclusively by the memory controller. It's transparent to applications including the kernel.
hardware,3bosvn,jorgp2,0,Wed Jul 1 01:53:21 2015 UTC,No.
hardware,3bosvn,Slyons89,1 point,Wed Jul 1 11:31:48 2015 UTC,"Spent a lot of time on that one, yeah?"
hardware,3bosvn,jorgp2,2,Wed Jul 1 14:13:26 2015 UTC,"No.  But seriously the Xbone has about as much power as a top of the line Laptop APU (The 8800fx). And that APU has lossles color compression, twin VCEs (For 2x 4K performance), an integrated southbridge, and has a 35watt TDP.  And have you seen how bloated the potatoes OSes are, there's only 5GB of ram available to games; and the OS takes priority for Compute resources."
hardware,3bosvn,Slyons89,2,Wed Jul 1 17:42:50 2015 UTC,"I think the top laptop APU from AMD is the A10-7400P, a 35 W part with 3.4 GHz max clockspeed.  And yeah, the Xbox OS is laughable for gaming performance. I did say that I don't know much about the PS4 OS. I've never used one."
hardware,3bosvn,jorgp2,1 point,Wed Jul 1 19:27:48 2015 UTC,The 8800p is Carrizo.
hardware,3bosvn,andromeduck,2,Wed Jul 1 20:44:04 2015 UTC,"No the memory is striped but not tied to ALUs or even groups of them, that wouldn't make any sense."
hardware,3bosvn,Qzrx,124,Wed Jul 1 01:50:30 2015 UTC,"Short version: GDDR5 has great throughput but terrible latency (good for moving a few big things infrequently).  DDR3/4 have great latency but less throughput (good for moving lots of little things frequently).  Generally speaking, applications are better served by the latter more than the former."
hardware,3bosvn,unpythonic,64,Tue Jun 30 22:59:47 2015 UTC,"Having worked in the nitty-gritty of GDDR5 there is one other subtle reason to avoid it: reliability. It's been several years but if I recall correctly while the data channels are protected against single or double bit errors, the command channel is not. I think the guys running the numbers came back and said that they expect one silent data corruption every six months, much less than the company target which was not less than every 7 years.  This isn't really that big of an issue for a video card: You remember that one frame last year that had a messed up texture? No, no, that one was an actual software bug... that other frame that had a few corrupt pixels in the texture? That was a silent data corruption.  Now if you're instead running software and get a silent data corruption, there's a good chance that your application will spontaneously crash and a small but not insignificant chance that the whole machine will crash."
hardware,3bosvn,Monday_Morning_QB,23,Wed Jul 1 01:12:55 2015 UTC,"Also the cell retention time for GDDR is around 16 ms. Normal DDR is 64 ms. Data is lost much quicker in GDDR, but it's not as important for the exact reasons you mentioned above."
hardware,3bosvn,narwi,4,Wed Jul 1 02:03:46 2015 UTC,"So? All dram requires refresh timings to be fullfilled, its part of the memory controllers job. if pc ram had refresh time of 16ms, everything in compliant systems would continue to work as is."
hardware,3bosvn,0Ninth9Night0,1 point,Wed Jul 1 09:33:23 2015 UTC,"Interesting, never knew this tidbit. Thanks!"
hardware,3bosvn,Monday_Morning_QB,7,Wed Jul 1 19:44:24 2015 UTC,"Here's another fun fact for you.  GDDR and HBM-styled memory have more IO lanes in their metal layers.  This is partly how they achieve high ""effective"" clock speeds.  Normal DDR3 has 8 IO lanes and GDDR has 16-32 depending on the design.  Source:  I work in the memory industry and look at chip layout fairly often. :)"
hardware,3bosvn,TRD099,0,Wed Jul 1 20:34:12 2015 UTC,And you got downvoted.
hardware,3bosvn,Rediterorista,-6,Thu Jul 2 00:16:52 2015 UTC,16ms = 60 Hz
hardware,3bosvn,theasocialmatzah,13,Wed Jul 1 02:33:58 2015 UTC,How would that work for when you want a cinematic 30 fps though
hardware,3bosvn,ZandalarZandali,7,Wed Jul 1 02:56:43 2015 UTC,Or butter 144hz
hardware,3bosvn,rokr1292,1 point,Wed Jul 1 03:14:11 2015 UTC,Isn't eizo making 240hz monitors now?
hardware,3bosvn,msdrahcir,2,Wed Jul 1 04:24:57 2015 UTC,they are 120hz interpolated to 240 - like sorta like tvs
hardware,3bosvn,CeeeeeJaaaaay,2,Wed Jul 1 04:40:29 2015 UTC,"As far as I'm aware they're not interpolated, they just use black frames in between for strobing."
hardware,3bosvn,rokr1292,1 point,Wed Jul 1 06:58:13 2015 UTC,Awh darn. Can any connector even handle that bandwith? Displayport?
hardware,3bosvn,koffiezet,4,Wed Jul 1 07:53:26 2015 UTC,"This has nothing to do with your monitor's refresh rate.  RAM requires to be 'refreshed' every X time or it loses it's data. This means the memory controller (or memory chip itself, not sure these days) has to read the memory and re-write it every X time. For GDDR this would have to be done every 16ms, normal DDR every 64ms.  It's explained a bit more here."
hardware,3bosvn,autowikibot,2,Wed Jul 1 07:29:54 2015 UTC,"Memory refresh:       Memory refresh is the process of periodically reading information from an area of computer memory and immediately rewriting the read information to the same area without modification, for the purpose of preserving the information.  Memory refresh is a background maintenance process required during the operation of semiconductor dynamic random access memory (DRAM), the most widely used type of computer memory, and in fact is the defining characteristic of this class of memory.  In a DRAM chip, each bit of memory data is stored as the presence or absence of an electric charge on a small capacitor on the chip.   As time passes, the charges in the memory cells leak away, so without being refreshed the stored data would eventually be lost. To prevent this, external circuitry periodically reads each cell and rewrites it, restoring the charge on the capacitor to its original level. Each memory refresh cycle refreshes a succeeding area of memory cells, thus repeatedly refreshing all the cells in a consecutive cycle. This process is conducted automatically, in the background, by the memory circuitry, while the computer is on, and is transparent to the user.  While a refresh cycle is occurring the memory is not available for normal read and write operations, but in modern memory this ""overhead"" time is not large enough to significantly slow down memory operation.     Relevant: EDRAM | Volatile memory | Semiconductor memory   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3bosvn,narwi,2,Wed Jul 1 09:31:42 2015 UTC,Just the same. the refresh cycle is not relevant to frame rate.
hardware,3bosvn,Anon123212321,6,Wed Jul 1 09:31:48 2015 UTC,"16 ms * 60 = 960 ms.  There's now 960ms in a second?  60 Hz = 16.66... ms, not that it's particularly relevant to the screen refresh rate."
hardware,3bosvn,t_Lancer,3,Wed Jul 1 09:33:57 2015 UTC,I don't think that has anything to do with what you think it does.
hardware,3bosvn,smoothsensation,1 point,Wed Jul 1 04:31:27 2015 UTC,Enlighten me on what what you are meaning here please.
hardware,3bosvn,Rediterorista,-2,Wed Jul 1 05:39:42 2015 UTC,"Nothing really, only that 60Hz is about 16ms. Interesting if you like to optimize your CS settings. 144Hz monitor would be more fitting."
hardware,3bosvn,smoothsensation,3,Wed Jul 1 14:24:43 2015 UTC,I'm still unsure how that is related to Ram latency and cell retention time for GDDR.
hardware,3bosvn,Rediterorista,0,Wed Jul 1 14:26:58 2015 UTC,It isn't as far as i am aware.   But perhaps it has influence on performance?
hardware,3bosvn,sabot00,1 point,Wed Jul 1 14:34:13 2015 UTC,Maybe if you eat an apple while using an Apple device it'll make it go faster.  Makes about as much sense as you do.
hardware,3bosvn,Rediterorista,1 point,Wed Jul 1 14:36:53 2015 UTC,"I never implied it had to make sense.  But thanks for your advice, what should i eat if i have a Win7,8 PC to make it faster?"
hardware,3bosvn,DubiousEthicality,35,Wed Jul 1 22:45:55 2015 UTC,"You've said mostly everything I was going to, but I'd like to explain why we ""waste time"" with slower memory.   My PC has an HDD, SSD, DDR3, GDDR5, L3 cache, L2 cache, L1 cache, and a 64-bit register. Why don't we just replace everything with what the register's made of, which can update itself 4.5 billion times per second? Heat, power, form factor, and cost. Every one of these technologies serves a different purpose, and increasing capacity/speed would result in compromises to all four of the above limiting factors. Essentially, you should be happy that storage devices are slower than RAM since it makes them cheaper and cooler."
hardware,3bosvn,1usernamelater,27,Wed Jul 1 22:47:51 2015 UTC,"and we have ram because it sits nicely in between cache and disk in performance and cost.  Go all disk? slooowww, go all cache? too expensive, RAM in the middle? just right."
hardware,3bosvn,elevul,1 point,Tue Jun 30 23:11:28 2015 UTC,Memristors!
hardware,3bosvn,epsys,0,Wed Jul 1 00:19:12 2015 UTC,"no way man, I just use quad channel raid 0 SSDs on PCI-Express, no ram needed !!!111one  actually,   no, nevermind, yes that would be a terrible idea. holy write cycle lifetime batman"
hardware,3bosvn,imbecile,18,Fri Jul 3 12:08:36 2015 UTC,"It's not just cost. A large part, if not the most important part of why registers are faster than caches and caches are faster than memory is because they are smaller and the electrons have shorter distances to to travel."
hardware,3bosvn,wtallis,8,Wed Jul 1 05:04:36 2015 UTC,"Speed of light occasionally enters in to things, but it's not the dominant factor for anything outside of the CPU die. DIMMs would be much faster with high-speed SRAM rather than DRAM, but they would also need to be housed in a water block."
hardware,3bosvn,epsys,4,Wed Jul 1 01:59:02 2015 UTC,"even on the CPU die (you can calculate this yourself-- how far does electricity travel in a single CPU cycle, assuming CPU overclocked and running at 4.5ghz? remember, light goes 3e8 m/s. Electricity typically 2e8 m/s).  so you see now?? a lot of pretty intense math science and engineering goes into keeping the different parts of the chip aligned up to the same 'clock', even though each cycle is executing at different nanoseconds in time, because the simple distance across the chip actually becomes not insignificant at the frequencies (3-5ghz) we're talking about.  however, since CPUs are pipelined pretty deep (AMD's Bulldozer i.e. FX-8150 == 12 deep pipeline for example), each function block is running at 4ghz/12, so 333mhz. In other words, silicon doesn't switch all that much faster than it did 10 years ago, we just started stacking it better-- and that's how the Pentium 4 managed to clock to 3.8ghz (Prescott aka 'Pres-hott' lol)-- it had 31 stages  edit: I guess actually the silicons do switch faster-- smaller channels and finFETs help the channel open and pinch shut faster. I'm getting rough. Still, there's an upper limit on switching frequency of silicon, and beyond that we'd have to move to germanium as a semiconductor. I seem to recall the upper limit for silicon is ~10ghz, of course we don't get near that, which is why we sometimes strain silicon against germanium lattice (to pick up some of the properties of the lattice arrangement of atoms being closer together in germanium compared with silicon). Germanium transistors can switch ~100ghz, but it's much more difficult to grow ingots of (what they slice up to get wafers) than silicon, and there's a big industry momentum to making silicon grow better and more efficiently, which is why nobody's really bothered switching to germanium yet, but why might one day. Another benefit of germanium is it only takes 0.2-0.3v to switch an electron from the valence band into the conduction band of the nearest germanium atom-- which would immediately reduce CPU power required significantly"
hardware,3bosvn,narwi,6,Wed Jul 1 03:54:22 2015 UTC,"Uhh, you have your math and pipelining wrong. Each stage of the pipeline executes at 4Gz, the ""clock speed"" of a processor is the rate at which the slowest pipeline stage can be completed. And yes, prescott did have loads of really short pipeline stages."
hardware,3bosvn,epsys,2,Wed Jul 1 05:08:19 2015 UTC,"might be right about that  yeah. you're right about that. I mistook the possibility of all stages running only once, concurrently, and the maximum frequency you could run the CPU at in such a setup (333mhz aka 4ghz/12) with the speed of each stage.  so then that answers/explains why/how silicon hasn't really gotten much faster in the last 10 years"
hardware,3bosvn,narwi,1 point,Wed Jul 1 10:13:56 2015 UTC,"Yes, silicon switching have got faster, but not a lot faster and still a lot of heat is generated even at small feature sizes. clock rate has not gone up in a meaningful way for a long time now."
hardware,3bosvn,epsys,1 point,Wed Jul 1 21:57:59 2015 UTC,Yes.
hardware,3bosvn,TopAce6,1 point,Thu Jul 2 09:13:09 2015 UTC,"im under the impression that the distance of the dimms from the cpu is also a notable factor, am I correct?"
hardware,3bosvn,epsys,4,Fri Jul 3 10:32:23 2015 UTC,"trace length is either accounted for to ensure the signals arrive at same time, or clock signal expanded sufficiently to account for the mismatch in trace length. If clock signal is expanded/lengthened, you obviously can't get as many clocks in per second, which yes would limit your bandwidth. So hence you see motherboards specifying max DDR/2/3 speed they'll support. Other stuff goes into it besides just trace length, but yes that's one thing. That was one of the benefits to AMD's HBM on the Fury X, the RAM is right there connected via interposer which is much closer than GDDR5 typically sits.  I guess yeah a really long trace length would also limit your bandwidth due to the added latency of sending the signal that far. This, for example, is what determines the maximum cable length in the USB2.0 specification-- since USB devices until v3.0 are individually polled, if the signal doesn't get to the chip in your mouse in time the host won't get the response in time and will think nothing is there and move on to talk to the next device, which probably would cause a collision or something since two would be talking at once.  One time in a coop job I was given the assignment of verifying the trace lengths on a motherboard were no longer than some distance from the median. That was a cool assignment, but I was an ungrateful little punk who thought it was boring. sigh. pay it forward"
hardware,3bosvn,TopAce6,2,Wed Jul 1 05:17:29 2015 UTC,oh I got lucky and found an obvious expert ( at least compared to most of us) thank you for the answer.
hardware,3bosvn,epsys,5,Wed Jul 1 05:21:21 2015 UTC,haha you're too kind. I'm too rusty to be considered an expert IMHO. 6 figure education goes to waste smh...
hardware,3bosvn,TopAce6,3,Wed Jul 1 05:27:46 2015 UTC,"the paradox of the more you know the dumber you feel?  compared to allot of people, im a computer expert (In certain areas) but the more I learn the more I realise I dont know shit.  I feel that computers are far too complex for a single individual to know the ins and outs of every aspect of a computer,  from the precise electronic engineering, the lithography of the silicon to the OSI layers of networking and every bit of software and firmware in between.... there's just too much imo.  anyways meds kicking in hard and making me ramble, but again thanks for the answer."
hardware,3bosvn,hopsafoobar,1 point,Wed Jul 1 05:45:37 2015 UTC,There are some phased array radar systems that use Germanium tech.
hardware,3bosvn,epsys,1 point,Wed Jul 1 06:02:04 2015 UTC,"wow, that's cool. still, can only grow like 100mm wide ingots, vs 300mm wide silicon wafers with 400mm under research"
hardware,3bosvn,Jakeattack77,1 point,Wed Jul 1 08:26:05 2015 UTC,I've heard that if we want to make proccess sizes extremely small like 2nm we might need to switch to germanium for that same reason of latice size
hardware,3bosvn,epsys,1 point,Wed Jul 1 22:00:37 2015 UTC,2nm isn't possible. the electrons jump across the dielectric at that node size
hardware,3bosvn,Darius510,5,Sat Jul 4 04:32:37 2015 UTC,"As they say, everything in computing is either a pipeline or a cache."
hardware,3bosvn,epsys,3,Sat Jul 4 17:34:31 2015 UTC,series of tubes?
hardware,3bosvn,wretcheddawn,2,Wed Jul 1 03:53:26 2015 UTC,"If you have a billion dollars I'm sure you can get 1TB of SRAM.  It would still be slower though, because you can't put that much RAM close enough to the CPU and retain the same latencies as a register.  Caches also use tricks like partial associativity to further improve performance, something that wouldn't be possible anymore and it would take longer to lookup the data you're looking for.  It also wouldn't help.  The CPU doesn't need to access all that information that quickly (it would result in imperceptible performance increases for the average workload if you already have enough system RAM and an SSD), and the instant you make a web service call that all becomes irrelevant.  With memristors, we may indeed do away with some of the layers."
hardware,3bosvn,0Ninth9Night0,2,Wed Jul 1 05:42:34 2015 UTC,"Are there any other types of memory? Not including quantum computing which... Which only sort of recently stored 7 ""bits"". I went on a bit of a wiki binge, but here's my source: ""In 1996, Lov Grover from Bell Labs invented the quantum search algorithm which yields a quadratic “speed-up” compared to its classical counterpart. A year later the first NMR model for quantum computation was proposed, based on nuclear magnetic resonance techniques. This technique was realized in 1998 with a 2-qubit register, and was scaled up to 7 qubits in the Los Alamos National Lab in 2000. ""  http://plato.stanford.edu/entries/qt-quantcomp/#BriHisFie"
hardware,3bosvn,elevul,1 point,Wed Jul 1 16:45:49 2015 UTC,"But memristors should be able to replace all that without those issues (cost aside, but that's just at the beginning)."
hardware,3bosvn,TeutorixAleria,5,Wed Jul 1 20:36:20 2015 UTC,"DDR3 timings, this PDF on page 48:http://www.hynix.com/datasheet/pdf/dram/H5TQ1G4(8_6)3AFP(Rev0.1).pdf  GDDR5, timings, this PDF on page 133:http://www.hynix.com/datasheet/pdf/graphics/H5GQ1H24AFR(Rev1.0).pdf  GDDR5 timings as provided by Hynix datasheet: CAS = 10.6ns tRCD = 12ns tRP = 12ns tRAS = 28 ns tRC = 40ns  DDR3 timings for Corsair 2133@11-11-11-28 CAS = 10.3ns tRCD = 10.3ns tRP = 10.3ns tRAS = 26.2ns tRC = 36.5ns  GDDR5 latency (on hynix chips at least) is barely any different to DDR3"
hardware,3bosvn,andromeduck,1 point,Fri Jul 3 12:08:19 2015 UTC,Yup! Double the clock cost + double the clocks and you net nothing.
hardware,3bosvn,epsys,1 point,Wed Jul 1 10:38:30 2015 UTC,Absolutely fantastic thank you for help for providing sources and linking
hardware,3bosvn,TeutorixAleria,1 point,Wed Jul 1 18:45:45 2015 UTC,I've been reposting this every time I see this myth for the last 2 years.   I originally found it on the talk page for GDDR5 on Wikipedia.
hardware,3bosvn,epsys,1 point,Fri Jul 3 10:37:23 2015 UTC,Tried to argue with a guy about ddr2/3 800/1600 having same real world latencies and he wouldn't have it because 99924 is bigger than 55515
hardware,3bosvn,TeutorixAleria,1 point,Fri Jul 3 10:43:52 2015 UTC,"It's not surprising to come across that in most places, but when people perpetuate this myth in a sub like this it makes me think that tech enthusiasts really know nothing.   You don't need to be an electronic engineer to understand timing and latency"
hardware,3bosvn,epsys,1 point,Fri Jul 3 10:53:49 2015 UTC,that's reddit
hardware,3bosvn,imallin,4,Fri Jul 3 10:59:52 2015 UTC,"GDDR5 actually doesn't have terrible latency at all,  it is just power hungry and expensive compared to DDR3, and the bandwidth doesn't benefit a CPU in anyway whatsoever."
hardware,3bosvn,epsys,2,Fri Jul 3 11:44:25 2015 UTC,power hungry?
hardware,3bosvn,imallin,5,Wed Jul 1 05:15:12 2015 UTC,"Compared to DDR3? Yes.  In fact, the reduction in power is one of the main benefits of HBM (for this generation at least).  It's really the only way AMD was able to make a GPU that big and keep the TDP from getting outrageous."
hardware,3bosvn,epsys,1 point,Wed Jul 1 05:45:06 2015 UTC,hm. I read somewhere that the GDDR5 chips combined only used about 10 watts-- consider that they don't even have their own heatsink-- and I'm just now remembering that ignores the complexity of connecting to it
hardware,3bosvn,imallin,2,Wed Jul 1 06:01:15 2015 UTC,"Yeah, i think a big part of the power draw comes from the interface.  The GDDR5 setup on the 290X accounts for 15-20% of the TDP.  http://www.anandtech.com/show/9266/amd-hbm-deep-dive/2"
hardware,3bosvn,epsys,1 point,Wed Jul 1 06:02:33 2015 UTC,"hm, wow"
hardware,3bosvn,GreatNull,1 point,Wed Jul 1 08:56:43 2015 UTC,"AMD presentation slides mentioned that they achieved around 60-70W power reduction by moving from GDDR5 to HBM (80W ---> 10W I think).         I don't know for sure, but depending on voltage, capacity, number of modules and frequency, power consumption should not exceed 5W per module. Given that 8GB and 16GB modules are available, GDDR5 power requirements are almost one order of magnitude worse."
hardware,3bosvn,epsys,2,Wed Jul 1 22:00:02 2015 UTC,according to other dude who linked anandtech article it's like 20% of the 290x's power budget-- not just the modules (since they're like 10 watts) but the interface and such
hardware,3bosvn,Timiniel,2,Wed Jul 1 08:15:02 2015 UTC,"GDDR5 actually doesn't have terrible latency at all   Terrible, no, that's an exaggeration, but it is usually less optimized for cas latency and prefetch delay and more for throughput and bandwidth. It'll be like a 1-2 nanoseconds difference, give or take. In a way, the higher bandwidth would be most likely wasted on the CPU..."
hardware,3bosvn,imallin,1 point,Wed Jul 1 22:01:37 2015 UTC,"Right.  It's not that GDDR5 wouldn't be perfectly fine for system RAM,  just that it would be way more expensive, more power hungry, and would gain no benefit from the bandwidth."
hardware,3bosvn,epsys,3,Wed Jul 1 12:26:15 2015 UTC,"why do they have great latency? Are you counting that in nanoseconds or in cycles? The cycles count is lower because the frequency is lower. Crank the frequency and cycles count jumps too...  peeps fuss about DDR4 vs 3 latency, 3 vs 2 etc, but neglect that when you compare DDR3-1600 to DDR2-800, yes the latency is ~double, but so is your bandwidth. So, in real-world-seconds (not frequency-dependent-unit like 'cycles'), you're getting a response from the RAM in the same or better time period-- what was 5 cycles in DDR2-800 becomes 9 cycles on DDR3-1600."
hardware,3bosvn,Timiniel,1 point,Wed Jul 1 21:09:16 2015 UTC,"Bandwidth is only important if you can saturate the bus. The vast majority of times you cannot and are therefore bottlenecked sometimes by the CAS latency and most times by the prefetch time of the module.  The comparison you suggested makes absolutely no sense. We gain nothing from it. You don't need bandwidth at all to realize the ddr3 1600 has an advantage with usual 9-9-9-24 timings over the 5-5-5(-15) timings of the ddr2 module.  What is interesting is comparing less obvious scenarios. DDR3 1333 MHz 9-9-9-24 vs the same ddr2 module you suggested, for example.  In the case of the GDDR5 sgram modules, the distance and relative placement of the modules in relation to the cores is what causes the extra latency. For many random accesses on the memory, the GDDR5 would suffer greatly since it would never be able to use it's large throughput, and these are more common than you'd think."
hardware,3bosvn,vm_linuz,2,Wed Jul 1 05:04:03 2015 UTC,"Until you can feed the data through the CPU faster, there's really no need. Remember, the RAM fills the cache through the memory controller, so simply getting to the processor doesn't mean things are being processed. Additionally, GPUs perform massively parallel computation which requires getting data to the cores much faster."
hardware,3bosvn,krista_,5,Wed Jul 1 12:21:44 2015 UTC,"RAM is a huge bottleneck in quite a number of applications, including biological and genetic research, high speed data processing, and even gaming.  A lot of programs are built around RAM's limitations, and en excessive amount of engineering is spent mitigating it, such as L1, L2, L3 caches and eDRAM...as well as NEUMA, RDMA networking, etc."
hardware,3bosvn,TeutorixAleria,-1,Wed Jul 1 03:49:32 2015 UTC,The reason we have ram is because of the limitations of cache. We don't have cache because of the limitations of ram.   If you could build a 16GB register for your cpu to use we wouldn't have DDR type memory at all.
hardware,3bosvn,Exist50,2,Tue Jun 30 22:59:10 2015 UTC,"Well, Intel is using sdram for some chips, and later will be using HMC while AMD uses HBM."
hardware,3bosvn,headband,1 point,Wed Jul 1 00:03:55 2015 UTC,Hbm and hmc are not competing standards they share a principle (tsv stacks) but the execution is very different.
hardware,3bosvn,Exist50,1 point,Wed Jul 1 10:44:25 2015 UTC,"They can still be competing standards, though I never meant to phrase the OP that way."
hardware,3bosvn,headband,0,Tue Jun 30 23:54:00 2015 UTC,but they're not....hbm is for graphics cards and mcdram is for the Phi
hardware,3bosvn,Exist50,1 point,Wed Jul 1 01:53:23 2015 UTC,"HMC=hybrid memory cube, by the way.  Also, AMD's planning on integrating HBM into many of its products, with an important one being a huge server APU. You can see how this would be beneficial for HSA-acceleration if that ever takes off, as well as in general for compute and feeding the integrated graphics in mainstream offerings. Since Xeon Phi basically competes in the same compute space as GPUs, it makes sense for it to have GPU-like memory technology. Knights Landing will also have 6 lanes of DRR4, though, while I think AMD's chip was rumored to have 4."
hardware,3bosvn,epsys,1 point,Wed Jul 1 02:03:15 2015 UTC,BORG OVERLORDS   run away!
hardware,3bosvn,headband,-1,Wed Jul 1 02:26:14 2015 UTC,uh...you don't think I know that  HMC and HBM are VERY different.
hardware,3bosvn,Exist50,1 point,Wed Jul 1 02:29:08 2015 UTC,"They are very different on a technical level, but what they are being used for is functionally the same between comparative offerings. HBM just also has other uses."
hardware,3bosvn,headband,-2,Fri Jul 3 10:39:34 2015 UTC,"no, it really is incredibly different. HBM actually does NOT have other uses."
hardware,3bosvn,Exist50,1 point,Wed Jul 1 02:31:51 2015 UTC,"How so? It can obviously be used for GPUs, and AMD has expressly stated that it will be used in other products, like APUs and maybe even CPUs in the future. How are the implementations so radically different in your mind?"
hardware,3bosvn,headband,-2,Wed Jul 1 02:35:25 2015 UTC,"an APU is basically a GPU...It's not in my mind...its in the specs for the devices. HBM is all about giant words where HMC is about reducing pincount, making it easier to branch out and eliminate delays waiting on the bus between multiple parts of the system."
hardware,3bosvn,willyolio,2,Wed Jul 1 02:37:16 2015 UTC,"latency.  video RAM is basically like a one-way highway.  Tons of information gets shoveled into the VRAM, which then just shoves all that information at the GPU.  The GPU puts out video.  It may as well be a 20-lane highway where everyone's destination is exactly the same and no lane changes are allowed.  System RAM is more like a city street.  Everything needs to go everywhere else.  You take info from RAM, move it to the CPU, CPU does calculations, sends it back to the RAM, takes some other information from elsewhere, does more calculations, etc. etc.  Tons and tons of back and forth.  you try to turn around on a 20-lane highway, you will fail hard.  You will sit on the side for an hour waiting for the lanes to clear before attempting a u-turn.  system RAM is made to be an efficient 2-way street with stops on every block and properly designed roundabouts."
hardware,3bosvn,imallin,3,Wed Jul 1 02:52:56 2015 UTC,"GDDR5 latency isn't bad by any stretch,  it is just power hungry, expensive,  and the bandwidth is completely unnecessary for CPU usage."
hardware,3bosvn,andromeduck,1 point,Wed Jul 1 03:03:12 2015 UTC,"It's basically cost and latency.   Cost because 500GB-2TB memory servers are a thing for compute oriented server's or cache related jobs.   Latency comes from their relative simplicity. Let's the CPU resume it's job much quicker.  GPUs are mainly bandwidth bound in memory because predicting it is too hard and expensive in terms of power and area so each unit basically has like 16 way hyper-threading which mitigates much of the blockage. Because similar nearby pixels need similar data it's cheaper to just load a big chunk at once and let it sort itself out.  GDDR5 gets higher bandwidth by being wider aka buffering data + more pins. Synchronizing these pins and filling the channel requires buffering and more timing control which leads to it being expensive, higher power etc.  It's like a Taxi vs School Bus as a means of transport."
hardware,3bosvn,MrPoletski,1 point,Wed Jul 1 01:46:26 2015 UTC,Woah... slow down. The first HBM GPU has only been out a week.  It came to GPU's first because of their bandwidth requirements.
hardware,3bosvn,reversethrust,1 point,Wed Jul 1 05:16:45 2015 UTC,I just wanted to add that IBM's Power8 memory controller can send a maximum of 230 gigabytes per second to the processor core - but good luck writing applications that can handle data that fast on a single processor package :). I think it manages that because from having a lot offset channels...
hardware,3boxr0,GenerationBlue,50,Tue Jun 30 23:30:09 2015 UTC,Why the fuck do I need Flash to view a couple of charts? They literally could have done this in a static html document for Christ's sake.
hardware,3boxr0,Sapiogram,27,Wed Jul 1 02:01:05 2015 UTC,Probably because it is ancient and no one at valve cares enough to fix it.
hardware,3boxr0,thetinguy,11,Wed Jul 1 02:17:55 2015 UTC,Kind of like their Android app
hardware,3boxr0,Pesceman3,9,Wed Jul 1 03:28:14 2015 UTC,That actually got an update about a month ago.
hardware,3boxr0,Nixflyn,8,Wed Jul 1 04:38:55 2015 UTC,It got an update a few days ago. Material (ish) design!
hardware,3boxr0,bobthemuffinman,2,Wed Jul 1 05:05:48 2015 UTC,"That Google, for some reason, still hasn't pushed to my tablet :|"
hardware,3boxr0,computertechie,1 point,Wed Jul 1 12:35:39 2015 UTC,"Which tablet, if I may ask?"
hardware,3boxr0,fliphopanonymous,1 point,Wed Jul 1 15:01:41 2015 UTC,"Galaxy Tab 4 7"". Still hoping for a Lollipop upgrade for it, too :("
hardware,3boxr0,computertechie,3,Wed Jul 1 15:05:09 2015 UTC,"That's not Google's fault.  It's Samsung's.  Hence, why I don't buy Samsung Android devices.  Touchwiz can throw itself off a cliff."
hardware,3boxr0,animeman59,1 point,Wed Jul 1 23:44:59 2015 UTC,"Oh, I know full well the 5. update is up to Samsung. The Steam update isn't, though."
hardware,3boxr0,computertechie,1 point,Wed Jul 1 23:46:29 2015 UTC,Should be out this month.
hardware,3boxr0,fliphopanonymous,1 point,Wed Jul 1 15:45:27 2015 UTC,The design was updated but the functionality is still terrible
hardware,3boxr0,Pesceman3,1 point,Wed Jul 1 19:11:02 2015 UTC,When I had the steam app installed it destroyed my battery life.
hardware,3boxr0,hdshatter,5,Sat Jul 4 21:52:29 2015 UTC,If it makes you feel better they utilized webgl for the summer event. So it isn't like they can't.
hardware,3boxr0,MINIMAN10000,1 point,Wed Jul 1 02:10:26 2015 UTC,"Can't view it on my phone. Really Valve, no mobile devices?"
hardware,3boxr0,hyperduc,1 point,Wed Jul 1 04:51:50 2015 UTC,Hit cancel then check the box saying stop this page from making the pop up and hit cancel again. The only thing you can't see is the bars but the percentages are colour coded too so it doesn't matter.
hardware,3boxr0,mokahless,11,Thu Jul 2 04:24:33 2015 UTC,"I would like to see GPU percentages categorized by discrete vs integrated, seems like that would be more useful for determine where the consumers are than looking at the overall combined nvidia/amd market shares."
hardware,3boxr0,mechanical_animal,1 point,Wed Jul 1 03:07:35 2015 UTC,It's estimated Nvidia owns 75%-80% of the dedicated gpu market.
hardware,3boxr0,hdshatter,12,Sat Jul 4 21:49:28 2015 UTC,"I have to say, i'm astonished AMD still have 25% cpu representation o_O Would've assumed they would've been phased out more.    Nvidia and AMD seems to have a 2:1 ratio"
hardware,3boxr0,groundonrage,18,Wed Jul 1 02:10:09 2015 UTC,Nvidia and AMD seems to have a 2:1 ratio   Which doesn't even really begin to show how badly Nvidia is crushing Amd in the enthusiast gaming segment. The gtx 970 alone has a 2.5% percent share compared to the entire R9 200 Series combined which has just 0.82%. That's downright absurd when you consider the Nvidia 900 series has been out for a full year less than the Amd 200 series.
hardware,3boxr0,Flexo_3370318,28,Wed Jul 1 02:22:07 2015 UTC,"Something seems a bit off about those numbers. I suspect Steam might classify some 200 series cards as their hd 7xxx predecessors. How else would AMD maintain a relatively stable marketshare in this chart? And yes, I know that probably includes a lot of integrated graphics."
hardware,3boxr0,Exist50,11,Wed Jul 1 02:33:54 2015 UTC,It's possible that the R9 200 series on the chart only counts the 290/290X (Hawaii) chips. Still pretty bad though considering how long they have been around.
hardware,3boxr0,Flexo_3370318,1 point,Wed Jul 1 02:49:03 2015 UTC,There's a lot of AMD GPU owners who felt no need to upgrade from their 7000 series cards to R9 200 cards.  This might change a little bit depending on how the Fury (non-X) cards turn out.
hardware,3boxr0,animeman59,3,Thu Jul 2 00:02:15 2015 UTC,they're not poking for part numbers and specs they're just checking what the OS reports so unless a 2xx reports as 7xxx to the OS/BIOS I think we can throw this one out unless you have evidence otherwise
hardware,3boxr0,andromeduck,3,Wed Jul 1 19:11:36 2015 UTC,"Windows has been shown to report some Tahiti 200 series cards as if they were from the hd 79xx series. I'm not sure if its the same with Pitcairn, but I spend a lot of time on /r/buildapc and this was a relatively common occurrence."
hardware,3boxr0,Exist50,1 point,Wed Jul 1 19:14:30 2015 UTC,But that's probably out of sync drivers and fixed now right?
hardware,3boxr0,andromeduck,1 point,Thu Jul 2 06:20:31 2015 UTC,"I'd assume so, but I suppose it depends how the program reads the card."
hardware,3boxr0,Exist50,1 point,Thu Jul 2 06:23:07 2015 UTC,Doubt it. My 7950 reports as an Rx 200 series. I never flashed the BIOS or anything either. I have to assume that all the reporting is still out of whack.
hardware,3boxr0,phrstbrn,6,Thu Jul 2 12:59:19 2015 UTC,"Based on your phrasing you could say Intel is crushing Nvidia ""in the enthusiast gaming segment"" because of the most used graphic chipsets they have several of the top % slots for a fairly large chunk of the total percentage.  The 970 is an outlier because it broke a pricepoint trend, something your point about the age of the cards utterly ignores.  The 2xx cards were expensive when they came out, as were the high performance 7xx cards, which is why the cheapest, the 760 is at the top.  The price/performance ratio didn't shift until after the 970 came out, and by then it was too late. Seeing as how most of the 2xx series was rebrands of existing cards, your numbers are going to be a bit skewed.  Point being, this is not ""the enthusiast segment"", this is the portion of Steam that elected to participate in the survey.  Aside from the integrated graphics, look at how many lower end cards that survey is including, GT 400-600 cards, some giving a few % a piece.  Those are hardly ""enthusiast"" cards.  Many are older OEM / prebuilt and mobile solutions still in use.  If you want to look at the enthusiast only cards, meaning if you don't look at cards that are more than a few years up to a decade old, there is not as much of a landslide.  ""Enthusiasts"" tend to stay a bit more modern.  This list is representative of what is being used by Steam gamers, not computer enthusiasts. Many of these gamers are very casual gamers. Maybe wrack up a lot of hours in DoTA, LOL, TF2, etc, but are obviously not playing Witcher 3 or other modern and intensive games on those integrated graphics, ATI 9600, or gt 400 cards."
hardware,3boxr0,Probate_Judge,8,Wed Jul 1 03:00:53 2015 UTC,"Oh, there's a pretty big difference when counting only higher-end cards.      AMD (share) Nvidia (share)    7800 series 1.57 660 2.16     660 Ti 0.67     760 2.69   7900 series 1.95 670 0.74     680 0.51     770 1.74     780 0.76     960 0.64   R9 200 series 0.82 780 Ti 0.35     970 2.50     980 0.69"
hardware,3boxr0,masturbateAndSwitch,-2,Wed Jul 1 03:30:24 2015 UTC,"Murdering?  Crushing?  For companies who's net worth is as disproportionate as AMD's and Nvidia's are, those numbers are about right.  That is the thing that nvidia fanboys don't quite understand.  They mistakenly think of ""competition"" as a combat event where the is a set survivor at some set finale or end period.  In the business world, there is no end.  These two companies could go on for decades like this.  In business, not everyone needs to have an equal piece of the pie to have a successful business. (not that AMD is doing that, but that is the fault of internal management and business decisions as much as anything else). The competition of businesses, as it relates to the term anti-competitive, is where competition is a healthy thing, both for the involved companies and the consumer.  It drives motivation and innovation and keeps prices low.  When people talk of ""crushing"" and ""murdering"" you sound like Charlie Sheen talking about ""Winning!"". People talking like that are obviously buying into some sort of barbaric tribalistic team mentality.  This is what happens when testosterone fueled over competitive xbox kids are encouraged to get into PC gaming, the community goes to shit.  You guys are the equivalent to the guy who disrupts whole rooms and annoys people trying to enjoy a calm game by yelling, ""BOOM, HEADSHOT, NIGGER"" every time he captures a piece in a game of chess."
hardware,3boxr0,Probate_Judge,15,Wed Jul 1 03:56:12 2015 UTC,"For companies who's net worth is as disproportionate as AMD's and Nvidia's are, those numbers are about right.   There was a time when AMD and ATI were separately worth more than NVIDIA."
hardware,3boxr0,random_digital,8,Wed Jul 1 04:24:52 2015 UTC,"I don't doubt it.  Market's often drift over time, especially as society's interest in things develop, or are directed.  10-15 years ago, and even before, the PC gaming world was vastly different.  Razer, as an outstanding example, got it's start when marketers and engineers teamed up.  Things like that drastically changed the face of the industry.  Now everything is gamer branded and designed and pressed onto them.  Gaming glasses, chairs, headphones, monitors, mice, keyboards, cases, drives, etc.  And aside from that ...facade...if you will(eg, not all things are necessarily performance related but very much gamer branded, take for instance LED's or the skull on Intel's SSD's), you've got what I mentioned earlier.  Intel and Nvidia have done some serious damage to AMD though some questionable(and some provable) anti-competitive tactics.(not going to get into debates about which practices are what, some people will go so far as to deny anything is anti-competitive and I'm not in the mood). Nvidia also heavily markets and promotes and stratagizes their product releases, they've trickled out their latest line over the better part of a year now and rumors of the next chip being the 950 are already out weeks after the 980ti...  It is no longer just a product, exactly what a lot of ad companies want to achieve, but it is news, it is a way of life, it is this continual exciting new thing.  It's not a periodic release of new hardware, it's a revolving circus that keeps people entertained.  Much like some video games, it's designed to take advantage of human psychology, the way they're rolling out is a manipulation in and of itself. People become addicted to the ongoing hype train the same way they do to leveling their character in WoW.  The more people that pay attention and talk about Nvidia, the more people that end up buying it because, well, if everyone's talking about it, it must be good, right?  TLDR  They've got some masterful marketers, PR and admen working for the company at present, and are obviously not averse to doing other ...oblique... things that people aren't necessarily so comfortable with once they realize what is going on, things that even fans/addicts of the hype refuse to see.  Altogether it is a powerful force, it's not surprising to see them grow so large.  It is depressing however.  Humans are just not very smart."
hardware,3boxr0,Probate_Judge,2,Wed Jul 1 04:52:15 2015 UTC,These two companies could go on for decades like this.   Have you not seen AMD's financials recently?  Or are you referring to the possibility of the spirit of AMD/ATI continuing on in an acquisition?  Because the current AMD entity most certainly cannot go on for decades the way they are going right now.
hardware,3boxr0,Evidence_Of_Absence,-4,Wed Jul 1 05:41:07 2015 UTC,"You may want to go back and read that line and the couple that come after that.  Context matters.  Edit: ""This"" meaning The two companies do not need a 50-50 split of the market, it could remain at 75-25 or whatever it is depending on how you want to look at it. A lower market share =\= negative profits.  That AMD is losing money is coincidental."
hardware,3boxr0,Probate_Judge,6,Wed Jul 1 06:46:22 2015 UTC,"If you're talking specifically about AMD as a company and their current situation, I'm not sure I'm following.  I agree with your point in general (I. E.  Companies don't necessarily need high market share to succeed) , but not as applied to the consumer PC hardware industry over a multi-decade time span.   Marketshare is not ""coincidental"" relative to profits in that instance.   I'm going to bed but can clarify later if you want.   EDIT:  The short version is that for consumer PC hardware, the development costs for a new product are high and relatively fixed if you want to be competitive.  The consumer end has very few product differentiators to drive sales--there's pretty much raw performance, power consumption, and price.  The enterprise/corporate end has a bit more leeway with items like semi-custom chips for consoles, new technologies for the server market, etc.  Industries where low market share is sustainable typically have either low development/R&D costs or heavily differentiated product lines.  That's how the analog and RF portions of the semiconductor world have managed to sustain (relative to the digital sector anyway) more companies with smaller market shares, some of which are significantly smaller than the top dogs.  Even that's going away as 2015 is starting to see even more M&A than is typical for the semiconductor industry.  The point is, unless AMD can either pull off the most impressive corporate restructuring of the past decade to bring down costs (unlikely--they've been playing this game for a few years now and already pulled most of the tricks they can) or deliver significant growth in the enterprise market, they HAVE to regain some marketshare to survive as AMD long term.  75/25 and shrinking (and even worse in the CPU space) is not somewhere you want to be in an industry prone to snowball effect as the guy with more money can spend more on R&D.  I'm not saying it needs to be 50/50, and it isn't panic time quite yet, but when you're talking about DECADES they will need to claw some of that back."
hardware,3boxr0,Evidence_Of_Absence,0,Wed Jul 1 07:21:23 2015 UTC,"Not cool, dude. I actually edited my comment before your response, but still. Not cool."
hardware,3boxr0,masturbateAndSwitch,0,Wed Jul 1 04:09:26 2015 UTC,"I will acknowledge that it wasn't a ninja edit, but it was 20+ minutes after your original post, and only 2 minutes between my reply and your edit. (RES shows post and edit times)  I started my reply long before you edited and therefore I did not see the change before I posted. You removed a lot of text, including the word ""murdering"".  *shrugs  That will happen on occasion."
hardware,3boxr0,Probate_Judge,1 point,Wed Jul 1 04:20:01 2015 UTC,"I stand by my use of the word ""murdering"" to describe the disparity in market share. The companies are in direct competition with each other, and one is clearly winning. For years the status quo has been Nvidia ~60% AMD/ATI ~40%, but lately, it's become very lopsided. The difference in market share translates directly to a difference in revenue, which creates a difference in income, which creates a difference in R&D and marketing spending, which contributes to a difference in market share. AMD's CPU and GPU businesses are both doing very badly right now, and it's not sustainable. The company will fail unless they right the ship or refocus."
hardware,3boxr0,masturbateAndSwitch,-4,Wed Jul 1 04:29:44 2015 UTC,http://bigriveradvertising.com/blogs/fredmoore/files/2012/03/charliesheen.jpg  Maybe you should have just gotten Direct TV
hardware,3boxr0,Probate_Judge,1 point,Wed Jul 1 04:54:42 2015 UTC,I don't get it.
hardware,3boxr0,masturbateAndSwitch,1 point,Wed Jul 1 04:57:11 2015 UTC,"Just a point to the Intel crushing NVIDIA, I've heard this survey reports the display adapter used to render the primary desktop (and all reason would agree). This means any laptop with switchable graphics (that tech that uses iGPU for desktop operation then kicks in NVIDIA GPU for gaming - lots of modern gaming laptops) would report Intel instead of NVIDIA, despite an NVIDIA GPU being in there and used for games. Might not knock Intel off majority but would definitely be a factor."
hardware,3boxr0,pb7280,2,Fri Jul 3 03:40:41 2015 UTC,"Yeah, and sort the graphics card chart by ""% change this month"": The GTX 970 increased in share by 0.20 percentage points, while the entire R9 200 series-- that's 8 cards from midrange to high-end, only increased in share by 0.03 percentage points. Some of that is upgrades within the R9 200 series, but that's still a huge difference. The only AMD stuff that's increasing at a reasonably high rate is their integrated graphics.    Edit: And that integrated graphics (AMD Radeon R7 graphics) went from ""-"" to 0.16% in a month, so it might just be a driver name change or something"
hardware,3boxr0,masturbateAndSwitch,1 point,Wed Jul 1 02:54:22 2015 UTC,"My 8150 is still running more than fast enough, no reason to phase it out"
hardware,3boxr0,BeatLeJuce,1 point,Wed Jul 1 13:40:14 2015 UTC,Probably a lot of people like me who built phenom x4 systems years ago and haven't had much reason to change yet.. I got a helluva deal on mine when I purchased
hardware,3boxr0,siscorskiy,1 point,Wed Jul 1 17:55:31 2015 UTC,"The Phenom series is still pretty good, and very affordable.  Same with their APU, and FX series.  For almost a year now, I've been giving people build advice for their PCs (local friends, friends of friends, and people in my TS circles), and the majority of them were really only concerned about playing things like LoL, DotA2, Team Fortress, and CS:GO.  If those are the games that they'll mostly be playing, then there's no need for a $200+ CPU in a build.  Same with an expensive motherboard or even a beefy video card.  With that in mind, you can see how the majority of those builds had AMD hardware inside.  Much cheaper than going the Intel/Nvidia route (which is what's inside my build).  Only a small percentage of those folks needed a machine that could play some modern titles at 1080p/High settings.  Then it's a toss up between AMD, and Nvidia depending on budget and expectation.  For those who wanted the absolute best machines you can build (around $2000 to $3000), I can count those people with one hand, and those were the Intel/Nvidia builds.  So I can see how AMD still has a good chunk of the CPU market, along with the GPU market.  But I really hope AMD goes 1:1 with their competitors in the future.  Having better competition means better hardware for everyone.  Nvidia's release of the GTX 980 Ti, because of their wariness of the Fury X, shows how this benefits the consumer."
hardware,3boxr0,animeman59,3,Wed Jul 1 23:59:09 2015 UTC,What processor has 5 cores?
hardware,3boxr0,Corncove,8,Wed Jul 1 08:48:18 2015 UTC,A broken one?
hardware,3boxr0,fgdadfgfdgadf,6,Wed Jul 1 10:49:57 2015 UTC,"Anyone remember triple-core CPUs? AMD made them for a while - they were effectively quad-cores, but one core didn't make it past testing and was deactivated. After a while, demand for those cheap triple cores had risen to the point that they started deactivating one core of perfectly fine quad-core CPUs, which is why I was able to reactivate it on my previous CPU."
hardware,3boxr0,DdCno1,3,Wed Jul 1 17:14:07 2015 UTC,"yup my buddy did that. that was a pretty decent, cheap CPU"
hardware,3boxr0,prodikl,1 point,Wed Jul 1 17:38:35 2015 UTC,"Built a few Athlon X3 445 based machines for the office, they are great CPU's and we were able to flash a few of them to quad core. Running strong for like 5 years now and they easily handle adobe apps and CAD. Best economy CPU's ever"
hardware,3boxr0,poematik,1 point,Wed Jul 1 22:34:31 2015 UTC,Mine lasted me five years as well and was only recently replaced by a faster CPU. A relative of mine is using it now and was happy about the upgrade. I'd expect it to work for a few more years.
hardware,3boxr0,DdCno1,1 point,Wed Jul 1 22:43:57 2015 UTC,"Yeah they are great. We recently upgraded the desktops with nvidia GT730's (i know, not a gaming card) and gave them all SSD's, and they are even faster now, for such a modest price."
hardware,3boxr0,poematik,1 point,Thu Jul 2 01:38:40 2015 UTC,Interesting that there isn't a big drop in AMD GPU numbers.
hardware,3boxr0,Exist50,6,Wed Jul 1 02:00:45 2015 UTC,A drop in sales transfers to a much slower drop in usage in systems.
hardware,3boxr0,finnybro,1 point,Wed Jul 1 02:43:02 2015 UTC,"Some other charts report a much more drastic swing in ownership towards Nvidia, not just sales. Remember, most people buying a GPU replace their old one."
hardware,3boxr0,Exist50,-1,Wed Jul 1 06:03:43 2015 UTC,Reading these comments reminds me how stupid both AMD and Nvidia fan boys sound when blabbering on about their prefered company.
hardware,3boxr0,thejshep,0,Wed Jul 1 09:42:02 2015 UTC,I would love to see a bell curve of the gpu share based on which tier the card is (like the one on toms hardware).
hardware,3bph1q,zmeul,11,Wed Jul 1 02:38:07 2015 UTC,What year is this ?
hardware,3bph1q,mariusg,3,Wed Jul 1 21:24:20 2015 UTC,"On the Apple Calendar, the year where they introduced the awesome concept of trim to the masses."
hardware,3bph1q,SteelChicken,2,Thu Jul 2 12:40:20 2015 UTC,Is there any reason to use this over trim enabler?
hardware,3bph1q,smoothblueglue,21,Wed Jul 1 07:02:22 2015 UTC,You've got the question backwards. Is there any reason to use a third-party software when a native solution exists?
hardware,3bph1q,58592825866,5,Wed Jul 1 09:35:14 2015 UTC,Is there any reason why they didn't have this in the first place?
hardware,3bph1q,animeman59,2,Wed Jul 1 23:41:59 2015 UTC,"As with MMS on the first iPhone, Apple had not invented it yet so it could not exist in their universe. /s"
hardware,3bph1q,joyfield,5,Fri Jul 3 12:13:03 2015 UTC,It's free? Also it's a system solution versus a third party solution so that's (usually) good.
hardware,3bph1q,0x60,4,Wed Jul 1 07:14:08 2015 UTC,Also might be permanent (i.e. not need fiddling every time you do an OS update).
hardware,3bph1q,Exist50,6,Wed Jul 1 07:38:36 2015 UTC,With Trim Enabler you had to disable Kext Signing. Not a good idea
hardware,3bph1q,JustFinishedBSG,2,Wed Jul 1 07:55:06 2015 UTC,"And breaks the boot process if one clears the nvram, though fixable if you know what to look for. I did that one time because I had an issue with OS X and didn't know about the issues with TRIM Enabler and kext signing (RTFM, I guess) and had to reinstall OS X completely."
hardware,3bph1q,Strayer,1 point,Wed Jul 1 10:09:50 2015 UTC,You can boot on the recovery partition / install disk and repair the kext signing / boot process there :)  Still annoying
hardware,3bph1q,JustFinishedBSG,1 point,Wed Jul 1 11:19:01 2015 UTC,Yeah I noticed that. Is there a way to reverse it with out screwing things up?  Edit: When you disable trim enabler you get a prompt to re-enable kext signing. So I guess I'm good. Is there a simple terminal command for checking if kext signing is enabled?  Also I've heard some SSDs don't work with this.
hardware,3bph1q,smoothblueglue,1 point,Wed Jul 1 14:49:37 2015 UTC,"trim enabler resets after every OS update, and it always begs you to pay them."
hardware,3bph1q,poematik,2,Wed Jul 1 07:53:33 2015 UTC,"Wooo, Apple is catching up, big news."
hardware,3bnh9q,TaintedSquirrel,27,Tue Jun 30 17:07:56 2015 UTC,"No VRAM cooling whatsoever...  Also, similar heatpipe issue we've seen in the past."
hardware,3bnh9q,BlayneTX,12,Tue Jun 30 17:43:22 2015 UTC,Looks like this cooler is made for some other card. Fury X?
hardware,3bnh9q,MCJeeba,8,Tue Jun 30 18:24:33 2015 UTC,I think it's for the air cooled Fury. Look at how big the heatpipe area is.
hardware,3bnh9q,BlayneTX,4,Tue Jun 30 18:29:48 2015 UTC,Oh yeah that's what I meant.
hardware,3bnh9q,Exist50,2,Wed Jul 1 00:22:08 2015 UTC,Fiji and GM200 are very close in die size. Those extra pipes might be for the HBM.
hardware,3bnh9q,MCJeeba,2,Tue Jun 30 22:41:06 2015 UTC,That's what I meant. The entire package.
hardware,3bnh9q,CEKPETHO,2,Tue Jun 30 23:59:20 2015 UTC,It's possible they designed this cooling solution for future GPUs because next year Nvidia is also moving to HBM. May explain lack of VRAM cooling. I also remember Titan X having VRAM on the back of the PCB with no backplate so maybe it doesn't really need heatsinks on VRAM?
hardware,3bnh9q,Exist50,2,Wed Jul 1 02:27:54 2015 UTC,"Well, the Fury does use HBM, so that's a possibility. Still, Asus has plenty of time to work on new cooling solutions for the 2016 cards. This one isn't anything revolutionary."
hardware,3bnh9q,CEKPETHO,1 point,Wed Jul 1 06:22:55 2015 UTC,"For the AMD 200 series Asus pretty much slapped coolers that were designed for Nvidia cards and all those cards ran hot because of it. Something about poor contact with GPU or something, I'm not entirely sure. It's possible this time they designed the coolers with HBM in mind for Fury and next year's Nvidia pascal cards and just slapped it on the 980ti. People have said in this post that performance of this cooler on this card isn't anything amazing. I could be wrong but this is one possibility."
hardware,3bnh9q,Exist50,1 point,Wed Jul 1 06:49:02 2015 UTC,"That was mostly for the 290 and 290x. They reused the cooler for the 780, a significantly larger GPU."
hardware,3bnh9q,drogean3,17,Wed Jul 1 07:33:01 2015 UTC,This is HUGE  Buyer Beware  Many of us AMD users got BURNED with their last generation of cards that had this same exact issue  ASUS is not the company you remember  https://rog.asus.com/forum/showthread.php?43057-R9-280x-DirectCU-II-Top-Artifacts!!   64 pages of UNRESOLVED defective card complaints  http://www.newegg.com/Product/Product.aspx?Item=N82E16814121803  almost 50% of reviews under 3 stars  source: Asus 280x user
hardware,3bnh9q,Nixflyn,11,Tue Jun 30 21:39:22 2015 UTC,Yeah I had two faulty ASUS 280X's.  I don't buy their products anymore because their customer service is so awful.
hardware,3bnh9q,Exist50,6,Tue Jun 30 21:40:52 2015 UTC,"I don't think that was necessarily a problem with Asus' cooling solution, but rather the 20%+ failure rate of the 200 series cards."
hardware,3bnh9q,Nixflyn,1 point,Wed Jul 1 04:56:50 2015 UTC,"It seems disproportional to a few manufacturers, though, like Asus and earlier on, MSI and Powercolor. Given how much of the 200 series were rebrands (presumably reliable from AMD), I can't see the 290 and 290x alone being enough to shift the failure rate."
hardware,3bnh9q,Kobayakawamiyuki,2,Wed Jul 1 06:21:19 2015 UTC,"Which brands use custom PCBs on which cards? It might have been an issue with reference specs. Sadly, it's very difficult to figure out the failure rate of each model of each card, but I'd definitely like to know."
hardware,3bnh9q,Logun0,1 point,Wed Jul 1 06:40:52 2015 UTC,"To be fair, there were many problems with the last gen AMD cards when they were released. If I remember correctly, MSI and Gigabyte also had issues with artifacting on the 280x. The 290/290x's also had some reliability issues. Sapphire seemed to have the best reliability, but even they were on the lower end of the 700 series reliability. Was the main reason why I initially went for a 760 before upgrading to a 780. I can't really remember, but I think Gigabyte was the only ones that had issues with their 700 series cards."
hardware,3bnh9q,Exist50,5,Tue Jun 30 21:52:54 2015 UTC,"Given the width of the chip - it looks like 5 pipes are in excess of what is needed to completely cover the chip, so does that mean then that The 5th pipe is not needed or...?    Also what are the implications of not having VRAM cooling? Will this lower the life of the card or just limit it's overclocking?"
hardware,3bnh9q,Exist50,12,Tue Jun 30 19:44:21 2015 UTC,"EVGA had a similar problem and this is how they responded:  http://www.eteknix.com/evga-respond-possible-design-flaw-gtx-970-acx/  And they ""fixed"" the issue with their 2.0+ model by abandoning HDT entirely:  http://i.imgur.com/rf92rzK.jpg"
hardware,3bnh9q,Kobayakawamiyuki,3,Tue Jun 30 19:46:10 2015 UTC,"Also, the Asus DirectCU II Hawaii cards: http://s458.photobucket.com/user/owikh84/media/290X%20DC2OC/DSC_0785.jpg.html"
hardware,3bnh9q,nekura_,1 point,Tue Jun 30 22:36:02 2015 UTC,So tiny.  They should use wider metal on top of the die for more heat dissipation area.  Does that even make sense (?).  Sort of like a heat spreader.
hardware,3bnh9q,Kobayakawamiyuki,3,Tue Jun 30 22:37:30 2015 UTC,This is how sapphire did it (tri-x): http://images.anandtech.com/doci/7601/S290HSF_Back.jpg  http://images.anandtech.com/doci/7601/S290HSF_Front.jpg  and vapor-x: http://www.pureoverclock.com/wp-content/uploads/2014/04/1sapphire-R290-voc.jpg
hardware,3bnh9q,Zarknox,6,Tue Jun 30 22:43:48 2015 UTC,"The two outer pipes will still help carry away heat despite not being in direct contact with the chip. It just isn't as effective as if all the pipes where connected or ideally if the chip covered all of the pipes. It will lower temperatures, just not nearly as well as the three in the middle.   As far as the VRAM cooling, many cards don't have it. In all likelihood, not much will actually happen except higher temps on the VRAM. Overclocking is still fine too. My Asus 780 DCUII is oc'ed +220 on the boost and + 400 on the memory on top of the stock clocks on stock voltage for well over a year now, often 24/7."
hardware,3bnh9q,Kobayakawamiyuki,5,Tue Jun 30 20:16:25 2015 UTC,Can you be more specific?
hardware,3bnh9q,XaeroR35,13,Tue Jun 30 18:03:19 2015 UTC,"Not all pipes are touching the chip, as well as the placement of the pipes over the chip. Nothing touching the VRAM so it is just getting whatever air gets to the circuits."
hardware,3bnh9q,SeaJayCJ,7,Tue Jun 30 18:10:43 2015 UTC,"Just curious, how is that an issue? Wouldn't it be ""overdoing"" it, and therefore no negatives? Just because its not specifically perfect for this card I don't see why it'd be bad besides the VRAM thing"
hardware,3bnh9q,Exist50,7,Tue Jun 30 21:45:36 2015 UTC,"Yes, it still does benefit the card in cooling, but it could also be designed better in order to save money on the manufacturing process. Copper is one of the more expensive metals there is. Also, adding in the heat pipes adds in more complexity and more points of failure both on the card and in manufacturing.  However, these aren't really problems that you as a consumer would have to deal with. The MSRP on non-reference cards seems to be ~$10-$20 over reference no matter what the design. The issue is probably paid more by the manufacturer's in cost of production than the consumer in cost of the card.  Me personally, I like having more pipes over the side of the chip. Also, the VRAM isn't really an issue. As long as there are fans blowing air near it, it is likely fine."
hardware,3bnh9q,SeaJayCJ,2,Tue Jun 30 22:06:45 2015 UTC,Why would they not cool the VRAM?
hardware,3bnh9q,narutoninjakid,2,Tue Jun 30 20:05:10 2015 UTC,"I didn't pick up on that. My GTX 770 DCU II has a VRM sink, why doesn't the supposedly-new version have it? Wtf, Asus!"
hardware,3bnh9q,Mr_Snail10,1 point,Wed Jul 1 04:57:23 2015 UTC,"vram, not vrm. The vrms are cooled in some way, at least."
hardware,3bnh9q,Acalys,1 point,Wed Jul 1 06:24:57 2015 UTC,You are correct. I didn't see that the VRM plate was attached to the main cooler. On my card it is a separate sink.
hardware,3bnh9q,nekura_,1 point,Wed Jul 1 06:34:44 2015 UTC,SO at this point is the Gigabyte 980Ti a better card for cooling ? I was looking to get maybe the Poseidon version of this or the EVGA 980Ti Hybrid but it seems its never in stock. Since im getting the Swiftech H240x i might just get an air card and expand the loop.
hardware,3bnh9q,Techdestro,7,Wed Jul 1 12:13:27 2015 UTC,"Slightly underwhelming to be honest.   Although I'll probably wait for a ""legit"" review before judging."
hardware,3bnh9q,TheMirrorVision,6,Tue Jun 30 20:21:54 2015 UTC,At $700 this really does not look promising although some of those benchmarks seem iffy. That's the same price as an EVGA Classified.
hardware,3bnh9q,nekura_,2,Tue Jun 30 19:22:53 2015 UTC,"I'm gathering parts (Define S, EKWB) for a Skylake build. I had planned on going with the Strix, but I guess the classy makes more sense."
hardware,3bnh9q,Razultull,4,Tue Jun 30 20:33:37 2015 UTC,I still love my 780ti classy with WC. Overclocked and cool:)
hardware,3bnh9q,attomsk,2,Tue Jun 30 20:50:51 2015 UTC,Apparently an EKWB for the 980 Ti Classy is currently up in the air... Hopefully they end up making one or else I'm going to be returning my 2 for some other custom-PCB option.
hardware,3bnh9q,Razultull,2,Wed Jul 1 00:49:01 2015 UTC,"yeah I read the EK response on overclock.net and EVGA forums. I bought everything except a rad and GPU block, so hopefully they will figure it out by the time Skylake is good to go."
hardware,3bnh9q,attomsk,3,Wed Jul 1 00:56:34 2015 UTC,"Well the temperature and noise results aren't very impressive at all. Those stock temps are a bit off too, that can't be the default fan curve."
hardware,3bnh9q,Zarknox,1 point,Tue Jun 30 18:34:58 2015 UTC,"The 980Ti is a beast to cool, those temps seem normal to me except I agree his reference 980 Ti seems unusually cool, usually those are at the high 70's low 80's"
hardware,3bnh9q,Exist50,1 point,Tue Jun 30 19:11:12 2015 UTC,I get the same temps on a modified mid-aggressive fan curve with the reference blower... Idk
hardware,3bnh9q,Zarknox,2,Wed Jul 1 13:04:13 2015 UTC,Gaming results are similar to my Gigabyte G1 980 Ti.  For comparison:   Firestrike: 17014 Firestrike Extreme: 9156   It probably would have been more useful if he had just listed GPU scores from 3DMark  Note: I have a 4790k @ 4.7GHz he has a 5930k @4.5GHz  Seems like a decent card.
hardware,3bnh9q,attomsk,1 point,Tue Jun 30 19:15:50 2015 UTC,"Question, what is your monitor? I've had a few issues with my g1 and I have 13,085 on firestrike. Is it because I'm using 1440p and you might be using 1080p?  http://www.3dmark.com/3dm/7575028"
hardware,3bnh9q,Zarknox,2,Wed Jul 1 05:17:27 2015 UTC,"Your results appear to be behind ""similar systems"". Might want to look into that."
hardware,3bnh9q,attomsk,1 point,Wed Jul 1 06:26:10 2015 UTC,"So 1440p doesn't cause the lower score?  I found that section, and it looks like the ones around me have the gpus clocked around 1000-1200 mhz core and the ones up around 15,000 have it clocked around 1400 ish so I'm gonna test and see if mines not going to the right clock speeds maybe  EDIT: did some tests.. if its not the monitor then IDK  http://imgur.com/a/bEGeb"
hardware,3bnh9q,Zarknox,2,Wed Jul 1 06:30:12 2015 UTC,"No, Firestrike runs at 1080p on my machine as well.  What you are seeing is your total score being lowered because your CPU score is low.  Take a look at your GPU score - its at 20157 which is great.  Your GPU is definitely kicking butt."
hardware,3bnh9q,attomsk,1 point,Wed Jul 1 07:24:23 2015 UTC,"OHH, didn't realize CPU mattered that much in firestrike etc. I saw a lot of OC 3570ks when I looked through results  I may try that"
hardware,3bnh9q,Zarknox,2,Wed Jul 1 07:27:11 2015 UTC,Cpu will bring scores down if they are below the gpu score but rarely increase them if they are above the gpu score
hardware,3bnh9q,attomsk,2,Wed Jul 1 07:38:04 2015 UTC,"I can't tell what my CPU score is, I assume it is the issue though because all the ~15,000 range are all overclocked to like 4.5ghz. Also how in the world did you get 17,000?"
hardware,3bnhae,poematik,25,Tue Jun 30 17:08:05 2015 UTC,"If its the same price as the 750ti and performs better, we have a real budget winner"
hardware,3bnhae,screwyou00,42,Tue Jun 30 20:06:46 2015 UTC,It also needs to not use anything other than the PCIe bus for power. That's the main reason why the 750ti is so popular. It's one hell of a card for only requiring the PCIe bus.
hardware,3bnhae,poematik,12,Tue Jun 30 20:16:02 2015 UTC,"yeah, i love low prof bus powered cards. so satisfying to just click it in and forget about it."
hardware,3bnhae,thinkythought,5,Tue Jun 30 20:25:58 2015 UTC,"All it really needs to do is beat the 370 for less money to be an awesome deal, honestly. And the 270x already does that."
hardware,3bnhae,163941,15,Tue Jun 30 22:30:17 2015 UTC,"I was really hoping that we would see a 960 Ti rather than a 950 Ti.  The 960 is under-performing relative to where it should be in the line-up, and I'm planning on a picking up a GM206 card as something to hold me over until the next generation. Too many games are hitting the 1.3GB VRAM limit of my GTX 570 now.  It's too bad that the GM200 cards are lacking the video decoding capabilities of GM206, because I would probably be looking to pick up a 980Ti instead."
hardware,3bnhae,feyenord,4,Tue Jun 30 22:48:50 2015 UTC,Yeah the 960 is really unattractive in price and performance (at least here in EU) and there's a huge hole between it and the 970. I'm probably just going to go with a used 280x (50€ cheaper here and 20% more performance) myself and wait for Pascal if they don't release anything by fall.
hardware,3bnhae,Omnislip,2,Wed Jul 1 05:59:36 2015 UTC,I went for the 280x too - minor regrets as it cooks me in my bedroom with 30 degree weather at the moment though!
hardware,3bnhae,JimJamJamie,1 point,Wed Jul 1 09:49:22 2015 UTC,"I have an overclocked 290, please tell me more of your plight"
hardware,3bnhae,163941,1 point,Fri Jul 3 14:13:01 2015 UTC,"Yeah the 960 is really unattractive in price and performance (at least here in EU) and there's a huge hole between it and the 970. I'm probably just going to go with a used 280x (50€ cheaper here and 20% more performance) myself and wait for Pascal if they don't release anything by fall.   I generally agree, but the video decoding hardware is what's pushing me towards the 960.  I had been hoping for a 960 Ti, but it sounds as though the 960 is the ""full"" GM206.  I'm not sure that Pascal cards will be out this year - I thought they were supposed to be using HBM2, and that's not due until 2016.  Either way, I'm planning on waiting until Metal Gear Solid V launches around September before buying a card.  Hopefully NVIDIA will be offering a copy of that when buying a new card, which would help offset the price. But anything would be better than the current Batman promotion."
hardware,3bnhae,fauxnick,1 point,Wed Jul 1 10:50:59 2015 UTC,"They can fit a Ti and a Ti Boost in that gap if you ask me! If I can recall it correctly it was the 650 Ti Boost that was such a great deal just a while ago.  Maybe the 960 can surprise us with a Ti and a Ti Boost. Mid-range cards are awesome, you can play most of the latest games in high settings for a fraction of the price of a high range, they keep their value better and so you sell them every year to someone who wants SLI and you get the latest version again! You can't future proof against new connections/compatibility and your own lust for a new card. Meanwhile you run cool and compact."
hardware,3bnhae,mduell,2,Thu Jul 2 09:24:56 2015 UTC,But the 960 is a full GM206 so a Ti would have to be a cut GM204 which raises the cost considerably.
hardware,3bnhae,163941,1 point,Wed Jul 1 05:40:47 2015 UTC,"But the 960 is a full GM206 so a Ti would have to be a cut GM204 which raises the cost considerably.   Good to know. I just thought they needed something in-between a 960 with 1024 CUDA Cores and a 970 with 1664 cores.  Perhaps a card with 1280 cores to replace the 960 at its current price.  If the 960 is a full GM206, I guess that means I'll be getting a 4GB 960 to hold me over until the next generation of cards.  If I'm going to buy something now instead of waiting, I want that 10-bit 4K H.265 video decoding, and GM206 cards are the only ones which have it."
hardware,3bnhae,TheImmortalLS,1 point,Wed Jul 1 10:22:30 2015 UTC,The mobile 970/980 chipsets are in this range.
hardware,3bnhae,Bingoose,0,Thu Jul 2 07:13:29 2015 UTC,If you're getting it for gaming I wouldn't bother. The 128-bit memory bus makes 4GB useless.  For video encoding though the 960 is the obvious choice as you say.
hardware,3bnhae,ZeM3D,1 point,Wed Jul 1 12:37:08 2015 UTC,Isnt that what the Ti naming implies?
hardware,3bnhae,everyZig,2,Wed Jul 1 06:58:41 2015 UTC,"No, Ti chips have both been fully enabled chips and cut-down chips in the past, there is no real trend there.  And even if there were, the reality is that there is nothing on a GM206 chip to enable so it becomes a 960Ti, they'd need to cut down GM204 even further or develop a new chip as an intermediary between the 960 and 970"
hardware,3bnhae,I-never-joke,1 point,Wed Jul 1 07:24:26 2015 UTC,Is this a joke?
hardware,3bnhae,newspaper_nerd,3,Wed Jul 1 01:58:01 2015 UTC,ETA?
hardware,3bnhae,IsaacM42,2,Wed Jul 1 02:24:14 2015 UTC,Can't say due to NDA.
hardware,3bnhae,rePAN6517,6,Wed Jul 1 01:41:40 2015 UTC,"If they could make a low profile 950ti, that would be sweet."
hardware,3bnhae,Vince789,-1,Wed Jul 1 02:01:08 2015 UTC,My dream.
hardware,3bnhae,perkam,7,Wed Jul 1 05:42:54 2015 UTC,How about something to fit in the $200-$300 price range?  There is absolutely nothing in there on the NVidia side.
hardware,3bnhae,rePAN6517,3,Wed Jul 1 05:43:56 2015 UTC,"Nvidia are probably waiting until there's something there from AMD, i.e. a 380X"
hardware,3bnhae,leafthegreen,1 point,Tue Jun 30 22:38:20 2015 UTC,"380X should be a godly card, but the 390 on Black Friday will be the best deal."
hardware,3bnhae,rePAN6517,-1,Wed Jul 1 01:40:07 2015 UTC,960?
hardware,3bnhae,gandalfblue,5,Fri Jul 3 19:07:10 2015 UTC,960 is only a 2GB card and can be had for under $200.  280x is also at that $200 price point and is a lot faster.  I really wish NVidia would come out with a 960 ti to close the huge gap between a 960 and 970.
hardware,3bnhae,Dullahan3470,4,Wed Jul 1 02:00:35 2015 UTC,"960 4GB edition? That's a little deeper into the $200 range, and while average FPS sees little increase from the 2GB edition on most benchmarks I've seen, it can really do a number (in some games) on 1% and 0.1% rates."
hardware,3bnhae,everyZig,4,Wed Jul 1 02:28:25 2015 UTC,"settle down, beavis"
hardware,3bnhae,Aldarro,2,Wed Jul 1 05:08:08 2015 UTC,Here's hoping for a passively cooled version for the ultimate htpc.
hardware,3bnhae,Aldarro,2,Wed Jul 1 02:32:57 2015 UTC,Isn't the 750Ti already Maxwell architecture?
hardware,3bnhae,wasdzxc963,11,Wed Jul 1 02:44:53 2015 UTC,"It is, but its maxwell V1.0, whereas all the GM2xx chips are maxwell 2.0, there are some feature differences.  And FWIW, the gap between the 750Ti and 960 could use something, and in the last few generations, a x50 series card lasts 18 months, meaning a 950(Ti) would be due in august."
hardware,3bnhae,MilkyTones,6,Wed Jul 1 15:16:40 2015 UTC,"Nah, it's 1st-generation Maxwell. Has the power efficiency benefits, but lacks most of Maxwell's advanced features."
hardware,3bnhae,honacc,4,Tue Jun 30 20:17:00 2015 UTC,Yop you right I am retarded.
hardware,3bnhae,fuzzycuffs,6,Tue Jun 30 20:22:03 2015 UTC,No worries! It being a part of the 700 series is a bit confusing. :)
hardware,3bnhae,honacc,0,Tue Jun 30 20:22:33 2015 UTC,"No, I was on several integrated product teams that worked on them. I should have remembered. It was not even that long ago. It just gets a bit confusing the older you get the more junk you have stored in your brain. And brains are not like SSD's, they have lots of issues akin to Mechanical Hard Drives lol."
hardware,3bnhae,Whats_a_narwhal,2,Tue Jun 30 20:35:55 2015 UTC,Why does Nvidia reserve the Ti suffix for just their flagship and low-end cards? It doesn't make sense.
hardware,3bnhae,HavocInferno,8,Tue Jun 30 20:45:58 2015 UTC,"The Ti suffix isn't reserved for anything  It just means Ti is better than no Ti, e.g. for the mid range 700 series 770 > 760 Ti (OEM) > 760  Its similar to AMD's X (e.g. 280 > 270X > 270) or before Ghz Edition (7950 > 7870 Ghz edtion > 7870)"
hardware,3bnhae,Tropcop,2,Tue Jun 30 20:52:24 2015 UTC,There comes the gtx 750 rebrand
hardware,3bnhae,drnick5,2,Wed Jul 1 02:00:11 2015 UTC,I still don't get why... There's 960 and 970 in a quite affordable range
hardware,3bnhae,luger718,30,Wed Jul 1 01:33:37 2015 UTC,Because you don't waste silicon. Whatever didn't bin for the 960 you sell as the 950Ti.
hardware,3bnhae,Yearlaren,1 point,Wed Jul 1 01:45:23 2015 UTC,"True, this seems to be a very common practice nowadays."
hardware,3bnhae,luger718,1 point,Wed Jul 1 10:43:22 2015 UTC,That's literally always how it has been done. This is not new at all.
hardware,3bnhae,rockycrab,1 point,Tue Jun 30 19:21:11 2015 UTC,It's been common practice since the birth of microchips.
hardware,3bnhae,Pillowsmeller18,15,Tue Jun 30 19:27:49 2015 UTC,"The 960 is closer to ""midrange"" than ""budget"".  What we could use is another price/performance landmark along the lines of a Radeon 7770 from 2012. That card was like $90 and it ran Far Cry 3, one of the most technically advanced games at the time, on medium-high with framerate in the 50s. It was effing remarkable.  The closest thing to it right now is the R7 250X which us just a rebranded 7770. While it was a great card 3 years ago, it's definitely showing its age now."
hardware,3bnhae,DoTheEvoIution,9,Tue Jun 30 21:26:58 2015 UTC,The 750ti is the fastest card you can get without upgrading your power supply (it doesn't need a 6 pin power cord) so I'm guessing they are trying to keep that market segment happy with a refresh.
hardware,3bnhae,LiberDeOpp,5,Wed Jul 1 10:28:43 2015 UTC,Yeah there's a large amount of people who only play one or two games on PC and nothing else. People who play league or dota or any mmo. Or for backup PCs. Cheap video cards are also good for making a budget pc faster since video and a few other things can be offloaded to it.
hardware,3bnhae,DoTheEvoIution,1 point,Wed Jul 1 16:39:27 2015 UTC,Or low resolution gamers like me (1600x900).
hardware,3bnhae,SPOOFE,3,Tue Jun 30 20:59:13 2015 UTC,"Yeah I use to game on a 19"" 1440x900, my 560ti was a beast. Now I have a 1080p monitor and its struggling."
hardware,3bnhae,DoTheEvoIution,5,Tue Jun 30 21:21:17 2015 UTC,You normally can't find 960s or 970s for $150 or less
hardware,3bnhae,SPOOFE,4,Tue Jun 30 21:35:03 2015 UTC,"I'm in the Philippines right now. Whatever electronics store I go to, they sell these (GTX x50's) as their top card (last time I looked it was the 650 which was Feb 2015). I have to go to specialized PC vendors for the better cards."
hardware,3bnhae,DoTheEvoIution,-11,Wed Jul 1 02:38:54 2015 UTC,"its replacing $150 gtx750ti, which is shit performance wise, since its even beaten by gtx650ti boost, not to mention plethora of 265, 260x, 270 that lurks in similar price range  gtx960 is shit as well, at least considering its the brand new architecture that just recently came out... damn how can it only trade blows with 285...  and I dunno since when its normal to just have $300+ price bracket category..."
hardware,3bnhae,SPOOFE,13,Wed Jul 1 02:54:36 2015 UTC,I got the 750ti for 100 bucks and it oc's like a champ. I think you're forgetting the 750ti was the first maxwell card and uses the pci slot as the only power source.
hardware,3bnhae,DoTheEvoIution,-10,Tue Jun 30 21:11:46 2015 UTC,"Not all of them, evga has shitty ones using pcie 6pin cables...  And because you bought it for $100 doesnt mean its priced $100 usually... I so hate it when someone goes - ""this is great, because I got payed 2/3 of normal price"". Well if you want to argue, you should take the normal price, shouldnt you?   r9 270 beats it by 40%! gtx650ti boost, its predecessor beats it! 265 beats it while being cheaper, considerably.   I mean yeah, here save $5 on the power consumption difference per year when you have 1KW/h for 10 cents...   and once you are getting 20fps less FPS than with 270, you would be so glad with the knowledge that theres no cable running between your psu and the graphic card... its such a warm feeling theres no clutter in the case..  I am not really hating on the card, I think its perfect for some folks... but lets try to be objective here and act like FPS matter..."
hardware,3bnhae,LiberDeOpp,5,Tue Jun 30 22:10:49 2015 UTC,"Well if you want to argue, you should take the normal price, shouldnt you?   Is that why you compared a 960 (MSRP: $199) to a 285 (MSRP: $249)?"
hardware,3bnhae,DoTheEvoIution,-5,Tue Jun 30 19:38:44 2015 UTC,"manufacturer's suggested retail price   when its released in to the wild...  not after one year when something else comes out... and its still recommendation...  my comparison is still new in shop vs new in shop...  deal available to you and me, not some occasional deals he can get because that guy was at the right spot..."
hardware,3bnhae,namae_nanka,4,Tue Jun 30 19:53:53 2015 UTC,"my comparison is still new in shop vs new in shop   ... Except when somebody else talks about the smashing deal they got on a 750 Ti, apparently."
hardware,3bnhae,makegr666,-1,Tue Jun 30 21:26:31 2015 UTC,"Hey guys, I got B&W 685 S2 for $200 on a sale in some house where they had divorce...  you should get it too as well  now lets talk about $200 price range speakers, I think 685s really do sound good for that price   -- guys?"
hardware,3bnhae,DoTheEvoIution,1 point,Tue Jun 30 22:01:04 2015 UTC,"I think you're having a stroke, yo."
hardware,3bnhae,makegr666,-5,Tue Jun 30 22:23:05 2015 UTC,I think   more like giving it your best try
hardware,3bnhae,LiberDeOpp,5,Tue Jun 30 22:36:00 2015 UTC,The 750 ti beats the 650 ti comparing the overclocked boost version is dumb. You can get overclocked editions of almost any card so try to compare apples to apples.
hardware,3bnhae,makegr666,0,Tue Jun 30 22:45:29 2015 UTC,"well you cant cram extra ROPs in it or fiddle with the memory bus to get it bit wider... but yeah, you can get that generous 50mhz boost on your own..."
hardware,3bnhae,DoTheEvoIution,0,Tue Jun 30 22:47:36 2015 UTC,Amusing to see you downvoted while those who spout off ignorantly are having a jolly good time.
hardware,3bnhae,poematik,12,Tue Jun 30 22:49:53 2015 UTC,"its replacing $150 gtx750ti, which is shit performance wise   Are you serious? I can't believe you're serious..."
hardware,3bnhae,TaintedSquirrel,-6,Tue Jun 30 22:31:24 2015 UTC,"WTF is wrong with you people, am I like only one who can read benches?   Here   How in the holly hell do you celebrate card that is slower than its predecessor or its competition? Yes, poewr consumption is awesome, and thats all? FPS does not matter anymore?"
hardware,3bnhae,OftenSarcastic,5,Tue Jun 30 22:42:12 2015 UTC,"A gtx 750 has been proved more powerful than a GTX 650 ti, not reaching a GTX 660, GTX 750ti being quite a bit more powerful than a GTX 660, but not half the way to a 670.  And even more now, with every update that the GTX 750 received, oh, and even the GTX 750 is MORE powerful than a 265 right now. Plus shadowplay, physx, and quite more things... (Built a simple rig for my GF with a 750)"
hardware,3bnhae,poematik,2,Wed Jul 1 01:32:48 2015 UTC,He's complaining about the 750ti not being as powerful as the 650ti boost which is an overclocked version of the 650ti.  He fails to ignore most of the 750ti's you can buy are overclocked as well.
hardware,3bnhae,screwyou00,-1,Tue Jun 30 19:43:43 2015 UTC,"Yes, I am serious, specially when I can benchmark every game and compare it to other benchmarks. I've seen higher number of FPS in games that were benchmarked a year ago, thus making those benchmarks not trusty for me. Updates are a thing."
hardware,3bnhae,screwyou00,0,Tue Jun 30 21:30:38 2015 UTC,well then maybe you can saddle up a unicorn and go to the land of BeenProved and bring some swag
hardware,3bnhae,DoTheEvoIution,3,Tue Jun 30 22:12:36 2015 UTC,"Actually, The 750ti is the most efficient price/performance ratio and most variations are totally PCI-e bus powered. It's a godsend for budget rigs or smaller cases."
hardware,3bnhae,SPOOFE,3,Tue Jun 30 22:29:09 2015 UTC,"From what I see, the 270 or 270X is #1 in performance per dollar across the board...  Around 30% faster than the 750 Ti for the same price."
hardware,3bnhae,poematik,1 point,Tue Jun 30 22:26:30 2015 UTC,Did Nvidia drop the price on that card? I remember it being behind AMD cards in price/performance and only being worth it for people needing/wanting the lower power draw.
hardware,3bnhae,DoTheEvoIution,2,Tue Jun 30 22:30:09 2015 UTC,not sure about nvidia but they are sub~150 on amazon last time i checked.
hardware,3bnhae,perkam,2,Tue Jun 30 22:35:32 2015 UTC,AMD has nothing against a card like the 750ti now. AMD made the first PCIe powered only video card - the HD 7750/R7-250E - and that was the best efficiency/performance at the time until the gtx 750(ti) came to the market.
hardware,3bnn53,CEKPETHO,8,Tue Jun 30 17:50:07 2015 UTC,Now what are it's spec for Sync range?
hardware,3bnn53,Seclorum,7,Tue Jun 30 21:20:05 2015 UTC,30-75hz if you trust this one.  https://pbs.twimg.com/media/CIVH2IKW8AAAkCr.jpg
hardware,3bnn53,dengudomlige,1 point,Tue Jun 30 23:41:48 2015 UTC,It only says 30-75hz via Displayport. That doesn't mean that it'll work in this range if you enable freesync
hardware,3bnn53,dcdead,1 point,Thu Jul 2 07:02:47 2015 UTC,That's pretty good.
hardware,3bnn53,JDAndChocolatebear,1 point,Wed Jul 1 02:02:58 2015 UTC,"Agreed, really doesn't need to be lower than 30."
hardware,3bnn53,Techdestro,1 point,Wed Jul 1 04:24:46 2015 UTC,"Yeah. If your getting lower than 30fps, you probably could lower a few settings to bring it up again."
hardware,3bnn53,Seclorum,7,Wed Jul 1 19:56:48 2015 UTC,"I absolutely love the ultra wide aspect ratio for gaming, reading, coding, and movie watching. It's really just so nice. I could never go back to 16:9 or 16:10.   Also IMO, Freesync/Gsync 75 Hz>144Hz. Originally I thought otherwise, but a synced display greatly improves the overall experience."
hardware,3bnn53,kennai,3,Wed Jul 1 07:47:35 2015 UTC,"Agreed. 21:9 is the new king in my book.  It's glorious for all you've listed, and I'd like to add that it's just perfect for my video editing window layout.  My only regret is that I splurged on the first LG 3440X1440 that came on the scene... I should have waited for Gsync/Freesync..."
hardware,3bnn53,Stingray88,1 point,Wed Jul 1 19:30:30 2015 UTC,Can I ask what monitor/s you're basing your experience on?
hardware,3bnn53,LazyGit,2,Wed Jul 1 13:43:49 2015 UTC,"I'm using three LG 29um67-P for the freesync setup, and the BenQ I used was/is the XL2411Z. My GPU is the 295x2.   Edit: And by BenQ, I meant 144Hz monitor."
hardware,3bnn53,kennai,1 point,Wed Jul 1 17:43:01 2015 UTC,21:9 sounds awesome but I think I'm going to wait until we have both freesync and 144hz. Hopefully freesync has a time to mature as an added bonus.
hardware,3bnn53,Juheebus,5,Wed Jul 1 23:55:53 2015 UTC,"still waiting for the 27"" 1440p IPS 144hz to show up in my country AND pray it doesn't arrive with defects.."
hardware,3bnn53,Raising,3,Wed Jul 1 05:51:46 2015 UTC,$1099   I had to check this wasn't an Australian based site.
hardware,3bnn53,dunkitando,7,Wed Jul 1 15:28:33 2015 UTC,"7W speakers with DTS Sound   How much of the cost of the monitor goes towards speakers? You'd think for a high end monitor, people would either a) buy separate speakers if they cared about audio at all or b) wouldn't need decent sound at all, eg if they're just working in an office or using it entirely for video editing, so I really feel that any investment in anything more than bare bones audio is wasted."
hardware,3bnn53,mmencius,5,Wed Jul 1 08:31:19 2015 UTC,What happened to the predator model? Is this a lower tier model? I thought Predator was coming in at 75hz and rumored to be at 100hz?
hardware,3bnn53,XaeroR35,2,Tue Jun 30 23:05:37 2015 UTC,"This is the Acer XR341CK, but this article is kinda inaccurate. According to TFTcentral it is confirmed at 75hz. No word on the G-sync version, XR341CKA, yet which is where all the rumors of 100hz have been coming from. It's possible that G sync can help in overclocking so there's still hope. And if Acer doesn't come through then there's still Asus' monitor that might.  Edit: After looking around, it looks like the Freesync version does not have the Predator logo, I'm not sure if it was ever part of the new lineup. I have no idea what will happen to the G sync version, but I'm assuming it will still be under the Predator name."
hardware,3bnn53,Deadband420,1 point,Tue Jun 30 23:11:29 2015 UTC,I've heard the g-sync model will be atleast 200€ more..
hardware,3bnn53,Techdestro,3,Wed Jul 1 04:26:23 2015 UTC,"Most likely, that's how all g-sync monitors are."
hardware,3bom94,Exist50,10,Tue Jun 30 22:00:21 2015 UTC,Thought I would try designing one.
hardware,3bom94,sircod,7,Tue Jun 30 23:50:29 2015 UTC,Ayyyyyyyy.   No one will know the difference :P
hardware,3bom94,Akutalji,-5,Wed Jul 1 00:12:14 2015 UTC,Do we really need a 3D model for the faceplate?  Pretty sure I could make one in any 3D CAD program faster than I could download it.
hardware,3bom94,tarheel91,12,Wed Jul 1 01:01:26 2015 UTC,Easier to give the exact dimensions in this way.
hardware,3bom94,zaures,-4,Wed Jul 1 01:10:51 2015 UTC,Or you could just list the overall dimensions and the hole placements in a simple drawing. I mean its just a rectangle with 4 holes....
hardware,3bom94,TheImmortalLS,3,Wed Jul 1 14:36:31 2015 UTC,Or you can give a downloadable file and save us some work.
hardware,3bmqi0,FishPhoenix,20,Tue Jun 30 13:50:57 2015 UTC,Holy fuck its almost as fast my 980s in SLI :/
hardware,3bmqi0,DrexelDragon93,1 point,Tue Jun 30 15:26:06 2015 UTC,"Sadly, that could be a conscious marketing strategy."
hardware,3bmqi0,0Ninth9Night0,14,Tue Jun 30 22:28:18 2015 UTC,Wow it is faster than 780ti by at least 75% across the board (100% in Witcher 3!).  Must...wait...for 16nm...gah
hardware,3bmqi0,hisroyalnastiness,3,Tue Jun 30 18:47:23 2015 UTC,I'm still on two 6970s. I'd like to wait for 16nm but I've been waiting four years for a new GPU and damnit I'm not waiting any longer.
hardware,3bmqi0,Ofactorial,1 point,Wed Jul 1 04:58:17 2015 UTC,I feel you brother ;-;
hardware,3bmqi0,reynardtfox,5,Tue Jun 30 20:42:10 2015 UTC,I'm trying to hold out for the 980 TI Lightning.
hardware,3bmqi0,feanor512,1 point,Tue Jun 30 16:54:34 2015 UTC,Yea I bought the normal one cause my current card is dying but I can't wait to see a 980 ti lightning and classified Duke it out.
hardware,3bmqi0,melgibson666,1 point,Wed Jul 1 20:13:56 2015 UTC,Isn't the lightning only for amd cards?
hardware,3bmqi0,ICanHazTehCookie,3,Tue Jun 30 17:40:21 2015 UTC,"They have made lightning editions of the 780, 770, 680. There may be more, but I don't know them off the top of my head."
hardware,3bmqi0,Kobayakawamiyuki,2,Tue Jun 30 18:51:49 2015 UTC,"Ah ok, thanks."
hardware,3bmqi0,ICanHazTehCookie,19,Tue Jun 30 18:54:24 2015 UTC,"The writing in this review is bad, even for tech writing. It's like having a conversation with an excitable 15-year-old.  The card, however, is awesome."
hardware,3bmqi0,foxtrot1_1,5,Tue Jun 30 17:51:40 2015 UTC,"guru3d is usually like this, unfortunately. At least the content (images / charts) is usually good"
hardware,3bmqi0,24759625,5,Tue Jun 30 20:00:12 2015 UTC,English is not his first language.
hardware,3bmqi0,Monday_Morning_QB,1 point,Tue Jun 30 21:59:28 2015 UTC,"Yes, but a bunch of the writing is idiomatic - that reflects a deep familiarity with English. Second-language speakers usually don't use metaphors the way this guy does, because it's difficult for them. It's best to avoid them if you're not a native speaker.  Also: editing is a thing."
hardware,3bmqi0,foxtrot1_1,7,Thu Jul 2 14:51:42 2015 UTC,"So bad, I couldn't go beyond page 2.   And why 36 pages??"
hardware,3bmqi0,neverfarts,8,Tue Jun 30 18:53:14 2015 UTC,36 pages is better than 1!
hardware,3bmqi0,foxtrot1_1,6,Tue Jun 30 20:25:26 2015 UTC,"I agree, I didn't even read it and came to these comments to get the gist of it. The last few reviews I've read were really difficult to get through - they're in dire need of some proper editing!"
hardware,3bmqi0,illexample,1 point,Tue Jun 30 19:45:24 2015 UTC,The last few reviews I've read from Guru3D sounds like they were written like a robot. Or by someone who is barely awake.  Edit: And they put up at least one incorrect chart showing the 390 strix
hardware,3bmqi0,SirCrest_YT,7,Tue Jun 30 20:35:05 2015 UTC,"oh man, the dream card.  I got the smaller brother version of this card on the MSI GTX 970 4G gaming and it's awesome.   The best part about these MSI cards are that you can't even hear them, they are whisper quiet.  And my GTX 970 OC's to 1510Mhz :). If you want to go higher u usually need to flash a BIOS that allows for higher POWER DRAW.  PS: When I first received my card I had some coil whine that was noticeable, but over time it seems to have vanished. I bought my card around September."
hardware,3bmqi0,pcisgood,4,Tue Jun 30 15:19:53 2015 UTC,Yeah a buddy of mine has the 970 version too.  I ordered the MSI 980 TI Gaming 6G a week ago; hoping it's here this week. I'm upgrading from the EVGA 670 FTW. This review has me pretty excited :)
hardware,3bmqi0,Z_FTW,4,Tue Jun 30 15:32:43 2015 UTC,"Just got this card in yesterday. I love it so far, really quiet and looks great. If anyone is considering this card I highly recommend it."
hardware,3bmqi0,SirCrest_YT,2,Tue Jun 30 16:56:01 2015 UTC,Any coil whine?
hardware,3bmqi0,Z_FTW,1 point,Tue Jun 30 20:36:19 2015 UTC,I've not noticed any.
hardware,3bmqi0,SirCrest_YT,3,Tue Jun 30 21:07:43 2015 UTC,"Awesome. I have a pair of MSI lightning 580's. I love em, they just don't have the kind of grunt I need in 2015. lasted me a solid 4 years.   Planning to move to a single one this weekend, excited."
hardware,3bmqi0,animeman59,2,Tue Jun 30 21:50:39 2015 UTC,Just ordered mine.  Can't wait!
hardware,3bmqi0,egzplicit,5,Tue Jun 30 23:24:32 2015 UTC,"I'm trying to decide if it's this or the Gigabyte G1... it looks like the G1 is 3-4 fps faster at 1440p and factory clock (maybe because of the extra 26Mhz) but also a bit louder (and with a higher risk of coil whine). Would be nice to see if a +26 on the MSI would match the FPS of the Gigabyte.  The overclocking page states +87Mv but the Conclusions page is saying +50Mv so I guess they went for +50Mv, same as with the G1. However, they had +100Mhz on the MSI vs +150Mhz on the Gigabyte.. I wonder why.  I think I'll give the Gigabyte a go and return it for the MSI if I get too much coil whine or if it's too noisy. Coming from a KFA2 680 with reference cooler, maybe the G1 won't be as loud as I think it is."
hardware,3bmqi0,attomsk,6,Tue Jun 30 15:15:47 2015 UTC,(and with a higher risk of coil whine)   I don't think there is any way to be certain about that yet.
hardware,3bmqi0,kaasmi,2,Tue Jun 30 16:37:17 2015 UTC,"Coil whine? I watched the Jayztwocents review and it was damn near silent when it showed the noise. https://www.youtube.com/watch?v=B2-gCXscH04 7:20ish ti hear reference one, then g1. Guess it's just a crapshoot like the 970s?"
hardware,3bmqi0,TaintedSquirrel,4,Tue Jun 30 16:45:06 2015 UTC,There's been a lot of complaints about coil whine on the G1 from the community.
hardware,3bmqi0,Heavenswake_,0,Tue Jun 30 17:20:51 2015 UTC,0 whine on mine and it's nice and silent.
hardware,3bmqi0,ArtemisFei,3,Tue Jun 30 19:59:06 2015 UTC,"I was having the same dilemma, but chose the G1 because of aesthetics (black and red doesn't fit at all in my current build), and the higher overclocking potential.  The coil whine issue did bother me, but when my G1 gets here tomorrow, I'm hoping I'm one of the lucky few that doesn't have any."
hardware,3bmqi0,egzplicit,3,Tue Jun 30 16:20:38 2015 UTC,Come back tomorrow and let us know :)
hardware,3bmqi0,narutoninjakid,1 point,Tue Jun 30 19:52:42 2015 UTC,"Yeah, this red and black is killing the Asus X99 deluxe build I’m trying to do. Was going for the ASUS Rampage V extreme but that’s out for now. Let us know as the G1 will be much better for me"
hardware,3bmqi0,SirCrest_YT,1 point,Wed Jul 1 17:13:37 2015 UTC,"Did it arrive yet? Hows the fan sound, noise, coil whine?"
hardware,3bmqi0,ArtemisFei,2,Wed Jul 1 20:30:47 2015 UTC,"Hey! It has actually, and so far, especially with my Fractal R5, it is very quiet. But then again, I can't compare it to the MSI version.  Coil whine, I've been running Arkham Knight, as well as played a round of league, and I hear a tiny, tiny whine when I'm running things over 80 or so FPS, but I literally have to put my ear in front of it, with the side panel off in order to hear it.  With the panel on, the only thing I hear are my case fans. But again, it might be due to my case.  All in all, I'm pretty damn satisfied with both the noise (or lack of), and its performance. It runs Arkham Knight pretty well...but if you need a 980 Ti to run this, then it's horribly optimized.  Pictures if you're interested: http://imgur.com/a/zqRyy"
hardware,3bmqi0,pcisgood,1 point,Wed Jul 1 20:36:09 2015 UTC,"i would get the MSI if you value quiet over like 2 FPS difference :)  MSI has a more restrictive TDP bios ,but you can go around it by flashing a custom BIOS if you put in some effort.  If you need help with custom BIOS look at overclock.net in the GPU section."
hardware,3bmqi0,Acalys,1 point,Tue Jun 30 15:36:53 2015 UTC,Flashing a BIOS for a card that only has one is extremely risky.
hardware,3bmqi0,PolyWit,1 point,Tue Jun 30 19:28:04 2015 UTC,"There's no more risk than when you flash your motherboard with a manufacturer updated BIOS/firmware, surely? I severely doubt it's ""extremely"" or even moderately risky."
hardware,3bmqi0,Acalys,1 point,Wed Jul 1 10:27:07 2015 UTC,If you fail your flash with a card that has only one BIOS it has no backup so it's bricked. The Classified is a good example of a card well suited for flashing.
hardware,3bmqi0,PolyWit,1 point,Wed Jul 1 16:14:38 2015 UTC,"Sure, agreed, but you're confusing the hazard with the risk. Bricking your card would be bad (hazard) but if there's a very low probability of it happening then the risk is low. To say it's ""extremely"" risky is wrong given that you'll find plenty of folks on, say, overclock.net flashing their card BIOS on non-Classified models. I'm sure EVGA would love us all to believe we need one though ;)"
hardware,3bmqi0,Colorfag,1 point,Wed Jul 1 16:39:13 2015 UTC,That black and white shroud is nice. Would go well with my color theme.
hardware,3bmqi0,Jack_BE,2,Tue Jun 30 21:43:13 2015 UTC,"current prices here have this baby at 850€, I'm waiting a tidbit for pricing to settle."
hardware,3bmqi0,Wargazm,1 point,Tue Jun 30 19:41:38 2015 UTC,"Fuck it, I pulled the trigger and bought one.  no idea when it'll get here, but the order is placed."
hardware,3bmqi0,Colorfag,1 point,Tue Jun 30 18:41:53 2015 UTC,"The consensus is that the reference design is better for SLI though?  Or would I still be better off with a fan design like this with its improved OC and board? I have an Air 540, so airflow isnt an issue."
hardware,3bmqi0,SirGregorius,2,Tue Jun 30 19:49:03 2015 UTC,Unless your mobo requires the cards to be installed with no space between you're totally fine. I switched from a regular atx tower to the 540 and the temps on my cards dropped like 6-8 C.
hardware,3bmqi0,Colorfag,1 point,Tue Jun 30 20:35:41 2015 UTC,"Im also concerned with CPU temp, since Im using my CLCs radiator as an exhaust"
hardware,3bmqi0,animeman59,1 point,Tue Jun 30 21:39:26 2015 UTC,"I've done the same.  Shouldn't be an issue at all.  If you're running any recent Intel CPU, then the thermals should be fine."
hardware,3bmqi0,SirGregorius,2,Tue Jun 30 23:38:16 2015 UTC,Ditto. You should have no issues. What I did was run the front and exhaust fan profiles to my GPU temps.
hardware,3bmqi0,Indoorsman,1 point,Tue Jun 30 23:59:50 2015 UTC,"So from what I'm reading in here, can the 980 TIs really differ that much? People are saying this card is way faster than even other TIs?"
hardware,3bmqi0,Indoorsman,3,Wed Jul 1 00:27:10 2015 UTC,"Based on this review, @1440p this card performs around 5-15 fps higher (depending on the game) compared to the stock 980ti. That's with the stock card not overclocked, and this card comes factory overclocked. Note that both cards can be overclocked further.   The other advantage is this card and typically the other non-reference cards will have superior cooling compared to the reference."
hardware,3bmqi0,550_Cord,1 point,Wed Jul 1 02:16:21 2015 UTC,"Cool thanks. Going to build at the end of the year, and it looks like there won't be too many new cards last what we have seen now. So a 980TI non reference is probably where I am going. But a lot of time between now and then."
hardware,3bmqi0,terp02andrew,11,Wed Jul 1 02:20:53 2015 UTC,Why wouldn't we want to be informed consumers?
hardware,3bmqi0,go_balls_deep,2,Tue Jun 30 14:33:14 2015 UTC,"Agreed. For a $650+ purchase, it never hurts to have experience/review with that exact SKU.  Some like reference (blower), others like non-reference. I've always liked non-reference, so yeah - reviews of the actual SKUs you are interested in are very important :P  Top of my list has always been the MSI Gaming version, Classy, or the STRIX. Considering eVGA is only marking up the Classified $50 this time, it is a return to more sane pricing.   If you don't know, check how eVGA was pricing the 980 (non-Ti) Classifieds haha."
hardware,3bmqi0,Xentinel,1 point,Tue Jun 30 14:36:20 2015 UTC,Yeah there are generally reviews on most variants when a new GPU is released. Not reviewing temps and overclocking capabilities of each variant would be silly.
hardware,3bmqi0,BlayneTX,0,Tue Jun 30 14:47:26 2015 UTC,Seems like I made a good purchase :)
hardware,3bnyny,zmeul,2,Tue Jun 30 19:09:45 2015 UTC,The shitty thing is that 4gb cards are so expensive. Over here almost 50€ more than a 2gb one. I am not sure if that's worth it.
hardware,3bnyny,Klorel,1 point,Wed Jul 1 16:49:35 2015 UTC,"I have a Club 3D 280X RoyalQueen.  It's a great card.  It OCs pretty well too, although I often have to have the fan at 100%.  Club 3D is the best brand you haven't heard of."
hardware,3bnyny,sev87,1 point,Wed Jul 1 01:40:55 2015 UTC,Club 3D is very active in Europe
hardware,3bnyny,wasdzxc963,1 point,Wed Jul 1 02:37:24 2015 UTC,Why are all the min fps so low?
hardware,3bnyny,fuccboi9000,2,Wed Jul 1 01:57:42 2015 UTC,I notice that on all AMD benchmarks. It's frame latency issues
hardware,3bnyny,Exist50,1 point,Thu Jul 2 21:26:17 2015 UTC,Looks like a pretty solid little card.
hardware,3bnoj8,zmeul,5,Tue Jun 30 17:59:54 2015 UTC,But now it doesn't look like an owl.
hardware,3bnoj8,iluvkfc,6,Wed Jul 1 00:02:18 2015 UTC,http://1.bp.blogspot.com/_rtOXMZlMTkg/SzGhkQ5gKQI/AAAAAAAACZg/pde10RQcPE4/s640/3.png
hardware,3bnoj8,arikv2,1 point,Wed Jul 1 01:15:16 2015 UTC,"If I have an fx8350, would I see a noticeable improvement from a 980 to a 980ti at 1440p?"
hardware,3bnoj8,SeismicAltop,11,Tue Jun 30 18:17:03 2015 UTC,"You'll have better luck getting an answer in /r/buildapc especially in their daily ""Simple Questions"" thread."
hardware,3bnoj8,faderprime,3,Tue Jun 30 19:27:54 2015 UTC,Resolution?
hardware,3bnoj8,peaceoutwhat,2,Wed Jul 1 02:42:14 2015 UTC,"Depends what games you play, what resolution, what settings, etc.  The easiest way is just to look at how close you are to being CPU bottlenecked already in the games you play, or how many games do you play where you are already CPU bottlnecked via close monitoring of CPU and GPU usage."
hardware,3bnoj8,aziridine86,1 point,Wed Jul 1 01:05:19 2015 UTC,How overclocked are you?  I would personally doubt a large improvement but happy to be proven wrong.
hardware,3bnoj8,ByronicHero56,2,Tue Jun 30 20:29:48 2015 UTC,5 GHz
hardware,3bnoj8,SeismicAltop,3,Tue Jun 30 20:31:15 2015 UTC,"Just curious, when did you get your 8350? And what's the voltage?"
hardware,3bnoj8,Zachariahmandosa,2,Tue Jun 30 21:07:25 2015 UTC,"Not your OP, but I have an 8320e that can do 5 GHz @ 1.59 V. Purchased in November."
hardware,3blu6m,MP9,118,Tue Jun 30 07:05:51 2015 UTC,When you fab silicon you don't want to waste any of it. So you bin various chips at different price levels to fit any budget and get all your silicon onto a PCB in someone's PC.   You should see SD card memory. Those margins are so razor thin that so long as the flash can hold some blocks it will be sold to you.
hardware,3blu6m,fuzzycuffs,59,Tue Jun 30 08:08:44 2015 UTC,"Hey OP, It's worth pointing out that what /u/fuzzycuffs describes is not something that manufacturers set out to do intentionally, rather it's a cost of doing business which they plan for and try to mitigate.   There are a dizzying number of things which can go wrong during fabrication, so they design in ways that allow them to gracefully degrade by decreasing clock speed, or blowing fuses which disable subsections of the chip (eg turning a top tier GPU into a mid tier GPU by disabling a block of shaders).  See: https://en.wikipedia.org/wiki/Product_binning"
hardware,3blu6m,phrotozoa,237,Tue Jun 30 13:29:02 2015 UTC,"It really is rather simple on a fundamental level. It basically happens like this:  ""Two of the cores on this sucker ain't workin', but the two that DO can get up to like 4.5GHz on just 1.3v.""  ""Okay Cletus. Nuke the two fuckers that ain't workin' and stamp this one with a ""G3258"" logo.""  ""Roger that. What about this little fucker? He's got 4 good cores. Runs stable in the mid-4s on all 4 cores, but doesn't wanna even touch that hyperthreading bullshit.""  ""That's an i5 4690K Cletus. Git 'her binned.""  ""Okay, but what about this little shit right here? Fuckin' thang won't even run at 2.7GHz over all 4 cores, but we can get one of 'em to turbo up into the mid-3s.""  ""That's an S or a T processor. We'll sell it as a low-power alternative for them enviroQUEENS who like to fuck trees and appreciate silence.""  ""Okay boss. What about this here one? It's godawful. Runs hot as fuck and has some of the worst single-threaded perfomance I've ever seen. It's got 4 cores though, and it seems to be stable if we slap a fuckin' car radiator on it. Where's this one go?""  ""Call AMD. Maybe those jackasses will find something to do with it."""
hardware,3blu6m,Oafah,36,Tue Jun 30 14:12:57 2015 UTC,You should write an article explaining this. I get this.
hardware,3blu6m,Wrong_Opinion,31,Tue Jun 30 14:26:54 2015 UTC,I really hope somewhere out there is a couple of hillbillies sorting through wafers every day like this.
hardware,3blu6m,SPOOFE,11,Tue Jun 30 16:37:22 2015 UTC,I was wondering why my processor came stained with tobacco juice.
hardware,3blu6m,cartermatic,7,Tue Jun 30 22:14:21 2015 UTC,So do all chips start out at the beginning of manufacturing with the intent of being the highest end chips?
hardware,3blu6m,Oafah,15,Tue Jun 30 17:34:29 2015 UTC,"Forget what you think you know about SKU numbers, codenames, and model numbers. All you need to know is this:  A Haswell core is a Haswell core. They're all designed the same, intended to perform the same, and are only separated by how they actually perform when tested.  An i5 4460 is just 4 Haswell cores that run stably at 3.2Ghz, with one core able to turbo up to 3.4GHz. If it had been able to clock up to 4.0GHz with a 4.4 turbo, and could support hyperthreading and a bit of overclocking, it would have been a 4790K. The only difference between the two is what ""bin"" they end up in."
hardware,3blu6m,Paladia,3,Tue Jun 30 17:47:48 2015 UTC,"A Haswell core is a Haswell core. They're all designed the same, intended to perform the same, and are only separated by how they actually perform when tested.   They have different amounts of cache however."
hardware,3blu6m,gigantism,1 point,Sat Jul 4 08:24:39 2015 UTC,"How does overclocking factor into this, then?"
hardware,3blu6m,GoldPanther,6,Tue Jun 30 17:57:56 2015 UTC,You basically change the clock that was set in the factory. Factory clocks are typically conservative for reliability purposes.
hardware,3blu6m,Ofactorial,3,Tue Jun 30 18:01:51 2015 UTC,"If the only difference between processor A and processor B is clock speed and $300, then by overclocking processor A you get processor B and save $300. Thing is, a processor may be binned lower because it isn't stable at that clock speed, or because it takes too much voltage to be stable at that speed. So the main difference is that with the more expensive chip you get a guarantee that it will run at the higher clock speed at the stated voltage and be 100% stable, whereas with the cheaper chip all bets are off (though these days the OC'ing room on CPUs is so high that if all that differs is clock speed you're all but guaranteed to at least hit the clock speed of the highest tier chip; this is why the most popular CPUs tend to be the lowest bins of the top tier)."
hardware,3blu6m,Oafah,1 point,Tue Jun 30 18:51:46 2015 UTC,They use the same process to find samples that tolerate higher voltages and yield better results. All of these things are discovered during the binning process.
hardware,3blu6m,TheBloodEagleX,1 point,Tue Jun 30 18:01:59 2015 UTC,"Origin & SKU matters when you want to absolutely best batch though. This is why ""golden chips"" are a thing for overclockers even for the same higher end chips. Sometimes you just get lucky with a chip you can push further than usual, sometimes those chips come from a certain fab (they're spread out in different countries)."
hardware,3blu6m,sbjf,1 point,Mon Jul 6 00:23:24 2015 UTC,"So are there lists of which processors are actually the same ones? Sometimes it's easy to guess which ones are the same (e.g. X5650 to X5690) since all that's different is the clock, but in ones that have feature differences, how do you know which ones are the same?"
hardware,3blu6m,dylan522p,6,Thu Jul 9 22:50:04 2015 UTC,"Sorta. They layout a few different dies, so a Titan x and 980ti are the same die or 980/970 or 4770k down to some pentiums"
hardware,3blu6m,TheImmortalLS,1 point,Tue Jun 30 17:47:53 2015 UTC,"Don't forget the mobile 970/980M! That's the reason why there is such a huge gap between desktop 960/970, if anyone wonders."
hardware,3blu6m,y801702,3,Thu Jul 2 07:29:41 2015 UTC,"One thing to keep in mind is that chips suffer from small defects in the manufacturing process. Since transistors are so small there are multiple points of failure, including things like small impurities in the silicon. Once a chip is made it is tested to see what areas do work and how fast can it run stable. Bad areas are discarded by blowing internal ""fuses"" or something like that. A model is just a guarantee that it has certain features. Of course, in case there is demand, the company may decide to intentionally disable some features in a chip even if they work perfectly, but AFAIK this tends to happen more when the manufacturing process for that chip is mature and thus there are less [partially] defective chips."
hardware,3blu6m,y801702,1 point,Tue Jun 30 18:24:26 2015 UTC,"And, of course, it may happen that there are physically different chips, for example a 4 core model and a 2 core one, since smaller surface (2 native cores are smaller than 4) mean more chips can be made at the same price."
hardware,3blu6m,TheBloodEagleX,1 point,Tue Jun 30 18:30:16 2015 UTC,"Yup! This is why ""golden chips"" are a thing for overclockers even for higher end chips. Sometimes you just get lucky with a chip you can push further than usual."
hardware,3blu6m,Exist50,13,Mon Jul 6 00:22:02 2015 UTC,Hyperthreading is likely more an artificial limitation than anything else.
hardware,3blu6m,Efferat,4,Tue Jun 30 16:31:20 2015 UTC,This was brilliant.
hardware,3blu6m,billbot,1 point,Tue Jun 30 16:06:34 2015 UTC,This is a thing of beauty.
hardware,3blu6m,headband,-7,Tue Jun 30 20:15:28 2015 UTC,"This might be slightly amusing but its just flat out wrong. It's not profitable to produce a chip if you have to go around disabling half of it on a significant number of parts, you have to get the yield up. Also there aren't chips that are just magically low power. It might be the same design but the drives need tweaked to enable that. For CPUs features are enabled or disabled for the target and pretty much the only thing binned out would be speed, and even then when the process is mature you will get artificial downgrades."
hardware,3blu6m,Oafah,3,Tue Jun 30 21:52:22 2015 UTC,"The compatibility for feature support is most certainly taken into account during binning, and low-power CPUs are indeed pulled from the same lot as their full-power cousins. Core nuking is also a thing, and AMD was the first to start using it back when ASUS enabled their core unlocker for the Phenom II series, which was precisely when we knew first-hand, for absolute certain, that all Phenom II processors were in fact intended to be quad core units that failed QC and were cut down for a lower binning. Afterward, manufacturers started physically destroying cores that didn't meet a minimum standard."
hardware,3blu6m,headband,-6,Tue Jun 30 23:01:46 2015 UTC,"It's absolutely not. The feature might not be tested if its disabled, but its way too much of a logistical nightmare to sort based off so many criteria when well over 99%are going to work fine anyways. Yes Amd did the core disabling years ago when multi core was a new thing and it didn't work out well for them."
hardware,3blu6m,sabot00,1 point,Tue Jun 30 23:18:45 2015 UTC,"AMD did core disabling because their yield rates were very good relative to market demand of their top models. If AMD makes Phenom 900's and 800's at an 80/20 ratio in terms of yield, but the market buys them at a ratio of 40/60, then either AMD can sell some Phenom 900's as 800's, or they vastly overproduce 900's and keep the same ratio (or they can just let 800's run out of stock).  PS: I use 800 and 900 to refer to the series, not specific models. However the principle is the same."
hardware,3blu6m,headband,-2,Wed Jul 1 23:23:34 2015 UTC,"If you have good yields you don't pull this crap, you spin a new reticle that natively has less cores and don't throw out good silicon just because of market demand."
hardware,3blu6m,sabot00,1 point,Wed Jul 1 23:55:00 2015 UTC,"Spinning a new die costs tremendous amounts of R&D.  Let's say AMD decided to make a native 3-core die to cater to that market (back in the Phenom II days the X3 was pretty popular, even today the triple-module FX-6000 series is considered a solid entry gaming CPU). They would have to reroute all of the memory traces, clocking mechanisms, cache traces, as well as reposition all of the fixed-function hardware such as the HyperTransport logic and port logic as well as change the shape of the L1/L2/L3 cache to accommodate the geometry of a 3-core design, or simply have wasted silicon space.  Then their engineers must verify that the design is glitch and bug free, that in the billions of transistors there aren't any issues. Then they must reconcile this theoretically perfect design with the implications of their fabrication node's reality. Certain transistor placements, even if theoretically free of electron tunneling / crosstalk, may not be able to be fabbed with a sufficient degree of precision as to lower the levels of such phenomenon enough to service a CPU.  That's just a small overview of the multitude of issues with designing a completely new die, even if the core architecture, cache design, and logic placement have all been done before."
hardware,3blu6m,headband,-2,Thu Jul 2 01:23:49 2015 UTC,"It's really not that much, these are incredibly minor changes. It's a stupid idea and thats why they only did it once...."
hardware,3blu6m,autowikibot,7,Thu Jul 2 01:39:20 2015 UTC,"Product binning:       In semiconductor device fabrication, product binning is the categorizing of finished products based on their thermal and frequency characteristics.      Relevant: Clock rate | Radeon HD 5000 Series | Radeon HD 7000 Series   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3blu6m,Ofactorial,6,Tue Jun 30 13:30:08 2015 UTC,"And this is why every now and then people will figure out how to ""unlock"" a CPU or GPU to a higher bin. By bypassing whatever mechanism is in place to disable the extra shaders/cores/whatever, a person can effectively turn their mid-tier component into a high-tier component. Of course, the obvious issue is that that functionality was disabled for a reason. Often it doesn't matter, but sometimes you unlock your processor to find out why it should have been left locked."
hardware,3blu6m,JDAndChocolatebear,7,Tue Jun 30 18:44:05 2015 UTC,"Sometimes they will disable chips that can be fully unlocked, because they're trying to fulfill demand. Thus is why when unlocking a chip, it can sometimes work fine."
hardware,3blu6m,olavk2,4,Tue Jun 30 19:23:11 2015 UTC,"Also another reason is simply because the other cores just require an insane amount of voltage, while technically they are fully functional cores, it would require a high amount of voltage thus better cooling to have them enabled so it is easier just to disable them."
hardware,3blu6m,luger718,3,Tue Jun 30 20:06:14 2015 UTC,I heard that sometimes there specs are very strict so even if a core ran a tiny bit over voltage spec it'd be turned off.
hardware,3blu6m,elementalist467,5,Tue Jun 30 21:44:04 2015 UTC,There is also a marketing drive.  AMD and nVidia and their partners have determined that there are markets at several different price points.  If we look at the AMD 2xx series there were products ranging from sub $100 with new capability in $15 to $50 increments.  Essentially a consumer with a $200 budget could find a product close to $200 rather than settling for the $150 product because the next increment was $250.
hardware,3blu6m,Teresi2Finger,2,Tue Jun 30 13:31:30 2015 UTC,How do they test these chips to know how quality one is? I've always been curious.
hardware,3blu6m,andromeduck,5,Tue Jun 30 13:28:24 2015 UTC,"By testing the silicon properties when it comes out. Mostly transistor speed, leakage and physical defects.   The speed determines determines the max clock speed, the leakage determines the voltage curve. Together they determine power. Physical defects determines how many parts will be active on the chip. If a part of a 4 core CPU dies it could become a dual core.  Lots of statistical models involved."
hardware,3blu6m,Teresi2Finger,1 point,Tue Jun 30 15:17:23 2015 UTC,That's really neat! So why can't every chip they manufacture be top of the line?
hardware,3blu6m,0pyrophosphate0,8,Tue Jun 30 16:03:03 2015 UTC,"Because fabricating semiconductors is a very precise and delicate process. With modern lithographic technology, where you're looking at structures ~20 nanometers across, any tiny spec of dust or impurity in the silicon (even just a few atoms) can mean a transistor that doesn't work. One bad transistor can easily ruin a fairly large area of logic circuits, maybe single-handedly making the difference between a dual-core and quad-core processor."
hardware,3blu6m,Artip,6,Tue Jun 30 16:34:40 2015 UTC,"Well, when you're making a few billion transistors that are a dozen or so nanometers across, there's going to be a few that come out fucked up."
hardware,3blu6m,blergleblarg,4,Tue Jun 30 16:34:14 2015 UTC,"For a point of reference, Intel is now producing chips with a 14nm resolution. The atomic radius of a silicon atom is around 110pm.  You're looking at a feature detail in a modern chip being around 127 atoms wide.  At that point, fluctuations in temperature and humidity will ruin your yield."
hardware,3blu6m,Aquarius100,2,Tue Jun 30 17:29:11 2015 UTC,"Not every chip comes out perfect. There are bound to be some errors, the ones which have relatively lesser errors are sold as the flagships."
hardware,3blu6m,Exist50,1 point,Tue Jun 30 16:27:22 2015 UTC,"At add to what /u/Aquarius100 said, look at the original Titan. Yields were so bad on that chip that it took them I think over half a year, or maybe a full year, until they could get it in products in an uncut form."
hardware,3blu6m,somnolent49,1 point,Tue Jun 30 16:33:22 2015 UTC,"Because of the speed which the technology is improving at. New, faster, smaller designs are always in the works. There isn't enough time to park on one design and try to get the yields to come out perfectly.   Another big issue is demand. If every chip could run perfectly, the would still be incentive to deliberately downgrade some of them so that you can sell silicon at different price points."
hardware,3blu6m,fzed1199,0,Tue Jun 30 17:25:59 2015 UTC,Read/write speeds
hardware,3blu6m,ThrobbingMeatGristle,18,Tue Jun 30 13:38:38 2015 UTC,"The GPU is mostly a giant ASIC and it has many hundreds of repeating components all intended to work in parallel. Some of these work and pass the tests, others don't and are turned off. To few left working and the whole chip is rejected. If there are enough working parts left to be a 390, then that is how it is sold. If there are enough left to be a 390x then these can be sold for a premium."
hardware,3blu6m,Destects,17,Tue Jun 30 11:17:32 2015 UTC,"The answers by /u/fuzzycuffs and /u/ThrobbingMeatGristle are the one's you're looking for, It's the same thing with processors.  In a simple model, we'll use the Intel I series of processors (I3, I5, I7).  All the chips are cut/printed/fabricated/whatever as the top tier I7's, but because of the a number of uncontrollable variables (e.g. contamination, microscopic imperfections in the silicon discs, etc.), not all of them come out perfect, every chip has to be tested for performance against a set of standard specifications; the chips that score bellow the threshold for an I7, but perform well enough to pass the I5 bar, get the bad cores locked and are labeled I5's, the ones' that can't pass the I5 bar get marked as I3's and the ones that fail even the I3 test are considered duds. It would be far more costly to print 3 separate lines (i3, i5, i7) of chips, even though they're all using the same architecture, so instead they print 1 line. You can think of an i3 as a lower quality i5, and an i5 as a lower quality i7, at least in my simplified model.  Another thing to note is that there are cases where even if a chip passes the i7 bar, it will still get a core locked or a cap placed, and be marked as an i5, like if newegg orders 500 i5's, they may print 600 chips, 300 that are i5's, 200 that are actually i7 quality but get labeled i5's, and 100 that are only i3 quality, and get put on a shelf for another day.   This is why things like ""core-unlockers"" work, because the chip still has all the features of the higher-level model, but it was marked as the lower level due to either quality or demand, sometimes you'll unlock a perfectly good core, sometimes you'll unlock it and realize it's just giving you problems.  So back to your question: It's because not every chip comes out perfect, but instead of throwing away the ones that aren't 100% to spec, they can make changes to the firmware, setting it up to operate at a lower level of performance, and sell it at a lower rate. Some money is better than none."
hardware,3blu6m,bl4ckj4ck445,1 point,Tue Jun 30 14:45:04 2015 UTC,"Does this mean that not all cpu's of the same type (i5 for example) are equal? If yes, is this difference noticeable? Or does this difference disappear due to capping the frequency of the cpu at a certain level?"
hardware,3blu6m,I3oomer,7,Tue Jun 30 17:54:57 2015 UTC,"Not all cpu's are equal. This is where the term ""silicon lottery"" comes into play. Now if you buy a I5-3570 it should perform the same as other I5-3570s but if you buy a 3570k (unlocked multiplier) some will overclock more than others thus having better performance."
hardware,3blu6m,Logical_Phallusy,1 point,Tue Jun 30 18:02:11 2015 UTC,"You'll see differences even with the top tier ""k"" models as far as how well they overclock. So even among the CPUs rated and sold for the same frequency there can and will be some variance."
hardware,3blu6m,Destects,1 point,Tue Jun 30 17:57:20 2015 UTC,"No, all i5's (of the same model #) are equal by default, but what it does mean is that some are capable of going beyond their specs. Some might be capable or reaching a higher frequency without losing stability than others, some have ""locked"" cores that are perfectly good, while others have ""locked"" cores that aren't usable and will cause stability issues if you try to access them.  This is still in the ideas of the simple model. There's a lot more to the whole process and world of silicon than all this."
hardware,3blu6m,Randomoneh,1 point,Tue Jun 30 20:38:18 2015 UTC,Does that mean that the cost of production of the most expensive i7 is actually similar to that of a cheapest i3?
hardware,3blu6m,blueberrypoptart,2,Tue Jun 30 21:54:43 2015 UTC,"The cost of production of the most expensive i7 would actually be the cost for a single top tier i7 along with the cost of all of the chips that failed to meet the bar. They just happen to be able to still find a market for the lower performing chips under a different tier to smooth out the costs. Saying that the cost of production of the most expensive version vs the cheapest is like saying the pristine version is the same cost to produce as the dud. While in a way true, it's overly simplistic. If I only ever wanted the lowest tier chip, I would be making it in a totally different manner.  Manufacturing is about the yields. You can't perfectly create X number of exactly Y chip. There will always be variability in performance of the output, so matter what their 'target' is, the output will always fall in some spectrum of chips."
hardware,3blu6m,Destects,2,Tue Jun 30 22:47:44 2015 UTC,"Pretty much. Given that they're both built with the exact same template, use the same materials, in same quantities. The i3 started out as an i7, but failed to meet quality requirements in one way or another."
hardware,3blu6m,TheBloodEagleX,1 point,Wed Jul 1 01:04:27 2015 UTC,I believe i3s are just lower binned i7s.
hardware,3blu6m,Theodoros9,20,Mon Jul 6 00:26:14 2015 UTC,I actually think there aren't enough cards. There is a real hole right now between the 960 and 970 that could be filled. I suspect its just the Australian price gouge. but there is a big difference between a $280 960 GTX and a $480 970 GTX.
hardware,3blu6m,shrewduser,6,Tue Jun 30 07:10:52 2015 UTC,i expect a 960TI or something
hardware,3blu6m,plank_,3,Tue Jun 30 08:12:48 2015 UTC,"There was a rumor for a 960Ti, but there's probably only so much they can do to fit in the gap when a 960 is quite literally half a full Maxwell 970/980 chip. A faster 960 would make them more competitive against a radeon 380, however until I hear otherwise I'm expecting to wait until the next full new chip to significantly shake up the market."
hardware,3blu6m,Yearlaren,1 point,Tue Jun 30 09:49:48 2015 UTC,They can make a further cut down GM204 a GTX 960Ti.
hardware,3blu6m,americancer,1 point,Tue Jun 30 14:48:53 2015 UTC,"I can't imagine what the RAM would look like with it even more stripped down, unless they just disable that extra 512MB lower-priority cache altogether or something (but that might result in a card that has better more consistent performance, confusing everyone in the market).  A 3.5GB or lower VRAM card might make sense - not sure."
hardware,3blu6m,Yearlaren,2,Tue Jun 30 19:56:15 2015 UTC,"Me neither, but I'm pretty sure it would still perform like one would expect, between a GTX 970 and a GTX 960, and that's what's important."
hardware,3blu6m,AssCrackBanditHunter,1 point,Tue Jun 30 22:15:14 2015 UTC,"960 is running around 200 right? And occasionally a sale knocks a 970 to the 275 range. That's a pretty narrow gap to try and fill, especially when most people would rather just shell out 20 bucks for the 970"
hardware,3blu6m,Theodoros9,4,Tue Jun 30 15:08:46 2015 UTC,"Like I said. In America the gap seems to be closer, but because in Australia the prices are higher the gap seems larger.  Nearly $500 for a 970 is still a lot of money here and thats typically the cheapest 970 you can find. Plenty are over $500."
hardware,3blu6m,AssCrackBanditHunter,1 point,Tue Jun 30 15:11:36 2015 UTC,Oh I totally missed your last sentence. Yikes. I'm sorry about those prices dude. I'm surprised there's not a way to order from America and have it shipped cheaper
hardware,3blu6m,Shandlar,2,Tue Jun 30 15:12:53 2015 UTC,"That is essentially what they are doing.  The USD got really strong this year and it's fucking things up like that.  The AUS is only 77c on the dollar now.  So right off the bat, $289 for a 270 is 370AUS.  I'm surprised they are north of 500AUS though.  Definitely a bit of fuckery going on there.  Does AUS have a huge import tariff?"
hardware,3blu6m,aziridine86,1 point,Tue Jun 30 15:46:30 2015 UTC,Right now GTX 960's are normally running $185-220 after MIR versus $310-360 for GTX 970's.  4GB VRAM GTX 960 push up a little higher running $215-240.  GTX 970's may rarely go on sale as low as $275 but that isn't the norm.  I would say there is a healthy price gap there for a GTX 960 Ti.
hardware,3blu6m,Tiramisuu2,4,Wed Jul 1 01:10:25 2015 UTC,The right answer is not simply binning.   As yield increases in a process the manufacturer cripples perfectly good silicon to match their pricing strategy.   Silicon is tested statistically not on a per unit basis.   And nvidia definitely practices market based pricing both regionally and socioeconomically as does Intel in the consumer market.   The ability that a multinational has to hide money  and shift it around makes it very hard to understand what real margins are like and often there is at least an implicit collusion in pricing.   So yup binning
hardware,3blu6m,Randomoneh,1 point,Tue Jun 30 19:25:16 2015 UTC,Silicon is tested statistically not on a per unit basis.   Hm. Interesting. Any proof of this?
hardware,3blu6m,kardkoach,2,Tue Jun 30 21:58:21 2015 UTC,Economies of scale : extracting as much as you can from 1 technology and have low average cost per unit.
hardware,3blu6m,hdshatter,1 point,Wed Jul 1 06:29:08 2015 UTC,They have been caught price fixing before.
hardware,3blu6m,RX10,0,Tue Jun 30 21:32:28 2015 UTC,"To accomodate as many possible budgets they can. You get X card for 200 , ad the better Y card for 300 . But maybe you have just 250, so a Z card for 250 would meet your budget giving you something slightly better than 200 X card"
hardware,3blu6m,toresimonsen,-8,Tue Jun 30 07:40:01 2015 UTC,"It presents an illusion of choice in a market that offers few meaningful choices.    Suboptimal cards pollute the consumer stream and force upgrades.  By providing hardware manufacturers with suboptimal components for off-the-shelf builds, the end-user is forced to upgrade.    The failure of suboptimal card sales also leads to an aggregate pricing strategy that increases the costs of the optimal cards to compensate for the losses of cards that are basically unmarketable directly to the consumer.  In general, consumer fatigue eventually sets in as consumers are presented with information overload that leads to poor decision making and reduces consumer satisfaction.  Confusion also helps marketeers overcome the obvious deficiencies of the product by diverting attention away from deficiencies which would otherwise make the product DOA.  For example, if you are busy comparing TDP, PPW, DB, and 3dmarks, you might not notice the lack of full support for the next-gen API's."
hardware,3blu6m,Oafah,5,Tue Jun 30 08:34:02 2015 UTC,"It's not that you're necessarily wrong, but that has very little to do with the simple question OP was asking.  The right answer is ""binning"". End of discussion."
hardware,3blu6m,toresimonsen,1 point,Tue Jun 30 13:28:18 2015 UTC,The processs of creating suboptimal cards goes beyond simply binning.  I find it interesting that binning itself appears to shift the cost of failure.
hardware,3blu6m,headband,-2,Wed Jul 1 11:09:39 2015 UTC,It's not that much effort. The two are pretty much the same card so it won't take any extra work.
hardware,3blu6m,Kubi74,-7,Tue Jun 30 07:09:32 2015 UTC,Because there is not enough competition.
hardware,3bkpdu,random_digital,215,Tue Jun 30 00:35:53 2015 UTC,"I don't think Lisa Su was talking about CPUs and GPUs. As it stands, AMD does not make personal computers, it makes personal computer components. I suspect this is just a quirk of the language used.   Just look at the wording. ""To stay away from"" implies that they are not already involved in this area.   Also, this could refer to AMD's experimentation with stressing graphics and HSA in consumer products. The server and workstation markets are generally more predictable in what they need and how to fulfill those needs.  Edit: I'll add that the title is misleading, since this isn't a direct quote at all."
hardware,3bkpdu,Exist50,49,Tue Jun 30 01:18:51 2015 UTC,"Probably referring to internet clamor for their Quantum proof of concept to become a retail product.  http://www.pcgamer.com/take-a-closer-look-at-amds-tiny-project-quantum-pc/  But in general Lisa Su has been all about getting into embedded products and IP integration services. So I wouldn't be surprised if they reduce investment in ""consumer"" components."
hardware,3bkpdu,TBradley,5,Tue Jun 30 03:02:47 2015 UTC,Is Quantum not coming to market?
hardware,3bkpdu,Exist50,8,Tue Jun 30 03:12:43 2015 UTC,Not unless another company talks AMD into letting them sell it. Or makes an approximation.
hardware,3bkpdu,TBradley,2,Tue Jun 30 03:14:38 2015 UTC,What do you mean? It seems like a pretty finished product.
hardware,3bkpdu,Exist50,23,Tue Jun 30 03:18:02 2015 UTC,"I don't know if you've looked at the inside of the thing, but it's far from a ""finished product"". They took an ASRock mini-ITX board and basically shaved off all the heatsink components they could, and made a custom waterblock for the whole thing.  http://i.imgur.com/fAS9j2f.jpg  http://i.imgur.com/pr1k08r.jpg  Funnily enough, this system uses an Intel chipset and CPU, presumably because no one makes a mini-ITX AM3+ motherboard."
hardware,3bkpdu,tadfisher,3,Tue Jun 30 03:45:08 2015 UTC,I would die if they were just trolling the shit out of us and had a CPU that was worth considering coming out.
hardware,3bkpdu,C4ples,7,Tue Jun 30 15:30:36 2015 UTC,Zen (if it arrives in a timely fashion) will be the only AMD CPU worth mentioning.
hardware,3bkpdu,Exist50,8,Tue Jun 30 17:38:09 2015 UTC,"And what's wrong with that? That's basically what all the high-end prebuilt manufacturers do. Usually they don't even modify anything.  If anything that hints that this is meant to be a serious product. AMD isn't stupid. They know that they'd be shooting themselves in the foot if they used an fx chip, since no one would buy it. Hell, AMD's own test systems use Intel CPUs. This is a vehicle to show off Fiji, not the CPU."
hardware,3bkpdu,Exist50,-6,Tue Jun 30 03:49:24 2015 UTC,"The irony of AMD showing off a super cool custom PC with an Intel chip appears to be lost on him. Regardless of it being just a slightly misguided vehicle to ""remind people of show off Fiji.."""
hardware,3bkpdu,systemghost,4,Tue Jun 30 06:22:49 2015 UTC,The cases have been handmade. Therefore a PC company would have to fund the tooling and production line. I don't think AMD is going to sink millions into such an endeavor given their current state.
hardware,3bkpdu,TBradley,1 point,Tue Jun 30 07:19:44 2015 UTC,what.. you mean like 3d printing/milling?  you over complicate this example
hardware,3bkpdu,imoblivioustothis,3,Tue Jun 30 05:42:46 2015 UTC,As in a production line with CnC machinery. 3D printing isn't economical for something like this.
hardware,3bkpdu,TBradley,-1,Tue Jun 30 19:20:02 2015 UTC,if they can bang out aluminum car wheels in no time at all i'm sure this wouldn't be much more difficult.  Just need the CAD files and raw materials.
hardware,3bkpdu,imoblivioustothis,1 point,Tue Jun 30 20:24:46 2015 UTC,And expensive equipment and tooling
hardware,3bkpdu,Alsnake55,0,Tue Jun 30 21:39:01 2015 UTC,"I would expect it to be a really niche product at best, but I see your point. Still, I don't think that is an insurmountable obstacle. They've already designed everything, it's just bulk production that's needed."
hardware,3bkpdu,Exist50,4,Sat Jul 4 00:50:30 2015 UTC,"""Just bulk production,"" huh?  Designing the manufacturing process is just as hard if not harder than designing a successful prototype.  It's also WAY more expensive."
hardware,3bkpdu,tarheel91,0,Tue Jun 30 05:53:03 2015 UTC,"In general, yes, but most of the components appear to be off the shelf. A lot of it is just fancy casing and the waterblocks, which aren't too hard to mass produce."
hardware,3bkpdu,Exist50,2,Tue Jun 30 13:00:28 2015 UTC,Spoken like someone who's never had to try and take something from prototype to production.  There is a lot of challenging work and capital involved in making that transition for even simple parts.  Your design will change multiple times even from your finalized prototype.  And then there's quality.
hardware,3bkpdu,tarheel91,6,Tue Jun 30 16:24:21 2015 UTC,"In the press call, we were told that it may not make it to market."
hardware,3bkpdu,Lelldorianx,0,Tue Jun 30 18:21:56 2015 UTC,Press call?
hardware,3bkpdu,Exist50,7,Tue Jun 30 06:26:00 2015 UTC,Yes. I work on the media side of the industry and attended the pre-release conference call discussing the Quantum box.
hardware,3bkpdu,Lelldorianx,1 point,Tue Jun 30 06:29:24 2015 UTC,Interesting. Good to know.
hardware,3bkpdu,Exist50,1 point,Tue Jun 30 06:48:09 2015 UTC,"I think that makes the most sense as well. I would love to have them partner with Alienware, who I would think would be just the vehicle for this kind of concept PC. Hell, it might bring me back to buying retail PCs instead of building them myself."
hardware,3bkpdu,phayzfaustyn,15,Tue Jun 30 07:03:46 2015 UTC,Well lets fucking hope so. The last thing Intel and Nvidia need is less competition.
hardware,3bkpdu,spyder256,12,Tue Jun 30 05:20:21 2015 UTC,"At this point, it almost can't get any worse on the CPU side. Carrizo actually seems surprisingly good (if anyone will bother to make a damn laptop with it), but AMD basically has nothing decent for the rest of the market."
hardware,3bkpdu,Exist50,1 point,Tue Jun 30 03:36:01 2015 UTC,And AMD refuses to release a desktop version of Carrizo.
hardware,3bkpdu,Elranzer,3,Tue Jun 30 03:44:17 2015 UTC,"Most of the things that make Carrizo good stem from optimization for the low power envelope. HDLs, for example, don't scale well towards high frequencies."
hardware,3bkpdu,Exist50,0,Tue Jun 30 15:39:22 2015 UTC,"http://www.anandtech.com/show/9319/amd-launches-carrizo-the-laptop-leap-of-efficiency-and-architecture-updates/4  from amd own slides, at 35 watt. There is an reduction of clock rate. The only reason why it is faster is due to increase IPC gains."
hardware,3bkpdu,idoithere,1 point,Tue Jun 30 16:25:55 2015 UTC,And AMD already only has a market cap under 2B. Getting out of a large segment like the PC industry could further downsize the company.
hardware,3bkpdu,BICEP2,0,Wed Jul 1 20:09:08 2015 UTC,Better to downsize then to go bankrupt which they are heading for if things don't change a lot. I was surprised the new cards barely gave there stock a bump and most places list them as a stock to be avoided.
hardware,3bkpdu,RiffyDivine2,0,Tue Jun 30 06:44:57 2015 UTC,That release of the new cards did not go very well...
hardware,3bkpdu,PhilipK_Dick,1 point,Tue Jun 30 13:11:48 2015 UTC,I thought everyone was beating off like a monkey to get one?
hardware,3bkpdu,RiffyDivine2,1 point,Tue Jun 30 18:53:37 2015 UTC,HBM yields were low. We won't really know how it went until supply overtakes demand.
hardware,3bkpdu,BrainSlurper,1 point,Tue Jun 30 19:08:37 2015 UTC,Yeah this was my question coming here. What this is saying is they wont produce video cards for their own computers (if they made them) OR supplying video cards for someone like Dell.  But they will continue to produce components to be used privately in PCs  ...I hope
hardware,3bkpdu,jbourne0129,1 point,Tue Jun 30 21:14:43 2015 UTC,Basically they don't want to start competing with their customers ( OEM pc builders ).
hardware,3bkpdu,1usernamelater,-7,Tue Jun 30 12:12:34 2015 UTC,"I don't think Lisa Su was talking about CPUs and GPUs.   She was.   AMD does not make personal computers it makes personal computer components.    I think you missed the ""for"" in her statement.  AMD's components are made for the personal computer market.  Nobody is talking about AMD making or not making personal computers - your leading thought is a red herring.   Lisa Su wants to wind the retail component side of the business down because they are losing money trying to compete with Intel and Nvidia.  The embedded markets are more profitable and much less competitive - basically, if you beat a competitor on price for one product you get business over multiple seasons.  This is the opposite of the retail component market where your products are competing with others across a range of tiers and the process repeats every three to six months.  AMD can't afford that kind of volatility anymore."
hardware,3bkpdu,steak4take,21,Tue Jun 30 18:30:26 2015 UTC,"That is completely at odds with everything talked about in AMD's financial analyst day, a far better representation of the company's direction that a paraphrased interview.   http://images.anandtech.com/doci/9239/AMDAssumptions.jpg  You can quite clearly see that embedded is far from the only market AMD's going for. If anything, it is likely secondary to professional graphics (where AMD has seen good growth in) and server (clearly the target of Zen and K12). These are high margin, profitable areas, and utilize the same tech as the consumer side.   http://images.anandtech.com/doci/9239/AMDPortfolio_575px.jpg  Again, AMD is continuing to invest in high end graphics and CPUs. Apparently, Zen alone takes the majority of the company's R&D money. That isn't a chip designed for the embedded market. It's meant for servers, where AMD doesn't have to compete with every Chinese chip designer with an ARM license.   As a final note, this isn't a direct quote, so it's wrong to assume there's a ""for"" in her actual statement. Furthermore, it's rather presumptuous to unilaterally say ""she was"" talking about one thing or another through so many layer of abstraction."
hardware,3bkpdu,Exist50,-6,Tue Jun 30 03:59:57 2015 UTC,"You seemingly don't understand how to read a marketing picture - servers are part of the embedded market, in point of fact, many server platforms are delivered in embedded form in a variety of methods - from palm sized through to modular blade style easily scalable solutions.  Secondly, professional graphics is not personal computer - not in terms of pricing, seasonal release scheduling or warranties.     Again, AMD is continuing to invest in high end graphics and CPUs.   Not for personal computers they won't.  Sure, some of the products for the prosumer side will bleed off into personal computing at th enthusiast level but it's not going to be their focus.   Apparently, Zen alone takes the majority of the company's R&D money. That isn't a chip designed for the embedded market. It's meant for servers   You keep using the word ""servers"" as if it's some cure-all.  Let me address your misconception : the server market for x86 is not a growth market.  x86 servers have decent challengers from the ARM side and have had for a couple of years now - so much so that Anandtech did an investigative piece on the subject in 2013.  And in terms, x86, AMD is still quite strongly overshadowed by Intel's Xeon in the server space in both the lowend and highend tiers.   Again, AMD is continuing to invest in high end graphics and CPUs.   Again, this does not indicate that they care a whit about the consumer desktop market anymore.  Hell, if the rumours are true - their potential buyers (samsung et al) are companies whose consumer focus runs entirely contra to the component market, so why would AMD be focusing on things which wouldn't make them valuable to potential buyers?  They wouldn't.  You're grasping at straws.   These are high margin, profitable areas, and utilize the same tech as the consumer side.    They are also slow moving and contractual over long periods.  And so what if they use the ""same tech""?  That doesn't mean that ""same tech"" will reach consumers by proxy.  Nope, this AMD preparing for war - batrening down the hatches and focusing on what might allow them to survive beyond the next season or at least what might make some parts of them saleable to other, larger, more directly focused parties."
hardware,3bkpdu,steak4take,7,Tue Jun 30 04:11:55 2015 UTC,"servers are part of the embedded market   Are there servers that use embedded chips? Yes, but they are far from the biggest chunk of the market. Pretty much all of Intel's lucrative server portfolio is in socketed chips. Furthermore, it's kind of funny to criticise how I read a chart that clearly shows embedded being distinct from server. I'm not sure how you're confusing this.   Not for personal computers they won't. Sure, some of the products for the prosumer side will bleed off into personal computing at th enthusiast level but it's not going to be their focus.   The development is not separate between these two departments. You can't simultaneously say AMD's developing for the professional market but not the consumer. It's the same tech, same R&D spending. There are only a few minor differences between AMD's consumer and professional chips.   x86 servers have decent challengers from the ARM side and have had for a couple of years now   If you'll read the conclusion to the anandtech article, the consensus was that no one had a reasonably competitive offering yet. That is now even more true with Xeon D for a low power SoC.   Let's make something clear here. AMD has basically nothing in servers right now. Regardless of whether or not its a growth market (with cloud computing becoming more popular, I think it is), If AMD can even take 10% from Intel, that would be huge, enough to make the company very solvent. It's an area where AMD can only grow.   And you are forgetting K12, the ARM sister core to Zen. If ARM truly has a shot, I think K12 will be the best bet, since AMD still has a lot of good server IP and chip design experience that other ARM companies lack.   Hell, if the rumours are true - their potential buyers (samsung et al) are companies whose consumer focus runs entirely contra to the component market,   A) These are nothing but rumors for now and B) Samsung's not in the component market?!? What are you smoking? They are easily one of the top manufacturers of computer and tech components.   so why would AMD be focusing on things which wouldn't make them valuable to potential buyers?    Maybe because AMD isn't looking to be bought....   And so what if they use the ""same tech""? That doesn't mean that ""same tech"" will reach consumers by proxy.   Yes, it basically does. Once you have a professional card, it is almost negligible to cut some VRAM, neuter the DP, and viola, you have a consumer card. Do you really believe there is so little money to be made in consumer discrete GPUs that what is more or less a sticker change isn't worth it?"
hardware,3bkpdu,Exist50,27,Tue Jun 30 04:46:43 2015 UTC,"Context is important. These comments come on the heels of shares of Mircon tanking ~50%. Micron blamed sluggish PC sales for their poor earnings. Since AMD's next earnings call isn't until mid-July, Dr. Su has to get out in front of the changing market conditions and reassure investors."
hardware,3bkpdu,oddsnends,6,Tue Jun 30 05:02:30 2015 UTC,"Wow, sounds like a good time to get into Micron. They have a lot of good stuff coming up..."
hardware,3bkpdu,III-V,4,Tue Jun 30 01:18:30 2015 UTC,HMC better work well.
hardware,3bkpdu,Exist50,5,Tue Jun 30 02:06:51 2015 UTC,"It's got lower latency than system memory... so I mean, that's already a win."
hardware,3bkpdu,III-V,17,Tue Jun 30 02:15:43 2015 UTC,"It dropped 18.2%, not 50. On that note, Hynix hasn't been having a good month either."
hardware,3bkpdu,Exist50,31,Tue Jun 30 02:29:30 2015 UTC,"It's down 50% from its 52 week high, and over 30% from just a few months ago.  Source:  I have entirely too much micron stock"
hardware,3bkpdu,Evidence_Of_Absence,9,Tue Jun 30 01:20:17 2015 UTC,Ouch. Sorry for your losses.
hardware,3bkpdu,Exist50,2,Tue Jun 30 02:06:38 2015 UTC,I can tell you that they are investing in their fabs if that's any long term reassurance.
hardware,3bkpdu,mrcavooter,6,Tue Jun 30 02:15:23 2015 UTC,I'm only down 25% and not particularly worried.  I have no issue holding it for years.  It's super undervalued.
hardware,3bkpdu,Evidence_Of_Absence,2,Tue Jun 30 02:55:53 2015 UTC,"Wow, that P/E is super low. Why was there such a tank this past year?"
hardware,3bkpdu,smoothsensation,6,Tue Jun 30 03:11:55 2015 UTC,"Biggest factor was a massive overreaction to the last quarter and revised guidance.  That was last week.  General market conditions and today's bloodbath haven't exactly helped.  Not sure what happened to drop it from the mid 30s to the mid/high 20s (probably just the first signs of market headwinds, and that entire sector has been trending downward for a bit now), that was before I got in.  I got in around $26.50 and kept on buying down to $24.50 or so, then again around $19.50, now I'm done for a bit.  Shouldn't have tried to catch a falling knife.  But yea, a P/E of 6 is ridiculously low.  I think my favorite part about the most recent sell off is that all the analysts who were downgrading dropped their price targets by like 10% to $30 or so, then the stock cratered by about 20% to $19.50.  Like I know analyst ratings are bullshit but you figure they'd be consistent with their deltas...  I'd be pretty surprised if I'm not at least even within 1-1.5 years, although obviously that would still be disappointing compared to what I was expecting."
hardware,3bkpdu,Evidence_Of_Absence,4,Tue Jun 30 04:26:23 2015 UTC,"Sorry, should have been more clear. Down 44% year-to-year, with about half those losses in the last month. And, yea, taken with Hynix, it looks like most PC component manufacturers are going to have a rough year."
hardware,3bkpdu,oddsnends,-4,Tue Jun 30 04:38:02 2015 UTC,"market conditions and reassure investors   Nothing AMD can do will cause that to happen, there stock is mostly viewed as garbage. I was shocked to see it still stay around 2.50USD even after the new cards came out. That's 10:1 in Nvidia's favor. Sadly the company will be lucky to dodge filing as bankrupt in the next few years."
hardware,3bkpdu,RiffyDivine2,4,Tue Jun 30 01:41:21 2015 UTC,"You do realize individual stock price means absolutely nothing when it comes to company evaluations, right?"
hardware,3bkpdu,reynardtfox,-1,Tue Jun 30 13:15:33 2015 UTC,"If it's a start up then yes it doesn't matter, in AMD's case it's a clear sign of trouble. Just look at there numbers and debt vs what they bring in each year. It says that investors have no faith in the company and most have been ditching the stock for awhile now. Nvidia stock is also cheap but shows some signs of activity other then people dumping it."
hardware,3bkpdu,RiffyDivine2,1 point,Tue Jun 30 15:32:37 2015 UTC,"Having cheap stock doesn't mean a thing (with a few exceptions if you really want to get pedantic).  Even for a start up.  Start-ups ideally wouldn't even have a stock price until they've matured.  Their initial evaluation would be from an investor via seed funding which is an entirely different thing from stock price.  A stock price would only occur once an IPO is made (like Facebook and Twitter did a few years ago).  AMD's stock at its current price is nothing new.  It's been hovering between 2.40 and 2.50 for the past 3 months with a few instances of it breaking 2.60.  Nvidia's sitting at a 3 month low despite having an overall upward trend for the past year so they're not in a terrible position when compared to AMD.  On top of that they're valued at 11.8b while AMD is at around 1.84b, so even by those numbers AMD is only 1/6th the size of Nvidia, not 1/10.  If you look at it in the context of the last year, then yeah, AMD is doing rather poorly and they have been for awhile, but the price of its stock on its own when compared to Nvidia's doesn't mean shit.  The way the stock trends (which in this case yeah AMD is in a lot of trouble here so you're not wrong here your logic is just flawed) along with market cap is what you should be looking at.  Google's stock might be at 523.80 and Apple's at 125.81 but that doesn't mean Google is worth x5 as much as Apple. Apple is 2x Google in size based off of Market Cap, but Google's P/E ratio is much higher meaning investors feel that Google has greater growth potential than Apple.  Point is, raw stock prices mean nothing unless put into context.  They're just numbers.  A company with a low stock price might have a lot more shares outstanding than a company with a high stock price, or maybe it's P/E ratio is better, etc. all of which provide much more information about how a particular stock is performing as opposed to its current stock price."
hardware,3bkpdu,reynardtfox,3,Tue Jun 30 18:42:38 2015 UTC,"Having cheap stock doesn't mean a thing.   ... Unless the stock used to be not-so-cheap, and hasn't split."
hardware,3bkpdu,SPOOFE,1 point,Tue Jun 30 18:54:15 2015 UTC,"Well yeah, that's a single instance in which it would matter, but that is context in which you can derive information out of it.  But if you just looked at Company X and Company Y and saw that Company Y's stock was worth more than Company X's and decided that Company Y was the ""better buy/company"" you'd be making that decision with poor judgment, which was my entire point to begin with.  edit: and before you go ""Well AMD's stock used to be worth 100 USD"" yeah, that's not a really relevant statistic anymore given that that was nearly 15 years ago when that was true.  For the past 5 years AMD's stock has been hovering between 2 and 10 dollars, and in the past year it's been well between 2 and 4.  2.50 isn't a great price for them to be at admittedly, but given their circumstances I'm surprised they've managed to maintain their shareholders as well as they have."
hardware,3bkpdu,reynardtfox,2,Tue Jun 30 22:11:53 2015 UTC,"Oh, I wasn't making any commentary on the AMD vs. nVidia thing. I just wanted to point out that stock price is only relevant compared to its own history."
hardware,3bkpdu,SPOOFE,2,Tue Jun 30 22:20:22 2015 UTC,"I made the edit just in case you were since I wasn't entirely sure where you were planning on taking your line of reasoning.  That said, I absolute you agree.  Stock price is only relevant when put into context/compared to its own history."
hardware,3bkpdu,reynardtfox,6,Tue Jun 30 22:37:00 2015 UTC,This would be very sad. Where is Cyrix when you need them?
hardware,3bkpdu,Imidazole0,23,Wed Jul 1 05:52:48 2015 UTC,Hopefully burning in hell with all of their shitty floating point units. Its been 18 years and im still pissed I couldn't get in a good game of quake without crashing.
hardware,3bkpdu,not_in_the_face,4,Tue Jun 30 00:51:21 2015 UTC,Heat death wouldn't be enough to eliminate Cyrix.
hardware,3bkpdu,Spreadsheeticus,6,Tue Jun 30 01:12:18 2015 UTC,VIA bought them. They invented the Mini-ITX format.
hardware,3bkpdu,Elranzer,2,Tue Jun 30 19:02:45 2015 UTC,"Rember their p-scale processors? P200 133mhz (performs as well as pentium 200, ran 133mhz. I want a Cyrix p5k!"
hardware,3bkpdu,Imidazole0,16,Tue Jun 30 15:39:01 2015 UTC,Amd has to find a profitable business or an exit strategy.  They are getting squeezed on all fronts.     A focused AMD might survive. They don't have the weight to be into everything.
hardware,3bkpdu,Tiramisuu2,4,Tue Jun 30 21:31:21 2015 UTC,If AMD consolidates their research into only the high-end market then they can carve out a presence the same they did when establishing themselves as the bang-for-buck vendor before the days of the 295X2.
hardware,3bkpdu,DJ-Foran,4,Tue Jun 30 02:18:28 2015 UTC,How big is the high end market?
hardware,3bkpdu,Anterai,1 point,Tue Jun 30 02:36:25 2015 UTC,"Just ball-parking this, but last year's annual report(check page 10) puts AMD's revenue at 61% from their top customers. Of those customers, the main ones being Sony/Microsoft were responsible for Embedded, custom solutions that accounted for their top Enterprise business end, shipping out over 90% product to the enterprise end.  ...I'd say their focus on discrete solutions is the low 30%(check page 96) under ""Computing and Graphics"" where they are currently operating on a loss. Bit of an anecdote here: AMD's ability to capture the console market and APU market is the only thing keeping them afloat right now."
hardware,3bkpdu,DJ-Foran,1 point,Tue Jun 30 16:12:58 2015 UTC,How do you get that they are currently operating at a loss based on page 96?
hardware,3bkpdu,Spreadsheeticus,1 point,Tue Jun 30 16:30:21 2015 UTC,You're going to have to be specific here: What are you seeing?
hardware,3bkpdu,DJ-Foran,2,Tue Jun 30 17:55:55 2015 UTC,"Oh- I'm just not seeing where they are operating at a loss on the table.  Seen your name popping up throughout this discussion, and you're right on about everything.  Just got hung up on this-  the table on page 96 does not appear to total like you'd expect on a balance sheet."
hardware,3bkpdu,Spreadsheeticus,30,Tue Jun 30 19:48:12 2015 UTC,"AMD plans to stay away from developing for the personal computer market because of its volatility. Instead, the company is looking to play where AMD’s strengths intersect with growing demand while trying to engender investor confidence with a steady release of new products that highlight progress for the long term.   I don't think they meant they want to stay away from creating enthusiast gpus and cpus, but rather move away from what they have been dabbling more heavily in for the past couple years - budget and basic APU's and CPU's. This would go more in line with what they have said recently about no longer wanting to be the ""budget option""."
hardware,3bkpdu,rationis,27,Tue Jun 30 22:11:15 2015 UTC,"I don't think AMD marketing would organize a PC gaming E3 event if that was the case.  Also, look at Fiji and Zen. The former is an enthusiast (and possibly professional) GPU and the latter will launch as an enthusiast and server product for PC, with an ARM derivative launching a year later. To me, Zen is a clear case of AMD voting with their wallet. They're looking at the x86 and ARM server markets and deciding which one will give them the best and the quickest return. Their choice is the x86 enthusiast + server market."
hardware,3bkpdu,Compatibilist,1 point,Tue Jun 30 01:08:42 2015 UTC,I'm thinking the opposite. Intel have the prebuilt and laptop market on lockdown.   The only place AMD can make a profit is components for enthusiasts. They want to distance themselves from budget parts for low end dell type machines.
hardware,3bkpdu,TeutorixAleria,4,Tue Jun 30 01:58:35 2015 UTC,"AMD plans to stay away from developing for the personal computer market because of its volatility   my read into this is that AMD will cut it's R&D budget. According to their year end finacial report, they spent $1.07B on R&D.  Which makes sense, because the R&D for HBM, and the ZEN is done. AMD can ride that tech for a few years, while trying to get back to profitability.  This news doesn't seem as bleak as if you read it this way."
hardware,3bkpdu,dasiffy,1 point,Tue Jun 30 02:10:47 2015 UTC,"The news may not, but the stock being less then 2.50USD a share is worrisome."
hardware,3bkpdu,RiffyDivine2,2,Tue Jun 30 13:25:31 2015 UTC,Is this in response to people asking them to produce project quantum? Seems like it.  Definitely isn't them getting out of the cpu gpu game like some have suggested.
hardware,3bkpdu,Gunjob,2,Tue Jun 30 12:25:01 2015 UTC,"Translation - ""We're done trying to please you fuckers - WE OUT!"""
hardware,3bkpdu,b00j,1 point,Tue Jun 30 12:58:02 2015 UTC,"This is... odd. Probably a reason why she wasn't quoted on this, right?  ""She said, however, that AMD plans to stay away from developing for the personal computer market because of its volatility. Instead, the company is looking to play where AMD’s strengths intersect with growing demand while trying to engender investor confidence with a steady release of new products that highlight progress for the long term.""  That doesn't make sense. It sounds like someone twisted this story into sounding like they're getting out of the PC market when she's really saying they're not going to be in the BULIDING PC market."
hardware,3bkpdu,Overcloxor,-5,Tue Jun 30 06:30:59 2015 UTC,"Well that's really disappointing and not good honestly. AMD needs to go back to what they were in the early 2000's when they had CPU's that could contend with Intel. The whole ""shove a lot of cores on it"" and ""APU's everywhere!"" strategies are not working."
hardware,3bkpdu,atriax,20,Tue Jun 30 11:10:53 2015 UTC,"How could they? AMD just doesn't have the capital to do what Intel does. It's like saying that HTC should just flood the market the same way that Samsung does. HTC already does this, but they are losing to Samsung's larger war chest(e.g marketing, technology, and capital). Running with the phone analogy, if AMD pulls an Apple and is able to turn Zen into a winner then we can see AMD carve itself a market presence."
hardware,3bkpdu,DJ-Foran,7,Tue Jun 30 19:53:17 2015 UTC,"I remember the days when Intel wasnt even in my mind. If I was getting a CPU, it was AMD. I remember my first Athlon 1ghz, what a beast."
hardware,3bkpdu,dravell,4,Tue Jun 30 00:59:11 2015 UTC,"There were never days where you didn't consider Intel, just where amd was the better option."
hardware,3bkpdu,dylan522p,3,Tue Jun 30 02:42:06 2015 UTC,"It's worth remembering that the early 2000s was when intel had P4/Netburst which wasn't best for enthusiasts. Since 2006 (when they ditched P4, and then developed from P3 to make Core) they haven't really made a similar mistake.  If anything the P4 is a mirror of AMD's Bulldozer, intel pushed a single core and ""megahertz matters"", Bulldozer pushed multicore. The performance per clock situation was mirrored too, AMD brought in the ""performance rating"" labelling for their processors, and for Core intel moved to model numbers."
hardware,3bkpdu,plank_,5,Tue Jun 30 01:17:08 2015 UTC,"To be fair, ""APUs everywhere!"" is exactly what is happening in the consumer market. It's just that people still value the CPU side and other things more than the raw graphics performance."
hardware,3bkpdu,Exist50,-3,Tue Jun 30 04:52:11 2015 UTC,This sounds bad. Who's going to buy fury if they are planning to divert their resources!?
hardware,3bkpdu,FPSNige,-5,Tue Jun 30 12:43:16 2015 UTC,"or, in non-marketing talk :  AMD CEO : ""AMD IS SAD IT GETS KICKED OUT OFF THE PERSONAL COMPUTER MARKET BECAUSE IT DOESN T SPEND ENOUGH MONEY IN DEVELLOPMENT"""
hardware,3bkpdu,MerryLane,2,Tue Jun 30 01:22:13 2015 UTC,"I dunno: that should be applying to the entire industry thus far, including Intel where a Nehalem chip is still relevant. That's not a bad thing, but it's not fair to pin it all on AMD. I mean, if you want to talk enthusiasts: isn't AMD moving in the right direction with Skybridge, then Zen? It's a whole lot more interesting to enthusiasts than yearly die-shrinks."
hardware,3bkpdu,DJ-Foran,1 point,Tue Jun 30 04:48:09 2015 UTC,Skybridge was canceled.
hardware,3bkcji,Creative-Name,5,Mon Jun 29 22:46:50 2015 UTC,It seems a bit too thin compared to the original. Can they still get the same build quality in the keys and case if they're aiming for such a small form factor?
hardware,3bkcji,Kaghuros,3,Tue Jun 30 02:34:43 2015 UTC,"At the very least, modern hardware should enable a thinner design. Whether they compromised elsewhere (should this become an actual product) remains to be seen."
hardware,3bkcji,Exist50,0,Tue Jun 30 04:26:09 2015 UTC,There's laptops thinner with full mechanical keyboards nowadays
hardware,3bkcji,dylan522p,5,Tue Jun 30 04:54:41 2015 UTC,"I assume you're talking about scissor switch keyboards, and not Cherry MX switches. I think there's only a single laptop with Cherry Switches and it's many times thicker than this looks. I would rather have the new ThinkPad keyboard than scissor switches, but I really want the old Thinkpad keys. The depth of the keypress is a big deal to me."
hardware,3bkcji,TrptJim,2,Tue Jun 30 05:20:33 2015 UTC,And this is presumably meant to be a workhorse laptop. You don't have to worry about fitting 980m SLI or anything crazy in there.
hardware,3bkcji,Exist50,2,Tue Jun 30 05:07:53 2015 UTC,"Internally, I am squealing like a teenage mod girl at a Monkees concert at the possibility of finally being able to replace my T410 with something better."
hardware,3bkcji,atomicthumbs,1 point,Tue Jun 30 16:14:26 2015 UTC,I would absolutely love for this to happen.
hardware,3bj5sj,speckz,23,Mon Jun 29 17:34:54 2015 UTC,4GB of RAM? I thought we'd moved on to 8 as standard...I guess the price is pushing the lower ends now.
hardware,3bj5sj,squashed_fly_biscuit,7,Mon Jun 29 20:36:51 2015 UTC,"The new i7 is 8gb as far as I can tell. Sadly, there is still no option for 16gb, though for what the computer can handle, it would probably be overkill."
hardware,3bj5sj,Keyboard_Frenzy,1 point,Mon Jun 29 21:36:55 2015 UTC,Is anyone going to use more than 4 for on this anyway? My desktop has 8 and the only time I ever get close to maxing it out is while playing BF4 + Firefox with a bunch of tabs + media player.
hardware,3bj5sj,adhoc92,7,Tue Jun 30 05:25:25 2015 UTC,With next gen graphics being quoted as 50% faster than current- I'm waiting :) plus... Throttle much?
hardware,3bj5sj,Imidazole0,4,Mon Jun 29 17:42:51 2015 UTC,"Unfortunately, introducing a new model of the SP3 probably means the SP4 is a ways out... I was really hoping for a summer release so I could nab one before school starts up again."
hardware,3bj5sj,Im12AndWatIsThis,5,Mon Jun 29 23:04:43 2015 UTC,"My guess is opposite of yours. That this lower storage i7 helps them clear out i7s that are stocked up and unsold, in preparation of sp4. With skylake and win10, I bet we see a sp4 before the end of November"
hardware,3bj5sj,Imidazole0,1 point,Mon Jun 29 23:16:58 2015 UTC,"I'm betting September as one of the first Skylake laptops/tablets (they may even try to squeak it onto shelves in time for back to school).  The Surface 3 is an okay Broadwell device for W10's launch in a month, but they need a flagship device that can take advantage of any and all new features of W10, including full USB Type-C support."
hardware,3bj5sj,Charwinger21,2,Tue Jun 30 07:39:37 2015 UTC,Imagine full support including external graphics adapter support... A killer GPU dock and then this might be the only system the average gamer/user would ever need.
hardware,3bj5sj,Imidazole0,1 point,Tue Jun 30 11:55:18 2015 UTC,"Throttle much   yep, that's why people talking about being able to play games on Surface don't know shit. they run a benchmark for 5 minutes and conclude everything's great when in reality there's no way the surface sustains those same framerates an hour in"
hardware,3bj5sj,Hariooo,3,Tue Jun 30 14:44:17 2015 UTC,"Sadly we can't upgrade the ssd without acquiring a dreamer and forfeiting warranty.   I'm wondering, what are the benefits of the surface pro? I know it has touchscreen and you have to buy a keyboard, but what makes it popular? I got a laptop 1-2 weeks ago for $400 with an i5-5200U and 6 GB of ram on 15.6"", and upgraded it with an $80 ssd (250 GB 850 evo) and I like it."
hardware,3bj5sj,TheImmortalLS,9,Mon Jun 29 23:34:04 2015 UTC,"A tablet can be very convenient for using on public transport, lying down in bed, or anywhere else where you don't have a surface (heh) to use a laptop on.  At the same time, having a real laptop for ""casual"" uses like taking notes, web browsing, or doing word processing has advantages over an Android or iOS tablet for those same purposes-- basically there are tasks ""for a laptop but not a tablet"" even though they don't require a powerful desktop.   And the Surface covers both of those bases. As a main device for somebody who doesn't need much power, it's not lacking too badly compared to a laptop yet it also does everything an iPad would. And a power user who has their gaming desktop for intensive tasks can use it for everything they would reasonably do on-the-go and won't be driven crazy by the limitations of Android and iOS."
hardware,3bj5sj,Unique_username1,2,Tue Jun 30 01:15:25 2015 UTC,Thanks!
hardware,3bj5sj,TheImmortalLS,-6,Tue Jun 30 06:26:01 2015 UTC,who the hell would pay $2000 for that!!!! I dont get it...
hardware,3bj5sj,joshrmacd,6,Tue Jun 30 02:42:23 2015 UTC,people that make a lot of money...  or take out lots of loans.
hardware,3bj5sj,t_Lancer,-9,Tue Jun 30 05:25:42 2015 UTC,The same people who bought previous versions of Surface: Nobody!
hardware,3bj5sj,mcotoole,6,Tue Jun 30 10:38:53 2015 UTC,Yes.  Because it makes sense to keep releasing iterations of products that aren't selling.  I see Surfaces everywhere I go now.  I guess you're not paying attention?
hardware,3bi4kc,Vince789,5,Mon Jun 29 12:22:57 2015 UTC,Exceptional article. This is why Anandtech is one of the best.
hardware,3bi4kc,Yourothercat,1 point,Mon Jun 29 18:05:18 2015 UTC,What others come even close to this?
hardware,3bi4kc,zxcdw,1 point,Tue Jun 30 11:31:06 2015 UTC,I would like to know as well.
hardware,3bi4kc,Yourothercat,2,Tue Jun 30 12:36:15 2015 UTC,"Techreport have excellent articles, here's their Note 4/Exynos 5433 review for comparison."
hardware,3bi4kc,Andrei_TV,1 point,Tue Jun 30 13:53:50 2015 UTC,"Thanks, I'll add it to the daily rounds."
hardware,3bi3co,speckz,11,Mon Jun 29 12:10:28 2015 UTC,"the point of the article, is basically delivery times are never right, shameless fraud is rare, and kickstarter outperforms vc for creating popular products.  but many kickstarters still use vc's   kickstarter to me seems like a way for people to be early adopters and help a product that they might like.  for business that start kickstarters it seems like the perfect way to not only showcase your product and get attention, but get capital that you can reinvest into your company with almost no strings.  it also makes you more competitive for good vc's and better terms.  it's a cool idea, that i think is here to stay as long as successful products are still coming out of it."
hardware,3bi3co,logged_n_2_say,4,Mon Jun 29 13:39:37 2015 UTC,While reading this my thoughts were. Venture capitalists fail at backing popular products. They have the one thing everyone else doesn't have which is money. Only after a successful kickstarter when the projects already got some money no strings attached they hop on the bandwagon and strap strapping their cash on with strings.  In other words they fail at identifying a good idea. Kickstarter identifies good ideas but doesn't financially reward its backers. Which then venture capitalists hop on and reap the rewards.
hardware,3bi3co,MINIMAN10000,1 point,Tue Jun 30 06:14:18 2015 UTC,"Kickstarters are very unique in that they sell to consumers as if they were investors with the interest of a return for both parties; product realization for the consumer, and future expansion for the kickstarter."
hardware,3bigk9,grohl,7,Mon Jun 29 14:20:49 2015 UTC,Uhhhh... this seems really obvious. What alternatives are there? Are we going to run software on our imagination now?
hardware,3bigk9,FabianN,3,Mon Jun 29 19:46:24 2015 UTC,"The options are to implement your networking processing algorithms in software to be run on a CPU or in hardware as part of an ASIC. ASICs are cheaper for mass-produced systems and can more easily be made to handle very high speeds, but when a flaw is discovered or you want a new feature, it takes years to develop and deploy an upgrade. CPUs are flexible but more power-hungry and only recently became competitive options for some network processing tasks."
hardware,3bigk9,wtallis,2,Mon Jun 29 22:00:25 2015 UTC,"Outside the data center, though, you really need to use software to paper over the flaws in the hardware. Anything that can be done within the performance envelope of x86 shouldn't be trusted to the kinds of ASICs that exist at the moment, and that includes everything about home networking."
hardware,3bigk9,wtallis,1 point,Mon Jun 29 16:16:04 2015 UTC,"Can you elaborate please? Clearly software is used to paper over flaws, because software can be altered relatively cheaply But what that has to do with the performance envelope of the CPU isn't clear to me.   You certainly don't want the CPU busy doing things a good PCI adapter can take care of (sound, network traffic, I/O)..."
hardware,3bigk9,frugal_masturbater,5,Mon Jun 29 21:04:29 2015 UTC,"Off the shelf network hardware tends to suffer from problems like bufferbloat. As an example, my gigabit ethernet switches add 7ms of latency under load for traffic leaving through a port that's only running at 100Mbps (and about 1.4ms added latency under load when it's running at gigabit speed). Since the switching and queuing is handled by an ASIC, it's not really fixable, and the ASIC vendors have been completely silent on when they're going to stop making switch chips that do dumb things like tail-drop.  On my router where packets are handled by software, I can upgrade the software to a version of Linux that has a modern AQM that not only keeps queueing latency low, it also does fair queueing (and NAT and firewalling) along the way. The price is that the '90s-era MIPS CPU core in my 5 year old router can't quite forward packets at 100Mbps in both directions simultaneously the way the ethernet switches can, but the ARM chips in current home routers can do it at hundreds of megabits/s and even an x86 Atom has no trouble doing line rate for gigabit ethernet.  The various offload engines found on networking SoCs and advanced NICs (for doing things like NAT or packet aggregation) are similarly subject to widespread design flaws that make them strictly harmful until you've actually run out of CPU power to handle things without the ASIC.  ASICs are great when they do the job right. Network ASICs don't, because they were all designed with bad priorities. The cable industry had to choose a sub-par AQM strategy to be part of the DOCSIS 3.1 standard because the ASICs in cable modems aren't flexible enough to use the state of the art AQM strategies and the CPUs in cable modems aren't powerful enough (because they were chosen under the assumption that the ASIC could do all the performance-critical stuff). But for the speeds the cable modems are designed to upload at, a sufficiently-powerful CPU would have added mere cents to the cost of the device once the reduced complexity of the ASIC was taken into account."
hardware,3bigk9,wtallis,1 point,Mon Jun 29 21:33:16 2015 UTC,"Interesting bit about DOCSIS and how actual implementation is impacted, thank you."
hardware,3bg65w,meatwad75892,19,Sun Jun 28 22:58:08 2015 UTC,"Everything looks very big for a 4k display, are you using anything else than 100% dpi scaling? The image you've taken a screenshot of only properly works at 100% dpi scaling afaik. There's also ""shadows"" on some of the text, but i dont know if im seeing a lossless or lossy version of the image so i cant say anything about that for sure."
hardware,3bg65w,Lysiticus,8,Sun Jun 28 23:50:04 2015 UTC,"Yes, I do. I set the scaling back to 100% (I use 200%) and opened those images back up.   Text is still legible in that one test image, purple ""4:4:4"" in the other test image is still there.  Those actual screenshots may or may not be lossy.. Just grabbed them via Snipping Tool as pngs, and they're just sitting in Google Drive. But yes, I also see the slight shadowy-ness, but it's still pretty clear. From what I've read on other forums that posted that test image in the first place, the bottom line should look like a jumbled up mess when not using 4:4:4."
hardware,3bg65w,Lysiticus,6,Sun Jun 28 23:59:34 2015 UTC,"While having the scaling set to 100% look at that image up close, are the letters having any ""transition"" pixels inbetween the surrounding color and the text color? Like a really faint version of the text color? I don't know if you'll understand what i mean :p"
hardware,3bg65w,Lysiticus,7,Mon Jun 29 00:05:47 2015 UTC,"It actually does, but it is very faint. Nowhere near as bad as the garbled mess that I'm ""supposed"" to see per examples on AVS forum, Geeks 3D etc.  http://www.geeks3d.com/20141203/how-to-quickly-check-the-chroma-subsampling-used-with-your-4k-uhd-tv/  But, on the other hand, I seem to be failing out when it comes to this test:   http://s807.photobucket.com/user/floatpoo/media/444_faq/tint-blue-rgb-1080p.png.html  Red & Magenta have some noticeable noise around them versus the other columns' text.  I'm going to take a guess that I'm not at 4:4:4, but short of these forced test images as examples, I see nothing funny or ""off"" everywhere else.  So if if I'm at 4:2:2 or 4:2:0, seems it will be good enough for my unknowing eyes."
hardware,3bg65w,IndigoMoss,6,Mon Jun 29 00:13:15 2015 UTC,"Probably 4:2:2, 4:2:0 would be very noticeable. But as you say, it shouldn't really be noticeable unless specifically looking for it."
hardware,3bg65w,robertotomas,16,Mon Jun 29 00:17:18 2015 UTC,"If haven't used nVidia in a while, but in AMD drivers you have a menu you can choose for pixel output, which has 4:4:4 rgb, etc. What's yours say?"
hardware,3bg65w,youra6,3,Sun Jun 28 23:43:42 2015 UTC,"I strongly doubt you are getting 4:4:4 at 60Hz.    Known 4K 60Hz 4:4:4 TVs  LG UB8500 and higher, available in sizes 49"" and larger. Some owners see horizontal lines from the passive 3D, even in 2D.    ...   Possible sets,  LG UB8000/UB8200, thinking of ordering one my self to test. Available 40""-55"". (Edit: Reading manual shows no HDMI port labeled 10-Bit, and specifically states 4:2:0.)   src 4K TVs that can do 4:4:4 at 60Hz over HDMI 2.0"
hardware,3bg65w,mmencius,3,Mon Jun 29 02:47:04 2015 UTC,"I believe I've arrived at the same conclusion. And it's fine with me, because everything still looks kickass on this display.  :)  Something I read somewhere else is that HDMI 2.0 currently exists in two forms: ""Draft"" and ""Full"", and of the two, draft is the one that cannot do 4:4:4 at 4K/60Hz.  Is draft vs. full a function of the physical connector itself, or the firmware? As in, could TV currently incapable of 4:4:4 at 4K/60Hz become capable of it via a future firmware update?"
hardware,3bg65w,exscape,3,Mon Jun 29 13:18:30 2015 UTC,"I'm glad I saw this thread because the manual itself says that this TV can only do 4:2:0 chroma.  HOWEVER:  There was a firmware update on 6/18/2015 (04.11.08) and I'm wondering if that update has to do with anything you are seeing.  Maybe I'm just stupid but I cant seem to find anywhere where LG shows you the change logs so I'm left in the dark about knowing what actually was in that update.  I emailed their support team just a moment ago, and I'll PM you what they tell me."
hardware,3bg65w,autowikibot,5,Mon Jun 29 05:29:57 2015 UTC,Can someone ELI5 4:4:4 chroma subsampling? Like... 4:4:4 and chroma and subsampling. ELI5 every syllable.
hardware,3bg65w,Ellimis,11,Mon Jun 29 06:52:47 2015 UTC,"I'm not sure every detail can be explained to a 5-year old, but...  Chroma subsampling itself means that instead of using three color channels (R/G/B) to represent the information about a pixel, the information is split into luminance/luma information (how bright each pixel is) and chroma information (its color, though you need the luma information also to figure the pixel color out).  After that split, the amount of color information is greatly reduced (this being the actual subsampling step), for example by halving the number of pixels in each direction, which removes 3/4 of the color information, greatly saving space. This works because the human eye is far less sensitive to color changes than it is to brightness changes.  These numbers then represent how much of the color information remains. 4:4:4 means everything remains, so that's the best possible case. 4:2:2 means the horizontal resolution of the color information is halved, but the vertical remains as-is. 4:2:0 means both are halved.  Exacty how the numbers work is explained in the Wikipedia article. I believe that trying to re-write it in my words would just make it more difficult to read, so I'll just link instead: https://en.wikipedia.org/wiki/Chroma_subsampling (see especially ""Sampling subsystems and ratios"")  Blu-ray and DVD both use 4:2:0 only, i.e. the worst commonly accepted option, and that still looks pretty darn good. The big problems arise with fine details, as are common on computer monitors."
hardware,3bg65w,Randomoneh,2,Mon Jun 29 07:08:13 2015 UTC,"Chroma subsampling:       Chroma subsampling is the practice of encoding images by implementing less resolution for chroma information than for luma information, taking advantage of the human visual system's lower acuity for color differences than for luminance.   It is used in many video encoding schemes — both analog and digital — and also in JPEG encoding.    Image i     Relevant: Luma (video) | Chrominance | VP9 | 4-2-2   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3bg65w,putin_vor,3,Mon Jun 29 07:09:09 2015 UTC,I explained it in great detail last week here: http://www.reddit.com/r/hardware/comments/3a74zp/amdmatt_fury_x_does_not_have_hdmi_20/csa91y3
hardware,3bg65w,sir_drink_alot,2,Mon Jun 29 20:26:44 2015 UTC,You are definitely doing something wrong since that font in your test image should be 1 line thick. Yours is double.
hardware,3bg65w,doodlydo0,1 point,Mon Jun 29 01:41:41 2015 UTC,"The source image is here.  http://www.ozone3d.net/public/jegx/201412/chroma-subsampling-test-4k-tv.png  You can see that all the text and the lines are 1px thick. On your screenshot they are all 2px thick. You clearly are not displaying the test at 100% scale.  Don't use Windows Photo Viewer, that program is absolute garbage. Install Irfanview or another good alternative."
hardware,3bg65w,jamend,1 point,Wed Jul 1 18:27:00 2015 UTC,"have the same TV and setup, it def does 4:4:4 at 60hz only from HDMI 3 input on deep color mode. Low input lag, works perfectly fine. I paid 1100 at the time, 599 is a great deal for this TV."
hardware,3bg65w,bphase,-1,Mon Jun 29 11:56:22 2015 UTC,"Honestly chroma sampling is not as big a deal as people make it out to be, all your videos are 4:2:0, all your photos are 420, your television stream is 420.  at 4K unless your doing graphic work on a monitor or somehting you won't be able to tell the difference.  remember your chroma resolution isnt REALLY 1/2  or 1/4,  it's still full ressolution because it's moving and its interpolated by the luma being ontop of it.    say a girl has a red dress, it's still red, the luma values go overtop and change the color from light to dark red, so your still getting full resolution often.  there are other instances such as greenscreen or txt where it CAN make a difference, but all and all 420 is fine."
hardware,3bg65w,crazy_goat,2,Mon Jun 29 15:41:02 2015 UTC,"That's fine for videos and photos, but have you actually tried it on a PC? It makes regular sized black on white text look horrible. It also makes a few of the default Windows mouse cursors invisible."
hardware,3bg65w,piovocsic,4,Mon Jun 29 19:14:51 2015 UTC,That's uhh.. interesting.
hardware,3bg65w,Kaghuros,1 point,Mon Jun 29 04:06:50 2015 UTC,Appropriate username.
hardware,3bg65w,King_Tryndamere,-8,Mon Jun 29 04:27:59 2015 UTC,Just cause you're sending a full 4:4:4 signal doesn't mean the panel is displaying it. Look at some test images designed to show the 4:2:2 chroma subsampling and you'll see it.
hardware,3bg65w,ShoutingDani,12,Mon Jun 29 05:01:06 2015 UTC,In his post he said he loaded up a couple of test images and they seemed to look like they were supposed to.
hardware,3bg65w,haekuh,7,Mon Jun 29 00:36:30 2015 UTC,Poor automod getting downvoted for what it was wrote to do. :(
hardware,3bhmt6,dylan522p,4,Mon Jun 29 08:11:42 2015 UTC,1.9 Billion but that's for a dual core Iris 6100.  http://www.anandtech.com/show/9320/intel-broadwell-review-i7-5775c-i5-5675c
hardware,3bhmt6,Scrabo,2,Mon Jun 29 08:52:01 2015 UTC,Yeah certain variants I can find but 4c GT3/4 I can't find
hardware,3bdwdi,wasdzxc963,41,Sun Jun 28 08:13:39 2015 UTC,"I just hate how the graph makes it look horrible in its lack of context. What they showed was this:   82.00% increase out of an expected (2/1)-1=100.0%  49.58% increase out of an expected (3/2)-1=50.00%  30.20% increase out of an expected (4/3)-1=33.33%   Without explanation, to someone who isn't reading, it looks horrible after the first card. They could have followed up by explaining this:   (82.00/100.0) = 82.00% increase  (49.58/50.00) = 99.16% increase relative to two card system  (30.20/33.33) = 90.60% increase relative to three card system   Or you can look at the functional number of cards relative to one:   4260 /4260 = 1.00 = 100.0%/card (Obviously)  7754 /4260 = 1.82 = 91.00%/card  11599/4260 = 2.72 = 90.76%/card  15102/4260 = 3.54 = 88.63%/card   I find that last part the most helpful."
hardware,3bdwdi,leafthegreen,3,Sun Jun 28 17:47:28 2015 UTC,Another way of doing that last bit could be like this ([Score]-[Score less 1 GPU])/[Base score] or (7754-4260)/4260=0.82  so here the preformance increase of each GPU over a previous # of GPUs relative to the baseline score for one GPU of 4260:    # of GPUS Score % increase from previous    1 4260 0%   2 7754 82%   3 11599 90%   4 15102 82%
hardware,3bdwdi,omega552003,56,Sun Jun 28 22:52:42 2015 UTC,"Holy shit, that Crossfire scaling is really, really good."
hardware,3bdwdi,SeaJayCJ,19,Sun Jun 28 08:34:46 2015 UTC,XDMA is a godsend for AMD. Running tri fire r9 290s and its smooth with great scaling.
hardware,3bdwdi,fzed1199,9,Sun Jun 28 13:11:38 2015 UTC,Well it is 3DMark.
hardware,3bdwdi,JakSh1t,11,Sun Jun 28 14:55:54 2015 UTC,"Anyone know how the 980 Ti compares?  This might be where the Fury X has the lead, since I believe CF scales better than SLI (unless if that's change now, then sorry my bad)  Edit: maybe not then"
hardware,3bdwdi,makar1,25,Sun Jun 28 10:01:16 2015 UTC,"The top 4-way 980Tis are around the 16-17000 level on that particular benchmark, compared to the Fury's 13286.  (18000 vs 15000 for GPU-only score)"
hardware,3bdwdi,SeaJayCJ,7,Sun Jun 28 10:31:59 2015 UTC,"Those are LN2 overclocking numbers, though."
hardware,3bdwdi,makar1,21,Sun Jun 28 11:07:40 2015 UTC,This is not LN2: http://www.3dmark.com/fs/5201721  Kingpin's Titan X result is what you really get with LN2: http://www.3dmark.com/fs/4951493
hardware,3bdwdi,SeaJayCJ,9,Sun Jun 28 11:12:08 2015 UTC,I guess it's probably asking too much to see scores at stock clocks.   Now we just need AMD to unlock the damn voltage so we can see how Fiji/HBM OCs.
hardware,3bdwdi,itguyy,-1,Sun Jun 28 11:19:18 2015 UTC,Seeing as there will be no custom partner boards and they are already hitting the 8+8pin power limit without unlocked voltage you aren't going to see much unless you install a custom VRM. So basically it is what it is.
hardware,3bdwdi,TheImmortalLS,1 point,Sun Jun 28 22:35:05 2015 UTC,8 pins provide more power in practice than dictated by specifications.    power limit will help when voltage is unlocked.
hardware,3bdwdi,itguyy,1 point,Mon Jun 29 05:05:14 2015 UTC,"I would make sure you have appropriately guaged psu cables then, and hope that the non-custom VRM can handle the power going through it when over volted. There were some Gigabyte 280x cards that had their VRMs blow when GPU mining due to the increased power draw that entailed so it's definitely something to be cautious of. I've also seen cables start to melt/catch fire when people were gpu mining on their cards, which admittedly might be something unique to mining but it's still a concern. There is a reason why these specifications exist."
hardware,3bdwdi,TheImmortalLS,1 point,Mon Jun 29 18:44:12 2015 UTC,"Stressful like Furmark, but as long as the VRMs are cooled adequately and are rated as such (ex. 60A at 60C, 40A at 90C, etc.), should be fine.  However, I heard the Fury X has 100C gaming VRM temperatures :/"
hardware,3bdwdi,SeaJayCJ,1 point,Mon Jun 29 22:44:09 2015 UTC,"People were getting insane power draws on overclocked 290X cards despite the supposed PCIe power limit. HardOCP got 500+W out of a single 290X, despite that being an 8+8 card."
hardware,3bdwdi,CrazyLineman777,-6,Mon Jun 29 07:41:37 2015 UTC,He asked about scaling not card performance. Good job
hardware,3bdwdi,logged_n_2_say,4,Sun Jun 28 15:30:13 2015 UTC,"i'm having trouble finding one with all 4 score, by the same user but here is 2x 980ti vs 4x980ti and it has pretty decent scaling considering the clock is slower on the 4x.  http://www.3dmark.com/compare/fs/5104687/fs/5092609  the 4x graphics score is ~94% scaling the 2x graphics score.  but scaling is usually really good on firestrike and other synthetics, compared to real world usage.  if i apply the same concept to the fury x graphics score, i get ~97% scaling of 4x compared to 2x, but the clock looks to be the same for the fury x."
hardware,3bdwdi,ultimateflip,3,Sun Jun 28 16:06:00 2015 UTC,How did you arrive at the 94% and 97%? The graphics scores on the page has the 4x980ti at ~87.1% higher than 2x (18698 vs 9995). The graphics score of 4xfury x is ~94.8% higher than 2x (15102 vs 7754).
hardware,3bdwdi,logged_n_2_say,6,Sun Jun 28 16:44:59 2015 UTC,"100% scaling would be the score multiplied by however many gpu's you increase it by, so you multiple 9995x2 = 19990 if you were want to compare the scaling of 4 gpus.  now compare the actual score to the 100% scaled score  18698/19990 = 0.935 x 100 = 94%  i did the scaling for fury as well  2x 7754/8520 = 91% scaling or 9.4138% difference 3x 11599/12780 = 91% scaling or 9.6887% difference 4x 15102/17040 = 89% scaling or 12.059% difference"
hardware,3bdwdi,ultimateflip,3,Sun Jun 28 16:52:54 2015 UTC,"Ah, okay. I used the bad method of comparing numbers."
hardware,3bdwdi,namae_nanka,1 point,Sun Jun 28 16:55:30 2015 UTC,"Actually you used the correct method, what he's done, amusingly, is to include the 100% of the first score.  Because the first by default is at 100% so the numbers so calculated for the second will be artificially inflated due to including them. And more amusingly will make the first card appear slower than the 100%."
hardware,3bdwdi,logged_n_2_say,1 point,Mon Jun 29 00:56:35 2015 UTC,"And more amusingly will make the first card appear slower than the 100%.   I don't know what you're saying, did you mean faster than 100%? Or, are you saying for scaling we shouldn't consider the first card at all? Also I mention it's flawed without a single card score for the 980ti, but used the same method to compare fury to 980ti. Regardless the same outcome is established, fury scales slightly better.   But the whole thing is flawed because the clocks aren't the same on the 980ti, again which I said in the post."
hardware,3bdwdi,makar1,1 point,Mon Jun 29 02:20:25 2015 UTC,"You can easily determine that by comparing 4x 980Ti with 4x Fury, and 1x 980Ti with 1x Fury."
hardware,3bdwdi,CrazyLineman777,-2,Sun Jun 28 16:43:08 2015 UTC,"Yes, so why didn't you do that, or link to an article that does?"
hardware,3bdwdi,makar1,1 point,Sun Jun 28 16:48:57 2015 UTC,You don't need an article to do a basic comparison between 4 numbers.
hardware,3bdwdi,Sofaboy90,-9,Sun Jun 28 16:53:32 2015 UTC,and youre saying that without a source ?
hardware,3bdwdi,makar1,9,Sun Jun 28 10:49:57 2015 UTC,The link above is using 3DMark. I'm quoting the 3DMark scores..  http://www.3dmark.com/hall-of-fame-2/fire+strike+3dmark+score+ultra+preset/version+1.1
hardware,3bdwdi,skilliard4,-7,Sun Jun 28 10:52:00 2015 UTC,HBM really helps the cards communicate to maintain high framerates.
hardware,3bdwdi,TheImmortalLS,1 point,Sun Jun 28 18:24:18 2015 UTC,Unrelated. Driver and pci-e bus.
hardware,3bdwdi,skilliard4,-4,Mon Jun 29 05:06:10 2015 UTC,"You and I know AMD's drivers are absolute rubbish. If NVIDIA can't manage this level of scaling despite having vastly superior drivers, it's quite obvious that this isn't the case."
hardware,3bdwdi,TheImmortalLS,1 point,Mon Jun 29 05:43:32 2015 UTC,HBM does not affect scaling. It's the pci-e bus and drivers that allow multi gpu scaling.
hardware,3bdwdi,ZeM3D,28,Mon Jun 29 16:15:18 2015 UTC,Holy shit that scaling is amazing
hardware,3bdwdi,BlayneTX,8,Sun Jun 28 09:35:34 2015 UTC,What about some game benchmarks? Is there still the stuttering issue that has been shown in gameplay videos? Does it require an X99 system to scale that well?
hardware,3bdwdi,Aelenning,6,Sun Jun 28 12:31:54 2015 UTC,I would be pretty sure it requires at least the 5930k from the x99 system for the extra pcie lanes in order to scale that well.
hardware,3bdwdi,logged_n_2_say,4,Sun Jun 28 15:05:52 2015 UTC,"in firestrike and synthetics, yes.  real world gaming, not likely.  but most with 3-4 x $650 gpu's will likely have an x99/x79 system anyway."
hardware,3bdwdi,continous,1 point,Sun Jun 28 15:14:49 2015 UTC,"Similarly, the scaling drop from 3rd to 4th tends to get exaggerated with real world gaming."
hardware,3bdwdi,BlayneTX,1 point,Wed Jul 1 10:12:45 2015 UTC,What about 2x Crossfire? Would two 8x pci slots be enough bandwidth?
hardware,3bdwdi,Aelenning,5,Sun Jun 28 15:23:56 2015 UTC,Yeah. I haven't seen benchmarks or anything but I believe that'd be plenty. I've heard 8x4x4 works with 3 way sli/xfire so 8x8 would work fine.
hardware,3bdwdi,astronomicat,4,Sun Jun 28 16:19:05 2015 UTC,Nvidia won't let SLI work on an x4
hardware,3bdwdi,Aelenning,1 point,Sun Jun 28 16:25:18 2015 UTC,My mistake. Was not aware.
hardware,3bdwdi,makar1,1 point,Sun Jun 28 16:32:44 2015 UTC,"An x4 PCIe 2.0 lane already bottlenecks the 290, so you'd really need an SLI-compliant board for Fury Crossfire."
hardware,3bdwdi,fzed1199,1 point,Sun Jun 28 16:50:52 2015 UTC,Id get hitching and really bad stutter when my x79 ran default pci e 2.0
hardware,3bdwdi,astronomicat,1 point,Mon Jun 29 04:42:57 2015 UTC,You can get z97 and z87 boards that have plx chips which make it possible to do 3 way and 4 way sli with lga 1150 chips
hardware,3bdwdi,fzed1199,1 point,Sun Jun 28 16:23:24 2015 UTC,Or a 4820k
hardware,3bdwdi,logged_n_2_say,1 point,Mon Jun 29 04:40:32 2015 UTC,Legit reviews got the numbers from Matt who works for amd.
hardware,3bdwdi,BlayneTX,1 point,Sun Jun 28 14:02:48 2015 UTC,Legit.
hardware,3bdwdi,C4ples,7,Sun Jun 28 15:22:54 2015 UTC,"Seeing as they're verified through 3dMark, yes. They are legit."
hardware,3bdwdi,iNoToRi0uS,4,Sun Jun 28 16:01:47 2015 UTC,The scaling is nearly perfect! Can't wait to see how it is with DX12
hardware,3bdwdi,namae_nanka,10,Sun Jun 28 14:24:19 2015 UTC,"The numbers have changed, using the graphics score instead of total score, since I commented on that article.    AMD Radeon R9 Fury X x1 = Overall Score 4,337 / GPU Score 4,260 AMD Radeon R9 Fury X x2 = Overall Score 7,557 / GPU Score 7,754 (82.0% scaling) AMD Radeon R9 Fury X x3 = Overall Score 10,801 / GPU Score 11,599 (49.58% scaling) AMD Radeon R9 Fury X x4 = Overall Score 13,286 / GPU Score 15,102 (30.20% scaling)     Now the third card still has the best scaling while the second has the worst."
hardware,3bdwdi,Gunjob,61,Sun Jun 28 08:52:24 2015 UTC,"From the Comments.   They did their scaling wrong like idiots..  you don't scale system total, you aren't adding a second cpu with the second card, and nor a 3rd or 4th CPU with the 3rd and 4th card.  The graphics score is what you scale, which was 15307,  Just 1 R9 Fury X scores - 3960 points graphics wise in this test. That means a perfect score of 100% scaling would be 15840.  That means these cards scale at 100% 99% 99% 99%.  It's damn near perfect scaling.  Source for single card score. http://wccftech.com/amd-radeon...  edit  you can even see it in their test.  2x fury X scored 7754 graphics - http://www.3dmark.com/3dm/7507... 4x fury X scored 15307 graphics - posted here  it lost a total of 201 points in scaling. That is damn near perfection."
hardware,3bdwdi,MilkyTones,24,Sun Jun 28 10:19:57 2015 UTC,I can't help but feel sorry for AMD when their stuff is put in a bad light even if it's by accident.
hardware,3bdwdi,TeutorixAleria,10,Sun Jun 28 13:46:51 2015 UTC,"Yeah what the fuck were they doing, 100% scaling would be around 1600 but for some reason by their calculations 100% should be 4500?   Is there an /r/badmath?"
hardware,3bdwdi,logged_n_2_say,7,Sun Jun 28 10:47:09 2015 UTC,"That means these cards scale at 100% 99% 99% 99%.   The user is right that the scaling percentage is wrong, but that is not the scaling scores of the gpu only. Its not 3960 for a single card, it's 4260 as it says in the article. The 3960 was a ""leaked"" score.   2x 7754/8520 = 91% scaling or 9.4138% difference 3x 11599/12780 = 91% scaling or 9.6887% difference 4x 15102/17040 = 89% scaling or 12.059% difference   Both of your links are broken. I'm guessing something abbreviated them with the ""..."" Ending."
hardware,3bdwdi,thinkythought,1 point,Sun Jun 28 14:06:58 2015 UTC,Holy shit this is literally the best scaling i've ever seen since the invention of modern crossfire and SLI.
hardware,3bdwdi,skilliard4,2,Sun Jun 28 23:11:25 2015 UTC,Would have been nice to see actual games benchmarked and not just Firestrike.
hardware,3bdwdi,continous,1 point,Sun Jun 28 18:25:08 2015 UTC,Like a legit review? That'd be heresy.
hardware,3bdwdi,Pyrominon,2,Wed Jul 1 10:14:32 2015 UTC,It sucks to see such incredible scaling put to waste because of the 4gigs of VRAM.
hardware,3bdwdi,fzed1199,17,Sun Jun 28 11:31:45 2015 UTC,Fingers crossed for Vram pooling
hardware,3bdwdi,Jacariah,3,Sun Jun 28 13:12:43 2015 UTC,"Wasn't it confirmed for DX12?  I thought I read that a few months ago, could be wrong though."
hardware,3bdwdi,Stingray88,11,Sun Jun 28 17:20:45 2015 UTC,If I'm not mistaken it still has to be implemented within the games themselves.
hardware,3bdwdi,_101010,2,Sun Jun 28 17:34:12 2015 UTC,Civilization BE already uses Split Frame Rendering (SFR).   Although it's true AFR is the norm everywhere else that results in the VRAM needed to be mirrored.
hardware,3bdwdi,fzed1199,3,Sun Jun 28 22:46:03 2015 UTC,The developers must support it in their games and would be dumb not to.
hardware,3bdwdi,x3tripleace3x,3,Sun Jun 28 17:51:23 2015 UTC,Why would they support a feature only a small fragment of consumers benefit from? The transition will be slow.
hardware,3bdwdi,jinxnotit,3,Sun Jun 28 18:07:10 2015 UTC,Because it automatically pools resources.
hardware,3bdwdi,_101010,2,Sun Jun 28 21:50:26 2015 UTC,It doesn't automatically do jack shit. It all has to be manually coded even if it's supported in DX12.  It's the method of implementing rendering. And it has its own downfalls. Go read about Split Frame Rendering.
hardware,3bdwdi,jinxnotit,5,Sun Jun 28 22:48:29 2015 UTC,"Yeah, you have to perform the Herculean task of telling the game via Direct X 12 API to automatically do SFR with all available GPU's. The horror.   DirectX 12 will remove the 4 + 4 = 4 idea and will work with a new frame rendering method called SFR, which stands for Split Frame Rendering. Developers will be able to manually, or automatically, divide the texture and geometry data between the GPUs, and all of the GPUs can then work together to work on each frame. Each GPU will then work on a specific portion of the screen, with the number of portions being equivalent to the number of GPUs installed.   http://www.tomshardware.com/news/microsoft-directx12-amd-nvidia,28606.html#sthash.56MtM7vW.dpuf"
hardware,3bdwdi,TeutorixAleria,2,Sun Jun 28 23:39:52 2015 UTC,I'm assuming that in SFR 4+4=!8 because some things will still be shared so we could see anything over 1x available graphics memory but probably nothing approaching double.
hardware,3bdwdi,jinxnotit,1 point,Mon Jun 29 16:38:07 2015 UTC,Even if you have to share a full gigabyte of VRAM between each card (In a worst case scenario.) in a dual 980ti/Fury set up. You would still have 6-10GB left to allocate for the frame on everything else.
hardware,3bdwdi,jinxnotit,1 point,Mon Jun 29 18:20:29 2015 UTC,It's automatically done in Direct X 12 via Split Frame Rendering.  Or... developers can take control and allocate how they want to pool those resources.
hardware,3bdwdi,frenchpan,0,Sun Jun 28 21:49:26 2015 UTC,"Look at how developers handle SLI/Crossfire right now. It's an after thought, a lot of games don't even launch with it enabled properly or the bugs are never ironed out all the way. I wouldn't get my hopes up for developers implementing a feature a very small percentage of their customer base could actually use."
hardware,3bdwdi,mmencius,5,Sun Jun 28 19:08:14 2015 UTC,"The reason SLI/CF sucks right now is because you can't manage it as a dev at all, there is no way to map multiple GPUs to a single opengl context, it is entirely controlled by the driver."
hardware,3bdwdi,cmVkZGl0,1 point,Sun Jun 28 19:38:53 2015 UTC,Wow the third card scales better than the second!
hardware,3bdwdi,brightboy,1 point,Sun Jun 28 19:10:47 2015 UTC,I think 4 of anything would be amazing.
hardware,3bdwdi,harmlessmaniac,-38,Mon Jun 29 03:21:14 2015 UTC,So I need 4 Fury X to match 2 980ti.  Nvidia still wins.
hardware,3bdwdi,Penderyn,15,Sun Jun 28 13:49:38 2015 UTC,Irrelevant username.
hardware,3bgxzd,Exist50,7,Mon Jun 29 03:15:40 2015 UTC,"The only thing I'm really wondering with the fury is how hot it will get with air cooling. Its more or less expected its going to perform 10%ish worse than the fury x.  I mean, from what I've read, one of the best things about HBM memory is its much easier and more efficient to cool the die, right? With that, the temps will be a bit higher than the fury x, but theoretically not as hot as other GDDR cards.   I'm interested in AMD's fury cards, but most likely waiting a bit for them to lower in price and for the tech to improve in general."
hardware,3bgxzd,EphemeralMemory,3,Mon Jun 29 03:25:11 2015 UTC,"Think it's gonna be a powerful tri-fan cooler on the Fury. Should be more than enough to cool a ~250W GPU, it'll end up in the 70-80s."
hardware,3bgxzd,bphase,3,Mon Jun 29 03:40:25 2015 UTC,I'm hoping PowerColor unveills a Devil13 Fury Duo.  The massive amount of fans that would go on that thing...
hardware,3bgxzd,stapler8,1 point,Mon Jun 29 05:20:04 2015 UTC,They wouldn't really need much more than the current one.
hardware,3bgxzd,EphemeralMemory,1 point,Mon Jun 29 07:17:43 2015 UTC,"I'm not sure. On one hand, the Fury should have power consumption akin to the 980ti and 290x/390x, which has been shown to be relatively easy to dissipate. On the other, however, if you use the same plate to connect the GPU and HBM, I do worry about the stability and longevity of it under the higher temps an air cooler would necessitate.   Maybe if you have one heatpipe per two stacks (one row) of HBM, flanking the main die with its pipes, that wouldn't be an issue."
hardware,3bgxzd,Seclorum,1 point,Mon Jun 29 03:28:48 2015 UTC,"I think AMD did a nice job with cooling for the product they had. I'm inclined to say that HBM is just new and for now, unoptimized, that including how they chose to cool the die. In the future though, better heatpipe placement (or number like you mentioned) will become better optimized.  Again, I just want to see how the temps compare to GDDR cards."
hardware,3bgxzd,JustaPassanger,1 point,Mon Jun 29 03:41:16 2015 UTC,All you really need is the kind of pipe density that a CPU block get's and then to fan out across the fins.   It's easier to cool it when all the heat is centralized in a relatively small area compared to almost 2/3rd of the cards surface needing contact and cooling.
hardware,3bgxzd,Seclorum,1 point,Mon Jun 29 06:05:17 2015 UTC,Not sure that's how it works. Giving a cooling block more heat to handle should theoretically reduce its efficiency but there are other factors. HBM allows the use of thinner thermal paste instead of thick thermal pads. So allowing the memory to make tight contact with a polished  heatsink will result in lower temps regardless of the slightly reduced efficiency.  And I think they can further improve that by optimizing fin area/density of the cooler etc...
hardware,3bgxzd,JustaPassanger,1 point,Tue Jun 30 04:23:32 2015 UTC,The memory is also much cooler than standard GDDR5 because it's running at less than half the speed. It makes up for this with it's enormous bus width.
hardware,3bgxzd,Seclorum,1 point,Tue Jun 30 07:41:34 2015 UTC,"True, I totally fogot about that but total power consumption is still somewhere at 250 watts I think. Not sure how much of that is concentrated at the GPU die."
hardware,3bgxzd,pb7280,1 point,Wed Jul 1 16:06:05 2015 UTC,"It's the die. The GPU core itself is over 600mm wide, which is enormous.   They obviously didn't just scale up the Hawaii or Tonga dies with more SMU's. Because if they did they would have a much higher power draw and TDP. But even accounting for efficiency gains through a more mature process and design it's still a really really big die."
hardware,3bgxzd,MerryLane,1 point,Wed Jul 1 16:22:30 2015 UTC,It's all about the TDP of the card when it comes to cooling and the Fury/X still has a very high TDP. Something a small card will struggle to keep cool with so little PCB area to build a cooler on. I expect that the cooler is going to be much longer than the PCB.
hardware,3bgxzd,Brzhk,1 point,Mon Jun 29 12:00:07 2015 UTC,"I wouldn't say it's easier to cool the die with HBM, just simpler. Before, with GDDR chips spread on the PCB, a much larger area needed to be actively cooled. Now, all the area of the PCB the chips used to take up won't need to be cooled, since it will be on the die. This however raises its own complications, as surface area plays a large part in heat dissipation (which is why heatsinks have so much surface area). Since the die is larger than before, but much smaller than the die plus RAM chips on traditional cards, it will have a much higher Watt/area density, which is much harder to cool.   The overall design of the cooler is simpler because they just need to place one big interface on the die plus cool the VRMs somehow (many cards just use separate heatsinks for this), rather than all the interfaces the heatsink needed with RAM chips on the board.  However with all that said and done, I believe HBM chips themselves use less power than GDDR chips so that means less heat. Yet they are also stacked, reducing the surface area even further, so I'm not sure how it will compare."
hardware,3bgxzd,Imidazole0,-1,Mon Jun 29 19:45:32 2015 UTC,"(Fury X - 10%) = it means a GTX 980 will always beat it in benchmarks right?  Btw : HBM doesn't make it easier to cool the die? o.O The die is the same, it's just there is an interposer under it, the HBM actually shares more easily the heat, coz it's so close ... Yeah the HBM, clocked lower, produce less heat by themselves compared to GRRD5, but the chip itself produces a lot.  So, if we make things fair, say the same 3rd party puts roughly the same fans/heatpipes on both cards : the fans will be set +/- the same to reach the same temperature. Then, let's make it realistic : the Fury has half the PCB : you basically have a furnace or an helicopter in your desktop."
hardware,3bgxzd,Yearlaren,1 point,Mon Jun 29 18:36:30 2015 UTC,"I guess it's also significantly harder to cool due to geometry. I have not looked into the details, but cooling a stack of hot pancakes is harder than cooling the same amount of pancakes laid all over the plate. The temperature in the center of the pile must be pretty high, but maybe geometry on such a small scale isn't very important."
hardware,3bgxzd,ShoutingDani,5,Fri Jul 3 01:53:37 2015 UTC,Fury will be hotter with the same performance.
hardware,3bgxzd,Meanest_Phlebotomist,15,Mon Jun 29 03:34:18 2015 UTC,That's the best case scenario. If it performs as good as the Fury X for $550 or less AMD has a winner.
hardware,3bgxzd,ShoutingDani,-1,Mon Jun 29 04:51:10 2015 UTC,"It's a fairly reasonable prediction for lower resolutions. At 4K, I think it's unlikely."
hardware,3bgxzd,Rygerts,4,Mon Jun 29 04:58:24 2015 UTC,"Ugh...if its same chip on the same clocks...and it doesn't throttle, it will be exact same performance at every resolution, just hotter."
hardware,3bgxzd,Meanest_Phlebotomist,1 point,Mon Jun 29 12:01:24 2015 UTC,"Well, yeah - but I doubt it will be the same chip at the same clocks."
hardware,3bgxzd,Phantom_Absolute,3,Mon Jun 29 13:02:59 2015 UTC,One can only dreamlike an overclocker...ill show myself out
hardware,3bgxzd,troublegoats,1 point,Mon Jun 29 15:09:39 2015 UTC,I expect it to perform the same or perhaps a tiny amount worse because the 390x's performance is pretty close to the fury x. If the fury performed like the 390x it would leave many potential buyers confused I think.
hardware,3bgxzd,jankenpwn,1 point,Mon Jun 29 09:16:53 2015 UTC,"If that was the case I don't think they'd sell it for $550.  Personally, I think it will be a bit faster than the 980 but notably slower than the Fury X."
hardware,3bgxzd,pb7280,2,Mon Jun 29 13:02:10 2015 UTC,"I don't think Fury will be a cut down chip, but I'm sure it will have a lower clock speed."
hardware,3bgxzd,pb7280,1 point,Mon Jun 29 13:56:39 2015 UTC,"i'm really hoping it isn't cut down/slower to any significant degree, I can't fit the radiator of the fury x in my case, and buying a new case would put me at a significant increase in price over the 980 ti , trying to stick with amd here though,"
hardware,3bgxzd,pb7280,1 point,Mon Jun 29 14:05:08 2015 UTC,"I haven't seen anything about this yet, but is there any word on being able to swap in existing AIO coolers onto the Fury?  My guess is that we won't be able to use the existing NZXT G10 bracket, but has anyone announced a new variant?"
hardware,3bgxzd,pb7280,1 point,Mon Jun 29 16:14:25 2015 UTC,"No one's made any announcements, but it doesn't seem unlikely."
hardware,3beb1c,pizzaiolo_,4,Sun Jun 28 12:33:05 2015 UTC,… to a seven year old ASUS motherboard that's no longer even in production.
hardware,3beb1c,duskwuff,5,Sun Jun 28 19:11:31 2015 UTC,Sadly newer boards disallow projects like Libreboot to run on them.
hardware,3beb1c,Jookia,1 point,Mon Jun 29 01:52:27 2015 UTC,Not totally true. Following board planned to be supported really soon. https://raptorengineeringinc.com/coreboot/kgpe-d16-status.php
hardware,3beb1c,Tlaurion,1 point,Fri Jul 10 02:27:51 2015 UTC,More info: www.phoronix.com/scan.php?page=news_item&px=Libreboot-Adds-ASUS-Mobo
hardware,3becyq,Randomoneh,34,Sun Jun 28 13:01:27 2015 UTC,"The bottleneck is and has always been the chips, we can always add more cooling(most notably the custom water cooling loops) however there is a limit too how much voltage a given chip can take at a certain temperature."
hardware,3becyq,olavk2,6,Sun Jun 28 13:12:13 2015 UTC,This. It has always been the chips
hardware,3becyq,alainmagnan,1 point,Sun Jun 28 20:57:06 2015 UTC,Although probably temps first and foremost. You don't hit 8ghz on an fx cpu without some serious cooling. You don't even come close but it can be done
hardware,3becyq,AssCrackBanditHunter,2,Mon Jun 29 00:37:24 2015 UTC,you can make a single perfect transistor do tens of GHz with ease. However the manufacturing consistency for transistors is way too low for you to make more than a couple of these transistors in a single multi billion transistor CPU.
hardware,3becyq,buildzoid,3,Mon Jun 29 00:54:01 2015 UTC,"1.)  If you're talking fT/fmax numbers, any modern CMOS transistor will ""do"" hundreds of GHz nominally.  But these numbers aren't particularly relevant to digital circuits, especially when you have something very large and your delay starts getting dictated by a long chain of circuits, clock routing, etc.  2.)  Speed doesn't vary anywhere near that much for transistors on the same process.  I'm not going to get into specifics, but it's nowhere near 10x for any measurable/model-able quantity (typically 6 sigma limits).  At that point you're essentially looking at a defective/completely failed transistor, not just parametric failure.  But yes, the design margins for a processor with billions of transistors and a high-volume market are going to be dictated by its worst-case corners and then some extra margin tacked on for safety.  That's one reason why binning exists and also why OCing is so easy unless you have very bad luck."
hardware,3becyq,Evidence_Of_Absence,1 point,Mon Jun 29 04:26:46 2015 UTC,"I think this is wrong. The limitation is heat now especially when scaling down & core count. The only thing that will change this is new materials aside from silicon, like graphene or light based."
hardware,3becyq,TheBloodEagleX,10,Mon Jul 6 00:59:26 2015 UTC,"Well I have a custom water cooled loop, everything is kept pretty cool. Despite keeping my 290X under 60C with a decent OC (this is a card that frequently runs at 95C with reference coolers), I cannot push the OC any further. I've upped the voltage, even used a modded BIOS that allows higher tweaking. Nothing I do helps.   Same story with my CPU. It's an old i5 2500k, and I have it stable at 4.8GHz. it idles at around 43C and hits maybe 75C on full load. I've tried upping the voltage enormous amounts, and despite keeping only a small increase in temps, I can't get to 4.9GHz."
hardware,3becyq,pb7280,3,Sun Jun 28 14:20:13 2015 UTC,"I feel like that's probably the next step for me. Overclocking has gotten a bit too easy these days, so a custom loop is really the last bastion left unexplored for me :p  It would give me an great excuse to test of those new cases (e.g. Define S) that were designed specifically with rads in mind."
hardware,3becyq,terp02andrew,1 point,Sun Jun 28 14:56:58 2015 UTC,"Yeah I need to get my one of those, I've tried stuffing radiators in my mid tower case, but it's at the point now where I can hardly close it!"
hardware,3becyq,pb7280,4,Sun Jun 28 15:05:38 2015 UTC,"Sub question: Money wise, does it makes sense to overclock or people mostly do it for fun? Considering you need to invest in better cooling, an unlocked cpu and a more expensive motherboard, I get the feeling that unless you want extreme performance (with limited real life impact), it's not worth it.  edit: I was talking about a CPU, but I guess the question applies to GPUs too."
hardware,3becyq,LeMAD,3,Sun Jun 28 19:10:35 2015 UTC,"I think it makes a ton of sense, the price difference between a 4690 and 4690k is tiny (15$ if we convert from my local currency). Getting a cooler like a 212 evo costs around 40$. 55$ is a small price to pay for 0.5-1Ghz boost in clock speed, and the cooler also gives you the benefit of reducing the noise coming from your PC."
hardware,3becyq,Thotaz,3,Sun Jun 28 21:49:53 2015 UTC,The Z motherboards are also quite a bit more expensive than their H counterparts.
hardware,3becyq,LeMAD,2,Sun Jun 28 21:59:54 2015 UTC,Are there no more P motherboards? Asking since I haven't looked into motherboards since I bought my P67
hardware,3becyq,ShoutingDani,1 point,Mon Jun 29 12:03:22 2015 UTC,Nope. Zs are the overclocking series for haswell
hardware,3becyq,AlphaGavin,1 point,Tue Jun 30 10:54:26 2015 UTC,interesting. You can use the integrated graphics on Z's right? Because I can't use it on mine;[
hardware,3becyq,ShoutingDani,1 point,Tue Jun 30 13:36:28 2015 UTC,Yeah you shouldn't have any problems... Maybe just the bios settings?
hardware,3becyq,AlphaGavin,1 point,Tue Jun 30 15:21:00 2015 UTC,Nah the p67 doesn't have a video out even. I cant use quicksync for streaming etc as well
hardware,3becyq,ShoutingDani,1 point,Tue Jun 30 16:05:35 2015 UTC,"You pay money to get a cooler that doesn't run at 5000 RPM with a 92mm lawn mower fan and a motherboard that isn't a shitter which often just means getting an ATX motherboard that isn't awful instead of a minimum priced mATX. if it's AMD they're all unlocked I believe or if it's Intel you sit there going ""Gee, for $3 I could get the iX I want with 100 more MHz clockspeed"" until you see the unlocked is $xx more, you buy that and kick it at a minimum 1000 Hz faster and realize for minimal effort and a slight premium your CPU is running faster than anything they sell. And that it probably still isn't fast enough if you're using your CPU for more than facebook."
hardware,3becyq,Marksta,3,Sun Jun 28 21:02:53 2015 UTC,"Temperature and voltage are just tools that help OCing. Temperature bottle necking is a result of your cooling, so technically your cooling is the bottleneck and not temperature. But every chip has its maximum regardless of cooling or voltage.    So you need to think of a CPU (rather incorrectly, but very simplified) as one transistor which has a wave going up and down, when that wave goes up it becomes a one and when it goes down it becomes a zero (like 1s and 0s in binary). So in a perfect world each transistor needs to be able to produce a clear one and a clear zero, however increase in frequency or increase in heat can cause the transistor to not clearly produce a one or zero. Frequency increases the rate that the waves come and go, and leaves less time in many cases for a charge or discharge of the transistor (there are different types, and transistors also can act like a capacitor and/or resistor), so you can increase the voltage which helps increase the amplitude (or height) of the wave, hopefully making it easier to differentiate between a 1 and a 0. While increasing the voltage helps increase the frequency of operation it also increases the heat and if it's too high can damage the transistor.  However anything that causes noise can also have an impact on reading whether or not the wave hit a 1 or a 0. Heat is one of those things that can not only cause noise but also mess with internal resistance. This is blatantly obvious when someone uses 1.3v on a CPU and gets 4.5ghz on air, but if they drop some LN2 on the CPU they can achieve like 5GHz at the same voltage.   Anyways so people OC to a certain point >>> increase voltage >>> increase stability + increases heat >>> too much heat causes instability >>> cooling it helps reduce instability  After a certain point cooling doesn't help much and neither does voltage, the CPU's maximum frequency is then restricted by design and material. So to answer your question, chips are their own bottleneck lol."
hardware,3becyq,sin0822,1 point,Sun Jun 28 19:28:40 2015 UTC,Would you agree with what user Imidazole says?  Is it really true that temperatures haven't been a serious obstacle for years?    Second question - how come instability comes much later with CPUs than it does with GPUs?
hardware,3becyq,sin0822,1 point,Mon Jun 29 01:31:54 2015 UTC,"I mean sure, temperatures aren't the biggest problem, you can always lower temperatures until a certain point. However, after you hit a certain temperature the CPU will throttle effectively negating the overclock. You also need to make sure that you run within acceptable temperature ranges like below 80C or whatever, but in the end its the processor itself which limits the overclock as we can overcome temperature with better cooling. In practical terms, sure you can think of temperature as the limit, but the CPU is the ultimate limit of its own overclock."
hardware,3becyq,Seclorum,1 point,Mon Jun 29 01:48:11 2015 UTC,"Second question - how come instability comes much later with CPUs than it does with GPUs?   Very good process design and implementation, Very good error correction and uncorrectable error identification."
hardware,3becyq,Imidazole0,2,Mon Jun 29 02:08:42 2015 UTC,"Temperatures haven't been a problem for many years now.   I think the ""state of overclocking"" is awesome though- with most chips being pushed 1ghz beyond factory clock."
hardware,3becyq,AssCrackBanditHunter,3,Sun Jun 28 18:35:06 2015 UTC,Seriously my 8320e is considered poorly binned because I can't push past 4.4ghz... Which is a full 33% increase in clock speed
hardware,3becyq,BKachur,2,Mon Jun 29 00:39:28 2015 UTC,"It's pretty impressive every since intel made the jump to i5-7 lines. I have an old i7-980 which came out in 2010 or 2011 and  was able to overclock it from stock 3.3ghz to 4.2 and maintain stability for everything I need. I honestly see no reason to upgrade my 4 year old cpu right now, something I never would have been able to say prior to the i7 series being released."
hardware,3becyq,Hanopp,1 point,Mon Jun 29 12:25:49 2015 UTC,"Its both at the beginning since with air cooling the temps are gonna be a problem when increasing voltages, but when u get into custom cooling, water, dry ice or even ln2 its definitely the chips that simply choke after a certain voltage threshold."
hardware,3becyq,Hanopp,1 point,Sun Jun 28 13:15:56 2015 UTC,"It seems modern CPUs are more limited by the temperature considering crazy speeds achieved by ln2, right?"
hardware,3becyq,terp02andrew,1 point,Sun Jun 28 14:02:32 2015 UTC,"Like i said, depends on your point of view. In areas of ""normal"" temps that are achievable with cooling for 24/7 usage (so no dry ice or ln2) yes the temperature is the main problem, since its hard to cool chips to temps that the silicon can take without getting fucked up. But ultimately, even ""bad"" chips can get crazy overclocks with ridiculous cooling combined with ridiculous voltages. Its all a matter of perspective. In realistic scenarios for normal end users the temperature will always be the limiting factor, for hardcore enthusiasts not so much."
hardware,3becyq,Razultull,1 point,Sun Jun 28 17:56:28 2015 UTC,"Yes and no.  While there is already much discussion about the crappy TIM Intel stuck in the Haswell chips (and even the 'improved' Devil's Canyon), most find that even under water - bad chips can't clock well haha.  There is still a great deal of luck involved. Obviously, proper methodology/experience will still get you 80-90% there, but you need a good chip first and foremost.  I still remember going to Microcenter, asking the associate for a ""Costa Rica"" and then being in awe cuz the guy actually knew what/why I was asking for that specific chip. Yeah it's not statistically significant, but it's a great camp fire story haha."
hardware,3becyq,hdshatter,1 point,Sun Jun 28 15:20:21 2015 UTC,"Lol reminds me of when the first gen i7s came out and I scouted the net for a specific batch of 950s that were quite rare. Finally found one on sale, last Amazon seller on the list, incidentally the first and last site I visited over a 2 week period."
hardware,3becyq,Penderyn,1 point,Sun Jun 28 19:14:57 2015 UTC,"IMO anything over a mild overclock is not worth the money, noise or reduction in the hardwares life expectancy.  Most aftermarket GPU coolers seem to be able to keep it cool enough at the maximum safe voltage, but expect some noise on a lot of them."
hardware,3becyq,krista_,1 point,Sun Jun 28 22:18:40 2015 UTC,"I HATE how my 3570k hits 70C at pretty poor overclocks – even though I know the chip is capable of so much more. I’ve actually just picked up a 3820 (SB-E), and I’m going to that and X79. At least then I’ll be able to say, that if I’m hitting thermal limits, its likely that I’m pushing my chip to its limit too.  PS: I should say, that I'm not prepared to risk my CPU, its warranty, and its resale value and get my hands dirty with delidding."
hardware,3becyq,TRD099,1 point,Mon Jun 29 09:42:26 2015 UTC,"Honestly, if you want more performance, OC is in a land of rapidly diminishing returns until RAM and the bus get a lot better.  A cold, non-cached pull from RAM is around 53 cycles, minimum, and can kick up to over 200 cycles pretty easily."
hardware,3bb159,zmeul,54,Sat Jun 27 14:31:03 2015 UTC,Nothing new here....
hardware,3bb159,XaeroR35,13,Sat Jun 27 15:08:42 2015 UTC,kitguru at it's best.
hardware,3bb159,dasiffy,-3,Sat Jun 27 17:02:54 2015 UTC,here's the original interview: https://recode.net/2015/06/19/oculus-rift-inventor-palmer-luckey-virtual-reality-will-make-distance-irrelevant-qa/
hardware,3bb159,dasiffy,14,Sat Jun 27 17:06:57 2015 UTC,"tbh i didn't even read it before my first post. Now that I've read it, this is really old news. I find it hard to believe this is the first time kG has made an article about VR.   A lot of words and saying nothing."
hardware,3bb159,warhead71,32,Sat Jun 27 17:20:56 2015 UTC,How many want to play using a devices that blocks all vision while playing ? Nothing casual about that
hardware,3bb159,slyf,6,Sat Jun 27 17:03:23 2015 UTC,"Oculus was able to ride without questions like this by saying ""this is future-tech, down the road it will be a consumer product, but not yet"" and now that they are releasing a consumer product they are ignoring those questions and crossing their fingers entering the market.  Sony might do better as it has predictable closed hardware you can depend on the consumer having the right setup for hardware wise and has a lot more leverage over developers and publishers.  Even the Oculus is demanding really high end PC specs...so the predictable nature of the closed PS4 should help there.    Oculus or Valve might benefit if Sony does well over on the playstation though as they can step in and offer the equivalent experience for PC gamer on whichever games are not VR-Exclusive to Sony.  The format war on the pc end will basically be Betamax and VHS..time will tell which is which.  There is a chance we can save both of them by offering just an open layer for VR which both headsets plug into...but that has not happened so far.  I do not doubt VR is amazing, but so far questions have gone unanswered because things are ""just an early devkit""..time will tell what really happens."
hardware,3bb159,Phrodo_00,6,Sun Jun 28 00:09:32 2015 UTC,"Sony might do better  Even the Oculus is demanding really high end PC specs   VR is a lot about responsiveness, and Sony has some pretty shit hardware, I really doubt the experience will be as good as on PC. (Also, the oculus works fine in my 660 and 6 years old processor)"
hardware,3bb159,slyf,6,Sun Jun 28 00:38:30 2015 UTC,You can target an ultra specific platform and get as smooth as an experience you can on that exact set of hardware.
hardware,3bb159,SomniumOv,3,Sun Jun 28 04:19:01 2015 UTC,"Yup. GearVR is running on a Galaxy Note 4, the PS4 may be a shitty PC but it's much more powerful than a phone."
hardware,3bb159,continous,1 point,Sun Jun 28 10:14:52 2015 UTC,"While that is true, you can only squeeze so much out of hardware. After a certain point the hardware simply cannot put out the power needed. I believe VR is one of those cases where you either have low fidelity, low responsiveness, or an expensive solution. For now that is."
hardware,3bb159,slyf,1 point,Tue Jun 30 02:27:45 2015 UTC,"If a phone can do it, the PS4 can do it"
hardware,3bb159,continous,1 point,Tue Jun 30 16:11:18 2015 UTC,"The phone cannot do it at the grade PCs do it. Unless you're talking pre-rendered cinematics, but even then."
hardware,3bb159,mack0409,1 point,Tue Jun 30 19:29:14 2015 UTC,"well recommended specs (yes I realise this build has a few niceties, but at this price point there really isn't a reason to not get a couple) for the oculus aren't that ridiculous"
hardware,3bb159,lossofmercy,6,Sun Jun 28 06:59:45 2015 UTC,"Razer (the developer of OpenVR) doesn't have Valve or Sony either, it's kinda irrelevant atm."
hardware,3bb159,lossofmercy,2,Sun Jun 28 02:10:22 2015 UTC,What a bone that Valve gave them. Razer is like 3 years behind Oculus and Valve.
hardware,3bb159,continous,1 point,Sun Jun 28 03:48:55 2015 UTC,"I nominate this for quickest goal post shift of 2015.  Razer may not be developing a VR headset, but making an open public standard for it is a very good thing. There also isn't much public information for it; for all we know it could be ran and managed in the same way OpenGL is. That is, many different companies pitch in for development."
hardware,3bb159,lossofmercy,1 point,Sun Jun 28 03:56:14 2015 UTC,"They are developing a headset for it, the one they had wasn't even position tracked at the time. I still don't think they have even solved position tracking, I don't see it on their hardware.  They kinda need hardware out there that implements everything according to their standard if they expect people to use it and develop for it (why would you design a VR game for a non-existent product?). I guess it would be moot if everything reVive does is considered the OpenVR standard and not SteamVR as they are calling it now, but I haven't seen an article that said that."
hardware,3bb159,lossofmercy,3,Sun Jun 28 04:47:12 2015 UTC,"There is nothing out in VR yet and quite a lot of experiments left, especially in terms of input. So its not going to ""just work"". Consolidation happens after the fact."
hardware,3bb159,continous,1 point,Tue Jun 30 02:30:02 2015 UTC,Consolidation happens after the fact.   Not necessarily. It is a very good idea to have consolidation ASAP. This is evident with graphics APIs and the like.
hardware,3bb159,kwahoo2,2,Tue Jun 30 04:52:36 2015 UTC,OSVR. OpenVR is a Valve's standard.
hardware,3bb159,kwahoo2,3,Sun Jun 28 06:49:18 2015 UTC,"I do not understand why do link to this.  OpenVR - Valve platform, basically SteamVR without Steamworks https://www.codeweavers.com/about/blogs/jramey/2015/06/18/it-s-all-about-the-team-at-e3-the-super-bowl-of-computer-gaming Supports Vive and old Oculus headsets.  OSVR - a higher level platform, build on top of many proprietary solutions. It supports OpenVR, because it it technically possible, but Valve does not care about OSVR."
hardware,3bb159,slyf,-1,Sun Jun 28 07:41:27 2015 UTC,"Well that is disappointing, shame on you Oculus"
hardware,3bb159,salgat,1 point,Tue Jun 30 02:30:54 2015 UTC,I wonder if they could do a cheap camera on the front that pops up a transparent PIP if it sees someone walk by.
hardware,3bb159,warhead71,2,Sun Jun 28 06:22:32 2015 UTC,Green screen at home and reverse green screen at arcade/dedicated setup would be supercool.
hardware,3bb159,abbzug,12,Sun Jun 28 06:48:10 2015 UTC,Why would any tech company target the mainstream before soaking the early adopters?
hardware,3bb159,communistsquared,22,Sun Jun 28 07:11:27 2015 UTC,so that kitguru would write more shitty articles
hardware,3bb159,Schmich,8,Sun Jun 28 04:19:34 2015 UTC,"First of all, when you have early adopters for new technologies it's not because they don't want mass adoption. It's because it's a necessity economically and technologically.  It takes time to develop technologies and that's why you don't find cheap 50"" AMOLED screens out there. There are hurdles to overcome for mass production and in the mean time you can sell to finance some of the R&D with the low production line.  Lastly, the part about VR being for the hardcore gamers only is due to the PC required as well as all the hassle of things not being plug and play. Again, it's not by choice to require a beefy PC. It's a technical/physical requirement as you NEED a high res screen for VR running at high frames per second."
hardware,3bb159,mack0409,2,Sun Jun 28 01:12:15 2015 UTC,"You don't really need a stupidly powerful PC, about $750 will get you all you need for the most part."
hardware,3bb159,masturbateAndSwitch,6,Sun Jun 28 08:00:07 2015 UTC,"I think the next generation of game consoles will have VR with low res and low detail graphics, then the generation after that will have actual good VR. The VR headset makers set their minimum requirements as high as they could on today's hardware where they would still have at least some market, but wanted more.    As it is, VR games are guaranteed to have low-detail graphics, considering that they need to render at 90fps locked, >1080p, 2 screens. While PC games that people are used to are designed to have enough eye candy that they can only run at 30-60 fps on contemporary hardware with the settings turned up.    The next gen of consoles should have gpus equivalent at least to the Oculus recommended spec, I would think, and then the one after that will be able to drive VR with high detail."
hardware,3bb159,SPOOFE,7,Sat Jun 27 17:01:49 2015 UTC,VR games are guaranteed to have low-detail graphics   There's generally a couple steps between Low and Very High/Ultra settings. ;)
hardware,3bb159,AMW1011,3,Sat Jun 27 18:27:22 2015 UTC,"Exactly, and its not like the difference between a medium-high settings hybrid and Ultra is very evident in most games."
hardware,3bb159,BroomSIR,13,Sat Jun 27 19:48:27 2015 UTC,Dude the next gen consoles may never come out and if they do it will be like 2021-2022.
hardware,3bb159,Kaghuros,5,Sun Jun 28 07:01:49 2015 UTC,"Yeah really. The xbox 360/PS3 were the best of console hardware for eight long years. The ""current gen"" consoles are only a bit better than a cheap gaming laptop. We're not getting real power for ages at this rate."
hardware,3bb159,BroomSIR,7,Sat Jun 27 17:14:48 2015 UTC,"Not only that but Microsoft and Sony won't make any money on the console itself for a few years until production costs drop and more games with expensive licenses get made. People who want good graphics and high fps go to pc gaming and the people who casually play games just use their phone or tablet, the console market is shrinking."
hardware,3bb159,SPOOFE,2,Sat Jun 27 18:08:26 2015 UTC,"Not only that but Microsoft and Sony won't make any money on the console itself for a few years until production costs drop and more games with expensive licenses get made.    ""From a profitability perspective, PS4 is also already contributing profit on a hardware unit basis, establishing a very different business framework from that of previous platform businesses.""  Sony's CEO, May 2014  Previous generations they developed specialized hardware for their systems. They spent billions developing Cell, for example. But this generation both MS and Sony have gone very conservative from a hardware standpoint, and I expect both to have mid-cycle refreshes that are more than just a chassis shrink. Sony in particular has an incentive to have a system that can do 4K.  I kinda expect console updates in the next two to three years."
hardware,3bb159,fzed1199,1 point,Sun Jun 28 00:28:16 2015 UTC,The one and ps4 actually make money per unit this generation whereas the previous generations of consoles took a hit till manufacture process and hardware became cheaper.
hardware,3bb159,continous,1 point,Sat Jun 27 17:44:54 2015 UTC,"I think the next generation of game consoles will have VR with low res and low detail graphics, then the generation after that will have actual good VR.   Due to the shitty situation of consoles, this may be true, but similarly, because of this, VR is mostly targeted at PC gaming.   As it is, VR games are guaranteed to have low-detail graphics   This is simply not the case, many demos have extremely high-fidelity graphics, and Unreal Engine as well as Unity are integrating VR compatibility to their engines.   1080p, 2 screens.   Many PC gamers are pushing into 4K and 5K territory with extremely high-end setups like SLI'd 980Tis and Titan Xs.   While PC games that people are used to are designed to have enough eye candy that they can only run at 30-60 fps on contemporary hardware with the settings turned up.   While this may be true, what is contemporary hardware? If you asked me, I'd consider a computer with an R9 270 and an AMD 6300 with 4 gigs of RAM pretty contemporary, to be quite honest, that just won't cut it. For really most modern games turned up, VR or not. The fact of the matter is that VR is not aimed at contemporary setups and is an enthusiast experience until contemporary hardware can deliver on that experience.   The next gen of consoles should have gpus equivalent at least to the Oculus recommended spec   Sure, but games will have moved on and made that redundant.  The fact of the matter is that consoles simply do not have the power to drive VR technology at the moment."
hardware,3bb159,Riekopo,4,Sat Jun 27 20:54:51 2015 UTC,I don't think that has to be true. It depends on how graphically demanding the VR games are made.
hardware,3bb159,Schmich,2,Sat Jun 27 20:59:53 2015 UTC,Most games will be really demanding so definitely just hardcore for now.
hardware,3bb159,slyf,2,Sun Jun 28 06:16:31 2015 UTC,Also because the product could make you sick and the illusion ruined if it starts dropping frames in a way that is not smooth so they are pushing for hardcore specs only
hardware,3bb159,Cyntheon,3,Sun Jun 28 07:03:39 2015 UTC,"And it will be after the Rift, Vive, etc. release too... Paying an extra $300 for a VR headset (aside from the screen you gotta buy anyways) isn't really in the grasp of a lot of gamers.  For example, if you're the type of gamer that gets a GTX X60 or X70 you could spend those extra $300 to get yourself an upgrade, or you could spend it on an Rift and probably not really be able to run it easily.  VR will only permeate the mainstream gaming market when it is able to replace a screen completely or lowers in price so much its no different than buying a headset. I'm surely not burning 1/3 to 1/4th of my budget on VR, specially when the GPUs I tend to get (X70s) probably won't be able to run the stuff I want without making sacrifices anyways (Oculus is 2100x1200 90Hz compared to a standard 1080p 60Hz screen)."
hardware,3bb159,flukshun,3,Tue Jun 30 02:50:06 2015 UTC,"Paying an extra $300 for a VR headset (aside from the screen you gotta buy anyways) isn't really in the grasp of a lot of gamers.   paying $500-$600 for large TVs, tablets, high-end phones, etc. for media consumption and light gaming is pretty accessible to the mainstream though. VR definitely has a place with those things. stuff like Google Cardboard is likely what will fill those roles though."
hardware,3bb159,Cyntheon,2,Sat Jun 27 18:30:17 2015 UTC,"People don't buy TVs, tablets, or phones for just gaming. Furthermore, a Rift does not replace a TV/PC screen so not only are you getting the actual PC/TV screen, but you also need to shed off an extra $300 for the Rift."
hardware,3bb159,Spore124,2,Sat Jun 27 19:43:19 2015 UTC,"People won't buy VR headsets just for gaming either. If anything, gaming will be a secondary feature for many casual buyers of VR a couple years down the line."
hardware,3bb159,elevul,0,Sun Jun 28 00:11:47 2015 UTC,Not yet.
hardware,3bb159,Abipolarbears,1 point,Sat Jun 27 18:14:28 2015 UTC,So a 970 isnt enough? Damn
hardware,3bb159,pb7280,1 point,Sun Jun 28 01:00:47 2015 UTC,"Well yeah we knew it would be for enthusiasts only in the beginning. That being said, a 290 isn't much over the top for an enthusiast, and will be even less so by the time rift rolls out. I'm more worried about the cost of the actual headset!"
hardware,3bb159,eraof5,0,Sun Jun 28 01:21:15 2015 UTC,When tey manage to use Oculus and a keyboard then things will go PRO.   It is hard playing MMO with Oculus without accessing a keyboard.
hardware,3bb159,Penderyn,1 point,Sun Jun 28 20:05:03 2015 UTC,do you look at your keyboard all the time when playing games? I don't!
hardware,3bb159,eraof5,1 point,Sun Jun 28 18:41:02 2015 UTC,"I dont always look at my keyboard. But I look at it several times, especially when I am changing the position of my hand from PVP/PVE to chatting with someone."
hardware,3bb159,Aegle,-6,Mon Jun 29 13:05:31 2015 UTC,"Most hardcore gamers I know don't care, these things give headaches and just look silly. VR needs to be like the chess game in Star Wars to work, not giant funny headsets."
hardware,3bb159,mmencius,3,Sun Jun 28 14:00:44 2015 UTC,"Good to know they could care less. That is, that they care at least a little bit. Because you couldn't care less if you don't care at all, but if you could care less, you must care at least a little bit.  Need more explanation?"
hardware,3bb159,plagues138,2,Sun Jun 28 07:43:38 2015 UTC,"what if they dont care at all, and then care even less. like, negative cares."
hardware,3bb159,fzed1199,2,Mon Jun 29 12:27:03 2015 UTC,There is a big market for racing fps and flying games. Being able to move your head will make a great advantage over conventional users.
hardware,3bb159,qwerqmaster,1 point,Mon Jun 29 18:12:48 2015 UTC,"That would be physically impossible. No one cares what people look like when wearing them, they're used at home"
hardware,3bb159,Penderyn,1 point,Sun Jun 28 01:55:03 2015 UTC,Dumbest comment of the thread award goes to.........
hardware,3bcntr,incyoubator,15,Sat Jun 27 23:26:47 2015 UTC,The ifixit teardown is better.   https://www.ifixit.com/Teardown/Amazon+Echo+Teardown/33953
hardware,3bcntr,BroomSIR,2,Sun Jun 28 01:07:56 2015 UTC,"Looks like they used different flash storage in the video one, though I can't identify the company."
hardware,3bcntr,Exist50,2,Sun Jun 28 01:54:43 2015 UTC,Step 5 •  echo have layer     im dying
hardware,3bcntr,atomicthumbs,14,Mon Jun 29 09:05:45 2015 UTC,What exactly is this point of this? Watched most of it and felt like I learned nothing valuable.
hardware,3bcntr,Exist50,4,Sun Jun 28 00:31:49 2015 UTC,"Too slow, too little information."
hardware,3bcntr,Exist50,1 point,Sun Jun 28 05:07:17 2015 UTC,What sorts of information would you have liked to see?
hardware,3bcntr,Exist50,2,Sun Jun 28 23:12:53 2015 UTC,"Ideally, if you can't name the parts on the mobo, at least a better look at them (such as in the ifixit teardown). A look at he speaker assembly also would have been nice."
hardware,3bcntr,t_Lancer,2,Sun Jun 28 23:21:16 2015 UTC,So sounds like your ideal would be a video version of the ifixit teardown (play by play w. same info they provide)
hardware,3bcntr,Gotenks0906,2,Sun Jun 28 23:23:39 2015 UTC,More or less.
hardware,3bcntr,FlabGab,2,Sun Jun 28 23:25:53 2015 UTC,great thx for the feedback. will help the channel for future videos
hardware,3bcntr,Lugia3210,1 point,Mon Jun 29 00:31:57 2015 UTC,"and if it's a video teardown, it might as well be similar to what Dave does on the EEV blog. Lot's of commentary to all the construction and electronics.  just perhaps less ranting and less repetition."
hardware,3bcntr,LightShadow,1 point,Mon Jun 29 06:43:05 2015 UTC,thx that was helpful
hardware,3bcntr,cajaks2,5,Wed Jul 1 17:36:23 2015 UTC,"I didn't think i cared that much about the Amazon Echo, but for some reason i watched your whole video."
hardware,3b8d1j,notzak,43,Fri Jun 26 20:07:08 2015 UTC,Intel has these. And some PCIe ones too. SanDisk has enterprise ones.
hardware,3b8d1j,0x60,13,Fri Jun 26 21:10:06 2015 UTC,"Yeah, but for $1000?"
hardware,3b8d1j,C4ples,9,Fri Jun 26 21:51:44 2015 UTC,"well if this is true, it goes to show that the marketing behind their 3VNand technology was partly true. They are somehow able to get those TB of storage into that form factor. Really interested to hear more about this."
hardware,3b8d1j,FitnessRegiment,13,Fri Jun 26 22:03:46 2015 UTC,I think the biggest thing holding SSDs back from going >1TB has been a controller limitation.
hardware,3b8d1j,III-V,8,Sat Jun 27 00:37:26 2015 UTC,Exactly that. Adressing large quantities of blocks can be a pain.
hardware,3b8d1j,hojnikb,2,Sat Jun 27 06:27:52 2015 UTC,what is needed to fix this limitation? Is it similar to a 32 -> 64 bit ram limitation?
hardware,3b8d1j,thewaterbottle4,7,Sat Jun 27 08:28:28 2015 UTC,We'll need to liquid cool our SSDs.
hardware,3b8d1j,Dr_professional,5,Sat Jun 27 09:33:56 2015 UTC,"""Hey did you see the new 10TB SSD that came out? This one has an i7 in it!"""
hardware,3b8d1j,bentan77,1 point,Sat Jun 27 14:21:13 2015 UTC,arm
hardware,3b8d1j,TRD099,0,Sat Jun 27 19:03:57 2015 UTC,In before dozens of people self righteously point out that SSDs don't benefit from hyper threading therefore there i7 is a waste of $100. Also nobody needs more than 512GB of SSD space. /s
hardware,3b8d1j,ScottLux,9,Sun Jun 28 22:44:21 2015 UTC,"If this is true my wallet is going to fucking hate me because in addition to the $1,300 for two new Fury Xs I'll be shelling out $1,000 for a new storage SSD.  This shit is an addiction and it is killing me."
hardware,3b8d1j,C4ples,21,Sun Jun 28 08:36:59 2015 UTC,"Eh I am not so sure if I am going to jump on this one, 2TB tech may be new and Samsung hasn't been so great lately with their new tech. I'd wait for some solid testing/reviews to come out, who knows what happens as you get close to 2TB."
hardware,3b8d1j,FitnessRegiment,7,Fri Jun 26 22:11:54 2015 UTC,Whats wrong with Samsung's new tech? 3d NAND seems pretty cool. Am I missing something here?
hardware,3b8d1j,Deluxe754,1 point,Fri Jun 26 22:15:50 2015 UTC,"current drives have slow downs, dataloss trim bug, firmware fixes that shorten lifespan of drives..."
hardware,3b8d1j,steepleton,2,Sat Jun 27 03:15:14 2015 UTC,So not really the tech but the drivers/firmware? Do you know what models are affected?
hardware,3b8d1j,Deluxe754,8,Sat Jun 27 04:42:19 2015 UTC,"The slow downs are the 840 Evos, which is a hardware issue. The Dataloss trim bug was the 840s and 50s, which was Linux only and I believe has been patched in the latest kernel, but is now up to distributions to update. The last one can be circumvented by not using the Samsung Firmware/not updating it."
hardware,3b8d1j,Vawqer,1 point,Sat Jun 27 04:51:39 2015 UTC,"Slowdowns effect regular 840 too, just Samsung never even tried to fix it"
hardware,3b8d1j,dylan522p,0,Sat Jun 27 05:51:07 2015 UTC,"Linux only and I believe has been patched in the latest kernel, but is now up to distributions to update.   they fix it by blacklisting the drive..."
hardware,3b8d1j,idoithere,7,Sat Jun 27 17:59:57 2015 UTC,Wouldn't it be cheaper to get like...4-5 500gb ones? Not like we don't all have sata ports to spare.
hardware,3b8d1j,angelofrawr,5,Sat Jun 27 09:41:31 2015 UTC,"personally, could easily get more drives, i got 8 sata ports on my motherboard... only using 2. Usually(there are people who run out of them) you dont run out of sata ports, however it is easier to manage if you only have 1 large drive vs 4 smaller ones."
hardware,3b8d1j,olavk2,8,Sat Jun 27 04:28:34 2015 UTC,No it's not. You have little-to-no redundancy in a 1 drive setup. 4 500gb drives in raid 5 or raid 10 would be far better than 1 1tb drive.
hardware,3b8d1j,chance--,1 point,Sat Jun 27 06:57:42 2015 UTC,You mean 2tb not 1. 500x4 = 2TiB
hardware,3b8d1j,Idkidks,2,Sat Jun 27 18:06:48 2015 UTC,"Nah, you're not factoring in redundancy.   Raid 10 (stripe+mirror) has 50% redundancy  4 * 500GB * 0.50 = 1000GB = 1.0TB  Raid 5 (parity) with 4 drives has 25% redundancy   4 * 500GB * 0.75 = 1500GB = 1.5TB"
hardware,3b8d1j,chance--,1 point,Mon Jun 29 00:47:52 2015 UTC,this is a terrible example.  do you really manage your SATA devices regularly?
hardware,3b8d1j,imoblivioustothis,1 point,Mon Jun 29 16:02:32 2015 UTC,You're all forgetting laptops. They don't have a lot to spare.
hardware,3b8d1j,MathiasBoegebjerg,1 point,Sat Jun 27 09:30:41 2015 UTC,"true but tbh in my laptop i dont use that much storage, also probably the reason i am forgeting laptops"
hardware,3b8d1j,olavk2,1 point,Sat Jun 27 19:27:34 2015 UTC,"Well, I do. I have 2 bays, and it's not enough."
hardware,3b8d1j,MathiasBoegebjerg,3,Sun Jun 28 12:59:34 2015 UTC,"No, it wouldn't really. 850 Pro is about $250 for 500GB, $500 for 1TB or that $1k for 2TB."
hardware,3b8d1j,bphase,3,Sun Jun 28 19:48:12 2015 UTC,"Huh, wow. Could have sworn I picked mine up cheaper but that's a fair price progression."
hardware,3b8d1j,angelofrawr,1 point,Sat Jun 27 13:45:45 2015 UTC,Laptop.
hardware,3b8d1j,elevul,4,Sat Jun 27 21:27:31 2015 UTC,"Personally I'm not going to buy a new drive until NVMe becomes more widely adopted (The page on the samsung drive is down, I can't tell if it has it or not). Not necessarily because it'll be faster but just because it's a more optimal way for SSds to operate."
hardware,3b8d1j,ocshoes,1 point,Sun Jun 28 18:20:13 2015 UTC,"The 840 series had issues, but their latest tech (850 series) hasn't had any problems."
hardware,3b8d1j,bizude,2,Sat Jun 27 08:38:53 2015 UTC,really? I heard it had data-loss issues.
hardware,3b8d1j,FitnessRegiment,2,Sat Jun 27 13:57:12 2015 UTC,"The 840 evo and 850 pro had problems even before trim, mostly related to firmware. The 1 TB evo uses a pro controller, so it has problems. Everything else is fine."
hardware,3b8d1j,TheImmortalLS,1 point,Sat Jun 27 14:06:39 2015 UTC,there's other SSDs? I use the 850 EVOs :(
hardware,3b8d1j,FitnessRegiment,1 point,Sat Jun 27 21:24:11 2015 UTC,"The 840 evo has nand problems. 840 pro fine. 850 evo 1 TB, 850 pro use same controller. Earlier, a firmware update bricked 850 pro, then someone discovered trim data loss Linux for the 850's in this sentence."
hardware,3b8d1j,TheImmortalLS,1 point,Sun Jun 28 09:25:57 2015 UTC,"Just did a quick google search, and I can't find anything like that regarding the 850 series."
hardware,3b8d1j,bizude,1 point,Mon Jun 29 04:51:39 2015 UTC,"my bad, read it on /r/hardware or /r/buildapc  no idea where it went."
hardware,3b8d1j,FitnessRegiment,3,Sat Jun 27 14:09:23 2015 UTC,I think your wallet is pissed because you carry so much cash in it.
hardware,3b8d1j,nancy_ballosky,2,Sat Jun 27 14:30:04 2015 UTC,Debatable. Such is the bane of good management of one's money.
hardware,3b8d1j,C4ples,1 point,Sat Jun 27 04:09:19 2015 UTC,So many people miss that point.
hardware,3b8d1j,LuckyTtam,2,Sat Jun 27 04:17:01 2015 UTC,Why not just get several smaller drives that are equivalent to 2tb. Wouldn't that be cheaper?
hardware,3b8d1j,Th3ShitRebellion,2,Sun Jun 28 22:12:11 2015 UTC,"Sure. I'm not worried about performance, but if space is an issue it's more beneficial to spend more on the form factor."
hardware,3b8d1j,C4ples,3,Sat Jun 27 02:19:50 2015 UTC,ssd' are like stacking 5 debit cards on top of each other. it's hardly a space issue
hardware,3b8d1j,imoblivioustothis,6,Sat Jun 27 02:22:49 2015 UTC,"Storage SSD? Is that really wise? They keep coming down in price so fast as well, I'd just get get a decent sized one and upgrade later. Unless you actually need that storage for some heavy processing or such, of course.  As for the GPUs, you should really go for the 980 Tis for 1440p."
hardware,3b8d1j,bphase,7,Sat Jun 27 09:34:05 2015 UTC,"Storage as in games. I keep OS/programs on drives separate from my games. With most of my Steam library installed as I currently do my 1TB of total current SSD space does not cut it. Movies and music are on a built NAS.  It's 6-of-1 for QHD resolutions. The Fury X is better as many times as it's worse than the 980ti, and I would rather not have the retarded Surround display restrictions since I do sometimes use the UHDs in Eyefinity."
hardware,3b8d1j,C4ples,3,Sat Jun 27 00:02:43 2015 UTC,"You could easily just move them to your NAS with SteamMover or something as needed. Or just not keep all games installed, although if you have terrible internet, I guess I can understand having them all installed. I very much doubt you're playing that many games at one time.  Your money of course, it seems you have plenty. Just doesn't seem very worthwhile to me."
hardware,3b8d1j,bphase,5,Sat Jun 27 00:20:00 2015 UTC,"I hop between games frequently. The easiest answer is simply uninstalling those that I don't play all the time, but why do that when I can spend money on my addiction?"
hardware,3b8d1j,C4ples,2,Sat Jun 27 01:46:04 2015 UTC,"Most loading times don't improve that much with ssds. Also, there are far better options for improving loading times:   massive amount of ram and DIMMDrive Steam mover and a regular ssd + hdd archive disk raid 0 several 500gb ssds raid 0 several hdds. hybrid hard drives   All of those options are cheaper than a 2tb ssd. 1,3, and 4 would be faster than a 2tb ssd.  It's sort of like installing an elevator in a 2 story house. If you have the money it's nice.. though an escalator would be better but even then.. is it really that big of a deal to walk up a flight of stairs?"
hardware,3b8d1j,needslipo,3,Sat Jun 27 01:57:37 2015 UTC,"Not always about load times these days, (AAA games and larger resolution displays)  you find that the GPU buffer will fill up and the game will need to pull off the HD - If that's a Platter they you will stutter and freeze.   Other options listed - yes, but simplicity is really key, raid and hybrid, like dual GPU's all can be the cause of issues.   Moving a game over from platter to ssd when you want to play it is tiresome.  Analogy kind of breaks down, when you suffer issues and don't enjoy the game you bought. So there are definitely people who will want a Large SSD >512MB.  Actually, even though I have SSD and HDD, with games on the HDD due to space issues, it's actually faster for me to redownload the game than it is to transfer it over from the 8TB of Green's"
hardware,3b8d1j,transmitthis,1 point,Sat Jun 27 03:09:51 2015 UTC,I don't worry so much about load and access times as I do the form factor. To me the FF is worth the investment made to keep it small. I would rather have two drives with high capacity which do not require extensive mods to support.  I would rather somewhat inconvenience myself with the cost in order to keep my system cleaner and more simple.  It's a matter of priority. To me the size matters a bit more than the storage capacity.
hardware,3b8d1j,C4ples,-1,Sat Jun 27 08:46:00 2015 UTC,"The Fury X is better as many times as it's worse   OK, I'll go there,  It isn't really, at least not to the reviews I have read. The impression I got is that, it runs really hot, which is why they put on that water cooler.  It has only 4GB of that new memory, which will mean your Game will be accessing a, in comparison, a really slow ssd.  (stutters lags and freezes)  It beats the Ti, in a few games, by about 2% It loses to the Ti in many games by around 14%  The pump has a high pitched noise - may be fixed for release  Almost no overclock potential, as it's already pushed to max.  While better than previous AMD, it still uses more power than ti  So really its hotter, noisier, slower, lower RAM, less efficient and the same price. Not saying it's a bad buy just in comparison to the competition its worse."
hardware,3b8d1j,transmitthis,7,Sat Jun 27 04:15:32 2015 UTC,"It isn't really, at least not to the reviews I have read.   Games at QHD where the 980ti is better by more than five frames according to TPU: 7/20   This is ignoring PCars because of the obvious issues the game has.    Alien Isolation 6 fps Batman: Arkham Origins 6 fps BF4 9 fps COD: AW 17 fps GTA V 8 fps Watch_Dogs 8fps Wolfenstein: New Order 23 fps   Every other game is within a 5 fps window(test system discrepancy) where 7/13 times the Fury X is faster, and it is 6 fps better in Dead Rising 3. So while, yes, the 980ti is slightly better(albeit be a handful of frames mostly), pretty much all of these games maxed out are 60fps. I'm cool with that.  As for UHD, we'll lower the discrepancy threshold to 2fps.  Games where the 980ti is better at UHD: 5/20   Batman: Arkham Origins 3 fps BF4 3 fps COD: AW 10 fps Dragon Age: Inquisition 5 fps Wolfenstein: New Order 9 fps   Games where the Fury X is better at UHD: 6/20   BF3 3 fps Bioshock Infinite 3 fps Civ: BE 11 fps Dead Rising 3 5 fps Far Cry 4 4 fps Tomb Raider 5 fps   Every other game is within that 2 fps window where 6/9 times the Fury X is better.  This is performance I'm definitely willing to buy into if it means a better multi-monitor experience for me.   your Game will be accessing a, in comparison, a really slow ssd   Seeing as how we have no idea how a product like this SSD would perform if it even did exist, let's cool it with the guesswork.  In any case, I don't see how the game being on an SSD has much of anything to do with the HBM's performance. It's subject to the same pitfalls GDDR5 does when it accesses data stored on an SSD.   The pump has a high pitched noise - may be fixed for release   It's being put into a custom loop. No big deal.   Almost no overclock potential, as it's already pushed to max.   It also currently has locked voltage control.   While better than previous AMD, it still uses more power than ti   I have free power.  EDIT: An acronym."
hardware,3b8d1j,C4ples,3,Sat Jun 27 08:55:55 2015 UTC,Driver updates will also help. It's refreshing to see someone who actually understands that the Fury X has more potential than current reviews let on 👍
hardware,3b8d1j,2kIrezumi,4,Sat Jun 27 15:24:41 2015 UTC,"It isn't really, at least not to the reviews I have read. The impression I got is that, it runs really hot, which is why they put on that water cooler.   Not entirely sure. It peaks at around 50-60 on the water cooling, while the 980ti averages around 80 I think. I don't know the exact temperature improvement water-cooling provides.   It has only 4GB of that new memory, which will mean your Game will be accessing a, in comparison, a really slow ssd. (stutters lags and freezes)   Generally not a problem. Seeing that the fury was able to play at 4k, I'm guessing that this shouldn't be an issue for a while, especially since no single GPU in the current gen can max games at 60fps and 4k.   It beats the Ti, in a few games, by about 2% It loses to the Ti in many games by around 14%   Yep, Fury is generally slightly under the 980ti by a few percent, or occasionally above by a similar amount. However, there are a few games where it drastically underperforms. That could be attributed to games like Project Cars, which, being NVIDIA Gameworks, are expected to perform poorly.   The pump has a high pitched noise - may be fixed for release   No clue, have heard some comments on this though.   Almost no overclock potential, as it's already pushed to max.   No information on this yet. Voltage controls aren't out yet, so we don't really know how much this can be pushed.   While better than previous AMD, it still uses more power than ti   Depends. Power consumption ratings I've seen for the two fluctuate wildly, as power draw is very heavily dependent on the random variations from card to card. Until we've got significantly larger sample sizes, we can't say very much. I think that, from the few that I've seen though, the Fury draws a bit more than the ti on average, though this is a gut number rather than any properly sampled result."
hardware,3b8d1j,Esyir,1 point,Sat Jun 27 18:53:14 2015 UTC,"It isn't really, at least not to the reviews I have read. The impression I got is that, it runs really hot, which is why they put on that water cooler.   The 980 ti Hybrid - you know, the water cooled version, which costs over $100 more than the stock 980 ti and Fury X, hits the same temperatures as the Fury X.  http://www.gamersnexus.net/hwreviews/1983-evga-gtx-980-ti-hybrid-review-and-benchmarks/Page-2"
hardware,3b8d1j,bizude,1 point,Sat Jun 27 12:41:09 2015 UTC,what monitors do you have?
hardware,3b8d1j,msdrahcir,2,Sat Jun 27 14:03:59 2015 UTC,"QHD IPS as the main and two UHDs as peripherals. One UHD is out for RMA right now, though.  Later this year the main will be replaced with new GPUs and G-Sync/Freesync QHD."
hardware,3b8d1j,C4ples,2,Fri Jun 26 22:48:31 2015 UTC,"QHD IPS as the main   Do you mean WQHD, or are you using your smartphone as your primary monitor?"
hardware,3b8d1j,msdrahcir,4,Fri Jun 26 22:51:31 2015 UTC,QHD and WQHD are the same thing. They are 2560x1440.  You might be thinking about qHD.
hardware,3b8d1j,C4ples,1 point,Fri Jun 26 22:56:22 2015 UTC,Well at over 10K of monitor your wallet must really hate you. Are two fury X's really going to suffice for that much screen? Don't you need more?
hardware,3b8d1j,msdrahcir,5,Fri Jun 26 23:05:30 2015 UTC,"The monitors really were not that expensive. $450 for each of the UHDs and $350 for the Korean QHD. The Korean monitor I got a long time ago and it's more than paid for itself.  The center QHD is used for gaming with the UHD being mostly for data display, so it's really not so bad."
hardware,3b8d1j,C4ples,1 point,Fri Jun 26 23:19:14 2015 UTC,"Word of warning, you can't use Gsync on AMD. Of course Apple, I mean nvidia, made it propriety hardware as opposed to AMD's open sourced Freesync. Worth noting nvidia don't support freesync  yet though (if ever)."
hardware,3b8d1j,FPSNige,1 point,Fri Jun 26 23:26:40 2015 UTC,I'm fully aware. I am satisfied with Freesync if it means I can use asynchronous resolutions for Eyefinity.
hardware,3b8d1j,C4ples,1 point,Sat Jun 27 03:53:55 2015 UTC,"Price wise two 1TB SSD's raided would be a much cheaper option.  Or if it's speed you want, a Sata interface is not where you should be looking.  c (NVMe / PCIe)  As for two x's, well I'm not even going there..."
hardware,3b8d1j,transmitthis,1 point,Sat Jun 27 04:08:53 2015 UTC,"I refuse to use 3.5"" bays in my cases, and if the case has a limited amount of 2.5"" mounts then I would like to stick to just them without having to cram SSDs in every nook and cranny.  Speed compared to other SSDs is not really a consideration. As long as it is faster than mechanical storage I'm good. I'm not looking for the fastest thing on the market."
hardware,3b8d1j,C4ples,-7,Sat Jun 27 08:35:15 2015 UTC,Fury x over the 980ti s? God why?!
hardware,3b8d1j,LiberDeOpp,5,Sat Jun 27 14:18:33 2015 UTC,Arguably QHD and definitely UHD.
hardware,3b8d1j,C4ples,2,Sat Jun 27 02:41:23 2015 UTC,The 980ti has better performance and much better overclocking. Honestly the 390x with 8gb looks like the real winner in Amds new lineup.
hardware,3b8d1j,LiberDeOpp,3,Sat Jun 27 02:45:21 2015 UTC,In the crossfire vs SLI reviews I've seen the CF Fury X beats SLI Titan X's by a small margin.
hardware,3b8d1j,ausgeo123,2,Sat Jun 27 03:41:32 2015 UTC,"In lower resolutions, yes. The 980ti has better performance. Card wattage means nothing to me, as my power is free.  With over clocking, I fully understand the review samples indicate that the Fury X cannot sustain the overclocks a 980ti can. However, that is not terribly important to me.  With DX12 imminent I am banking on the pooled frame buffer solving the VRAM disparity."
hardware,3b8d1j,C4ples,-2,Sat Jun 27 16:25:48 2015 UTC,"Cool story bro, bank on it."
hardware,3b8d1j,Griffith1984,1 point,Sat Jun 27 04:11:24 2015 UTC,not even close in higher resolutions
hardware,3b8d1j,imoblivioustothis,1 point,Sat Jun 27 04:47:09 2015 UTC,"They have a small description on the product page, but it appears that they stack the communications lines so more space is available on the pcb to implant additional memory modules.     Sounds good to me on paper."
hardware,3b8d1j,rancydmilk,1 point,Sat Jun 27 09:34:39 2015 UTC,"Yep. They're geared toward a different market (Intel, SanDisk -> data centers, businesses; Samsung 850 Pro -> prosumers). The Intel and SanDisk ones are for data centers, so they fulfill a completely different set of requirements. For example, mainly read-only servers usually necessitate higher random IOPS (which the Intel has about a 4.5x advertised advantage). They don't need to lower the prices because the companies buying these know what they need and will pay for it."
hardware,3b8d1j,0x60,1 point,Fri Jun 26 23:58:17 2015 UTC,"And that is my point. 2TB SSDs have been out of the reach of the general populous for some time.  Hell. They kind of still are with a $1,000 price point, but they would certainly be MORE affordable.  With this you would no longer have to be a corporate or enterprise entity to even consider it. That is what makes it so different and worth notice."
hardware,3b8d1j,C4ples,1 point,Fri Jun 26 22:17:54 2015 UTC,"Ahh, gotcha."
hardware,3b8d1j,0x60,2,Fri Jun 26 22:21:02 2015 UTC,Sandisk has some 5tb SSds
hardware,3b8d1j,Pokemon5475,0,Fri Jun 26 22:24:50 2015 UTC,Why is so price increase so drastic I pay 400 for 1tb but 4000 for 2tb? Lol..
hardware,3b8d1j,dmg36,8,Sat Jun 27 14:20:54 2015 UTC,It's available in an e-shop in Denmark too: http://www.proshop.dk/Harddisk/Samsung-850-EVO-SSD-2TB-2493137.html for about $900
hardware,3b8d1j,Slazher,2,Sat Jun 27 12:05:33 2015 UTC,"That's the EVO, the OP linked a Pro."
hardware,3b8d1j,SirMaster,10,Fri Jun 26 22:42:07 2015 UTC,"Still 2TB, so seems they're both coming in a bit."
hardware,3b8d1j,bphase,6,Fri Jun 26 23:39:02 2015 UTC,"very very interesting, I can find it on a few shady looking sites but not on any major retailers yet. Samsung did confirm having them, can't find source right now, they said that there wasn't a market for them yet."
hardware,3b8d1j,FitnessRegiment,6,Sat Jun 27 00:04:02 2015 UTC,Especially on SATA
hardware,3b8d1j,hacknut937,1 point,Fri Jun 26 21:09:33 2015 UTC,The evo is available on major french retailers. they say it will be available in july.
hardware,3b8d1j,ant51508,2,Fri Jun 26 22:02:31 2015 UTC,Mirror?
hardware,3b8d1j,Exist50,2,Sat Jun 27 00:50:15 2015 UTC,"SanDisk have 6TB ones in the works on cMLC, and 3.92TB ones in productions for the HP 3PAR storage arrays (though in a consumer application they would be smaller due to greater sparing of blocks. They're not cheap though."
hardware,3b8d1j,drinkwineandscrew,2,Sat Jun 27 02:35:26 2015 UTC,"NVME VNAND WHEN?  They have a controller, they have the flash, they have the infrastructure and we have a standard that's compatible with mobos . Give us a 2.5inch FF drive Samsung."
hardware,3b8d1j,Le_rebbit_account,2,Sat Jun 27 13:23:35 2015 UTC,"Back in May Fixstars unveiled a 6TB 2.5"" SSD. So this isn't exactly groundbreaking."
hardware,3b8d1j,secretsanta13x,7,Sat Jun 27 21:02:11 2015 UTC,"For a hefty price. And you cant exactly call that a proper ssd, considering this is really just 52 eMMC packages raid0'd together with a fpga."
hardware,3b8d1j,hojnikb,-5,Sat Jun 27 01:20:54 2015 UTC,so?
hardware,3b8d1j,imoblivioustothis,12,Sat Jun 27 06:29:09 2015 UTC,Its a sledgehammer approach to high capacity ssd with its own set of potential issues.
hardware,3b8d1j,hojnikb,-4,Sat Jun 27 09:35:35 2015 UTC,comparable to other devices? I don't see the big deal.
hardware,3b8d1j,imoblivioustothis,2,Sat Jun 27 09:38:28 2015 UTC,"If it is in fact 52 eMMC chips RAIDed together, then /r/hojnikb is right. This is an exceptionally shitty way of making a large capacity drive.  You're essentially increasing the chance of drive failure by an order of magnitude. This is also not a sustainable model for improving upon capacity, power efficiency, and reliability.  I wouldn't consider a drive like this unless it is being install in an already redundant array. Even then, it is far from the best option."
hardware,3b8d1j,sk9592,0,Sat Jun 27 10:16:05 2015 UTC,by an order of magnitude.   an order of magnitude is log scale.. sure you mean that?
hardware,3b8d1j,imoblivioustothis,1 point,Sun Jun 28 17:39:29 2015 UTC,"Fine. If you want to nitpick semantics, then ok. You were right and I was wrong. Happy?  It is not an order of magnitude greater. It is only 52 times greater."
hardware,3b8d1j,sk9592,0,Sun Jun 28 18:59:18 2015 UTC,it's not nitpicking if you're the one making shit up and acting like it's a bigger deal than it is.
hardware,3b8d1j,imoblivioustothis,3,Sun Jun 28 19:21:16 2015 UTC,When exactly did I start making shit up? I just used the colloquial meaning of a word rather than it's scientific meaning. You realized that you couldn't refute my broader point so you started to nitpick on inconsequential details.
hardware,3b8d1j,sk9592,0,Sun Jun 28 21:53:27 2015 UTC,Sounds like a BS http://www.samsung.com/global/business/semiconductor/minisite/SSD/global/html/ssd850pro/specifications.html
hardware,3b8d1j,JewInDaHat,2,Sun Jun 28 21:55:34 2015 UTC,could be a leak or just an error. Let's wait and see if any of the review sites pick it up.
hardware,3b8d1j,FitnessRegiment,3,Fri Jun 26 21:55:06 2015 UTC,If you google the product number you find a lot of them.  MZ-75E2T0B/EU
hardware,3b8d1j,Mkvarner,1 point,Fri Jun 26 22:16:29 2015 UTC,Or you know counterfeit.
hardware,3b8bid,JC713,120,Fri Jun 26 19:55:54 2015 UTC,"Not sure if nobody won, but I think AMD needed to win."
hardware,3b8bid,Yearlaren,92,Fri Jun 26 20:36:43 2015 UTC,"People set their expectations too high.  The 980 Ti is only a few weeks old, at an amazing price.  This is AMD we're talking about here -- not the second coming of Jesus.  People expected Fury X to be 10+% faster than the 980 Ti for a comparable or less price, ie: 290 and 290X!  It's just unrealistic.  6 months of hype certainly didn't help the situation."
hardware,3b8bid,TaintedSquirrel,58,Fri Jun 26 20:38:46 2015 UTC,"It partly is AMD's fault. They said they can't be the cheaper solution, so everyone expected the Fury X to be faster than at least the GTX 980Ti, not to trade blows with it. AMD could drop the price of the Fury X to make it more competitive, but that would mean they still are ""the cheaper solution""."
hardware,3b8bid,Yearlaren,21,Fri Jun 26 20:57:40 2015 UTC,The Fury X may not have to trade blows with the 980Ti but if the Fury is the same thing with an air cooler at $550 then it's easily the best buy in its price bracket no questions asked.
hardware,3b8bid,Pinecone,0,Sat Jun 27 07:03:56 2015 UTC,I thought the Fury X had HBM while the Fury doesn't?
hardware,3b8bid,QWieke,21,Sat Jun 27 12:19:01 2015 UTC,Pretty sure the Fury also has HBM.
hardware,3b8bid,Tzahi12345,6,Sat Jun 27 12:47:44 2015 UTC,So it does. Nice.
hardware,3b8bid,QWieke,2,Sat Jun 27 14:11:20 2015 UTC,r9 Nano as well. Every Fiji-derived card will have 4GB of HBM.
hardware,3b8bid,Exist50,-12,Sun Jun 28 04:33:37 2015 UTC,It's already cheaper than the ti if you account for the AIO cooler price.
hardware,3b8bid,JustaPassanger,24,Sat Jun 27 03:45:30 2015 UTC,"Please don't include ""if's"" (conditions) when comparing. Straight up, it's at an equal pricing. Instead, compare just performance and just pricing.  The AIO cooler is always going to be included on the Fury X because of what AMD dictated, just like how the Titan X will always come with the iconic blower shroud."
hardware,3b8bid,TheImmortalLS,6,Sat Jun 27 04:27:06 2015 UTC,The point is there really isn't such a compact card paired with an AIO option with 980Ti. So there is a niche and with such an expensive and low volume GPU that may be enough demand to sell out for a while.
hardware,3b8bid,TBradley,0,Sat Jun 27 06:07:57 2015 UTC,"""There's a niche for it"" and ""it's cheaper if"" are two completely separate lines of argument, though."
hardware,3b8bid,SPOOFE,1 point,Sat Jun 27 18:20:03 2015 UTC,"Ok then. When you go buy a used car, pls don't buy the one that's fully loaded, go for the standard model for the same price.  Not sure if you logic is off or if you're just fanboying but the fact that it comes with a AIO can't be ignored. Nvidia charges you at least 100 more for that."
hardware,3b8bid,JustaPassanger,3,Sun Jun 28 09:58:25 2015 UTC,"It's a video card. The only two things that most buyers really care about are performance and price. The Fury X has less performance for the same price, and it seems unlikely that overclocking will change the situation - CLC or not.   Some people really like CLCs, but if it doesn't enable significant overclocking (we'll have to wait and see on that, but it's not promising for the Fury X) I really don't see the advantage."
hardware,3b8bid,Meanest_Phlebotomist,3,Sun Jun 28 14:21:56 2015 UTC,"You're talking about ""performance"" like the ti is twice as fast or something. Let me remind you that it's 5-10% and in some games the Fury is faster. We still don't know what'll happen once they come up with a few more updated drivers and unlock the card.  Even in it's current state, it might look like people prefer performance over anything but they really don't and I'd muuuuch rather have a Fury-x than a reference design Titan-x or even a aftermarket ti. The biggest reason why most will go with the ti is that Nvidias advertisment works very well."
hardware,3b8bid,JustaPassanger,11,Sun Jun 28 21:32:17 2015 UTC,Then it comes down to whether you'd pick size or temperature over performance. And most would take performance.
hardware,3b8bid,playingwithfire,8,Sat Jun 27 04:27:07 2015 UTC,Except couple months ago when everyone complained about how hot AMD cards ran.
hardware,3b8bid,CEKPETHO,2,Sat Jun 27 12:42:18 2015 UTC,"If it's so hot that it's throttling and running the fan as high, noisy speeds then that's a problem.  If it's running fairly hot, but not throttling, and not running the fans at noisy levels, then it's 100% fine. Temperature itself is never the problem, only the noise/throttling that can come with poorly controlled temperatures."
hardware,3b8bid,Cassiuz,3,Sat Jun 27 15:01:30 2015 UTC,"Yeah, those reference coolers were awful, but PR damage was done and people never let them forget it even when custom cards weren't affected by this issue. Now they're over compensating it to avoid same situation."
hardware,3b8bid,CEKPETHO,1 point,Sat Jun 27 15:14:04 2015 UTC,I remember when AMD cards were the more efficient and cooler cards than Nvidia.  How the times have changed.
hardware,3b8bid,animeman59,1 point,Sun Jun 28 23:25:29 2015 UTC,"Which they don't if you're not using a reference cooler, which you never should anyway."
hardware,3b8bid,letsgoiowa,1 point,Sat Jun 27 15:01:32 2015 UTC,"Most complained about the heat they put out, not necessarily the load temps. It still puts out more, but it's reduced from the Hawaii cards, which is nice."
hardware,3b8bid,Nixflyn,2,Sat Jun 27 23:58:00 2015 UTC,"Depends. Smaller cases are getting more and more popular. When was the last time you saw a full tower case? And if you think about it, even mid tower cases are mostly empty.  Lower temps means lower noise so I'd happily sacrifice 5-10% performance for a silent PC. It's not just me, this is the point that most gamers end up at. In practice, stability and less noise beats all other factors."
hardware,3b8bid,JustaPassanger,2,Sun Jun 28 09:49:35 2015 UTC,which means they're making a tiny margin
hardware,3b8bid,daubertMotion,-6,Sat Jun 27 11:56:39 2015 UTC,"That's one of the dumbest logic statements made in this sub, congrats."
hardware,3b8bid,Darkstryke,-4,Sat Jun 27 05:40:34 2015 UTC,You must be foreign to basic adding and substracting......or just maybe dumb.
hardware,3b8bid,JustaPassanger,2,Sun Jun 28 09:40:30 2015 UTC,Slightly cheaper would have been nice but I guess that's not possible with the cooling solution.
hardware,3b8bid,Schmich,2,Sat Jun 27 00:38:58 2015 UTC,Imagine how this conversation would be going if they'd actually released it 6 months ago like we thought they would.
hardware,3b8bid,ocshoes,1 point,Sat Jun 27 08:20:28 2015 UTC,leaked performance months ago had this hype build. It hasn't lived up to those leaks yet.
hardware,3b8bid,imoblivioustothis,1 point,Sat Jun 27 08:35:56 2015 UTC,"Yep one of the things that has kept them afloat is being the ""cheaper option"" If they want to do away with that then they will have to have something else,like best performance (the title intel currently has in the CPU market which is why they can charge more) otherwise they will fail. I got nothing against them but they have already put themselves in an awkward position that I hope they will get out of. It seems they would do better if they improved their software and stayed as the cheaper but similar performing solution."
hardware,3b8bid,steveng95,5,Mon Jun 29 00:44:19 2015 UTC,"It seems like the Fury X sold out really fast though, i know it did in my country (AUS).  https://www.the-newshub.com/technology/amds-latest-flagship-gpu-sells-out-on-day-1"
hardware,3b8bid,spyder25000,32,Fri Jun 26 22:54:03 2015 UTC,"That's to be expected - supply is low. AMD's problem is that the 390 & 390x NEVER went out of stock. Not once. When the 980 & 970 came out last fall, they were going in and out of stock until after Christmas, and that was with a healthy supply of product. Same thing happened when the Titan X launched and then the 980ti (albeit only for couple weeks or so). The fact that AMD isn't selling out of their volume sellers (the 390/390x) isn't going to help their market share or their bottom line. It's great to sell 1,000 $650 halo cards, but selling  25,000 $419 cards that produce bigger profit margins (no HBM, AIO cooler) is a LOT better. Honestly, a big group of people have already upgraded, grabbing up 980s and 970s like crazy for the nearly year they went on sale with no new competition from AMD or picking up some of the 290x's that were practically being given away in an attempt to maintain market share. The numbers don't lie - while AMD was slow coming to market with anything new (why they didn't print up a bunch of 390x boxes for their Hawaii GPUs 6 months ago is beyond me) Nvidia took more and more market share. AMD doesn't make bad products, they make great products about a year too late."
hardware,3b8bid,thejshep,27,Fri Jun 26 23:25:51 2015 UTC,"AMD doesn't make bad products, they make great products about a year too late.   This sums up AMD's 390/Fury launch down to a T."
hardware,3b8bid,the_solutionza,2,Sat Jun 27 05:51:41 2015 UTC,"I wouldn't even argue a year. If the Fury X came out on the heels of the Titan X launch, it could have been an excellent contender at a $800ish price tag.   Coming out after the $650 GTX 980 Ti and needing to selling at a much lower price than AMD was internally planning to took a lot of steam out of their launch.  Three months makes a world of difference."
hardware,3b8bid,sk9592,6,Wed Jul 1 14:04:18 2015 UTC,"It has been said time and again. AMD just came to the party a little too late. These cards were originally expected around January/February. AMD could have taken advantage of the 970 VRAM issue. They did not.   Many people waited patiently but many gave up waiting because Nvidia just had so many good options available. Even with the VRAM problem present in the 970, it still knocks the ball out of the park.   Then, you have not only the great performance from these cards but the fact that Nvidia has been bundling games with them, left and right. In Fall and Winter was the Far Cry 4 bundle, Spring was the Witcher 3, then both W3 and Batman and now Batman. These were even more of an incentive to pick up a Maxwell card.   Maybe the pace of AMD sales will pick up. I kind of hope so. Yet, a great many people have upgraded already."
hardware,3b8bid,CaptSkunk,9,Sun Jun 28 09:39:22 2015 UTC,That's because AMD couldn't produce as much as they wanted because of low HBM yields. It's because of this that some reviewers didn't get a card.
hardware,3b8bid,Yearlaren,16,Fri Jun 26 22:56:45 2015 UTC,low HBM yields   Source? Something decent and not WCCFTECH level.
hardware,3b8bid,namae_nanka,4,Sat Jun 27 02:21:55 2015 UTC,Not really a good source but Linus said something similar in the latest WAN show.  He said he knows through contacts he doesn't want to disclose that supplies are very short on supply. His guess was that there are less than 1000 Fury X cards in NA.  Source: https://www.youtube.com/watch?v=t7vvjAr8bKE at around 7:45
hardware,3b8bid,Mr_s3rius,10,Sat Jun 27 12:18:15 2015 UTC,Spoiler: there aren't any  SKHynix has not said there were any problems with HBM yields
hardware,3b8bid,TehRoot,1 point,Sat Jun 27 05:10:54 2015 UTC,"Well, there are only 2 listings on ebay for the Fury X - which are predictably overpriced. Considering when the 980, Titan X & 980ti all launched and were in short supply there were dozens of people trying to flip their cards. This tells me that there just aren't very many in the wild. If that's due to HBM or something else I can't say but there certainly aren't many out there right now."
hardware,3b8bid,thejshep,2,Sat Jun 27 19:02:50 2015 UTC,Fury X just started shipping like 3 days ago.
hardware,3b8bid,TehRoot,1 point,Sat Jun 27 19:28:21 2015 UTC,I know - that's when I usually see people trying to flip for a profit. Buying a high end GPU on launch day and shipping it overnight is a good way to make a quick $100 - that's why most place limit you to one or two units for the first few weeks.
hardware,3b8bid,thejshep,2,Sat Jun 27 19:36:42 2015 UTC,"Errr, you can't flip products that haven't shipped yet. Amazon just started shipping today, only a couple have been doled out by newegg.   There aren't many out right now because they haven't really started shipping yet..."
hardware,3b8bid,TehRoot,1 point,Sat Jun 27 19:38:06 2015 UTC,"I'm not sure if it's because of HBM levels specifically, but at launch there were tiny amounts of cards available.  Over here in the UK the largest computer hardware shops had less than 10 available."
hardware,3b8bid,Cassiuz,5,Sat Jun 27 15:02:54 2015 UTC,Kinda makes me wonder how they expect to stock BOTH Fury and Fury X.
hardware,3b8bid,TaintedSquirrel,2,Fri Jun 26 23:02:44 2015 UTC,Hopefully yields will get better when Fury is ready.
hardware,3b8bid,Yearlaren,1 point,Fri Jun 26 23:13:21 2015 UTC,"There's really no solid evidence that yields are bad for either the GPU or memory. And they will inevitably improve with time, which is important to consider as these cards will tide AMD's high end over until maybe mid next year."
hardware,3b8bid,Exist50,2,Sun Jun 28 04:35:55 2015 UTC,I'd love to see the numbers sold. When something is hard to obtain prices go up which is bad considering the product isn't as strong as its alternative.
hardware,3b8bid,spyder25000,2,Fri Jun 26 23:03:43 2015 UTC,Linus said on the wan show about 1000 for north America
hardware,3b8bid,Cozmo85,1 point,Sat Jun 27 01:47:52 2015 UTC,"He also made it clear that this is his best educated guess based on a source he has at one retailer. He didn't claim that as a fact.  Also, his source is most likely NCIX, which honestly is kinda a second/third rate retailer."
hardware,3b8bid,sk9592,1 point,Wed Jul 1 14:10:22 2015 UTC,Well they had 10 review samples for an entire continent and there's at least 2 reports of review units elsewhere being DOA so it's probably safe to say they don't have a whole lot of them to sell.
hardware,3b8bid,nawoanor,0,Sat Jun 27 01:38:31 2015 UTC,I know Linus got a DOA unit
hardware,3b8bid,thejshep,44,Sat Jun 27 19:03:32 2015 UTC,"The thing that stands out to me about Fury is that Fiji is GCN1.2 pushed to it's limits on 28nm, and the main part of the show is HBM support replacing GDDR.   Fury doesn't seem like a push for everything at once, they're putting out a product for real-world usage of HBM, but more conservative on the architecture. Presumably next year for 16nm they'll redesign the architecture as they need to redesign the whole line anyway.  Nvidia did it the other way around and a bit earlier, the 750 was the test release for Maxwell, and the 900 series the main big Maxwell release, and then next year stepping down to 16nm they'll do a smaller iteration on the architecture but add HBM2 support."
hardware,3b8bid,plank_,6,Fri Jun 26 20:11:21 2015 UTC,They could not fit the entire architecture on the interposer with the HBM.  It was too large using 28nm.  So they had to compromise it.
hardware,3b8bid,w00t692,5,Fri Jun 26 20:18:00 2015 UTC,"How do you mean when you say ""the entire architecture""? It's not as though a GPU can run on a partial architecture.  Putting the memory aside, Fiji is GCN 1.2 core, but running faster and over double the amount of them compared to a 380."
hardware,3b8bid,plank_,18,Fri Jun 26 20:51:25 2015 UTC,Rop count is suspected as what was sacrificed
hardware,3b8bid,w00t692,3,Fri Jun 26 20:57:08 2015 UTC,It has the same number of ROPs as the previous cards did but they seem to perform better than the previous cards' ROPs did as well according to TechReport. Were they supposed to be adding more?
hardware,3b8bid,Kaghuros,20,Fri Jun 26 22:20:39 2015 UTC,The 390X and the FuryX both have 64 ROPs and a core clock of 1050mhz. The 390X has 2816 SPs and 176TMUs. The FuryX has 4096 SPs and 256 TMUs. The resulting gain from the 45% increase in SPs and 45% increase in TMUs is 24% at 4K and 15% at 1080p. The only thing that the Fury X doesn't have more of is the ROPs so there is either not enough ROPs or something somewhere else went really wrong because the extra SPs and TMUs in the Fury X don't seem to be helping anything anywhere near as much as they should be. We will know if this is the case for sure once a cut down Fiji based GPU comes out because the cut down chip will have fewer TMUs and SPs potetially clocks but will have the same amount of ROPs. If the amount of SPs and TMUs in cut down Fiji is much less than in full Fiji and performace doesn't drop proportionally to the decrease in SP and TMU count then we can assume that the SPs and TMUs in Fiji are slower than those in the 390X. This is either due to a bottleneck like the un changed ROP count or because the AMD has downgrade the power of the SPs and TMUs in Fiji. The last possibility is that right now Fiji is broken driver side due to the new memory architecture that Fiji has.
hardware,3b8bid,buildzoid,3,Fri Jun 26 23:07:38 2015 UTC,"Actually Fury doesn't double the front end as well from Tonga, so it's on parity with Hawaii there as well. ROPs though are a different story as Tonga did substantially better with delta compression and Fury also has HBM.  I don'tthink there's any chance of SPs and TMUs being 'slower'."
hardware,3b8bid,namae_nanka,3,Sat Jun 27 02:43:59 2015 UTC,Rumor was doubling the rops to 128.
hardware,3b8bid,w00t692,2,Fri Jun 26 22:21:19 2015 UTC,"ROP count is doubled from 380 as well, but not the geometry processing hardware."
hardware,3b8bid,namae_nanka,6,Sat Jun 27 03:04:41 2015 UTC,But it wasn't doubled from hawaii which has 64.  Fury X should've had at least 96-128 rops.
hardware,3b8bid,w00t692,1 point,Sat Jun 27 04:13:29 2015 UTC,"Tonga's 32 were doing better than a 290's 64 in TR's review, and fury is keeping up with a higher clocked 980Ti with 88ROPs at 4k where they matter the most."
hardware,3b8bid,namae_nanka,1 point,Sat Jun 27 07:54:23 2015 UTC,"has 96 ROPs.  There's a LOT of areas where fiji's performance is fucking weird.  In heaven it's worse than my 980 gtx.  In tessmark it BARELY beats the 290x.  So... what's going on.  And yes i know some of these things aren't related to ROPs, but it sure is odd."
hardware,3b8bid,w00t692,2,Sat Jun 27 14:22:23 2015 UTC,"Effective 88 I think.  http://techreport.com/blog/27143/here-another-reason-the-geforce-gtx-970-is-slower-than-the-gtx-980  Fury is basically a double tonga without the doubling of the geometry processing so tessellation performance is not any higher and geometry performance is on par with hawaii. And as pcgh showed, the tessellation performance varies quite a bit by driver.  Seeing some of the hiccups it's getting in some games, I think AMD went overboard on their 4GB HBM optimizations."
hardware,3b8bid,namae_nanka,2,Sat Jun 27 14:43:16 2015 UTC,That was solely tied to the gtx 970.  The 980 ti has 96 ROPs the same as the Titan X.  The ROPs are tied to the L2 cache and memory Controllers.   If this was the case there would be less memory bandwidth and less memory capacity available to be used all at once.
hardware,3b8bid,w00t692,1 point,Sat Jun 27 15:04:09 2015 UTC,"Tonga's 32 were doing better than a 290's 64 in TR's review   That's just untrue. TR didn't test Tonga against a 290, and the 285 was weaker than a 280X (which also has 32 ROP's).  Please stop spreading FUD."
hardware,3b8bid,SPOOFE,0,Sat Jun 27 18:30:03 2015 UTC,"Of course I didn't mean gaming performance you chump, though it was  AT.  http://images.anandtech.com/graphs/graph8460/67234.png  Since shaders don't cut down AMD's ROPs effectiveness, the 290 is basically equal to a 290X for effective ROPs unlike some other gpus,  http://techreport.com/r.x/2014_10_1_Heres_another_reason_the_GeForce_GTX_970_is_slower_than_the_GTX_980/3dm-color.gif"
hardware,3b8bid,namae_nanka,0,Sun Jun 28 02:13:28 2015 UTC,"I love how much you backtracked, yet I'm the chump.  Please stop spreading FUD. You don't know what you're talking about, like, ever."
hardware,3b8bid,SPOOFE,1 point,Sun Jun 28 04:22:56 2015 UTC,But the GCN 1.2 ROPs are much better (roughly twice as effective) as GCN 1.0 and 1.1 ROPs. That puts the Fury X in a much better light.
hardware,3b8bid,Exist50,1 point,Sun Jun 28 04:39:14 2015 UTC,Pixels per clock?
hardware,3b8bid,w00t692,1 point,Sun Jun 28 13:29:02 2015 UTC,"Tessellation performance, at least. Compare the 280 and 285: http://images.anandtech.com/graphs/graph8460/67232.png"
hardware,3b8bid,Exist50,21,Sun Jun 28 15:30:41 2015 UTC,"Nice article  ""There is a rather big gap between the GTX 980 and GTX 980 Ti just waiting to be filled. ""  AKA Fury Nano"
hardware,3b8bid,anon1821,24,Fri Jun 26 20:52:52 2015 UTC,AMDs chief gaming scientist or whatever said the nano is an alternative to the 290/390 (non x) if you want a tiny gpu.  So not really going to fill that gap if true.
hardware,3b8bid,StealthGhost,4,Fri Jun 26 21:28:04 2015 UTC,clears throat nano is not exactly a tiny gpu clears throat
hardware,3b8bid,namae_nanka,11,Sat Jun 27 02:46:55 2015 UTC,"for what it is, the form factor is small."
hardware,3b8bid,winnix,2,Sat Jun 27 11:44:59 2015 UTC,"I'm pretty sure he means that the Fiji GPU is huge, which is very true. The graphics card, on the other hand, is small."
hardware,3b8bid,Exist50,3,Sun Jun 28 04:42:42 2015 UTC,Tiny for a high-end card.
hardware,3b8bid,Meanest_Phlebotomist,1 point,Sat Jun 27 13:00:41 2015 UTC,"I think he means that Fiji (the actual GPU) is large, though the card is small."
hardware,3b8bid,Exist50,2,Sun Jun 28 04:43:10 2015 UTC,"Fine, if we're being technical (since no one else seems to have gotten what you meant), it's a tiny graphics card. The GPU itself, however, is huge."
hardware,3b8bid,Exist50,1 point,Sun Jun 28 04:40:42 2015 UTC,Well it was my best impersonation of Huddy in text.
hardware,3b8bid,namae_nanka,4,Sun Jun 28 07:00:47 2015 UTC,Drop the price of the Fury to $500. Gap filled.
hardware,3b8bid,random_digital,2,Sat Jun 27 00:08:04 2015 UTC,"Eh, i disagree TBH, the gap between the 960 and 970 is bigger relatively speaking (both in price and performance), and that price range will see much more sales."
hardware,3b8bid,everyZig,2,Sat Jun 27 07:14:38 2015 UTC,Overclock the  980.   If you can hit 1550 mhz you've got 95 percent the performance of a reference 980 ti hehe.
hardware,3b8bid,w00t692,13,Fri Jun 26 21:01:29 2015 UTC,"If you can hit 1550 mhz    Living in a world where that's possible, it's the future!"
hardware,3b8bid,Blubbey,5,Fri Jun 26 22:10:55 2015 UTC,"I know right.  just a few years ago, my 580 choked hitting a 50 mhz overclock from 832 mhz.  That was an msi lightning model too."
hardware,3b8bid,w00t692,2,Fri Jun 26 22:14:08 2015 UTC,"I was absolutely blown away by my 960 SSC turboing above 1400 MHz on default settings, all while keeping the stock cooler at ~30% fan speed and temps around 60 C, maxwell is frigging awesome"
hardware,3b8bid,everyZig,6,Sat Jun 27 07:16:34 2015 UTC,if. Most people won't hit that stable.
hardware,3b8bid,HavocInferno,1 point,Sat Jun 27 15:17:12 2015 UTC,"Sure, but you don't need to hit those speeds to be a high value card at 499."
hardware,3b8bid,w00t692,1 point,Sat Jun 27 16:17:08 2015 UTC,"No, Fury (non-x). The r9 Nano (no Fury in the naming) won't have the performance grunt probably, but the Fury should."
hardware,3b8bid,Exist50,16,Sun Jun 28 04:41:53 2015 UTC,"AMD better haul ass on software over the next 2 months, or it's gonna be a sad year for them. Again."
hardware,3b8bid,Trollatopoulous,42,Sat Jun 27 01:09:08 2015 UTC,"Fury CF is already beating Titan X SLI in the one review I saw, so if they're going fully fury on them like 295X2, so they've at least something going for them."
hardware,3b8bid,namae_nanka,36,Sat Jun 27 02:48:30 2015 UTC,"Why the fuck are you getting downvoted?  Nothing here is wrong.  The Digital Storm benchmarks, while very limited, showed the Fury X crossfire clearly beating Titan X SLI.  http://www.digitalstorm.com/unlocked/amd-fury-x-crossfire-gaming-benchmarks-vs-sli-titan-x-idnum361/"
hardware,3b8bid,AMW1011,8,Sat Jun 27 06:32:23 2015 UTC,Why the fuck are you getting downvoted? Nothing here is wrong.    There was one thing wrong with it - a thing that you've fixed. Providing a source.  Thanks for that :)
hardware,3b8bid,Mr_s3rius,-7,Sat Jun 27 12:24:33 2015 UTC,"It's not beating it. It's winning and losing in a bit under 50% of the benchmarks, and by a couple of frames. The 980Ti however is the main competitor, and it's faster than the Titan X. I think it's a safe bet to say the 980Ti would win."
hardware,3b8bid,forestcollector,5,Sat Jun 27 22:36:27 2015 UTC,"The 980Ti however is the main competitor, and it's faster than the Titan X.   Reference to reference, no."
hardware,3b8bid,Exist50,1 point,Sun Jun 28 04:44:58 2015 UTC,Let's not base judgments on a single review & without taking OC into account.
hardware,3b8bid,Trollatopoulous,5,Sat Jun 27 11:56:37 2015 UTC,"Sure for the former, however since its competition is 980Ti, unless Titan X was throttling severely, that wouldn't be as big of a problem as for a single card."
hardware,3b8bid,namae_nanka,-1,Sat Jun 27 13:58:10 2015 UTC,"I don't follow. Both the Titan X & 980 ti both can OC a lot, unlike the Fury X. Sure, with sli/cf there's more concerns about cooling, but all cards have adequate cooling so that even with multiple cards OCing isn't worse than in a single-card scenario. That's without even accounting for custom water cooling."
hardware,3b8bid,Trollatopoulous,-3,Sat Jun 27 16:27:31 2015 UTC,"They're going to have to win at more price points than the niche $1,300 zone."
hardware,3b8bid,TruckChuck,7,Sat Jun 27 08:19:20 2015 UTC,They already do.
hardware,3b8bid,namae_nanka,6,Sat Jun 27 09:38:11 2015 UTC,"Yep. The 290 and 290X are cheaper than their respective competitors and perform just as well or better, while the glorious 280X still has a lot of life left in it at around a $180 price point, where it handily smokes the 960 AND has a larger bus, more VRAM, and more mature drivers to boot."
hardware,3b8bid,letsgoiowa,-7,Sat Jun 27 15:04:59 2015 UTC,If rebrands are your thing.
hardware,3b8bid,TruckChuck,7,Sat Jun 27 09:41:56 2015 UTC,these cards aren't designed for kiddos saving up their mcdonalds paychecks. These are for enthusiasts who upgrade because they feel like it.
hardware,3b8bid,imoblivioustothis,-11,Sat Jun 27 08:39:33 2015 UTC,lololol
hardware,3b8bid,TruckChuck,2,Sat Jun 27 08:40:10 2015 UTC,well? you have 1300$ to drop on a pair of cards right now? I sure don't.  People who buy these RIGHT NOW do.
hardware,3b8bid,imoblivioustothis,3,Sat Jun 27 08:47:08 2015 UTC,there are next to no problems with their drivers on existing hardware.  team green has BSODs and CSD all over the place right now.
hardware,3b8bid,imoblivioustothis,2,Sat Jun 27 08:38:44 2015 UTC,"I'm not talking about problems, I'm talking about performance. They need to up the performance of Fury X through drivers because right now they will keep dropping market share/brand image otherwise."
hardware,3b8bid,Trollatopoulous,1 point,Sat Jun 27 11:57:35 2015 UTC,"They probably have some room there. Maxwell's been out for a good while now, so there's been plenty of time to optimize, but Fiji is very new (esp. HBM), and though GCN 1.2 has been out in the 285/380, I doubt it's been prioritized much."
hardware,3b8bid,Exist50,5,Sun Jun 28 04:46:25 2015 UTC,"I'm sorry but AMD did screw up. There is this thing called competition. They failed to deliver a better product at a lower cost, therefore they lose. I'll reconsider my position once they solve the pump noise issue and unlock the vcore. Might actually be a really good deal if those two things are done."
hardware,3b8bid,eponabestpony,4,Sat Jun 27 19:42:08 2015 UTC,"What if they release the Fury with identical specs to the Fury X, minus the all-in-one liquid cooling solution? That would mean slightly less performance than the 980 Ti while being $100 cheaper. Sometimes, better value means better competition."
hardware,3b8bid,Anaron,12,Sun Jun 28 10:14:26 2015 UTC,"I'm getting a fury ex, because such a minuscule difference versus the support for AMD is worth it"
hardware,3b8bid,TheAngryBlueberry,7,Sat Jun 27 02:26:10 2015 UTC,I own their stocks. Im buying their card lol.
hardware,3b8bid,enzolt,-6,Sat Jun 27 04:08:49 2015 UTC,Uhhh...I'm not saying you should sell but....you should probably sell.  The general consensus is that AMD matched Nvidia at best.
hardware,3b8bid,TruckChuck,10,Sat Jun 27 13:23:59 2015 UTC,Its fine. I've held it since 2012 and forgot about it. I'll hold on until Zen.
hardware,3b8bid,enzolt,2,Sat Jun 27 15:30:33 2015 UTC,"Just that now with Fiji and upcoming Zen, AMD stock is actually going up, at elast temporarily.  After all, up until now, AMD stock has been low, idk, maybe even an all time low."
hardware,3b8bid,HavocInferno,1 point,Sat Jun 27 15:16:36 2015 UTC,"You sound like a business expert, I'll trust what you say."
hardware,3b8bid,JakSh1t,0,Sun Jun 28 20:11:10 2015 UTC,Glad there are people like you to do that. I want AMD to do well I just don't want to buy any of their stuff.
hardware,3b8bid,jewscales,11,Sat Jun 27 02:32:01 2015 UTC,"AMD employees can't take ""people on the internet want us to do well"" to the grocery store and feed themselves."
hardware,3b8bid,0pyrophosphate0,2,Sat Jun 27 03:48:06 2015 UTC,Yep. That's why I'm glad other people are willing to buy their stuff.
hardware,3b8bid,jewscales,4,Sat Jun 27 03:53:39 2015 UTC,I really like their stuff so I buy it as well... Unfortunately I feel your outlook is the popular opinion and that's why they're not doing so well :/
hardware,3b8bid,Stiev0Kniev0,1 point,Sat Jun 27 04:21:21 2015 UTC,Same. I want them to be competitive but when it comes to my rig I buy the best card.
hardware,3b8bid,TruckChuck,20,Sat Jun 27 08:20:14 2015 UTC,"I don't think AMD anticipated the 980 Ti to be as fast as it is, nor as cheap as it is.  They probably expected ~10% slower than the TX for $700+ and instead Nvidia issued a knock-out punch.  Hopefully Fury X's sales will suffer and we can get some price cuts in the next few weeks.  But it seems like AMD isn't producing many of these cards; possibly held back by low HBM supply.  Despite all of the 'negative' reviews, Fury X is still sold out everywhere.  The truly sad part is, for anyone below 4K (1440p/1080p ideally) an overclocked GTX 980 will actually give you slightly more performance than an OC'd Fury X on average.  The card really is just a failure at every resolution except 4K."
hardware,3b8bid,TaintedSquirrel,45,Fri Jun 26 20:36:23 2015 UTC,"980ti having a ""huge advantage"" over fury x at 4K is completely false."
hardware,3b8bid,hilo8914,12,Fri Jun 26 21:56:49 2015 UTC,"After OC, the 980 Ti is around 10-15% faster at 4K on average.  I would call that huge.  Things will change when FX's voltage gets unlocked, but it's just speculation for now."
hardware,3b8bid,TaintedSquirrel,32,Fri Jun 26 23:46:17 2015 UTC,Why are you comparing an overclocked card to a stock card? Make the comparison fairly after the voltage is unlocked.
hardware,3b8bid,hilo8914,19,Fri Jun 26 23:56:31 2015 UTC,Normally I agree but considering the Fury X's overclocking capabilities it is a point to bring forward.
hardware,3b8bid,Schmich,9,Sat Jun 27 00:37:29 2015 UTC,Are you of the belief that they won't unlock it ever?  Because that's one damn pessimistic opinion of AMD if so.
hardware,3b8bid,jinxnotit,5,Sat Jun 27 03:21:18 2015 UTC,It's not on AMD to unlock it. Overclocking tools need to be adjusted for that. I think the guys from MSI who maintain Afterburner said that the update for unlocking Fury X's voltage isn't going to come out in the next weeks.
hardware,3b8bid,Mr_s3rius,1 point,Sat Jun 27 12:21:02 2015 UTC,"It is on AMD with their overdrive/powertune software. They could unlock it tomorrow if they wanted.  But they are opting for the slow, steady, and safe route."
hardware,3b8bid,jinxnotit,8,Sat Jun 27 13:53:32 2015 UTC,"And in the meantime they are loosing the pr battle. Either there is something wrong and they can't, they don't have the budget to do it in a timely fashion, or they are incompetent. They should have had plenty of time to test it themselves.   I honestly can't put it in a good light. They need to hurry the fuck up."
hardware,3b8bid,logged_n_2_say,8,Sat Jun 27 14:16:11 2015 UTC,"Absolutely correct on everything.  There is zero reason to buy a Fury X today unless you just really want to vote with your wallet and tell Nvidia to go screw. And gamers aren't that forward thinking.  There are just a ton of compromises and excuses on the launch of Fury. The hardware it's self is beastly, but then you look at the software side and it's absolutely crippled the hardware. No overclocking, no optimization to speak of.   Until we get to Windows 10. Where there is supposed to be a massive driver update, as well as a big catalyst update waiting to impact across their entire range. But it's that same old AMD song of ""Hurry up and wait"". And that's not what people want, it's what I'm so disappointed in.  I think the 980ti launching so soon and for so cheap caught them with their pants around their ankles. And they're trying to adapt."
hardware,3b8bid,jinxnotit,6,Sat Jun 27 14:25:45 2015 UTC,Almost every reviewer and a larger number of fanboys compare an overclocked and over boosted nvidia card to a reference amd card.
hardware,3b8bid,fzed1199,3,Sat Jun 27 11:48:23 2015 UTC,"There is a significant amount of AMD users on the OCN forum. I would not discount the AMD user base so quickly.  (Mind you, any AMD launch thread is not going to be very civil, so you need some thick skin to actually glean usable information from GPU launch threads in general)"
hardware,3b8bid,terp02andrew,2,Sat Jun 27 14:41:30 2015 UTC,"To be fair, you have to compare both cards overclocked if you're going to overclock them.  The Fury X has the allure of an extremely nice cooler at very competitive speeds and smaller profile than a 980Ti, so those wanting water but not comfortable redoing the sink/wanting to, but still want to overclock it's not a terrible option right now.  We'll have to wait until more people get their hands on them to see how the overclocks handle, but it looks like the HBM is providing pretty solid scaling when you only get core overclocks from what I've seen from the initial benchmarks while overclocked."
hardware,3b8bid,Dippyskoodlez,3,Fri Jun 26 23:53:46 2015 UTC,"980 Ti OC's up to 25%.  FX OC's about 10%.  At stock, the 980 Ti is already a hair faster at 4K (< 5%)."
hardware,3b8bid,TaintedSquirrel,9,Fri Jun 26 23:55:41 2015 UTC,"I've seen it go the other way for Fury at 4k. Tom's, sweclockers and of course ixbt has it at ridiculous."
hardware,3b8bid,namae_nanka,10,Sat Jun 27 02:45:47 2015 UTC,Fury X OC's about 10% at stock voltage.
hardware,3b8bid,TBradley,2,Sat Jun 27 06:38:15 2015 UTC,"I think his point is that even at stock, the Ti already overclocks like a beast. I have mine at stock voltage +200 core and +300 memory. Boost goes to 1402."
hardware,3b8bid,babajabajaba,4,Sat Jun 27 11:41:36 2015 UTC,"OK, but ""20-25%"" will mean voltage adjustment for a lot of people.   I pushed the core clock peed up to +200MHz and promptly had a failure in my Unigine 4.0 testing. That was a bit too low for a failure, so I bumped the voltage up in Precision to +56mV and re-ran the test. Now we were cooking with grease.   http://www.overclockersclub.com/reviews/nvidia_geforce_gtx_980_ti/5.htm  Is your 980 Ti bog standard reference or custom? Because custom usually already have a tweaked default voltage."
hardware,3b8bid,TBradley,1 point,Sat Jun 27 12:44:51 2015 UTC,Oh no doubt.  I have a reference board with the ACX 2.0 cooler. I also increased the power target to 110%.
hardware,3b8bid,babajabajaba,1 point,Sat Jun 27 13:22:27 2015 UTC,"The problem with going the ""a lot of people"" route is that it swings both ways: There are those that won't even hit 10% on a stock voltage Fury."
hardware,3b8bid,SPOOFE,12,Sat Jun 27 20:01:15 2015 UTC,"The Fury X doesn't have unlocked voltage yet.  Making any statements about it's ability to overclock, positive or negative, is completely useless right now."
hardware,3b8bid,AMW1011,3,Sat Jun 27 06:31:11 2015 UTC,"We can, however compare stock voltage overclocking. The 980ti over clocks more than the Fury X without touching the voltage. From memory (I could be wrong, I'm on mobile atm), Logan from tech syndicate's ti hit +150 and +100 on core and memory respectively without touching voltage. The highest core overclock I saw on a Fury X was +85 but I have heard one got close to +100. Going by this sample, the TI overclocks 50% more than the X at stock voltage (not taking into account memory overclocks because I cannot recall the Fury X memory overclocking results)."
hardware,3b8bid,TheRealGecko,-1,Sat Jun 27 11:05:15 2015 UTC,"Why would you give a damn about stock voltage Overclocking?  Honestly, if you're Overclocking you are certainly prepared to tweak voltages. It makes no sense, its an arbitrary measurement."
hardware,3b8bid,AMW1011,2,Sat Jun 27 19:03:46 2015 UTC,"I agree with you, under any normal circumstances we'd be tweaking voltages to achieve higher overclocks than we'd be able to achieve at stock voltages. Herein we can make comparisons between the two cards (unless I'm missing something in which case, please correct me).   Is it reasonable to assume (at least for the moment before we learn anything extra) that the Ti overclocks better than the Fury X given:    Stock voltage overclocks are higher on the Ti?   The Ti is able to overclock even further when fed extra voltage?"
hardware,3b8bid,TheRealGecko,1 point,Sat Jun 27 19:15:48 2015 UTC,"Unfortunately it isn't reasonable to make that assumption or even prediction.    Comparing overclocking on stock voltages is only a measurement of the stock voltage headroom left for each card.  This isn't very useful since we have no idea how much headroom AMD has given the stock voltage, it could be barely stable at best or have plenty and just be poor at overclocking.    We can infer from Hawaii which is a similar architecture, that the voltage headroom is at an absolute minimum to keep power draw low.  Hawaii tends to have horrific overclocking headroom on stock voltages which is why from the beginning aftermarket cards shipped with higher voltages and higher or equal power consumption (lower temperatures allow for less leaky chips meaning the lower temperatures over the reference cards should have allowed power draw to go down).  At the same time with the thermal threshold and the ability to keep the chip from becoming leaky due to low temperatures, you could make the argument that AMD should or would have given plenty of voltage headroom to impress in the early reviews.  Then we get to the part where Fiji isn't just a bigger Hawaii and is a tweaked architecture and therefor we have no idea what its capable of.  As you can see all of this is pretty useless speculation, and it makes much more sense to wait for the voltage to unlock and for the experienced overclockers to figure this chip out."
hardware,3b8bid,AMW1011,2,Sat Jun 27 19:44:40 2015 UTC,980Ti overclocks very well without touching the voltage too.
hardware,3b8bid,hans_ober,0,Sat Jun 27 12:02:37 2015 UTC,Which means nothing.  If you are Overclocking you are prepared to tweak voltages.
hardware,3b8bid,AMW1011,2,Sat Jun 27 19:06:27 2015 UTC,"Sure, but everyone was going on about how the Fury X is limited only because of voltages, I pointed out that the 980Ti over-clocks much better without even touching the voltage."
hardware,3b8bid,hans_ober,1 point,Sun Jun 28 04:44:17 2015 UTC,"Sample size is important. the 980 is mature silicon at this point(failed titan X bins), the fury x has been handled by about 100 reviewers total."
hardware,3b8bid,Dippyskoodlez,-2,Sat Jun 27 00:03:51 2015 UTC,Let me just save this one for later.
hardware,3b8bid,jinxnotit,11,Sat Jun 27 03:23:58 2015 UTC,Or you could look at it as basically Nvidia just undercut a ton of future profits to make sure AMD didnt shit on them.  Basically made 2 of their cards that are like 4 months old completely irrelevant (TX/980).
hardware,3b8bid,stuckinatrance,9,Fri Jun 26 22:40:34 2015 UTC,Solid business and the consumer wins...
hardware,3b8bid,PhilipK_Dick,2,Sat Jun 27 02:51:53 2015 UTC,"I can guarantee Nvidia is still making .money on the TI, AMD's margins on the FX probably aren't anywhere near as good."
hardware,3b8bid,ClockworkOnion,-1,Sat Jun 27 14:03:21 2015 UTC,"They can now still dedicate Titan X and 980 chips for the next Maxwell refresh, and lower the shipped stock. Means at least now they can already start using them for a GTX ""1070"" and ""1080(Ti)"" or however they'll be called."
hardware,3b8bid,HavocInferno,2,Sat Jun 27 15:15:31 2015 UTC,Non-mobile link.
hardware,3b8bid,MMMMorshu,2,Fri Jun 26 21:14:54 2015 UTC,There is a rather big gap between the GTX 980 and GTX 980 Ti just waiting to be filled.   Non-X Fury anyone? We'll see in July.
hardware,3b8bid,LongBowNL,1 point,Sat Jun 27 18:30:44 2015 UTC,The article suggests the emergence of something in the $500 range in July.  Is that in the shape of a new SKU or do you think they were suggesting a price cut?
hardware,3b8bid,quadraphonic,2,Sat Jun 27 13:16:57 2015 UTC,"Fury is going to be $550, and we dont quite now what Fury Nano will be priced at, performance wise it is rumored to be between a 290 and 390, so putting a $500 pricetag on it simply for the power savings seems unlikely."
hardware,3b8bid,everyZig,1 point,Sat Jun 27 14:16:39 2015 UTC,I think we're all hoping that Fiji is lackluster because AMD are focusing on 14nm + HBM2 next year instead of putting a lot of resources into 28nm's third and final year.
hardware,3b8bid,58592825866,2,Sat Jun 27 02:49:48 2015 UTC,"I hope so, if they have been doing with the gpu end what they have been doing on the cpu end(focusing on 14nm and zen) then with any luck amd might be beating nvidia next year, and i hope so as i am a fan of AMD however i am starting to want to upgrade and this generation over all(including NVidia) doesnt seem good enough for me and i dont want to jump shit to NVidia because shitty practices."
hardware,3b8bid,olavk2,1 point,Sat Jun 27 06:45:22 2015 UTC,"Don't upgrade this generation, then. You know you probably don't need the power, and we'll probably get a lot more power next generation."
hardware,3b8bid,dsteele713,2,Sat Jun 27 14:28:03 2015 UTC,"Hopefully the Fury series is a cash in to serve as a sort of pitstop for investing in HBM for them - a way to justify the cost and, at the same time, give them a chance to test any engineering problems with the card, rather than a fully polished product."
hardware,3b8bid,grogleberry,1 point,Sat Jun 27 09:39:22 2015 UTC,"Interesting point, but there doesn't appear to be any good way to compare 1:1 the advantage HBM has over GDD5.  You have people trying to downclock GDDR5 and/or complaining loudly that the voltage is locked on Fury X (for now at least).  As time goes by, we'll hear more stories of people trying to compare/contract the two memory interfaces to try and determine how much benefit HBM has."
hardware,3b8bid,winnix,2,Sat Jun 27 11:50:08 2015 UTC,How is Fiji lackluster? At 4K gaming resolution it's neck and neck with a card 100 dollars more that's air cooled.  A month away from windows 10. Why should they put a lot of effort in to optimizing for 8.1?
hardware,3b8bid,jinxnotit,0,Sat Jun 27 03:38:17 2015 UTC,"So...AMD: ""Keep on waiting""TM"
hardware,3b8bid,TruckChuck,1 point,Sat Jun 27 13:23:31 2015 UTC,"The two big disappointing things about the Fury X are that it's usually worse than the 980 ti, at best merely ties with it in games, and yet costs just as much, and that despite having a built-in liquid cooler it can barely be overclocked. If the card cost $550 it would be a clear winner, but unfortunately it's priced to compete directly with an indisputably better card."
hardware,3b8bid,Ofactorial,-17,Mon Jun 29 05:08:37 2015 UTC,AMD Defense Force in 3... 2... 1...  edit: Hi guys!  Nice to know AMD still has the money for a PR team.
hardware,3b8bid,nawoanor,-1,Sat Jun 27 01:37:06 2015 UTC,"It's not surprising. Nvidia could almost certainly blow away any of their own current product offerings if they wanted to. However, they will only release just enough at any given time to keep AMD just barely alive. If Nvidia were to deliver the full potential they are capable of then AMD would sink and Nvidia might be split up by the government. A similar phenomenon is happening in the x86 CPU industry with Intel."
hardware,3b8bid,ScottLux,0,Sat Jun 27 22:44:21 2015 UTC,Would fury X be cheaper if it just used 6gb of regular memory instead of HBM? Is HBM even what makes it as powerful as it is?
hardware,3b8bid,abacabbmk,1 point,Sat Jun 27 14:44:27 2015 UTC,"HBM allows the card to be what it is. The main benefit was probably power consumption, but the form factor too was only possible via HBM. In any case, AMD would not have bothered to make this card with GDDR5. Fiji is a proving ground for HBM and likely the start of a new product line.  Just think, a 14nm Fiji GPU with 4GB of HBM is looking quite good for a mid-range HBM-centric lineup in 2016."
hardware,3b8bid,Exist50,-7,Sun Jun 28 04:49:47 2015 UTC,"i'm disappointed that because of the drivers it does worse on 1440/1080, but i'll still buy it once the prices cool down 200+ euros more than in U.S. f that"
hardware,3b8bid,Perunsan,12,Fri Jun 26 22:04:30 2015 UTC,It's as reliable as the pendulum on a clock:   Nvidiots are always talking shit about AMD drivers being terrible but they're FINE!   ...   As soon as the drivers get better this new AMD card is going to kick Nvidia's ass!
hardware,3b8bid,nawoanor,14,Sat Jun 27 01:43:23 2015 UTC,"hehe, but drivers being terrible is a quite different thing from drivers not having double-digit performance improvements.  Right now,   game crashed on AMD cards   Must be AMD!! God damn you AMD!!   game crashed on nvidia cards   Must be something else! Game dev., my overclock, other software, PEBKAC!"
hardware,3b8bid,namae_nanka,3,Sat Jun 27 02:50:52 2015 UTC,"well they said it themselves that they worked on drivers for 4k and look at the results both 390x and fury stand on par with nvidia at that resolution. The cards have the power to do better but the drivers are like a anchor holding them back : just look at 390x the ""refresh"" of 290x it's a 2 year old card and competes with 980 isn't that crazy? not only that but nothing changed aside from a slight overclock and better aftermarket cooling."
hardware,3b8bid,Perunsan,1 point,Sat Jun 27 09:09:46 2015 UTC,"Two separate things. People criticizing AMD's drivers usually talk about bugs and crashes, but people saying they should do better are talking about Hawaii-like performance improvements over time. Look at Kepler for an example of an architecture that is not being optimized for with drivers and compare launch performance between the 290x and 780ti with the present day. What used to be a clear lead for the 780ti is now essentially non-existent or reversed entirely."
hardware,3b8bid,Exist50,1 point,Sun Jun 28 04:53:20 2015 UTC,Either AMD's drivers are horribly unoptimized at launch or they're really good at improving performance over time. The R9 280X and GTX 770 is a good example.
hardware,3b8bid,Anaron,2,Sun Jun 28 10:15:57 2015 UTC,"I'd say the latter. The 280x in the form of the 7970 (GHz) was around for a while before it was rebranded, but they still managed to improve performance over time, at least compared to Kepler cards. Some of that probably lies on Nvidia for their lack of effort, but AMD should get credit for its work in that area."
hardware,3b8bid,Exist50,0,Sun Jun 28 15:34:48 2015 UTC,"AMD comes out with a new GPU and it's slower than the competition but gradually through game-specific fixes and price drops it becomes more reasonable after being reused for three product generations; Nvidia has a card that's consistently fast.  In an AMD fan's eyes this shows Nvidia's ""lack of effort"".  Wow."
hardware,3b8bid,nawoanor,1 point,Sun Jun 28 18:42:13 2015 UTC,"You conveniently forget prices. AMD releases a card that beats anything Nvidia has for cheaper, then is quickly ""beaten"" by Nvidia at a significantly higher price point, only to fall behind in the long run. This being the 290x vs 780 and 780ti.  The fact that a $400 (at release) r9 290 is now often beating a $700 (at release) 780ti is laughable. How can you possibly say Nvidia is better because of that?   Nvidia has a card that's consistently fast   And that's the problem. It's not consistent at all."
hardware,3b8bid,Exist50,1 point,Sun Jun 28 18:52:05 2015 UTC,"Also, only Pitcairn has been in 3 product generations. And the driver improvements have been for all GCN-based cards. Find a new argument. Kepler has in 2 generations throughout the product stack, so surely it should have just as much an advantage, if not more."
hardware,3b8bid,Exist50,0,Sun Jun 28 18:54:00 2015 UTC,They only rebranded a GPU three consecutive generations!  Find another argument!   lol
hardware,3b8bid,nawoanor,1 point,Sun Jun 28 20:30:20 2015 UTC,"Then by that logic, how long have the low end Nvidia (gt series) cards been in the stack? Also, it says a lot about Nvidia that such an old card is still competitive. Further evidence that AMD cards have better staying power.  You seem to think 2 generations is perfectly fine, but 3!?! Well that's an atrocity. /s"
hardware,3b8bid,Exist50,1 point,Sun Jun 28 20:33:52 2015 UTC,"280X is allegedly a high-end card targeting gamers.  Nvidia's low-end cards are for HTPCs and entry-level laptops at best, where system requirements haven't changed in easily 5 years or more and AMD has nothing to compete with.   Also, it says a lot about Nvidia that such an old card is still competitive.   Not really.  Compare the die size of a 280X to the die size of an equivalent-performing Nvidia GPU.  It's been so long since AMD last produced a competitive GPU that Nvidia has had the time to refine their design and leave themselves tons of profit margin.  If AMD dropped all their prices by $100 they'd be taking a loss.  If Nvidia dropped all their prices to match they'd still be comfortably profitable.  This is why there's been no big price:performance gains like in the past.  You seem to be getting the wrong impression of me.  I'm not happy with Nvidia, I'm upset with AMD.  I want things to get faster and cheaper but they can't put out a competitive product and it's pissing me off.  My 970 is the first Nvidia GPU I've ever purchased.  I've been with AMD/ATI since my 9800 Pro over a decade ago.  They used to be the better option but they've been in a tailspin for so long I was forced to settle for Nvidia."
hardware,3b8bid,nawoanor,1 point,Mon Jun 29 22:21:41 2015 UTC,We don't know it's the drivers. Could be the low ROPs or something.
hardware,3b8bid,bphase,-6,Sat Jun 27 03:44:03 2015 UTC,It's not like they came out at the same time....
hardware,3b8bid,Kubi74,-4,Fri Jun 26 20:38:50 2015 UTC,"There is a rather big gap between the GTX 980 and GTX 980 Ti just waiting to be filled.   Not really....  the 980 Ti is only 50% faster than even the 970, but twice the price. The 980 sits in between (closer to the 970)."
hardware,3b8bid,mmencius,1 point,Sat Jun 27 04:10:52 2015 UTC,But closer to the 980ti in price. It is not a tenable position for the higher end card to offer better value.
hardware,3b96vg,TheRealGecko,31,Sat Jun 27 00:17:07 2015 UTC,"I feel like AMD seriously missed something here, whether it be drivers or software capable of unlocking voltages and memory for overclocking. I know its new architecture, and could very possibly improve dramatically with future updates as HBM is optimized. In it's current state, it's a dismal overclocker that performs around 5% less than the 980Ti...for the same price. I find it odd they release the water cooled, and more expensive version first. Had this been the $550 Fury that was air cooled, I doubt there would be nearly as much animosity towards the Fury's release and ensuing performance results.   A $550 Fury that was nearly the 980Ti's performance levels would have been a much better outcome. It would be more in line with AMD's approach to ""bang for your buck"" quality.  Sort of just scratching my head on this one"
hardware,3b96vg,rationis,13,Sat Jun 27 00:56:46 2015 UTC,We simply don't know that the Fury will be at nearly 980Ti performance levels. Most reports seem to indicate it will perform somewhat lower than the Fury X. So what if it matches the GTX 980? Suddenly that's not the value for money that AMD was known for. They also publicly stated the want to get away from that image in the pre-Fury X launch hype.
hardware,3b96vg,BlayneTX,35,Sat Jun 27 12:17:39 2015 UTC,"copy-pasting my comment from another thread.   It's not locked from AMD, it's locked because there is no update to afterburner/etc. to work with the board voltage controller.   It's not AMDs job to provide that support. The Titan X had the same issue before it was unlocked. The 980ti didn't because it uses the same voltage controller as the Titan X.   Now, if the BIOS is limited, that's AMDs fault, but until the controller is supported it's a 3rd party issue. It is the same 290x controller, but the implementation was most likely changed since it's a new board meaning update.  http://forums.guru3d.com/showpost.php?p=5106990&postcount=44"
hardware,3b96vg,TehRoot,9,Sat Jun 27 05:07:53 2015 UTC,"Perhaps they should have given a couple cards pre-release to the 3rd party companies so they could have started to create software to overclock HBM and voltages. Its like watching a Corvette running on 6 cylinders, I'm pretty damn certain it could be much more powerful than it is stock, as the stock settings on the card seem quite conservative."
hardware,3b96vg,rationis,9,Sat Jun 27 10:20:06 2015 UTC,"MSI and Sapphire/etc are responsible for that. Alexy didn't receive his board from MSI yet since he's the developer now for afterburner. AMD doesn't handle that, and neither does nVidia."
hardware,3b96vg,TehRoot,6,Sat Jun 27 13:30:40 2015 UTC,"It may not be AMD's responsibility, but it seems like if they wanted this card to look as good as possible at the time reviews went live, they could have made sure that there was some way to unlock voltage (e.g. via AMD Overdrive) rather than waiting on MSI and the developer of Afterburner to take their time.  Maybe people who overclock aren't a very big market segment, but it would have avoided all these discussions about how the Fury X ""only overclocks 10%"" versus much more for the 980 Ti."
hardware,3b96vg,aziridine86,-8,Sun Jun 28 01:50:36 2015 UTC,"Wow, so AMD needs to make sure overdrive includes voltage overclocking, but it's perfectly okay for Titan X to drop without overvolting and we wait for about a month for it to drop?   The 980ti uses the same voltage controller as the Titan otherwise what is happening with Fury would be happening to it.   AMD overdrive has never included overvolting support. nVidia doesn't even provide any inhouse overclocking tools."
hardware,3b96vg,TehRoot,8,Sun Jun 28 07:15:16 2015 UTC,"I never said they had to do anything.   I'm not saying anything is or isn't okay for AMD or Nvidia, but they could have taken extra steps to get ahead of this one way or the other.   The fact is Nvidia doesn't catch flak for things like this and AMD does. That may be unfair, but if I were AMD I would be taking steps to mitigate that regardless.  People didn't make a fuss about Titan not having overvolting support at launch. In part that is because it overclocks decently on stock voltage, and in part that is just because it is an Nvidia product so its get the benefit of the doubt. Unfortunately the Fury X doesn't get either of those things so it would have been smart to provide reviewers with a method to demonstrate the full potential of the card at the time reviews went up."
hardware,3b96vg,aziridine86,2,Sun Jun 28 08:49:56 2015 UTC,"The fact is Nvidia doesn't catch flak for things like this and AMD does. That may be unfair, but if I were AMD I would be taking steps to mitigate that regardless.   Exactly! I'm sick of AMD fanboys complaining about how each and every thing isn't exactly fair. This is business in the real world. You do the best you can with what you have. You don't make excuses for what you could have done better in retrospect.   They SAW the Titan X's overclocking being crippled by voltage limitations on launch. They had MONTHS to make sure the same thing didn't happen to them. They didn't do anything about it, and now all the reviews say that the Fury X doesn't overclock well.  You deal with it and move on to the next challenge. That's just how life works."
hardware,3b96vg,sk9592,1 point,Sun Jun 28 23:24:35 2015 UTC,"AMD doesn't support over-volting, it's always been a 3rd party feature.   I don't know why everyone seems to be freaking out over this when it's exactly the same whenever a new card comes out that doesn't share a VC."
hardware,3b96vg,TehRoot,6,Sun Jun 28 08:51:06 2015 UTC,"And as for why people are freaking out, I'd say part of it is due to the ""overclocker's dream"" comment but most of it is simply down to (unreasonably) high expectations for the performance of this card.  People didn't get the stock performance they were expecting so now they expect to get that performance via overclocking.   I imagine the reception for this card would be very different had the Fury X been named the R9 390x or maybe ""R9 395x"" and if all this hype hadn't been built up ahead of time."
hardware,3b96vg,aziridine86,0,Sun Jun 28 08:58:30 2015 UTC,I thought this used the same voltage controller as Hawaii anyway?  According to TechPowerUp:   We've seen the IR 3567 voltage controller on the R9 390x/290X before. It supports software voltage control and monitoring via I2C and is well supported in overclocking software.   Is that an error on their part?
hardware,3b96vg,aziridine86,1 point,Sun Jun 28 08:56:17 2015 UTC,"They share a VC, but how it's used is different, which is why you can't over-volt it out of the box. People have started playing with AB config settings and they've gotten certain things to happen, but powertune acts wonky.   Alexy has said they'll be adding support when the card gets to him from MSI."
hardware,3b96vg,TehRoot,2,Sun Jun 28 08:58:01 2015 UTC,"Yeah, pretty much. The Titan X gets away with it because it was always a stupid prestige purchase released at a time when nothing could compete with it. The Fury X doesn't get that luxury because it's competing with the 980 Ti, a card which soundly beats it right now because the Ti can actually be overclocked."
hardware,3b96vg,PolyWit,1 point,Sun Jun 28 14:38:14 2015 UTC,"This makes a lot of sense. Otherwise, this makes the liquid cooler entirely pointless. Any decently designed air cooler could displace the heat coming out of a stock clocked Fury X."
hardware,3b96vg,sk9592,1 point,Sun Jun 28 23:19:56 2015 UTC,"Partly, because the into price is always highest.  Since the 980ti dropped sooner than anyone anticipated, do you think the water cooled edition could have been intro'ed (later)  at $550 instead?  They needed the highest possible margin on the most expensive form factor."
hardware,3b96vg,winnix,1 point,Sat Jun 27 11:53:43 2015 UTC,"They should have released the air cooled version first. I think they approached the release backwords - releasing the more extreme and overclockable version first, then the more budget minded version later. Had they dropped the $550 one first, I don't think anyone would be complaining that it was slightly slower than the 980Ti seeings as how the GTX980 was just recently $550 and no where as close to the 980Ti's performance as the Fury is."
hardware,3b96vg,rationis,1 point,Sat Jun 27 12:30:23 2015 UTC,"whether it be drivers or software capable of unlocking voltages and memory for overclocking.   I'm more inclined to think it's voltages. Do you think memory overclocking would be that helpful? The aggregate memory bandwidth of the Fury X is a lot higher than any other GPU on the market.    I find it odd they release the water cooled, and more expensive version first.   I'm pretty sure AMD intended to release the Fury and Fury X a bit later for a much higher price. Nvidia releasing the GTX 980 TI for $650 was too compelling a product for the price and tripped AMD's hand.  AMD couldn't sit around and twiddle their thumbs while everyone waiting for a high end next gen GPU got fed up and bought a GTX 980 TI.  The Fury X is what they had available for release. They had been working internally on this reference design for a while now. Their board partners such as Sapphire and XFX probably had much less time to work on their designs for the air cooled Fury. That's likely the reason that launch is coming later.  TL;DR: AMD thought they had more time and could charge more for the Fury and Fury X until the GTX 980 TI came out."
hardware,3b96vg,sk9592,-3,Sun Jun 28 23:14:44 2015 UTC,I've been struggling with this too. Something is just not right. I feel like their up to something.
hardware,3b96vg,klymen,20,Sat Jun 27 04:12:53 2015 UTC,Loud persistent pump noise and the worst coil whine he's ever heard. Well this certainly helps to cement my decision for the Ti.
hardware,3b96vg,BlayneTX,3,Sat Jun 27 12:11:51 2015 UTC,This stuff annoys me more than a few fps. I'd happily take a slightly slower card for one that doesn't make noise like this when you're not gaming. For many anyway gaming takes up a relatively small amount of my system usage and annoying noises utterly ruin hardware.  When the Ti is slower and quiter that is a deal breaker.
hardware,3b96vg,Theodoros9,-1,Sun Jun 28 09:31:15 2015 UTC,"Could you hear it? I'm not sure if it's because I watched this on a laptop with crappy speakers, but I couldn't hear anything personally."
hardware,3b96vg,iRawrz,8,Sat Jun 27 16:47:20 2015 UTC,Look at HardwareCanucks video of it. Man.
hardware,3b96vg,Oafah,3,Sat Jun 27 17:00:21 2015 UTC,"I didn't hear the pump too much, but the coil whine was very distinct and loud on my headphones.  I use one of those Corsair CPU watercoolers and if it's anything like that, it's definitely not quiet."
hardware,3b96vg,aelder,3,Sat Jun 27 17:09:03 2015 UTC,Oh I can definitely hear it now that I'm at my desktop. That's pretty bad.
hardware,3b96vg,iRawrz,2,Sat Jun 27 17:18:09 2015 UTC,"Yeah, my 970 has bad coil whine in loading screens where the frame rate goes up to like 900, in normal games or in Windows you can't hear it at all."
hardware,3b96vg,Penderyn,2,Sat Jun 27 22:07:35 2015 UTC,"I heard it now, too after cranking my speakers up to max. To be fair, i don't know much about coil whin or how it's supposed to sound, but i could hear an annoying sound. However, the annoying sound of the 970 was a lot more clear to me for some reason. Edit: Just watched the Hardwarecanucks video. I could hear it much better there and it's really annoying."
hardware,3b96vg,Pandora734,2,Mon Jun 29 12:29:41 2015 UTC,Try this one: https://www.youtube.com/watch?v=XfyQzroYnrI  It's pretty loud.
hardware,3b96vg,BlayneTX,3,Sat Jun 27 18:00:56 2015 UTC,"holy fuck, that gives me dentist flashbacks"
hardware,3b96vg,Raising,2,Sat Jun 27 22:15:55 2015 UTC,"Yeah that sucks. I wasn't going to get it personally, but I can't even recommend it to anyone now because of that."
hardware,3b96vg,iRawrz,1 point,Sat Jun 27 22:39:35 2015 UTC,"AMD R9 Fury X Coil Whine/Buzzing Noise [0:34]   Video showing how the rig sounds WITHOUT the Fury X [noise comparison]:   James Sunthonlap in People & Blogs  2,482 views since Jun 2015   bot info"
hardware,3b96vg,youtubefactsbot,1 point,Sat Jun 27 23:43:16 2015 UTC,"That AIO probably has air in the impeller housing, but that coil whine is truly annoying."
hardware,3b96vg,Darkstryke,4,Sat Jun 27 22:16:17 2015 UTC,"there wasn't a bandwidth problem with GDDR5 to begin with   Ehhh, bandwidth is always a problem."
hardware,3b96vg,Le_rebbit_account,3,Sun Jun 28 09:02:06 2015 UTC,Total bandwidth on the Fury is only 30% higher than the GDDR5 in the 390/X.
hardware,3b96vg,makar1,-15,Sat Jun 27 17:04:02 2015 UTC,Cannot stand this dude... is he that popular?
hardware,3b96vg,ndr2h,17,Sat Jun 27 17:16:19 2015 UTC,Yes. He is that popular. He's to-the-point and very entertaining.   Edit: Transitionnnnnnnn!
hardware,3b96vg,fr33andcl34r,8,Sat Jun 27 17:24:30 2015 UTC,"When I first saw him, I found him annoying. But I've begun watching him a lot and he's just a very straight forward, matter-of-fact kind of person. Doesn't seem to really dwell too much on unimportant topics.   I've grown to enjoy his presentation style."
hardware,3b96vg,SirCrest_YT,1 point,Sat Jun 27 19:23:17 2015 UTC,Same here
hardware,3b96vg,Clob,0,Sat Jun 27 14:17:22 2015 UTC,Him and Linus are my favourite reviewers.
hardware,3b96vg,canadianvaporizer,-5,Sat Jun 27 15:41:09 2015 UTC,You're not alone brother.
hardware,3b96vg,xandergod,-1,Sat Jun 27 19:54:04 2015 UTC,Im not sure why people complain about coil whine if they really care about audio they will get an external DAC with either a ground loop isolator for 40 bucks or an expensive enough DAC to remove the whine.
hardware,3b8guy,bad-r0bot,21,Fri Jun 26 20:36:26 2015 UTC,too bad that card slots disappear more and more.  16gb more space? pay 100 bucks!
hardware,3b8guy,Klorel,20,Sat Jun 27 00:19:10 2015 UTC,"People want better phones, what could we do to improve them?  Bring back microSD slots?  You're fired!"
hardware,3b8guy,Wonka_Vision,11,Sat Jun 27 07:49:27 2015 UTC,This is actually crucial for me in my decision to buy a phone.
hardware,3b8guy,im-a-koala,1 point,Sat Jun 27 17:46:19 2015 UTC,"Me too.  I need a 64 GB card currently to store all my music, but will likely upgrade to at least a 128 GB card at some point in the ""near"" future (this year).  Luckily my Galaxy S4, which has a slot, seems to be going strong; I hope I can get another 2 years at least out of it."
hardware,3b8guy,i010011010,-1,Mon Jun 29 04:04:57 2015 UTC,I don't mind as long as I have enough space for everything I do. I bought the OnePlus One 64GB model and am very happy with it even though it lacks expandable storage. Current usage  edit: vid is a folder with shows to watch while travelling.
hardware,3b8guy,GeneticsGuy,2,Sat Jun 27 18:10:22 2015 UTC,But a micro SD slot would add back 12µm of width.  We know what consumers want and it is a phone as thin as a cracker.
hardware,3b8guy,BJabs,4,Tue Jun 30 08:34:32 2015 UTC,"This is why I love my Dell Venue 11. When I bought mine, I swapped out the 64gb M.2 SSD it came with with a 500GB one I got from some Crucial SSD clearance sale for 125 bucks, sticking the 64 into my desktop motherboard to boot from. And I nabbed a great deal on a 128Gb micro SD card the the venue 11 also has a slot for. With the optional keyboard that is built with an additional battery, and with all my storage space, I haven't even touched my laptop in over 6 months. It's seriously awesome.  I can't believe how much money people are still shelling out to Apple just to upgrade from 16Gb yo 64, let alone 128 lol."
hardware,3b8guy,def-,6,Sun Jun 28 03:17:22 2015 UTC,"Did you mean to link this article from March 1st? This articles announces its existence, but not its availability.  This announces its availability:  http://www.droid-life.com/2015/06/25/sandisks-200gb-microsd-card-now-available/"
hardware,3b8guy,TheImmortalLS,2,Sat Jun 27 06:08:51 2015 UTC,"Ah, yes. I read my article in Dutch so I had to find an english one. I didn't read it thought completely."
hardware,3b8guy,Sisaroth,6,Sat Jun 27 07:51:05 2015 UTC,186 Gibibytes or 200GB (corrected)  Sandisk press release
hardware,3b8guy,39th_Step,10,Fri Jun 26 20:38:54 2015 UTC,"It's the other way around. 186 GiB, 200 GB."
hardware,3b8guy,TheImmortalLS,3,Sat Jun 27 01:11:09 2015 UTC,"How the hell did I manage to fuck that up?! Yeah, it should be the other way around. Thanks!  edit: wait, no. GB is in 1024 and GiB is in 1000. In their press release they state: 1GB=1,000,000,000 bytes. Actual user storage less. So you'd need less GB to make 1 GiB."
hardware,3b8guy,DarthFrog,5,Sat Jun 27 07:45:16 2015 UTC,"1024 bytes = 1 KiB, 1024 KiB = 1 MiB   1000 bytes = 1 KB, 1000 KB = 1 MB  A binary unit is larger."
hardware,3b8guy,ooll2342,2,Sat Jun 27 21:26:57 2015 UTC,"Aha, I am wrong then. I'll correct it."
hardware,3b8guy,DarthFrog,2,Sat Jun 27 21:38:44 2015 UTC,Maybe you are confused because in windows its 1024 bytes = 1 KB. (Microsoft doesn't follow the standard.)
hardware,3b8guy,master_guru88427,1 point,Mon Jun 29 06:36:36 2015 UTC,"Yeah, that's what I'm saying. I put used google to convert GB to proper GiB like Windows uses. I mixed up the two numbers when I wrote it down."
hardware,3b8guy,DarthFrog,-6,Mon Jun 29 09:07:02 2015 UTC,then why didn't you change the title accordingly?
hardware,3b8guy,master_guru88427,10,Fri Jun 26 22:29:23 2015 UTC,It was too late.
hardware,3b8guy,DarthFrog,2,Fri Jun 26 22:31:09 2015 UTC,Can't change titles.
hardware,3b8guy,ooll2342,2,Sat Jun 27 21:28:37 2015 UTC,"W00t!  Now all I need is a Rockbox-capable music player that will use this card.  If not Rockbox, at least one that has Ogg Vorbis support."
hardware,3b8guy,DarthFrog,1 point,Fri Jun 26 21:46:27 2015 UTC,Sansa clip zip. I have a 4gb with a 64gb micro SD card and rockbox on it.
hardware,3b8guy,elevul,1 point,Sat Jun 27 14:06:34 2015 UTC,64 GB?   How did you manage that?  SanDisk's specification for both the Clip Zip and the Fuze+ says that it supports SDHC upto 32 GB. This 200 GB card is SDXC.  Are there any Ogg Vorbis players that will use SDXC?  I wonder if the FiiO X3 II will be able to use this SDXC card.
hardware,3b8guy,elevul,2,Sat Jun 27 23:08:22 2015 UTC,TL; DR 32gb is never the max  I use a HTC HD7 (wp7) that was 16gb as my music player.  I looked online and all the forum posts said that 32gb was the max.  I threw a 64gb card in and it worked.  Once 128gb cards drop in price I'm upgrading again.  Might just jump to 200 and be done.
hardware,3b8guy,Sixteenbit,1 point,Sun Jun 28 14:50:55 2015 UTC,"Going from 32 GB to 64 GB, did you still use a SDHC card?  According to SanDisk, SDXC is not backwards compatible with SDHC."
hardware,3b8aln,decoy11,31,Fri Jun 26 19:48:44 2015 UTC,I don't get why they used Project Cars as a showcase for FuryX performance out of all their other benchmarks. That game is bogged down in software for AMD and isn't a good indication of hardware performance in the majority of games.
hardware,3b8aln,Scrabo,35,Fri Jun 26 21:42:24 2015 UTC,"Our overall performance numbers come from the geometric mean of the scores across five of the six games we tested. (We chose to exclude DiRT Showdown, since the results skewed the average pretty badly and since AMD worked very closely with the developers on the lighting path tested.)    http://techreport.com/review/23527/review-nvidia-geforce-gtx-660-graphics-card/11  much credible, very non-biased, wow"
hardware,3b8aln,namae_nanka,29,Sat Jun 27 02:30:39 2015 UTC,So they ignored the AMD-biased game and included the Nvidia biased game. Hahahaha.
hardware,3b8aln,BraveDude8_1,5,Sat Jun 27 10:48:04 2015 UTC,"http://techreport.com/review/23527/review-nvidia-geforce-gtx-660-graphics-card/11  review-nvidia-geforce-gtx-660-graphics-card  660  September 13, 2012"
hardware,3b8aln,Le_rebbit_account,1 point,Sat Jun 27 17:27:17 2015 UTC,"I know. Same website, thought it was funny."
hardware,3b8aln,BraveDude8_1,0,Sat Jun 27 17:28:56 2015 UTC,"It depends, sometimes you just need to cull extememe outliers if it's unrepresentative of the overall situation you're trying to quantity.  If they had a better statistical model with a bit more data they probably wouldn't have needed it for a good fit but this works too."
hardware,3b8aln,andromeduck,4,Sat Jun 27 18:09:51 2015 UTC,Always good to have more Kanter. Too bad he doesn't post much on Real World Tech anymore.
hardware,3b8aln,OftenSarcastic,7,Fri Jun 26 22:07:48 2015 UTC,"AMD used to dominate at MSAA, why the hell would they choose their entry into HBM to back away from that?  Not that post processing can't be good too, but pretending that ROP's wont be as necessary seems foolish."
hardware,3b8aln,lild3an,6,Sat Jun 27 08:03:25 2015 UTC,"but pretending that ROP's wont be as necessary seems foolish.   Die constraints, there's roughly a soft limit of about 600mm2 which it is about (596mm2 give or take)."
hardware,3b8aln,Blubbey,2,Sat Jun 27 16:48:44 2015 UTC,"As a hardcore pixel junkie, I'm pretty damn sad about this.  Video cards have always been an easy choice for me, and was elated when I saw the ""leaked"" 128 ROP count.  With nVidia raising the bar on how to be dicks, and AMD backing off my favorite thing about them, its getting less clear.  MSAA and to a lesser extent SSAA(too expensive for new games) are easily my favorite effects."
hardware,3b8aln,lild3an,6,Sun Jun 28 07:16:54 2015 UTC,"It sounds like AMD was (is?) gambling on software aliasing algorithms moving to shaders.  If that happens AMD has a bigger advantage in DX12's async compute engines vs NV.  I think TR summed it up pretty succinctly with: There is a difference between skating to where the puck will be, and where the puck is now.   Also I like how they looked into the benchmarks that AMD released before the NDA lifted.  For those that didn't listen to it, they found that for a lot of the games AMD turned off texture filtering/anisotropic filtering, turned up the quality of shader effects and in most cases did not use MSAA (that used ROPs) but instead used FXAA/SMAA because they have better shader throughput.  That explains why some reviewers didn't see similar results that AMD put out."
hardware,3b8aln,PappyPete,7,Sat Jun 27 21:07:12 2015 UTC,AA is less relevant at 4K.
hardware,3b8aln,mduell,14,Sat Jun 27 16:52:56 2015 UTC,"Tech Report remains to be one of the most credible sources for hardware reviews. They are incredibly transparent about their methodology and their passion for not only analyzing, but understanding today's hardware shows in their content. Bravo."
hardware,3b8aln,JeffroGymnast,6,Fri Jun 26 20:57:01 2015 UTC,"I completely agree, I don't really read/watch many review sites due to biased opinions, but this is the 2nd time I've watched a youtube video of theirs, and it's just so intriguing how in-depth they get with these things."
hardware,3b8aln,Porcupanda,1 point,Fri Jun 26 21:10:34 2015 UTC,TechReport are simply the most nonbiased tech site out there. Not to mention one of the more skilled ones.
hardware,3b8aln,alabrand,-9,Fri Jun 26 22:13:13 2015 UTC,Non-biased? You're cute.
hardware,3b8aln,melgibson666,5,Sat Jun 27 11:20:27 2015 UTC,"First off, thanks. It's nice to hear that I'm cute since I'm otherwise in reality really fucking ugly. Practically poster child for the word ugly. Second, they're the most nonbiased tech site, everyone and everything has flaws."
hardware,3b8aln,alabrand,-5,Sat Jun 27 16:27:03 2015 UTC,God dam that's a lot of self loathing. u k m8?
hardware,3b8aln,jesusisnowhere,1 point,Sat Jun 27 21:15:34 2015 UTC,Not really. I've been bordering between mania and depression and worse for the last couple of 5 years. Started with me looking at a photograph of me taken without my knowing. I looked like a failed abortion. I have no self-esteem anymore.
hardware,3b8aln,alabrand,1 point,Sun Jun 28 01:06:53 2015 UTC,"The way he says ""tie"" instead of ""tea-eye"" makes me mad"
hardware,3b8aln,Blowmewhileiplaycod,0,Wed Jul 1 17:28:21 2015 UTC,That young guy didnt sat anything at all
hardware,3b9lqj,Exist50,8,Sat Jun 27 02:41:39 2015 UTC,"Ever play with thermoelectric couplers / peltiers?  Apply voltage- one side gets hot the other gets cold. Put some old cpu cooler on each side, and you've either got a heating or cooling element. You can freeze water or boil it.   Or put a voltmeter to it. The temperature difference from one side to the other generates electricity."
hardware,3b9lqj,Imidazole0,1 point,Sat Jun 27 03:38:44 2015 UTC,Very little power though.
hardware,3b9lqj,TheImmortalLS,6,Sat Jun 27 21:29:23 2015 UTC,Most people keep the stock cooler for there cards when they watercool them for when they sell them. Get a better resale as not everyone uses watercooling
hardware,3b9lqj,heratic666,2,Sat Jun 27 04:13:54 2015 UTC,Use them on a phone and overclock the SoC?   Cool your forehead when you have a headache?   Cool your wrists when you're overheating?   Collect them?   I collect heatsinks... I don't have any big fancy gpu ones yet though.
hardware,3b9lqj,CarVac,3,Sat Jun 27 03:08:03 2015 UTC,"I was thinking of repurposing one, if possible, for the most overkill raspberry pi cooler ever. The tricky part is finding them. Surely there's at least some people with useless reference heatsinks sitting around."
hardware,3b9lqj,CarVac,2,Sat Jun 27 03:12:13 2015 UTC,A GPU one would be difficult to use on a Raspberry Pi because it's made to be flat though... you could certainly put a big copper spacer there though.
hardware,3b9lqj,ICanHazTehCookie,2,Sat Jun 27 03:28:54 2015 UTC,"Maybe post on /r/watercooling looking to buy people's coolers, since they'll have waterblocks on. However don't expect to get them for free (or even for cost of shipping), as not having the air cooler will hurt their chances of selling it when they upgrade in the future. Maybe some people wouldn't mind selling it for just the cost of shipping, but I would."
hardware,3b9lqj,44444444444444444,2,Sat Jun 27 05:56:04 2015 UTC,try /r/hardwareswap
hardware,3b9lqj,pb7280,1 point,Sun Jun 28 03:44:46 2015 UTC,"I'm more worried if I can find one for my card that only came with a waterblock, in case I need to put it in a different PC some day."
hardware,3b9lqj,pb7280,1 point,Sat Jun 27 23:57:14 2015 UTC,"Some companies sell aftermarket air coolers, but they tend to be a bit expensive."
hardware,3b9lqj,JaketheAlmighty,1 point,Sun Jun 28 00:11:07 2015 UTC,"Man next time I'm just putting the block on myself, screw warranties."
hardware,3b9lqj,pb7280,1 point,Sun Jun 28 00:47:17 2015 UTC,Or buy EVGA. Removal does not void their warranty. You just have to put the card back in its stock configuration before shipping it if you ever need to RMA it.
hardware,3b9lqj,JaketheAlmighty,1 point,Mon Jun 29 23:49:24 2015 UTC,"Unfortunately I'm usually with red team which doesn't get EVGA. I heard XFX is the same though, but I emailed them asking once and never got a response."
hardware,3b9lqj,pb7280,1 point,Tue Jun 30 00:13:42 2015 UTC,Afaik XFX used to but might not anymore - they've been hitting their warranties for awhile now. (Like the recent removal of their lifetime warranties)
hardware,3b6bak,4980347698,117,Fri Jun 26 09:19:21 2015 UTC,And of course they won't reduce the price.  Someone better prove me wrong.
hardware,3b6bak,MilkyTones,24,Fri Jun 26 10:30:23 2015 UTC,"That cooler is worth like what, $5-10?"
hardware,3b6bak,Beckneard,20,Fri Jun 26 15:36:24 2015 UTC,"You also have to consider in shipping and bigger packaging. Really what they should do is remove the cooler and make the K and non-K series the same price. I mean just the fact they charge more for an unlocked processor is already a joke, this would kind of make up for it."
hardware,3b6bak,reallynotnick,24,Fri Jun 26 17:27:13 2015 UTC,Aren't the unlocked processors binned higher?
hardware,3b6bak,mikelj,22,Fri Jun 26 17:40:53 2015 UTC,"Yes, obviously. K processors are those that have been demonstrably shown to operate stably at higher than stock clock speeds. That's how a processor ends up getting the K label in the first place."
hardware,3b6bak,Oafah,20,Fri Jun 26 18:43:41 2015 UTC,"So why shouldn't they charge more for them? It's not ""just unlocked"" it's a better sample."
hardware,3b6bak,mikelj,3,Fri Jun 26 20:17:56 2015 UTC,I never said they shouldn't.
hardware,3b6bak,Oafah,5,Fri Jun 26 21:36:42 2015 UTC,He was referring to another commenter above you.
hardware,3b6bak,suparnemo,3,Fri Jun 26 22:29:06 2015 UTC,Indeed I was.
hardware,3b6bak,mikelj,2,Sat Jun 27 01:29:33 2015 UTC,How is it that they lock a CPU from being overclocked?
hardware,3b6bak,BuildYourComputer,1 point,Sun Jun 28 18:41:35 2015 UTC,Don't they have CPU (tray) vs (box)?
hardware,3b6bak,TheImmortalLS,2,Sat Jun 27 04:33:07 2015 UTC,"Yes but tray comes without warranty...sadly. If there is any, then it is extremely limited and the tray variants are usually more expensive...for reasons..."
hardware,3b6bak,CaptSkunk,1 point,Sun Jun 28 09:59:11 2015 UTC,"Yes, but box for consumers will still be above tray for oems"
hardware,3b6bak,dylan522p,0,Sun Jun 28 02:22:41 2015 UTC,Yes but....the monies
hardware,3b6bak,HalfBakedJake,6,Fri Jun 26 17:42:44 2015 UTC,A bit more probably. Solid hunks of aluminum tend to be pricey (unless you get them from china). I love using them for random projects with TECs.
hardware,3b6bak,Knight_of_autumn,4,Fri Jun 26 18:19:44 2015 UTC,In the old days if you didn't get a cooler it was five to ten dollars cheaper.
hardware,3b6bak,RiffyDivine2,5,Fri Jun 26 19:24:32 2015 UTC,Nobody overclocks on stock coolers. Why do they include them witj overclockable CPU's anyway?
hardware,3b6bak,Knight_of_autumn,4,Fri Jun 26 19:52:20 2015 UTC,"For spare parts! I broke the latching pin on my non-stock cooler once, and was able to borrow one from the stock cooler to replace it."
hardware,3b6bak,Pipinpadiloxacopolis,1 point,Fri Jun 26 21:24:15 2015 UTC,No idea but I guess if you are new to building you would want it but those of use with a bucket full of them really I see no point and I really don't want another one.
hardware,3b6bak,RiffyDivine2,1 point,Sat Jun 27 00:53:33 2015 UTC,"Solid hunks of aluminum tend to be pricey (unless you get them from china).   Aluminum is pretty cheap, about $0.75/lb. Even after machining removing half of it you're only at $1.5/lb for the final product."
hardware,3b6bak,mduell,1 point,Sat Jun 27 01:55:34 2015 UTC,"Ok, sorry, aluminum pre-machined to be an effective heatsink with fins"
hardware,3b6bak,Knight_of_autumn,2,Sat Jun 27 01:56:49 2015 UTC,"Great, so less than a dollar of aluminum in the quantities Intel is buying..."
hardware,3b6bak,mduell,56,Sat Jun 27 05:43:12 2015 UTC,Why would they reduce the price when they have absolutely no competition in the high end/enthusiast market? AMD doesn't compete with anything except the low end i3 market. Gotta wait for Zen before they actually have competition.
hardware,3b6bak,skilliard4,10,Fri Jun 26 10:33:21 2015 UTC,Midrange market as well.
hardware,3b6bak,Schmich,6,Fri Jun 26 11:47:08 2015 UTC,Technically amd doesn't really compete until youre buying 8370s or higher or until you're overclocking your chips
hardware,3b6bak,AssCrackBanditHunter,-1,Fri Jun 26 13:00:01 2015 UTC,"Disagree. AMD still has a leg in the midrange and lower end market.   As we turn farther away from 2012/2013, purchasing a Piledriver chip from AMD seems more and more passé, however the FX-6350 and FX-8350 still give you great performance for the price. The gains you'd see on an Intel i5 chip won't always be significant for current games and won't necessarily be worth the $50-$150 premium.   Comparing performance hit for hit of course Intel is on top but that only matters for high end/enthusiast builds. All other tiers it's about the best bang for you buck and Intel is still pretty pricey."
hardware,3b6bak,mechanical_animal,5,Sat Jun 27 01:43:01 2015 UTC,Gotta wait for Zen before they actually have competition.   Depending on how things turn out that still might not be the case.
hardware,3b6bak,Blubbey,11,Fri Jun 26 14:01:47 2015 UTC,"If Zen was being released this year I would give them a shot, but the fact that it's scheduled for late 2016, and AMD has been terrible about shipping their CPUs on time anyway, gives me little hope."
hardware,3b6bak,jmac,1 point,Fri Jun 26 14:57:48 2015 UTC,"On /r/amd someone mentioned Q2 2016, so around this time of the year. Not sure which is correct though."
hardware,3b6bak,thoosequa,-4,Fri Jun 26 17:53:06 2015 UTC,We're pretty late in Q2
hardware,3b6bak,Kevo_CS,5,Fri Jun 26 18:06:19 2015 UTC,2016
hardware,3b6bak,mtocrat,4,Fri Jun 26 19:13:26 2015 UTC,"At least it'll be 14 nm, while Intel will also be at 14 nm. Must be better than the current situation, at least. Whether that's enough to make Intel decrease prices, I'm not convinced about."
hardware,3b6bak,bphase,3,Fri Jun 26 14:47:10 2015 UTC,Except Intel 14nm is ~.75 die shrinks better on terms of actually measurements.
hardware,3b6bak,dylan522p,2,Fri Jun 26 17:09:56 2015 UTC,"True, can't really just compare the number these days."
hardware,3b6bak,bphase,2,Fri Jun 26 17:53:26 2015 UTC,What does that even mean?
hardware,3b6bak,1337Gandalf,1 point,Fri Jun 26 21:09:04 2015 UTC,A die shrink generally means a certain amount of sgrink jn gate size and interconnects and such. It's closer to a full die shrink on interconnect and less on the gate. Although Intel makes a reallllly tall gate on second gen FinFet (14nm). Measurements for various nodes can be found.
hardware,3b6bak,dylan522p,1 point,Fri Jun 26 22:41:06 2015 UTC,"I see. I always thought a die shrink meant the transistors got smaller, but wasn't aware of the rest."
hardware,3b6bak,1337Gandalf,1 point,Sat Jun 27 00:40:28 2015 UTC,"Can you link some sources? IIRC, Samsung's 14nm is about as dense, though may not be as good electrically."
hardware,3b6bak,Exist50,1 point,Sat Jun 27 02:32:33 2015 UTC,What makes you think Samsungs is as dense? Because q mobile SOC where they packed the transistors as tight as possible vs Intel who spreads these tiny transistors out for better heat on many dies. They can still make extremely dense processors too. The 14nm atoms are more dense than exynos
hardware,3b6bak,dylan522p,1 point,Sat Jun 27 14:53:02 2015 UTC,"Again, can you actually list some sources? I'm not saying you're wrong, but I'd rather not rely on your word."
hardware,3b6bak,Exist50,1 point,Sat Jun 27 15:01:17 2015 UTC,Even if Zen is a strong competitor for i5 6600k that will be a big improvement in the competitive landscape
hardware,3b6bak,rePAN6517,3,Fri Jun 26 23:58:15 2015 UTC,There is so little competition that they can afford to waste 1/2 the die area on IGPU that no one even uses on the K parts. :/
hardware,3b6bak,hisroyalnastiness,5,Fri Jun 26 20:55:47 2015 UTC,i3s low end?
hardware,3b6bak,Yearlaren,6,Fri Jun 26 16:42:43 2015 UTC,i3=low end  i5= mid range  i7=high end
hardware,3b6bak,skilliard4,28,Fri Jun 26 16:44:22 2015 UTC,I thought it was:  Pentium/Celeron = Low end  i3 = Mainstream  i5 = High end  i7 = Enthusiast
hardware,3b6bak,Yearlaren,16,Fri Jun 26 18:35:05 2015 UTC,"It is. What you're experiencing right now is elitism, first-hand. It's best not to argue."
hardware,3b6bak,Oafah,9,Fri Jun 26 18:42:31 2015 UTC,Pentium/Celeron = Bottom of the barrel  i3 = Very low end  i5 = Low end  i7 = Mid range  X99 = Above mid range  18 core xeons = High end  :D
hardware,3b6bak,meowffins,9,Fri Jun 26 20:10:41 2015 UTC,"If you are not running 18 cores, quad Titan X GPUs, and Raid 0ing your SSDs then you don't have a real PC."
hardware,3b6bak,Scuderia,1 point,Fri Jun 26 21:44:32 2015 UTC,Question: Is it elitism from the i5 users or from the i7 users?
hardware,3b6bak,DJ-Foran,2,Fri Jun 26 20:00:45 2015 UTC,"The i7 users, I imagine. Most people that get high-end CPUs are gamers. And most gamers don't need an i7 because gaming performance is near-identical (compared to an i5)."
hardware,3b6bak,Anaron,2,Fri Jun 26 20:03:47 2015 UTC,For future reference: GTAV CPU Benchies
hardware,3b6bak,DJ-Foran,1 point,Fri Jun 26 23:57:45 2015 UTC,Thanks.
hardware,3b6bak,Anaron,1 point,Sat Jun 27 07:03:44 2015 UTC,I wouldn't use pclab.pl for benchmarks. They tend to be a bit... off... to put it lightly.
hardware,3b6bak,Exist50,2,Sat Jun 27 02:33:47 2015 UTC,At least /r/pcmasterrace warns people about that in their wiki. With quite the amusing video too.
hardware,3b6bak,spiral6,1 point,Sat Jun 27 05:40:11 2015 UTC,"The X99 users...  I'm perfectly happy with my 4690K. Sure, I took a few looks at it's i7 brother and the X99 variants but they just did not make sense for me or my bank account.   Now, if I had the money, then yes, I'd get nothing but the best, if I need it or not."
hardware,3b6bak,CaptSkunk,1 point,Sun Jun 28 09:54:54 2015 UTC,"I still wouldn't think it worth the money if the buyer knew what they wanted and what they were doing. That is, take a 4690k and overclock it to stock 4790k speeds. If we discount the HT that the 4790k had, would the performance not be the same?"
hardware,3b6bak,DJ-Foran,2,Sun Jun 28 14:34:08 2015 UTC,"Depending on what you are doing, pretty much. I do a lot of gaming with my rig but I will also run VM's or do a little compiling. It works great for that and many benchmarks have shown that the 4690K is within 1-5 FPS of the 4790K, depending on the game and other hardware.   That is the main reason why I got the 4690K. It seems to be in a sweet spot. It has power, speed and will last a good 4 years or so. To get the i7, I'd have had to spend €100 more and not really got anything for it. I'm loving my chip. Maybe my next build will be on whatever the enthusiast platform is, in 4 years."
hardware,3b6bak,CaptSkunk,1 point,Sun Jun 28 14:44:42 2015 UTC,"Yeah X99 is for server or for workstation, not for gaming."
hardware,3b6bak,jordsti,0,Mon Jun 29 15:59:56 2015 UTC,"I think the majority of people who subscribe to this subreddit, as well as /r/buildapc, /r/gamingpc, /r/pcgaming, and a whole host of others, are guilty of it to some degree.  I build systems out of used components and give charity builds to kids who couldn't otherwise afford it. Using components that long outdate the ""I"" naming scheme, I can put together something that runs modern games at playable frame rates, with decent quality at console-or-better resolutions. I usually sell these systems (when I'm not donating them) for $200-$400. That, I think everyone would agree, qualifies as the ""low-end"", and it's still perfectly fine.  As such, you can imagine why I'd take particular exception to people who say things like ""You need an i5 for gaming"" and ""1080p is outdated""."
hardware,3b6bak,Oafah,2,Fri Jun 26 20:14:20 2015 UTC,You don't need an i5 for gaming.  But it's getting pretty close to needing something at least capable of 4 threads though.
hardware,3b6bak,w00t692,1 point,Fri Jun 26 21:17:43 2015 UTC,"If you look at Linus' video on the subject, they make it quite clear that people on dual cores are usually fine, and those with hyperthreaded dual-cores are slightly better off still. We're still a long way from ""need"" outside of Far Cry 4, as it's not like these games just won't run otherwise."
hardware,3b6bak,Oafah,1 point,Fri Jun 26 21:36:19 2015 UTC,Pentium and Celeron are the budget CPU's from Intel. When you just need something cheap and now.
hardware,3b6bak,CaptSkunk,8,Sun Jun 28 09:51:41 2015 UTC,Intel doesn't just produce i series cpus. The celeron and pentium cpus are the real low-end.
hardware,3b6bak,i_am_cat,-7,Fri Jun 26 17:04:32 2015 UTC,An i3 is still low-end.
hardware,3b6bak,Whats_a_narwhal,4,Fri Jun 26 17:59:21 2015 UTC,An i3 is completely fine for most people.
hardware,3b6bak,Yearlaren,-5,Fri Jun 26 18:35:39 2015 UTC,I never said it wasn't. Most people do not need more than a lower-end processor.
hardware,3b6bak,Whats_a_narwhal,-1,Fri Jun 26 18:55:25 2015 UTC,Then what means a processor is low-end?
hardware,3b6bak,Yearlaren,-2,Fri Jun 26 20:19:33 2015 UTC,"When something is low-end it means it's positioned under the mid-range product, which the i3 is. The Pentium and Celeron being under the i3 does not make the i3 not low-end.  I never thought I'd see the day where people are downvoting me and legitimately arguing over the i3 being low-end or not."
hardware,3b6bak,Whats_a_narwhal,4,Fri Jun 26 20:32:05 2015 UTC,"It objectively is not the ""low end"". Not by any measure.  The i3 series is what Intel themselves labels as ""mainstream""."
hardware,3b6bak,Oafah,1 point,Fri Jun 26 18:41:18 2015 UTC,they are still competing with previous generations of intel.  there has to be some performance/value increase otherwise people will stay with what they have or buy previous generations.
hardware,3b6bak,logged_n_2_say,-1,Fri Jun 26 16:51:47 2015 UTC,"Yeah, because if you give AMD time, they always find a way to beat their competition."
hardware,3b6bak,eXXaXion,0,Fri Jun 26 16:47:38 2015 UTC,"Well its been 46 years, how much more we gotta give them?"
hardware,3b6bak,Iliketrainschoo_choo,1 point,Fri Jun 26 19:29:09 2015 UTC,"This comment has been overwritten by an open source script to protect this user's privacy.   If you would like to do the same, add the browser extension TamperMonkey for Chrome (or GreaseMonkey for Firefox) and add this open source script.     Then simply click on your username on Reddit, go to the comments tab, and hit the new OVERWRITE button at the top."
hardware,3b6bak,dontnation,5,Fri Jun 26 21:48:36 2015 UTC,"They'll probably even increase it, due to Skylake being a relatively enticing upgrade with the platform change and no competition. It also seems to be targeting overclockers with big coolers and the high TDP limit.  Competition would be nice for sure. Although if Skylake actually is a big improvement from Haswell, I suppose most won't mind the increased price too much, considering how well processors age these days."
hardware,3b6bak,bphase,13,Fri Jun 26 14:45:50 2015 UTC,"Article says ""may not"".   OP writes ""will not""."
hardware,3b6bak,Sawgon,10,Fri Jun 26 21:41:40 2015 UTC,Think about all the metal and plastic that will be saved from sitting inside your CPU box. They should offer it as a cheap/free option for you to apply for through the Intel site though.
hardware,3b6bak,HalfBakedJake,48,Fri Jun 26 17:41:35 2015 UTC,"I think this is a good change, who needs a stock cooler when you know you will replace it yourself."
hardware,3b6bak,Klorel,59,Fri Jun 26 09:19:52 2015 UTC,"indeed, just a piece of waste.   the only bad thing about it is that the price won't get reduced due to this. only intels margin will rise."
hardware,3b6bak,Pillowsmeller18,35,Fri Jun 26 10:00:56 2015 UTC,"Well stock coolers help if the main one breaks and you got nothing to use. Also if you plan to make a new build and turn the old one into a home server, you can use the after market cooler on the new one and the stock on the old one."
hardware,3b6bak,olavk2,6,Fri Jun 26 11:30:57 2015 UTC,"this, also another thing is the stock cooling fan can be used for other things(personally i used the fan on my am3+ cpu for VRM heatsink cooling to get higher overclocks)"
hardware,3b6bak,1-Ceth,0,Fri Jun 26 12:07:37 2015 UTC,"Out of curiosity, what kind of increases did you manage by cooling the VRMs directly? I've got passive heatsinks on my mobo. I know on GPUs some waterblocks don't even bother covering them, although mine does.   Would you say it's worth getting a fan pointed at my mobo VRMs?"
hardware,3b6bak,olavk2,1 point,Fri Jun 26 13:24:34 2015 UTC,"i would say so, the temps dropped by maybe 20-30C when adding the fan from the stock hsf that came with the fx 8320."
hardware,3b6bak,buildzoid,1 point,Fri Jun 26 13:33:34 2015 UTC,I put a fan on the VRM of an Asrock 970M Pro3 board. It has a 4+1 phase VRM. Without the fan it can barely do 4.5Ghz but with the fan it can do 4.7Ghz on 1.5V.
hardware,3b6bak,bphase,3,Fri Jun 26 13:44:40 2015 UTC,"Coolers don't simply break. The fan might, I suppose, but then it's much easier to just grab a random suitable fan from your case or something than switch the damn thing."
hardware,3b6bak,sk9592,1 point,Fri Jun 26 12:49:33 2015 UTC,"Air coolers don't randomly break, but a liquid cooler's pump can fail or tube can leak."
hardware,3b6bak,ShinseiTom,3,Wed Jul 1 13:47:22 2015 UTC,"Me, definitely.  No overclock, no need for aftermarket.  Just more money down the drain.  Stock coolers are perfectly able to cool a stock chip given decent airflow with every cpu I've had.  When I want to overclock though I have a nice AIO water cooler I got to fit in my 250D, but since a friend is using that case and my current case can't fit it right now, it's going unused.  Stock cooler to the rescue!"
hardware,3b6bak,mechanical_animal,4,Fri Jun 26 14:55:15 2015 UTC,"The K in Skylake K denotes an unlocked processor for overclocking.  The point is that since you're buying the unlocked version of a processor, you're also likely to replace the cooler for higher clocks. If you're not going to be doing overclocking why get the unlocked cpu?"
hardware,3b6bak,III-V,3,Sat Jun 27 01:49:02 2015 UTC,Better resale value
hardware,3b6bak,Raydonman,2,Sat Jun 27 14:35:12 2015 UTC,"I bought a mini ITX and as such didn't know what would fit in my case. I've been running stock since November. It's definitely loud, and doesn't cool as well as it should, but I've never had a single problem with heating even here in Hawaii with no air conditioning.   tl;dr stock cooler gets the job done...kinda"
hardware,3b6bak,Dark_Crystal,2,Fri Jun 26 14:45:49 2015 UTC,I always test a new board and CPU combo with the stock cooler and get everything set up before moving the board and CPU into my case and getting the watercooling hooked up.
hardware,3b6bak,DrexelDragon93,6,Fri Jun 26 16:54:29 2015 UTC,"Why would you (if you know you are buying a K processor) go through the trouble of putting on the stock cooler and then having to reseat, clean and reapply the new custom cooler a week later when it arrives. Silly."
hardware,3b6bak,DrexelDragon93,6,Fri Jun 26 11:15:44 2015 UTC,A custom cooler that is like 10x better than the intel stock one is about $15.
hardware,3b6bak,DrexelDragon93,11,Fri Jun 26 14:30:15 2015 UTC,"Okay in the same vein then, wait an extra few days until you have the money to buy everything at once."
hardware,3b6bak,random_guy12,15,Fri Jun 26 14:57:48 2015 UTC,"If you have to wait for a paycheck to have money on hand for PC components, you shouldn't be buying these PC components. You can't afford them.  (Unless you're a dependent)."
hardware,3b6bak,Aurailious,5,Fri Jun 26 15:00:54 2015 UTC,But you are already spending upwards of £200 for a CPU?
hardware,3b6bak,Aurailious,2,Fri Jun 26 15:12:11 2015 UTC,"So, you are spending a lot of money, but complaining that you don't get a cheap stock cooler?"
hardware,3b6bak,Kinaestheticsz,6,Fri Jun 26 15:13:35 2015 UTC,Not to mention it isn't good financial sense to be spending that much money on computer components that you live paycheck to paycheck.
hardware,3b6bak,Iliketrainschoo_choo,1 point,Fri Jun 26 15:14:21 2015 UTC,Am I the only one that saves enough money for the whole computer before buying it?
hardware,3b6bak,SCREAMING_FLESHLIGHT,2,Fri Jun 26 15:21:18 2015 UTC,"No you're not, this guy is just making poor financial decisions."
hardware,3b6bak,Stingray88,1 point,Fri Jun 26 15:47:42 2015 UTC,"Why would I spend £15 when I can just spend £0, wait for my next paycheck and get a £100 cooler?   If you are so strapped for cash that you have to wait until your very next paycheck to buy a £100 cooler... you shouldn't be buying these components in the first place. You should work on having more savings so that making a big purchase like this all at once isn't a problem."
hardware,3b6bak,Stingray88,-3,Fri Jun 26 16:02:01 2015 UTC,"I'm ""strapped for cash"" because I put a significant amount away each paycheck   OK, so you have a savings... but can't afford a £100 cooler until your very next paycheck?   I'm not about to change my spending habits because some guy on the internet decided I need to blow more money in one go.   There's literally no difference. You're spending $100 now, or your spending it on your next paycheck. It's not any different except for the fact that by spending it later, you have to unmount the stock cooler, clean off the CPU, remount the new cooler... when you could just wait and do it all at once.   Do you people seriously spend your entire salaries on computer parts?   No we don't. I don't know why you would think anyone here does that when everyone is saying don't spend all your money if you have to wait for your next paycheck to be able to buy a £100 cooler.  You're missing the point people are making here. It's not about blowing all your money at once. It's just about buying your CPU and CPU cooler at the same time. It makes far more sense. We're not telling you what to do with your money. We're telling you what makes sense, and you're free to do whatever you want with that.   Savings are for important things like emergency travel, emergency medical treatment, and basic survival. Not expensive computers.   You can have multiple groups of ""savings"". I'm aware you have emergency savings, that's great. You can also save for computer parts too in a separate pool."
hardware,3b6bak,nyislanders,3,Fri Jun 26 16:09:19 2015 UTC,So save your money and buy them at the same time.
hardware,3b6bak,nyislanders,5,Fri Jun 26 16:31:39 2015 UTC,??? All I said was to save money. I don't understand how that's bad advice. Not calling anyone poor/stupid. I'm not exactly swimming in money either.
hardware,3b6bak,Stingray88,1 point,Fri Jun 26 17:08:10 2015 UTC,"Edit: if you're about to click the reply button to suggest uninformed personal finance advice, don't.   Everyone who replied gave you informed advice."
hardware,3b6bak,Stingray88,1 point,Fri Jun 26 19:31:14 2015 UTC,"You have no idea how I divide my salary; how could you possibly have provided informed advice?   I don't need to know. We're giving you computing advice. You've stated elsewhere in this thread that you put away 30% of your income into savings. You're probably doing OK financially if that's the case, so really it's just a matter of computing advice.  Going through the hassle of installing the stock cooler, only to replace it with a more expensive cooler when your next paycheck comes is pretty absurd when you can afford to buy it all at once. That's the advice we're giving you. If you don't want it, that's fine.   I suspect your eagerness to give financial ""advice"" when it wasn't even requested demonstrates you'd rather argue for the sake of arguing.   No, my eagerness to give you advice is because I love to give computing advice. It's part of why I post to so many tech related subreddits.   It is of no relevance to you why I prefer to not install the CPU and an aftermarket cooler at the same time, whether it be financial reasons, personal preference or no intention to overclock for a while, only that that is what I like to do.   We're aware of what you like to do. We're just give you good, solid advice. Take it or leave it."
hardware,3b6bak,Stingray88,2,Fri Jun 26 20:08:19 2015 UTC,"I haven't repeated myself, all of my comments are unique replies to your comments. I'm responding to your comments, because you're not just leaving it. You're making counter points to our advice as well as making other points, and I'm responding to those points. Just having a conversation, that's all. If you truly want to end a conversation with someone you're disagreeing with, it's generally best not to end with a point they may want to counter."
hardware,3b6bak,canebarbone,13,Fri Jun 26 17:31:03 2015 UTC,"Finally coasters manufacturers will start selling again, the competition by unused intel stock coolers was too much to handle... ;)"
hardware,3b6bak,TruGW2,2,Fri Jun 26 17:34:05 2015 UTC,"Graphics cards make better coasters, I use stock coolers for doorstops."
hardware,3b6bak,Helicase21,4,Fri Jun 26 17:46:59 2015 UTC,will 1151 be OK with pre-existing 1150/1155 coolers?
hardware,3b6bak,ElDubardo,8,Fri Jun 26 16:08:49 2015 UTC,That stock cooler saved my life when my h220 pump died... It took almost 2 week for my replacement to arrive. I would have died of boredom without my desktop
hardware,3b6bak,Arudinne,-1,Fri Jun 26 16:13:33 2015 UTC,Do you not live near enough to an electronics store to buy a cheap temporary replacement?
hardware,3b6bak,ElDubardo,4,Fri Jun 26 16:25:09 2015 UTC,"I could, but why pay for temporary? I had the stock cooler dusting somewhere! Just saying it served a purpose after all!"
hardware,3b6bak,Anaron,-1,Fri Jun 26 17:49:47 2015 UTC,"So you'd rather ""die of boredom""? That comment makes it seem like getting a temporary cooler in time is impossible."
hardware,3b6bak,Nezonic,3,Fri Jun 26 17:56:45 2015 UTC,"No, he'd rather have the convenience of that stock cooler lying around."
hardware,3b6bak,Anaron,-2,Fri Jun 26 18:08:25 2015 UTC,That's painfully obvious. I was referring to his dying of boredom comment. I doubt he'd suffer 2 weeks just to save $15.
hardware,3b6bak,__Cyber_Dildonics__,3,Fri Jun 26 18:37:13 2015 UTC,Does this chip have integrated graphics on it?
hardware,3b6bak,Phantom_Absolute,11,Fri Jun 26 18:50:19 2015 UTC,"Almost certainly, yes."
hardware,3b6bak,jordsti,1 point,Fri Jun 26 10:04:42 2015 UTC,"Yes, probably Iris Pro GPU"
hardware,3b6bak,tenthkarma,2,Fri Jun 26 20:52:53 2015 UTC,"Um, the biggest problem for me here is that the Skylake K processors are allegedly rated for 95W while the non-K processors are rated for 65W, and they get a huge clock advantage off the gate. This is a big change from Devil's Canyon, which only increased the TDP from 84W to 88W, or not-devil's canyon K units, which had no increased TDP.  It doesn't seem like you're getting the same CPU in K and not-K for skylake, so the argument that ""K is just for overclockers"" is faulty. So why can't we get a stock cooler with the K processors, again? I have little interest in overclocking, but I have a big interest in getting a CPU with a full TDP rating.   Source: Wikipedia, which is based off of those earlier leaks. I'm about 50-50 that those are correct, though."
hardware,3b6bak,drnick5,3,Fri Jun 26 20:51:28 2015 UTC,Anyone remember the days when you could buy JUST a CPU without the cooler for a slight discount? (like $5 or $10).
hardware,3b6bak,LFKhael,3,Fri Jun 26 16:41:47 2015 UTC,Like...today?
hardware,3b6bak,bankkopf,3,Fri Jun 26 16:56:03 2015 UTC,Disadvantages of Tray options include already sent back CPUs by overclockers and less/no warranty time at least in Germany. And that is often with hardly any savings compared to Boxed.
hardware,3b6bak,itssmacks_,5,Fri Jun 26 17:17:06 2015 UTC,I don't really care I have enough coolers lying around. This is certainly something that might annoy one of the many people who build their pc and wait a few weeks until they have the cash to buy an aftermarket cooler however.
hardware,3b6bak,Pesceman3,5,Fri Jun 26 20:06:51 2015 UTC,There's really no reason to use an aftermarket cooler unless you plan to overclock.  Out of all the builds I've helped my friends with only 1 uses an aftermarket cooler.  So this is a rather annoying change from Intel.
hardware,3b6bak,freshmas,11,Sat Jun 27 00:27:54 2015 UTC,"Why would you buy the K series chip, then?"
hardware,3b6bak,vhodges,7,Sat Jun 27 07:04:49 2015 UTC,"Because it has the highest stock clock speed.  I'd be happy to by a locked CPU if it matched that 4gz.    I am not planning to overclock (it's for a server - and frankly I have other things to do besides futzing around with the innards of my machine all the time) so the need to buy a cooler adds $50-$60 to the build cost, not a lot, but for me probably the difference between a haswell build and a skylake build (given RAM and MB will likely be more costly as well)."
hardware,3b6bak,Dark_Crystal,2,Fri Jun 26 12:51:05 2015 UTC,"Overclock is basically auto on most boards these days. Hell, a bunch will even do stability tests for you. Go for higher turbo speeds, so that when you need it the speed is there."
hardware,3b6bak,Pesceman3,2,Fri Jun 26 13:17:15 2015 UTC,"In my case it was because I planned to overclock in the future, but I can't say for someone else.  I used the stock cooler for about a year before installing a noctua."
hardware,3b6bak,lengau,2,Mon Jun 29 16:02:32 2015 UTC,"That is exactly my situation right now. I have a 4770k (I think) running at standard frequency and with the stock cooler. It works just fine for my workload. When I decide it's time for a new machine, I'll overclock it and if necessary get an aftermarket cooler, which should buy me enough time to wait for the next generation of processors."
hardware,3b6bak,Arudinne,4,Sun Jun 28 15:39:58 2015 UTC,There's really no reason to use the stock cooler unless you hate the sound of quiet.
hardware,3b6bak,gotbannedtoomuch,2,Fri Jun 26 18:44:30 2015 UTC,There's really no reason to use an aftermarket cooler unless you plan to overclock.   or if you don't like the chainsaw sound of a stock cooler
hardware,3b6bak,itssmacks_,1 point,Fri Jun 26 21:58:04 2015 UTC,What is the point of buying a K SKU if you are not intending to overclock?
hardware,3b6bak,Raising,2,Fri Jun 26 23:02:01 2015 UTC,"cpu world is stale as fuck, my >5 year old i7980x overclocked to 4.4ghz has the same firestrike physics score(15k~)as a stock i7 5690x..  if I would upgrade now to the most popular cpu (i7 4790k) even overclocked it would not reach that performance.."
hardware,3b6bak,ShotgunPanda,1 point,Fri Jun 26 12:17:01 2015 UTC,"Preach. I'm still sticking with my i5-2500k until Zen comes out and if its not a flop maybe I'll upgrade for the increased core count. Not many applications nowadays warrant the extra performance unless you're manually programming them yourself.  If not, I'll probably just grab a used i7-2600k and do a swap."
hardware,3b6bak,tsk05,1 point,Fri Jun 26 14:00:50 2015 UTC,"The firestrike physics score is hugely dependent on the GPU, weird method of comparing CPUs. The passmark CPU score of the 4790 is 15% better than 980, and given that you can overclock the unlocked version of both to 4.4 GHz it should only widen that lead. That's also assuming only for parallel processing, for a single core the 4790 is a lot better (given that the 6 cores of the 980 are 15% behind the 4 cores of the 4790)."
hardware,3b6bak,Raising,1 point,Fri Jun 26 14:09:22 2015 UTC,what ? firestrike physics score has nothing to do with gpu..  I have the same score with a 5850 and two 7950's
hardware,3b6bak,tsk05,1 point,Fri Jun 26 14:37:31 2015 UTC,Here is the link to 3DMark benchmarks for Firestrike Physics. Select differing numbers of GPUs and notice the drastic score changes for exactly the same processor.
hardware,3b6bak,Raising,1 point,Fri Jun 26 16:58:18 2015 UTC,seriously ? those are overclocked with liquid nitrogen etc and the reported clocks are not reliable..
hardware,3b6bak,phigo50,1 point,Fri Jun 26 14:13:29 2015 UTC,"I only ever go for retail CPUs for the warranty, I'm pretty sure that all of the ones I've bought still have the coolers in the boxes."
hardware,3b6bak,Liam2349,1 point,Sat Jun 27 21:42:35 2015 UTC,"They will be extortionate. Intel CPU prices have been going up and up  since AMD stopped competing. We have to pray that Zen can force them to drop the pricing, or that Zen will be at least on par, which I don't see happening but we can hope for some competition at least."
hardware,3b6bak,Darkstryke,1 point,Fri Jun 26 16:57:26 2015 UTC,Good. I can't count the number of OEM coolers I have thrown in the metal bin over the years. Might shave a whole $5 off the price.
hardware,3b6bak,RiffyDivine2,0,Fri Jun 26 15:42:24 2015 UTC,"Fine by me, got a dozen of them laying around and enough other coolers. I can't remember the last time I saw anyone use the stock cooler."
hardware,3b6bak,58592825866,0,Sat Jun 27 08:14:20 2015 UTC,Intel trying to squeeze as much margin as possible out of their products now that AMD is almost entirely out of the game (until late 2016).
hardware,3b6bak,1337Gandalf,-2,Sat Jun 27 10:18:34 2015 UTC,"AMD's dying, and they decide to basically raise their price? is anyone surprised by this?"
hardware,3b6bak,skilliard4,-34,Sun Jun 28 02:10:03 2015 UTC,"When their stock coolers aren't even enough to cool systems running at stock speeds, it's probably a good decision to stop shipping CPUs with them. It's just a waste of metal.  edit: not sure why the downvotes. Friend bought an i7 4790K, ran at stock speeds(even disabled turbo clocks), thing would overheat with the stock cooler and would BSOD when reaching 100 degrees Celcius. Replaced Cooler with an aftermarket cooler, never had an issue ever again.  Meanwhile I run an AMD stock cooler and I managed to get a stable overclock with temps below 50 Celsius. It's sad how poor Intel stock coolers are."
hardware,3b6bak,Zaev,27,Tue Jun 30 08:53:22 2015 UTC,"I somehow think that said stock cooler wasn't installed correctly, or there were other extenuating circumstances."
hardware,3b6bak,Chameleon3,5,Tue Jun 30 14:24:56 2015 UTC,"That is definitely the issue. I had a stock cooler on my 4790K for a day and it ran cool enough, don't remember it going over 75-80°C, but I didn't stress test it since I was waiting for an aftermarker cooler.  Next day I install a watercooler and it heats so much in stress tests that it started throttling. After reapplying the cooler three more times (and using a tip someone mentioned for that specific cooler, using rubber dampeners behind the mobo to increase the pressure) it was working perfectly. The CPU doesn't go over 60-70 during stress tests (IIRC).  Bottom point being, applying any cooler the wrong way will cause heat issues."
hardware,3b6bak,Zaev,2,Tue Jun 30 15:13:18 2015 UTC,H100i? Did you buy your rubber thingies online somewhere?
hardware,3b6bak,Chameleon3,1 point,Tue Jun 30 16:13:43 2015 UTC,"Haha yup, exactly, the H100i. I did not buy the rubber thingies online though, they came with something I bought while building my gaming PC.   I have no idea what they were included with though, I just found them in the pile of random crap inside my motherboard box where I chucked everything."
hardware,3b6bak,Zaev,1 point,Fri Jun 26 14:59:30 2015 UTC,"Ah. I have the same cooler, and while I'm getting good performance out of it, it's not as good as I'd expected so I thought I'd try the rubber spacer trick."
hardware,3b6bak,Chameleon3,2,Sun Jun 28 02:52:52 2015 UTC,"I would personally definitely recommend doing it. It was the same for me, the performance was acceptable, but not what I expected.   After doing the rubber spacer trick, it's working reeeaally well. It handles really well when I'm rendering video and other CPU heavy tasks and I even have the fans set on a quiet configuration."
hardware,3b6bak,lengau,1 point,Fri Jun 26 14:21:18 2015 UTC,"Do you consider a full (from scratch) Chromium build a reasonable stress test? (It used up all 8 virtual cores for several hours) If so,  stock cooler on a 4770k let it go all the way up to 82°C."
hardware,3b6bak,musef1,14,Fri Jun 26 19:20:08 2015 UTC,"Downvoted because it's not true. You're friend must have messed up or they had a fault somewhere.  There's three PC's in the house I am in, all Intel, all with a stock cooler and no problems. One of those machines is an OC'd i5 2500k. Myself and two friends (so 3 machines) had a Core 2 Quad with stock coolers, again no problems. They now have  a 4770k and 4790k with stock cooler, no problems. So that's 8 intel coolers doing their job perfectly fine."
hardware,3b6bak,Teethpasta,13,Fri Jun 26 22:26:38 2015 UTC,You're friend fucked something up pretty bad. Intel has a better stock cooler than AMD.
hardware,3b6bak,Maysock,4,Fri Jun 26 21:04:24 2015 UTC,not sure why the downvotes.    Because it's painfully clear that the stock cooler for the intel in your story was incorrectly installed.
hardware,3b6bak,1-Ceth,3,Fri Jun 26 09:53:39 2015 UTC,"Few points:  1) Your friend absolutely needs to get a bigger/cooler case, move the case to a cooler location, or needs to consider that maybe he placed the CPU block wrong.   2) I don't have anything to support this claim, but because FX processors tend to run hotter, they need better stock coolers.   This issue is compounded by the fact AMD targets the entry-level market, so users may be less likely to understand heat issues and will just blame AMD. This means that AMD's stock stuff needs to be even better, so that their builds will run when a new builder shoves his PC inside an almost airtight cabinet.   Again, no real support for this, just my assumptions as to why the FX stock may be better.   3) I believe the major reason you're getting downvoted is because you're making definite claims based on anecdotal evidence. If you're going to cite anecdotes, you should at least try to sound a bit less certain."
hardware,3b6bak,Dark_Crystal,2,Fri Jun 26 12:26:57 2015 UTC,When their stock coolers aren't even enough to cool systems running at stock speeds   That hasn't been true for years.
hardware,3b6bak,skilliard4,0,Fri Jun 26 14:42:55 2015 UTC,its true of the i7 4790K.
hardware,3b6bak,Dark_Crystal,1 point,Fri Jun 26 14:48:25 2015 UTC,"Nope, the i7s TDP is actually LOWER than the 4790k."
hardware,3b9j1t,Boy1998,26,Sat Jun 27 02:15:06 2015 UTC,"It's not so much an inherent issue of Freesync, but how it's implemented. For example, here's one with a 30-160Hz range: https://pcmonitors.info/aoc/aoc-g2460pf-and-g2770pf-freesync-gaming-models/  It can only do 160Hz with Freesync active."
hardware,3b9j1t,Exist50,6,Sat Jun 27 04:22:11 2015 UTC,Wow 30 fps that is pretty low reaching. I like it.
hardware,3b9j1t,MINIMAN10000,16,Sat Jun 27 16:02:01 2015 UTC,"Yeah, as far as I'm concerned, if you can't keep 30fps minimum in the games and at the settings you want to play, you're better off investing in other things than a 160Hz monitor."
hardware,3b9j1t,Exist50,18,Sat Jun 27 16:13:04 2015 UTC,It's an issue not with AMD but the monitor manufacturer.   Freesync the protocol can do anywhere from 5hz to 240hz.   But if the scaler chip that monitor manufacturers use isn't up to snuff then they limit the range heavily.
hardware,3b9j1t,Seclorum,7,Sat Jun 27 06:15:41 2015 UTC,It's an issue not with AMD but the monitor manufacturer.   Give them an open standard and the freedom to implement it poorly and they will. Who knew?
hardware,3b9j1t,thoomfish,14,Sat Jun 27 07:27:48 2015 UTC,"It's not a poor implementation (they're still good monitors), it's simply the easiest one. These things take time, you can't just snap your fingers and suddenly the implementation is perfect. They did what they could in the time they had and it's quite clear that things are improving with each new release. In 3 years all this will be forgotten & the range of choices will be immense."
hardware,3b9j1t,Trollatopoulous,-1,Sat Jun 27 16:30:09 2015 UTC,"I look forward to the day they take both 'X-Sync' Technologies and implement them both at the same time.   We already know G-Sync, the mobile version, doesn't require a G-Sync module to function, and Freesync just needs a scaler capable of running displayport 1.2a."
hardware,3b9j1t,Seclorum,6,Sun Jun 28 01:30:24 2015 UTC,The benq xl2730z has a VRR window of 40-144hz and can use overdrive and freesync  :) it's just the Asus mg279q that has that stupid limitation... Asus lost themselves a sale with me...
hardware,3b9j1t,captainvera,1 point,Sat Jun 27 16:35:45 2015 UTC,On the tftcentral review it says that Asus said it was a limitation of the panel.
hardware,3b9j1t,ICanHazTehCookie,2,Sun Jun 28 02:13:45 2015 UTC,"It was but not entirely. I think they said Asus had to decide between something like a 60-144 Hz window and a 35-90 Hz window. In the end, ithink they chose right considering that freesync's entire purpose is to make gameplay feel smooth at sub-60 FPS. Of course that doesnt mean it wont benefit anyone above 60."
hardware,3b9j1t,aranurea,1 point,Sun Jun 28 16:15:47 2015 UTC,Yeah that sounds similar to what I read. I agree that they made the right choice.
hardware,3b9j1t,ICanHazTehCookie,11,Sun Jun 28 18:29:37 2015 UTC,"AMD doesn't have direct control on how FreeSync is implemented, opposed to nVidia's G-Sync - in the end, it may never be a ""thing""; VRR window on some of the models is just plain idiocy    take for example Acer's XG270HU  I heard a rumor that Acer is outright refusing to recall this model, even from unsold stock, to fix the overdrive issue via a FW update - they expect buyers to ship, on their expense, the monitor to Acer  this would never happen on G-Sync side, nVidia would've slapped them silly; this is happening because AMD doesn't have direct control on what and how manufacturers use FreeSync"
hardware,3b9j1t,zmeul,2,Sat Jun 27 02:29:23 2015 UTC,plain* idiocy
hardware,3b9j1t,maybachsonbachs,6,Sat Jun 27 22:51:27 2015 UTC,Good. Gpu makers should not control Monitor makers.
hardware,3b9j1t,Teethpasta,2,Sat Jun 27 18:46:19 2015 UTC,Free Market !!!!!  e z p z. Votes as always only count when the wallet is involved.  Ty capitalisim.   Guess i won't be buying acer ;-).   Enjoy that acer less sales for you !
hardware,3b9j1t,PJSaltz,-18,Sat Jun 27 07:03:18 2015 UTC,"Words present in your post indicate that this might be a question or a request for help. If you review our rules you will notice that most questions don't belong here. This includes PC building questions and requests for tech support. If this is the case, please delete this post and resubmit it to either /r/buildapc or /r/techsupport. If this isn't the case, feel free to ignore this post.  I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns."
hardware,3b7xq5,spikey341,35,Fri Jun 26 18:15:21 2015 UTC,"We dont know.   The 980ti OC's twice as much at least, than the Fury does without Voltage tweaking.   A lot depends on just how stable the Fiji core is with extra voltage thrown at it, because it's easily got enough cooling and power being delivered to the card to support it, but until they unlock the voltage we really wont know just how far it can be pushed."
hardware,3b7xq5,Seclorum,-14,Fri Jun 26 19:02:31 2015 UTC,You can still add a fan to the radiator and get a 10% to 15% decrease in temps
hardware,3b7xq5,Batrster,12,Fri Jun 26 20:48:26 2015 UTC,On a thin rad like that going to push pull is going to make very little difference. You are talking 2-3c so closer to 5%
hardware,3b7xq5,Cozmo85,-9,Fri Jun 26 21:02:18 2015 UTC,No but when the air is not flowing correctly the hot air stays inside heating the components a lot
hardware,3b7xq5,Batrster,5,Fri Jun 26 21:08:08 2015 UTC,A thin rad doesn't need a ton of pressure to effectively pass air through
hardware,3b7xq5,Cozmo85,4,Sat Jun 27 01:49:20 2015 UTC,"The air is not staying inside. This radiayor is thin enpugh that the high static pressure fan that it comes equipped with is more than enough.   If you had a thicker radiator, then you wpuld have a point.   Push pull doesn't affect all radiators."
hardware,3b7xq5,JarJarBanksy,3,Sat Jun 27 06:31:57 2015 UTC,"It's a thin rad. You don't need a push pull setup on that. Hell, the only time you would really need a push pull setup is if you're getting an Alphacool Monsta rad. It's pretty useless on anything else.  Unless you want that ~1 degree difference. If that was the case, I would just use one of those liquid metal thermal compounds."
hardware,3b7xq5,Boy1998,3,Sat Jun 27 01:03:53 2015 UTC,"Temps aren't the problem though - AMD clocked the core high in an attempt to catch the 980ti. It's yet to be seen if extra voltage will help but as of right now, the limiting factor is something that can't be changed - the silicone."
hardware,3b7xq5,thejshep,6,Fri Jun 26 23:31:47 2015 UTC,"Yeah, we can't change the fury x's silicone implants."
hardware,3b7xq5,JarJarBanksy,3,Sat Jun 27 06:33:06 2015 UTC,"I want my next card with DD grade silicone, the next new gamer spec. :)"
hardware,3b7xq5,Seclorum,1 point,Sun Jun 28 01:46:03 2015 UTC,"But I can, and I will. For science ;) 😘"
hardware,3b7xq5,AssCrackBanditHunter,1 point,Sat Jun 27 12:06:03 2015 UTC,"With the fan and rad it's got now its more a problem of ambient temps.   It doesn't look like the rad has that many fins per inch, making it not terribly restrictive to airflow in the first place."
hardware,3b7xq5,Seclorum,18,Fri Jun 26 21:15:54 2015 UTC,"Maybe, but probably not it's main competition is the 980ti which overclocks extremely well without voltage. In other words 980ti stock voltage oced>furyx stock voltage overclocked. So the only reasonable prediction is that it will still be worse than the 980ti even when overclocked with voltage increases."
hardware,3b7xq5,Swaggerlilyjohnson,20,Fri Jun 26 18:42:39 2015 UTC,Holy run on sentences batman.
hardware,3b7xq5,Anally_Distressed,12,Fri Jun 26 18:50:25 2015 UTC,Hopefully your comment isn't pulled for having poor performance on PC-based internet machines.
hardware,3b7xq5,regack,1 point,Fri Jun 26 19:45:21 2015 UTC,What do you mean I should turn my settings down? I meet the minimum requirements!
hardware,3b7xq5,Tonkarz,3,Sat Jun 27 17:37:24 2015 UTC,"That's not necessarily a good indicator because the relationship between stock voltage and stock clocks isn't going to be the same for every card.  Sometimes you've got just enough voltage to be stable, sometimes you've got a big buffer area."
hardware,3b7xq5,tarheel91,2,Fri Jun 26 22:52:42 2015 UTC,Yeah but there is quite a large difference right now so even if Fiji scales much better with voltage I still would bet money on furyx losing
hardware,3b7xq5,Swaggerlilyjohnson,3,Fri Jun 26 23:18:37 2015 UTC,"If it tanks voltage as well as the fx series, it might be a startlingly good over clocker. Right now I've got a poorly binned 8320e and I was able to raise the clock speed from 3.3ghz to 4.4ghz. That's a 33% increase on a shitty chip. If the furys are as well produced as we're told, who knows what they can do"
hardware,3b7xq5,AssCrackBanditHunter,3,Sat Jun 27 12:08:12 2015 UTC,"Will be surprised if people get it much past 1250 stable considering other GCN 28nm cards. If the AIO manages to get people into the 1300-1350 territory consistently that would be a decent ""dream""."
hardware,3b7xq5,TBradley,10,Sat Jun 27 12:55:08 2015 UTC,Considering they shipped a chip with an overkill cooler but locked the voltage I wouldn't get your hopes up.  Treat it as a nice surprise if it turns out they just needed a little more time to get the software/bios ready.
hardware,3b7xq5,OftenSarcastic,32,Fri Jun 26 20:10:32 2015 UTC,"It's not locked from AMD, it's locked because there is no update to afterburner/etc. to work with the board voltage controller.   It's not AMDs job to provide that support. The Titan X had the same issue before it was unlocked. The 980ti didn't because it uses the same voltage controller as the Titan X.   Now, if the BIOS is limited, that's AMDs fault, but until the controller is supported it's a 3rd party issue. It is the same 290x controller, but the implementation was most likely changed since it's a new board meaning update.  http://forums.guru3d.com/showpost.php?p=5106990&postcount=44"
hardware,3b7xq5,TehRoot,3,Fri Jun 26 21:21:17 2015 UTC,Oh. That's what I get for not paying enough attention to GPU overclocking. I thought there was an actual issue with locked voltage.
hardware,3b7xq5,OftenSarcastic,11,Fri Jun 26 21:28:14 2015 UTC,"Nope. As of right now, we are waiting for 3rd party support to be updated.   Unless the card has a lock on the BIOS, but at this point I'd call it unlikely, I've only seen a handful of AMD cards have voltage locks and almost all of them were 3rd party AIBs like the DD 7970, but those can be easily removed with a flash, which I'll do regardless to remove powertune."
hardware,3b7xq5,TehRoot,1 point,Fri Jun 26 21:31:32 2015 UTC,"Can you expand on ""powertune""? What exactly is it and why do you want to remove it from the Fury X?"
hardware,3b7xq5,JimmyCem,1 point,Fri Jun 26 23:53:33 2015 UTC,"https://en.wikipedia.org/wiki/AMD_PowerTune  it's annoying to me as an overclocker. I hate boost clocks, they tend to lower my minimums when I'm overclocking because the load shifts slightly. I don't care about power efficiency honestly so I remove it with a BIOS flash.   For gaming it's maybe not as noticeable, but if the load drops and the clock drops, then load increases and the clocks don't increase fast enough, you can get stuttering, so disabling/removing it can improve the gaming experience as well."
hardware,3b7xq5,TehRoot,1 point,Fri Jun 26 23:57:20 2015 UTC,"Can't they just overclock through catalyst control center? Back when I owned a 270x I could tweak power, voltage, and fan curve via catalyst"
hardware,3b7xq5,AssCrackBanditHunter,1 point,Sat Jun 27 12:09:23 2015 UTC,I'm assuming voltage is the one area AMD won't provide CCC-based tweaking for due to the massive number of people who would immediately kill their cards out of ignorance and then have semi-solid legal ground to say it was AMD's fault for including the funciton?
hardware,3b7xq5,Pizza_Nova_Prime_69,2,Sun Jun 28 16:47:42 2015 UTC,"There's a giant disclaimer absolving AMD of any claims, but overclocking is far less likely to kill your card then overvolting is, and it's basically impossible to prove if you did it or not.   nVidia doesn't even provide any overclocking tools so...meh"
hardware,3b7xq5,TehRoot,6,Sun Jun 28 16:50:16 2015 UTC,None. It's reaching +100 celcius on VRM even with stock voltage. DOA for OC'ers
hardware,3b7xq5,parasemic,4,Fri Jun 26 19:40:24 2015 UTC,Guess what the Titan X's or 980Ti's VRMs hit at stock? ~100C.  Doesnt stop them from achieving some nice OC.
hardware,3b7xq5,HavocInferno,6,Sat Jun 27 15:23:26 2015 UTC,"its hitting  100C on water?  Got a source,  thats nuts."
hardware,3b7xq5,Juts,4,Fri Jun 26 20:16:57 2015 UTC,"http://translate.google.com.au/translate?hl=en&sl=de&u=http://www.pcgameshardware.de/AMD-Radeon-Grafikkarte-255597/Tests/Radeon-R9-Fury-X-Test-1162693/  ""This value was measured with our proven savegame in Risen 3, which serves as the starting point for our FPS measurements""  Less demanding games?"
hardware,3b7xq5,bentan77,2,Fri Jun 26 21:06:23 2015 UTC,Damn. Thats going to come back to bite them.
hardware,3b7xq5,Juts,12,Fri Jun 26 21:17:14 2015 UTC,No it won't. The Tmax on the VRMs is 135C. 100C is fairly average.
hardware,3b7xq5,TehRoot,1 point,Fri Jun 26 21:18:38 2015 UTC,So they wont fail but it could hurt overclocking.
hardware,3b7xq5,Cozmo85,7,Fri Jun 26 21:49:42 2015 UTC,"Doubtful. I'm putting mine under a full cover block anyway when I get it. I see your point, but powertune would prevent the VRMs from killing themselves, it's still unlikely anyone casually overclocking would hit 135C, and the VRMs seem to be around 90C anyway when they're in 99% of games."
hardware,3b7xq5,TehRoot,3,Fri Jun 26 21:52:16 2015 UTC,"Fury runs the same VRM chips as Hawaii, AFAIK.   I've run my 290x vrms crazy hot. Coming out of a game after 2 hours in BF4 and GPUZ showing a 127c max gave me a lot of faith in AMD's reference mosfets (and made me drop my OC completely)   It always scares the hell of out me from instinct, but I just got a 1440p display, so I'm considering just saying fuck it and nuking my card just to see how hard I can push it and how long it will last.   But I'm actually kind of afraid that it will soldier on for years, defying my harsh persecution and never giving me an excuse to buy Fury. Now that is a good problem to have."
hardware,3b7xq5,chapstickbomber,4,Fri Jun 26 23:57:02 2015 UTC,My 290x before I got rid of it would pull 420W at load with about 138C load on the VRMs. Never gave me any problems.
hardware,3b7xq5,TehRoot,6,Fri Jun 26 23:59:46 2015 UTC,"Holy hell.  400W Master Race. I'm joining right now.  AMD should really let people know about the technical capacity of their hardware. I mean, I only just discovered how hardcore and top end they build their PCB's. 12 layer. 150C+ 70A MOSFETS. I mean, HOLY HELL"
hardware,3b7xq5,chapstickbomber,1 point,Sat Jun 27 04:36:20 2015 UTC,"I thought the sapphire 290 tri-x had great VRM cooling, but i OC'ed 50 mV to ~1.1V and 1125 MHz. Furmark had the VRMs at 128C, which was at the point the card started throttling.  I immediately exited furmark and was like ""nope never again"""
hardware,3b7xq5,TheImmortalLS,5,Sat Jun 27 04:44:40 2015 UTC,"And by ""nope never again"", I assume you mean you dropped the voltage and clock very slightly because it was hitching your frame times, tested it there, and you are running there now 24/7.  Things throttle because manufacturers don't want to get sued. Grandma's Pentium 4 machine from 2003 has been thermally throttling since 2008, once the airflow became almost completely obstructed by cat hair. But her machine still runs now and she just sent you a comment on Facebook about how you need to comb your hair better.   Run chips at their absolute physical limits to your sound and heat limits for your body in the chair until the chip detonates or becomes comically obsolete.   Anything else is pure cowardice."
hardware,3b7xq5,chapstickbomber,1 point,Sat Jun 27 09:27:10 2015 UTC,"I could only put my cards through torture with the Arctic Accelero Extreme IV.   Though I could only hit 1210 core on my 290x, but that was on air."
hardware,3b7xq5,TehRoot,1 point,Sat Jun 27 04:48:12 2015 UTC,Hold out to next gen my friend. Stay strong with me. Soon amd and Nvidia will have 16nm cards with hbm 2.0. Soldier on!
hardware,3b7xq5,AssCrackBanditHunter,1 point,Sat Jun 27 12:10:55 2015 UTC,"I did a pretty hard run on my 290X last night, methodical with a spreadsheet and all (at least until several drinks into this session).   Best stable run was 1125/1600 +200mV.  I'll tell you h-whut, that damn core acts up like a toddler when it goes over 65C, artifacting. The two H100 fans on its 120 rad are so comically loud at max though that I would never actually run that for gaming.   Next run I'm flashing the PT1 bios and nuking it. See how hard the board will go before throttling at an acceptable noise level.   1440p Freesync is in the mail so I honestly don't need Fury for just one of them, since VRR is visually equivalent to ~30% GPU upgrade, which is about what Fury is, so whateva. Nuke OC on a 290X should be fine ( ͡o ͜ʖ ͡o) /u/buildzoid"
hardware,3b7xq5,chapstickbomber,1 point,Sat Jun 27 15:01:19 2015 UTC,"Jesus Christ, I read that the Vapor-X cooled the VRMs well but didn't realize the gap was so big.  My 290 Vapor-X VRMs cap out at 85C and 68C at 1200MHz, which is obviously a big OC at maximum voltage. At stock it's significantly cooler. I would shit my pants they were above 100C."
hardware,3b7xq5,Liam2349,1 point,Sun Jun 28 03:02:50 2015 UTC,"Why? The reference MOSFETs are rated for 70 amps each at 125C, and there are six of them. I used to worry about 100+ VRM temps on my ref 290X until I actually read the data sheet for the chips.  AMD apparently doesn't fuck around with its PCB or board components."
hardware,3b7xq5,chapstickbomber,1 point,Sun Jun 28 05:21:18 2015 UTC,"TitanX and 980ti both push similar heat, at stock voltages on their VRM's."
hardware,3b7xq5,Seclorum,1 point,Sun Jun 28 01:42:31 2015 UTC,It needs a fan to push the radiant heat. I wound be surprised if popping off the lid knocks it down by 10c.
hardware,3b7xq5,fzed1199,3,Sat Jun 27 11:56:41 2015 UTC,The lid was removed for those tests. This is how it looks like sealed up.
hardware,3b7xq5,bentan77,1 point,Sat Jun 27 12:02:49 2015 UTC,It has a flattened copper pipe directly touching them. The pipe is just before the inlet to the main block itself.
hardware,3b7xq5,Seclorum,3,Sun Jun 28 01:41:30 2015 UTC,there is no difference between a block covering the VRM's and a flattened pipe that covers their full width.
hardware,3b7xq5,cheekynakedoompaloom,-1,Fri Jun 26 21:03:02 2015 UTC,Actually there is. Look at any full cover block.
hardware,3b7xq5,DarkStarrFOFF,6,Fri Jun 26 21:31:26 2015 UTC,"fluid flow volume+conduction rate of material+contact area is all that matters(fluid choice, intake temperature etc do matter but thats outside the scope of pipe vs block).  if you get a high heat conduction piece of material fully touching the vrms and get enough fluid flow over that material it does not matter how much bigger the chunk of metal is that is not touching. and economically and physically it's really hard to beat laminar fluid flow through a flattened copper pipe that covers the full width of the object to be cooled."
hardware,3b7xq5,cheekynakedoompaloom,2,Fri Jun 26 22:06:12 2015 UTC,Yup.  It's likely that the VRM's are just releasing a higher W/mm2 which means a higher temperature to balance out the heat transfer equation.
hardware,3b7xq5,tarheel91,-5,Fri Jun 26 22:17:39 2015 UTC,"VRMs are, which aren't cooled at all. Well, in fact they're apparently heated by the beyond-retarded copper pipe thing that runs over them on the way out (bringing heat)  Source, edit:  ""The story changes during our stress test. The water-cooling rule of thumb comes to mind right away: use one centimeter of radiator length per 10W of power. Almost 90 °C at the motherboard slot indicates that the VRM pins have passed 100 °C. This certainly isn’t a great way to run the card long-term, but then again, stress tests aren’t an everyday usage scenario. Still, it would have been nice to see some reserves for overclocking.""  http://www.tomshardware.com/reviews/amd-radeon-r9-fury-x,4196-8.html"
hardware,3b7xq5,parasemic,4,Fri Jun 26 22:50:59 2015 UTC,"The Copper pipe is the INLET to the block not the outlet. The VRM's thermal limit is 130c, 100c is normal for them."
hardware,3b7xq5,Seclorum,1 point,Fri Jun 26 20:17:57 2015 UTC,"Who told you that, if its your personal experience you must have a problem inside your case."
hardware,3b7xq5,Batrster,2,Sat Jun 27 06:28:59 2015 UTC,someone linked sources above. Looks like they just didnt cool them.
hardware,3b7xq5,Juts,2,Fri Jun 26 20:50:45 2015 UTC,They have a copper pipe directly touching them all before feeding into the block. It's even squared off to get better contact.
hardware,3b7xq5,Seclorum,1 point,Fri Jun 26 21:19:26 2015 UTC,"Multiple reviews. Do you even read them? http://www.tomshardware.com/reviews/amd-radeon-r9-fury-x,4196-8.html"
hardware,3b7xq5,parasemic,-15,Sat Jun 27 06:25:51 2015 UTC,Because OCers totally use stock cooling and not an aftermarket cooler/waterblock or LN2 pots. Obviously depending on how far they plan on pushing it.
hardware,3b7xq5,DarkStarrFOFF,7,Fri Jun 26 20:56:27 2015 UTC,Nobody games with LN2 pots.
hardware,3b7xq5,Cozmo85,-8,Fri Jun 26 19:44:10 2015 UTC,"I didn't say they did. Reading in to my comment much? He said OCers. Professional overclockers run LN2, OP didn't exactly specify whether he was talking about normal end users or what. He said overclockers.  TL:DR Reading comprehension, do you have it?"
hardware,3b7xq5,DarkStarrFOFF,5,Fri Jun 26 21:02:57 2015 UTC,Man you're an asshole.
hardware,3b7xq5,melgibson666,8,Fri Jun 26 22:03:07 2015 UTC,"Still, youre already paying 100e premium for the water cooling so changing it to a proper one will make it very incompetitive vs. 980Ti unless the gains are insane"
hardware,3b7xq5,parasemic,-5,Sat Jun 27 01:04:01 2015 UTC,"Not really unless there is a Fury X model with the same specs but without water cooling. As far as gains, definitely depends. (Real) Water cooling can allow for some pretty high clocks when VRMs and Core max at ~50c. My 7970 is running 1200 Core."
hardware,3b7xq5,DarkStarrFOFF,1 point,Fri Jun 26 19:54:08 2015 UTC,"I think the dream comment was from the person interviewing Lisa, in response to her saying the cooler can dissipate 500W and there's 2 8 pin headers and only a 275 TDP on stock.   But you have to remember, AMD was really counting on a die shrink for this generation, but their chip company didn't deliver. All that memory bandwidth needs lots of shaders to take advantage of it, and they managed to pack 50% more than the 290X on the same fab process. That's a lot of shader cores in there, a lot more than the fab process or the Fiji architecture was designed to handle. This leads to instability, meaning less overclocking. Also generates a lot of heat, probably the reason for mandatory water cooling (with the air cooled version having locked off cores).   I don't know enough to tell you how they managed to keep the 290X TDP while increasing the core count by 50%, but it may have something to do with needing 2 8 pin power headers. Bottom line is the overclocking capabilities may be lacking because of the wizardry they pulled to fit all those cores."
hardware,3b7xq5,pb7280,1 point,Fri Jun 26 20:54:49 2015 UTC,"Currently the OC potential is limited because the various OC programs currently dont have updates that let them manipulate the voltage regulators on the card.   It's not that AMD locked it, but that the circuitry they used is new and software devs need to update their programs to interface."
hardware,3b7xq5,Seclorum,1 point,Sun Jun 28 00:06:10 2015 UTC,"I know this but even at stock voltage it OCs less than the 980 Ti. Voltage and heat don't mean everything, there's a certain part the card just won't go any higher because it's inherently unstable.   No doubt it'll be higher once voltage manipulation is available, but I wouldn't say it's the ticket to making it perform as well as everyone thought it would. Next generation should be better, the fab process is almost halving I believe, Fiji was designed for 20nm but they're skipping that and going straight to I believe 16 (current/past gen is 28nm)."
hardware,3b7xq5,pb7280,1 point,Sun Jun 28 01:33:47 2015 UTC,Fiji is 28nm...  That's why the core is over 600mm wide.
hardware,3b7xq5,Seclorum,1 point,Sun Jun 28 08:45:38 2015 UTC,"That's my whole point, they designed the architecture for I think 20nm it is, but their chip company couldn't get that ready in time so they had to revert to 28nm for release."
hardware,3b7xq5,pb7280,1 point,Sun Jun 28 16:46:00 2015 UTC,Pretty sure its been confirmed its the same voltage regulator as the 290x.
hardware,3b7xq5,hdshatter,1 point,Sun Jun 28 22:40:49 2015 UTC,It was their Chief Gaming Scientist who said it.
hardware,3b7xq5,sev87,1 point,Sun Jun 28 22:25:45 2015 UTC,"Hmm that's what I heard the first time I heard that quote, but I haven't seen the interview myself so you could be right."
hardware,3b7xq5,pb7280,1 point,Sun Jun 28 16:48:48 2015 UTC,"Unlikely. AMD are playing catch-up here. It would be insane to release the card with anything less than the highest stable clockspeeds, especially considering that the card's nearest rival, the 980ti, beats it in most benchmarks."
hardware,3b7xq5,brizk1984,1 point,Sun Jun 28 22:41:36 2015 UTC,"I have been trying to overclock the fury x for the past few days, not much luck... max I got to manage running is 6.5% increase (1118mhz)  But noticed that if the application or games manages to push the heat over 60 degrees celsius, artifacts will eventually show up on screen.  I can't imagine voltage tweaking will help much if you are comparing to Nvidia's GTX 980 ti, which seems to be already overclocking an extra 300-400mhz by itself via GPU boost.  I think the card may already be close to max speed at the moment even on stock :P  But only time will tell... hopefully AMD will surprise us by a driver update that overturns everything I see.  I am too lazy to switch back to my EVGA Titan X SC, took me 2 hours to figure out how to fit the extra radiator into my case with another cpu water cooler already there lol"
hardware,3b7xq5,everyZig,1 point,Sun Jun 28 12:57:49 2015 UTC,"But noticed that if the application or games manages to push the heat over 60 degrees celsius, artifacts will eventually show up on screen.   Wow, that doesnt look too good, and doesnt bode too well for the aircooled fury, perhaps the HBM isnt that temperature resistant?"
hardware,3b7xq5,brizk1984,1 point,Sat Jun 27 07:40:27 2015 UTC,Lets hope is just something they can tweak with a driver / bios update...   But if u keep the card at stock everything is butter smooth for day to day gaming :)
hardware,3b7xq5,Reapexx,1 point,Sat Jun 27 15:55:05 2015 UTC,"We gotta wait until people unlock the voltage and maybe tweak other things.  As it stands, the Fury X doesn't seem to have very much headroom at all."
hardware,3b7xq5,Hanopp,0,Sat Jun 27 20:45:58 2015 UTC,Short Answer: Slim to none.
hardware,3b7xq5,headband,-11,Sat Jun 27 00:50:40 2015 UTC,Slim to none.  When you have to watercool something just to meet spec thats not really a sign of having much margin.
hardware,3b7xq5,Darkstryke,-6,Sat Jun 27 16:06:03 2015 UTC,1 in 1000
hardware,3b7ih8,skunkboy72,15,Fri Jun 26 16:26:32 2015 UTC,"Just as additional information, it doesn't tell the whole story. Granted it's a pretty good way to compare at a glance, but those are the times to first word only.  So for instance   DDR3 2400 c11, first word is 9.17ns.   The 8th word is (cas*2+7)/freq = 12.08ns.  DDR3 1866 c8, first word is 8.58ms but  the 8th word is 12.33ns.  As I said though, the chart is a pretty good way to make an academic guess."
hardware,3b7ih8,Anergos,3,Fri Jun 26 17:32:01 2015 UTC,"What do you mean by ""word""?  Does that have to do with the timings, like when they advertise ""9-9-9-24"" or something?  I know that the first one is the CAS latency, but are they all referred to as ""words""?"
hardware,3b7ih8,LimEJET,14,Fri Jun 26 18:30:38 2015 UTC,"A word is how much data the CPU reads per ""read"". For DIMM modules that's 64 bits. There's a handy chart of delays for successive read delays here.  For reference, The latency chart (9-9-9-24) is read as (all delays are in number of clock cycles):   Delay between signal getting to the memory controller, and the controller starting to respond. Delay between the MC powering a column of memory and powering a row. Time it takes to power down one data line and power up another (effectively, the time it takes until the memory is able to take a new request after doing an operation). Total time from MC to currently requested word being available (on the MC, not at the CPU).   At least, that's how I understand this article."
hardware,3b7ih8,freedompower,6,Fri Jun 26 18:46:10 2015 UTC,If you wonder what CAS is: https://en.wikipedia.org/wiki/CAS_latency
hardware,3b7ih8,KamiCrit,5,Fri Jun 26 17:17:43 2015 UTC,"In day to day casual computing, how dose faster RAM speed help the consumer/gamer?"
hardware,3b7ih8,Blubbey,13,Fri Jun 26 18:23:17 2015 UTC,Only if you're gaming on an APU will you notice any meaningful gains with faster memory iirc.
hardware,3b7ih8,Smagjus,2,Fri Jun 26 19:47:35 2015 UTC,The difference is minor but not completely negligible as the other posters said. The highest I did measure was 8% more FPS with 2400MHz against 1600MHz which was in Planetside 2.
hardware,3b7ih8,winnix,2,Sat Jun 27 09:36:54 2015 UTC,"I may be outdated here, but I recall that for APUs 2400 MHz is the plateau point."
hardware,3b7ih8,Teethpasta,2,Sat Jun 27 11:59:52 2015 UTC,Not at all
hardware,3b7ih8,Kubi74,0,Fri Jun 26 18:49:40 2015 UTC,2400 cl10 FOR LIFE!  Well at least until ddr4 gets better/cheaper :D
hardware,3b7ih8,putin_vor,5,Sun Jun 28 23:01:15 2015 UTC,"Shouldn't faster be green and slower red?  Also use PNG, not JPG for charts."
hardware,3b7ih8,y801702,-6,Sat Jun 27 10:42:10 2015 UTC,"No, the colors are there to help my brain distinguish the difference between the values.  The colors were chosen because they are high in contrast.  Why does it matter what format the image is in?  The information in the chart isn't going to change because it's in .png or .jpg."
hardware,3b7ih8,itskisper,4,Sat Jun 27 12:13:32 2015 UTC,"JPG works better for photos, and PNG for images with plain colours. JPG is a lossy format, and that means it is not going to show the original image but an approximation, better as quality (and size) increases. For images like this, with large regions with the exact same colour, PNG usually compresses better and shows exactly the original image, thus it is easier to read it. If you look closely the text in your image you can spot the compression artifacts typical of JPG when contrast is too high (i.e. between black text and light background)."
hardware,3b7ih8,Scuttlebutt91,-2,Sat Jun 27 22:02:15 2015 UTC,Will it being in .jpg vs.png change the numbers and letters I typed into excel?
hardware,3b7ih8,oGsBumder,3,Sat Jun 27 23:11:58 2015 UTC,No he's just telling you how to preserve the image quality to prevent ugly compression issues like artifacts. No need to be a little bitch about it mate just offering some advice.
hardware,3b7ih8,Kernoriordan,-1,Sun Jun 28 03:58:07 2015 UTC,"And my point is, the image quality has no effect on the transmission of information this image displays.  It's not a pretty picture. It's not a piece of art where the aesthetics matter.  There is no artistic value in this chart.  Here's the chart in .png and here's the chart in .jpg.  There is practically no difference.  Unless you decide to zoom all the way in or have a giant monitor that you sit inches from, you're not going to see the compression.    The colors are a bit off, but the colors are still different enough in both formats to distinguish between the different shades.  And when I actually compare the .png and the .jpg uploaded to imgur to the original excel file, the .jpg colors are closer on my monitor.  But my guess is that has to do with imgur, cause the .png file on my computer matches the colors in the excel file."
hardware,3b7ih8,Kernoriordan,1 point,Sun Jun 28 12:49:27 2015 UTC,You can literally see the fuzz in the .jpg image
hardware,3b7ih8,Imidazole0,0,Mon Jun 29 13:45:46 2015 UTC,I'm sorry to offend your obviously immaculate vision.
hardware,3b7ih8,con5id3rati0n,0,Mon Jun 29 20:21:55 2015 UTC,So?
hardware,3b7ih8,EERsFan4Life,1 point,Tue Jun 30 23:47:08 2015 UTC,you make it out like you made this but I'm pretty sure this chart is ripped from Anandtech
hardware,3b7ih8,con5id3rati0n,0,Sat Jun 27 18:38:23 2015 UTC,I don't doubt that Anandtech probably has similar charts.  But I made this particular one in Excel.
hardware,3b7ih8,EERsFan4Life,1 point,Sat Jun 27 19:36:10 2015 UTC,Fair enough :) thanks then!
hardware,3b7ih8,Randomoneh,2,Sun Jun 28 09:38:38 2015 UTC,That's pretty good!
hardware,3b7ih8,Qvoovle,2,Fri Jun 26 16:44:20 2015 UTC,Thanks!
hardware,3b7ih8,EERsFan4Life,2,Fri Jun 26 16:47:34 2015 UTC,Does the same general trend here also apply to DDR4?
hardware,3b7ih8,ault92,3,Fri Jun 26 17:32:38 2015 UTC,Yes DDR4 is still Double Data Rate DRAM. It's just the new format.
hardware,3b7ih8,autowikibot,3,Fri Jun 26 17:56:18 2015 UTC,"Out of curiosity, how come on average the CAS latency for DDR4 is so much higher than DDR3?  Just a simple DDR4 search on newegg, for example, shows most of them at around 15 or 16."
hardware,3b7ih8,crash250f,3,Fri Jun 26 18:00:42 2015 UTC,"The CAS Latency is a higher number of cycles, but the modules also operate at much higher frequencies. The actual timed latency is roughly the same for both."
hardware,3b7ih8,jetiger,1 point,Fri Jun 26 18:08:04 2015 UTC,So what's so great about DDR4?
hardware,3b7ih8,americancer,2,Sat Jun 27 02:00:22 2015 UTC,"Lower voltage, higher density."
hardware,3b7ih8,ChiFu360,2,Sat Jun 27 03:40:38 2015 UTC,Also higher theoretical bandwidth.
hardware,3b7ih8,JeffroGymnast,2,Sat Jun 27 10:27:08 2015 UTC,"Each DDR generation does this. We are increasing the speed by doubling the size of the prefetch buffer each time. So move from DDR, to ddr2, to ddr3,to ddr4, each time you roughly double the frequencies and the latencies.  This is because ram itself doesn't actually get any faster, you're just reading more of it at a time internally to the chip.  I guess its a bit like raid0.  DDR4 dimms are 64 bits wide, and say, 3000mhz, used in quad channel for a 256bit memory interface. But inside the silicon chip itself, you will see much wider, lower frequency interconnects.  This is oversimplified and not accurate, but pretend we have a 3000mhz dimm with a latency of 8 cycles. That's because while its a 64 bit wide device, inside its 512 bits wide, and runs at 375mhz. That's the same data rate so the bandwidth is the same, but if I give a command in clock cycle #1, and get a response back the next cycle, 8 cycles have gone past outside the chip - hence latency."
hardware,3b7ih8,Kernoriordan,2,Fri Jun 26 22:28:02 2015 UTC,"I assume the equation CAS/(Frequency/2) * 1000 = speed still applies, but I'm not 100%.  I got it from the wikipedia DDR3 page."
hardware,3b7ih8,iluvkfc,1 point,Fri Jun 26 17:59:24 2015 UTC,"DDR3 SDRAM:       In computing, DDR3 SDRAM, an abbreviation for double data rate type three synchronous dynamic random-access memory, is a modern type of dynamic random-access memory (DRAM) with a high bandwidth (""double data rate"") interface, and has been in use since 2007. It is the higher-speed successor to DDR and DDR2 and predecessor to DDR4 synchronous dynamic random-access memory (SDRAM) chips. DDR3 SDRAM is neither forward nor backward compatible with any earlier type of random-access memory (RAM) because of different signaling voltages, timings, and other factors.    Image from article i     Relevant: Socket C32 | GDDR3 | Mac Mini | Comparison of Sony Vaio laptops   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3b7ih8,Reapexx,1 point,Fri Jun 26 18:00:10 2015 UTC,"Question.  I haven't dedicated much time to learning about hardware these last few years but from what I remember, higher speed memory didn't have too significant of an effect on effect on overall performance.  This is backed up in my mind by seeing pretty much the cheapest possible RAM recommended to most system builders.  I think I remember there being an exception for OCing but that was at least 5 years back and I'm not sure if it still works that way.    This also somewhat conflicts in my mind with the ideas that there's a huge bottleneck in a system between the memory and CPU, which is the whole reason CPU designers spend so much effort these days prefetching and branch predicting and all that."
hardware,3b7ih8,bphase,2,Fri Jun 26 18:09:25 2015 UTC,"Yeah, having faster ram does not do much to improve general everyday tasks. I'm trying to remember back to my college classes here, so I might be very wrong on some of this stuff. Someone correct me if I'm wrong.   Let's say you have 10 percent faster ram. This doesn't mean that all of your memory accesses are 10 percent faster - only the accesses that go out to your ram. Many (most?) of your memory accesses will be cached on the cpu chip. The ram is only used when the cached memory does not have the data needed, so it has to go out and get that data from ram. This is actually a huge hit in performance. I'm sure you have heard that going out to a hard drive takes a very long time. Well, going to ram is like the same thing when compared to cache on the cpu.   My professor used an analogy. Let's say you are working on a broken pipe in your basement and you need a certain tool, sad a hammer. First, you check your toolbelt. If the hammer is there, you pull it out and use it. This is like L1 cache. If it is not there, you go to your toolbox,which is like L2 cache. If it's not there either, then you have to go to RAM. This is like getting into your car and driving to the local hardware store. If it's not there either, then you have to go to the hard drive. This is like going to Alaska from the continental United States. On foot.   So why is branch prediction important? Well imagine if you could cut out as many trips to the hardware store as possible. Every trip you save will save you a lot of time. Even if it is fairly fast ram, it is still very slow compared to cache."
hardware,3b7ih8,Mr_s3rius,1 point,Fri Jun 26 19:12:45 2015 UTC,"It depends, like literally any computing scenario, on what you're trying to do.  If you care a lot about the highest possible framerate your system can achieve in Quake III: Arena, for example, work on your memory bandwidth, because that's almost definitely going to help.  smile This is about the only scenario I can come up with off the top of my head, but I'm sure some applications would benefit greatly!  For your needs, I bet the basic stuff is just fine: if you don't know if you need it, you probably don't need it.  This is always a good rule to follow unless (you really trust the advice of a close and intelligent friend and they are telling you that) your specific scenario calls for a change.  I really care, for example, about the feel of responsiveness in my system; I also like to have way too much RAM so microstutters from disk reads are eliminated, even when running several games and VMs simultaneously.  As a result, I have stupid amounts of fast RAM.  When I bought my RAM back when it was very low priced, the DDR3-1866 CAS 9 stuff was cheaper than DDR3-1600 CAS 10; the decision was easy.  I always go for the best deal for the money that I can unless there are documented (reviews) reliability problems with the chips.  I got my 32GB of RAM for about $180 during that sale and am still feeling happy with the purchase.  Personally, I do feel that most people would fail to notice the difference between speeds unless they do a lot of high-fps first-person gaming - and even then it's only a small change in overall system latency, from what I understand.  But when you care about total slowdown across the whole system, it's kind of like adding a crossbar that's five pounds lighter in a car - every little fucking bit of performance you can edge out helps to reduce total input-to-output latency, increasing feedback between user and computer.  IF I COULD, I WOULD LIVE ON A MOTHERBOARD."
hardware,3b7ih8,WTFppl,2,Fri Jun 26 19:17:23 2015 UTC,That last sentence there really touched me <3
hardware,3b7ih8,Mr_s3rius,0,Fri Jun 26 21:07:36 2015 UTC,"32GB fir $180? Jesus, that's like stealing it."
hardware,3b7ih8,DocMcNinja,1 point,Fri Jun 26 21:00:17 2015 UTC,"Not to nitpick, but I'd like to point out that it should be written ""CAS/(equivalent Data Rate in MHz/2) * 1000  By that, I mean that the actual frequency DDR3 memory runs at is half of its advertised equivalent speed. I get that you understand that because you included the division by 2, but I think you are confusing frequency with data rate.   EX: ""1600 MHz"" ram runs at 800 MHz frequency, for an equivalent of 1600 MT/s (million transfers per second)  Hope this helps."
hardware,3b7ih8,WTFppl,1 point,Fri Jun 26 21:03:45 2015 UTC,More testing for RAM speeds in gaming and how the perform - http://www.anandtech.com/show/7364/memory-scaling-on-haswell/7
hardware,3b7ih8,nonameowns,1 point,Sat Jun 27 18:40:34 2015 UTC,"I know ""red is faster, blue is cooler etc"" but I really think in this case green should indicate fastest and red, slowest.  Like these."
hardware,3b7ih8,na1nsxr,1 point,Fri Jun 26 16:31:07 2015 UTC,"I really think in this case green should indicate fastest and red, slowest.   ayy"
hardware,3b7ih8,Imidazole0,0,Fri Jun 26 16:42:04 2015 UTC,lmaooooooo
hardware,3b7ih8,Anally_Distressed,1 point,Fri Jun 26 18:02:47 2015 UTC,"Red = redline, most power, bad efficiency Green = slow, energy efficient, cool."
hardware,3b68n3,zmeul,30,Fri Jun 26 08:39:33 2015 UTC,"The bad news is this is the last turn of the crank for a copper version of the interconnect   Wow, crazy. Guess silicon photonics are really going to have to become a thing."
hardware,3b68n3,III-V,13,Fri Jun 26 11:36:11 2015 UTC,"Signal integrity is going to be so much fun!  Seriously, PCIe 3.x is already insane."
hardware,3b68n3,spiker611,2,Fri Jun 26 13:35:56 2015 UTC,Why'd you write 3.x ? Is there a 3.1 I don't know about?
hardware,3b68n3,luger718,6,Fri Jun 26 19:59:40 2015 UTC,"Yes - https://en.wikipedia.org/wiki/PCI_Express#PCI_Express_3.x  In September 2013, PCI Express 3.1 specification was announced to be released in late 2013 or early 2014, consolidating various improvements to the published PCI Express 3.1 specification in three areas: power management, performance and functionality.[20][34] It was released in November 2014,[35] but as of March 2015 no consumer products using the standard are available for purchase."
hardware,3b68n3,andre2003s,1 point,Fri Jun 26 21:14:42 2015 UTC,Ahhhhhh okay. Intel's cpus have to support 3.1 to even have a motherboard with it right?
hardware,3b68n3,luger718,5,Fri Jun 26 21:45:18 2015 UTC,"Or, maybe with more die shrinks we'll get more pci lanes"
hardware,3b68n3,cuddlefucker,1 point,Fri Jun 26 14:02:52 2015 UTC,Doesn't really work like that.  To get more lanes we need better bumping (denser pin-oits) and better platform controllers.  Intel is unifying some of this communication stuff in the platform after Skylake I think but starting at servers/workstations so it probably won't trickle down for a while.
hardware,3b68n3,andromeduck,4,Tue Jun 30 14:43:35 2015 UTC,"mhmmmm, let's see if this turns out just like lightpeek. remember that guys?"
hardware,3b68n3,Illford,16,Fri Jun 26 21:21:16 2015 UTC,"So either Skylake-E gets pushed back to freaking 2017, or x179 won't come with pcie 4. Good to know."
hardware,3b68n3,p4block,4,Fri Jun 26 11:59:28 2015 UTC,Wasn't there just news/rumors that Broadwell-E was cancelled and Skylake-E will come early 2016? I guess it must come without PCIe 4.0.
hardware,3b68n3,bphase,1 point,Fri Jun 26 14:57:48 2015 UTC,Yeah that's what I heard but they already laid out Broadwell e it's just yields are ass and skylake e dies will be done being laid out by the time they can make monstor dies.
hardware,3b68n3,dylan522p,23,Fri Jun 26 22:42:55 2015 UTC,"For those of you of the opinion that this is boring news, given how well PCI 2.0 still performs in the overwhelming majority of scenarios, consider this: The first improvement you absolutely need to make to support future technology, of any kind, is advancing throughput for communication. You can't do anything else with technology if there doesn't exist a large enough channel for the data to pass through, as evident with SATAIII and the limitations its consequentially imposed on SSDs.  This sort of advancement is not intended to benefit the current end-user, but moreso benefits developers who have a knack for understanding the needs of tomorrow."
hardware,3b68n3,Oafah,5,Fri Jun 26 12:22:04 2015 UTC,"I was going to say, does anything actually saturate PCI 3 bandwidth fully?"
hardware,3b68n3,ctskifreak,17,Fri Jun 26 14:46:25 2015 UTC,"Infiniband, gpu compute, ect.."
hardware,3b68n3,maseck,6,Fri Jun 26 15:38:04 2015 UTC,"That's fair, I was thinking more in terms of personal use exclusively. I completely agree with /u/Oafah to be clear - we should be pushing the boundaries all the time, I just wasn't sure if there was something that would benefit from it at this point in time."
hardware,3b68n3,ctskifreak,1 point,Fri Jun 26 15:49:55 2015 UTC,chicken and the egg
hardware,3b68n3,dasiffy,3,Sat Jun 27 13:00:01 2015 UTC,The animal that laid the egg wasn't a chicken but the egg was a chicken egg. The egg came first.  Hardware is created to take advantage of technology that exists. Increase the bandwidth and hardware is likely to follow. Doesn't mean that all uses for it will take advantage of it but just there will be some hardware will take advantage of it.  It costs money to increase hardware and if you have no performance increases due to connection limitations it would be a waste to push beyond the limits of the connector.
hardware,3b68n3,MINIMAN10000,1 point,Sat Jun 27 15:55:06 2015 UTC,"The animal that laid the egg wasn't a chicken but the egg was a chicken egg   I happen to believe that very thing...... but .... a hamster(female) could have mated with a pigeon(male) and birthed a chicken. Thereby, the chicken could have came first.  The computer was invented before the discovery of electricity. The cell phone was an idea made before the technology existed.  I'm sure there have been instances where hardware was waiting for connector tech to catch up. Look at UHD tv's."
hardware,3b68n3,dasiffy,6,Sat Jun 27 16:13:13 2015 UTC,"The interconnect to the ""southbridge"" is PCIe based, so more bandwith for that would be good (since a lot of things including often PCIe 1x slots ""hang off"" of the southbridge)."
hardware,3b68n3,Dark_Crystal,3,Fri Jun 26 17:41:43 2015 UTC,"It's not about that. Not even multi GPU setups saturate pcie 3.0 x16, but if you only have the silicon space for 16 pcie transcievers, eg haswell, by making them 4.0 you'd get twice the bandwidth out of them at lower latency. Suddenly, a 4 way GPU system on pcie 4.0 x4 per GPU would be acceptable. That Samsung m.2 ssd you have using up 4 lanes only needs 2 for the same performance, etc."
hardware,3b68n3,ault92,1 point,Fri Jun 26 22:35:41 2015 UTC,I bet consumers cpus will drop to 10x of pci 4 while xeons and enthusiast would be actually getting way more bandwidth.
hardware,3b68n3,dylan522p,10,Fri Jun 26 23:40:26 2015 UTC,new connector   Groan
hardware,3b68n3,gnu_bag,13,Fri Jun 26 14:59:00 2015 UTC,sounds like it will be backwards compatible with PCIe 3.0 - it will accept PCIe 3 cards
hardware,3b68n3,Raising,2,Fri Jun 26 15:01:11 2015 UTC,I doubt any current videocard maxes out pcie 2.0 16x
hardware,3b68n3,Liber_Vive,1 point,Sat Jun 27 22:42:27 2015 UTC,But Skylake comes with PCIe 4 and it comes out in September. Can someone explain?  EDIT: only on Skylake-E
hardware,3b68n3,CeJij,6,Fri Jun 26 13:21:06 2015 UTC,It's Skylake-E (not skylake-s) that maybe will get PCIe 4.0 and Skylake-E is at the moment scheduled for 2016 Q3. So like p4block said either Skylake-E will be delayed or won't get PCIe 4.0.
hardware,3b68n3,NLWoody,1 point,Fri Jun 26 14:00:23 2015 UTC,source?  even wikipedia sais 20 lanes (max I guess) PCIe 3.0
hardware,3b68n3,InfiniteZr0,-20,Fri Jun 26 13:35:34 2015 UTC,pcie 2.0 is still fast enough
hardware,3b68n3,NLWoody,25,Fri Jun 26 11:19:09 2015 UTC,"If we said ""it's fast enough"" we'd still be using single core Pentium 2s right now"
hardware,3b68n3,Stingray88,-8,Fri Jun 26 12:26:39 2015 UTC,ehh no? pentium 2s were bottlenecking some opperations
hardware,3b68n3,skilliard4,12,Fri Jun 26 12:49:08 2015 UTC,PCIe 2.0 is bottlenecking some operations too.
hardware,3b68n3,feelix,-5,Fri Jun 26 17:13:37 2015 UTC,Maybe synthetic data transfer benchmarks intended to test speeds.
hardware,3b68n3,III-V,11,Sat Jun 27 07:39:26 2015 UTC,"Yeah, foresight is for losers."
hardware,3b68n3,NLWoody,13,Fri Jun 26 12:52:52 2015 UTC,"Ever heard of an SSD?   Edit: Every time this comes up, people think that graphics cards are the only application of PCIe. Networking can benefit tremendously from this. SSDs are a wash right now -- IIRC, PCIe isn't being fully utilized as of yet for consumer SSDs, but that will change before you know it.   Regardless, think of the cost implications. If manufacturers only need one lane, instead of 16, that means that your electronics are cheaper. If Intel doesn't need to include 4 PCIe controllers on die, and can now do fine with just 1, that's a lot of space saved, that they can either allocate elsewhere for your benefit, or reduce costs."
hardware,3b68n3,III-V,-8,Fri Jun 26 11:32:48 2015 UTC,Sata?
hardware,3b68n3,narwi,10,Fri Jun 26 12:04:20 2015 UTC,"SATA is far slower than PCIe right now. If they do happen to come out with a newer generation, PCIe will still be faster, but I think they're (SATA-IO, the group behind it) is trying to push for people to move to PCIe anyway."
hardware,3b68n3,III-V,-11,Fri Jun 26 13:17:38 2015 UTC,And it is fast enough there too. Except if your only application is copying files between two disks.
hardware,3b68n3,narwi,10,Fri Jun 26 12:11:30 2015 UTC,"Except if your only application is copying files between two disks.   Math doesn't work that way. As long as you are copying large files greater than 0% of the time, you can benefit from higher data streaming rates.  Until file copies take place faster than human reaction time, there will always be a benefit for higher speeds."
hardware,3b68n3,Stingray88,-5,Fri Jun 26 13:22:15 2015 UTC,"This is like saying that everybody would benefit from using pcie x4 SSDs. Which is untrue, despite the fact their file transfers between such disks would be faster."
hardware,3b68n3,Dark_Crystal,3,Fri Jun 26 14:45:56 2015 UTC,"Except it isn't untrue. Everybody absolutely would benefit from PCIe x4 SSDs. It's not for a lack of benefit that people don't all use them today, it's for a lack of value. At the high cost to buy such an SSD, there is very little value for grandma to buy in. The benefits for her simply aren't worth the cost. But in 10-20 years, those prices will be very different. So different that such an SSD (or better) would likely become the default option.  Think about when SSDs first came on the market, it made little sense for anyone not running a high performance expensive machine to have one. These days they've become cheap enough that even some cheap laptops that grandma might buy come with them standard. Give it another 5 years, and most new computers will be sold with an SSD.  Every improvement in computing has a benefit vs cost relationship, and that's the value."
hardware,3b68n3,Madpiggy,3,Fri Jun 26 17:20:29 2015 UTC,Not for the interconnect from the CPU to the southbridge. Not for 1x cards.
hardware,3b68n3,freaky88,3,Fri Jun 26 17:42:31 2015 UTC,"Me taking a 45 minute bus ride to school is fast enough, but a 5 minute car ride would be freaking awesome."
hardware,3b68n3,PadaV4,2,Fri Jun 26 20:00:15 2015 UTC,"hahahah it cracks me up when people say this in the tech world. Get the fuck out of here, technology needs to get faster, cheaper and smalle, not stay the same."
hardware,3b68n3,narwi,-13,Fri Jun 26 16:32:53 2015 UTC,But do we really need PCIe 4.0. Most hardware is far from saturating the PCIe 3 bandwidth.
hardware,3b68n3,Unique_username1,15,Fri Jun 26 11:38:50 2015 UTC,"when multiple PCIe devices ""talk"" to each other, that bandwidth suddenly becomes cramped"
hardware,3b68n3,computertechie,7,Fri Jun 26 11:52:46 2015 UTC,Do you have an example for such an application?
hardware,3b68n3,narwi,10,Fri Jun 26 12:12:08 2015 UTC,on top of my head without much research? CUDA applications with Tesla accelerators
hardware,3b68n3,AssCrackBanditHunter,8,Fri Jun 26 12:31:08 2015 UTC,"AMD graphics cards are beginning to do away with the Crossfire connector and communicate with each other 100% over the PCIe bus. I remember reading somewhere that NVidia was working towards this as well, but every NVidia card that is SLI-capable still requires the additional connector.  That might be fine when each still has 8x PCIe lanes allocated to it, but then you dedicate 4 out of your 16 to an SSD.., and then deal with chipset or CPU limitations that may prevent splitting PCIe lanes in any multiple other than 4x or 8x and now those cards are trying to talk to each other and to the system over 4x PCIe each.  The introduction of PCIe SSDs and the slow but steady advance of graphics cards will put limits on PCIe bandwidth soon. PCIe 3.0 is definitely enough right now, but it won't be in 5 years."
hardware,3b68n3,narwi,5,Fri Jun 26 12:31:33 2015 UTC,Multi-GPU setups (especially AMD since they ditched the Crossfire connector) and transferring files between multiple PCIe disks.
hardware,3b68n3,stuckinatrance,-13,Fri Jun 26 12:33:14 2015 UTC,"transfering files is not actually a credible application. As far as multi-gpu setups go, you want to read up on how little transferring actually happens and what kind of bandwidth is used."
hardware,3b68n3,spiker611,8,Fri Jun 26 12:40:41 2015 UTC,The only thing I'm gonna say to you is never underestimate what technology can do and never underestimate what is going to be needed
hardware,3b8wnn,Penderyn,1 point,Fri Jun 26 22:43:56 2015 UTC,I hope they make a double wide version of this case
hardware,3b8wnn,DeadlyDuckie,1 point,Sat Jun 27 04:04:41 2015 UTC,Like the air 540 or something?
hardware,3b8wnn,DeadlyDuckie,1 point,Sat Jun 27 07:56:37 2015 UTC,Yeah
hardware,3b8wnn,klymen,1 point,Sat Jun 27 08:17:48 2015 UTC,No... Like my house.
hardware,3b8k0n,olavk2,15,Fri Jun 26 21:00:23 2015 UTC,"Yes, this. Also, it means you're less likely to return your CPU damaged so its not Intel's problem.  The pin density is the main one though. Am3 is 941 pins, haswell is 1150. Intel use a smaller process as well, so have the option of a smaller package as the space needed for LGA lands is the limiting sizing factor.  Intel CPUs partly have more pins as most of the pcie lanes come from the CPU, with dmi to the pch. AMD put all the pcie lanes on the chip set, with hypertranspirt only to the CPU itself. Different strategy.  This is also one of the reasons tdp isn't directly comparable between Intel and AMD. The same components are not on the same chips."
hardware,3b8k0n,ault92,-14,Fri Jun 26 21:08:44 2015 UTC,"No, just no."
hardware,3b8k0n,jorgp2,6,Fri Jun 26 22:42:40 2015 UTC,"I wouldn't say that AMD ""prefers"" PGA over LGA. Their server platform uses LGA is known as LGA 1944 or Socket G34. I suspect this is due to the amount of interconnects that must be made; things like memory controllers, PCIE lanes, and other such things requires more pin connections. Also, their server platform has MCM (multi chip module) CPUs much like the Core 2 Quads and the new Broadwell chips with L4$. Those configurations need more pins for more connections, and PGA is not a very space-efficient socket interface.  The main advantage of the PGA design for AM3+ boards is backwards compatibility. If you have an AM3+ board, you can run AMD K10.5 processors (Athlon II, Phenom II), Bulldozer, and Vishera processors. That is a much larger range than Intel's current mainstream socket LGA 1150 which can only support Haswell and Broadwell. Another slight advantage is that in the event that one pin on your CPU bends, it is much easier to correct because the pins have much more individual space.  Other than that, there are hardly any advantages to PGA. The pin density is much lower, making CPU packages larger (cue that's what she said jokes). Motherboards are often less expensive than the CPU, making them much easier to replace in the event that pins break. Also, PGA CPUs with broken pins must be discarded while socket replacement can be done on LGA motherboards albeit with very special tools."
hardware,3b8k0n,JonF1,3,Sat Jun 27 21:52:34 2015 UTC,"Well if your thermal paste is old enough, when you try and pull off your heatsink it will rip the AMD cpu right out of the mobo with it, so...That's convenient."
hardware,3b8k0n,arikv2,2,Sat Jun 27 02:01:40 2015 UTC,"personally never had an issue with this as i always wiggle the heatsink loose before actually pulling, it is such a simple thing to do and almost eliminates the danger"
hardware,3b8k0n,everyZig,2,Sat Jun 27 13:12:43 2015 UTC,Which is why you ALWAYS wiggle the heatsink a bit to break the thermal paste bond before trying to pull it off
hardware,3b8k0n,AndreyATGB,1 point,Sun Jun 28 13:00:18 2015 UTC,"Happened to my friend, though I'd say that's more an issue with the way the CPU is secured (or not) to the board. AMD CPUs don't seem to have the huge holding bracket that Intel uses, making it practically impossible to accidentally take the CPU out unintentionally."
hardware,3b8k0n,krista_,2,Mon Jun 29 05:40:43 2015 UTC,"Essentially, it's a damage/value tradeoff."
hardware,3b8k0n,Tiramisuu2,2,Sun Jun 28 18:57:47 2015 UTC,As the owner of a dual xeon lga motherboard with damaged pins I officially hate lga.   I  have never had an issue with a pin but the board is effectively Garbage once an lga wire is ruined.   Repairing the motherboard takes tooling that the local electronics tech simply won't have.
hardware,3b8k0n,III-V,1 point,Fri Jun 26 21:44:51 2015 UTC,Just needs a BGA rework station... Only 2 grand :)
hardware,3b8k0n,Tiramisuu2,1 point,Sat Jun 27 20:32:22 2015 UTC,"Exactly,  you would think there would be a nice little business in motherboard repair but most people seem to treat the motherboards as disposable.   The dual xeon boards are pricey enough to pay for a couple of hours of labour to repair them,  but I suspect the success rate would be low.   Shorting out the pins on the lga grid probably blows resistors  / caps downstream and you could spend a full day tracing circuits and hunting for faults.   8 hours of shop time with no guarantees of success."
hardware,3b8k0n,III-V,2,Sun Jun 28 00:01:43 2015 UTC,"It wouldn't take 8 hours, maybe 1 hour max. You can make or buy a BGA stencil, and the balls will flow exactly where you want them to, giving you virtually a 100% success rate. Might take a bit longer if you're making the stencil yourself, of course.  The issue I see is that desktop motherboards just aren't worth the money. They are way too expensive to justify that labor, and they don't (typically) store anything like user data, so there isn't really an issue of sentimental value increasing the value of the repair.  Even a server board doesn't make sense. Send it in to manufacturer... and eat the cost of whatever they charge. That's basically what you'd have to do. There just isn't enough volume to justify doing the research and buying the tools to do such a repair, and I am not aware of anybody that'd be crazy enough to do so (unless they had already previously purchased that kind of BGA rework equipment for other large devices).  As such, it's only really worth it to offer that kind of service to markets that do have tons and tons of board level repair volume -- laptops and mobile devices, which are frequently exposed to liquid spills, drops, and inexperienced technicians."
hardware,3b8k0n,wazerbeamfire,1 point,Sun Jun 28 01:05:26 2015 UTC,"Ghetto fix  heatgun, flux, solder iron, and a new socket (balls installed already)"
hardware,3b8k0n,Imidazole0,5,Sun Jun 28 01:43:02 2015 UTC,You suck.
hardware,3b8k0n,rzwerzdsb,1 point,Sun Jun 28 11:52:03 2015 UTC,"I was told to report it leaving the field blank when it misbehaves, maybe it helps ..."
hardware,3b7hy7,random_digital,21,Fri Jun 26 16:22:38 2015 UTC,"For all the hate that Chrome OS gets from enthusiasts, for the average consumer who wants a cheap computer and isn't going to to play games or use much of anything other than the web browser, you really can't beat the performance that these devices get. If you buy a prebuilt laptop with a celeron CPU running Windows 8.1 and plenty of bloatware including the free trial of Norton/McAfee/etc, you're not going to be happy with performance for very long. On the other hand when you buy a Chromebook all you have to do is update it when it prompts you to and you're worry free with a quick web browsing machine. It's also more reliable for people who don't know how to take care of their computers and always end up downloading crap.  If it's got a good 1080p display it's already a good option in this price range."
hardware,3b7hy7,Kevo_CS,34,Fri Jun 26 22:07:12 2015 UTC,"Oh no, how will Intel/AMD/Microsoft survive without the garbage spec tablebound tablet market?"
hardware,3b7hy7,Maysock,1 point,Fri Jun 26 16:39:22 2015 UTC,"My Acer aspire e5 was pretty good. 15.6"" with an i5 5200U and 6 GB of ram for $400. Had a 500 GB hdd I replaced and I have 38 W*hr, which gives me quite the battery life.   Other brands were overpriced imo."
hardware,3b7hy7,TheImmortalLS,23,Mon Jun 29 05:18:15 2015 UTC,ChromeOS  Multitasking   Pick one.
hardware,3b7hy7,AnxiousInfusion,7,Fri Jun 26 16:41:07 2015 UTC,Gotta have 30 Chrome tabs open
hardware,3b7hy7,i-know-not,1 point,Sat Jun 27 02:17:57 2015 UTC,My RAM can barely handle one tab open
hardware,3b7hy7,pb7280,2,Sun Jun 28 14:22:14 2015 UTC,whispers download more
hardware,3b7hy7,TheImmortalLS,1 point,Mon Jun 29 05:18:34 2015 UTC,How did I not see this coming
hardware,3b7hy7,pb7280,17,Mon Jun 29 08:00:51 2015 UTC,So it looks like it's a glorified tablet pretending to be PC.
hardware,3b7hy7,zarazek,11,Fri Jun 26 16:35:04 2015 UTC,"Sweet, a chromebook... haven't seen one of those before"
hardware,3b7hy7,regenshire,7,Fri Jun 26 16:52:14 2015 UTC,gonna run some sweet google apps?
hardware,3b7hy7,XGUNNARX,7,Fri Jun 26 18:09:06 2015 UTC,"Type up a text document and then surf the world wide web man, what else could someone want?"
hardware,3b7hy7,TheOnlyHighlander,3,Fri Jun 26 20:09:11 2015 UTC,"I'm too German to tell wheter you're kidding or not, but that's probably what 90% of the time spent using a computer consits of. I recently had to install BOINC to give my Xeon something to do, poor thing was sitting at 1-5% load for hours. Gaming and CAD only make up like, five hours per week?"
hardware,3b7hy7,dagmx,3,Sat Jun 27 16:45:56 2015 UTC,"My work recently gave me a chrome book and it's pretty sweet actually. More useful than a tablet, a lot cheaper than a laptop. Really all I use it for is my email + web based research and portable work book.  It doesn't replace my main desktop and I wouldn't get it for myself as a full on laptop replacement, but for what it is, it's pretty darn good"
hardware,3b7hy7,39th_Step,6,Sat Jun 27 19:54:51 2015 UTC,"Forbes is probably the shittiest website when it comes to tech, full of half-baked articles, half-truths, clickbait titles, their godawful ""thought of the day"" and pop-ups en masse."
hardware,3b7hy7,zmeul,4,Fri Jun 26 22:28:00 2015 UTC,computer   I think he misspelled tablet :)
hardware,3b7hy7,III-V,3,Fri Jun 26 16:57:44 2015 UTC,"I think he misspelled ""pile of shit."" Fuck Acer."
hardware,3b7hy7,expert02,1 point,Sat Jun 27 00:34:50 2015 UTC,Sensationalist garbage bullshit.
hardware,3b7hy7,TehFuckDoIKnow,1 point,Fri Jun 26 22:36:28 2015 UTC,Sounds like my Jude remix
hardware,3b7hy7,ragewind,1 point,Fri Jun 26 19:49:07 2015 UTC,acer are crap one day they will learn what customer support is but until then never touching with a barge pole
hardware,3b7hy7,TheImmortalLS,1 point,Sat Jun 27 09:58:49 2015 UTC,"I'm wondering, why is Acer bad? Is it build quality, plastic snapping, etc?"
hardware,3b7hy7,skilliard4,-3,Mon Jun 29 05:19:32 2015 UTC,Since when did NVIDIA make CPUs?
hardware,3b7hy7,thesynod,1 point,Fri Jun 26 23:03:58 2015 UTC,Tegra is like 10 years old now.
hardware,3b7hy7,skilliard4,-3,Sat Jun 27 01:20:25 2015 UTC,why on earth are they using a 10 year old CPU?
hardware,3b7hy7,ocshoes,5,Sat Jun 27 03:46:02 2015 UTC,"Come on; the Tegra K1 is based on Kepler architecture and was released some time in what looks like August 2014. /u/thesynod was just saying that the Nvidia has produced CPUs under the Tegra name for like 10 years. (Although it appears to be closer to about 7 years.)  It's fairly common knowledge that Nvidia has the Tegra line, they made such a big deal about it back when Tesla first started putting them in their cars.  Fun fact of the day, the Ouya had a Tegra in it."
hardware,3b7hy7,kkjdroid,2,Sat Jun 27 08:37:32 2015 UTC,So did the Zune HD!
hardware,3b7hy7,everyZig,1 point,Sat Jun 27 10:50:48 2015 UTC,"Yup, we have a tablet with a Tegra 2 (the original 10 inch galaxy tab), and my previous phone (LG Optimus 4x HD) had a Tegra 3  Quite decent chips from what ive seen of em"
hardware,3b7hy7,Exist50,1 point,Sat Jun 27 16:44:35 2015 UTC,"Eh, they never did meet expectations. Power consumption in particular has been a sore point."
hardware,3b518c,Maimakterion,68,Fri Jun 26 00:56:01 2015 UTC,"Well, I appreciate the honesty."
hardware,3b518c,bullohnie,17,Fri Jun 26 02:29:40 2015 UTC,I guess now we need to call into question why Guru3d has benchmarks way off the mark from everyone else.
hardware,3b518c,random_digital,33,Fri Jun 26 03:20:05 2015 UTC,"why Guru3d has benchmarks way off the mark from everyone else   They don't really :o  Showing relative FPS (Fury/980Ti).  Averages per site:          HardOCP     Anandtech   TechPowerUp Guru3D  PcPer   Tomshardware 1440p   80%         90%         94%         95%     94%      99% 2160p   95%         97%         99%         100%    99%     105% Games   5           10          22          9       7       8   Edit: Looking at the choice of games, Tomshardware has mostly picked games where the Fury X performs well. Other sites have the Fury X scoring equal or higher in 5 of Tomshardware's game choices.  HardOCP has only 1 game come out in favour of the Fury X at 4K, and are generally below the bench average except in Battlefield 4. They also introduced Dying Light which is one of the worst performers for the Fury X at 1440p.  Edit2:  Game @ 2160p/4K             Guru3D      Average     Reviews The Witcher 3: Wild Hunt     91.89%      98.05%     4 Grand Theft Auto V           97.67%      93.26%     6 Shadow of Mordor            101.89%     103.22%     4 Bioshock Infinite            94.74%      97.52%     3 Metro: Last Light           102.63%     106.13%     4 Tomb Raider                 105.00%     107.36%     3 Thief                       100.00%     103.80%     2  Hitman Absolution           102.27% Battlefield Hardline        106.67%"
hardware,3b518c,OftenSarcastic,7,Fri Jun 26 07:47:14 2015 UTC,"You conveniently left out the reason for poor performance in Dying Light - VRAM usage :P  [H]ardOCP's review of the 980Ti specifically looked at this, and they noticed that memory usage scaled on both the Titan X and the 980Ti. Furthmore, even the 980's 4GB was getting rightfully taxed at 4083MB of 4096MB available.  Now I took another look at the 390X review: http://www.hardocp.com/image.html?image=MTQzNDYxMjU0OWwxR0JRekpFNXFfNV8zX2wuZ2lm  The 8GB 290X rebrand, the 390X, is a better performer :P"
hardware,3b518c,terp02andrew,2,Fri Jun 26 15:08:12 2015 UTC,"But is that memory use indicative of the active working set or is some of the data simply cache leftover for future use?  The R9 390X is 10% faster than the R9 290X at 1440p and 11% faster at 2160p, which is reasonably consistant with the 10% core clock increase and the 20% memory clock increase.  If 4 GB RAM is a bottleneck in Dying Light then how does the GTX 980 outperform the Fury X, having the same amount of RAM but half the memory bandwidth? It can't store more data and it can't acquire new data faster.  Secondly, if 4 GB RAM is a bottleneck, then how does the Fury X gain ground on the 980 Ti when the resolution is increased to 2160p? Shouldn't that require even more memory?  2560x1440       R9 290X 4 GB    R9 390X 8 GB    R9 Fury X 4 GB  980 4 GB    980 Ti 6 GB Average         45.4            49.8            51.7            56.7        73.2 Min             33              34              37              40          52 Max             60              67              77              73          97 FuryX base, avg 87.8%           96.3%           100.0%          109.7%      141.6%  3840x2160       R9 290X 4 GB    R9 390X 8 GB    R9 Fury X 4 GB  980 4 GB    980 Ti 6 GB Average         31.4            34.9            39.8                        44.9 Min             24              27              30                          34 Max             37              41              49                          54 FuryX base, avg 78.9%           87.7%           100.0%                      112.8%"
hardware,3b518c,OftenSarcastic,2,Fri Jun 26 15:55:02 2015 UTC,"Great question. I think it's more of reduced performance - specific to the card in question.   The other is simply CPU overhead. As resolution increases, it masks these deficiencies in AMD's drivers. I do think Windows 10 will help AMD especially in that regard."
hardware,3b518c,terp02andrew,2,Fri Jun 26 16:36:57 2015 UTC,"Great question. I think it's more of reduced performance - specific to the card in question.    The R9 290X vs. R9 390X is about as close as we can get to testing a specific card with two different amounts of RAM and it pretty much correlates with the clock increase.  The Fury X even has a higher minimum frame rate than the 390X so it's not running into worse spikes.   The other is simply CPU overhead. As resolution increases, it masks these deficiencies in AMD's drivers. I do think Windows 10 will help AMD especially in that regard.   CPU overhead would be an issue if they were testing on an i3 or running something like Star Swarm. They're using a 4.8 GHz i7."
hardware,3b518c,OftenSarcastic,2,Fri Jun 26 17:01:26 2015 UTC,"I would say yes and no to that assertion. If you recall, nVidia driver comparisons were made when Mantle was first released. Mantle was really the first time you saw CPU-limitations of AMD drivers being properly addressed.  http://pclab.pl/art55953-3.html It's a great comparison b/c you can see that AMD gains both from Mantle and from CPU-overclocking.  Of course, I would argue 1080P is such a low-resolution these days that people are using it primarily for twitch shooters (120Hz/144Hz panels). And it's great for that, but it's made possible b/c you can run a Haswell/DC at 4.5-4.7Ghz and really stretch your GPU to its full capability.  So as to whether we see these same gains in Fury X, as AMD drivers continue to develop - especially in Windows 10? I would hope we see improvement. But without Mantle, there's always that nagging question if their own software continues holding them back, before even considering other factors (game engine complexity, necessity of CPU overclock).   If you design for the lowest denominator, I don't think we can expect everyone to be running the same overclocks that you or I are accustomed to seeing :p"
hardware,3b518c,terp02andrew,2,Fri Jun 26 17:16:49 2015 UTC,"Right, but Dying Light isn't Battlefield 4 Multiplayer. And AMD have had 1½ years to work on their drivers.  If you look at CPU scaling in Dying Light it's pretty flat above a fast i5 or 8 core Piledriver. It doesn't really start dropping FPS until they clock their i7 below 3 GHz.  Now they're testing with a GTX 980 but I doubt AMD drivers are going to eat up 1.8 GHz worth of CPU speed across 4 cores (+ hyper-threading). The 290X does fine against the GTX 970 in all three resolutions in their GPU test as well.  Edit: as a side note, the better scaling at 4K is pretty consistent among most titles tested and I doubt CPU overhead is an issue in every single one. It's not even an issue in the single player part of the Battlefield 4 test linked."
hardware,3b518c,OftenSarcastic,1 point,Fri Jun 26 17:48:59 2015 UTC,"So G3D has fury performing slower/their 980Ti is boosting higher.  Another review with Fury slightly faster at 4k,  http://cdn.sweclockers.com/artikel/diagram/10139?key=4693cc3598cecb9df77ed3d75f5a8da4  game selection is making quite a difference.  http://www.sweclockers.com/test/20730-amd-radeon-r9-fury-x/6#content"
hardware,3b518c,namae_nanka,6,Fri Jun 26 16:15:12 2015 UTC,"I must have missed something, how was the guru 3d review different?"
hardware,3b518c,KlamKhowder,5,Fri Jun 26 06:01:47 2015 UTC,Only hope I see at this point for AMD is to price the Fury X $50 below  the 980 Ti and price the Fury in competition with the 980. I'm sure this will definitely get them some sales in those tiers.
hardware,3b518c,Zahloknir,6,Fri Jun 26 04:37:46 2015 UTC,Here in germany it is currently 50 EUR cheaper. If it turns out to be a decent overclocker it would be worth considering.
hardware,3b518c,barthw,2,Fri Jun 26 07:35:25 2015 UTC,"It was 90 EUR cheaper in spain on release date, now it's only 60."
hardware,3b518c,p4block,1 point,Fri Jun 26 10:09:27 2015 UTC,"Huh, what site? Cheapest reference 980ti I've found is 675€ and Fury X 699€."
hardware,3b518c,JepuJee,2,Fri Jun 26 13:08:31 2015 UTC,Mh i checked Mindfactory and TI's were more like 750ish
hardware,3b518c,barthw,1 point,Fri Jun 26 16:08:08 2015 UTC,Thats where I've been checking also mostly  Gigabyte 980Ti Gaming G1 707€  Gigabyte 980Ti reference 675€  Apperantly the G1 should be quite beastly and just a few € more than the fury.
hardware,3b518c,JepuJee,2,Fri Jun 26 20:33:38 2015 UTC,"yeah you are right. Well then there's only hope the FuryX will get a bit cheaper once it's out. I own a 970 G1 Gaming myself and it's quite a bit faster than a reference 970, so yeah."
hardware,3b518c,barthw,1 point,Sat Jun 27 07:02:02 2015 UTC,But AMD said they can't continue being the cheaper solution. They needed the Fury X to beat the GTX 980Ti.
hardware,3b518c,Yearlaren,48,Fri Jun 26 20:48:12 2015 UTC,Too bad. I was hoping this was all a misunderstanding. So are we good to order our Ti's or what?
hardware,3b518c,BlayneTX,11,Fri Jun 26 01:59:00 2015 UTC,Well there's still Fury (air cooled) to be released and that could cause price drops.
hardware,3b518c,bentan77,2,Fri Jun 26 05:43:14 2015 UTC,"Ehhh, I don't think so. If anything, maybe by $30 or something."
hardware,3b518c,BuildYourComputer,3,Sat Jun 27 16:32:25 2015 UTC,"Could have done it already but even if the cards are on par with each other, nvidia still has all it's tech that AMD doesn't so if all the physx hairworks so on and so on."
hardware,3b518c,RiffyDivine2,13,Fri Jun 26 12:47:07 2015 UTC,"Yep, that is a go. AMD just hung on for life with this card, I don't consider it the savior it was claimed to be."
hardware,3b518c,Zahloknir,-2,Fri Jun 26 02:08:06 2015 UTC,"You know as much as it hurts me to say it, you are right... nvidia wins... we all lose, no competition = consumer fucked."
hardware,3b518c,Kubi74,101,Fri Jun 26 02:45:48 2015 UTC,"Are you all just fucking stupid or something?  The card performs at worst 5-10% slower.  Big fucking deal.  That's the difference between an average of 66 FPS and an average of 60 FPS.  At 4K it's less than a 2% difference on average.  Do you play every game with your frame counter on and then immediately output afterburner logs into a spreadsheet so you can jack off later?  What matters is ""CAN IT PLAY GAMES WELL AT X QUALITY SETTINGS/RESOLUTION?"" and the answer is ""Why yes, it can.""  In terms of real world gaming experience, both cards are fucking great experiences.  There's a bigger framerate difference between the fucking 5960x and 4690K.  What were you expecting?  Every single price/performance tier AMD and Nvidia are neck and neck.  FuryX is on par with a 980Ti, 390x is on par with a 980, 390 is on par with 970, 380 is on par with a 960 etc etc.  That is by definition competition you fucking melodramatic moron.  Getting real irritated with the ""Omg Fury X isn't outperforming the TitanX by 50% for cheaper, AMD is dead"" bullshit.  If either AMD or Nvidia was capable of blowing each other away in performance, they would.  Both of them are doing their best, and as it turns out their best is pretty damn similar to each other."
hardware,3b518c,Dullahan3470,48,Fri Jun 26 07:18:08 2015 UTC,"The problem is the market naturally leans toward Nvidia.  They are crushing AMD in market share so AMD needs to impress to gain some back.  If you have $650 and are looking at benchmarks like this, especially if you're not one of the very few people with a 4k monitor, you'll go with the 980 ti unless you already have a bias toward AMD, but as market share shows, that isn't a big chunk of people.  There are more issues too.  We already know the 980ti overclocks like a beast and we hoped the Fury X would too with its AIO cooler but we still don't know because reviewers can't overclock yet which is a huge fucking problem as it's the only saving grace for the card IMO."
hardware,3b518c,StealthGhost,3,Fri Jun 26 07:47:41 2015 UTC,If I had $650 in my pocket for video today and didn't have a 4k monitor then I'd get a 4k monitor and hang on to my 290 for a few more months to see where this battle leads.
hardware,3b518c,cestith,0,Fri Jun 26 17:33:58 2015 UTC,"You should see what the 980Ti on the hybrid cooler overclocks like, it made me giggle a bit with joy."
hardware,3b518c,RiffyDivine2,1 point,Fri Jun 26 12:55:54 2015 UTC,"I have my reference card at 1450MHz at the moment, pre-ordered the hybrid add-on cooler as it came out cheaper overall."
hardware,3b518c,makar1,1 point,Fri Jun 26 15:19:16 2015 UTC,"Waiting on my Ti cards, got a pair of 980's in my machine right now with hybrid coolers on them and love it. Under load overclocked hard and it's hitting only 43c."
hardware,3b518c,RiffyDivine2,12,Fri Jun 26 16:39:35 2015 UTC,"The problem is, once you start looking at non-reference 980ti's, the performance gap widens to over 20% in some games - that's a pretty serious difference. Since AMD won't allow non-reference Fury Xs and the Fury almost certainly won't be faster than the X, that leaves them at a huge disadvantage. The 980ti is still a very new card but in a few months, the non-reference/factory OC cards will be the norm, just like they were with the 980 and the 780 before it."
hardware,3b518c,thejshep,7,Fri Jun 26 10:19:06 2015 UTC,"Yeah, totally agree with this. I think the problem is that AMD hyped Fiji up as something that would destroy TitanX. Not explicitly, but all the HBM hype with those monster specs led everyone to believe it would be significantly faster than Nvidia's best offering. They fell short on execution, it's true. But overall, it's still a very good card and it will sell. Both FuryX and 980Ti are ideal for 1440p gaming and there isn't a massive difference in performance there to warrant such an extreme reaction. If AMD makes their architecture more capable of utilizing the raw processing power at it's disposal, they could beat Nvidia in the next generation of GPUs."
hardware,3b518c,cyclegap,7,Fri Jun 26 07:27:36 2015 UTC,Isn't the overclocking ability severing limited compared to the 980Ti though?
hardware,3b518c,coolnow,4,Fri Jun 26 10:56:06 2015 UTC,"Everything is limited compared to the 980 Ti.  Has there ever been another GPU where you could reliably expect a 40% overclock? (from reference base speed to OC'd boost speed)  But yes, right now the Fury X has locked voltage so it's not even a contest, maybe 5-10% OC.  In theory that should be unlocked soon, assuming there's not some critical issue with OC'ing a HBM-based chip."
hardware,3b518c,capn_hector,4,Fri Jun 26 15:18:46 2015 UTC,"40%? Don't be ridiculous. It makes no sense at all to compare reference to boost anyway. You're supposed to compare base to OC base or boost to OC boost. Which is still an impressive 1100->1400, or 27%, although I'm not sure you can call that reliable. If you can, then that's amazing."
hardware,3b518c,bphase,2,Fri Jun 26 16:29:32 2015 UTC,"Most rating sites I saw were getting at least 1400 boost.  Now whether you say that 1100 is the real ""base speed"" and 1000 is a ""power saving speed"" or that 1000 is the real ""base speed"" and 1100 is an ""auto overclock setting"" is a matter of perspective.  Remember that none of this is set in stone, NVIDIA could have released with the base being 1200 and the boost being 1300 if they wanted, it just would have a higher advertised TDP, higher risk of throttling, etc.  Or they could have swung the other direction and tried to take the TDP down farther  The only fixed points of reference are ""the manufacturer baseline at which they guarantee a reference cooler won't throttle"" and ""the limits of the silicon with good cooling"", everything else is arbitrary.  From that perspective, I lean towards taking the official marketing value of 1000 as the ""base clock"" and the 1400 as the ""overclock"".  That's just semantics though."
hardware,3b518c,capn_hector,1 point,Fri Jun 26 16:50:53 2015 UTC,"Which is still an impressive 1100->1400, or 27%, although I'm not sure you can call that reliable. If you can, then that's amazing   I've honestly not seen a 980ti yet not hit 1400-1450 or so with ease. My titan did 1250 pretty easily on the mediocre reference cooler.  That's pretty damn reliable so far. Nvidia hit a home run."
hardware,3b518c,Dippyskoodlez,2,Sat Jun 27 00:47:29 2015 UTC,"i feel like overclock percentage should be quoted based on fps gains, not core clock increase. Just my two cents. But yeah, we're still talking like a 20-25% fps increase on the 980ti, and only about 5% for the fury x (for the time being)."
hardware,3b518c,spikey341,2,Fri Jun 26 16:50:27 2015 UTC,"FPS is arbitrary though.  How many FPS are you going to get OC'ing your 980 Ti on 640x480 min-settings Quake III Arena?  Even in percent terms rather than absolute, you're going to get different performance with different games/resolutions/quality due to different things being the bottleneck.  Core/memory clocks are really the only platonic way to approach that."
hardware,3b518c,capn_hector,3,Fri Jun 26 16:55:49 2015 UTC,"Because MSI, Sapphire, and ASUS haven't updated their OC utilities to support Fury X's VBIOS yet and/or AMD hasn't given them the technical info to that end yet and/or AMD has a driver release incoming to that end.  For all we know, Fury can take 1.4V and push 1400MHz."
hardware,3b518c,chapstickbomber,1 point,Fri Jun 26 15:22:55 2015 UTC,"Right now, by all reports.  Locked down."
hardware,3b518c,ShinseiTom,1 point,Fri Jun 26 14:59:46 2015 UTC,We don't know anything yet.  It's voltage locked still.
hardware,3b518c,Dullahan3470,1 point,Fri Jun 26 15:53:21 2015 UTC,Yes.
hardware,3b518c,RiffyDivine2,3,Fri Jun 26 12:54:09 2015 UTC,"I'm not saying that the user you replied to  wasn't being melodramatic, but I don't understand your line of reasoning. Even if the 980Ti only performs 5-10% faster at 1440p (not to even mention the overclocking headroom on the 980 Ti) at the same price point, why on earth would anyone consider the Fury X over the Ti? Not to mention that AMD has got literally nothing else (of consequence) going for it, while nvidia has been super aggressive with physX, superior tessellation, HBAO+ etc. in many current gen games (basically just gameworks).   Nvidia have already captured the lion's share of the market in recent years, so AMD always has always had to play the price/performance angle. If they fail on that - the one thing that has made me buy AMD GPUs for my last two builds - then there is no reason, except silly brand loyalty, for a consumer to choose the weaker option."
hardware,3b518c,Han_soliloquy,3,Fri Jun 26 15:59:47 2015 UTC,"Form factor, noise, freesync is about $200 cheaper than gsync, AMD has better multimonitor support, AMD is more focused on DX12 support etc.    AMD also tends to support their cards longer.  The 7xxx cards are still strong performers.  Nvidia actively cripples their cards in drivers. (When they finally optimized Kepler for The Witcher 3, people were reporting gaining 15-20 FPS average...)  If Gameworks bullshit is a deciding factor in a GPU purchase, you're already looking at Nvidia exclusively anyway.  The Fury X offers a watercooled performance card for the same price as a reference design 980Ti."
hardware,3b518c,Dullahan3470,1 point,Fri Jun 26 16:13:37 2015 UTC,"Price/Performace has always been a driving factor in my GPU/CPU purchases. More precisely, performance in the games/type of games that I play matters more to me.   Not everyone wants a water-cooling solution, AMD is not allowing non-reference Fury Xs and there is currently no sign of any real overclocking support. The performance gap widens when you consider non-reference 980 Tis, overclocking and the rumored imminent price drop. Noise is one of those things I personally would easily ignore in favor of raw performace/$ (the reason I went with a stock 6950 followed by an R9 290).   If you lose the tin-foil hat for a second, conventional wisdom is that Nvidia has always been winning the driver battle - and not necessarily because Nvidia is so good at it - it's just because AMD has been worse, even considering the improvements over the past few gens.  I can tell you that while PhysX is a total gimmick - it is undeniably attractive. HBAO+ blows traditional SSAO out of the water (something I didn't believe until I saw the impact in Far Cry 4) and tessellation (also something I scoffed at initially) is now one of my favorite graphical enhancements, because once you see the difference, you can't play without it. (Again, this is based on the games that I play and enjoy).   Monopolizing practices aside, Gameworks is also the reason (same as ""The way it's meant to be played"") why many new AAA games favor Nvidia hardware - and if those games are the ones you enjoy playing, well, that's an extra notch in the performance belt for Nvidia.  All that said, if the Fury X would have beat the Ti by even as little as 10-15%, and maybe even the Titan X as some people expected, it would be my top choice, no questions asked."
hardware,3b518c,Han_soliloquy,17,Fri Jun 26 16:31:43 2015 UTC,"but why would someone buy a card that costs exactly the same, but performs just slightly worse (even if it is in the 1%-5% area) and has less VRAM?"
hardware,3b518c,communistsquared,24,Fri Jun 26 07:40:43 2015 UTC,"it's smaller and runs 30C cooler. VRAM is a non-issue, otherwise it wouldn't do so well at 4k gaming."
hardware,3b518c,nater99,11,Fri Jun 26 08:39:22 2015 UTC,"It's only 30C cooler because it's water cooled, which in itself might be a point against for some people. Some people don't really want water cooling, and others might not have the case for it."
hardware,3b518c,gizza,1 point,Fri Jun 26 09:18:07 2015 UTC,which is why there will be a fury version of it
hardware,3b518c,Sofaboy90,3,Fri Jun 26 13:56:02 2015 UTC,Which will very likely be a toned down version of it.
hardware,3b518c,Zahloknir,1 point,Fri Jun 26 14:02:47 2015 UTC,"yeah but it wont be watercooled, thats my point"
hardware,3b518c,Sofaboy90,1 point,Fri Jun 26 14:36:34 2015 UTC,"Well yeah, but then the smaller and cooler argument I was responding to is no longer relevant."
hardware,3b518c,gizza,2,Fri Jun 26 15:34:10 2015 UTC,"VRAM isn't an issue yet, but a couple years down the line it could be."
hardware,3b518c,Raphman90,5,Fri Jun 26 10:28:12 2015 UTC,If you're the type of person buying the Fury X they can probably upgrade when it does become a problem.
hardware,3b518c,Blubbey,0,Fri Jun 26 13:57:24 2015 UTC,It will probably be an issue within the next year.
hardware,3b518c,Zahloknir,1 point,Fri Jun 26 14:03:36 2015 UTC,I'm curious as to where you're getting that information. Got anything to back that up?
hardware,3b518c,1800hurrdurr,1 point,Fri Jun 26 14:40:20 2015 UTC,"Any reviews with vram usage will show that 4G is not enough at 4k, period. 6g is acceptable, but you can still find usage scenarios that breach that threshold."
hardware,3b518c,Dippyskoodlez,1 point,Sat Jun 27 00:48:50 2015 UTC,"http://www.guru3d.com/articles-pages/gta-v-pc-graphics-performance-review,9.html  Safe to say there are current games that saturate 3-3.5 gigs of VRAM in the 1080p-1440p resolution respectively. About 1-2 years back, people were actually recommending 2gb cards as plenty. I'm just taking a sort of educated guess in saying 4GB will be the minimum you want in a graphics card for playing new titles mid 2016."
hardware,3b518c,Zahloknir,1 point,Fri Jun 26 15:29:19 2015 UTC,"It runs cooler on a AIO cooler, which the 980 Ti also can use from EVGA. I wonder if it's much cooler then."
hardware,3b518c,RiffyDivine2,8,Fri Jun 26 12:56:51 2015 UTC,"And the Ti is more expensive with the AIO, right?"
hardware,3b518c,ShinseiTom,1 point,Fri Jun 26 15:00:46 2015 UTC,I believe it's 749 or around that in USD is the fury with it only 650?
hardware,3b518c,RiffyDivine2,2,Fri Jun 26 16:40:04 2015 UTC,"The EVGA coolers are another $100 over the base price, so $650 vs $750.  Which is honestly still a reasonable price comparison since the 980 Ti is faster on most games."
hardware,3b518c,capn_hector,1 point,Fri Jun 26 15:20:19 2015 UTC,"I dunno, even if you manage to get 20% more performance and we assume a 60 FPS baseline, you're paying $8.3 per each extra FPS."
hardware,3b518c,Dullahan3470,1 point,Fri Jun 26 16:07:26 2015 UTC,"Or another way to look at it would be that ($750 / $650) = 15% increase in price, so a 20% increase in performance would actually be improving the price-to-performance ratio.    At the high end it's rare that you even get a 1:1 increase.  And people at the ultra high end tend not to care that much about cost-per-FPS - otherwise they'd be gaming on lower-res monitors that could be fed by cheaper cards and slower CPUs.  The best price-to-perf ratio is probably to just use your IGPU, because it's free, right?"
hardware,3b518c,capn_hector,1 point,Fri Jun 26 16:39:01 2015 UTC,"I did it because my whole PC is silent now, but I thought the fury with the cooler was also 750 not 650. At 650 that is pretty cheap for what it is. Installing those coolers however was a bitch and a half."
hardware,3b518c,RiffyDivine2,-13,Fri Jun 26 16:38:52 2015 UTC,"Except VRAM is totally an issue. VRAM usage regularly goes above 4GB when playing a modded game in 4K. Also, when games finally utilize HBM, Nvidia's new cards will be out anyway."
hardware,3b518c,avi6274,1 point,Fri Jun 26 09:11:18 2015 UTC,freesync saves you $2-300
hardware,3b518c,Hariooo,1 point,Fri Jun 26 18:14:15 2015 UTC,"It's not exactly the same price, it comes with water cooling. And explain how 4GB of HBM memory is worse than 6GB of DDR5."
hardware,3b518c,JustaPassanger,0,Fri Jun 26 20:57:35 2015 UTC,"Because of form factor, regional pricing, it's quieter, features (freesync is significantly cheaper than gsync), fanboyism etc.  The performance difference is small enough that it's most likely going to depend on two things: AMD CPU overhead versus Nvidia CPU overhead (especially at higher framerates where the CPU starts to come into play) and game level optimizations.  Far cry 4 for example favours the Fury X.  And most people don't shop strictly by price/performance.  If they did that, they'd all have swooped up those $230 R9 290's that were floating around for months."
hardware,3b518c,Dullahan3470,1 point,Fri Jun 26 15:57:23 2015 UTC,"fanboyism   last time i checked, the vast majority of gamers prefer nvidia over amd. /r/buildapc may not, but the average gamer does."
hardware,3b518c,communistsquared,1 point,Fri Jun 26 16:48:44 2015 UTC,Thank you.
hardware,3b518c,KittenMetten,1 point,Fri Jun 26 12:53:17 2015 UTC,"Its not on par if its 5-10% slower. Given that they are the same price, I dont see why you would ever go for a Fury X."
hardware,3b518c,abacabbmk,1 point,Fri Jun 26 15:40:35 2015 UTC,"so you're telling me 5-10% lower performance for exactly the same price is a good buy?  Oh, and as far as we cam tell, no overclocking, so that gap, currently only gets bigger.  Nvidia retains its marketshare edge, and AMD gained nothing."
hardware,3b518c,PaulTheMerc,1 point,Fri Jun 26 16:20:43 2015 UTC,"I think someone who bought either card would be very happy with how it performed and wouldn't notice a difference between the two out of the box.  Some games Nvidia wins, some games AMD wins.  DX12 could still shake things up even more."
hardware,3b518c,Dullahan3470,1 point,Fri Jun 26 17:50:21 2015 UTC,nvidia wins most of the time.
hardware,3b518c,abacabbmk,1 point,Fri Jun 26 17:56:27 2015 UTC,"But a stock ti outperforms it, and you can OC another 15%+ out of them. You can't get that much from OCing the fury x"
hardware,3b518c,piovocsic,1 point,Fri Jun 26 17:31:00 2015 UTC,"Unfortunately you're just looking at the stock Fury X vs Stock 980 Ti. The kind of people who buy those cards WILL overclock them and when you start with the overclocking is where the 980 Ti just gives it a beatdown.   https://www.youtube.com/watch?v=iEwLtqbBw90  If you look at the MSI or the G1 cards at 1440p they are almost 20% faster than the Fury X OC'd and they aren't OC'd. If you were to apply an OC to them, yeah it's over for the Fury X. Your better option at the $650 price point is to buy the 980 Ti and OC the hell out of it. The release of the Fury X has already brought down the price of the 980 Ti in the UK to 509 from 549 so that's a bonus.    Here's hoping the FX drops to $599, then you've got competition."
hardware,3b518c,DexRogue,-3,Fri Jun 26 19:20:17 2015 UTC,Both of them are doing their best   HAHAHAH! you are so ignorant! then you go around calling people names..... ROFL!
hardware,3b518c,Kubi74,20,Fri Jun 26 12:44:01 2015 UTC,"A tad over-dramatic, wouldn't you think?"
hardware,3b518c,ohfuckreddit,48,Fri Jun 26 03:42:40 2015 UTC,"Seeing as AMD dropped from 40% market share to 24% with the release of the 900 series, dude's not too far off."
hardware,3b518c,Kubi74,6,Fri Jun 26 04:07:45 2015 UTC,No I do not.
hardware,3b518c,epsys,5,Fri Jun 26 03:54:10 2015 UTC,AMD needs a tighter lip. If they had not even hinted at this card...and dropped it...would have left NVidia scrambling. sigh
hardware,3b518c,christes,15,Fri Jun 26 06:26:17 2015 UTC,I'm pretty sure Nvidia has enough corporate espionage going on to know regardless.
hardware,3b518c,thejshep,-1,Fri Jun 26 09:16:28 2015 UTC,"I'm pretty sure they kill kittens too. Just like you, I don't have any proof and don't plan to offer any anyway, but I'll still throw out the accusation."
hardware,3b518c,Schmich,10,Fri Jun 26 10:06:52 2015 UTC,I think it is a bit obvious that both companies get enough leaks to know what is more or less going on. The price and release timing of the 980Ti is a good example of this.
hardware,3b518c,namae_nanka,3,Fri Jun 26 11:41:40 2015 UTC,lol they don't even need spies when they got bought to court for price-fixing.
hardware,3b518c,heratic666,4,Fri Jun 26 15:59:05 2015 UTC,Yep say hello to inflated prices and little to no incentive for nvidia to push the price/performance levels of there next GPU'S   Titan Y 10% faster then titan X and only $3000 with 24gb of ram.  Fuck Yeah
hardware,3b518c,barthw,2,Fri Jun 26 05:46:54 2015 UTC,Yeah but we will all be onto next generation when most games finally use dx12
hardware,3b518c,exscape,1 point,Fri Jun 26 02:55:11 2015 UTC,Yup. Thanks to the bystander effect morons will continue to buy nvidia GPUs despite them consistently shafting consumers (3.5GB)
hardware,3b518c,BrainSlurper,-5,Fri Jun 26 07:09:44 2015 UTC,"NVIDIA are rumored to be considering a price cut for the 980 Ti, 980 and possibly 970. http://www.kitguru.net/components/graphic-cards/anton-shilov/nvidia-is-considering-a-price-cut-of-high-end-graphics-cards/  That's competition, and we all win."
hardware,3b518c,cartermatic,20,Fri Jun 26 16:36:01 2015 UTC,"If you consider nvidia ending up with a monopoly and the GPU industry stagnating competition, then sure. Not sure about the winning part though."
hardware,3b518c,StealthGhost,1 point,Fri Jun 26 06:57:11 2015 UTC,"Your example isn't competition of course, but Nvidia potentially cutting the price of their graphics cards in response to the release of a competitors is competition."
hardware,3b518c,TehRoot,1 point,Fri Jun 26 07:04:21 2015 UTC,The only positive of AMD going under (or close to it) would be being sold to someone with a ton of money who wants into the gpu game and wants to crush Nvidia.  Maybe Samsung?
hardware,3b518c,RiffyDivine2,9,Fri Jun 26 14:08:39 2015 UTC,Samsung doesn't care about the desktop gpu market
hardware,3b518c,elimi,3,Fri Jun 26 08:27:29 2015 UTC,Samsung   Has expressed no interest in the home user GPU market. Even with the banking reports that AMD would be bankrupt by 2017 I still think Intel and Nvidia would throw money at them just to keep them open.
hardware,3b518c,thoosequa,1 point,Fri Jun 26 10:48:30 2015 UTC,Samsung is so bad with software tho...
hardware,3b518c,exscape,3,Fri Jun 26 13:01:19 2015 UTC,That is already discussed as bad source. They provide no citation in their article and base it off practically nothing. Unless the Fury is able to provide similar benchmarks to the 980ti I don't see why Nvidia would need to drop the prices of their newest flagship card not even 2 months after launching it
hardware,3b518c,RiffyDivine2,2,Fri Jun 26 11:40:52 2015 UTC,"It's 2% faster than the 980 Ti at 4K (on average), though slower at 1080p and 1440p. 4K Full article (in Swedish), linked where the game benchmarks begin"
hardware,3b518c,RiffyDivine2,2,Fri Jun 26 09:28:14 2015 UTC,That site is just clickbait.
hardware,3b518c,willyolio,2,Fri Jun 26 13:40:44 2015 UTC,Intel can't buy them unless it's just the GPU division and Intel and Nvidia really need AMD to stay afloat.
hardware,3b518c,bphase,2,Fri Jun 26 12:58:37 2015 UTC,I'd rather just be  patient and see how the competition is during black Friday.
hardware,3b518c,melgibson666,2,Fri Jun 26 10:05:53 2015 UTC,"If you're gonna wait that long, might as well wait for the next-gen GPUs some time next year...  Nah, maybe not. Not many games coming out now, until it starts really heating up Oct/Nov with tons of awesome games coming up in Q4/15 and Q1/16."
hardware,3b518c,Seclorum,1 point,Fri Jun 26 12:52:16 2015 UTC,"Yeah cause even if the fury X is close to the Ti the Ti is going to overclock much better. Now before people go ""oh but voltage is locked!"" Without bumping the voltage on the Ti it still overclocks much better than the fury X. And scales like a mofo when overclocked. I think most people spending around 700 bucks on a video card will be overclocking too. I mean if you aren't then shame."
hardware,3b518c,Flabbyflamingo,9,Fri Jun 26 07:04:17 2015 UTC,"I think most people spending around 700 bucks on a video card will be overclocking too. I mean if you aren't then shame.    Some people just want a drop in solution, no monkeying around.   It is a shame but that's reality."
hardware,3b518c,MrPoletski,11,Fri Jun 26 16:24:14 2015 UTC,One of those is me. I just want to play my games at glorious 1440p just by dropping it in.
hardware,3b518c,Flabbyflamingo,3,Fri Jun 26 04:03:09 2015 UTC,"You should probably carefully insert it, bro."
hardware,3b518c,thejshep,5,Fri Jun 26 05:42:43 2015 UTC,She likes it rough.
hardware,3b518c,Syn246,5,Fri Jun 26 06:06:11 2015 UTC,Then those people will get a factory overclocked card - something AMD also doesn't offer. Look around at how people react when you try to compare the GI 980ti to the Fury - AMD guys start crying foul - it's not a fair comparison... But it really is - it's a drop-in solution that's in the same price range as the Fury X but beats it by big margins (over 20% sometimes).
hardware,3b518c,w00t692,2,Fri Jun 26 07:46:48 2015 UTC,"Agreed. Non-reference 980 Tis exist in the world. You can buy them with money and use them in your computer today. You cannot buy a non-reference Fury X. Too bad, but that doesn't mean the non-reference 980 Ti suddenly ceases to exist."
hardware,3b518c,makar1,0,Fri Jun 26 07:51:54 2015 UTC,"I can understand that, but at the same time, it's literally free performance, that the card was designed to do, and if you don't take advantage of it, that's your own damn fault.  on literally ANY 980 ti you could possibly purchase, bumping the power limit, altering the fan curve and pushing the core by +100 and mem by +250 will work and will net a solid 10% performance gain (possibly more depending on chip binning and how high it boosts to)"
hardware,3b518c,Seclorum,1 point,Fri Jun 26 10:14:21 2015 UTC,Got my reference 980Ti to 1450MHz so far. I'm sure it will go a little higher as I haven't had a crash yet.
hardware,3b518c,namae_nanka,1 point,Fri Jun 26 16:10:39 2015 UTC,Well sure.   But there are those people who dont want to mess around with it. And they full well know about what they could 'gain' and dont care.
hardware,3b518c,TehRoot,1 point,Fri Jun 26 12:10:59 2015 UTC,"Depends on which voltage Ti runs on, I was surprised to see 980 running on higher voltage than 290X while boosting."
hardware,3b518c,spikey341,1 point,Fri Jun 26 15:22:50 2015 UTC,AMD cards never really overclock well without some type of core voltage bump.  +100 is pretty good without that voltage access.
hardware,3b518c,FapFlop,1 point,Fri Jun 26 15:43:12 2015 UTC,"i'm waiting until they unlock the voltage. i mean, it comes watercooled. overclocking must be good right?"
hardware,3b518c,Lurkermostofthetime,1 point,Fri Jun 26 15:57:55 2015 UTC,It'll be here Thursday.  ..along with a new 144Hz monitor  ....and an R5 Windowed...
hardware,3b518c,Seclorum,14,Fri Jun 26 10:50:15 2015 UTC,Such honesty. Now how about tell me when is the next price cut?
hardware,3b518c,MrPoletski,9,Fri Jun 26 16:47:09 2015 UTC,"It just came out.   Expect 3-4 months before a price cut of the Fury line.   You also have the straight Fury, Nano, and Dual Fury coming."
hardware,3b518c,BootyCrab,8,Fri Jun 26 14:20:10 2015 UTC,"Fury, Nano, and Dual Fury coming.    Dual Fury? called the Dury?"
hardware,3b518c,Sofaboy90,18,Fri Jun 26 02:36:39 2015 UTC,"Two fast, two Furyos."
hardware,3b518c,Seclorum,2,Fri Jun 26 05:41:11 2015 UTC,http://imgur.com/5Eko2rG
hardware,3b518c,SomniumOv,3,Fri Jun 26 07:47:53 2015 UTC,"Fury, Nano, and Dual Fury coming.    2 Fast 2 Furious :)"
hardware,3b518c,FuckingIDuser,-2,Fri Jun 26 11:25:45 2015 UTC,"No, the Furry."
hardware,3b518c,RASQ37,-2,Fri Jun 26 14:03:00 2015 UTC,Tail buttplug included for 16k entertainment.
hardware,3b518c,MrPoletski,16,Fri Jun 26 15:45:42 2015 UTC,I don't understand why everyone is disappointed. It's almost as good as the 980ti ffs. 4GB is enough 99% of the time. It probably wont be long before the price drops $50 or so and then it will be a great card.
hardware,3b518c,thoosequa,5,Fri Jun 26 08:49:37 2015 UTC,"It's already a great card, just not quite as great as the masses had been lead to believe."
hardware,3b518c,gumol,11,Fri Jun 26 10:08:48 2015 UTC,"The disappointment comes from the promises. When the card was announced at E3 Lisa Su said it would be the fastest GPU in the world. A few days later first benchmarks from AMD were released showing absolutely stunning numbers. First rumors came up that the Fury X was a Titan X competitor and the Fury a 980ti competitor. At one point it was even rumored the card leaps in front the Titan X.  When it was first released numbers showed that it stayed well below the Titan X and can barely beat the 980ti. The tl;dr version of the whole situation is: massive hype train crashed, expectations were higher than the results.  AMDs next hope is the Fury, since there is no information about it we don't know what it's like. However if it represents the same specs as the Fury X, just minus the water cooling unit then we're in for a good surprise. It would undercut the 980ti by 100$ while performing in a similar range and allowing aftermarket coolers, which the Fury X does not."
hardware,3b518c,RASQ37,7,Fri Jun 26 10:44:29 2015 UTC,"When the card was announced at E3 Lisa Su said it would be the fastest GPU in the world.   I think she meant the double GPU card. Nvidia doesn't have any Maxwell double GPUs, so I guess her point is valid."
hardware,3b518c,MrPoletski,6,Fri Jun 26 05:41:30 2015 UTC,Believing rumors = disappointment.  Don't ride the hype train. It always crashes.
hardware,3b518c,ault92,4,Fri Jun 26 11:27:11 2015 UTC,Don't ride the hype train. It always crashes.    must be overclocked...
hardware,3b518c,RiffyDivine2,0,Fri Jun 26 06:58:56 2015 UTC,"AMD fired that hype train and overloaded the engines trying to push it harder.  Now everyone is disappointed, they would have been better off just releasing it or just being honest."
hardware,3b518c,Blubbey,1 point,Fri Jun 26 09:50:58 2015 UTC,With all there troubles they needed this to be a win for them so hype the shit out of it and hope for the best.
hardware,3b518c,DHFearnot,3,Fri Jun 26 07:08:53 2015 UTC,"A few days later first benchmarks from AMD    Don't believe company released benchmarks ever, they only show themselves in the most positive light."
hardware,3b518c,thoosequa,-4,Fri Jun 26 11:27:50 2015 UTC,"Imagine how hard it was for her to put on a fake smile holding the fury gpu knowing how much worse all around than the 980ti it was. In her head thinking ""Our best attempt still falls short... I've sold any dignity I had trying to pass this off as the next great thing, I hope they fall for it."""
hardware,3b518c,SHEADYguy,7,Fri Jun 26 07:16:21 2015 UTC,"What are you talking about? Across all recent reviews the Fury either hits a few frames below or a few frames above the 980ti? It is not a bad GPU, it is not crushed by it's Nvidia competitor and it certainly does not fail in any areas.   The reason why there was so much disappointment was because the community thought that HBM would leap AMD in front of the Titan X. If that would have been the case, Nvidia could have packed their shit. Say 10% more performance by 2/3s of the price, that would have been stunning, but it didn't happen. That's about it. It's not a bad card and it doesn't perform much worse then the 980ti."
hardware,3b518c,terp02andrew,1 point,Fri Jun 26 12:04:17 2015 UTC,"Also everyone is ignoring how much of an improvement this is over Hawaii, especially in perf/watt"
hardware,3b518c,RiffyDivine2,2,Fri Jun 26 13:59:36 2015 UTC,I wouldn't say especially: It's amazing specifically BECAUSE of the perf/watt :p
hardware,3b518c,namae_nanka,-1,Fri Jun 26 08:56:30 2015 UTC,So if it's pretty much a level playing field between the two cards wouldn't that put nvidia in the lead just from the gameworks tech stuff or does AMD also have something like it?
hardware,3b518c,Charwinger21,2,Fri Jun 26 09:12:36 2015 UTC,"They do, but it wouldn't help.  http://cdn3.wccftech.com/wp-content/uploads/2014/09/tfx_tr_perf.png"
hardware,3b518c,RiffyDivine2,2,Fri Jun 26 13:40:09 2015 UTC,"AMD has equivalents (many of which are open), and have announced that they are going to be pushing them harder in the future."
hardware,3b518c,thoosequa,2,Fri Jun 26 15:21:54 2015 UTC,"Well maybe in the future it will help with the shrinking market shares, hope that future is sooner than later."
hardware,3b518c,RiffyDivine2,1 point,Fri Jun 26 12:03:10 2015 UTC,Nvidia AMD has Tress-FX which is basically the Gameworks equivalent
hardware,3b518c,RiffyDivine2,1 point,Fri Jun 26 15:21:00 2015 UTC,You mean AMD?
hardware,3b518c,defensivedig,2,Fri Jun 26 12:21:13 2015 UTC,That's what marketing is in a nutshell.
hardware,3b518c,X-Ecutioner,6,Fri Jun 26 13:10:32 2015 UTC,"People are disappointed because this card was supposed to be AMDs lifeline, and was gonna destroy what Nvidia had. It turns out to quite a bit slower than the 980ti at anything below 1440p and 4k, without the same overclocking potential, and without all the Nvidia exclusive features that are in all the AAA releases now. For a card thats $650 4GB of VRAM being enough ""99% of the time"" isn't good enough when theres a card with more features, better overclocking headroom, and 6GB of VRAM which is enough all of the time.  It's not an awful card, just a disappointing card becuase its not as good as what Nvidia can provide and AMD really needed something that was better."
hardware,3b518c,defensivedig,9,Fri Jun 26 12:14:54 2015 UTC,"People are not disappointed, they are just fanboys who think that AMD cards suck if they dont beat Nvidia by 50% margin. This new lineup is bloody good but if half of the gaming scene lives in a green house, it doesnt matter how good those AMD products are. I currently have 970 and Im really disappointed for its drivers and performance considering how my 290 worked before. Fury is almost 100 cheaper here than 980Ti with liquid solution so its pretty nobrainer for me. Im not going to pay extra cash for that 5 fps difference. ""More features"" yeah cool hairworks that rapes your fps and physx which most of the time makes no difference."
hardware,3b518c,thoosequa,6,Fri Jun 26 13:09:52 2015 UTC,"At least in the US, the Fury X is the same price as the 908TI, and is slightly slower. And even if you dont care about physx, hairworks, shadowplay, and whatever other software Nvidia provides that I can't remember right now, doesn't mean other people won't. There are also more and more games that run significantly worse on AMD cards than Nvidia cards, so that slight performance difference becomes a lot larger there.  And people sure as hell are disappointed. Look at the rest of this thread! And every other thread on the Fury X! Half the people here are disappointed!"
hardware,3b518c,Mr_s3rius,5,Fri Jun 26 12:01:25 2015 UTC,"more and more games that run significantly worse on AMD cards than Nvidia cards   There's a reason for that, and the reason is not AMD cards being so much worse than Nvidia cards. There have been confirmed reports in the past that Nvidia pays developers to cripple performance on AMD GPUs. If you're going to give Nvidia close to 100% of the market share this issue is not going to resolve.   And people sure as hell are disappointed. Look at the rest of this thread! And every other thread on the Fury X! Half the people here are disappointed!   That's because half the people are misinformed. I see people constantly saying how it is so much worse than the 980ti, when it really isn't. It wins some it loses some but overall it's neck on neck with the 980ti."
hardware,3b518c,thoosequa,4,Fri Jun 26 06:57:48 2015 UTC,"There have been confirmed reports in the past that Nvidia pays developers to cripple performance on AMD GPUs.   Source? Because up until now I've only heard rumors.  Not gonna say that they don't, but if you say you know of confirmed instances, please put them out here."
hardware,3b518c,Mr_s3rius,1 point,Fri Jun 26 07:19:08 2015 UTC,"Rather than re-linking the entire compendium /r/AMD has carried together bits and pieces of evidence, that form together a rather big picture. I'll re-link part of their wiki here.  Edit: also this, unsure if this is in their wiki https://www.youtube.com/watch?v=fZGV5z8YFM8&feature=youtu.be&t=36m43s"
hardware,3b518c,TehRoot,5,Fri Jun 26 07:30:26 2015 UTC,"Well, that's unfortunate. No proof, just accusations and rumors of the same old stuff.  I had hoped something proper might've turned up - like a leaked contract between Nvidia and game devs."
hardware,3b518c,thoosequa,1 point,Fri Jun 26 09:34:08 2015 UTC,These contracts have enough lawyers on them that leaking them is bad juju.
hardware,3b518c,Mr_s3rius,-1,Fri Jun 26 09:59:18 2015 UTC,"No one would leak a contract, too risky given that you'd be sued into the depths of hell. Even an anonymous tip could probably be backtracked because there's just a number of people signing such contracts, and it's probably not gonna be the new intern at Crytek. I've found the interview with Richard Huddy in combination with the Crysis 2 video very convincing.   And while it's not open sabotage or illegal, it's still unethical that all of Nvidias APIs are not being disclosed with AMD so they can tune their drivers for it, while AMD APIs are disclosed to all involved developers, so Nvidia can adjust their software according to it.  I'm currently running a GTX GPU myself, but given the shady business tactics and unethical conduct Nvidia provided I would really like to switch to an AMD product. This of course only works when AMD can show some real competition, so fingers crossed for the standard Fury."
hardware,3b518c,thoosequa,3,Fri Jun 26 10:12:06 2015 UTC,"No one would leak a contract, too risky given that you'd be sued into the depths of hell.   The same could probably be said of a bunch of stuff available on wikileaks.  Of course I understand that these contracts are closely guarded, but they're also the ultimate source of evidence and would show us how deep the rabbit hole actually goes.  Without any hard evidence, the only thing left are theories and rumors. And although I like Huddley a lot as a person, I'm not going to believe him unconditionally either.  That being said, I'd be surprised if Nvidia didn't try to manipulate the market with -at best- questionable means. But I'd also bet money that some of these instances of supposed sabotage are just developers fucking up or being lazy, and that there are other instances of actual sabotage that we've not heard a single word about.   (Going off on a tangent: some of the allegations on the wiki are utterly laughable. ""Markets their cards as fastest, gets beat by something half the price"" is not competitor sabotage? ""Intel hired Anita Sarkeesian""???)"
hardware,3b518c,Dullahan3470,1 point,Fri Jun 26 10:30:18 2015 UTC,"According to this benchmark it wins some and loses some, as expected. Neck on neck."
hardware,3b518c,w00t692,-7,Fri Jun 26 10:52:19 2015 UTC,"If you're running this kind of GPU on a 1080p set up, you're a fucking moron anyway.  It's a 4K gaming card.  Those are the benchmarks that matter.  If you want to do 1080p or 1440p gaming, there are better, cheaper options for it.  Unless you're one of those crazies trying to do 144 FPS gaming.  In which case, I hope you have a water cooled 5960x or you're going to have a bad time."
hardware,3b518c,Dullahan3470,2,Fri Jun 26 10:37:15 2015 UTC,"No, it's not a 4k gaming card.  Unless you consider average framerates below 60 fps in literally every game to be a ""solid performer"".  It's a perfect 1440p solution at best."
hardware,3b518c,w00t692,1 point,Fri Jun 26 11:22:16 2015 UTC,"You turn down the graphics slightly to maintain 60, or you enjoy a very playable 40-50 FPS experience."
hardware,3b518c,Dullahan3470,1 point,Fri Jun 26 12:54:33 2015 UTC,"Or, since you're gaming at 4k, you purchase 2 of these and you play at 60 like you're intended."
hardware,3b518c,w00t692,1 point,Fri Jun 26 13:26:52 2015 UTC,"That's also an option, but don't underestimate the quality of an experience you can get at 4K with single GPU options.  Especially such high end ones.  You can make very minimal sacrifices and see huge leaps in framerate in a lot of games."
hardware,3b518c,Dullahan3470,1 point,Fri Jun 26 07:27:00 2015 UTC,"I know how i feel about experiences where the average framerate isn't at least 60.  They suck, they suck in nearly every single game i've ever played that did that."
hardware,3b518c,w00t692,1 point,Fri Jun 26 12:21:27 2015 UTC,"You can get an average of 60 FPS experience at 4K though, that's what I'm saying.  I ran a single 980 for a few months at 4K before picking up a second one and it was pretty easy to get a quality 60 FPS experience without making the game look like shit.  In battlefield 4 turning ""Lighting quality"" and ""effects quality"" from very high to high doubled my framerate and I couldn't notice any significant visual difference."
hardware,3b518c,defensivedig,1 point,Fri Jun 26 15:52:56 2015 UTC,"This is called compromise.  Some games take more of this than others.  Considering bf4 is a shooter, you wouldn't want it to drop below 60 fps literally ever ever."
hardware,3b518c,Dullahan3470,1 point,Fri Jun 26 16:00:04 2015 UTC,"Even at 4k though. It's almost exactly the same as the 980ti except in a few games which its worse in, but its missing those Nvidia specific features, and it has less VRAM, which is pretty much OK now, but 6 will last a lot longer."
hardware,3b518c,spikey341,1 point,Fri Jun 26 16:15:15 2015 UTC,"By the time anything actually requires 6 GB of Vram, the 980Ti won't be able to run it well anyway.  You'd need SLI to really benefit from it.  Most games only truly need around 2.5-3 GB right now even at 4K.  They will use extra available vram (most tend to use up to around 5 GB max, much to the TitanX's disdain)  The whole lack of knowledge around how video memory works is really annoying. (Like reviewers playing a game and saying ""OH MUH GAWD IT USES 4 GB OF VRAM ONLY AT 1080p"". )   Devs work within a memory budget.  The vast majority of the market is still rocking 2 GB or 3 GB cards.  The only time you can even exceed it is with lots of MSAA. (I've seen 8x MSAA use 2.5 GB of extra VRAM at 4K...)"
hardware,3b518c,Dullahan3470,1 point,Fri Jun 26 16:16:45 2015 UTC,"however we're starting to see more ports using the 5gb or so of shared console vram, doing the same on pc. GTA V, for instance, and once you factor in the increased resolution on pc it starts to push over our old 3gb cards and into-and-over the 4gb territory. Not saying it's a good thing, just what developers are going to default to."
hardware,3b518c,RiffyDivine2,1 point,Fri Jun 26 16:22:04 2015 UTC,"GTA V with everything except MSAA enabled, including extended draw distances, FXAA etc uses 4 GB of VRAM at 4K. Took this a few seconds ago.  Consoles may have 5 GB available and while devs have certainly got lazy and begun to put things they don't really need in memory, most game engines tend to stream things as needed rather than pool assets and any developer worth their salt is going to design a system that will only require the minimum needed and then use the extra vram to store extra stuff.  The consoles aren't powerful enough to truly require 5 GB of VRAM, but they do use it to stream textures better."
hardware,3b518c,Schmich,0,Fri Jun 26 16:23:45 2015 UTC,"That's been my line of thinking lately, if they are on par with each other then nvidia wins because of it's tech. Doesn't AMD have some kind of counter to that tech?"
hardware,3b518c,abacabbmk,1 point,Fri Jun 26 07:39:28 2015 UTC,"The disappointment is that it's not an über card as first thought. You want the price to be as low as to be a no-brainer as well, now people don't know what to buy.  Personally I'm also pretty disappointed at two things: HBM didn't magically increase performance dramatically. AMD still hasn't been able to lower the TDP of its architecture. Basically the 980Ti can do what the Fury X can on the same power whilst still being on GDDR5.  One thing we shouldn't forget though is that the 980Ti is a really great card and AMD is almost on par with it! If eg. Nvidia had somehow failed to get out a nice 9xx series the Fury X would have seemed like an über card."
hardware,3b518c,jacksonmills,7,Fri Jun 26 16:02:06 2015 UTC,Figured as much.
hardware,3b518c,BlayneTX,13,Fri Jun 26 18:08:12 2015 UTC,So a company goes out of its way to tell people that any hopes of their card being better than reported are misinformed? ...Not sure how to feel about this one.
hardware,3b518c,epsys,99,Fri Jun 26 18:24:34 2015 UTC,Be happy that they're honest about it.
hardware,3b518c,StealthGhost,1 point,Fri Jun 26 12:09:25 2015 UTC,still going to wait for Anandtech's review.  I feel they still have a reputation for some level of honest testing that would sway me in this war where we've got a few on both sides of the isle
hardware,3b518c,epsys,4,Fri Jun 26 11:46:15 2015 UTC,The numbers are up but the review is on hold.  Ryan was/is sick.  http://www.anandtech.com/bench/product/1513?vs=1496
hardware,3b518c,jacksonmills,4,Fri Jun 26 02:32:21 2015 UTC,"pretty comprehensive, thanks  kinda funny how .. we don't really need the article now"
hardware,3b518c,Kinaestheticsz,-2,Fri Jun 26 02:10:10 2015 UTC,I think that is one of the conflicting emotions here. Not sure if it is going to prove dominant.
hardware,3b518c,SirCrest_YT,19,Fri Jun 26 02:24:04 2015 UTC,"Honesty is something that you don't find much at all in the corporate environment. Especially from a PR standpoint.  I'd echo what  /u/BlayneTX said. Be happy that they actually took the high road and were honest, even if to a slight detriment.  Because if they lied, and were found out later on, they would be in a FAR FAR deeper rut than they are now.  If you have conflicting emotions on this, then honestly you need to reevaluate your own bias."
hardware,3b518c,logged_n_2_say,6,Fri Jun 26 06:36:48 2015 UTC,And the internet WOULD have figured it out. This was probably the safest approach.
hardware,3b518c,jacksonmills,2,Fri Jun 26 08:31:08 2015 UTC,"Personally Id rather be honest, than look incompetent. The ""drivers"" wouldn't have saved the cards unless they royally fucked up.   Meaning a driver version a week older isn't going to be drastically different. Unless they have time to really get into it, the changes are going to be small steps this near launch."
hardware,3b518c,glr123,0,Fri Jun 26 09:00:52 2015 UTC,Emotion 1) I am happy they are honest. Emotion 2) I am disappointed this card is not as good as the hype made it out to be.   Pretty simple in terms of conflicting emotions. No bias involved.
hardware,3b518c,logged_n_2_say,2,Fri Jun 26 03:32:29 2015 UTC,"No, they said that there weren't different driver versions out there. If you actually go through that thread linked and read the damn thing he says ""Wait and see what is around the corner"" and other things. In other words, they are continuing on development.  If you look at the 290X, it had somewhat lackluster performance on day 1 too. Now, it can go toe to toe with the 980 in some cases.  People need to just fuckin chill out! They are all up in arms over a very competitive cards and are blowing everything out of proportion. Of course the card will get better, that's how this stuff works."
hardware,3b518c,glr123,2,Fri Jun 26 03:49:40 2015 UTC,"I think people are just exhausted from soonTM and ""wait till.""  The omega drivers didn't come out till over a year after the 290x, right?  Fury is still a good card, they just over promised too early."
hardware,3b518c,logged_n_2_say,3,Fri Jun 26 04:02:24 2015 UTC,"But they hardly promised anything, other than standard marketing rigmarole. It was people that hyped it up to ridiculous levels."
hardware,3b518c,abacabbmk,1 point,Fri Jun 26 12:26:16 2015 UTC,"any sort of leak is marketing, and even the official amd benchmarks were pretty far off from typical real world testing.  if they knew it was going to be ~ a 980 ti performance, the message should be ""980 ti performance but value in water cooler and newest memory bandwidth.""  instead we got ""most advanced gpu"" and ""overclockers dream"" and hype hype hype.  i think the gutting of the pr department has come back to bite them slightly and hindsight is always 20/20, but i think they backed the wrong message.  a value message would have carried better, since it's closer to the truth.  not to mention, i think they missed a big opportunity by launching the wrong card first.  if fury vanilla is the same on air for $100 less, go with that first and refine the clocks and overclocking on the fury x for a later launch.  the value message would be clear as bell."
hardware,3b518c,Eruntien,1 point,Fri Jun 26 13:41:25 2015 UTC,They dont want potential consumers to sit on the sidelines waitng for 'real' benches. Or have them expect better benches then be disappointed again when what they see is what they get. Kicking the can down the road is only going to piss off amd fanboys who are holding out for more information. Everyone else has already made their choice.
hardware,3b518c,mmencius,2,Fri Jun 26 07:25:40 2015 UTC,"IMO the Fury is looking to be a much better card as for many peopl it is 200 cheaper (in aus 100 by the looks) which will is huge saving, we also have to consider jow freesync is 200 cheaper on average so someone upgraing there monitor and gpu is incentivised to go amd as those are big savings."
hardware,3b518c,Meretrelle,2,Fri Jun 26 12:23:38 2015 UTC,I love the Santino avatar.
hardware,3b518c,ptd163,1 point,Fri Jun 26 15:12:47 2015 UTC,"I wonder how this ""rep"" explains this difference   Battlefield 4  logs FTP Driver Results 130,3 106,2 72,2 34,8   IXBT's Driver 140,3 121,0 84,6 55,6  Well, when the new driver comes out we'll see what's what.."
hardware,3b518c,Cueball61,1 point,Fri Jun 26 15:47:26 2015 UTC,"This the probably the worst possible scenario. AMD Fury X is hyped up as AMD's saviour. Something that can finally compete with Nvidia in that premium high end market. However, from reviews and independent benchmarks all they did was hang on for dear life. Which is not good for anyone except Nvidia... and maybe Intel.  I respect their honesty in this matter."
hardware,3b518c,ChRoNicBuRrItOs,-3,Fri Jun 26 12:54:38 2015 UTC,"Talk about grabbing the knife and twisting it into yourself.  The driver issue was a bit of a saving grace for them here, and they've now gone and debunked it. This statement probably sold some 980Tis."
hardware,3b518c,RiffyDivine2,2,Fri Jun 26 14:44:10 2015 UTC,At least they're honest. That's pretty rare these days.
hardware,3b6gc7,ainsey11,2,Fri Jun 26 10:30:28 2015 UTC,"Looks awesome! Though it's pretty damn large for even a M-ATX case. It's shorter than most ATX cases by a few inches but just as long and just as wide, if not more.  $220 USD is pretty steep price too... Though, if I had that budget for a case and I was looking at a fully customizable fully featured M-ATX case, THIS is what I'd be going for. Not to mention that it looks absolutely stunning."
hardware,3b6gc7,Stiev0Kniev0,1 point,Fri Jun 26 16:29:52 2015 UTC,That GPU sag kills it for me. I think I'm going to get a case with a horizontal mobo tray so I never have to worry about it.
hardware,3b6gc7,BlayneTX,1 point,Fri Jun 26 22:51:38 2015 UTC,Forgive my ignorance but isnt this basically a DIY EVGA Hadron Air?
hardware,3b6gc7,iamstephen1128,1 point,Sat Jun 27 15:15:41 2015 UTC,"No. It's MUCH larger, and has more room for radiators. I think the EVGA Hadron has a single 240mm radiator? This has room for at least two."
hardware,3b66jj,antdude,16,Fri Jun 26 08:07:07 2015 UTC,Did I just see The Guardian punctuate a piece with a reaction gif?  The Guardian?
hardware,3b66jj,Ponkers,16,Fri Jun 26 10:00:53 2015 UTC,but please donate if you value our quality journalishms :^)
hardware,3b66jj,39th_Step,6,Fri Jun 26 11:49:15 2015 UTC,gotta get the clics thanks to dank memes!
hardware,3b66jj,u-r-silly,8,Fri Jun 26 11:58:06 2015 UTC,"Seriously, the intensity of the radio waves a router uses are weaker than that of an incandescent lightbulb. People need to chill out and read a book..."
hardware,3b66jj,III-V,6,Sat Jun 27 00:42:53 2015 UTC,I had to double check it wasn't from the onion but I guess these days you can shove any product if you wrap it in thin layer scientific bullshit presented in a user friendly manner.
hardware,3b66jj,irwiss,2,Fri Jun 26 13:37:29 2015 UTC,"Back when I was in high school, a neighboring school had an outbreak of mass hysteria where lots of kids were convinced the routers were making them sick. Of course the symptoms were random from kid to kid and occurred only if they could see the router (if the router was hidden and on, no symptoms. If they could see the router but weren't aware it was off, symptoms.) I'll bet my left nut if this product was on the market a couple years sooner, the schoolboard would have bought these up to placate the helicopter parents"
hardware,3b66jj,AssCrackBanditHunter,1 point,Sat Jun 27 12:03:59 2015 UTC,features three settings: “wall penetration”   Like glory holes?
hardware,3b66jj,iLike2Teabag,-10,Fri Jun 26 13:33:10 2015 UTC,"Unpopular opinion, but I still dont think we fully understand the effects of a lot of things, including wireless frequencies. If you pay attention the scientific community has flipped and flopped for years over things, how long did we use lead paint, bpa, cigarettes were once considered healthy, etc.  Im not one to give up technology though, but i think its fair to say there is a possibility it could have some adverse affects (doesn't have to be a direct relation to cancer)"
hardware,3b66jj,Put_It_All_On_Blck,10,Fri Jun 26 08:46:27 2015 UTC,We've done extensive studies on ionizing and non-ionizing radiation and its effect on cells. It is safe. The other things you mentioned were considered safe because of a lack of scientific data. There is no lack with radiation of all kinds.
hardware,3b66jj,foxtrot1_1,1 point,Fri Jun 26 16:19:33 2015 UTC,Old guy here.  I remember back in the 50's that you could go into a shoe store and put your foot under an xray machine and look at the bones of your foot on a display.  We kids used to play with it and look at our hands when our parents were looking for shoes.
hardware,3b66jj,Slamdunkdink,2,Fri Jul 3 22:16:21 2015 UTC,Here's a good video explaining why were pretty certain https://youtu.be/wU5XkhUGzBs  enjoy
hardware,3b66jj,Spidertech500,3,Fri Jun 26 18:32:58 2015 UTC,There's a major difference between unpopular opinion and unsubstantiated opinion.
hardware,3b66jj,zjm7891,4,Fri Jun 26 18:35:02 2015 UTC,aka fear mongering
hardware,3b6wij,Arthur_C,1 point,Fri Jun 26 13:33:45 2015 UTC,TL;DR — a prototype is a product before mass production. You can use 3d printing to make one.  Did I miss something?  BTW - this would do better in r/tech
hardware,3b6wij,dasiffy,0,Fri Jun 26 14:55:24 2015 UTC,"If you’re building a product with the goal to start a hardware company. Then, you should have one thing in mind and one thing only: build a product your users will love.   Grammar mistake in the first sentence... I closed the tab. Clearly wasn't proofread :/"
hardware,3b5szf,Potatriarchy,16,Fri Jun 26 05:11:13 2015 UTC,"Because you cannot control what get's cached on the tiny Solid State portion of the drive.   Without that kind of control you end up only being able to rely on the regular HDD read and write rate.   The SSD isn't big enough to matter, and the HDD isn't fast enough to justify the expense."
hardware,3b5szf,Seclorum,5,Fri Jun 26 05:37:22 2015 UTC,"This, as someone who has a SSHD in my laptop(it came with it, planning to upgrade to ssd soon) the speed differance vs HDD is not noticable, the only reason this laptop boots and behaves faster than my previous one is because my previous one was a vista laptop and was powered by a core 2 duo."
hardware,3b5szf,olavk2,2,Fri Jun 26 07:50:28 2015 UTC,"My dads new laptop has a SSHD and it really is not any faster than a normal HDD, I did speed tests and it was only a tiny bit faster."
hardware,3b5szf,ScottieNiven,8,Fri Jun 26 10:24:16 2015 UTC,"By the sound of it, it's the usual problems with most hybrids. Nissan's CEO used to say hybrids were like mermaids: when you want a fish you get a woman, and when you have a woman you get a fish.  Basically on some hybrids (not all), you get inherit the disadvantages of both sides and not the advantages. In this case: still slow (HDD disadvantage) whilst being smaller in size (SSD disadvantage - compared to full HDD), more expensive (SSD disadvantage) and still have a mechanical drive than can break when moved."
hardware,3b5szf,Schmich,1 point,Fri Jun 26 11:50:50 2015 UTC,"Nissan's CEO used to say hybrids were like mermaids: when you want a fish you get a woman, and when you have a woman you get a fish.   That's such a good line."
hardware,3b5szf,SirCrest_YT,1 point,Sat Jun 27 20:15:10 2015 UTC,"I bet the board of directors wasnt too thrilled with that CEOs analogy considering Nissan sells gas electric hybrid vehicles... Unless that was before they sold hybrids and he pulled a Steve Jobs (deriding a technology out class of products until Apple is ready with their own, than promoting the device as the nearest and most innovative thing ever)"
hardware,3b5szf,ScottLux,2,Fri Jun 26 16:29:14 2015 UTC,"A SSHD would be expected to not cache the data in a benchmark run, it would behave very badly otherwise. It caches data that is accessed repeatedly over time."
hardware,3b5szf,narwi,1 point,Fri Jun 26 15:20:00 2015 UTC,"So size and control of the solid state component was really just the biggest problem then. That's what I assumed. So do you think that with those two things fixed, SSHDs would potentially beat out HDDs and SDDs? Or are they just naturally an inferior technology?"
hardware,3b5szf,Seclorum,3,Fri Jun 26 15:30:43 2015 UTC,"Really it's a stopgap tech that cant compete against either of it's basic parts.   Why get such a small SSD when there are plenty of cheap options that give you much more space so you can effectively use it as something other than the most pared down OS only drive?  And for HDD, you can get better drives with greater density at cheaper prices."
hardware,3b5szf,Teethpasta,2,Fri Jun 26 15:40:33 2015 UTC,"Ah I see. Good points. It basically would just be much more cost-effective to get a normal SSD and a separate hard drive instead of an SSHD. I assume good, high quality SSHDs would have to be purchased from specialized vendors since no big company would want to invest in those fixes anyway. And not to mention actually fixing the issues with SSHDs would definitely make their already high prices rise even further to the point where it would stop being worth it."
hardware,3b5szf,headband,4,Fri Jun 26 15:50:11 2015 UTC,By buying an actual ssd.
hardware,3b5szf,hans_ober,2,Fri Jun 26 08:07:16 2015 UTC,"Yeah SSDs are definitely better right now. But I still would like to see companies try to actually improve SSHD tech since it likely COULD have the potential for pretty fast mass storage if the big corporations had their technicians pull their heads out of their asses and actually give them decent solid state components. And of course if the absurd price tag went down, I would assume a lot more people would be willing to buy SSHDs."
hardware,3b5szf,hans_ober,2,Fri Jun 26 15:22:08 2015 UTC,"It could have been something in like 2008 or so, but now it's way too late to the party."
hardware,3b5szf,Cueball61,3,Fri Jun 26 17:00:17 2015 UTC,"Someway to control whether the SSD works in cache or storage mode.  I'd say bump up the SSD to ~64GB, and have an option to somehow give it it's own partition for a Windows/ essential apps install. The PCB will be a bit bigger, but it save a port."
hardware,3b5szf,hans_ober,3,Fri Jun 26 07:56:53 2015 UTC,"Exactly what I have been thinking! The main issue just seems to be that SSHDs have such puny solid state components that they just barely qualify as ""hybrids""."
hardware,3b5szf,wretcheddawn,2,Fri Jun 26 15:23:59 2015 UTC,"Yeah, the current arch just uses the flash as a small 'cache', and the user has no control over what gets cached.  Sure, its bigger, better & faster more useful than the 32/64MB DRAM cache on a normal HDD, but 8GB is too small for it to be useful really useful.  Combine 64/128GB NAND (make the PCB a bit bigger) with a ~500GB HDD and you've got a deal. Pre-partition the SSD and HDD, so OEM's & users can tell them apart and install the main OS on the first part.  EDIT: Now that I think of it, it doesn't seem very nice compared to the N2 /M2 form factors. It would be better for the SSD and HDD to have their own SATA ports for performance (queuing etc). Would have been a good idea a few years back, but now laptop makers should add a small form factor SSD port (can't quite remember it) + SATA for HDD."
hardware,3b5szf,JimJamJamie,1 point,Fri Jun 26 17:16:43 2015 UTC,"Western Digital came out with something like this. It's pretty expensive though, and doesn't work as a cache I don't think,must two drives in one (it's designed for laptops)"
hardware,3b5szf,TheBloodEagleX,1 point,Sat Jun 27 09:44:51 2015 UTC,"Yeah, I read about that too. IIRC, you needed to use their software to configure the artitions etc. I think Anandtech reviewed it."
hardware,3b5szf,krista_,2,Sat Jun 27 11:58:20 2015 UTC,"They're poorly designed.  The SSD isn't big enough to offer any real performance improvement, and many times the HDD is too small too.  They're also too expensive for what you get.  They could be fixed by using larger SSDs in the 64-128GB range, but that would drive the cost up even more, or by just buying an SSD."
hardware,3b3xts,Liber_Vive,35,Thu Jun 25 19:41:00 2015 UTC,"Here's the actual source for why:  http://investorshub.advfn.com/boards/read_msg.aspx?message_id=86312251  Excerpt:   Secondly, there were indeed some problems identified with Haswell's integrated VRs when silicon came back over a year ago. You know who found those problems? The team in Israel. I've been told that the folks on that team do not like some aspects of the IVRs on HSW (I don't know if it's the implementation, or the whole concept, or whether they doubt the ROI, or what). One of the benefits of having two world class design teams at your disposal is that they are always pushing each other, always questioning each other, not to mention the friendly competition that always goes on."
hardware,3b3xts,III-V,6,Thu Jun 25 23:49:59 2015 UTC,Oh man someone actually came out and said it.
hardware,3b3xts,Th3Loonatic,31,Fri Jun 26 08:22:44 2015 UTC,"From what I read, it was heat.   But the funny thing is that I've read that the TOCK after cannonlake is supposed to bring it back on die anyway."
hardware,3b3xts,Seclorum,41,Thu Jun 25 19:56:12 2015 UTC,"Regulators go on, regulators go off. You can't explain that."
hardware,3b3xts,meowffins,12,Thu Jun 25 20:28:35 2015 UTC,"Maybe it'll be a bone thrown to the OC crowd. Y'know, give them a generation that can overclock like Sandy Bridge did, let them coast on that for a few years, then do it again."
hardware,3b3xts,LFKhael,5,Thu Jun 25 23:43:38 2015 UTC,We can only hope...
hardware,3b3xts,bphase,5,Fri Jun 26 02:50:43 2015 UTC,"Intel Wars 4, A new hope..."
hardware,3b3xts,Seclorum,4,Fri Jun 26 03:20:36 2015 UTC,"Unlikely, the new Skylake chips will still use the same toothpaste thermal paste they have been using since Ivy Bridge"
hardware,3b3xts,roro_mush,3,Fri Jun 26 14:12:28 2015 UTC,"I would bet that properly applied toothpaste would yield better temps than their current paste, atleast for a few weeks :p"
hardware,3b3xts,Lysiticus,2,Fri Jun 26 16:47:30 2015 UTC,God Sandy bridge. Good times.
hardware,3b3xts,melgibson666,2,Fri Jun 26 10:54:23 2015 UTC,"Still good times. I have upgraded so many components--some several times over--whilst my 2600k from circa summer 2011 remains, happily operating @ 4.7ghz under 60C most of the time."
hardware,3b3xts,Syn246,2,Fri Jun 26 16:16:17 2015 UTC,"Woo yeah got the 2500k from around the same time, not even thinking about upgrading yet. Stable at 4.8!"
hardware,3b3xts,pb7280,1 point,Fri Jun 26 20:06:45 2015 UTC,Yes there really hasn't been a need to upgrade past Sandy bridge if you're just gaming.
hardware,3b3xts,melgibson666,2,Fri Jun 26 18:04:51 2015 UTC,The IVR is technically a really good thing because it provides much more accurate voltage control than an external VRM and it lower the current requirements on the motherboard VRM.
hardware,3b3xts,buildzoid,3,Fri Jun 26 14:32:06 2015 UTC,It's crazy how nature do that.
hardware,3b3xts,TRD099,3,Fri Jun 26 07:00:04 2015 UTC,if only there were something ... to regulate - .- ..... this behavior - .-
hardware,3b3xts,epsys,-1,Fri Jun 26 09:05:29 2015 UTC,"They're going back on after Cannon Lake too, lol."
hardware,3b3xts,III-V,2,Thu Jun 25 23:42:52 2015 UTC,The guy he replied to said that
hardware,3b3xts,dylan522p,2,Fri Jun 26 05:10:14 2015 UTC,Brain gets a bit fried at the end of a workday...
hardware,3b3xts,III-V,3,Fri Jun 26 11:38:31 2015 UTC,Probably the better fabs will make the heat not an issue. They just pulled the trigger on moving it on die too quick.
hardware,3b3xts,Teethpasta,2,Thu Jun 25 20:45:41 2015 UTC,"Yeah but cannonlake is still years out, they announced it's delayed due to low yields, it's gonna be worse than broadwell"
hardware,3b3xts,Jakeattack77,1 point,Tue Jun 30 08:31:45 2015 UTC,Hence why there already are rumors of a Skylake refresh coming before cannonlake.
hardware,3b3xts,Seclorum,13,Tue Jun 30 08:57:06 2015 UTC,"They produce too much heat on the die, while the die gets smaller and smaller (thus too much heat density).  I also heard about things like causing some instabilities that'd limit the frequencies, but I'm not certain.  I still don't understand why they were put on the die, then taken off, then put back, tbh"
hardware,3b3xts,MerryLane,3,Thu Jun 25 21:11:16 2015 UTC,Two different teams one does Sandy then skylake other does Haswell and whatever is after cannonlake
hardware,3b3xts,dylan522p,5,Fri Jun 26 05:11:26 2015 UTC,"As far as I know, these are all rumors. We don't really know yet whether the FIVR will be removed from Skylake."
hardware,3b3xts,Compatibilist,2,Thu Jun 25 21:51:18 2015 UTC,This. Doesn't Intel require an integrated FIVR in order to support their new power management tech? I remember them talking about switching the voltage much quicker in order to save power. Can't do that with off-die regulators.
hardware,3b3xts,JeffroGymnast,2,Thu Jun 25 22:55:30 2015 UTC,"I'm sure it does have some practical benefits as far as heat goes, particularly in laptops where chips run insanely hot and are frequently limited by heat rather than any other factor. But I also wonder if it lets Intel quote a power consumption number that exaggerates the efficiency gains compared to Haswell. I wouldn't say it's deceptive to list the power consumption of the CPU only, that just makes sense. But the power consumption without the VRMs included will obviously be lower and look better than the same figure with the VRMs included. That power still gets consumed, on the motherboard instead of in the chip, but when you look at the CPU's specs it won't contribute to the TDP. Similarly, the price of the CPUs may look lower while motherboards will end up costing more, because they need to manage voltage regulation. This could make the Skylake CPUs appear cheaper with no practical savings, or make them appear the same price when the whole setup is actually more expensive than Haswell.  I'm actually a bit disappointed by this decision as an overclocker as well. Lower heat would appear to benefit overclocking, but in Haswell there isn't as much effect from a motherboard being a ""good or bad overclocking board"" due to its VRM design. They all pretty much work the same as long as the same features and voltage levels are allowed by the BIOS. The Skylake equivalent of overclocking the G3258 on the cheapest bare-bones H81 board probably won't exist. If the current situation with AMD systems (you really need to buy a quality/expensive mobo if you want to OC) is comparable you certainly wouldn't be able to get away with that, and it makes sense. If your mobo is doing something tangible for you, especially power management when you are overclocking and increasing power demand, buying cheap crap will once again have consequences."
hardware,3b3xts,Unique_username1,-2,Thu Jun 25 21:21:56 2015 UTC,I think this is an great move!  Hopefully we can take advantage of it and get lower chip temperatures.
hardware,3b4nfx,zVitiate,7,Thu Jun 25 22:55:34 2015 UTC,Graph Format of the 4K Results  Ti/Fury X Only 4K  Because tables are hard to make comparisons on.
hardware,3b4nfx,Cueball61,2,Fri Jun 26 14:26:52 2015 UTC,Fury is really getting hammered in BF4.
hardware,3b4nfx,random_digital,2,Fri Jun 26 16:32:37 2015 UTC,"Which is pretty bad, considered that's an AMD title."
hardware,3b4nfx,BuildYourComputer,1 point,Sat Jun 27 16:38:00 2015 UTC,"Even more sad is, it only wins one round and ties or loses any other. Very very sad. I wanted competition not horse shoes."
hardware,3b4nfx,continous,5,Tue Jun 30 04:25:56 2015 UTC,I am starting to become a fan of there reviews. The videos and the overall flow of them is nice.
hardware,3b4nfx,klymen,5,Fri Jun 26 02:27:15 2015 UTC,"I'm going to sit awhile on the sidelines awaiting for voltages to be unlocked for the Fury X before making my decision. Also, the Nano seems tempting."
hardware,3b4nfx,rationis,2,Thu Jun 25 23:32:53 2015 UTC,"I'm going to do the same but I'll wait until a DX12 game comes around the corner (hopefully Deus Ex). In fact, I may just forgo the Fury X and 980 Ti until the jump to 14/16nm."
hardware,3b4nfx,aranurea,2,Fri Jun 26 00:55:42 2015 UTC,"Yea, I think I'll wait until the 8Gb version comes out, then I'll switch to 1440p/144hz. Its hard to justify swapping out my 290X for one of these when it still does extremely well in all games."
hardware,3b6ddk,mizzu704,5,Fri Jun 26 09:49:37 2015 UTC,"Devices that use unusual screws, non standard connectors, etc are just spitting in the face of consumers. People who care enough are going to get around it. But meanwhile I can't just use a normal screwdriver to open up a gamepad to clean it once in a while or get the dust out of a console. I don't know anything about repairing Apple products but I have to assume they design the devices like only a true prick would do."
hardware,3b6ddk,maybachsonbachs,7,Fri Jun 26 12:26:30 2015 UTC,"Heh, if you put the wrong screw in a particular hole on the iPhone 5s, and iPhone 6, you'll brick the phone, because they routed some critical traces under it. Luckily I can fix that with a microscope, but these things are literally the width of a human hair.   They definitely try to make them difficult to get into, by using pentalobe screws, but most of their devices are very, very serviceable. Very easy to break something of you don't know what you're doing, because they're so tiny and mechanically fragile, but once you know your way around, they're cake. Some of their older laptops were a nightmare, though (PowerBook stuff)."
hardware,3b6ddk,III-V,1 point,Fri Jun 26 12:37:15 2015 UTC,Great informative video and experience of the tech. Good luck with the bill
hardware,3b4ip0,ExplosiveTaco07,13,Thu Jun 25 22:17:05 2015 UTC,"id love to see better multi threading support for mmo/rts games.    any game with large amounts of players/units/npcs are cpu bound, because only a few cores are being used.  you could have a 980 ti with a 4790k at 5ghz and drop to 20 fps in games like gw2/ps2/skyrim/civ5/sc2    hopefully dx12 will solve these issues"
hardware,3b4ip0,e6600,1 point,Fri Jun 26 02:35:09 2015 UTC,"God GW 2 is so annoying. I have one poor core, set to full load. Poor thing just wanted to share its work, but no."
hardware,3b4ip0,continous,3,Tue Jun 30 04:28:58 2015 UTC,"Almost definitely. Right now bf4 uses all 8 of my 8320e, gtav can occasionally max out all of my threads, and the witcher also uses all 8. This is the year of new engines for games, and with that is coming engines that can scale to any number of cores. It's beautiful"
hardware,3b4ip0,AssCrackBanditHunter,10,Fri Jun 26 13:21:28 2015 UTC,"Most games are GPU bottlenecked ... So ... Okay for handling more cores, but that's not the priority. But yes, some game engines can use as many cores as they are handled, even more than 8c+8t  Also, there are some games here and there, that can use 8 cores, but that need as high as possible single thread performance (people say it's coz one thread sends all the requests to the GPU and stuff, which should be corrected in dx 12).  The thing is, if you OC a 5960x to the same frequency than a 4790k, enable/disable hyperthreading (meh still confused about it improving or not the perfs), then you outperform it, in games that love cores. But you'd need a good chip watercooled.  Also, what you seem to forget, is that nowadays you rarely ""just play"". Sometimes you have 127 pages on your browser while recording and streaming yourself, all that while listening to music and using PLEX to show your grandmother her favorite 1960 serie. That's when more cores are better."
hardware,3b4ip0,MerryLane,4,Thu Jun 25 22:27:39 2015 UTC,"Yep, that is the thing I've realized over the last few years. Single core performance continues to climb and as a result the big benefit of Multiple cores is just multitasking ability. More things open together is more smoothly interacted with.   And I love my 500 chrome tabs."
hardware,3b4ip0,SirCrest_YT,6,Fri Jun 26 04:06:53 2015 UTC,"And I love my 500 chrome tabs.   How much memory do you have? 256GB?  on a more serious note: yeah, multitasking is probably where you see the most increase in performance when getting more cores, i went from a i5-2400 to a fx 8320(overclocked to 4.5GHz) and while gaming performance hasnt improved at all, when having a bunch of tabs open and streaming while playing a game has definetly improved."
hardware,3b4ip0,olavk2,1 point,Fri Jun 26 12:15:23 2015 UTC,"Assuming that the hardware physically exists and the cost is reasonable - fewer bigger cores is better than the equivalent amount of performance spread across multiple weak cores.  Whenever you can get away with it - single-thread performance is king, and that's a fact.  There's really diminishing returns from more cores.  The switch to A64 x2 processors was godly back in the day.  4 cores, still nice for compression, gaming with stuff open in the background, etc.  8 cores, not really that useful unless you're doing high-performance computing."
hardware,3b4ip0,capn_hector,1 point,Fri Jun 26 14:32:05 2015 UTC,"given the choice I'll go for something a bit slower if it has more cores. For example, I traded my ip35-e mobo and e2180 that I had overclocked to 3.4ghz for an AMD motherboard that unlocked a Phenom2 X3 to quad core and overclocked to 3.5ghz.  Was it as fast in single threaded tasks as the dual-core e2180? No. Actually, it might have been, but go with it so I can make this point. Did I need that faster single-threading for any of my games? No, I was hitting 60fps. Ergo, I had absolutely no trouble in Unreal Tournament 3 (UE3 fantastically parallelized engine IMHO) and...that's about the last game I really actively played.  But anyways, having the quad core is cool for the future. If I still had that build I'd be fine with it, but I needed more RAM, so I built my current FX-8310 rig at 4.3ghz with 32GB RAM. Slower single threading than the Intel options, but was cheaper, and I'm set for DX12 and future 21st century architecture aka scale to > 4 threads games"
hardware,3b4ip0,epsys,2,Fri Jun 26 09:30:43 2015 UTC,"Utilizing? Sure. To the point of having noticeable performance differences, not likely.  Unless we got processors where every cache level was shared among cores, most situations where the CPU's a bottleneck (ArmA 3 being a big contender) are situations because we aren't able to parallelize without writing to L3 (If memory serves me right), main memory, or disk, which are much slower than the cache."
hardware,3b4ip0,stapler8,1 point,Thu Jun 25 23:10:17 2015 UTC,"We already have several engines that 'can' use up to 8 threads, but most of them dont actually use that many threads in practice because the majority of threads would be sitting idle waiting for something else."
hardware,3b4ip0,Seclorum,3,Thu Jun 25 23:33:55 2015 UTC,"Yup. Which is what I tried to come across with the first line.  It's not that they can't be used, it's that it's not worth parallelizing something that can just as easily be loaded onto another core with a minimal performance hit."
hardware,3b4ip0,stapler8,1 point,Fri Jun 26 02:03:07 2015 UTC,"To respond to your thread title - I think PC games utilizing more than 4 cores will be the norm as we move into the DX12 era.  I think we're shifting away from hand-rolled graphics engines and towards using big-name engines like Unreal and Unity.  Those engines have had attention paid to multithreading and SLI/CF and thus access to those attributes is going to increase.  I also agree that many games do not currently use more than 4 cores, because before Mantle/Vulkan/DX12 you were using high-level languages that did not thread well.  That translates into one thread doing most of the work, which translates to low load on the remaining cores.  However - that said, you should NOT run out and buy an AMD octa-core for gaming.  As mentioned above one core does most of the heavy lifting in games (handles game state and makes GPU calls), and you cannot just substitute in 2 weak cores for 1 powerful core while also keeping your performance.  Threading the game loop can drastically increase complexity of development, and there's only so many tasks before you start hitting diminishing returns (eg from locking or thread-fencing causing cores to idle to wait for the ""hard"" tasks).  4 cores is useful in a high-performance engine, 8 cores is plausible, but 16 cores isn't really useful in the current state of things."
hardware,3b4ip0,capn_hector,1 point,Fri Jun 26 14:27:40 2015 UTC,"Nah, GPUS are still a huge bottleneck and until that is fixed then I highly doubt it"
hardware,3b4ip0,Eruntien,1 point,Sun Jun 28 14:50:18 2015 UTC,"Nah, GPUS are still a huge bottleneck and until that is fixed then I highly doubt it"
hardware,3b4ip0,Eruntien,1 point,Sun Jun 28 14:50:39 2015 UTC,"Nah, GPUS are still a huge bottleneck and until that is fixed then I highly doubt it"
hardware,3b4ip0,Eruntien,1 point,Sun Jun 28 14:50:40 2015 UTC,"If programmers learn more about hardware, sure. Don't hold your breath though, it has taken them years to even get to 4 cores."
hardware,3b4ip0,sin0822,1 point,Mon Jun 29 05:07:41 2015 UTC,"Already happening.   I bought a haswell i3 thinking I would destroy any FX-8320 in games, but the FX actually beats the i3 in Witcher 3."
hardware,3b4ip0,Begoru,5,Fri Jun 26 00:39:23 2015 UTC,"CPU Witcher 3 GTA V Price    i3-4130 84 FPS 68 FPS $113.99 USD   FX-8350 89 FPS 72 FPS $165.93 USD    I think those are the only two games where the FX-8350 outperforms it. You have the i3-4150 which is a tad bit faster and oddly cheaper than the i3-4130 (according to PCPartPicker). Also, the performance gap is even smaller between your CPU and the FX-8320.  An i3 CPU is still a better choice now and by the time games make full use of 4+ cores, then it'll be outdated and you'd be due for an upgrade.  Sources for the FPS numbers:  http://www.techspot.com/review/1006-the-witcher-3-benchmarks/page5.html  http://www.techspot.com/review/991-gta-5-pc-benchmarks/page6.html"
hardware,3b4ip0,Anaron,2,Fri Jun 26 03:42:52 2015 UTC,"only two games   yeah, but you gotta ask what happens when you get a modernly designed game? For example, I can encode the BBB mpeg4 into Android Tablet 720p mp4 at 220fps using handbrake where those i3's are hitting about 100fps, or have to sacrifice encode efficiency and crank the bitrate in Quicksync, wasting precious phone storage space, to get similar quality.  Which is to say, when DX12 hits and we start seeing modern-threading in games, these 8 core AMDs are going to shine for a bit longer. All the consoles are 8 cores so the games are at least 6, too. NBA2k15 is a good example of a game where the i3's simply can't keep up with the throughput, I presume for the physics, of the 8 core AMD Bulldozers and Piledrivers. That's just a sign of times to come."
hardware,3b4ip0,epsys,1 point,Fri Jun 26 09:34:03 2015 UTC,"you probably have problems with mins too, unless you've got a quad core i3"
hardware,3b4ip0,epsys,1 point,Fri Jun 26 09:31:14 2015 UTC,"i3 owner here. I'll regret not getting an i5 when the time comes that games runs a lot better on more than 2 cores. We've been promised that games would use 4 or more cores for a while now, but when I bought my i3 2 years ago the price difference between it and an i5 was significantly larger than the performance difference in games, and that still holds true today.  Hopefully with DX12 quad-cores will be much faster than dual-cores, and it is then that I'll finally get a 4 core CPU."
hardware,3b4ip0,Yearlaren,0,Fri Jun 26 21:04:44 2015 UTC,"Next few years? No.   There would need to be some quantum leaps in mathematics to allow proper N-Body Physics calculations to be mufti-threaded, but beyond that there really isn't much reason for a game to use more than 4 threads, considering they barely saturate the 4 as it is. Games are more GPU limited as well as PCIe limited. Transferring data off the CPU-Ram zone and into the GPU-Vram zone needs to become much faster with lower latency."
hardware,3b4ip0,Seclorum,5,Thu Jun 25 23:32:14 2015 UTC,"Games are very rarely PCIe limited, and I use the term ""very rarely"" to be generous. Take a look at the abundance of benchmarks of latest generation flagship GPU's at x16, x8, and x4 lanes."
hardware,3b4ip0,sabot00,1 point,Fri Jun 26 02:20:36 2015 UTC,Oh true. But they do a lot to minimize how much they try and send over the bus.
hardware,3b4ip0,Seclorum,1 point,Fri Jun 26 03:13:18 2015 UTC,"By nature there's not that much information to send over the bus.  The visual scene is created in the graphics card itself, all the CPU sends over is the scene information, commands, and textures.  A PCI-E 2.0 x16 slot is 8GB/s, considering how even the biggest games currently are usually smaller than 40GB, that's less than 5 seconds to transfer the whole of the game.  The only thing that needs to be sent to the GPU in real time is small texture LoD changes, scene changes, and the rendering information, all of which is relatively small. The large textures are already loaded into VRAM."
hardware,3b4ip0,sabot00,1 point,Fri Jun 26 03:22:44 2015 UTC,"Biggest games are quickly approaching 100gb. Star citizen will be at least that big.   But that's beside the point.   One of the most common issues that crop up is exceeding the onboard Vram limit and being choked by the PCIe bus speed for loading data from the system.   But as you said, it really hasn't been that big a deal. They just shove more Vram onboard because there isn't much else to send."
hardware,3b4ip0,Seclorum,1 point,Fri Jun 26 03:37:47 2015 UTC,"and being choked by the PCIe bus speed for loading data from the system   huh? even if you had 100GBps you'd still be severely bottlenecked by your RAM. DDR3-1600 'only' does 13GBps, a far cry from the 250-500GBps of bandwidth GDDR5 has"
hardware,3b4ip0,epsys,0,Fri Jun 26 09:47:09 2015 UTC,oh my gosh you guys reallyneed to think about this more! do you honestly think the game engine transfers the entire game into the video card before playing?  All 40 GB? Have you ever looked at the GPU memory utilization in GPU-z? We're usually at 2-4GB these days. It only loads the resources it needs for a given level.
hardware,3b4ip0,epsys,1 point,Fri Jun 26 09:42:29 2015 UTC,"no, they really don't. I personally think we could have been fine with AGP for years to come given a little bit of engine tweaking. 2GBytes/second is nothing to sneeze at-- all you have to do is transfer the resources, your level loading takes 10 seconds anyhow, that's 20GB of data you can pump while you're loading-- and then just the usual commands to render this, render that...etc."
hardware,3b4ip0,epsys,1 point,Fri Jun 26 09:41:00 2015 UTC,"they barely saturate because the render pipeline is largely single threaded, and because nobody bothers coding cognisant AI-- they just script it."
hardware,3b4ip0,epsys,0,Fri Jun 26 09:38:49 2015 UTC,"I think that discussions of 4 vs 6 vs 8 etc CPU core utilization is going to seem really silly to people looking back from 2020 and onward. I believe that we're going to see a whole reshuffling of what we now think of as a ""CPU"". There are already interesting concepts such as clustering one ""traditional"" CPU core within groups of many GPU cores."
hardware,3b4ip0,AnxiousInfusion,1 point,Thu Jun 25 22:28:55 2015 UTC,"Eventually, sure maybe. But this discussion is still going to be relevant for the coming couple or few years.  As to the topic... I guess with consoles having >4 cores and DX12 coming out, maybe having 6 cores or so will make a difference in the future. But then again it seems like the GPU is going to be handling most everything, so I really wouldn't know. For sure you won't need over 4 cores, since console CPUs are tons slower and mainstream desktop CPUs won't have over 4 any time soon."
hardware,3b3hgd,IByrdl,12,Thu Jun 25 17:43:14 2015 UTC,Oh my! This is the first curved screen with radius of curvature below 3 meters.  1000R or even 700R can't come soon enough.
hardware,3b3hgd,Randomoneh,5,Thu Jun 25 17:47:43 2015 UTC,"By 1000R, is this the radius in milimeteres?"
hardware,3b3hgd,DomUK89,3,Thu Jun 25 18:17:11 2015 UTC,"Yeah. First curved TVs had radius of curvature of ~4.5 meters = 4500R. Samsung heavily marketed the 4200R version, IIRC.  In their Investors Forum 2014 pamphlet they mention 1000R.  Soon NVIDIA and AMD will have to adjust to this via control panels by post-processing rectilinear to cylindrical, depending on the viewing distance and R-number."
hardware,3b3hgd,Randomoneh,1 point,Thu Jun 25 18:44:25 2015 UTC,But why do they call it the R3501?!
hardware,3b3hgd,bphase,5,Thu Jun 25 19:04:57 2015 UTC,"It's 35"" and the first in the lineup?"
hardware,3b3hgd,Puk3ify,3,Thu Jun 25 22:53:44 2015 UTC,Did they mention the contrast ratio? I assume this is the same panel as the predator z35.
hardware,3b3hgd,Darius510,5,Thu Jun 25 19:00:08 2015 UTC,"if this is priced decently that'd be an awesome monitor. I always thought that ultrawides with not 3440x1440 or whatever were bad, but after seeing one I changed my mind. It was only 29"" though, so maybe I'm wrong. but with 144hz thats awesome! glad to see them improving."
hardware,3b3hgd,Zarknox,8,Thu Jun 25 18:45:08 2015 UTC,"2560x1080 at 29"" seems ok. At 35""? No way. Anything that big should be 3440x1440."
hardware,3b3hgd,Stingray88,6,Thu Jun 25 19:59:26 2015 UTC,"I'm rocking an AOC 2560x1080 at 29"". It's good at this size, but I wouldn't go for anything larger than this at that resolution."
hardware,3b3hgd,TheRealGecko,3,Thu Jun 25 22:04:34 2015 UTC,"Lots of people play on bigscreen tvs @1920 so I think 2560 on a 35"" would be pretty nice for most people.  I still use my 55"" 1080 tv to game and I think it looks good."
hardware,3b3hgd,shadowofashadow,7,Thu Jun 25 20:22:15 2015 UTC,"You don't sit as close to a big screen TV as you would a monitor. Viewing distance is very important when determining if PPI is sufficient. At 8 feet a 1080p 55"" TV is going to look better than a 2560x1080 35"" monitor a few feet in front of you."
hardware,3b3hgd,Stingray88,2,Thu Jun 25 20:33:43 2015 UTC,"This guy gets it. I sit maybe 18"" max from my computer screen. First thing I thought when i saw this was that it would look horrible at close distance.   I never saw the appeal of using a TV for a monitor. Your going to get just as good or better field of view from a monitor and the picture, resolution, response time and refresh rate are all going to be 10x better and be cheaper 90 percent of the time."
hardware,3b3hgd,stuckinatrance,3,Thu Jun 25 21:15:25 2015 UTC,"Yeah using a television as a monitor close up is definitely not a great idea. They generally look terrible compared to even a cheap monitor, unless you're sitting further away.  One thing that's driving me nuts lately is some of my video editing coworkers all decided that buying a 40"" 4k TV to use as their computer monitor as opposed to multiple smaller monitors is the way to go. It looks like crap because they all bought TVs with 4:2:0 chroma subsampling. The UI and any text on screen is incredibly blurry. Ugh..."
hardware,3b3hgd,Stingray88,2,Thu Jun 25 21:21:52 2015 UTC,"Good 4k monitors (27"" IPS) are available for $550. Severely compromising quality to save a few bucks on something you will be using for hours a day every day ( like your monitor or chair) is not wise. IMO  Edited: damn autocorrect, I need to not post from my phone"
hardware,3b3hgd,ScottLux,2,Fri Jun 26 16:34:58 2015 UTC,People at work do this all the time thinking its helping them and it gives ME a headache if i look at it for more than 20 seconds.  But random joes think bigger = better.
hardware,3b3hgd,stuckinatrance,2,Fri Jun 26 17:06:43 2015 UTC,"Not bad. I don't like that it is 35"", not very good for DPI. I have that resolution and it is great but my screen is 30"" so DPI is not an issue. The 144hz speed is really good. But it would take at least a 980ti to get in that ballpark though. Maybe even a Titan X. Also I prefer IPS panels - less ghosting and better colors."
hardware,3b3hgd,redditwentdownhill,12,Thu Jun 25 18:38:53 2015 UTC,"I'm sick of hearing about the Titan X. Nobody should buy it, unless you need that VRAM for a reason which isn't gaming. The 980 Ti comes in aftermarket variants with better coolers and so overclocks better and is therefore better than the stupid Titan X. The X doesn't even have double precision fpf"
hardware,3b3hgd,mmencius,-7,Thu Jun 25 23:30:19 2015 UTC,"Well not everyone likes to OC their graphics card, and the Titan X is faster than a 980 Ti as standard. It serves its purpose."
hardware,3b3hgd,redditwentdownhill,5,Thu Jun 25 23:38:35 2015 UTC,"OK, then you can buy a Zotac Amp Extreme edition of the 980 Ti with a 1250 MHz factory overclock. Costs about $680 and beats Titan X out of box."
hardware,3b3hgd,mmencius,-7,Thu Jun 25 23:40:12 2015 UTC,"But you can OC a Titan too, beyond what a 980Ti can do. Also some games have issues with an overclocked card, so if you had to run at stock speeds the Titan would still be faster. They have their place."
hardware,3b3hgd,redditwentdownhill,7,Thu Jun 25 23:56:37 2015 UTC,"Wat   Well not everyone likes to OC their graphics card, and the Titan X is faster than a 980 Ti as standard.... But you can OC a Titan too   So the reason to get a Titan X is because you don't want to overclock, but you'll overclock anyway?  A factory overclocked card is guaranteed at those clockspeeds and takes no effort from you.   But you can OC a Titan too, beyond what a 980Ti can do.   No, the best GTX 980 Tis on the market will definitely overclock better than a Titan X since the board partners can add better cooling, better supporting hardware (caps/vrms), and bin their GPUs.   Also some games have issues with an overclocked card,   What?  Name one.  Clockspeed is clockspeed, the worst that could happen is that the game doesn't scale as well with extra clocks in which case you'd still have the same performance as a Titan X.   They have their place.   If you need 12GB of Vram then the Titan X has a purpose, but otherwise it does not."
hardware,3b3hgd,AMW1011,-4,Fri Jun 26 01:13:55 2015 UTC,"So the reason to get a Titan X is because you don't want to overclock, but you'll overclock anyway? A factory overclocked card is guaranteed at those clockspeeds and takes no effort from you.   No what I said was a Titan X has an advantage over a 980ti of running at that speed without an overclock. And yet you can overclock it too if you want.   No, the best GTX 980 Tis on the market will definitely overclock better than a Titan X since the board partners can add better cooling, better supporting hardware (caps/vrms), and bin their GPUs.     All the benchmarks I've seen show it going higher than a 980ti oc.      What? Name one. Clockspeed is clockspeed, the worst that could happen is that the game doesn't scale as well with extra clocks in which case you'd still have the same performance as a Titan X.     I've seen people having problems with at least a dozen games. And I'm not a big gamer so I bet there are more: http://battlelog.battlefield.com/bf4/forum/threadview/2955065222971067571/    http://www.overclock.net/t/1408172/far-cry-3-doesnt-like-overclocking     If you need 12GB of Vram then the Titan X has a purpose, but otherwise it does not.   It does where I said it does. And potential future proofing thanks to the higher ram."
hardware,3b3hgd,redditwentdownhill,2,Fri Jun 26 01:32:58 2015 UTC,"No what I said was a Titan X has an advantage over a 980ti of running at that speed without an overclock. And yet you can overclock it too if you want.   Of course, but you can get a myriad of GTX 980 Tis that will overclock as well or better than a Titan X and the Titan X does not have a performance per clock lead high enough to supersede that.  They will also outperform the Titan X with factory overclocks if you don't want to overclock yourself.  Remember, at the same exact clockspeeds the Titan X is barely 5% faster overall, which can easily be made up by or surpassed with overclocking.  Besides, ~5% is not even perceptible in actual gameplay.  Common wisdom says that you need at least 10% more average FPS to have a noticeably better gaming experience.     All the benchmarks I've seen show it going higher than a 980ti oc.   Show me them, here is mine. Note that the Zotac AMP! Extreme is 150MHz faster and still cheaper than the Titan X which is massive.  If you are talking about both overclocked, as I stated above that simply cannot be the case.   I've seen people having problems with at least a dozen games. And I'm not a big gamer so I bet there are more: http://battlelog.battlefield.com/bf4/forum/threadview/2955065222971067571/[1]   There is a difference between having problems with an unstable overclock and magically getting less performance with a stable overclock.  Those examples you pulled up are examples of unstable overclocks.  I've played countless hours of BF4 with a 250MHz core overclock with an R9 290 crash free.   http://www.overclock.net/t/1408172/far-cry-3-doesnt-like-overclocking   This was caused by driver errors with AMD where the cards wouldn't remain in 3d mode.  I don't think you can honestly count this as ""having problems with overclocking"" since it also effected stock cards or factory overclocked cards.   And potential future proofing thanks to the higher ram.   Impossible to know and unlikely.  The Titan X's memory bandwidth may bottleneck much before 12GB ever becomes useful."
hardware,3b3hgd,AMW1011,-6,Fri Jun 26 01:43:12 2015 UTC,"Of course, but you can get a myriad of GTX 980 Tis that will overclock far higher than a Titan X   Show me proof. According to these links the Titan X OC wins at most games. http://www.anandtech.com/show/9306/the-nvidia-geforce-gtx-980-ti-review/17    http://www.anandtech.com/show/9059/the-nvidia-geforce-gtx-titan-x-review/17      Show me them, here is mine.  That is comparing an OC 980TI with a stock Titan...   No you haven't. There is a difference between having problems with an unstable overclock and magically getting less performance with a stable overclock. Those examples you pulled up are examples of unstable overclocks.   No, I have seen many people talking about it with many games. And many of them say they have no problems with other games and even running benchmark tools at full load for 4 hours. Try actually reading the links.   Impossible to know and unlikely. The Titan X's memory bandwidth will bottleneck much before 12GB ever becomes useful.   It has potential. You can't make that claim, you said yourself it is impossible to know. Yet."
hardware,3b3hgd,redditwentdownhill,4,Fri Jun 26 01:56:58 2015 UTC,"Show me proof. According to these links the Titan X OC wins at most games. http://www.anandtech.com/show/9306/the-nvidia-geforce-gtx-980-ti-review/17[1] http://www.anandtech.com/show/9059/the-nvidia-geforce-gtx-titan-x-review/17[2]   This is pretty unbelievable that you posted this with sincerity.  You cannot compare scores from two different testing sessions.  There are too many variables, including margin of error, the clock speed differences, and the fact that this still isn't taking into account aftermarket cards.   No, I have seen many people talking about it with many games. And many of them say they have no problems with other games and even running benchmark tools at full load for 4 hours. Try actually reading the links.   I did and explained to you why that isn't the case: BF4 doesn't have such an issue and Far Cry 3 had a greater issue that isn't what you claimed.  Some games are much more demanding of stability than others, of course.  However no game magically doesn't work with overclocking.    If you want to be pointlessly obstinate and keep claiming you've seen it then lets factor out overclocking entirely.  The Titan X is at most 5% faster than the GTX 980 TI at the same clockspeeds.  This isn't perceptible in gameplay and is meaningless as it is.   It has potential. You can't make that claim, you said yourself it is impossible to know. Yet.   No you are confused.  It is you who couldn't make that claim.  What I said was that it is impossible to know and gave a similarly unfounded counter piece of speculation.  I suppose we agree here which is good.  Lets find more common ground."
hardware,3b3hgd,AMW1011,-7,Fri Jun 26 02:05:53 2015 UTC,I'm sick of hearing about the Titan X. Nobody should buy it   Hahah get mad cuz you can't afford the top dog... that's sad.
hardware,3b3hgd,questionsqu,5,Fri Jun 26 02:01:13 2015 UTC,Not top. 980 Ti overclocked beats Titan X. How sad you must be.
hardware,3b3hgd,mmencius,-11,Fri Jun 26 02:03:18 2015 UTC,"I'm not sad at all, I have a 970GTX which is more than enough for my resolution. You are the one who is mad because you keep hearing about the Titan X and it bothers you sooo much. Poor you!"
hardware,3b3hgd,questionsqu,4,Fri Jun 26 02:17:26 2015 UTC,"Great, we're both happy with our 970s. I just don't like misinformation. Maybe you enjoy misinformation."
hardware,3b3hgd,mmencius,-10,Fri Jun 26 02:51:18 2015 UTC,"What misinformation? And clearly you are not happy with your 970, you wanted a Titan and you get mad every time you hear the name. TITAN TITAN TITAN! Also I am buying a second 970 so I have far better than you."
hardware,3b3hgd,questionsqu,7,Fri Jun 26 02:53:16 2015 UTC,"When people say a Titan X is superior to a 980 Ti, that is false. Nobody should buy a Titan X which is inferior to but much more expensive than, for instance, a Zotac Amp extreme 980 Ti.   Also I am buying a second 970 so I have far better than you.   What are you, a child?"
hardware,3b3hgd,mmencius,-2,Fri Jun 26 03:05:55 2015 UTC,"Nope, Titan is better at everything except price. And yes I'm a child, and I also have better than you."
hardware,3b3hgd,questionsqu,3,Fri Jun 26 18:59:07 2015 UTC,No it's not. Zotac 980 ti is better in performance.
hardware,3b3hgd,mmencius,7,Fri Jun 26 21:24:34 2015 UTC,"Maybe at max settings for games, but you could play comp games like CS:GO easily at 144hz."
hardware,3b3hgd,spencer32320,1 point,Thu Jun 25 19:11:54 2015 UTC,"There's more ghosting on IPS than TN, since TN can go down to 1ms and use strobing better."
hardware,3b3hgd,CeeeeeJaaaaay,7,Thu Jun 25 22:50:44 2015 UTC,"This monitor is not a TN panel, it is a VA panel."
hardware,3b3hgd,redditwentdownhill,-4,Thu Jun 25 23:09:54 2015 UTC,My statement still applies.
hardware,3b3hgd,CeeeeeJaaaaay,2,Thu Jun 25 23:15:54 2015 UTC,"No it doesn't... TN panels have nothing to do with this thread, or my comment. They are also bad, face it."
hardware,3b3hgd,redditwentdownhill,2,Thu Jun 25 23:21:17 2015 UTC,"What I meant is that VA is still faster than IPS, so there's less ghosting and better strobing."
hardware,3b3hgd,CeeeeeJaaaaay,-3,Thu Jun 25 23:29:41 2015 UTC,"Last time I researched this it said IPS was still faster. ""Their response times also look good on paper, but unfortunately not in the real world. Even if the response time for white to black is low, it is often considerably higher between two dark tones, leading to Ghosting effects."" - http://www.tnpanel.com/tn-vs-ips-va/"
hardware,3b3hgd,redditwentdownhill,0,Thu Jun 25 23:34:53 2015 UTC,"That applies to every type of pannel. VA is faster than IPS, there have been 120Hz VA screens (or well at least 1) for 2 years now."
hardware,3b3hgd,CeeeeeJaaaaay,-1,Fri Jun 26 02:57:06 2015 UTC,"Still debatable, I can find no links to show it being faster and plenty of links that say it has ghosting issues. I'd still take IPS for the colors, and the speed is good enough nowadays."
hardware,3b3hgd,redditwentdownhill,1 point,Fri Jun 26 03:07:48 2015 UTC,"There's an Eizo VA panel with 1ms GTG. That's needed if you want proper strobing support, with crosstalk and strobing time adjustments."
hardware,3b3hgd,CeeeeeJaaaaay,1 point,Fri Jun 26 03:19:18 2015 UTC,Nope the 980ti is a cut down titan x
hardware,3b3hgd,Teethpasta,1 point,Thu Jun 25 20:00:15 2015 UTC,My titan x's are watercooled and voltage unlocked. Clocking easy to around 1550MHz so not sure what you are talking about.
hardware,3b3hgd,ectom,1 point,Thu Jun 25 20:38:33 2015 UTC,If this thing had freesync or g-sync I would pick it up in a heartbeat.   That's the only thing stopping me from ordering it right now.
hardware,3b3hgd,bizude,0,Thu Jun 25 20:50:53 2015 UTC,I think 1080 is already cutting it on my 24 inch.
hardware,3b3hgd,freshandsticky,0,Thu Jun 25 20:56:41 2015 UTC,Not 1440p. Bummer.
hardware,3b0ytk,Purp3L,214,Thu Jun 25 02:21:19 2015 UTC,"Every time a reasonably significant AMD thing happens, there's always a bit of talk about them being sold/bought/traded/dying/whatever. What follows is usually a string of fanboyism, some logic, and one or two people mentioning an ""x86 license"" - But what is that?   AMD and Intel have a complete patent cross licensing agreement.  They also have secret conditions for this agreement.  If a company bought out AMD, they would obtain this cross licensing agreement, but it would come with these conditions.   Well, let's jump right in and start at the beginning: Intel invented the 16-bit instruction set architecture used in many, many computers today.   That's not the whole story, but by the time they developed the 16 bit instruction set, they had 100% ownership of it, so for all intents and purposes it is equivalent.   This first came to be in the Intel 8086 microprocessor, and it looked like a little NAND gate your kid might use in an Intro to EE class today.   Huh?  The 8086 was made of many thousands of transistors.   With a few exceptions, many of Intel's chips through the 80's followed the xx86 naming convention. This is when we started calling it the ""x86 architecture,"" despite the fact that everything that falls into that architecture actually employs 16/32/64-bit processing.   That's ... a really gross over-simplification of the story.  And it missed some very important issues.  Intel developed the following sequence of CPUs that were backward compatible with the 8086:  8088, 80186, 80286, 80386, 80486 renamed to i486, Pentium, Pentium Pro, Pentium-MMX, Pentium II, Pentium III, Pentium IV, Pentium-M, Core, etc.  The reason to spell this out is to point out a few things.  Intel sued AMD for the use of the names ""80386"" and ""80486"" in their compatible products, which Intel claimed as trademark.  A judge ruled that under no circumstances could anyone trademark a pure number.  So Intel changed their product naming convention to include letters, then later they made up the word Pentium ...  The 8086, 8088, and 80186 were 16 bit architectures.  The 80286 was a 16 bit architectures with a lot of extended instructions for something called ""protected mode"".    Since barely any software used the 80286-specific instructions, Intel basically abandoned some of them in developing the 80386, a 32-bit architecture with a much more useful and extensive protected mode (it was aimed at being able to implement real operating systems like Unix; something you really couldn't do on the 16 bit CPUs).  As if to thumb their nose at the snobby ""RISC-types"", the 80486 added a few instructions and changed implementation to be largely pipelined.  The Pentium was further pipelined, and made super-scalar.  The Pentium-MMX added 64-bit integer SIMD instructions.  The Pentium Pro was an out-of-order 5-port super-scalar (but missing the MMX instructions).  The Pentium II added the MMX instructions to the Pentium Pro architecture.  The Pentium III added 128-bit floating point SIMD instructions (called SSE).  Intel would later clone AMD's 64 bit x86 architecture (aka x86-64 or AMD64) and use the non-descript name ""EMT64"" as a ""capability"" in the later Pentium IV's.  The name ""x86"" was actually adopted by AMD and Cyrix, the clone chip manufacturers, as Intel started using names like Pentium where they could enforce their trademark.  If you look through ALL of Intel's documentation, you will never find them using the notation ""x86"" to describe their own products.   x86 processors fall under the ""CISC"" heading (C is for Complex,   The term ""CISC"" was invented by the same people who invented the term ""RISC"".  It was just a point of propaganda.  RISC was supposed to have all sorts of advantages because it was simpler.  The problem is that pipelining, super-scalar, out-of-order execution, branch-prediction, speculative execution, and register renaming techniques all came to the x86 architectures.  Even symmetric multithreading came to the x86!  At the end of the day, the only real difference between RISC and CISC was that the instructions didn't need any extra decode stage on RISCs.  But then you suddenly look at the PowerPC 970 architecture, and they've added an instruction decode stage!  Now why would they do that, if they were a RISC?  Could it be that adding extra instructions (the AltiVec set) screwed them up so that they were forced to add a decode stage anyway?  (And therefore ALL of their advantages versus ""CISC"" vanished.)   Now, let's focus on the CISC processors for a second, because that's important. Because they're complicated (literally), some functions of the processors are secret-y intellectual property (IP).   This is also true of RISCs.  All serious microprocessors are heavily covered by patents out the whazzoo.   In this case, mainly patents that Intel has on various specific instruction sets. These are usually the core functions of processors, that can't really be changed without reinventing the wheel. Intel holds a bunch of these patents, and AMD has a few patents as well.   Uhh ... the instruction sets aren't specifically patented.  It is the method of implmentation, or technique that is patented.  During the late 90s, AMD actually was producing more patents than Intel.   Because Intel has all these patents, manufacturers interested in using these secret fancy things have to get a license from Intel.   All CPUs are covered by patents.  AMD's approach was to produce more patents, so that if Intel came to sue them, AMD would be able to counter-sue.  And they did this to each other.  A lot.  That's why they currently ended up with their patent cross licencing agreement.   THAT is the x86 license you hear mentioned (usually in regards to AMD being sold). This license is non-transferable, and has a bunch of other fun limitations that basically means that nobody but AMD is allowed to use it to make processors. Even if another company bought AMD, they couldn't use the license if they renamed, gutted, or otherwise majorly changed the operating structure.   I am not sure that's true.  Remember, if the agreement between AMD and Intel ceases, AMD can actually resume their lawsuits and injunctions against Intel.  The 64 bit x86 instruction set was completely designed by AMD, remember.   As /u/wtallis pointed out, patents on the abstract instruction set architecture are what actually matters here.   That's not correct.  Otherwise IBM, the old Dec, Sun, Hp and anyone who made a microprocessor might easily go try to sue Intel (and vice versa).  Only in weird cases, like the ""Equator patent"" which was a claim against the Itanium stealing a technique for fast switching of instruction sets, are the actual instructions crucial.   There's currently only three companies that are licensed by Intel to manufacture x86-architecture processors. Intel (duh), AMD, and VIA - a little-known Taiwanese company that makes motherboard chipsets, memory, and a few CPUs that really aren't anything special.   IBM has an x86 license as well.  This was a condition for purchasing their CPUs for their PCs while they were making the 80386/80486 CPUs.   They do in-house R&D, and then contract with TSMC or GlobalFoundries or something to make the actual chips.   VIA has this license because they bought out Cyrix.  Cyrix has a license because they successfully sued Intel.  (Success, in this case, means Intel was forced to settle, which included a blanket license for making x86s.)   Anyways, the whole reason that this stuff comes up in discussion is because so few companies are licensed to compete with Intel in the market. There's absolutely insane costs with R&D for this stuff, not to mention the actual fabrication - that's why TSMC and GlobalFoundries are successful.  Sadly, it also means that the barrier for entry is really, really high - It's near-impossible for some startup to gain the momentum and financial backing to break into this industry, even if a design is groundbreaking. It's disappointing, and to some extent is can seem anti-competitive. I don't know enough about the industry to say much more than that, but I'm hoping others who do will chime in.   Actually, it is really the R&D barrier that is the only problem.  When the companies ""Transmeta"" and ""RISE"" were making x86 processors, the way they worked around the lawsuit problem is they partnered with IBM for manufacturing.  So technically, IBM was making the x86s.  Since IBM had a license, Transmeta and RISE were immune from being sued by Intel.  (Or more to the point, there's no way Intel would win.)  This is a straightforward strategy that any company could use.  But in the end people gave up competing with Intel, because their CPUs were just too damned good.  Only AMD could keep up.   So that's the gist of it, in a really short summary. If you're interested in learning more about any of this stuff, I recommend starting by researching keywords like ""Instruction Set Architecture,"" ""x86,"" ""Logic Gates,"" and so on. Start with those, wander around the internet or a library (a what?) until you find something easily digestible, and get comfy While it's complicated stuff, it's also incredibly fascinating.   Or you could randomly run into redditors who happened to have worked at one or another of these companies in the past.  ;)"
hardware,3b0ytk,websnarf,73,Thu Jun 25 05:56:23 2015 UTC,"This entire post is a gross oversimplifcation, you're right. That was the point of me doing this. I tried to glean what I could from whatever I could, and threw it all together to try and understand the bigger picture. Clearly this annoys some people, but all I'm trying to do is learn.   I'm not sure where to edit in all your remarks, so it's a good thing it's the top comment. Thank you for providing a lot of solid information, and thank you for taking the time to correct me."
hardware,3b0ytk,jlt6666,8,Thu Jun 25 13:26:48 2015 UTC,Hey! Stop being reasonable.  This is Reddit OK?  Now... FIGHT!
hardware,3b0ytk,Spoonshape,1 point,Fri Jun 26 06:05:17 2015 UTC,Then kiss...
hardware,3b0ytk,dexter311,27,Fri Jun 26 15:40:45 2015 UTC,"Huh? The 8086 was made of many thousands of transistors.   I think he means the package - the 8086, like most processors of the time, were supplied in an dual inline package, just like logic chips are today."
hardware,3b0ytk,andromeduck,14,Thu Jun 25 09:45:25 2015 UTC,Yep! That's exactly what I meant.
hardware,3b0ytk,websnarf,6,Thu Jun 25 16:08:50 2015 UTC,Wasn't that what Nvidia wanted to do with Denver?   What's the deal with that if they could just do it through IBM?  Would it be because they no longer have bleeding edge in house fabs?
hardware,3b0ytk,williadc,12,Thu Jun 25 07:28:29 2015 UTC,"Wasn't that what Nvidia wanted to do with Denver?   I know nVidia was making an x86.  But Intel did a deal with them that stopped it.  I don't know any details, I stopped following x86 for a few years during this period.  Remember nVidia has graphics patents, and Intel ships graphics chips -- if Intel wants to start lawsuits, it will get awfully bloody.   What's the deal with that if they could just do it through IBM?   Well part of the problem is that you have to make it worth IBM's while.  I think one of the factors that caused Transmeta to fail is that even for what they shipped, they were giving up too much of their profits to IBM.  It is a way -- it is not necessarily a way that is guaranteed to work (in fact it has never worked).   Would it be because they no longer have bleeding edge in house fabs?   Oh -- IBM is probably a little behind where Intel is, but only a little.  Being a big company, they have poured their money into staying leading edge there."
hardware,3b0ytk,websnarf,17,Thu Jun 25 08:14:28 2015 UTC,"IBM sold their fabs to GLOBALFOUNDRIES last year.  The linked press release says they're still doing component research, but GF gets to use it."
hardware,3b0ytk,Evidence_Of_Absence,11,Thu Jun 25 08:38:15 2015 UTC,I wasn't aware of that!  That's what I get for not keeping up.
hardware,3b0ytk,lovelikepie,9,Thu Jun 25 08:43:31 2015 UTC,"IBM is significantly behind Intel as far as digital CMOS is concerned.  Their CMOS stuff is mostly common platform collaborations, except AFAIK Samsung doesn't let them have the latest fancy stuff (at least for free).  I believe IBM's best digital process that's widely available is 22nm SOI.  Basically, they're behind their own industry partners by more than a year, and definitely behind Intel.  As for pouring money into staying leading edge, they sold their semiconductor fab business to GloFo for NEGATIVE ~1.5 billion dollars (really, the business and some contractual supply agreements, but still) or so because it was a gigantic money pit."
hardware,3b0ytk,Evidence_Of_Absence,3,Thu Jun 25 14:27:38 2015 UTC,IBM had a SOI 22nm in production
hardware,3b0ytk,Viper_ACR,3,Fri Jun 26 00:04:06 2015 UTC,"Yea you're right, I've edited my post.  They were only shipping large-scale commercial volumes in like early to mid 2014 though, so the larger point about timelines still stands.  AFAIK Samsung has been shipping 14nm for a bit now, GloFo has licensed it, and IBM is just now adapting it to SOI and publishing things.  So they'll be fully absorbed by GloFo by the time it's actually ready."
hardware,3b0ytk,websnarf,1 point,Fri Jun 26 00:10:26 2015 UTC,Didn't IBM give it's foundry stuff to GlobalFoundries because they were way behind Intel?
hardware,3b0ytk,Viper_ACR,1 point,Fri Jun 26 02:54:54 2015 UTC,According to a link from /u/williadc apparently so.
hardware,3b0ytk,everyZig,1 point,Fri Jun 26 03:27:28 2015 UTC,"Yeah, I was about to say becauase I remember that being a thing last year- my dad used to work for IBM and my friend is interning at Global Foundries right now and I remember reading about that."
hardware,3b0ytk,unpythonic,6,Fri Jun 26 04:10:24 2015 UTC,"This first came to be in the Intel 8086 microprocessor, and it looked like a little NAND gate your kid might use in an Intro to EE class today.   Huh? The 8086 was made of many thousands of transistors.   I think he means in terms of packaging, early CPUs were simple DIP chips, which at least externally look somewhat similar to a simple 7400 series logic chip (such as a quad of NAND gates)  EDIT:   Actually, it is really the R&D barrier that is the only problem. When the companies ""Transmeta"" and ""RISE"" were making x86 processors, the way they worked around the lawsuit problem is they partnered with IBM for manufacturing. So technically, IBM was making the x86s. Since IBM had a license, Transmeta and RISE were immune from being sued by Intel.   Does IBM have a license for the old intel x86 stuff, or do they have a cross-license for AMD64 as well? otherwise that trick is pretty much pointless these days."
hardware,3b0ytk,Spoonshape,6,Thu Jun 25 07:19:00 2015 UTC,"Does IBM have a license for the old intel x86 stuff, or do they have a cross-license for AMD64 as well? otherwise that trick is pretty much pointless these days.   Patents last 20 years. The first Pentium came out in 1993. Anything covering the original Pentium or older is off patent. Additionally, patents are usually obtained before the chip is shipping for revenue as it would be incredibly embarrassing to have information about your newest tech leak ahead of the product and have some industrious person run around filing patents based on those leaks. So chances are pretty good that the patents for MMX are also expired.  The obvious things you need to license these days are the 64bit extensions and SSE. The bigger problem is that the whole industry is a patent minefield.  ""Obvious"" is such a subjective term that it takes an outrageously expensive and very risky lawsuit to challenge the obviousness of a patent because the other side can simply say ""if it was so obvious, why weren't you doing it first?"" A single register that holds an entire 4x4 matrix of 32bit single precision floats? Seems pretty obvious since matrices are usually carried as single blocks in mathematical equations. Even if your CPU doesn't copy the x86/AMD64 instruction set, you had better have a fat wallet if you want your CPU to do anything like that. What about instructions for common matrix operations? Seems pretty obvious that this is something you would do if you had the transistor budget, right? Well a common vector operation is ""multiply and add"". Although it is pretty damned obvious that anyone with the time and budget to add this functionality would want to do so, somebody has already patented it.  I don't know of any CPU that has built-in instructions for doing quaternion operations on matrix registers, but I would be absolutely shocked to learned that isn't already patented. It seems like some people have a gift for teasing out things that are so obvious nobody would think to patent them... those people are IP gold mines."
hardware,3b0ytk,unpythonic,1 point,Thu Jun 25 16:24:27 2015 UTC,"So, is anyone actually making pentiums now that they are out of patent?"
hardware,3b0ytk,websnarf,2,Fri Jun 26 15:46:26 2015 UTC,"Not that I know of. It'd be a pretty niche market considering that ARM is inexpensive to license, has better power efficiency and better performance. While the ISA implementation is out of patent, the actual implementation by Intel is still a trade secret. It would be difficult for anyone to recover their R&D costs and their investors should have them hanged for not either making their own ARM design or working with Intel on an Atom/Silvermont design."
hardware,3b0ytk,everyZig,7,Fri Jun 26 16:27:26 2015 UTC,"Does IBM have a license for the old intel x86 stuff, or do they have a cross-license for AMD64 as well? otherwise that trick is pretty much pointless these days.   Well, AMD is the owner of the AMD64 instruction architecture.  And they are FAR easier to make a deal with than Intel (Transmeta managed to get a license for it for very little cost)."
hardware,3b0ytk,websnarf,1 point,Thu Jun 25 08:18:39 2015 UTC,"Hmm, hadnt considered that, AMD will probably be pretty easy if you bring a bag of money"
hardware,3b0ytk,Spoonshape,3,Thu Jun 25 08:26:54 2015 UTC,"Well or if you do some strategic deal with them, like make bus and/or motherboards that somehow favor AMD graphics cards.  AMD tended to favor strategic deals with Transmeta and Cyrix, since they tended not to show up with bags of money."
hardware,3b0ytk,everyZig,1 point,Thu Jun 25 08:46:34 2015 UTC,http://cdn.cpu-world.com/CPUs/8086/S_AMD-ID8086B.jpg  A little bigger and more pins than a nand gate...
hardware,3b0ytk,MandrakeQ,1 point,Fri Jun 26 15:44:21 2015 UTC,"I know that, but it still is a DIP module rather then current PGA/LGA chips, to a layman the black package with two rows of pins would look the same, anyone unacquainted with the workings of a cpu/computer wouldnt notice that the 14 pin NAND gate package could in no way house a functionally relevant CPU."
hardware,3b0ytk,docholiday,3,Fri Jun 26 16:49:57 2015 UTC,"Anyways, the whole reason that this stuff comes up in discussion is because so few companies are licensed to compete with Intel in the market. There's absolutely insane costs with R&D for this stuff, not to mention the actual fabrication - that's why TSMC and GlobalFoundries are successful. Sadly, it also means that the barrier for entry is really, really high - It's near-impossible for some startup to gain the momentum and financial backing to break into this industry, even if a design is groundbreaking. It's disappointing, and to some extent is can seem anti-competitive. I don't know enough about the industry to say much more than that, but I'm hoping others who do will chime in.   Isn't the financial backing needed to fund RnD? I think OP is still correct.   No other company has the capital to drive the RnD needed to compete with Intel's CPUs."
hardware,3b0ytk,Viper_ACR,4,Thu Jun 25 07:35:24 2015 UTC,"Depends on what platform they are competing with Intel on. Servers? Good luck... but tablets, non-cellular mobile devices, RaspPi type boards, etc? All possible."
hardware,3b0ytk,docholiday,1 point,Thu Jun 25 11:17:43 2015 UTC,"Yeah, and Intel is getting into those areas too with the Atom and Quark processors, and their takeover of Altera to get into the FPGA business too.   Now if they would actually fix the Galileo SBC and make it work well, that would be sick."
hardware,3b0ytk,Viper_ACR,2,Fri Jun 26 02:59:27 2015 UTC,"Now if they would actually fix the Galileo SBC and make it work well, that would be sick.    I have one of those boards, and yes, they really blew it with the software. Totally missed the mark."
hardware,3b0ytk,websnarf,1 point,Fri Jun 26 11:34:01 2015 UTC,"I had to use the Gen 2 board for a makerspace-like seminar this past semester in college... holy shit it still had issues.   I couldn't fuck around in the Linux side with C at all, and the SPI bus was all fucked up for some reason. So that was a major pain to deal with since we were trying to hack a proprietary quadcopter TX/RX protocol from some Chinese manufacturer using 3-wire SPI."
hardware,3b0ytk,MandrakeQ,3,Fri Jun 26 12:15:47 2015 UTC,Isn't the financial backing needed to fund RnD? I think OP is still correct.  No other company has the capital to drive the RnD needed to compete with Intel's CPUs.   AMD was competing toe to toe with Intel for more than a decade.  The reason they fell behind was not because R&D was too costly for them.
hardware,3b0ytk,Alpha21264,6,Thu Jun 25 08:16:46 2015 UTC,"Given that AMD had to spin off Global Foundries, I'd say the inability to fund the RnD for new process nodes was a big part of their decline. There is no way they could compete on performance when they were a process node behind Intel."
hardware,3b0ytk,lucun,2,Thu Jun 25 14:35:59 2015 UTC,"Intel has always been a manufacturing company that happens to have a processor design group. Their CPU design, until recently, hasn't classically been very good. Throughout the years they have faced architectures by smaller competitors that were far superior. DEC, IBM, not so much SUN, AMD, and now Apple (well they aren't smaller). However, their incredibly successful manufacturing market has allowed them to effectively outmaneuver all of them"
hardware,3b0ytk,Viper_ACR,3,Fri Jun 26 00:08:57 2015 UTC,"This first came to be in the Intel 8086 microprocessor, and it looked like a little NAND gate your kid might use in an Intro to EE class today. Huh? The 8086 was made of many thousands of transistors.   I think he means the package is a DIP based package similar to what the NAND gate chips normally come in (although the 8086 has a lot more pins). Here's a microcontroller using a similar package."
hardware,3b0ytk,djlemma,1 point,Thu Jun 25 09:50:54 2015 UTC,Fucking PICs..... lol.  A lot of processors were in in DIP package form. I actually just found a whole bunch of 68000s in storage earlier today.   They look ancient as fuck.
hardware,3b0ytk,olavk2,2,Fri Jun 26 02:56:23 2015 UTC,"VIA has this license because they bought out Cyrix.   So THAT'S what happened to Cyrix.  I kept remembering that there was another company in the PC processor market when I was young, but couldn't remember the name, and had no clue what happened to them."
hardware,3b0ytk,yifanlu,2,Thu Jun 25 21:31:31 2015 UTC,"Or you could randomly run into redditors who happened to have worked at one or another of these companies in the past. ;)   so i assume you have worked for one of those companies in the past? that is pretty cool, also thanks for that little write up, was a good read :D"
hardware,3b0ytk,websnarf,1 point,Fri Jun 26 07:59:27 2015 UTC,"(And therefore ALL of their advantages versus ""CISC"" vanished.)   Can you explain this in more detail? Do you believe that there is no advantage of RISC over CISC? From my limited knowledge in the subject, I thought that a main problem of CISC processors is that they were optimized for speed and throughput rather than power and efficiency; hence why ARM is dominating the mobile and embedded space."
hardware,3b0ytk,yifanlu,2,Thu Jun 25 20:54:20 2015 UTC,"Can you explain this in more detail? Do you believe that there is no advantage of RISC over CISC?   The advantage of RISC over CISC is that it is easier to design, and if well designed, does not require an extra ""decode the instruction"" stage in their execution pipeline.  There are claims that RISC is easier to target a compiler for, but the best compilers in the world all output x86 code.   From my limited knowledge in the subject, I thought that a main problem of CISC processors is that they were optimized for speed and throughput rather than power and efficiency   This is just a matter of manufacturer's design choice.  The PowerPC, for example, draws a LOT more power per instruction executed, than a state of the art x86 (and the latest x86s are faster per clock than any PowerPC).  On the other hand an ARM will typically use far less power than an x86.  Trading power for performance is an independent design choice that really doesn't depend on CISC versus RISC."
hardware,3b0ytk,websnarf,1 point,Thu Jun 25 21:08:52 2015 UTC,"I agree that ""trading power for performance"" is mostly a design choice, but is it really independent of the instruction set? More complex instruction set would require more specialized hardware (even if you take account of the micro-code implementation). I heard that Intel is playing around with turning parts of the processor off when they're not used; but a more RISCy design would not have parts that are not used. Having smaller area would give you more leeway to optimize for power or performance."
hardware,3b0ytk,kyuubi42,3,Thu Jun 25 21:23:48 2015 UTC,"I agree that ""trading power for performance"" is mostly a design choice, but is it really independent of the instruction set?   The original 80386 was designed using 275,000 transistors.  And ARM7 uses 578,977 transistors.  So ... no.  It depends on the implementation trade offs.  The very first 32 bit ARM processors, of course, were tiny and could be implemented in 25,000 transistors.  But these didn't even have floating point units.   More complex instruction set would require more specialized hardware (even if you take account of the micro-code implementation).   This is true, and has always been touted as RISC's basic advantage.    Non-x86 vendors would call the extra mechanisms required by the x86 (predecode bits, microcode engine, complex address modes, trace cache) the ""x86-tax"".  However, most of the techniques used to deliver really serious performance (think of the size of the typical on-chip cache) on your CPU, dwarfs the ""x86 tax"".   I heard that Intel is playing around with turning parts of the processor off when they're not used;   They already do this.  (It's more generally called ""gated clocks"").   but a more RISCy design would not have parts that are not used   Oh ... I see.  Well, the x86 tax generally does not cost a lot of power.  What costs power are the really big units that use most of their parts all at once, like the floating point unit.  The latest Intel CPUs definitely turn off their floating point units when they are not being used; I am fairly certain that the Power PC does not.  In the case of the ARM, it is so slow, I don't think it makes a difference whether you supply it power or not.   Having smaller area would give you more leeway to optimize for power or performance.   When including x86 processors in the discussion, no other vendors is even entitled to say the word ""performance"".  Performance doesn't come from having a smallest possible die area in one possible design.  It comes from how many design techniques to leverage parallelism one way or another, usually as a matter of brute force, you can put together in an architecture at once."
hardware,3b0ytk,headband,1 point,Thu Jun 25 21:48:42 2015 UTC,"Well, risc designs also tend to use much less die space which can allow them to be cheaper and lower power (leads to higher yields per die and lower overall leakage for example).  I may be wrong, but it was also my understanding that modern x86 chips have trended to a simpler risc-like core with a ton of microcode in front as a sort of compatibility layer, in order to take advantage of the best of both worlds?"
hardware,3b0ytk,gaggra,-5,Fri Jun 26 06:26:20 2015 UTC,"Seriously, its like the kid did some research, didn't understand it completely and filled in the gaps with bs he hoped his teacher would believe."
hardware,3b0ytk,headband,29,Thu Jun 25 06:54:29 2015 UTC,Notice that /u/Purp3L asked for corrections and /u/websnarf provided a wealth of information without being condescending and hostile.
hardware,3b0ytk,garrett04,0,Thu Jun 25 14:46:13 2015 UTC,even if he said that its still NOT COOL  I see this crap all the time. Internet forums are generally pretty uneducated and this stuff will be parroted over and over by people who think its 100% correct.
hardware,3b0ytk,ikahjalmr,19,Fri Jun 26 01:45:53 2015 UTC,"Other than this being a school assignment, you're not wrong. I actually say this in basically my first sentence... I'm doing this for the sake of my own interest, and encourage others to input when they have more valid experience or knowledge than I do. It's a topic I've literally never seen discussed, and I wanted to learn more. If you find fault with that then there's not much I can do about it."
hardware,3b0ytk,ikahjalmr,-1,Thu Jun 25 13:17:36 2015 UTC,You just did the OP's school assignment for him. Reddit is such a good place to be.
hardware,3b0ytk,lucun,4,Thu Jun 25 15:20:23 2015 UTC,"As I mentioned elsewhere, this isn't a school assignment. It's something I was genuinely interested in and wanted to start a discussion about."
hardware,3b0ytk,captain_awesomesauce,1 point,Thu Jun 25 16:14:17 2015 UTC,"Would it not be better to just suggest people to look up basic computer architecture/its history, rather than various keywords?"
hardware,3b0ytk,wtallis,2,Thu Jun 25 17:58:41 2015 UTC,"Perhaps. There's a LOT out there though, which is why I suggested some more specific topics. If someone wants, they can always broaden their search later."
hardware,3b0ytk,dexter311,2,Thu Jun 25 18:28:32 2015 UTC,That's a good point
hardware,3b0ytk,astalavista114,1 point,Thu Jun 25 19:20:19 2015 UTC,Some call it an interview source. OP has good research skills if this was for an assignment.
hardware,3b0ytk,dexter311,-6,Thu Jun 25 20:46:20 2015 UTC,"TL;DR: Hey OP, let me show you why I'm smarter than you. Now give me Karma or I'll shame everyone else, too."
hardware,3b0ytk,neilmohr,29,Thu Jun 25 14:29:38 2015 UTC,"In a discussion like this, you really shouldn't use a blanket term to refer to ""intellectual property"". Patents, copyrights, and trademarks are all different things and the differences matter. Patents on the abstract instruction set architecture are what actually matters here, and patents are guaranteed to expire within an ordinary human's lifespan. The Pentium Pro patents are expired or on the verge of expiring. So at this point, any ""x86 license"" that AMD has is really only about new-ish stuff like the SSE family of instruction set extensions, and only the ones that are novel and non-obvious enough to be deserving of patent protection."
hardware,3b0ytk,tadfisher,9,Thu Jun 25 03:48:32 2015 UTC,"Further to this, Intel and AMD have cross-licensing agreements where AMD can use Intel's x86 IP, and Intel can use AMD's x86-64 IP (among other agreements).  This isn't a one-way transaction where Intel holds all the cards."
hardware,3b0ytk,Kaghuros,2,Thu Jun 25 09:43:36 2015 UTC,"Doesn't AMD's x86-64 IP rely on Intel's x86 IP, so if the cross-licensing fell through, neither could use x86-64 at all without out getting sued for breach of IP."
hardware,3b0ytk,Maiq_Knows_Much,3,Thu Jun 25 12:07:18 2015 UTC,"Basically yeah.  Intel had their own 64-bit architecture (Itanium) but AMDs ended up being the standard.  Since it's built upon x86, the cross-licensing deal was inevitable."
hardware,3b0ytk,anastrophe,8,Thu Jun 25 13:06:01 2015 UTC,"Thank you. I've edited this in, and cited you as a source.   However, I'd like to point out that Intel isn't just restricting use on those SSE patents, which were developed as a response to AMD's insights into vector processing extentions to the architecture. Intel files tens or hundreds of patents each month, ranging from location estimation to new or unique ways to regulate voltage.   There's remarkably little information on what the x86 license actually grants rights to, in specifics (at least that I've been able to find recently). It potentially could be an ever-changing range of patents and rights to technology that continues to emerge and evolve."
hardware,3b0ytk,hojnikb,3,Thu Jun 25 04:17:21 2015 UTC,"I'm sure IBM had a licence or some right that enabled it to produce its own line of 486 DX2/DX4 processors, way back when. But it has its own PowerPC architecture which is more lucrative."
hardware,3b0ytk,hans_ober,17,Thu Jun 25 07:50:15 2015 UTC,"A little piece of history: VIA inherited their license when they purchased Cyrix, which was a chip manufacturer that was a bit more well-known in the '90s. They produced chips compatible with the 386 all the way up through the Pentium II (P6) architecture.  Their MediaGX chip, released in the mid-90s, was the first x86 ""system-on-a-chip""; it had on-die video and audio hardware, a PCI bus, as well as a memory controller, long before Intel and AMD started integrating these features. A later iteration of this, the Geode processor, was sold off to AMD after the VIA acquisition.  You could say that the current trend of integrated graphics hardware started here, and you can draw a direct lineage from Cyrix chips to the latest AMD APUs. At least you could conjecture that they gave AMD the idea :)"
hardware,3b0ytk,lucun,4,Thu Jun 25 05:10:13 2015 UTC,They're also (as far as I know) the only CPU maker to successfully sue Intel.
hardware,3b0ytk,hojnikb,2,Thu Jun 25 07:44:23 2015 UTC,"AMD won two antitrust lawsuits against Intel as well, though I believe one was settled out of court."
hardware,3b0ytk,jonythunder,12,Thu Jun 25 20:23:50 2015 UTC,"(C is for Complicated, followed by Instruction Set Computer).   C is for ""Complex""."
hardware,3b0ytk,sabot00,4,Thu Jun 25 04:09:42 2015 UTC,"Dont Intel use sorta hybrid RISC/CISC, where it breaks down complicated x86 instructions to a simpler ones ?"
hardware,3b0ytk,LongBowNL,7,Thu Jun 25 06:49:51 2015 UTC,Apparently the whole thing is a hybrid thin these days. ARM is not RISC; x86 is not CISC. Both of them are somewhere is the middle.
hardware,3b0ytk,docholiday,3,Thu Jun 25 07:01:41 2015 UTC,I remember hearing it's CISC on the outside and RISC on the inside after instructions are broken down.
hardware,3b0ytk,wtallis,2,Thu Jun 25 09:54:38 2015 UTC,I heard the same thing. Kinda makes sense...
hardware,3b0ytk,docholiday,-5,Thu Jun 25 10:38:53 2015 UTC,"Indeed, but tomahto tomato. Don't nitpick, the post was good!"
hardware,3b0ytk,MMMMorshu,17,Thu Jun 25 04:12:21 2015 UTC,"Complex and complicated are very different in the world of Computer Science.  ""Complex is better than complicated."" - The Zen of Python"
hardware,3b0ytk,rndnum123,3,Thu Jun 25 05:21:36 2015 UTC,"Nah, he's right. I went on to say complicated, and then added that in as an afterthought - hence the garbled wording. I've changed it to Complex in the OP, as it should be."
hardware,3b0ytk,astalavista114,5,Thu Jun 25 04:19:41 2015 UTC,"If anyone is interested in reading the license, it's here (minus the company secret thingies): http://www.sec.gov/Archives/edgar/data/2488/000119312509236705/dex102.htm"
hardware,3b0ytk,dylan522p,5,Thu Jun 25 07:37:34 2015 UTC,"While CISC vs RISC are popular terms, few chips actually fall neatly into one class vs another. Therefore, I hear very few people who actually work on processors use those terms these days."
hardware,3b0ytk,astalavista114,1 point,Thu Jun 25 11:21:42 2015 UTC,"Interesting. What terms are used, then?"
hardware,3b0ytk,dylan522p,6,Thu Jun 25 13:28:56 2015 UTC,"The distinction between RISC and CISC was meaningful in the '80s, but fell apart by the mid-'90s. Nowadays, people talk about specific microarchitecture features: in-order or out-of-order, superscalar, speculative, SISD vs SIMD and SIMD width. The distinction between a load/store architecture vs a register+memory architecture can still be made but most ISAs no longer fall into just one of those categories (eg. most machines have at least some instructions that can mix register and memory operands, but their SIMD instruction sets may be more exclusively load/store)."
hardware,3b0ytk,astalavista114,2,Thu Jun 25 18:01:13 2015 UTC,"Agree with what you said. I would also include pipeline depth (there is a huge trade-off there that some would say was screwed up on P4), hyper-threading (a fairly interesting architectural design), and when one gets into the actual implementation, things like place&route vs custom blocks becomes interesting. Inside custom, questions like circuit style come up. Power management (clock gating, on-chip regulators, etc) also dominate many processor implementation discussions these days due to laptops, mobile, etc.  I have been out of the loop for a few years, so I am sure there are things I am missing."
hardware,3b0ytk,dylan522p,4,Fri Jun 26 11:33:08 2015 UTC,Can somebody explain to me why AMD's x86 license is non-transferable?
hardware,3b0ytk,Viper_ACR,5,Thu Jun 25 06:02:33 2015 UTC,"Likely because its written in the licensing terms somewhere. IRC AMD could be bought by another company, then the whole cross patent licensing agreement seems to become invalid, at which point the two companies could start sue each other again (both have important patents that are infringed by the other) and work out a new cross patent licensing agreement."
hardware,3b0ytk,everyZig,6,Thu Jun 25 06:32:55 2015 UTC,"cough x86-64, which AMD developed from x86. Intel wouldn't be able to use it because it because it was AMD tech, and AMD couldn't use it because it relies upon x86, which Intel developed.  Now that would be a highly entertaining shitshow if it wasn't for the fact that we are pretty dependent on x86-64 these days."
hardware,3b0ytk,seven_seven,1 point,Thu Jun 25 11:59:43 2015 UTC,Fairly certain that if AMD was bought or majorly restructured intel could use the x86-64 and not get sued for infringement by the new company
hardware,3b0ytk,_chrisc_,2,Thu Jun 25 21:01:23 2015 UTC,"Depends on the terms of the cross licence. I know that if AMD were to get majorly restructured or bought out, then the terms of the agreement mean that whatever became of AMD, and their new parent, would not retain the licence to the x86 IP, so that Intel still controls who has access to the x86 IP. I would assume that similar terms are also in place for x86-64.   That said, Intel could just revive itanium if things turned into too much of a shitshow."
hardware,3b0ytk,barthw,2,Thu Jun 25 21:25:13 2015 UTC,No for Intel they are allowed to continue to use x86 no matter what if amd gets bought out
hardware,3b0ytk,GoGoGadgetReddit,2,Fri Jun 26 00:55:12 2015 UTC,"What about the -64 extensions to x86 though? AMD has most of the IP for that, which are part of the cross-licensing."
hardware,3b0ytk,wtallis,2,Fri Jun 26 01:00:45 2015 UTC,And the agreement says being bought out doesn't mean intel stops getting x86-64 just amd might lose their licensing. Same vice versa. or else someone could have bought amd and held intel by the balls without much costs.
hardware,3b0ytk,GoGoGadgetReddit,1 point,Fri Jun 26 01:11:08 2015 UTC,"That said, Intel could just revive itanium   Pretty sure they actually use Itanium in some server processors currently."
hardware,3b0ytk,wtallis,5,Fri Jun 26 03:55:36 2015 UTC,"As rndnum123 says, its most likely written into the agreements to protect the licensee against hostile takeovers of the license.  If the license was transferable, AMD could be taken over by whoever, and that party would then suddenly have a license to intel patents/tech without intel agreeing to it. Intel should be allowed to license their tech however they want, opening up a backdoor for anyone to simply buy a license (by buying AMD, and potentially spinning off any unwanted parts) without intel's approval would violate that."
hardware,3b0ytk,GoGoGadgetReddit,2,Thu Jun 25 07:16:07 2015 UTC,The simplest explanation is money.
hardware,3b0ytk,wtallis,3,Thu Jun 25 06:28:08 2015 UTC,Shouldn't those patents have expired by now?
hardware,3b0ytk,Viper_ACR,3,Thu Jun 25 05:28:44 2015 UTC,"New instructions continue to get added to x86, like AVX and transactional memory, which each get patented individually."
hardware,3b0ytk,MisterKrift212,3,Thu Jun 25 20:42:24 2015 UTC,"Great write up, much appreciated!"
hardware,3b0ytk,MisterKrift212,5,Thu Jun 25 04:34:08 2015 UTC,"Whatever language written into any legally bound license agreement supersedes patent expirations. So for example, if a party willingly enters into a license agreement to pay X dollars every year for 30 years, then the fact that a patent only lasts 20 years (in the US) does not alter or prematurely end that license agreement.  Also, copyrights protect chip die layout artwork. Copyrights last significantly longer than patents, and may be a factor here as well."
hardware,3b3wdb,C2233,7,Thu Jun 25 19:29:49 2015 UTC,"I believe the Zotac ships at a higher overclock than the G1, BUT it has no idle fanless mode. And since the G1 is going to give just about the best overclock possible with air, I'm going G1 100%. The review for the Zotac said it was fairly loud."
hardware,3b3wdb,Zarknox,2,Thu Jun 25 20:02:44 2015 UTC,"To be fair, custom fan ramps aren't fucking witchcraft, but yeah the G1 is the better card."
hardware,3b3wdb,continous,1 point,Tue Jun 30 04:30:00 2015 UTC,"I assume if it doesn't advertise 0 rpm mode that it'd be possivly a bad decision to do that, although I've heard of people doing it to like the 970s"
hardware,3b3wdb,Zarknox,1 point,Tue Jun 30 04:33:48 2015 UTC,Not necessarily. As long as it idles with the fan off at under 70C then fans off isn't insane.
hardware,3b3wdb,continous,1 point,Tue Jun 30 04:42:31 2015 UTC,Considering my G1 with the 144hz issue thing idles at like 57C and caused my CPU with a Noctua D15 on it to rise like 10-15C idle I'd really suggest not letting GPU idle above like 50C lol
hardware,3b3wdb,Zarknox,3,Tue Jun 30 04:59:41 2015 UTC,That sounds like a different sort of issue altogether.
hardware,3b3wdb,continous,2,Tue Jun 30 05:07:20 2015 UTC,"Not sure what you mean, 144hz displays don't let NVIDIA cards idle causing it to heat up   Purely temps from the GPU"
hardware,3b3wdb,Zarknox,2,Tue Jun 30 05:12:54 2015 UTC,"My point is that if the temps are under 70 but causing issues still, whether the fan is off or on doesn't matter."
hardware,3b3wdb,continous,2,Tue Jun 30 05:26:17 2015 UTC,"I guess no issues, I'd just rather not have my GPU running as if it were high load all the time."
hardware,3b3wdb,Zarknox,2,Tue Jun 30 05:27:48 2015 UTC,"I can understand that, I'm just saying that it'd be silly to blame that on the fan being off. :P Take for example my GPU (MSI G1 980). I run 2 monitors off of it and one of them is at 75Hz. To boot the ambient temperature is around 40-45C and the card idles at just above ambient when the fan is off. But god forbid I launch foobar2k and play some music. Then it hikes up to 60C. However I still keep the fans off. Why? Because it still stays under 70C and I prefer it to be quieter than cooler than 60C. It is a personal preference sorta thing."
hardware,3b3wdb,continous,3,Tue Jun 30 05:42:48 2015 UTC,"Comparing benchmark OCs, it seems that the Zotac will go around 1235MHz core and the Gigabyte will reach around 1302MHz  http://hexus.net/tech/reviews/graphics/84005-zotac-geforce-gtx-980-ti-amp/?page=13  http://www.guru3d.com/articles_pages/gigabyte_geforce_gtx_980_ti_g1_gaming_soc_review,36.html"
hardware,3b3wdb,Nixflyn,3,Fri Jun 26 05:30:40 2015 UTC,The correct answer is Galax.
hardware,3b3wdb,TRD099,2,Fri Jun 26 08:22:10 2015 UTC,Not familiar with their products. They seem to be focused on the upper echelon enthusiast. Makes me dream. I just learned about Zotac yesterday. Any other cool 980 ti chips coming I should wait for?
hardware,3b3wdb,McBits,3,Sun Jun 28 09:40:52 2015 UTC,"MSi might be coming out with a 980Ti Lightning, it should be pretty good. EVGA makes the Classified and KINGPIN editions, those are great too."
hardware,3b3wdb,TRD099,2,Sun Jun 28 09:50:58 2015 UTC,"I noticed the Classified 980Ti. I would pay for EVGA's service, but the Lightning is just so appealing."
hardware,3b3wdb,McBits,3,Sun Jun 28 10:05:07 2015 UTC,They might come out with a Classy KINGPIN edition. Honestly i'd probably go for the GALAX because of the white PCB and because it will most likely overclock better than the others under water.
hardware,3b3wdb,TRD099,2,Sun Jun 28 11:18:50 2015 UTC,"Might depend on a few factors.  can you fit a triple slot GPU in your case and mobo?    I understand Zotac has some sort of extended warranty, might need to compare those too.  Also what are the factory OC's? I understand Gigabyte wants you to install their software to get the true OC.  Has some lights and some silent mode button.  I'm not sure what type of lighting Zotac has, but if LED's matter to you, look into that as well..."
hardware,3b3wdb,djfakey,3,Thu Jun 25 19:37:19 2015 UTC,Triple slot?! Which one is the triple slot one?   They both advertise 4way SLI... How can that be possible with a triple slot card?
hardware,3b3wdb,Wargazm,1 point,Thu Jun 25 21:42:45 2015 UTC,"ok.  ...you can see my confusion, though, right?  What the hell is /u/djfakey talking about?"
hardware,3b3wdb,Wargazm,1 point,Thu Jun 25 22:07:22 2015 UTC,Zotac is triple slot.  Check it.   http://hexus.net/media/uploaded/2015/6/0c0ffe90-0d33-4df8-8a40-7ad41d435da3.png  And review here  http://m.hexus.net/tech/reviews/graphics/84005-zotac-geforce-gtx-980-ti-amp/  Anyway 4 way SLI can be achieved via mix and match 980 Ti so it's possible to mix 3 other dual slot cards and have this on the bottom most slot in a large case.   ZOTAC extreme offers 5 year warranty as well which is nice. If you don't want to mess with software overclock the zotac comes out of the box with a higher overclock as well which is nice. I would lean toward zotac if you could fit it but both are legit.
hardware,3b3wdb,djfakey,2,Thu Jun 25 22:27:54 2015 UTC,"Not saying you're wrong, especially since the review backs you up. But that pic still looks like a dual slot card to me."
hardware,3b3wdb,Wargazm,1 point,Thu Jun 25 23:15:22 2015 UTC,The cooler might not take up an entire 3 slots but if you look at the PCI bracket it's three wide. Space in the middle. Makes it more obvious to see.
hardware,3b3wdb,djfakey,1 point,Fri Jun 26 00:54:20 2015 UTC,"....but that still would only block 2 pci slots.  Hence, a dual slot card.    If this is a ""triple slot card"" then what is a dual slot card?    edit: This is what I consider to be a triple-slot card."
hardware,3b3wdb,Wargazm,1 point,Fri Jun 26 18:07:55 2015 UTC,edit   Hey looks like hexus link I provided was regarding the regular amp! My mistake there. However the fact remains the extreme that OP discussed is a triple slot.   from zotac website you can see the gallery which does look like the extreme is a triple slot cooler.   http://www.zotac.com/products/graphics-cards/geforce-900-series/product/geforce-900-series/detail/geforce-gtx-980-ti-amp-extreme-zt-90505-10p/sort/starttime/order/DESC/amount/10/section/gallery.html
hardware,3b3wdb,djfakey,1 point,Sun Jun 28 22:09:48 2015 UTC,"The Zotac 980 Extreme coolers are about 2.25 of a slot. The 980ti Extreme cooler looks a little taller than the 980 variant, so maybe more like 2.5 of a slot. Zotac's Omega coolers are 3 slots."
hardware,3b3wdb,BlackTriStar,2,Sun Jun 28 23:44:40 2015 UTC,you forgot to add the asus strix in this discussion :)
hardware,3b3wdb,Hidden__Troll,2,Fri Jun 26 00:13:51 2015 UTC,It's made by Asus so obviously not worth discussing it heh.
hardware,3b3wdb,TRD099,1 point,Fri Jun 26 00:35:56 2015 UTC,Why's that? Care to elaborate? I'm in the market for a 980ti so any info would be great! I was gonna wait for the release of the strix
hardware,3b3wdb,Hidden__Troll,2,Fri Jun 26 08:23:04 2015 UTC,"Well Asus is just solid like Corsair for example, they make okay stuff nothing really impressive. Galax, Gigabyte, EVGA and MSi are the brands you should looking at."
hardware,3b3wdb,TRD099,1 point,Fri Jun 26 12:33:58 2015 UTC,"Agreed when only discussing GPUs, but Asus makes exceptional components in other segments such as motherboards. I'd take an Asus motherboard over an EVGA or MSI."
hardware,3b3wdb,Syn246,2,Fri Jun 26 15:01:50 2015 UTC,"I'd never get an Asus motherboard especially for serious overclocking, EVGA and Gigabyte first, MSi and ASUS next."
hardware,3b3wdb,TRD099,1 point,Fri Jun 26 16:21:27 2015 UTC,"This was true maybe 10 years ago, but just like n newegg, they have built up a reputation and then started slacking."
hardware,3b3wdb,headband,1 point,Fri Jun 26 16:23:51 2015 UTC,"With the latest auto extreme fab asus should be pretty competitive, and the 980 to strix is built based on that. Plu s it looks badass"
hardware,3b3wdb,rogergamer,1 point,Fri Jun 26 17:20:03 2015 UTC,"output connector for me. I'd go with the G1. I also think it looks killer. Heres' a ""stellar"" review: http://www.techpowerup.com/reviews/Gigabyte/GTX_980_Ti_G1_Gaming/"
hardware,3b15wj,TaintedSquirrel,24,Thu Jun 25 03:22:04 2015 UTC,Don't know how much of an effect cpu has on firestrike but they also bumped cpu speed to 4.5ghz from 4ghz on the test.
hardware,3b15wj,Cozmo85,8,Thu Jun 25 05:17:27 2015 UTC,"You can look at just the graphics score.  It went from 16k to nearly 20k.   Edit: Guru3D's article including 3DMark Firestrike scores  http://www.guru3d.com/articles-pages/amd-radeon-r9-fury-x-review,25.html"
hardware,3b15wj,logged_n_2_say,4,Thu Jun 25 12:35:48 2015 UTC,"where's the original graphics score? all i can see is the 19,321 for the overclocked, but the OG only shows the total score."
hardware,3b15wj,logged_n_2_say,2,Thu Jun 25 15:09:57 2015 UTC,"Guru3D's review that has the same setup.   http://www.guru3d.com/articles-pages/amd-radeon-r9-fury-x-review,25.html  A number inline with a 20% boost. It's actually just below what my two 290s at stock score, which is around 20-21k 21598."
hardware,3b15wj,w00t692,2,Thu Jun 25 19:27:43 2015 UTC,"rock and roll, thanks."
hardware,3b15wj,Amenohitotsunokami,4,Thu Jun 25 19:56:17 2015 UTC,4 to 4.5 ghz is worth at least 6-800 points for sure on a 5960x.  Maybe more.
hardware,3b15wj,Dnc601,4,Thu Jun 25 15:10:03 2015 UTC,Wouldn't it be even better using a CPU that can reach 5 GHz? I'm asking because I'm still on my i5 2500k and am getting a custom loop.
hardware,3b15wj,Amenohitotsunokami,18,Thu Jun 25 07:58:33 2015 UTC,"Yes,  but the point is to get a consistent score based off of the processor being a constant and the gpu being the variable.  They are going to compare these tests to the rest of their oc tests as well."
hardware,3b15wj,w00t692,1 point,Thu Jun 25 09:06:17 2015 UTC,I see.
hardware,3b15wj,savagebunny,2,Thu Jun 25 09:34:28 2015 UTC,More cores is actually better on the physics test.  A 5960x at stock speed will shit on an overclocked i7-4790k.
hardware,3b15wj,glr123,4,Thu Jun 25 15:10:51 2015 UTC,"Mentioned it was changed in CCC, most likely they could've already unlocked the BIOS R/W, lifted that limitation of overclocking HBM, higher headroom on power limit (Thus wide range PL in CCC) and went to town."
hardware,3b15wj,savagebunny,2,Thu Jun 25 04:28:36 2015 UTC,I'm sure we will see that in the coming weeks!
hardware,3b15wj,zmeul,1 point,Thu Jun 25 06:23:08 2015 UTC,"Hopefully. I won't be getting it since I already have my 980, but I hope for new owners they can do this without massive risk of bricking your card. Properly done, likely won't happen, but then again anything is possible"
hardware,3b15wj,Griffith1984,14,Thu Jun 25 21:46:05 2015 UTC,how exactly did they do it? just by increasing the power limit!?
hardware,3b15wj,Seclorum,16,Thu Jun 25 03:32:48 2015 UTC,Did you see the 980 TI score was still way better? Because that's my takeaway.
hardware,3b15wj,Colorfag,36,Thu Jun 25 03:56:37 2015 UTC,"That's because the  980ti can OC way higher because it has proper controls available.   Fury X does not.   Still, it's a shame with all their hype that they didn't work with anyone or give us the controls we need."
hardware,3b15wj,XGUNNARX,5,Thu Jun 25 04:52:17 2015 UTC,"Yeah, they OCd the 980 TI and it got a ridiculously better Firestrike score."
hardware,3b15wj,Colorfag,19,Thu Jun 25 04:21:25 2015 UTC,well is the voltage unlocked on the TI?  you wont get better fury overclocks until the voltage is unlocked :/
hardware,3b15wj,XGUNNARX,3,Thu Jun 25 04:29:12 2015 UTC,"I dont know what they did. The last line of the article ""To put the above result in to perspective we managed an overclocked score in 3D Mark on our Gigabyte GeForce GTX 980 Ti of  19359 points. ""  The Fury X only achieved 16963 after being overclocked."
hardware,3b15wj,Griffith1984,14,Thu Jun 25 04:58:57 2015 UTC,they bumped up the clock and memory... without overvolting... you can get much higher clocks and memory with more volts! what im saying is the TI is unlocked.. you can increase the volts.. you cant do that right now with the fury...   same thing goes for cpu's you can only overclock so much on stock volts...
hardware,3b15wj,diggs747,13,Thu Jun 25 05:11:57 2015 UTC,First it was wait till the announcement at E3 you will see what this card can do. Then it was wait till people have the card and you will see it's a beast. Then it was wait till the right people get the card those guys are anti AMD. Then it was wait till they optimize their drivers. now it's wait till we can get the right tools to overclock this bad boy.   I am sorry but people like you is why AMD get's away with this kind of shit
hardware,3b15wj,Griffith1984,13,Thu Jun 25 07:00:48 2015 UTC,"I don't think you should be downvoted, this is exactly how I feel too. I'm a huge amd fan, they almost always have the best bang for buck. But for the last few months I waited patiently, eating up every rumor- only to be somewhat disappointed. Amd did Ok, but this time they are not the best bang for buck. Even though I dislike Nvidia, I'm always going to go with the best card for my hard earned money. This time Nvidia won, and for once it's the AMD guys making excuses."
hardware,3b15wj,Pyrominon,2,Thu Jun 25 08:13:46 2015 UTC,Thank that's my point if we keeping praising AMD for their failures they will never even try to suceed.
hardware,3b15wj,Silas13013,0,Thu Jun 25 16:57:49 2015 UTC,"Are you serious? You can't compare over clocking on a card with locked voltage, vs one with it unlocked. It's apples and oranges."
hardware,3b15wj,w00t692,9,Thu Jun 25 08:27:24 2015 UTC,The point I think he is trying to make is that we have been waiting on the fury release for a while now.  It's exhausting to keep hearing over and over 'just wait to see if it's good'.  We all know that the 'being the best' life cycle of any single computer component is very short; usually about a year or so before the manufacturer replaces it.  'Waiting' is something that doesn't sit well since the component is on the clock from the moment it hits the shelves.
hardware,3b15wj,AMW1011,5,Thu Jun 25 12:49:03 2015 UTC,"Sure you can.  You don't even need to touch the voltage on a 980 ti to crush the amount of overclock the fury x is capable of.  So considering that, people don't have high hopes."
hardware,3b15wj,Griffith1984,2,Thu Jun 25 14:02:14 2015 UTC,"That's irrelevant. Overclocking headroom on stock voltage doesn't matter at all.  The Fury X is obviously more conservative with its voltage to keep power draw down, just like reference Hawaii."
hardware,3b15wj,Orjan91,3,Fri Jun 26 08:31:00 2015 UTC,"Yes I am serious, you ask why I am serious because an excuse is an excuse. Once the voltage is unlocked it will be just wait till we get better blocks on this bad boy. Then in 6 months it will be hey this card is beast Nvidia sabotaged it."
hardware,3b15wj,Pyrominon,-4,Thu Jun 25 16:56:52 2015 UTC,"When the manufacturer locks the voltage on purpose, yes you can!  The 980ti and fury x are in the same price range, the fact that the 980ti has unlocked voltage gives it a huge advantage, yes. But it is by AMDs own choice that their voltage is locked. When comparing two GPUs in the same class/price range, you compare them in all aspects,and you dont give one of them more slack because they lack something..."
hardware,3b15wj,epsys,11,Thu Jun 25 09:20:07 2015 UTC,"Locking the voltage when a new card releases on a new architecture is standard practice, especially on a card which isn't getting after-market versions. Nvidia only recently unlocked the voltage of the Titan X. The voltage is unlocked on the 980ti, because well, its the same fucking card as the Titan X which has been out for months. Try informing yourself next time before rolling your face across the keyboard."
hardware,3b15wj,everyZig,1 point,Thu Jun 25 10:05:25 2015 UTC,WHY DO YOU HAVE CRUSH OUR HOPES AND DREAMS
hardware,3b15wj,namae_nanka,5,Fri Jun 26 09:54:39 2015 UTC,"So, riddle me this batman, if a <10% GPU OC and a 20% memory OC combine to produce 20% performance gain, is the Fury X completely memory bandwidth limited in firestrike, despite its HBM?  And it still gets beaten to hell and back by a 980Ti..."
hardware,3b15wj,TheImmortalLS,13,Thu Jun 25 07:54:24 2015 UTC,"This isn't a 20% performance increase, they are giving the whole score after overclocking their CPU as well. Same for 980Ti. And they screwed up the url, this is the one that takes to the 980Ti overclocked benchmark.  http://nl.hardware.info/reviews/6129/9/gigabyte-geforce-gtx-980-ti-g1-gaming-review-overkloktests  They should use the firestrike ultra or extreme test at least instead of this one."
hardware,3b15wj,Dnc601,3,Thu Jun 25 09:42:27 2015 UTC,It does seem odd.
hardware,3b15wj,Nixflyn,1 point,Thu Jun 25 08:04:04 2015 UTC,"As it has been said in this thread,  the 980ti has voltage unlocked,  unlike the fury.  Unlocked voltage grants higher ocs"
hardware,3b15wj,thejshep,8,Thu Jun 25 09:07:27 2015 UTC,"I think you're not getting what the above poster was saying. A 10% core OC and a 20% memory OC should definitely not up a benchmark by 20% unless that memory was a bottleneck, which it totally shouldn't be. Performance just doesn't come out of nowhere. Something else has changed. Another poster here mentioned that they also OC'd the CPU from 4.0 to 4.5. Things make more sense now."
hardware,3b15wj,Nixflyn,7,Thu Jun 25 10:36:15 2015 UTC,I'm guessing a lot of that score was due to physics performing better bc of the cpu oc.
hardware,3b15wj,HeyFux0r,3,Thu Jun 25 10:44:08 2015 UTC,That's exactly what's happening.
hardware,3b15wj,w00t692,0,Thu Jun 25 11:26:39 2015 UTC,"Both benchmarks where runwith cpu @4.5  Edit: OK i understand.  Firestrike does 3 tests GPU, combined, and CPU.  with my AMD rig i tried this out  9590 stock 4.7 with GTX 970 @ 1114 stock 8845  9590 OC 5.1 with GTX 970 @ 1114 stock 9245  i7 5820K 3.8 same GTX 970 @ 1114 stock 10352"
hardware,3b15wj,TehRoot,2,Thu Jun 25 12:04:35 2015 UTC,"vs the 980 ti sure.  But for the whole performance increase from 14k to their final result, the cpu went from 4 ghz to 4.5"
hardware,3b0zx9,glr123,15,Thu Jun 25 02:30:38 2015 UTC,"The anomalies in terms of FPS can be accounted for by the system differences in what drives the GPU, RAM, CPU, SSD, MOBO, and the like can add up to several percentage difference, and then the rest of it can be explained by what they're benchmarking(position, game settings, driver options enabled, ect.)  The power consumption numbers are entirely silicon lottery. In certain cases you see the Fury X drawing less power than the 290x, other times more, sometimes the same, it's a mix mash. The 290x numbers are in turn lottery. I assume due to the numbers that the Fury X pulls around the same power as the 980ti/Titan X for all intents and purposes."
hardware,3b0zx9,kennai,3,Thu Jun 25 06:19:59 2015 UTC,"If anything, I think that it just means we need to be a little more impartial when comparing benchmarks. As always, people are picking and choosing depending on which way they fall (amd vs nvidia preference) and their favorite reviewers. Of course that will always happen, but I think in general it means people shouldn't be too disappointed by day one performance."
hardware,3b0zx9,TehRoot,10,Thu Jun 25 06:21:31 2015 UTC,Averaging benches together would be the easiest choice.
hardware,3b0zx9,Cueball61,6,Thu Jun 25 06:43:03 2015 UTC,"It astounds me that there isn't a site for this. I wonder if the review sites would even partake in an aggregate benchmark service, it could take away viewership but also provide it through link backs..."
hardware,3b0zx9,Alarchy,1 point,Thu Jun 25 08:22:45 2015 UTC,"Since the test conditions (CPU, scene, motherboard, RAM, etc.) are so varied, I don't think averaging would be useful at all. Maybe just % comparisons within one site's benchmarks, and then a comparisons of the different percentages?"
hardware,3b0zx9,0Ninth9Night0,1 point,Thu Jun 25 15:01:13 2015 UTC,"Or at least just agree on a common testing methodology. Obviously it is common practice to run multiple tests. What isn't the same however, is the platform and hardware used during the test. The devices used to measure TDP/thermal output, wattage draw, etc. A few sites at least attempt to utilize the most objective method and explain their reasoning, but that's far from all reviews being commensurate."
hardware,3b0zx9,namae_nanka,7,Thu Jun 25 17:42:42 2015 UTC,"I saw this posted in the /r/AMD subreddit and thought it was interesting. One of the things I had noticed today reading reviews was that they didn't really seem to agree with each other very well. There appeared to be quite a few inconsistencies.  Fortunately, someone else took the time to collect all this information. It is definitely worth thinking about before too many opinions are made!"
hardware,3b0zx9,LiberDeOpp,15,Thu Jun 25 02:32:16 2015 UTC,"They seem to use different scenes, some difference would come from there as well."
hardware,3b0zx9,LiberDeOpp,6,Thu Jun 25 02:49:53 2015 UTC,"I've read most of the reviews and I've come to the conclusion that if you want overclocking go 980ti, if you want AIO cooler go Fury X. Otherwise it's fanboys choice.  If I had the money for either of these I wouldn't get either as right now I think 1080p is perfect until we get HBM 2.0. 4k/3d just isn't viable, monitors and multi-card setups are just too costly."
hardware,3b0zx9,LiberDeOpp,3,Thu Jun 25 05:19:28 2015 UTC,"I agree with you on pretty much all of your points. I think that the freesync/g-sync debate also factors in, as that will be anywhere from $150-300 cheaper for a freesync option, and then the 5-10% decreased Fury X performance looks pretty reasonable.  My only reason I would really consider the 980Ti over the Fury X would be for 1080p 144Hz gaming, as I think that is still a pretty hard goal to reach and the 980Ti has a little bigger lead at the lower resolutions."
hardware,3b0zx9,Pyrominon,3,Thu Jun 25 05:50:30 2015 UTC,"I like the 980ti, if I had the money to burn, the after market coolers and oc turn them into real monsters. I also use a vg248qe so I'm on board with the 144hz crowd."
hardware,3b0zx9,foxtrot1_1,0,Thu Jun 25 06:20:43 2015 UTC,"We don't know what OC will do to a Fury X yet, so we can't really make any conclusions before the data comes in."
hardware,3b0zx9,spencer32320,-3,Thu Jun 25 06:23:51 2015 UTC,http://us.hardware.info/reviews/6158/19/amd-radeon-r9-fury-x-review-amds-new-flag-ship-graphics-card-overklocking-results  Early data is not too promising for fury x so far.
hardware,3b0zx9,fullofbones,4,Thu Jun 25 06:44:56 2015 UTC,"The voltage is still locked, you can't possibly draw any reasonable conclusions about its over clocking capabilities until it's unlocked."
hardware,3b0zx9,Darkstryke,1 point,Thu Jun 25 08:32:49 2015 UTC,"It is, however, weird that the voltage is locked on review models with AIO coolers."
hardware,3b0zx9,eternia5,2,Thu Jun 25 13:56:00 2015 UTC,Not really. It's pretty normal for a new architecture to have its voltage locked when it releases. They usually unlock it within a month.
hardware,3az90e,DarthFrog,10,Wed Jun 24 18:15:32 2015 UTC,but essentially the disk is donezo.   What do you think about the logistics of refreshing the HDD's helium to get it reading again? Is this going to be added to HDD recovery specialists' repertoire?
hardware,3az90e,SPOOFE,68,Wed Jun 24 20:14:06 2015 UTC,Heard they were flying off the shelves
hardware,3az90e,L1cketyspl1t,16,Wed Jun 24 21:37:49 2015 UTC,Eventually the Helium will escape - I'm sure the manufacturers made sure this would happen shortly after the warranty expires.
hardware,3az90e,sfacets,6,Thu Jun 25 00:20:31 2015 UTC,"Yeah, that's pretty much what I figured.  The drives are intended for the enterprise market AFAIK.  Thus as long as they hold up for the 5 year warranty period it's good enough for the manufacturer."
hardware,3az90e,jonythunder,4,Wed Jun 24 23:53:46 2015 UTC,"Nah, don't count on it. On enterprise hardware it really matters if you can have a drive 8 or 10 years without any hitch. Believe me, it's better in the long run to lose a few thousand/million and have that turn out into ""reliable, quality and long-lasting"" rep, because that matters a lot for some companies. Except seagate, they just don't care about their rep, lol"
hardware,3az90e,jonythunder,3,Thu Jun 25 02:01:36 2015 UTC,Agree with your evaluation of Seagate; I won't buy their consumer drives anymore.  I've had more Seagate drives fail on me than any other brand by far.
hardware,3az90e,MrTurek,6,Thu Jun 25 04:16:45 2015 UTC,"In the IT lab where I work at uni, almost all drives are seagate consumer drives (80GB, very old, SATA-1). When I got there and noticed that, and that all where from the same batch, I felt like I was working inside a room full of explosives waiting to go off..."
hardware,3az90e,DomUK89,39,Thu Jun 25 04:36:58 2015 UTC,"If the helium can leak out, wouldn't it eventually result in a vacuum inside the drive?"
hardware,3az90e,axapak,29,Thu Jun 25 04:49:28 2015 UTC,Your question resulted in a few interesting posts and doesn't deserve to be in the negative. It annoys me to see scientifically curious people being responded to negatively. That's not a culture we should be nurturing.  edit: Balance has been restored! :D
hardware,3az90e,CarVac,34,Wed Jun 24 19:45:33 2015 UTC,Wouldn't air just displace the leaking helium?
hardware,3az90e,axapak,36,Wed Jun 24 22:51:33 2015 UTC,"No, because helium can go through things air cannot."
hardware,3az90e,JmamAnamamamal,2,Wed Jun 24 19:46:13 2015 UTC,I guess it would depend on how large the leak was.
hardware,3az90e,Rothaga,25,Wed Jun 24 22:12:10 2015 UTC,"No, it's not a leak thing. Helium will go outside through 'holes' which air cannot fit."
hardware,3az90e,Retmas,4,Wed Jun 24 23:13:25 2015 UTC,Thus my original question.  What happens to the drive after the He leaks out?
hardware,3az90e,FartingBob,8,Wed Jun 24 23:31:41 2015 UTC,"Uh, if it was in a true vacuum, wouldn't it have no gas to slow down the disc? I guess I don't really understand why we're using helium in the first place."
hardware,3az90e,Rothaga,10,Thu Jun 25 02:09:42 2015 UTC,"i slept through most of physics, but wouldnt the helium just bleed out until a pressure equilibrium was attained? e.g. it wouldnt go true vaccum, just ""neutral pressure"" or something sciency-sounding?  e: better idea, OP, can you crosspost to /r/askscience?  e2: nevermind"
hardware,3az90e,FartingBob,2,Thu Jun 25 02:42:11 2015 UTC,"IIRC They are using helium because it provides less air resistence, which means things are easier to move and there is more consistency which means they can move things a bit closer to eachother, which increases density.  Its not a massive difference, but when you look at how precise a modern HDD has to be to read and write millions of bits per second then a tiny improvement results in a noticeable difference."
hardware,3az90e,JmamAnamamamal,1 point,Thu Jun 25 02:47:20 2015 UTC,"Right, so a vacuum wouldn't have any gas to provide resistance."
hardware,3az90e,axapak,1 point,Thu Jun 25 17:14:16 2015 UTC,"A (near) vacuum would mean almost no resistance, but is much harder to maintain in such a form factor, helium is a compromise i guess. I'm no expert on HDD manufacturing though."
hardware,3az90e,ornothumper,1 point,Thu Jun 25 17:28:24 2015 UTC,"Probably nothing. Unless the drive requires the internal pressure provided by the helium, it will just stay the same."
hardware,3az90e,CarVac,1 point,Thu Jun 25 18:59:05 2015 UTC,"Ahh, I see."
hardware,3az90e,reynardtfox,-1,Thu Jun 25 11:14:44 2015 UTC,"This comment has been overwritten by an open source script to protect this user's privacy.   If you would like to do the same, add the browser extension TamperMonkey for Chrome (or GreaseMonkey for Firefox) and add this open source script.     Then simply click on your username on Reddit, go to the comments tab, and hit the new OVERWRITE button at the top."
hardware,3az90e,Anarch157a,5,Thu Jun 25 00:57:15 2015 UTC,"No, helium goes through anything, even if it's not leaking.   Of course if you have an actual leak, helium will escape rather rapidly."
hardware,3az90e,JewInDaHat,3,Thu Jun 25 01:29:03 2015 UTC,that or it'd collapse after a certain point when atmospheric pressure became too much for it (or maybe never if it is really constructed that robustly).
hardware,3az90e,Retmas,39,Thu Jun 25 01:31:33 2015 UTC,"You can't have a vacuum forming spontaneously inside the drive while having normal pressure on the outside. If the drive is well sealed enough to keep larger molecules out (mostly N2,O2 and H2O, those are responsible for more than 99% of Earths atmosphere), the most that can happen is the drive will slowly leak He until the pressure inside matches the pressure outside, then it'll reach equilibrium, at which point no more He should leak out.  If drive manufacturers did their homework, this equilibrium should be reached by the time the drives reach their expected life spans. Depending on manufacturer, this should be around the 5 or 6 year mark.  To tell the truth, I'd be more worried about the lifespan of the main spindle than the ammount of He."
hardware,3az90e,electrocamel,44,Wed Jun 24 19:47:52 2015 UTC,No. It will stop at He equilibrium not just gas pressure equilibrium. The pressure of He inside will be equal to the partial pressure of He outside. But He is a very rare gas in the atmosphere. Roughly 0.000524%. So the pressure inside the hard drive would be equal to 0.000524% of atmospheric pressure. This pressure is called a medium vacuum. Of course its only true if we assume that no other gas can pass the barrier.
hardware,3az90e,CarVac,5,Wed Jun 24 20:09:01 2015 UTC,oh hey i guess we dont need askscience after all. i should read the whole thread before shitpoasting up top. (gonna link to this a little higher up mate)
hardware,3az90e,TheImmortalLS,5,Wed Jun 24 20:25:11 2015 UTC,Doesn't that only apply to liquids via osmosis?
hardware,3az90e,CarVac,10,Thu Jun 25 02:48:53 2015 UTC,"Nope, any equilibrium system."
hardware,3az90e,bobby177,1 point,Wed Jun 24 20:30:49 2015 UTC,Including dust in pc's   Nothing is dust proof. Filters only work by making dust in smaller.
hardware,3az90e,andrewcooke,1 point,Wed Jun 24 22:11:34 2015 UTC,"You just had to say that to me, didn't you?   I have my scratch built case pull all of its air through a MERV-11 furnace filter and inside is as clean as the day I built it.  That said, I know it's not dust proof, just that over the lifetime of the parts it will never collect enough dust to matter (though I do replace the filter every 90 days)."
hardware,3az90e,Arduous_Armadillo,3,Thu Jun 25 08:13:48 2015 UTC,"That just makes no sense to me. So if you have a (perfectly rigid) box surrounded by 200000000000Gpa of oxygen, in an enormous room, thousands of miles wide, and you fill the box with 1Pa of helium, the helium will escape until it equalize with the helium in the room, resulting in a near-vacuum inside the box?"
hardware,3az90e,Badass_Norwegian,14,Thu Jun 25 13:11:47 2015 UTC,"well, assuming that the oxygen cannot enter.  it makes sense if you think about the atoms as little balls bouncing around against a membrane (wall) with holes in it.  if the oxygen atoms are big balls - larger than the holes - then they just bounce off the wall.  so all you care about are the little balls (the helium).  and then it's pretty clear that you get equilibrium when the frequency that the little balls ""inside"" bounce against the wall (sometimes going through holes) equals the frequency that the little balls ""outside"" bounce against the wall (also going through holes sometimes).  because equilibrium is when as many little balls enter as leave.  (well, you asked about 200000000000Gpa - maybe at that point oxygen is solid and so blocks the helium; the above is only true when the gases are approximated well by bouncing balls (""ideal gases""), which is true in ""normal"" conditions)."
hardware,3az90e,JewInDaHat,4,Thu Jun 25 01:04:50 2015 UTC,"Yes, it's a little counter intuitive, but the reason is because equilibrium means when input equals output.  So think if it as being the rate at which helium escapes being equal to the rate it goes into the drive. This is because helium is the only element small enough to penetrate, so it's the only one we look at."
hardware,3az90e,Badass_Norwegian,1 point,Thu Jun 25 01:48:25 2015 UTC,Wouldn't it be helium AND hydrogen? Hydrogen is smaller than helium.
hardware,3az90e,dragonbud20,1 point,Thu Jun 25 01:42:32 2015 UTC,"Helium is a noble gas (do not form a molecule, separate atoms are flying around only) so it penetrate better than hydrogen that is not noble and form a H_2 molecule. Hydrogen is also a very rare gas in the atmosphere even more rare than krypton, roughly 10 time rarest than helium"
hardware,3az90e,dragonbud20,1 point,Fri Jul 3 23:18:36 2015 UTC,WAIT WE ARE ALL WRONG I just remembered about quantum tunneling and atoms not having a fixed size.  F*ck you quantum physics.
hardware,3az90e,epsys,-1,Fri Jul 3 23:40:47 2015 UTC,this would only apply if the gases could flow both ways. Our assumption here is that only the helium can flow out and nothing can flow in in which case the helium will reach equilibrium with gas pressure.
hardware,3az90e,JewInDaHat,7,Sat Jul 4 02:50:48 2015 UTC,"No it won't.  It's the partial pressure of He that's the relevant parameter, not the total air pressure."
hardware,3az90e,autowikibot,1 point,Thu Jun 25 01:13:38 2015 UTC,Honestly it's been a while since physics class so I'm just going off the bits I can remember; I'm likely mixing up things in my head.
hardware,3az90e,TheImmortalLS,0,Thu Jun 25 02:08:19 2015 UTC,I feel like when it's this low there aren't helium molecules lined up waiting their turn in the atmosphere to bump into the helium molecules pushing out of the enclosure
hardware,3az90e,epsys,1 point,Thu Jun 25 03:09:20 2015 UTC,In gases collisions between molecules are very rare. Especially compared to collistion rate between He and molecules of the barrier of the container it successfully passed at the point it could collide with the atmospheric gas molecules. He particle that made its way through the barrier will simply fly away with almost no resistance from gas molecules outside.
hardware,3az90e,TheImmortalLS,1 point,Thu Jun 25 02:43:44 2015 UTC,"Mean free path:       In physics, the mean free path is the average distance traveled by a moving particle (such as an atom, a molecule, a photon) between successive impacts (collisions),  which modify its direction or energy or other particle properties.  The following table lists some typical values for air at different pressures and at room temperature.    Image i     Relevant: Inelastic mean free path | Knudsen number | Electrical mobility | Thermal conductivity   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3az90e,sfacets,1 point,Thu Jun 25 07:42:03 2015 UTC,Helium out is equal to helium in. Equilibrium.
hardware,3az90e,andrewcooke,1 point,Thu Jun 25 07:42:24 2015 UTC,"ah, so sometimes one escapes, but one gets back in. I guess that works"
hardware,3az90e,fgalv,1 point,Thu Jun 25 08:12:49 2015 UTC,Yup. It depends on concentration mostly.
hardware,3az90e,sfacets,3,Thu Jun 25 16:45:44 2015 UTC,"by the time the drives reach their expected life spans.   Or warranty expiry dates, anyhow."
hardware,3az90e,sfacets,2,Fri Jun 26 04:56:46 2015 UTC,"yes, practically (not a perfect vacuum, but pretty good, assuming other common gases can't enter).  unfortunately the top-voted answer to you is incorrect."
hardware,3az90e,hacknut937,4,Wed Jun 24 23:58:27 2015 UTC,not unless they also built a vacuum pump into your hard drive
hardware,3az90e,autowikibot,1 point,Thu Jun 25 01:53:05 2015 UTC,"Would this be terribly difficult? (serious) It wouldn't need to be integrated to the drive itself, but could be a component of the PC/Case."
hardware,3az90e,splenetic,1 point,Wed Jun 24 20:20:45 2015 UTC,"This is an interesting question... and if there is a vacuum, wouldn't the drives spin that much better?"
hardware,3az90e,wasprocker,5,Wed Jun 24 23:59:44 2015 UTC,"https://en.wikipedia.org/wiki/Flying_height  actually, it would be a very very bad thing"
hardware,3az90e,Bacontroph,1 point,Wed Jun 24 23:57:51 2015 UTC,"Flying height:       The flying height or floating height or head gap is the distance between the disk read/write head on a hard drive and the platter. The first commercial hard-disk drive, the IBM 305 RAMAC, used forced air to maintain a 0.002-inch (51-micron) spacing between the head and disk. The IBM 1301, introduced in 1961, was the first disk drive in which the head was attached to a ""hydrodynamic air bearing slider,"" which generates its own cushion of pressurized air, allowing the slider and head to fly much closer, 250 microinches (6.35 micrometers) above the disk surface.      Relevant: 10 nanometres | TopoFlight | Disk read-and-write head | Ader Éole   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3az90e,CarVac,1 point,Thu Jun 25 02:23:25 2015 UTC,"Yes, but the heads would crash into the disk. Hard disk heads use fluid dynamic effects to ""fly"" a fraction of a millimetre above the disk surface. The closer you can get them to the disk surface the higher the read/write density but you don't want them touching because friction would increase massively and you'd very quickly wear away the recording surface."
hardware,3az90e,thisisfor_fun,0,Thu Jun 25 02:24:07 2015 UTC,Or the decreased pressure inside the harddrive helps the helium stay inside
hardware,3az90e,noipv4,20,Thu Jun 25 10:40:29 2015 UTC,The drives are hermetically sealed so it theoretically should be gas tight.  This includes being impervious to the tiny He atom.  Make sure you secure it in your case with screws though or the drive will float away.
hardware,3az90e,epsys,54,Thu Jun 25 01:25:47 2015 UTC,"Helium doesn't care about hermetic seals, it will diffuse straight through metal over time.   That said, the hermetic seals help slow the loss."
hardware,3az90e,autowikibot,10,Wed Jun 24 22:04:20 2015 UTC,"Exactly.  Also hermetically sealed means air-tight, against much bigger molecules of N2 & O2.  It won't hold He indefinitely."
hardware,3az90e,foshaug,1 point,Wed Jun 24 22:06:33 2015 UTC,"I thought hermetically sealed referred to a specific gas that was being contained, not just the general reference to airtight?"
hardware,3az90e,cheesyguy278,3,Thu Jun 25 02:05:36 2015 UTC,true dat https://scholar.google.ch/scholar?q=helium+diffusion+through+material&hl=en&as_sdt=0&as_vis=1&oi=scholart&sa=X&ei=m0SLVYmnDMTtUoLwgOAC&ved=0CBsQgQMwAA
hardware,3az90e,skalpelis,1 point,Thu Jun 25 06:05:43 2015 UTC,my eyes! I'm blind!
hardware,3az90e,cheesyguy278,7,Thu Jun 25 00:01:03 2015 UTC,"Hermetic seal:       A hermetic seal has the quality of being airtight. In common usage, the term often implies being impervious to gases. When used technically, it is stated in conjunction with a specific test method and conditions of usage.    Image i     Relevant: Seal (mechanical) | Gas compressor | Mason jar   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3az90e,Tuna-Fish2,7,Thu Jun 25 02:45:55 2015 UTC,Aren't we already running low on earth's helium supply?  Sounds like a bad idea to wasting it on improving spinning drive speed if you ask me.
hardware,3az90e,tolesto,55,Wed Jun 24 22:04:38 2015 UTC,"If we want to stop wasting helium, lets stop filling party balloons with it before we stop using it for more practical and useful things."
hardware,3az90e,Tuna-Fish2,15,Wed Jun 24 22:20:34 2015 UTC,"Balloons use a minuscule amount compared to other uses, e.g. cooling MRIs, superconducting magnets, electronics manufacturing, and rocket fuel. I do agree that we shouldn't waste it but banning party balloons would do nothing in the big picture."
hardware,3az90e,jonythunder,6,Wed Jun 24 22:24:30 2015 UTC,God damn I hope we start mining the moon and asteroids for Helium before we don't have enough to get rockets into space.
hardware,3az90e,Tuna-Fish2,7,Wed Jun 24 23:11:55 2015 UTC,"In rockets, Helium is mostly used as an inert pressure gas. It could easily be substituted with a different gas, with only a slight decrease in efficiency due to increased molecular mass.  Running out of Helium won't mean no more rockets."
hardware,3az90e,djdes,1 point,Wed Jun 24 23:55:01 2015 UTC,then why would they use expensive helium?
hardware,3az90e,NEXT_VICTIM,8,Thu Jun 25 00:22:55 2015 UTC,"The cost of a rocket launch is something like ~30-100 million. The cost of the helium used in that launch is ~ a dollar. Helium has the advantage of being light and being very chemically inert, making it safe to mix both with the propellant and the oxidizer."
hardware,3az90e,epsys,1 point,Thu Jun 25 00:52:50 2015 UTC,"Just to pitch in what /u/Tuna-Fish2 said. The difference in molecular weight from changing He (MW=4.002602 g/mol) to, let's say, Neon (MW=20.1797 g/mol) would imply a 5.0416x increase in mass. That might not sound like much to you, but since rocket engines use innert gases to pressurize fuel and oxidizer tanks to help the pumps and keep a stable pression inside the tanks we are talking about a lot of weight.  I've tried to do some rough calculations to give you an estimate, but there's a lot of data to go through and I'm still in finals ^^. Let's just say that in space, every single m/s of delta-v is important, and that by changing the gas you use it would decrease it.   In case of a satelite, it might not sound much if the delta-v change is somewhat in the area of 100m/s, but believe me that that amount is humoungous when we are talking about orbit-correction maneuvers that use very little delta-V. Now scale it several times because rocket engines use helium to keep their fuel tanks pressurized after the fuel is expended (else the tanks would implode) and you might realize that by using Neon instead of Helium as a pressurizing gas, it would have been needed a rocket bigger than the Saturn V to get to the moon :p  PS.: If this validates my ""thesis"", I'm an aerospace engineering student :p"
hardware,3az90e,please_no_photos,2,Thu Jun 25 01:02:53 2015 UTC,"However, there are options other than using an inert gas. Liquids could in principle be pressurized by a piston, that is, by designing the tanks to crumble in a controlled manner, and vaporization of the content of the tank is always a (potentially hazardous) method of maintaining pressure.  Helium is merely the most convenient currently available method."
hardware,3az90e,choppersb,2,Thu Jun 25 04:37:54 2015 UTC,"According to HGST's helium drive page, balloons are 19%, but hard drives are 1%.  It has to be true because they made a little infographic."
hardware,3az90e,TeutorixAleria,1 point,Thu Jun 25 04:41:55 2015 UTC,"If we swapped the volume used for party balloons into drives, it could potentially make a significant difference over time."
hardware,3az90e,please_no_photos,1 point,Thu Jun 25 04:04:44 2015 UTC,only running out of stockpiles we built after discovery during WW2 when we wanted it because Hitler would have wanted it for his Hindenbergs
hardware,3az90e,RPGillespie6,2,Thu Jun 25 07:20:07 2015 UTC,"There's actually plenty of helium, there's a shortage of cheap helium.  If the helium was recovered from natural gas, there'd be no shortage whatsoever.  Helium is somewhat of a renewable resource as it is a product of radioactive decay - any radio-isotope that decays by alpha emission is a helium generator.   Also the amount of helium used in all these drives combined is fairly inconsequential.  Especially compared that used for kids birthday party. balloons."
hardware,3az90e,skalpelis,2,Thu Jun 25 02:47:37 2015 UTC,I know I've heard this too but how can that be possible? Don't we just extract He from natural gas?
hardware,3az90e,funk_monk,9,Thu Jun 25 05:34:56 2015 UTC,"My understanding of it is that it's not that we can't get more, it's that we can't get more cheaply. The US government stockpiled a bunch and then later decided to sell off the a large surplus of the stockpile.  The price was fixed at a very low cost and has remained low.  ""Low"" is with respect to the cost of collecting the helium that is released when collecting natural gas, so natural gas companies have no economic incentive to add means of Helium collection to their gas extraction process."
hardware,3az90e,sfacets,6,Wed Jun 24 22:31:29 2015 UTC,Natural gas is not a renewable resource
hardware,3az90e,skalpelis,3,Wed Jun 24 23:36:35 2015 UTC,"Right, but we still have a pretty good amount of that left."
hardware,3az90e,autowikibot,1 point,Wed Jun 24 22:42:10 2015 UTC,Helium is easy to make - just fuse two hydrogen atoms
hardware,3az90e,JmamAnamamamal,16,Wed Jun 24 22:50:05 2015 UTC,"And while you're at it, just subtract three protons and electrons from lead. It's real simple."
hardware,3az90e,Sapiogram,4,Wed Jun 24 22:51:37 2015 UTC,You'll need to ditch some neutrons along the way or your gold will turn into mercury.
hardware,3az90e,Tuna-Fish2,2,Wed Jun 24 23:17:25 2015 UTC,subtract three protons and electrons from lead   Gold?
hardware,3az90e,kuuttis,1 point,Wed Jun 24 23:59:44 2015 UTC,Transmutation of base metals.
hardware,3az90e,funk_monk,2,Wed Jun 24 23:56:38 2015 UTC,"Alchemy:       Alchemy is an influential tradition whose practitioners have, from antiquity, claimed it to be the precursor to profound powers. As described by Paul-Jacques Malouin in The Encyclopedia of Diederot it is the chemistry of the subtlest kind which allows one to observe extraordinary chemical operations at a more rapid pace; ones that require a long time for nature to produce.  Defining objectives of alchemy are varied but historically have typically included one or more of the following goals: the creation of the fabled philosopher's stone; the ability to transmute base metals into the noble metals (gold or silver); and development of an elixir of life, which would confer youth and longevity.    Image from article i     Relevant: Alchemy and chemistry in medieval Islam | Alchemy (microarchitecture) | Outline of alchemy | Alchemical symbol   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3az90e,funk_monk,1 point,Thu Jun 25 00:11:13 2015 UTC,"That is actually fairly simple and do-able, just not cost-efficient."
hardware,3az90e,smartcoda,5,Thu Jun 25 00:12:04 2015 UTC,"""fairy simple"""
hardware,3az90e,Arduous_Armadillo,3,Wed Jun 24 23:30:38 2015 UTC,"And by ""not cost-efficient"", what you mean is that the process costs some 6 orders of magnitude more than the value of the output product. Transmutation is really not cost-efficient.   A fun fact is that as gold makes for good accelerator targets, we are actually currently net destroying gold (in very small amounts). I wonder what the old alchemists would think -- we learned the true secret of transmutation, and we use it to turn gold into base materials."
hardware,3az90e,Omnislip,2,Thu Jun 25 00:03:52 2015 UTC,...yet
hardware,3az90e,hacknut937,1 point,Thu Jun 25 00:28:21 2015 UTC,"Like I said I another comment, nuclear reactions are cool and all but to get any reasonable quantity of mass you're either faced with massive energy surpluses which will happily ruin your day or an infeasible energy defecit."
hardware,3az90e,Omnislip,1 point,Wed Jun 24 23:49:46 2015 UTC,And then work out what you're going to do with an energy surplus fifty times larger than current annual power demands...    I derped. It's actually about one hundred.
hardware,3az90e,Tuna-Fish2,2,Fri Jun 26 12:53:30 2015 UTC,"Extremely childish thought, but I'd be worried they'd be a bit squeaky"
hardware,3az90e,CassandraVindicated,1 point,Wed Jun 24 23:46:54 2015 UTC,It might cause bits to float up the memory stack
hardware,3az90e,PigSlam,2,Wed Jun 24 22:47:15 2015 UTC,"If the drive is properly airtight it will hold the helium for a long time.  I wouldn't want to have to repair a drive that is truly airtight, though! Seems like more of a novelty to me."
hardware,3az90e,PigSlam,13,Thu Jun 25 01:45:29 2015 UTC,you can't really repair drives these days anyway.
hardware,3az90e,PigSlam,1 point,Wed Jun 24 19:49:22 2015 UTC,I imagine the manufacturers might do some tinkering - this is what I had in mind anyway.
hardware,3az90e,narwi,2,Wed Jun 24 20:15:13 2015 UTC,"They don't. The extent of repair that they do these days is reflashing the firmware, regenerating the indirection tables, and possibly reducing the labeled capacity before selling as refurb. No-one is going to actually physically touch the drive, it's just not worth it.  The only people who have done physical drive repair in the past few years are the companies that do expensive data recovery. I don't know how well they deal with He-filled drives."
hardware,3az90e,Lostxprophit,1 point,Wed Jun 24 22:01:10 2015 UTC,I imagine they just do the repairs and lower the spindle speed. That's assuming that they bother putting it back into the enclosure in the first place.
hardware,3az90e,djdes,1 point,Thu Jun 25 00:33:02 2015 UTC,"If that's the first thing you thought of upon hearing about the technology, do you think that maybe the company selling and backing the warranty probably looked into it a bit before they sold them?"
hardware,3az90e,Lostxprophit,1 point,Thu Jun 25 01:41:59 2015 UTC,"Of course they did. But I'm doubtful that they can seal the drive well enough to contain the He indefinitely.  Thus my question, what happens after the inevitable loss?"
hardware,3az90e,Probate_Judge,0,Thu Jun 25 00:23:01 2015 UTC,"What if the time span is 20 years?  What if it's 200 years? I'm giving them the benefit of the doubt here that they looked into it, determined the leakage rate will be low enough that enough helium will stay inside the enclosure until the the drive is profoundly obsolete."
hardware,3az90e,tom6561,2,Thu Jun 25 02:00:12 2015 UTC,"I would agree that if the usable lifespan of the drive is 20 years that my question is moot as I agree they would be obsolete long before then.  However, I'm still interested in the physics of the He drives: what happens when the He leaks out?"
hardware,3az90e,BrainSlurper,1 point,Thu Jun 25 02:07:01 2015 UTC,"In terms of physics, the helium is used to reduce the friction of the spinning disks, since the gas has a lower density than air.  If the pressure were reduced, this would also reduce the friction.  I suppose this would also reduce the heat transfer capabilities of the case, so that could lead to premature failure of the bearings, or other parts, but if it were designed to work with 0 to max pressure inside, then it shouldn't matter."
hardware,3az90e,djdes,1 point,Thu Jun 25 04:35:11 2015 UTC,"Helium has a thermal conductivity of 0.142 W/min K, air has a thermal conductivity of 0.024 W/min K.  Hitachi says they're using He to reduce the turbulence, lower temperature and decrease power consumption   The higher thermal conductivity of He would help achieve the lower temps.  I wonder if that is why Hitachi is using He instead of a vacuum?   In which case, my original question would be answered by: the loss of He will lead to premature drive failure due to excess heat.   After all if it were just the reduction in turbulence/friction, a vacuum would be more effective."
hardware,3az90e,farhil,1 point,Thu Jun 25 04:56:39 2015 UTC,"Vacuum would not only be very expensive, they would need to completely redesign how the head assemblies work, as in present hard drives, the heads run on a air (or helium) cushion. Vacuum == no more cushion, heads need to be fixed differently."
hardware,3b24ns,soberjournalist,3,Thu Jun 25 10:29:07 2015 UTC,Awesome!  Decent quality budget mobos are still around.
hardware,3b24ns,stapler8,2,Thu Jun 25 15:44:12 2015 UTC,Nice job Digi Times
hardware,3b59ao,redditwentdownhill,32,Fri Jun 26 02:06:40 2015 UTC,We already had this exact post linked earlier today so I'll summarize.   Clickbaity title No good verification Same wishful thinking No facts   That's Kitguru for you. Baseless rumors endlessly reposted to drive clicks.
hardware,3b59ao,Seclorum,7,Fri Jun 26 03:17:43 2015 UTC,"That would be really nice but I doubt it will happen.  EU prices could stand to go down, for sure."
hardware,3b59ao,TaintedSquirrel,1 point,Fri Jun 26 02:11:24 2015 UTC,"I hope so, they are high."
hardware,3b59ao,epsys,1 point,Fri Jun 26 02:15:41 2015 UTC,EU   VAT makes everything expensive   really nice   this would seriously kick AMD when they're down :( I hope they don't
hardware,3b59ao,Xanabilek,5,Fri Jun 26 09:04:01 2015 UTC,"""I hope AMD sticks around so that they'll force Nvidia to cut down their prices""  ""I hope Nvidia won't cut their prices down so that AMD will be able to stick around"""
hardware,3b59ao,epsys,-2,Fri Jun 26 12:13:16 2015 UTC,"basically, yup! what's your point? competition is good. Without AMD, Intel and NVidia would charge a lot more. IBM isn't even related to AMD, but regularly chooses to invest in them and to be so because they're afraid of Intel being the major CPU manufacturer on the block. They know the danger of that kind of monopoly..."
hardware,3b59ao,continous,1 point,Fri Jun 26 17:03:05 2015 UTC,"what's your point?   That's hypocritical and stupid.   competition is good.   So hope for competition, not that one company fails and the other wins. That would instate a monopoly.   Without AMD, Intel and NVidia would charge a lot more.   The inverse is true as well.   because they're afraid of Intel being the major CPU manufacturer on the block.   Or perhaps because investing in such a company is a good idea.   They know the danger of that kind of monopoly...   Well apparently you do not.  While it does not currently look like AMD will have a monopoly, we certainly don't want them to. To try and put NVidia or Intel under would instate a monopoly, and AMD, I guarantee, will turn into what everyone hates about both NVidia and Intel, because money means more than PR."
hardware,3b59ao,abacabbmk,5,Tue Jun 30 04:46:48 2015 UTC,there is no new driver
hardware,3b59ao,SupremeGunman,3,Fri Jun 26 02:35:33 2015 UTC,All I want is a free copy of Fallout 4 with my 980Ti and I'll buy a new video card...
hardware,3b59ao,BlayneTX,7,Fri Jun 26 04:32:29 2015 UTC,That would be a sickening death blow. Do you have any evidence?
hardware,3b59ao,TruGW2,2,Fri Jun 26 02:31:50 2015 UTC,I doubt Nvidia would reduce prices to try to kill off AMD. More likely that it would be due to cards not selling well in certain regions prompting a price drop to boost sales.
hardware,3b59ao,BlayneTX,0,Fri Jun 26 03:26:56 2015 UTC,"Yeah, when I commented there was no link to a story about certain regions only."
hardware,3b59ao,TruGW2,3,Fri Jun 26 03:35:47 2015 UTC,"I didn't even read the article, I just know that those in the UK are getting screwed.   https://www.overclockers.co.uk/showproduct.php?prodid=GX-287-MS   Via Google:  599 British Pound equals 943.45 US Dollar"
hardware,3b59ao,dannybates,0,Fri Jun 26 03:46:10 2015 UTC,Was looking at buying the HoF 980ti but its £670. Can I file for fraud? It feels like it.
hardware,3b59ao,CJKay93,2,Fri Jun 26 07:53:48 2015 UTC,According to thomascook.com I could net an all-inclusive 7-night cruise around the Greek Isles for that price and still have £70 left over.
hardware,3b59ao,AssCrackBanditHunter,1 point,Fri Jun 26 11:23:22 2015 UTC,"According to my math, I could buy a 980ti and still have a few hundred dollars left over"
hardware,3b59ao,Joshposh70,1 point,Fri Jun 26 13:27:45 2015 UTC,"Overcockers price gouging, it's normal."
hardware,3b59ao,dannybates,1 point,Fri Jun 26 13:05:42 2015 UTC,Please tell me where else I can get this card in the UK
hardware,3b59ao,Nixflyn,3,Fri Jun 26 13:53:07 2015 UTC,"No new driver. The driver used in Fury X benchmarks was correct.  https://www.reddit.com/r/hardware/comments/3b518c/amd_rep_denies_wrong_driver_rumor/  EDIT: Also, where's your source?"
hardware,3b59ao,Nixflyn,1 point,Fri Jun 26 02:51:33 2015 UTC,I edited the post.
hardware,3b59ao,ElDubardo,1 point,Fri Jun 26 02:56:05 2015 UTC,Thanks
hardware,3azss3,4LTRU15T1CD3M1G0D,22,Wed Jun 24 20:42:37 2015 UTC,"It is a top performing card. I think the ""fixing"" is that amd hyped it up to be way beyond the competition."
hardware,3azss3,Imidazole0,18,Wed Jun 24 23:47:46 2015 UTC,I'm not sure if it was even AMD as much as the fanboys hyping it up
hardware,3azss3,CharlotteWins,12,Thu Jun 25 02:13:24 2015 UTC,"I don't know, their presentation was full of promises."
hardware,3azss3,Imidazole0,19,Thu Jun 25 03:04:40 2015 UTC,And their graph showing it beating the 980ti in every game lol.
hardware,3azss3,TruckChuck,2,Thu Jun 25 05:44:55 2015 UTC,Sure they cherry picked their benchmarks but it was just a frame or two advantage which really says nothing about beating a 980ti.
hardware,3azss3,MilkyTones,2,Thu Jun 25 16:33:01 2015 UTC,"""Overclocker's dream"""
hardware,3azss3,attomsk,1 point,Thu Jun 25 17:00:13 2015 UTC,Yeah I'm still waiting for proof of that! Man they said all the right keywords in their presentation.
hardware,3azss3,Imidazole0,6,Thu Jun 25 17:13:24 2015 UTC,"yes, it was definitely this entire subreddit bleeding AMD colors for like a week straight"
hardware,3azss3,poematik,20,Thu Jun 25 04:34:15 2015 UTC,Just point to the 290X.  It came out and was slower than the 780. Now it competes with the 980 at 4K.
hardware,3azss3,jinxnotit,9,Thu Jun 25 00:56:30 2015 UTC,"it was the ""titan killer"" at the time of release, but yes it does compete with a 980 at higher resolutions (when overclocked near 390x)  290x release http://tpucdn.com/reviews/AMD/R9_290X/images/perfrel_2560.gif  current http://tpucdn.com/reviews/MSI/R9_390X_Gaming/images/perfrel_2560.gif"
hardware,3azss3,logged_n_2_say,3,Thu Jun 25 01:30:43 2015 UTC,"What am I looking at here? Are you saying that the 290X now has the performance of the 390X in the second graph or just that the 290X has changed compared to the 780? Because if it's the latter, then it's something like a 7% improvement if I'm getting my maths right. Are these the same games being tested?  If it's the former then that's not comparable at all because the 390X has double the VRAM of the 290X."
hardware,3azss3,LazyGit,1 point,Thu Jun 25 13:02:33 2015 UTC,"it's a little of both.  they are changing games slightly, but kepler has been losing performance (acknowledged by nvidia) and 290x has gained some through dirver improvements.  as for the 290x performing like a 390x, a 290x will in a majority of use cases perform near identical with the same clocks as a 390x.  a 390x has stock higher gpu and memory clock, and when a 290x matches, the performance is essentially the same except at UHD resolution but you really wouldn't want to run a single 390x at UHD unless it's light gaming and in those situations an overclocked 290x will perform near identical.  proof 1 - a 390x was downclocked to meet a 290x. performance was the same.  proof 2  - the HIS 290x runs at the same clocks as a stock 390x.  it has essentially the same performance, until you get to UHD resolution, then the 390x vram helps with 10-6% performance increase."
hardware,3azss3,logged_n_2_say,0,Thu Jun 25 13:34:30 2015 UTC,"It's not even the same resolution between those two graphs, which could very easily explain the performance difference."
hardware,3azss3,computertechie,5,Thu Jun 25 13:40:03 2015 UTC,"although true, they are close enough that the relative performance to one and another can be looked at.  1440p to 1600p is very similar. they changed resolutions and games in the time frame, and there is no crossover other than 1600x900 and 1920x1080.  the reason that i choose the 1440p and 1600p because those with the top tier cards were likely above 1080p.  here is the orginal:  290x  390x  and here is the 1080p scores  290x  390x  but as i said, a big reason for the discrepancy is the reduction of kepler performance - https://www.reddit.com/r/pcgaming/comments/37hxro/nvidia_driver_update_to_fix_600700_series_kepler/  and the increase in amd drivers."
hardware,3azss3,logged_n_2_say,2,Thu Jun 25 13:51:23 2015 UTC,"I saw it especially when the 680 and the 670 came out, my hd 7970 was close to the 670 in performance, now it beats out the 770 and comes close to the 780."
hardware,3azss3,zarif98,1 point,Thu Jun 25 16:01:08 2015 UTC,"Good spot, the second graph is a lower resolution."
hardware,3azss3,LazyGit,1 point,Thu Jun 25 13:51:08 2015 UTC,"also, it should be noted the slightly higher resolution of the first score (the 1600p) should favor the AMD due to the higher bandwidth.  however the separation between the 780 and the 290x is greater with the slightly smaller resolution (1440p), proving the issue is not with the difference in resolution."
hardware,3azss3,logged_n_2_say,1 point,Thu Jun 25 15:29:32 2015 UTC,It might favour higher bandwidth but it also favours the more powerful GPU. Good performance at higher resolutions isn't just about RAM and bandwidth.
hardware,3azss3,LazyGit,2,Thu Jun 25 18:26:04 2015 UTC,Can a single 290X consistently get 60+ FPS on 1440p?
hardware,3azss3,rationis,4,Thu Jun 25 07:29:39 2015 UTC,"Heavily depends on the game, with a decent overclock you can get around 60 fps on GTAV, FC4, Metro LL, BF4, etc. But other games, like Crysis 3, Witcher 3, and Dragon Age, you'll have to substantially lower the settings."
hardware,3azss3,pb7280,2,Thu Jun 25 09:40:09 2015 UTC,"I do on most games, but Witcher 3 (newest game I've played) gets around 45fps on Ultra without hairworks. I have two now though for 4k, gets 60 on almost all games on ultra (except W3 is around 50). They're both overclocked to 1150/1550."
hardware,3azss3,rationis,3,Thu Jun 25 10:02:51 2015 UTC,"290X user here, and yep, this is true. My card used to trade blows with the 780, then traded blows with the 780Ti, and now, with a heavy overclock, can compete and even beat a slightly overclocked 980. The card has definitely gotten faster over the 2 years I've had it with driver updates. This is my hope for the Fury X, just wished it had more than 4Gb of memory."
hardware,3azss3,EleMenTfiNi,1 point,Thu Jun 25 09:34:28 2015 UTC,Wait.. when the 290x came out it was toppling the 780.. that is why NVidia had to put out the 780ti.   http://www.extremetech.com/gaming/169402-to-slay-a-titan-amds-radeon-r9-290x-piledrives-nvidias-high-end-product-line/4
hardware,3azss3,jinxnotit,3,Fri Jun 26 15:02:18 2015 UTC,"Is that what they are saying? Or are you just talking out of the side of your neck?  While I agree there isn't much reason to buy the card now, you only have to wait a month to see performance improve. I'm waiting to see what driver updates do once Windows 10 hits."
hardware,3azss3,335is,15,Thu Jun 25 14:18:54 2015 UTC,"I don't think the Fury X performance is broken. The reviews that I've seen show it competing with the 980ti very well. Both cards have their strengths, but they're fairly evenly matched. They're the same price too."
hardware,3azss3,w00t692,4,Thu Jun 25 15:00:22 2015 UTC,The only point in time they're relatively evenly matched is at 4k.  And honestly neither of them produce playable framerates in many games at 4k with anything but reasonable or low settings.
hardware,3azss3,mmencius,6,Wed Jun 24 23:10:20 2015 UTC,I thought Fury X and 980 Ti were 30 fps minimum on 4K with ultra settings on GTA 5. That's one of the hardest games to run.
hardware,3azss3,w00t692,0,Wed Jun 24 23:26:57 2015 UTC,I'd rather my games not hit 30 minumum framerates lol.  in witcher 3 i play at 1080p because the lowest framerate i'll drop to is mid 40's and that's rare.
hardware,3azss3,mmencius,3,Thu Jun 25 02:36:50 2015 UTC,True but you have freedom to turn down settings. Often there's very little visual difference between high and ultra/very high but substantial performance difference.
hardware,3azss3,XaeroR35,1 point,Thu Jun 25 03:22:30 2015 UTC,You should not have too turn down settings on a brand new $700 card
hardware,3azss3,mmencius,3,Thu Jun 25 04:21:11 2015 UTC,This comment doesn't make much sense. You don't have to turn down settings if you play at a lower resolution. You do have to turn them down if you wish to play at a high resolution and high frame rate.
hardware,3azss3,EleMenTfiNi,1 point,Fri Jun 26 02:24:40 2015 UTC,That seems rather silly.. what would anyone crossfire or SLI for then? They build games beyond the scope of a single card.
hardware,3azss3,w00t692,-8,Fri Jun 26 02:52:15 2015 UTC,Meh.
hardware,3azss3,Darius510,-7,Fri Jun 26 15:35:49 2015 UTC,"What's the fury's strength? Because I'm not really seeing it in the benchmarks. I'm seeing a slightly slower card with 50% less VRAM, higher power consumption and the hassle of water cooling."
hardware,3azss3,atriax,24,Thu Jun 25 04:31:01 2015 UTC,TIL 4 is half of 6
hardware,3azss3,MrPoletski,4,Thu Jun 25 00:19:46 2015 UTC,TIL 4 is half of 6   HL3 confirmed?
hardware,3azss3,WhenisHL3,4,Thu Jun 25 02:17:19 2015 UTC,"By mentioning Half-Life 3 you have delayed it by 1 Month. Half-Life 3 is now estimated for release in March 2545    I am a bot, this action was performed automatically. If you have feedback please message /u/APIUM- or for more info go to /r/WhenIsHL3"
hardware,3azss3,PeterIanStaker,17,Thu Jun 25 11:18:39 2015 UTC,atriax was right. Saying 6 is a 50% increase over 4 is not the same thing as saying 4 is 50% less than 6.
hardware,3azss3,reynardtfox,2,Thu Jun 25 11:21:33 2015 UTC,a 50% increase is not the same as a 50% less. a 50% increase in reality is equal to 33% less.  Here's a proof. 2*1.5=2+.5(2)=3 So 50% more than 2 is 3.  What is 50% of 3?  1.5.  Is that equal to 2?    So if you want to make 3 equal to 2 you have to subtract 1.  1/3 = 33% =/= 50%.  QED BITCH.  this math lesson brought to you by a summer teaching assistant who has nothing better to do with his life.
hardware,3azss3,foxtrot1_1,2,Thu Jun 25 02:55:56 2015 UTC,"Water cooling isn't a hassle when it's AIO but yeah, the FuryX is definitely not a clear upgrade over the 980ti (especially not when you factor in OC)."
hardware,3azss3,GeckIRE,26,Thu Jun 25 03:08:21 2015 UTC,"I think direct x 12 could be a huge boost for the fury x.   http://videocardz.com/56728/amd-radeon-r9-fury-x-reviewers-guide  Theirs some tests here showing how well the Fury was doing over the titan.   Edit: point your attention to the ""api overhead feature test"" part with the draw calls."
hardware,3azss3,regenshire,13,Thu Jun 25 13:41:46 2015 UTC,You bring up a good point about DirectX 12.  A lot of things do point at AMD possibly having an advantage with DirectX 12 when it releases.  I wonder how long until we start seeing games that will actually utilize DirectX 12.
hardware,3azss3,Hamakua,41,Thu Jun 25 13:59:41 2015 UTC,unfortunately for the Fury X probably well past Pascals release date.
hardware,3azss3,Yearlaren,6,Wed Jun 24 21:34:43 2015 UTC,I wonder how long until we start seeing games that will actually utilize DirectX 12.   Probably after AMD releases the Fury X's successor.
hardware,3azss3,brazilistic,2,Wed Jun 24 21:45:58 2015 UTC,"But hey, that would still mean cheap Fury X's that perform very admirably and better than Nvidia's old lineup. Budget gamers everywhere rejoice!"
hardware,3azss3,jaerdoster,4,Wed Jun 24 21:50:45 2015 UTC,"Well, if PS4 and Xbone are DX12 able, quickly. But if not, we will wait a loooooong time."
hardware,3azss3,SOD03,4,Wed Jun 24 22:47:58 2015 UTC,"Since Xbox is going to get Windows 10 in the future, it will get DX12. The PS4 will not use Windows 10, so no DX12."
hardware,3azss3,jaerdoster,-3,Fri Jul 3 13:36:48 2015 UTC,"And so, no DX12 for AAA multiplatform titles, since they will use DX9/11 for easier port."
hardware,3azss3,computertechie,5,Thu Jun 25 06:48:19 2015 UTC,The PS4 doesn't use DirectX in any capacity as it does not run Windows; Sony has a OpenGL-esque custom language for it.
hardware,3azss3,foxtrot1_1,4,Thu Jun 25 10:00:07 2015 UTC,"No, that's not how porting or game dev works."
hardware,3azss3,cheekynakedoompaloom,1 point,Thu Jun 25 12:09:32 2015 UTC,the driver architecture stuff should be visible immediately even in dx11. seeing dx12 will be nice but amd should get a nice performance bump just by people using windows 10.
hardware,3azss3,w00t692,7,Thu Jun 25 13:38:07 2015 UTC,This is the feature test in 3dmark they reference.  I have a 980 gtx and a 4690k @ 4.5 ghz  here  I don't know what this means... tbh.
hardware,3azss3,ElDubardo,3,Thu Jun 25 13:58:01 2015 UTC,Rendering at 720p...
hardware,3azss3,w00t692,2,Wed Jun 24 21:54:43 2015 UTC,"That's the default setting... I don't see where it says they tested it differently.  Lemme try 1080p, since that's the highest i can go.  edit: still got 14,716,738 so whatever i dunno."
hardware,3azss3,ElDubardo,1 point,Wed Jun 24 23:24:42 2015 UTC,"That is pretty nice indeed! Mine went at 10mils in 720p (As you said, it's default!)"
hardware,3azss3,WillWorkForLTC,1 point,Thu Jun 25 00:39:45 2015 UTC,Yeah it's been noted already the Fury X does not shine to it's full potential at 1080p or 1440p but rather at 4K. So yeah. What you said.
hardware,3azss3,whereismyfix,1 point,Thu Jun 25 00:46:58 2015 UTC,"And yet it has only 4GB VRAM and no HDMI 2.0, so it's not compatible with a lot of UHD TV's out of the box."
hardware,3azss3,Jack_BE,2,Thu Jun 25 01:43:19 2015 UTC,"On top of this, the thing I am missing most in these reviews is how Mantle performs on the Fury X, as Mantle is about the closest thing you currently have to compare to DX12 performance."
hardware,3azss3,w00t692,1 point,Thu Jun 25 04:48:00 2015 UTC,"I got 16,093,920 draw calls per second when i did the test on my system with a 980 gtx and 4690k @ 4.5ghz"
hardware,3azss3,ClockworkOnion,1 point,Thu Jun 25 17:30:16 2015 UTC,"Nvidia cards will get the same boost, still leaving the FuryX behind."
hardware,3azss3,andromeduck,4,Thu Jun 25 05:35:21 2015 UTC,current nvidia gpus have much worse support for dx12/vulkan than current (gcn and newer) amd gpus.
hardware,3azss3,oxYnub,-1,Wed Jun 24 22:12:18 2015 UTC,Nvidia was the first to have WHQL for Windows 10 a few weeks ago. Has AMD posted yet?
hardware,3azss3,andromeduck,9,Wed Jun 24 23:25:36 2015 UTC,"WHQL   As if this means anything. In the past nvidia had released ""whql"" driver that disabled the fan on GPUs making them overheat."
hardware,3azss3,ClockworkOnion,-1,Wed Jun 24 22:27:52 2015 UTC,WHQL is bigger deal in W10.  It's not an exhaustive test suite because that's impossible but it is another layer of assurance.
hardware,3azss3,GeckIRE,-1,Wed Jun 24 23:41:47 2015 UTC,Not really they're about equal with features that developers will actually use.
hardware,3azss3,makar1,-5,Thu Jun 25 03:06:32 2015 UTC,"Maybe when their using hbm 2 yeah. Did you read it? While I'm sure they will get a boost, they did the comparison with a titan showing the fury far ahead."
hardware,3azss3,Chunke,4,Thu Jun 25 08:23:30 2015 UTC,An increase in draw calls does not imply an increase in gaming performance.
hardware,3azss3,communistsquared,0,Thu Jun 25 15:32:31 2015 UTC,So much misinformation on this subreddit. What the fuck does memory bandwidth have to do with higher draw call rate??
hardware,3azss3,Reapexx,1 point,Thu Jun 25 00:29:13 2015 UTC,can handle more draw calls =/= better FPS in all games
hardware,3azss3,regenshire,12,Wed Jun 24 22:31:37 2015 UTC,"I have a gut feeling that the Fury X is missing out on a lot of its potential performance due to less-than-optimal drivers.  I don't have any proof to back that up, though."
hardware,3azss3,glr123,5,Wed Jun 24 23:22:08 2015 UTC,"Can they improve performance with driver updates? Yes, of course they can.  Will it be a huge difference.  In my opinion, it is doubtful that further driver optimizations would greatly improve the card, and I believe that is wishful thinking.  Sure, there probably will be improvements of performance for certain games as they release new drivers, but I wouldn't bet on anything game changing.  The biggest question is if they can OC better then current indications on the card."
hardware,3azss3,Seclorum,16,Fri Jun 26 08:43:10 2015 UTC,"I don't know that I agree. Look at day 1 290X performance versus present day. Massive improvements. Can AMD pull off similar magic here? Well, time will tell. Fine tuning HBM utilization, DX12, etc. could make a difference and we will just have to wait and see."
hardware,3azss3,spencer32320,3,Thu Jun 25 16:08:55 2015 UTC,The biggest question is if they can OC better then current indications on the card.   We really need updated tools that can play around with the Fury X's Voltage. Currently nobody can seem to get the voltage to change at all.
hardware,3azss3,namae_nanka,3,Wed Jun 24 23:49:46 2015 UTC,That's cause it's locked by AMD. They should be unlocking the voltage soon.
hardware,3azss3,Darius510,2,Wed Jun 24 20:53:20 2015 UTC,Fury X matches or beats the 980Ti at 4k so driver updates can improve that. At lower resolutions drivers could surely help since they'd get the cpu usage down.  It'll surely help with those frame time spikes that techreport saw in their review.  Fury doesn't really have a new architecture but it's very different in its strengths vs. 290X so it's possible that drivers might push it further.
hardware,3azss3,Seclorum,7,Wed Jun 24 21:33:42 2015 UTC,Sure they can. But so can NVIDIA. So I don't expect it to ever beat a 980 Ti.
hardware,3azss3,Cozmo85,15,Wed Jun 24 22:12:42 2015 UTC,"That's supposing Nvidia intentionally gimped both the 980ti and Titan-X driver profiles, just so they could roll out an update in the future should AMD come out with comparative hardware.   That does not seem likely."
hardware,3azss3,Seclorum,8,Wed Jun 24 22:40:18 2015 UTC,That's also supposing amd has had months to get working drivers and simply didn't bother
hardware,3azss3,oxYnub,5,Thu Jun 25 02:24:34 2015 UTC,"You would think they would always release a product with fully optimized drivers, but it's almost as if AMD has a single driver guy who has to juggle everything.  It's been like 9 months since they released a WHQL driver, IE a non beta driver."
hardware,3azss3,Seclorum,8,Wed Jun 24 21:50:44 2015 UTC,"WHQL   Why do you want whql, non beta, drivers. Last 2-3 nvidia whql drivers crash constantly, it means absolutely anything other than $$$ for microsoft."
hardware,3azss3,atriax,1 point,Wed Jun 24 22:10:51 2015 UTC,"Then it sounds like we need a new 'stable' specification for drivers. Because based on comments it seems like WHQL doesn't mean stable, it mean utter piece of shit software that never works."
hardware,3azss3,MrPoletski,3,Thu Jun 25 00:09:52 2015 UTC,"How many times does it have to be said that WQHL doesn't mean shit? It's literally a stamp of ""lol microsoft got free money from us."" And taht's it. There were certified drivers that Nvidia released that turned the fans off on their cards and then burned them up and destroyed them."
hardware,3azss3,Seclorum,1 point,Thu Jun 25 00:34:36 2015 UTC,That it doesn't kill windows is probably all that M$ care about when WHQL'ing a driver.
hardware,3azss3,Killmeplsok,0,Thu Jun 25 08:25:38 2015 UTC,"At the same token, if it's shit, then why have them at all? Why do they continue to release nothing but 'Beta' drivers. Implying that they have known problems and never truly ever get them 'solved.'"
hardware,3azss3,stormgeneral,6,Thu Jun 25 15:32:11 2015 UTC,"Because releasing a WHQL driver costs money, and its not small sum by the way, that's a lot of money going to Microsoft for almost nothing in return.  Beta drivers on the other hand can be released anytime without additional costs, and these driver, even if they say ""beta"" is not beta, they should be as stable as those WHQL drivers as it went through the same process except paying Microsoft.  Why have them at all? Some companies needs anything installed on their hardware to be certified for security reasons,  most consumer should not be bothered about the WHQL certifications."
hardware,3azss3,oxYnub,1 point,Thu Jun 25 02:19:03 2015 UTC,"Because releasing a WHQL driver costs money, and its not small sum by the way, that's a lot of money going to Microsoft for almost nothing in return.   You mean a few hundred dollars for a company like AMD - https://en.wikipedia.org/wiki/WHQL_Testing"
hardware,3azss3,Darius510,1 point,Thu Jun 25 11:14:20 2015 UTC,"It takes time and time is money, especially for AMD who usually releases later than nvidia."
hardware,3azss3,Zlojeb,2,Thu Jun 25 02:27:49 2015 UTC,"That's also supposing the fury x drivers aren't as good as they'll ever be.   Back when AMD released mantle, NVIDIA released a major driver upgrade that improved dx11 performance across the board. All they need is a fire lit under their ass. If the fury ever did surpass the 980 Ti through drivers, NVIDIA would do whatever it took to regain the lead."
hardware,3azss3,Darius510,2,Thu Jun 25 03:12:05 2015 UTC,"I heard there even aren't any specific drivers for Fury X yet. Current are for 390X(and working really nice for 390X it matches 980, but that's not the point)."
hardware,3azss3,Zlojeb,1 point,Thu Jun 25 04:07:05 2015 UTC,Where'd you hear that?
hardware,3azss3,Seclorum,7,Thu Jun 25 08:26:45 2015 UTC,"Honestly can't remember. I read like 12 reviews today. I think it was in one of these.    Guru3D http://www.guru3d.com/articles-pages...-review,1.html TechPowerUp http://www.techpowerup.com/reviews/AMD/R9_Fury_X/ PC Perspective http://www.pcper.com/reviews/Graphic...Finally-Tested Anandtech  http://www.anandtech.com/bench/product/1496?vs=1513 tom's hardware http://www.tomshardware.com/reviews/...ry-x,4196.html HardOCP http://www.hardocp.com/article/2015/...1#.VYrqpvlVhBd Linus Tech Tips https://www.youtube.com/watch?v=-CDFNOTZy8o&feature=youtu.be Bit-tech http://www.bit-tech.net/hardware/gra...ury-x-review/1 Hexus http://hexus.net/tech/reviews/graphi...x-4gb/?page=15 PC Gamer http://www.pcgamer.com/amd-radeon-r9...980-ti-killer/ IGN http://ca.ign.com/articles/2015/06/2...-fury-x-review Overclock3D http://www.overclock3d.net/reviews/g...ury_x_review/1"
hardware,3azss3,JustaPassanger,2,Wed Jun 24 22:39:10 2015 UTC,"AMD is well known for putting out drivers in advance for compatibility with new hardware, but it takes awhile before they fully optimize for it."
hardware,3azss3,Darius510,1 point,Thu Jun 25 00:09:01 2015 UTC,Seems likely but my question would be is it magic that the Fury-x ended up costing as much as a 980ti and is pretty much on par performance wise (if they optimize the drivers a bit more).
hardware,3azss3,JustaPassanger,1 point,Thu Jun 25 00:13:03 2015 UTC,"No, it wasn't magic, it was a mistake to release it at the same price. It's not a competitive card. Give me one good reason to buy it over a 980 Ti other than a philosophical stance against NVIDIA or hope that one day it'll get better through drivers."
hardware,3azss3,Teethpasta,1 point,Thu Jun 25 00:17:57 2015 UTC,It has a ~120$ AIO cooler and is extremely silent under load.
hardware,3azss3,glr123,1 point,Thu Jun 25 00:07:38 2015 UTC,Considering nvidia cards get worse over time that's not likely. See 770 and 780.
hardware,3azss3,JakSh1t,12,Thu Jun 25 06:15:15 2015 UTC,"And Nvidia is great performance but always deceptive marketing (970, arkham knight) and buggy drivers every other week. Both have their issues.  A rock and a hard place."
hardware,3azss3,Lanessar,8,Thu Jun 25 12:59:01 2015 UTC,I'm going back to Matrox!
hardware,3azss3,ICanHazTehCookie,3,Fri Jun 26 21:21:20 2015 UTC,I'm going back to my Voodoo  5 5000. Get off my lawn!
hardware,3azss3,ICanHazTehCookie,2,Fri Jun 26 01:03:48 2015 UTC,"The problem is not possible performance issues that arose from the vram, but the fact that nvidia intentionally lied to millions of us and didn't even apologize afterwards. They basically said we should be happy that we got 3.5gb of well-performing vram instead of 3gb, even though they marketed it as 4gb. And even the 3.5gb section is not as fast as their specs claim."
hardware,3azss3,atriax,2,Wed Jun 24 22:15:15 2015 UTC,"For pure performance I agree, but some people care about the kind of business that they are supporting as well, even if you don't."
hardware,3azss3,glr123,2,Wed Jun 24 22:31:03 2015 UTC,I don't care if they fuck me in the ass as long as they use good lube!
hardware,3azss3,Stoffendous,1 point,Wed Jun 24 23:38:44 2015 UTC,"That's fine, but I don't appreciate being lied to as a consumer. If you care about the benchmarks, good on ya, but I decided to vote with my wallet."
hardware,3azss3,Thunder_Bastard,1 point,Thu Jun 25 03:12:18 2015 UTC,"Couldn't agree with you more. I tried buying an ati/amd card before thinking it would get better in time cause of drivers and a new directx and it never did. Edit: that was the x1800xl.  This time I'm looking at benches on release date, and if this is what we're getting my money is going to Nvidia."
hardware,3azss3,Darius510,1 point,Wed Jun 24 23:44:32 2015 UTC,But you must be wrong.  According to the AMD fanboys it is a GOOD thing AMD only comes out with two beta drivers in 8 months.
hardware,3azss3,ICanHazTehCookie,1 point,Thu Jun 25 01:13:20 2015 UTC,"Still waiting on that driver that's supposed to make freesync as good as g-sync. And the driver that's supposed to fix the CPU overhead issue. The magical ""make a fury faster than a 980 Ti"" driver is going to have to get in line."
hardware,3azss3,Darius510,2,Thu Jun 25 01:31:07 2015 UTC,"Freesync has been improving quite a bit, actually. Ranges are getting bigger, and it works with overdrive now as well."
hardware,3azss3,ICanHazTehCookie,1 point,Thu Jun 25 01:34:27 2015 UTC,"Is that because the new monitors are better, or the drivers have improved things on early models?"
hardware,3azss3,Darius510,1 point,Thu Jun 25 02:20:14 2015 UTC,"Nothing has improved on earlier models afaik. I do know that the mg279q's freesync range of 35-90hz is a limitation of the panel. And the xl2730z has a range of 40-144hz, so freesync is definitely capable of that. iirc freesync can theoretically go from 9-240hz. However the drivers do still need improvements, for example crossfire still doesn't work with freesync."
hardware,3azss3,ICanHazTehCookie,0,Thu Jun 25 00:44:36 2015 UTC,"Well so much for the magical driver fix then. I mean that was the refrain from everyone when freesync first came out. ""You'll see, drivers will fix it!""   Still waiting."
hardware,3azss3,atriax,1 point,Wed Jun 24 23:18:39 2015 UTC,"The point is that freesync as a whole is improving, which a driver update will likely accompany at sometime."
hardware,3azss3,Lanessar,2,Thu Jun 25 00:55:50 2015 UTC,"I'm still waiting on Nvidia to stop fucking people with proprietary bullshit like gsync. Oh yeah, and the fact that the last three games released with an Nvidia logo on it required massive fixes (project cars, witcher 3 gimped the shit out of old Nvidia cards, and not batman.) Great company tho. 8/8."
hardware,3azss3,HaMMeReD,1 point,Thu Jun 25 00:25:26 2015 UTC,"... you do realize the gimping on TW3 lasted all of 3 weeks before it was fixed, right?"
hardware,3azss3,a_monkie,-1,Thu Jun 25 01:14:14 2015 UTC,The only reason people make open formats is because they don't want to pay the guy who invented the proprietary one that came before.
hardware,3azss3,Ofactorial,1 point,Thu Jun 25 02:04:53 2015 UTC,is there any benchmarks out there that compare crossfire fury X's to SLI 980TI's at 4K?
hardware,3b12qr,RTomassi,5,Thu Jun 25 02:54:48 2015 UTC,"Quote ""When GTA V was pushed to consume more memory than the Fury X has onboard, the experience became extremely stuttery, choppy, and graphically glitchy as the card offloaded duties to system memory, which is far slower than HBM."" That's not part of being a ""4k powerhouse"".."
hardware,3b12qr,kardkoach,2,Thu Jun 25 17:22:14 2015 UTC,"Good, honest-sounding review with few punches pulled. I held off on buying a card and got excited when AMD promised the fastest single-GPU card available. Add to that the promising gaming benchmarks they released, and I was all set to purchase. Seems the claims just don't hold up under scrutiny.  Just ordered the other card featured in this review: EVGA's ACX-chilled GTX 980 Ti SC+. Seems to beat everything available except AMD's older dual-GPU card."
hardware,3b12qr,FlobeeWanKenobee,4,Thu Jun 25 19:22:02 2015 UTC,"Spoiler alert, as the author of the PCWorld review: You won't be disappointed with that EVGA card. I have a separate review of it going up on the site tomorrow--though it seems you already know how it's going to go reading the Fury X review :-)  Glad you enjoyed that review, by the way."
hardware,3b12qr,StickiestCouch,2,Thu Jun 25 21:23:51 2015 UTC,I went straight from your review to Newegg. Eager to read your work tomorrow.
hardware,3b12qr,FlobeeWanKenobee,1 point,Thu Jun 25 21:37:06 2015 UTC,AMD's older dual-GPU card.   Well that's not fair. If you wanted 2 GPUs you may as well have bought 2 980s.
hardware,3b12qr,continous,1 point,Tue Jun 30 04:49:41 2015 UTC,"No, I only have room for one card... Mini-ITX. So I chose the strongest card that could fit, and was crossfire/SLI-free."
hardware,3b12qr,FlobeeWanKenobee,2,Tue Jun 30 05:07:52 2015 UTC,Ah. If that's the case I can understand. I just really hate when people compare dual-GPU cards to single-GPU cards as if it were apples to apples.
hardware,3b12qr,continous,1 point,Tue Jun 30 05:27:24 2015 UTC,"The funny thing is, in some of those benchmarks, the 980 Ti beats the dual-GPU card! Pretty cool."
hardware,3b12qr,FlobeeWanKenobee,1 point,Tue Jun 30 05:38:55 2015 UTC,"Yup. The 980Ti is a real beast. Been considering selling my 980 and stepping up, but it really isn't that worth it."
hardware,3b12qr,continous,1 point,Tue Jun 30 05:44:52 2015 UTC,"980 is plenty strong. I was coming from a 750 Ti though, definitively wanted a big upgrade."
hardware,3b12qr,FlobeeWanKenobee,1 point,Tue Jun 30 05:48:26 2015 UTC,"Ah. Yeah, atm I'm debating either going for a dual 980 setup or single 980Ti setup. Will probably be more future-proof to go for a 980Ti."
hardware,3b12qr,continous,1 point,Tue Jun 30 05:50:31 2015 UTC,"With the power you have, you could be fine waiting until Nvidia gets HBM next year. I would hold off. I guess it depends on if you're gaming at 4k or not at the moment."
hardware,3b12qr,FlobeeWanKenobee,2,Tue Jun 30 06:02:51 2015 UTC,"Yeah, I probably will end up waiting. When HBM comes out I'll probably wait for an 8 or more gig card as well."
hardware,3axyzt,Vince789,85,Wed Jun 24 12:00:45 2015 UTC,"If you read through all the threads, you see 2 main topics - people defending the Fury's performance and people making excuses for it (drivers, lack of voltage control etc). While most of the points I've seen are valid, neither of these are what a company want the talking points of their top tier card to be on launch day."
hardware,3axyzt,thejshep,20,Wed Jun 24 15:27:33 2015 UTC,I think AMD has been beat by the hype as much as big Maxwell.
hardware,3axyzt,thejshep,6,Wed Jun 24 16:12:44 2015 UTC,I think they mishandled this release.    The card is decent though.  Might look better after some driver updates...
hardware,3axyzt,PhilipK_Dick,5,Wed Jun 24 17:16:00 2015 UTC,The promises were what really let me down. They were talking about it being efficient and using less power while also being the fastest card when it's the 2nd-3rd fastest and still uses more power than Maxwell.
hardware,3axyzt,EnsoZero,5,Wed Jun 24 19:23:21 2015 UTC,Yea I think its because they really didn't know about what the 980ti was capable of until it was too late. Now its just down to overclocking (and who knows maybe the drivers will have a hand to play in its performance). Their drivers/software might not be properly utilizing the hardware.Muscles are useless when you don't know how to use them.
hardware,3axyzt,steveng95,3,Wed Jun 24 22:19:16 2015 UTC,"Either way it's pretty clear that buying either the Fury or 980 Ti or any high end video card at this point is a waste, with 16nm and HMB2 coming next year we'll see much bigger advances over the current generation of cards."
hardware,3axyzt,EnsoZero,3,Wed Jun 24 22:35:38 2015 UTC,Yep. Like always you are gonna have people who wait and keep what they have or go with the current gen gpus (and maybe sell them when the new ones come out)
hardware,3axyzt,steveng95,1 point,Wed Jun 24 22:39:59 2015 UTC,Selling old cards has worked pretty well for me.  Good way to defray things a bit.
hardware,3axyzt,Jordanis,5,Wed Jun 24 22:48:27 2015 UTC,and people making excuses for it (drivers ...  might look better after some driver updates   Dude.
hardware,3axyzt,yjgfikl,1 point,Wed Jun 24 22:54:11 2015 UTC,"Hey, I'm going to buy a 980ti.  Doesn't mean the FuryX isn't a good card.  That launch was handled about as poorly as possible and the company will be bought out sooner than later - but the card isn't a bad piece of hardware."
hardware,3axyzt,PhilipK_Dick,1 point,Thu Jun 25 00:12:40 2015 UTC,Just not for $649
hardware,3axyzt,bigluvin69,1 point,Thu Jun 25 02:52:59 2015 UTC,"I think maybe they understimated how good big maxwell would be and had a target to beat that turned out to be too low.   I have to give nvidia credit- maxwell seems to be a better architecture than even the latest version of GCN. Not by a huge margin but a small margin is enough. Fiji is a large improvement over any of AMD's previous cards but the GPU architecture itself is at best, on par with maxwell.   On the other hand, HBM, the compactness of the PCB, and the willingness to design a flagship card with compactness and water cooling shows a lot more innovation from AMD. Both AMD and nvidia were kind of fucked in the ass by the utter failure of the 20nm TSMC process node which I think it was clear that AMD was relying on. It seems that nVidia realized sooner that they would have to keep putting out cards on 28nm, and planned accordingly, before AMD did.   I'm not blown away by the fury X but I think AMD is going in the right direction in more ways than nVidia. Big Maxwell is an excellent GPU architecture that is pushing GDDR5 to its limits and has nowhere to go. AMD needs to focus on beating nVidia's efficiency in their architecture (as they did with the release of the 7000 series) because that will still be important at 20nm or 14nm or whatever the hell next process node actually gets some GPUs made on it. The sheer efficiency of Maxwell shows that there is still vast amounts of room for improvement in architectural efficiency in GCN."
hardware,3axyzt,slapdashbr,1 point,Thu Jun 25 06:06:26 2015 UTC,"The thing that works me is how slow AMD is at releasing new products. This time next year we'll be talking about Nvidia's new Pascal based GPU (14mn, HBM2 etc) while AMD will probably be at least a year off from their next new card. They bed ed Fury to be leaps and bound ahead ti hold them over during that prices but sadly it didn't work out that way."
hardware,3axyzt,thejshep,35,Thu Jun 25 18:22:52 2015 UTC,I think Tom Logan was right - the Fury X beats the 980 hands down which is probably what AMD set out to do. It's just that pesky Nvidia bringing out the 980 Ti threw a spanner in the works.
hardware,3axyzt,phigo50,15,Thu Jun 25 18:31:32 2015 UTC,the fury (without x) has to do that.
hardware,3axyzt,Klorel,12,Wed Jun 24 16:23:39 2015 UTC,I still think that the Nano will be the most important card for AMD this Generation
hardware,3axyzt,baltuin,3,Wed Jun 24 16:41:36 2015 UTC,Why do you say that? Only true top end card in SFF?
hardware,3axyzt,bigwilly90210,2,Wed Jun 24 17:44:06 2015 UTC,Size. So much power in such a small package
hardware,3axyzt,ThatMattyIce,2,Wed Jun 24 22:52:25 2015 UTC,"Yeah, it's a slight disappointment but at the very least it hasn't become a self-destructive sequence yet.  Maybe the Fury X will overclock well.  Who knows.  It's not a bad card by any means, but it's definitely not as good as what it was hyped up to be.  Not yet at any rate."
hardware,3axyzt,reynardtfox,1 point,Thu Jun 25 02:50:56 2015 UTC,Doesn't look that way. The review I read mentioned that even with +50% PowerTarget the card didn't go beyond 1144Mhz and produced graphic errors when set to 1200Mhz.
hardware,3axyzt,t-master,1 point,Wed Jun 24 16:13:58 2015 UTC,I thought all the Fury X's that were released were stock cards that couldn't be changed by the manufacturers?  Do you have link to that review o_o?
hardware,3axyzt,reynardtfox,1 point,Wed Jun 24 21:46:16 2015 UTC,"This isn't the review I meant (and it's unfortunately in German), however the results are similar. They've only reached 1115 and 1130mhz with their cards before the errors began, which is a meager +65/80mhz. http://www.computerbase.de/2015-06/amd-radeon-r9-fury-x-test/10/#abschnitt_uebertaktbarkeit"
hardware,3axyzt,t-master,1 point,Wed Jun 24 23:49:49 2015 UTC,=/ darn.  That's with corespeed overclocks right?  Maybe with voltage changes it'll be different.  Who knows.  I'll definitely wait before picking between the 980ti and the Fury X.  That or maybe I'll just wait till HBM gen 2 becomes a thing.  TY for sharing btw.
hardware,3axyzt,reynardtfox,1 point,Thu Jun 25 00:59:42 2015 UTC,"Quite a few 980 ti s can overclock 500mhz on stock voltages, so it doesn't look promising for amd"
hardware,3axyzt,diggs747,1 point,Thu Jun 25 01:08:39 2015 UTC,This is true.
hardware,3axyzt,reynardtfox,1 point,Thu Jun 25 01:24:45 2015 UTC,It's an equalizer to the 980 Ti and so much so that I wonder if this is deliberate.
hardware,3axyzt,godsayshi,3,Thu Jun 25 01:59:58 2015 UTC,"this is the kind of card i'd wait for black friday/boxing day sales.  by then, if drivers need fixing, they should have been fixed, and 3rd party manufacturers will have their own variants, and the price in general should be cheaper anyway.  i'm just glad this card wasn't an october release or something like that, with no good longer-term reviews when the big sales hit."
hardware,3axyzt,willyolio,2,Thu Jun 25 21:34:44 2015 UTC,"No third party versions for the fury x,  it is amd only like the Titan x is nvidia only."
hardware,3axyzt,TheOneWatcher,3,Wed Jun 24 18:10:03 2015 UTC,But the fury is basically going to be the same card without watercooling anyway right?
hardware,3axyzt,willyolio,2,Wed Jun 24 19:24:36 2015 UTC,Not entirely sure yet.  Their are rumors of it being slightly cut down. Which would make sense as they need some form of binning the chip.
hardware,3axyzt,spencer32320,1 point,Wed Jun 24 19:44:03 2015 UTC,"Short answer: We dont know.  Long answer: Probably will be cut down like the 980ti was cut down from a titan.   I'd expect some SMU's disabled but that's about it really. It's entirely possible it's exactly the same, just on air, but it's debatable. Too many rumors to be sure of anything with the Straight Fury."
hardware,3axyzt,Seclorum,1 point,Wed Jun 24 20:23:12 2015 UTC,People take their video cards and brand loyalty pretty f'n seriously.
hardware,3axyzt,HaMMeReD,1 point,Thu Jun 25 04:48:03 2015 UTC,"It is weird for all their talking up about, ""An OC monster!"" yet without proper voltage controls we cant put that to the test.   And given the performance delta between 1080p and 4k it's almost like the card really really doesn't want to fire off at a fast framerate at all.   Given a few months for proper drivers it will probably settle down but I'm not sure many early adopters will care to wait that long to get a card with a reputation of 'underperforming.'   Such a shame, it has so much promise but to have some key things not work right is inexcusable given the time it's already taken to get to market."
hardware,3axyzt,Seclorum,1 point,Thu Jun 25 02:05:34 2015 UTC,I think they can tweak it at best but it'll never be the monster we were led to believe it was.
hardware,3axyzt,thejshep,33,Thu Jun 25 04:45:39 2015 UTC,"PM me if you want me to add any other reviews (but I'm going to sleep now, I'll update when I wake up)  Also note the overclocks were at stock voltages, so we still have to wait to see its full potential  More:  anandtech (not a full review) http://www.anandtech.com/bench/product/1496?vs=1513  bit-tech http://www.bit-tech.net/hardware/graphics/2015/06/24/amd-radeon-r9-fury-x-review/1  digitaltrends http://www.digitaltrends.com/video-card-reviews/amd-radeon-r9-fury-x-review/  guru3d http://www.guru3d.com/articles-pages/amd-radeon-r9-fury-x-review,1.html  hexus http://hexus.net/tech/reviews/graphics/84170-amd-radeon-r9-fury-x-4gb/  hardocp http://www.hardocp.com/article/2015/06/24/amd_radeon_r9_fury_x_video_card_review  hardwarecanucks http://www.hardwarecanucks.com/forum/hardware-canucks-reviews/69682-amd-r9-fury-x-review-fiji-arrives.html  legitreviews http://www.legitreviews.com/amd-radeon-r9-fury-x-video-card-review_167134  LinusTechTips (no benchmarks) https://www.youtube.com/watch?v=-CDFNOTZy8o  maximumpc http://www.maximumpc.com/amd-radeon-fury-x-review/  overclock3d http://www.overclock3d.net/reviews/gpu_displays/amd_r9_fury_x_review/22  pcgamer http://www.pcgamer.com/amd-radeon-r9-fury-x-tested-not-quite-a-980-ti-killer/  pcper http://www.pcper.com/reviews/Graphics-Cards/AMD-Radeon-R9-Fury-X-4GB-Review-Fiji-Finally-Tested  pcworld http://www.pcworld.com/article/2939712/amd-radeon-r9-fury-x-graphics-card-review-amds-thoughtful-4k-powerhouse.html  sweclockers http://www.sweclockers.com/test/20730-amd-radeon-r9-fury-x  techpowerup https://www.techpowerup.com/reviews/AMD/R9_Fury_X/  techreport https://techreport.com/review/28513/amd-radeon-r9-fury-x-graphics-card-reviewed  tomshardware http://www.tomshardware.com/reviews/amd-radeon-r9-fury-x,4196.html"
hardware,3axyzt,Spreadsheeticus,2,Thu Jun 25 10:16:00 2015 UTC,"I'd watched the LinusTechTips earlier, which appeared to have average FPS and wattage comparison.    Are these something other than benchmarks?"
hardware,3axyzt,LogicalJake,6,Wed Jun 24 12:02:24 2015 UTC,They were using benchmark numbers from techreport.com due to their Fury X being DOA.
hardware,3axyzt,Scrabo,13,Wed Jun 24 20:13:04 2015 UTC,"Seems AMD got themselves back into the position they were in with the GTX 780ti where they were a bit behind but much more competitive at 4K with a negligible difference in power consumption. Although they don't have a terrible stock cooler this time so at least it's quieter, cooler and smaller.  I kinda thought it would do a bit better than 30-40% over a 290X given the raw hardware advantage. I wonder if software is holding them back. Hard to tell how this is going to turn out for AMD. 4K is still a small market and they don't have the big price-performance advantage in most places. The Fury and the Nano might make more sense."
hardware,3axyzt,bphase,4,Wed Jun 24 21:14:46 2015 UTC,"I kinda thought it would do a bit better than 30-40% over a 290X given the raw hardware advantage.   Yeah, but then again for example pixel fillrate and polygon throughput aren't improved at all. They didn't improve everything from the 290X, so maybe they're hitting those limits... hard to say though."
hardware,3axyzt,TehRoot,3,Wed Jun 24 14:11:48 2015 UTC,It's because they didn't increase ROP count.
hardware,3axyzt,Kinaestheticsz,2,Wed Jun 24 14:57:28 2015 UTC,It amazes me how this shortsight could pass through such a large corporation as AMD. This card would almost assuredly fly if it weren't so darn ROP limited.
hardware,3axyzt,TehRoot,3,Wed Jun 24 16:15:40 2015 UTC,There was most likely a technical limitation. Early leaks showed 128 ROPs but that might have been at an earlier point in development.
hardware,3axyzt,Drudicta,1 point,Wed Jun 24 18:21:21 2015 UTC,"ROP count   This is a new term for me as I'm kinda in and out hardware wise, what's ""ROP"" stand for?"
hardware,3axyzt,TehRoot,1 point,Wed Jun 24 18:22:26 2015 UTC,https://en.wikipedia.org/wiki/Render_output_unit
hardware,3axyzt,autowikibot,2,Wed Jun 24 22:45:09 2015 UTC,"Render output unit:       The render output unit, often abbreviated as ""ROP"", and sometimes called (perhaps more properly) raster operations pipeline, is one of the final steps in the rendering process of modern 3D accelerator boards. The pixel pipelines take pixel and texel information and process it, via specific matrix and vector operations, into a final pixel or depth value. The ROPs perform the transactions between the relevant buffers in the local memory – this includes writing or reading values, as well as blending them together.     Relevant: Vector graphics | Texture mapping unit   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3axyzt,Drudicta,1 point,Wed Jun 24 22:46:57 2015 UTC,Thanks~
hardware,3axyzt,Bacontroph,1 point,Wed Jun 24 22:47:23 2015 UTC,I do wonder if they haven't figured out how to properly utilize the capabilities of HBM with it being a new memory architecture and all that.
hardware,3axyzt,slapdashbr,1 point,Thu Jun 25 00:34:19 2015 UTC,"I think it's partly software but mostly architecture that's holding them back.   Maxwell was amazing. Nvidia made an enormous leap in efficiency at the same process node that I don't think I've ever seen in any product development chain in the history of computing. AMD honestly has only just now caught up, almost, not even completely, with Fiji on GCN 1.2 or whatever it is. I think nVidia realized/planned to keep making GPUs on 28nm sooner than AMD and it hurt AMD not to adjust to the failure of TSMC to get 20nm graphics off the ground. However, I think AMD is better positioned to take advantage of the next process node. They clearly have more room for improvement in architecture efficiency and they now have a lead in designing cards with HBM, which requires some new manufacturing techniques that nVidia will have to learn (or pay for)."
hardware,3axyzt,OftenSarcastic,17,Wed Jun 24 21:17:49 2015 UTC,"Average of 31 games from HardOCP, Anandtech, TechPowerUp, Guru3D, Tomshardware and PcPer.   UHD/4K/2160p  Card        Relative FPS    Card Watt   System Watt     Load Temp   Load Noise R9 Fury X    98.72%         265         390             60          37 GTX 980 Ti  100.00%         240         375             84          43   The card and system wattage averages come from separate reviews.  Worst case, Project CARS: 72.27%. Best case, Dead Rising 3: 118.39%.     1440p  Card        Relative FPS R9 Fury X    91.95% * GTX 980 Ti  100.00%   * There are 5 games where the R9 Fury X is seriously underperforming. Without those 5 the relative FPS is 96.04%.    Dying Light     70.63% GRID Autosport  69.24% Project CARS    66.20% WoW: WoD        74.20% Wolfenstein NO  73.09%   Best case is Dead Rising 3 again at 111.23%.     Comparison between sites  Showing relative FPS (Fury/980Ti).  Averages per site:          HardOCP     Anandtech   TechPowerUp Guru3D  PcPer   Tomshardware 1440p   80%         90%         94%         95%     94%      99% 2160p   95%         97%         99%         100%    99%     105% Games   5           10          22          9       7       8   6 reviews tested Grand Theft Auto V:  GTA V           HardOCP     Anandtech   TechPowerUp Guru3D  PcPer   Tomshardware 1440p           85%         85%         87%         86%     75%     92% 2160p           91%         90%         97%         98%     87%     97%   5 reviews tested Battlefield 4:  Battlefield 4   HardOCP     TechPowerUp Guru3D      PcPer   Tomshardware 1440p           75%         84%         88%         89%     89% 2160p           94%         89%         92%         90%     92%   4 reviews:  The Witcher 3   HardOCP     TechPowerUp Guru3D      Tomshardware 1440p           87%          93%        87%          96% 2160p           96%         100%        92%         104%  Far Cry 4       HardOCP     Anandtech   TechPowerUp Tomshardware 1440p            83%        104%        103%        110% 2160p           106%        109%        111%        115%  Sh.o.Mordor     Anandtech   TechPowerUp Guru3D      Tomshardware 1440p            98%        103%         98%        100% 2160p           101%        105%        102%        105%  Metro:LL        TechPowerUp Guru3D      PcPer       Tomshardware 1440p           96%         N/A         104%        109% 2160p           97%         103%        114%        111%   Remember the various sites run with different detail levels and likely benchmark in different areas of the game. For UHD/4K/2160p I chose the highest settings when there were multiple choices on one site.  Assumption about how PcPer listed their average relative FPS results:    GTA V ""-33%"" = 980Ti 133%, Fury 100%   Metro ""+14%"" = 980Ti 100%, Fury 114%"
hardware,3axyzt,simscience22,2,Thu Jun 25 18:37:43 2015 UTC,Nice. Can't wait to see how well Fiji XT overclocks once voltage control is unlocked.
hardware,3axyzt,roaur,2,Wed Jun 24 19:28:54 2015 UTC,Interesting. Thanks for posting this information.
hardware,3axyzt,Schmich,1 point,Wed Jun 24 19:54:15 2015 UTC,If you still have eg. an excel sheet can you also compare between the websites? Would be interesting to see if the bias match!
hardware,3axyzt,OftenSarcastic,2,Wed Jun 24 20:08:38 2015 UTC,There isn't a lot of overlap in game choice. The only game that all four websites used is GTA V.  GTA V   HardOCP     Anandtech   TechPowerUp     Guru3D 1440p   85%         85%         87%             86% 2160p   91%         90%         97%             98%   Three sites:  The Witcher     HardOCP     TechPowerUp     Guru3D 1440p           87%          93%            87% 2160p           96%         100%            92%  Far Cry 4       HardOCP     Anandtech       TechPowerUp 1440p            83%        104%            103% 2160p           106%        109%            111%  S.o.Mordor      Anandtech   TechPowerUp     Guru3D 1440p            98%        103%             98% 2160p           101%        105%            102%   Remember the various sites run with different detail levels. For UHD/4K/2160p I took the highest detail settings when there were multiple choices on one site.     Edit: Some more averages per site:          HardOCP     Anandtech   TechPowerUp     Guru3D 1440p   80%         90%         94%             95% 2160p   95%         97%         99%             100% Games   5           10          22              9
hardware,3axyzt,HeliconPath,21,Wed Jun 24 20:40:04 2015 UTC,Was really hoping ATI would take back the lead this time around. At least it's competitive though!
hardware,3axyzt,Bacontroph,4,Wed Jun 24 20:56:11 2015 UTC,"In my book the performance is close enough for me to buy it.  The smaller form factor and cooling push it over the edge really which may be why AMD decided to go with an integrated water cooler.   The Nanos look really interesting though, can't wait to see where those fall out on the price/performance curves."
hardware,3axyzt,x0y0z0,5,Wed Jun 24 12:14:04 2015 UTC,"Yeah. Just got a 970 (and hole new pc after my 1 year old system was stolen). I'm not regretting my my choice honestly. My pc with a R2 290 was not performing as I thought it should. Idtech 5 games like the 2 recent wolfenstein games ran like shit. Even Modern Warefare had constant hiccups, high framerates but but constant stuttering! I'm an AMD fan but honestly I don't know what was wrong way my 290. Was it normal? Did I have a faulty card? Drivers? I don't know and whatever. I just want my stuff to work and so far with my 970 that's what I got. I don't want to loose faith in AMD but my personal experience was not great. Good luck to those buying new AMD cards atm. Hope you fair better than I did."
hardware,3axyzt,bigcrab,3,Wed Jun 24 21:20:46 2015 UTC,290x wolfenstein  runs smooth @ ultra 1200p
hardware,3axyzt,x0y0z0,1 point,Wed Jun 24 17:17:00 2015 UTC,I saw a lot of people having my problems with 290 and 290x. And a lot claims they don't. Hence my confusion. I'd love to know what the actual reason is for this instead of more conflicting reports.
hardware,3axyzt,dsteele713,2,Wed Jun 24 21:06:05 2015 UTC,How did your system get stolen? Was your house broken into and they took everything?
hardware,3axyzt,x0y0z0,1 point,Wed Jun 24 23:08:05 2015 UTC,Yup. Just like that.
hardware,3axyzt,fucusr,2,Thu Jun 25 01:33:29 2015 UTC,"Ditto, I was an AMD fan boy through and through until I owned a 6870 and a 290x. The 290x stuttered and crashed drivers, the 6870 just simply hated certain games and gave crap framerates and stuttering. bought a 970 and it has done its job with no issues so far knock on wood!"
hardware,3axyzt,barthw,3,Thu Jun 25 07:17:33 2015 UTC,"Seems to be a trend, which is kind of troubling. I have been in the AMD camp since 2008 with a HD4870X2, HD6870, R9 270X but for my new build it's a 970 since january. Before that i was pretty much an Nvidia guy starting with the Riva 128 and owning pretty much every generation until i switched from the 8800 GTS to team red :)"
hardware,3axyzt,HeyFux0r,2,Wed Jun 24 19:12:59 2015 UTC,which is kind of troubling   yep I have been in the AMD/ati camp since 90's. I did so for what I thought was sensible reasons but they are waning. This year was the first time I have ever built a pc for my self that has both Intel and Nvidia components.
hardware,3axyzt,dstew74,3,Wed Jun 24 19:31:33 2015 UTC,I haven't owned a Nvidia card since their flagship was in an 4X AGP slot.  Not the 90s but well over a decade ago.  I have a Fury X on back order but I'm thinking of finally throwing in the towel and grabbing a 980 Ti.
hardware,3axyzt,slapdashbr,1 point,Wed Jun 24 22:03:32 2015 UTC,"I mean... both companies have, at times, been the best choice for performance, for performance per dollar, or for performance per watt. Generally I prefer AMD if it is suitable because generally they are the best performance per dollar, and if you aren't building a super high end system, that is the most important metric.   When the 7000 series came out, AMD shit on the best cards nvidia had. nvidia couldn't beat that- although the 680 was good, it was only a hair better performing than the 7970 and it depended a lot on your overclock and specific games. then Nvidia had the 780, and they were clearly better. Then AMD had the 290X that was better still. Then the 980, etc.   The fury X vs the 980 Ti is fairly similar to the 7970 vs the 680, the really remarkable thing is how long we've been on fucking 28nm which is veritably ancient technology by computing standards. There has been no Moore's Law progress in the GPU field since god damn 2011!!!!"
hardware,3axyzt,dstew74,1 point,Thu Jun 25 04:03:59 2015 UTC,"Groan... I know.  Swallowing another 28nm video card is hard.  At least it has HBM 1.0.    I spent more time today looking over the reviews and size of the Fury X.  Looking from the benchmarks from the 15.20 driver, performance is much improved over the 15.5 driver people used.      It's going into a Ncase M1 and, honestly, it'll be a perfect fit for me.  I'm going to keep it and hope performance continue to improve with driver releases."
hardware,3axyzt,slapdashbr,1 point,Thu Jun 25 18:30:53 2015 UTC,yeah I have a silverstone SG05 which I could fit it in... although I'm going to wait on the nano to see how good it is and how expensive it is cause I don't plan on jumping to 4k yet.
hardware,3axyzt,x0y0z0,2,Thu Jun 25 23:43:01 2015 UTC,My first Nvidia card was the Riva TNT 2 Ultra. That was a great card. I was blown away by Quake 2 on opengl.   Then eventually I got the Geforce 4 mx 440. That was a time when low/mid end cards either couldn't run new games at all due to old shader models or it ran games so badly you might as well not bother. What a piece of shit that card was.  So the 970 is my first nvidia card since that (except for laptops that I don't count) and it's great to just play games smoothly without having to tinker and research poor performance and strange problems.
hardware,3axyzt,HeliconPath,3,Fri Jun 26 01:30:07 2015 UTC,Yeah you wanted the geforce 4ti :D
hardware,3axyzt,barthw,2,Wed Jun 24 22:29:47 2015 UTC,"Imagine the amazement when i popped the first gen 3dfx Voodoo into my PC to see accelerated and anti aliased graphics for the first time. This was Quake2 too which i played heavily (and competitively) from 1997 to 2002. Great times. I also remember the MX cards to be crap, from the 4 series i owned the Geforce 4 4200Ti which was kind of the 970 of that time, awesome card!"
hardware,3axyzt,Xellith,3,Wed Jun 24 22:48:39 2015 UTC,Yeah.  I might be switching back to green this time.
hardware,3axyzt,ColossalExodus,2,Thu Jun 25 06:10:58 2015 UTC,"Correct me if I am wrong, but the R9 295x2 is competitive right?"
hardware,3axyzt,IsaacM42,1 point,Wed Jun 24 18:49:23 2015 UTC,It's still faster than all other single PCB GPUs [Including Titan X].
hardware,3axyzt,ColossalExodus,1 point,Wed Jun 24 17:46:26 2015 UTC,"The only issue I see with that card in particular is the power consumption, I think it required quite a high draw from one rail if I am not mistaken. Oh and maybe crossfire being janky on occasion. But you can pick those up for 650 USD."
hardware,3axyzt,rePAN6517,1 point,Wed Jun 24 18:06:00 2015 UTC,It hasn't been ATI since 2006 :)
hardware,3axyzt,dstew74,2,Wed Jun 24 20:02:23 2015 UTC,AMD now worth less than the money they spent on acquiring ATI.
hardware,3axyzt,invesari,31,Wed Jun 24 20:13:09 2015 UTC,"I am withholding judgement for two reasons:  1) No voltage control on a liquid-cooled card. Why didn't AMD make sure reviewers could properly OC the card is beyond me, especially after calling the card ""an overclocker's dream"" at E3.  2) Higher performance at 4K is indicative of a CPU overhead. I want to see how good this card is at running DX12 apps."
hardware,3axyzt,Chunke,21,Thu Jun 25 04:05:02 2015 UTC,"Agree with your first point but not the second. If anything, 4k only serves to make GPU a bigger bottleneck than the CPU. The FuryX has better performance at 4k because of the higher memory bandwidth."
hardware,3axyzt,bphase,13,Wed Jun 24 12:54:59 2015 UTC,"He means that good 4K performance means it's CPU bottlenecked at lower resolutions. Which could well be true, but it could also be the memory bandwidth."
hardware,3axyzt,w00t692,5,Wed Jun 24 13:14:02 2015 UTC,"1.) Fair enough, but that was their decision and choice to do at launch, and it was a poor choice for sure. 2.) On a 4.4 ghz 5960x?  Not a chance, and certainly not to the point seen.  It seems more indicative of either memory bandwidth really starting to shine or something about the architecture lends itself to running at 4k."
hardware,3axyzt,w00t692,6,Wed Jun 24 13:31:20 2015 UTC,"If the fury x is superior in dx12 by a large margin, i'll buy it then.  But right now pretty much all games are dx9-11, most being 11 nowadays.  It's gonna be a while before games take advantage dx12, but i can certainly hope some games are updated with a dx12 render mode, which would be amazing."
hardware,3axyzt,TehRoot,10,Wed Jun 24 17:41:38 2015 UTC,The voltage control depends on 3rd party support not AMD. Preliminary look shows that the Fury is most likely using the same or a similar voltage controller to the 290x.  We should get an afterburner/etc. update soon.   http://www.overclock.net/t/1561860/various-amd-radeon-r9-fury-x-reviews/310#post_24083916
hardware,3axyzt,djisadud,5,Wed Jun 24 15:57:43 2015 UTC,"Exactly, when the Titan X came out, the voltage could not be controlled in AB. Only PX could change the voltage."
hardware,3axyzt,EpicNoob1983,2,Wed Jun 24 14:42:08 2015 UTC,I have deleted all my content out of protest. Reddit's value comes from it's content. Delete all your content and Reddit becomes worthless.
hardware,3axyzt,TehRoot,1 point,Wed Jun 24 17:01:42 2015 UTC,I imagine that since they went to GCN 1.2 ~ tonga ~ that the changes there that lowered power consumption were applied to Fiji.
hardware,3axyzt,downeverythingvote_i,25,Wed Jun 24 17:32:07 2015 UTC,"As a 290X owner that was quite hyped for the Fury I am quite disappointed.  The Fury X is not something that can sit in your case comfortably for a couple of years. Even with the upcoming DX12, and the improvements it brings, the Fury X is not going to take you to the future of 2016/17 gaming. We are sitting on a precipice here where you could say that 90% of games will run completely smoothly (60 FPS ++) on an OC 290X and OC GTX 970 at the 1080p and 1440p resolutions. There are a handful of games, like Witcher 3, that will punish your system, but the 290X and GTX 970 run the game at a visual quality and framerate that are still gorgeous.   The new high end cards needed to be at least 50% more powerful to warrant their higher prices. The 980Ti variants succeed in this regard yet are stuck with GDDR5 and the Fury X falls flat as a performance competitor but has HBM (what a waste). 2016 will bring 14nm/16nm from AMD and NVidia respectively. Both will sport the second generation of HBM and we will have 8-16GB cards (remember that HBM bandwidth goes up with the amount that you have) with 2-4x the bandwidth of Fury X. There will be PC games and mods that will take advantage of this. Buying a Fury X or 980Ti when you have a 290X or GTX 970 (and equivalents) is a waste of money for any practical gamer.   The immediate difference between this generation's 28nm beasts and the first generation of 14nm/16nm cards won't be so dramatic, yet they will be packing the cutting edge features that will at least help them age better when the following generations arrive. By the end of 2016 it's most likely that the 980Ti and Fury X will be pieces of shit compared to the performance leaders at that time. Basically don't expect these cards to age as gracefully as the 290X has, since the adoption of the more advanced HBM2 (the performance and size gain is just stupidly huge compared to the first gen on Fury X) alongside the revolutionary API/Software changes puts the 980Ti and Fury X in an awkward position.   Remember, these are current gen cards designed with future gen support, yet the transition between DX11 to 12 is going to be much more drastic than DX9 to 11, where the difference is big enough that hardware strengths developed specifically to deal with DX9/11 are going to be useless going forward."
hardware,3axyzt,Chunke,5,Wed Jun 24 17:33:09 2015 UTC,"I don't claim to know the decision making process for features at AMD/Nvidia but I really think AMD should've waited for 16nm before unleashing HBM. I'm willing to bet only about 70-75% of that delicious memory bandwidth is being used right now. And that's all because of the godawful pixel fill rate (64 ROPs, really??) and an okay number of shader cores. 16nm would've allowed them to bump up shader and ROP counts by a good 25-30%. Then you'd really see HBM shine.   I know this is probably just a case of hindsight = 20/20, but if AMD had released the 300 series last year and Fiji maybe 3-4 months from now (giving them time to jump on to 16nm), they'd have the best performing GPU for at least 9 months. It would get rid of a number of issues:   300x series would no longer be a letdown. It would still be the same product it is right now but would be released much earlier to compete with 980 and 970. AMD could charge a premium for 16nm+HBM. The performance gains would justify it. The lower power draw due to the process and HBM would justify it. Right now, I'm guessing $650 is not what they want to see Fury at. The margins are too low for a flagship card. And more price cuts may be on the way."
hardware,3axyzt,bphase,5,Wed Jun 24 15:14:45 2015 UTC,"but if AMD had released the 300 series last year and Fiji maybe 3-4 months from now (giving them time to jump on to 16nm), they'd have the best performing GPU for at least 9 months.   This wouldn't be possible. We're still about a year or more off from a die shrink."
hardware,3axyzt,0pyrophosphate0,5,Wed Jun 24 16:02:38 2015 UTC,"They did the right thing. Next generation, Nvidia and AMD will both have HBM2, 16nm production, and their own architectural improvements to worry about. That's a lot that can potentially go wrong, and AMD already has their feet wet in at least one of those. Not to mention, they'll already have a year of HBM-related driver tweaks under their belt, inspired by how real games perform in the field on real HBM hardware. That may or may not amount to much, but an advantage is an advantage.  There are plenty of advantages to being first, even if the consumer doesn't necessarily benefit right now."
hardware,3axyzt,Juheebus,0,Wed Jun 24 17:07:10 2015 UTC,"Yea I'm disappointed as well. I was planning on doing a big upgrade this year but I'm going to wait now. I want to build a nice mITX skylake PC so I'm most likely going with a used 970/980 (seen 980's for $400, r9 nano may drop the price further) and xeon e3 1200 v5 when it's released (not going to overclock in a small case, dont need integrated, hyperthreading for dx12, cheaper than i7) to carry me until next year."
hardware,3axyzt,felixar90,5,Wed Jun 24 19:04:52 2015 UTC,Holy crap that card is the coldest I've seen since the passive cooling days.
hardware,3axyzt,w00t692,2,Wed Jun 24 19:43:08 2015 UTC,AIO water cooling is the shit.  If you have a card that can fit the kraken g10 bracket or something similar you should do it!  My 980 runs between 47-52c depending on load in games with an H75 in single fan configuration mounted like ass at the bottom of the case.
hardware,3axyzt,felixar90,1 point,Wed Jun 24 15:31:48 2015 UTC,"Well, I already have my custom loop with 2 quadruple rads so whatever card I buy, I strip down and put my own water blocks on it.  That single chip ought to make things much simpler tho. Most of the time spent installing a water block was meticulously cutting and placing thermal pads on all the memory modules.   With 3 R9 290X, it reaches about 60C at load, and that's my quieter setting."
hardware,3axyzt,Knight_of_autumn,4,Wed Jun 24 17:52:03 2015 UTC,"So we still do not have a card that can do 4K at 60FPS this gen? Well, there is always next year."
hardware,3axyzt,Seclorum,2,Wed Jun 24 18:43:53 2015 UTC,Well it can probably exceed 60fps if you drop some settings.   But many people dont get a flagship card to play at anything less than absolute balls out maxed.
hardware,3axyzt,felixar90,7,Wed Jun 24 18:05:19 2015 UTC,Aww yiss. I am pleased to see the 3 Displayports. I was disappointed that my 290X only had 1 displayport connector.
hardware,3axyzt,parasemic,3,Thu Jun 25 04:36:22 2015 UTC,"I'm dissapointed to see DVI-D missing, though 980Ti seems like a much better deal for 1080p 144Hz anyway."
hardware,3axyzt,mdnpascual,11,Wed Jun 24 15:08:48 2015 UTC,"Guru3D and anandtech reviews benchmarks are the only one who has the Fury X performance near 980ti. All other ones are portraying a pretty big gap vs 980ti.  Welp, it's a bit of a disappointment. If there's a fury X in stock on the vendor I'll go later. I'll get it, if none, I'm getting the 980ti."
hardware,3axyzt,TheSpooneh,3,Wed Jun 24 15:14:23 2015 UTC,Large gap in 1080 and 2160. Fury X edges out in 4k+  Depending on what your needs are you'd pick a 980TI or a Fury X accordingly imo.
hardware,3axyzt,Nixflyn,1 point,Wed Jun 24 13:52:04 2015 UTC,2160 is 4k. I assume you meant 1440p.
hardware,3axyzt,namae_nanka,1 point,Wed Jun 24 20:53:00 2015 UTC,"It's slightly better than 980Ti at 4k here,  http://www.sweclockers.com/test/20730-amd-radeon-r9-fury-x/17#content  Techpowreup have it about equal if you remove project cars at 4k."
hardware,3axyzt,warenb,3,Thu Jun 25 06:34:09 2015 UTC,"I was thinking everyone sure has made a heck of a racket with AMD's all-new hardware architecture but their flagship and best effort (+water cooling) so far has only came out on par overall with the 980 Ti and has to price it accordingly. Well, here's to hoping there is plenty of headroom from the new setup via ""teething issues""."
hardware,3axyzt,pal25,7,Wed Jun 24 17:38:06 2015 UTC,Literally a beast   I don't think they know what this word means.
hardware,3axyzt,Anally_Distressed,14,Wed Jun 24 15:41:26 2015 UTC,It makes no sense to get the Fury X over the 980ti at its current price point. It really needs to be priced lower if its performance can't match or exceed its Nvidia counterpart.   Hopefully the Fury will be more competitive.
hardware,3axyzt,jauntylol,25,Wed Jun 24 16:58:36 2015 UTC,"It makes no sense to get the Fury X over the 980ti at its current price point   Well, I'll give you four reasons:  -you can't hear the card  -has a shorter PCB  -Nvidia tends to forget older architectures as Kepler proves when they release newer architectures. Nowadays my 670 DC2 is slower than a 7870 (or 270/270x/370) while it was quite faster by an awful lot when released.  Is it enough to make people buy this cards? I don't know and I don't think so, especially with the whole AMD stigma around.  The only three things that could really change Fiji's destiny are:  -updated drivers, the cards are really bottlenecked at lower resolutions  -the air cooled fury which will cost 100 $ less  -most importantly unlocked overclock potential"
hardware,3axyzt,bigpappaflea,13,Wed Jun 24 13:48:00 2015 UTC,"""Gamers"" buy cards to play games with. I want to play games with my card so I am going to buy the single gpu solution that runs my favorite games better. Looks like the 980 ti wins in GTA, Bioshock, Witcher, and Shadow of Mordor."
hardware,3axyzt,SirMaster,27,Wed Jun 24 14:04:50 2015 UTC,-you can't hear the card   Have you listened to a recording?  The pump has an audible and IMO somewhat annoying whine to it.
hardware,3axyzt,Penmerton,0,Wed Jun 24 14:10:44 2015 UTC,"Are you sure it's the pump? guru3d is saying it's coil whine (which is even worse imo, that would be an instant return for me)"
hardware,3axyzt,Anally_Distressed,5,Wed Jun 24 14:22:07 2015 UTC,"I'm currently running crossfire 7970s, so the AMD stigma really doesn't concern me.   I try to look at things as objectively as possible, and so far it does seem like the 980ti is a better buy, strictly from a performance perspective.   Most aftermarket coolers are not really noisy at all, and while size may be useful for certain niche small form factor builds, most ATX cases can accommodate much larger cards, making the size advantage a fairly moot point.   I definitely agree with your point on drivers and vcore unlocked OC potential though. Maxwell has substantial overclocking headroom and unless the Fury has the same headroom to match it just seems to look worse for team red. Hopefully that's not the case, but most GCN cards seems to have a 10-15% headroom, and I don't expect the Fury to be much different.   I honestly think the Fury's success will really just depend on how AMD prices it at this point."
hardware,3axyzt,Spazboy9490,11,Wed Jun 24 18:36:54 2015 UTC,"I disagree. With the 980ti you get  -its faster in most games and test  -same price   -it has Nvidias Drivers, and optimization  (still crap but better then AMDs in my experiences)  -it has all of their tech (inb4 "" its cancer to gaming"")   Now for points you brought up from top to bottom.  -with non-reference card you can't hear it ether, unless you are trying to burn the card  -you may have a shorter pcb, but you have to find a place for a huge radiator . So this pretty much makes the argument of having a small case with a small card invalid. because most small cases (worked with most) wont have room for one  -I disagree completely. I still have 2 680s running the exact same if not better as the day I got them (possible dying card on your side?)  -while the fury will be less then the fury x. its going to be slower and its going to run be hotter   -AMD is known for not being easy to overclocking due to the amount of heat it puts out and vcore that is need to get anywhere.   Compared to the 980ti where you can easily go from 1000mhz to 1400+ mhz with very little (less then 24mVs) or no voltage adjustment    -while I have no experience with the furyx on overclocking. I know that memory overclocking is permanently disabled.   All that being said. I'd Hold off on the furyx AND the 980 ti because HBM is so new and both AMD and Nvidia have said they are working on HMB 2.0 for there next gen cards.   on mobile so grammar and spelling might be shit."
hardware,3axyzt,jauntylol,-1,Wed Jun 24 14:37:12 2015 UTC,"it has Nvidias Drivers, and optimization (still crap but better then AMDs in my experiences)   This is a con to me.   -it has all of their tech (inb4 "" its cancer to gaming"")   Which only provides problems on Nvidia or not Nvidia cards. All gameworks or Nvidia supported games are plagued by problems and terrible performance issue. Stuff like physx doesn't even work on gpu but cpu.   -I disagree completely. I still have 2 680s running the exact same if not better as the day I got them (possible dying card on your side?)   Recent game benchmarks clearly show kepler cards struggling against old competitors which were slower at release.   -while the fury will be less then the fury x. its going to be slower and its going to run be hotter   but it will be priced as a reference 980, so let's see   Compared to the 980ti where you can easily go from 1000mhz to 1400+ mhz with very little (less then 24mVs) or no voltage adjustment   Never heard anybody getting more than 250 mhz on clock without overvoltage   -while I have no experience with the furyx on overclocking. I know that memory overclocking is permanently disabled.   This is still not for sure. But I agree.  Mind you, yes, I'd buy a 980ti over a Fury X right now. But, with unlocked overclock and better DX11 drivers I think Fury X would shine and give me better longevity.  As Kepler owner I just feel cheated for Nvidia's support in last games and drivers. I go significantly slower than AMD cards like 270x. My GTX 570 can't even catch a 260x right now.  edit:I own both a 670 DC2 and an EVGA 570 SC I have on a second computer, jesus. I'm just stating that when I bought my 670 (especially the DC2) was fighting game to game with a 7970. Now it is slower in many games than a 270 (so a 7870, which was comparable to a gtx 660). Meanwhile my 570 became with time slower (on newer games) than a 7790 which when released was way worse.  That's why I'm saying that Kepler (and Fermi before) driver development was terrible on Nvidia, while AMD cards (thanks to gcn architecture) tend to stay more performing with time.  I mean you can go and check many reviews on your own, a 780ti which at release was faster than a 290x now is struggling behind a 290 in GTA 5 or The Witcher 3 or Civ BE."
hardware,3axyzt,Chunke,6,Wed Jun 24 14:44:23 2015 UTC,"GTX 570 isn't even Kepler though, it's Fermi :O"
hardware,3axyzt,Bitech2,1 point,Wed Jun 24 16:12:56 2015 UTC,my 670 DC2  As Kepler owner  My GTX 570   Typo?
hardware,3axyzt,jauntylol,1 point,Wed Jun 24 16:30:01 2015 UTC,Never had two computers at home? One for gaming and one for downloading/office/occasional gaming you let your father use or something?
hardware,3axyzt,SPOOFE,0,Wed Jun 24 19:39:59 2015 UTC,"Stuff like physx doesn't even work on gpu but cpu.   Gettin' tired of this. If you want interactable physics it's going to the CPU anyway. GPU stuff is only good for eye candy. Don't get me wrong, eye candy's great, but stuff that actually affects the player and gameplay is on the CPU."
hardware,3axyzt,HaMMeReD,1 point,Wed Jun 24 19:42:30 2015 UTC,"Physics processing it going to have to move to the GPU for everyone eventually.   We currently do, I dunno, 0.001% of realistic physics processing, and it scales very difficulty.   Things like soft-body physics, volumetric smoke, fluids, fire etc, not going to be done on todays CPU architectures."
hardware,3axyzt,SPOOFE,1 point,Wed Jun 24 18:53:55 2015 UTC,"Yup, the reason it's so desirable is you can simulate tens to hundreds of thousands of objects, and the water physics look breathtaking. But until there's a way to get the updated position/momentum/whatever data back to the engine running on the CPU, it's just pretty rendering... which is fine for some things (like water or explosion debris), but not so hot for a building collapsing on the player or something."
hardware,3axyzt,HaMMeReD,3,Thu Jun 25 02:09:36 2015 UTC,"Smoke demo from batman. https://www.youtube.com/watch?v=_Gp8mgmqE84  Since they only drive forward you don't get the full effect, but it's still a good demo."
hardware,3axyzt,SPOOFE,1 point,Thu Jun 25 02:15:58 2015 UTC,"It's true, it's pretty, but note that the smoke is reacting to the player (and other level geometry), not changing game world conditions that affect the player. Also note how imperfect and averaged it is; very cushiony, billowy. I'm not criticizing - again, it looks gorgeous - but it's a perfect example of a task that can be simulated on GPU without having to do much talk back to the CPU. None of the resultant calculations that render the smoke can readily be sent backwards through the GPU pipeline and back to the game engine thread(s) running on the main processor. They're sent off to be displayed on the screen."
hardware,3axyzt,HaMMeReD,1 point,Thu Jun 25 02:18:56 2015 UTC,"It's accessible to the CPU, it's just not realistic or feasible to have smoke interact in that way on the CPU it's just not fast enough.   However, if you had a gameplay mechanic that needed it, you could probably sample a small percentage of the smoke particles and use it to produce your world-changing effect. (or run a reduced simulation in CPU for your physics/state engine)"
hardware,3axyzt,ShadowthecatXD,3,Thu Jun 25 02:40:56 2015 UTC,"You can't hear a good non-reference 980 ti either, I have the evga ACX version and it's literally silent. I see no reason to go for a fury based of the reviews I've read so far other than brand loyalty."
hardware,3axyzt,GthrowawayG,0,Thu Jun 25 03:05:13 2015 UTC,Also after Nvidia 970 I don't think i can buy Nvidia anymore... Although I either I won't get the fury either ... Sigh life is hard
hardware,3axyzt,jauntylol,1 point,Wed Jun 24 14:22:32 2015 UTC,But wallet is happy unless you really need to upgrade.
hardware,3axyzt,PhilipK_Dick,0,Wed Jun 24 14:38:18 2015 UTC,"You shouldn't upgrade from a 970 until the next generation anyway - unless you are getting a monitor with more pixels.  What's wrong with your 970 anyway?  Are you still talking about the 3.5 controversy?  Unless you are running at 1440p, you wouldn't know by performance and for 1440p, you have the wrong card."
hardware,3axyzt,Hariooo,7,Wed Jun 24 16:13:14 2015 UTC,freesync is still $300 cheaper than gsync  that's a hell of a reason even if it's a slightly worse implementation
hardware,3axyzt,Anally_Distressed,6,Wed Jun 24 19:36:36 2015 UTC,"That's actually a really good point, I haven't considered it from that perspective."
hardware,3axyzt,Chunke,5,Wed Jun 24 15:24:16 2015 UTC,Wasn't AMD going to push for GPU-agnostic FreeSync? So that a FreeSync monitor could be used with any GPU? I'm not sure actually. I thought that was one of the plus points about AMD's response to GSync.
hardware,3axyzt,Hariooo,5,Wed Jun 24 15:28:32 2015 UTC,that's great for them and intel but nvidia has shown no signs of jumping on board
hardware,3axyzt,downeverythingvote_i,4,Wed Jun 24 16:17:50 2015 UTC,"Adaptive sync is GPU agnostic, it's part of the Display Port 1.2a standard. What this means is that NVidia can choose to support Adaptive sync over Display Port, being a standard feature means that NVidia doesn't have to be lisenced by AMD or pay AMD-"
hardware,3axyzt,glr123,1 point,Wed Jun 24 16:19:41 2015 UTC,"Ya sure they want to, but even if they do it's up to Nvidia to support it."
hardware,3axyzt,attomsk,4,Wed Jun 24 17:16:20 2015 UTC,"Huh? It's $200 cheaper if we are comparing similar 1440p ips monitors.  That $200 includes ULMB mode on the G-Sync panel and G-sync actually works up to 144Hz  Personally, I would never spend $650 on a GPU and $600 on a monitor to get a worse experience.  When I am spending a minimum of $1250 and another $200 means a  faster GPU with good overclocking and a fully featured monitor, then that's what I am going with."
hardware,3axyzt,Hariooo,0,Wed Jun 24 16:37:18 2015 UTC,the acer gsync ips compared to the asus freesync ips is roughly a $300 in the regions where the latter has been released.  if the difference ends up being $200-250 okay. i don't think it takes away from the larger point I'm making
hardware,3axyzt,attomsk,7,Wed Jun 24 15:46:15 2015 UTC,I live in the US and the difference is exactly $200 here
hardware,3axyzt,Hariooo,0,Wed Jun 24 15:54:12 2015 UTC,okay
hardware,3axyzt,Kinaestheticsz,2,Wed Jun 24 15:57:25 2015 UTC,"Yeah, but the difference is that GSync on the Acer works from 30-144Hz (and technically below the 30Hz limit), whereas the Asus FreeSync IPS's VRR range is a lovely 35-90Hz range. Which is just miserable. That isn't just a small amount worse implementation. That is a huge difference."
hardware,3axyzt,ConsolePeasantNoMore,1 point,Wed Jun 24 16:03:40 2015 UTC,When does it come out?
hardware,3axyzt,TehRoot,16,Wed Jun 24 18:26:51 2015 UTC,The overclocking potential for the card seems rather poor. All the reviews I've seen haven't been able to get past a 1150 Mhz OC on the GPU.
hardware,3axyzt,barthw,18,Wed Jun 24 14:02:59 2015 UTC,There's no voltage or memory controls yet. Calm down
hardware,3axyzt,hibbel,14,Thu Jun 25 02:58:28 2015 UTC,"Still Maxwell can do quite a bit more without touching voltages, so i remain skeptical."
hardware,3axyzt,Nixflyn,3,Wed Jun 24 12:18:17 2015 UTC,"My ASUS 980 Strix on stock voltage goes to 1400 (up from 1279, IIRC). Reference is 1197, IIRC. I can run it at 1500 but no amount of voltage will keep it from crashing randomly and rarely. Still, 1400 will have to suffice."
hardware,3axyzt,TehRoot,1 point,Wed Jun 24 12:20:52 2015 UTC,"Sadly, the Asus Strix models just don't have the crazy power delivery systems that the Gigabyte and MSI models do. Pretty damn quiet though."
hardware,3axyzt,attomsk,2,Wed Jun 24 13:10:20 2015 UTC,AMD cards always need more voltage. +100 is fairly impressive for stock. I know getting an extra 100 out of my 290x took a decent bump to Vcore.
hardware,3axyzt,TehRoot,5,Wed Jun 24 14:39:07 2015 UTC,"There will be no memory overclocking, already confirmed by AMD.  Also if they were going to unlock the voltage for these cards, you would think they would give that ability to the reviewers right?"
hardware,3axyzt,attomsk,2,Thu Jun 25 10:14:44 2015 UTC,"erm, voltage unlocking is a 3rd party problem..we have to wait for afterburner/etc. to roll out an update to support it  http://www.overclock.net/t/1561860/various-amd-radeon-r9-fury-x-reviews/310#post_24083916  https://forums.geforce.com/default/topic/822755/unlock-voltage-for-titan-x-in-msi-afterburner-/  http://forums.guru3d.com/showpost.php?p=5040708&postcount=2037"
hardware,3axyzt,TehRoot,1 point,Wed Jun 24 13:11:58 2015 UTC,"right you are, but it is still too bad that reviewers were stuck at stock."
hardware,3axyzt,TehRoot,-1,Wed Jun 24 15:43:22 2015 UTC,It's the same at almost every base GPU review....it's just that people are really sperging today
hardware,3axyzt,RavenBlade87,3,Wed Jun 24 15:47:00 2015 UTC,"Almost every card is the same at release, there's an update that unlocks it."
hardware,3axyzt,LRed,9,Wed Jun 24 15:59:43 2015 UTC,"The card runs at 50c at load, that's an insane amount of headroom for when drivers and utilities allow proper OC."
hardware,3axyzt,thejshep,21,Wed Jun 24 16:01:22 2015 UTC,"I find that temperature is generally a very small part of overclocking a card. More important will be how the voltages hold up and how much power will be needed for a few Mhz bump.  Considering what Maxwell can do without touching voltages that much or at all, I highly doubt AMD will win there but I will await further results to be sure I guess."
hardware,3axyzt,w00t692,8,Wed Jun 24 12:42:54 2015 UTC,"Thank you. I always put my cards under water where temps aren't even a factor. My 980ti's will touch on 1500 mhz and not hit 55c, it's the voltage that is the limiting factor."
hardware,3axyzt,exscape,1 point,Wed Jun 24 12:44:12 2015 UTC,"I tend to agree, but after slapped a kraken g10 on my 980 and water cooled it, the stable overclock went up by 100 mhz from the 25c drop in temps."
hardware,3axyzt,ShoutingDani,2,Wed Jun 24 12:25:41 2015 UTC,"If overclocking causes instability, I doubt drivers and utilities are the main cause."
hardware,3axyzt,slapdashbr,2,Wed Jun 24 13:07:52 2015 UTC,"In my experience you generally hit the wall of the core clock before your card starts throttling due to temps (I mean artifacts, crashes etc not due to overheating) and get to a point where no amount of extra voltage makes the card stable above certain core clock. Just my personal experience, not sure if I am right."
hardware,3axyzt,attomsk,1 point,Wed Jun 24 14:33:36 2015 UTC,"agreed, most people commenting here don't seem to understand how overclocking works or what actually limits it... Gee if only I could put 3V and 400W through my GPU I could get it to 2.8GHz right?"
hardware,3axyzt,TehRoot,1 point,Wed Jun 24 16:00:48 2015 UTC,Core Temperature isn't the only factor in overclocking...   Look how hot the Fury X's VRM's are:  http://www.overclock.net/content/type/61/id/2498531/width/350/height/700/flags/LL  That's over 100c. Those VRM's might not go much further.
hardware,3axyzt,thejshep,2,Wed Jun 24 13:44:54 2015 UTC,VRM max temp is 125C
hardware,3axyzt,Seclorum,2,Wed Jun 24 14:17:12 2015 UTC,"I think they upped the stock clocks in response to the 980ti. Once both these cards are overclocked, the 980ti is a very clear winner."
hardware,3axyzt,thejshep,1 point,Thu Jun 25 18:34:23 2015 UTC,Reviewers dont have tools for voltage control. None of the 3rd party apps they were using to overclock were able to adjust the voltage. The app developers need to update.
hardware,3axyzt,Seclorum,4,Wed Jun 24 13:42:28 2015 UTC,I think we're going to find that amd had push this card pretty far already - they knew what they were up against with the 980ti and still couldn't make a card to beat it. There were rumors that they reworked the clocks after the ti's launch and I'm betting that's where most of the overclock headroom went. I still think it's pretty amazing that two different companies went about making two different cats with completely different approaches and wound up at the same performance and price point.
hardware,3axyzt,Nixflyn,8,Wed Jun 24 16:17:10 2015 UTC,"I'm still hesitant to judge it's OC potential until we have proper tools for the job.   What interest's me also is the scaling of performance.   It get's crushed at 1080p and soundly defeated in most 1440p but 4k it's much closer, yet people keep bringing up how it should be terrible at 4k because of it's low Vram yet we dont see any evidence of Vram bottlenecking at that high resolution."
hardware,3axyzt,PhilipK_Dick,2,Wed Jun 24 14:31:54 2015 UTC,"People just keep freaking out about ""the next big game"" that might use more than 4GB. Whatever, I'm gonna measure with what we have now because I'm not psychic. My usage is 1080p 120Hz + lightboost (maybe 1440p 120Hz+ next upgrade) anyway, so 4GB is way more than enough. But 980 Ti wins for the same price at 1080p."
hardware,3axyzt,IsaacM42,2,Wed Jun 24 15:18:58 2015 UTC,I'm pretty sure AMD was hoping to charge more for the Fury X but the market was set by the 980ti.
hardware,3axyzt,0pyrophosphate0,2,Wed Jun 24 15:48:04 2015 UTC,"So, this means AMD will drop its price right? Especially when you consider that the ti is also 650 and outperforms it in most cases."
hardware,3axyzt,mojorific,6,Wed Jun 24 16:13:22 2015 UTC,"Maybe just down to 600? The 980 Ti is faster, but only by a handful of percent, and the Fury X is sexy as fuck. They perform so close, even when it comes to efficiency, anybody is pretty much just going to choose the brand they prefer, and I wouldn't fault anyone for choosing either one over the other.  That said, the 980 Ti does still perform a touch better for the same money, and that's not a position people are prepared to face with an AMD card."
hardware,3axyzt,Seclorum,4,Thu Jun 25 10:20:48 2015 UTC,"It looks like Nvidia played their cards right and released the 980ti just to be slightly higher than the competition.  I applaud AMD for bringing new technologies into their video cards (water cooling, and HBM).  It's unfortunate that they are always finishing second best however.  I guess they push the '4K' angle because that is one of their strengths in this segment.  Most could care less about 4k at this point however.  I hope they can come out with some improvements that can put them on par or better than the 980ti for future revisions.  We will see!"
hardware,3axyzt,forestcollector,2,Wed Jun 24 19:29:03 2015 UTC,"Given that the 980ti is basically a crippled Titan-X it's still a huge loss for Nvidia's bottom line.   Every one of those 980ti's are leaving 200-350 dollars on the table that they could have charged on Titan-X's.   200-350 dollars per card just to put it in a price bucket to beat the Fury X.   Sure the 980ti is a great card, but I'm willing to bet there are some investors out there who are pissed about the lost revenue."
hardware,3axyzt,everyZig,2,Wed Jun 24 18:20:06 2015 UTC,The Titan X was intentionally priced at a really absurd point. It was never going to be a card for the masses. This was Nvidia's strategy all along. Are you already forgetting that they did the same thing with the last Titan? The original was outdated by the 780 Ti quite soon after it was released.
hardware,3axyzt,TomHicks,1 point,Wed Jun 24 19:34:41 2015 UTC,"The original was outdated by the 780 Ti quite soon after it was released.   for pure gaming, yes, for the intended market of the original kepler titans, no. the 780Ti has much lower double precision performance then the Titan, so for prosumers who need double precision CUDA performance, the Titan was still surpreme."
hardware,3axyzt,everyZig,1 point,Wed Jun 24 15:41:43 2015 UTC,Titan X is crippled in double precision performance.
hardware,3axyzt,TomHicks,1 point,Thu Jun 25 04:40:50 2015 UTC,"I know, hence me mentioning the original kepler titans  The point of the Titan X sort of escapes me currently, besides people just showing off they have money to burn on PC stuff"
hardware,3axyzt,everyZig,1 point,Thu Jun 25 06:55:33 2015 UTC,"I know, hence me mentioning the original kepler titans   heh just wanted to let other people know, I've seen quite a few people hold that misconception.   The point of the Titan X sort of escapes me currently   Same here"
hardware,3axyzt,i_mormon_stuff,1 point,Thu Jun 25 08:55:50 2015 UTC,"Every one of those 980ti's are leaving 200-350 dollars on the table   Except not everyone who bought a 980Ti would have bought a Titan, or a 980Ti at a higher price point, even if FuryX didnt exist. Prices arent 100% elastic."
hardware,3axyzt,jigssaw,3,Thu Jun 25 18:38:53 2015 UTC,"Honestly with HBM and everything I was hoping it would be 1-2% faster than the 980 Ti. It looks to be 3-5% slower instead in all the games I have interest in and 0.5-1% slower in ones I don't care for.  That's a tough sell at the same price point especially with the other drawback, that integrated water cooler. And I say drawback because I can't mount two of those in my case for Crossfire, just won't fit.  It's disappointing, especially after all this buildup and hype. I really wanted AMD to blow the doors off NVIDIA but it's just not happening.  Now having said all this, I don't begrudge anyone who decides to buy this over the 980 Ti. It's disappointing but it isn't a bad card, it just came out a bit too late in the year, had AMD released this 2-3 months before the 980 Ti the reception would be completely different."
hardware,3axyzt,i_mormon_stuff,1 point,Thu Jun 25 18:50:33 2015 UTC,Well they are making dual Fiji which should take care of that can't mount two of them issue
hardware,3axyzt,Esyir,2,Thu Jun 25 18:54:27 2015 UTC,Personally I prefer two single chip cards for both aesthetics and performance.
hardware,3axyzt,zerghumper,1 point,Thu Jun 25 08:54:25 2015 UTC,Isn't there the air-cooled Fury for that.
hardware,3axyzt,cestith,2,Wed Jun 24 17:21:47 2015 UTC,"I was really thinking that with HBM the stock FuryX would easily beat a stock 980ti. A lot of people here are saying to withhold judgment until we see the Fury's overclock potential, but I was hoping for a clear winner out the gate.  Perhaps with overclocks and driver tweaks the fury will pull ahead, but I doubt this will cause the Nvidia / AMD price war I was hoping for. Still, not a bad turnout from AMD by any means. Just not the 980ti killer the company really needed right now."
hardware,3axyzt,0pyrophosphate0,1 point,Wed Jun 24 18:03:56 2015 UTC,Umm... they say going from 28nm features to 14nm features will result in a chip half the size... on a square chip...
hardware,3axyzt,cestith,3,Wed Jun 24 18:12:57 2015 UTC,"Everything doesn't uniformly scale down from 28nm to 14nm. Many parts of the chip have to remain the same size, or only shrink slightly."
hardware,3axyzt,everyZig,1 point,Thu Jun 25 09:53:43 2015 UTC,"It won't cut things to a quarter of the size, but it will do better than half."
hardware,3axyzt,TheOneWatcher,1 point,Wed Jun 24 16:32:58 2015 UTC,"Well, half the dimensions, a quarter of the surface.. depends on your point of view, right before they described it as a 5x5cm chip i think"
hardware,3axyzt,rokr1292,1 point,Wed Jun 24 18:17:12 2015 UTC,"I think so.  I am more preferential to the nvidia cards , but I am interested to see what happens."
hardware,3axyzt,bert_lifts,1 point,Wed Jun 24 19:28:40 2015 UTC,I really want one. But all I play is csgo. Someone tell me I don't need to replace my 290 yet.
hardware,3axyzt,MisterScrewtape,1 point,Wed Jun 24 21:26:34 2015 UTC,csgo is cpu dependent... So no.
hardware,3axyzt,Drudicta,1 point,Thu Jun 25 08:51:02 2015 UTC,"It seems to me that the drivers for the Fury X aren't entirely ready for primetime.  PCPer noted in their review that AMD said to them that many game profiles in the Catalyst drivers aren't yet optimized for a HBM system.  While I'm not totally on board with the idea that memory issues are at the core of sub-4k performance, I think the need to optimize the driver for a large GCN chip is pretty reasonable.  As far as I'm concerned the real test for Fiji comes when the drivers start to stabilize, the aftermarket Fury's come out (so we can compare the 980ti aftermarket versions with aftermarket Fury).  Plus as others have noted, GCN tends to need more voltage to get good overlocks.  Since the Fury X can clearly dissipate far more heat than Fiji is producing, we really need to wait until voltage can be adjusted."
hardware,3axyzt,TehRoot,1 point,Wed Jun 24 19:46:08 2015 UTC,"I'm severely disappointed. I was waiting for this, excited, and I'm crushed. I don't want to buy another Nvidia card due to some practices and to support the little guy but...   They hyped the fuck out of this, and it's both expensive and performs worse than a GTX 980Ti in anything but 4k. I play in 1080P and try to max everything out.  I'll just wait for prices to drop and buy a used card off Ebay again when a <$300 price point pops up."
hardware,3axyzt,lorywindrunner,1 point,Thu Jun 25 05:44:02 2015 UTC,You play at 1080p and you're complaining?
hardware,3axyzt,Drudicta,1 point,Thu Jun 25 10:12:36 2015 UTC,I play at 1080 but I have to drive 144 fps.
hardware,3axyzt,TehRoot,1 point,Wed Jun 24 15:16:56 2015 UTC,"Yes I am complaining. I'm complaining because I've always had to buy used, mid tier stuff about a year, sometimes 2 after it comes out.  I'm complaining because in some games I don't get 60fps, and I want 60 fps. I want a tiny bit of AA. I want to crank all the settings to max otherwise except for the shit that adds blur."
hardware,3axyzt,w00t692,1 point,Wed Jun 24 22:27:42 2015 UTC,"So buy a TI then, or buy a 290x and save some money."
hardware,3axyzt,Nixflyn,0,Thu Jun 25 07:03:54 2015 UTC,I've read 3 reviews now and see that I'm not purchasing this card.   Sucks man :(  I really think the 980 ti with its tricky boost clock really caught amd off guard.
hardware,3az95x,willyolio,19,Wed Jun 24 18:16:35 2015 UTC,"I like their testing methodology. The frame time graphs especially as they can point out issues not easily seen in pure FPS graphs.   And overall their conclusion is rather consistent with other sites testing.   The 980ti is absolutely faster but the margin is rather tight.   Perhaps AMD can smooth things out in drivers and perhaps DX12 software plays nicer with the hardware, but until that happens the 980ti looks like the clear best value @650 pricepoint."
hardware,3az95x,Seclorum,6,Wed Jun 24 19:01:08 2015 UTC,"yup.  honestly, while going from 30 to 60 to 120 fps is nice, what's the most noticable is when a single frame stutters for 100 milliseconds.  and that, IMO, is the true problem you're trying to eliminate when you shell out the money for a high-end card.  i wouldn't really care about going from 60 to 70 fps, but i would care if it meant going from 1 frame stutter to 0.  those get hidden from fps graphs.  And even places that include a ""minimum FPS"" graph don't really show how bad or how often it happens."
hardware,3az95x,Seclorum,1 point,Wed Jun 24 19:17:29 2015 UTC,"Yeah. HardOCP has it's graph of framerate over time as well, but the implementation here is better imho. Especially when they link up what is happening when it does stutter."
hardware,3az95x,seishi,0,Wed Jun 24 19:42:06 2015 UTC,I believe pcper.com was the first to start using frame times.
hardware,3az95x,seishi,7,Thu Jun 25 04:57:07 2015 UTC,"no, it was techreport.  they introduced it in 2011.  http://techreport.com/review/21516/inside-the-second-a-new-look-at-game-benchmarking  when pcper started it they even acknowledged TR  http://www.pcper.com/reviews/Graphics-Cards/Frame-Rating-New-Graphics-Performance-Metric"
hardware,3az95x,dhettinger,-2,Thu Jun 25 05:14:46 2015 UTC,ok
hardware,3az95x,phxline,6,Thu Jun 25 06:06:09 2015 UTC,"I like that they are willing to consider that things might improve despite coming to the same conclusion as everyone else. While I'm in the market for a new high end card I personally feel the need to wait a little longer and see if drivers are able to make any headway.   Thanks for posting this up, I had missed it while combing through all the coverage."
hardware,3az95x,WildZontar,0,Wed Jun 24 18:54:41 2015 UTC,sooo..how long are you willing to wait?
hardware,3az95x,Stoffendous,1 point,Thu Jun 25 03:18:29 2015 UTC,"Until the next generation of cards is on the horizon, at which point you might as well wait to see how those pan out."
hardware,3az95x,wolfcry0,-5,Thu Jun 25 18:35:54 2015 UTC,"Been there, done that, got the tshirt. My money goes to the fastest card on release -> Nvidia."
hardware,3axz0l,RedZerg,38,Wed Jun 24 12:01:02 2015 UTC,"Picure me ""not impressed"". Solid choice if some wants high-end card and is either die-hard AMD fan or is after the liquid cooling solution."
hardware,3axz0l,Dommy73,6,Wed Jun 24 12:29:42 2015 UTC,"or is after the liquid cooling solution.   Why would they be? Liquid cooling is just a means to an end. If it performs the same as other air coolers, why would it be any more desirable?"
hardware,3axz0l,58592825866,3,Wed Jun 24 23:20:20 2015 UTC,Kinda neat.
hardware,3axz0l,BlayneTX,1 point,Wed Jun 24 23:26:13 2015 UTC,Wouldn't it be quieter than a traditional blower style card?
hardware,3axz0l,getoutofheretaffer,2,Wed Jun 24 23:30:26 2015 UTC,Aftermarket air coolers are much quieter than blower cards too.
hardware,3axz0l,58592825866,1 point,Wed Jun 24 23:34:08 2015 UTC,"Of course, but we're taking about flagship cards."
hardware,3axz0l,getoutofheretaffer,1 point,Wed Jun 24 23:37:22 2015 UTC,Ultra silent builds?  I haven't said that it would be appealing to majority of users. I would still go with 980 Ti at this point.
hardware,3axz0l,Dommy73,2,Wed Jun 24 23:57:28 2015 UTC,But closed loop coolers are generally louder than air.
hardware,3axz0l,58592825866,1 point,Thu Jun 25 00:13:08 2015 UTC,When you leave on the stock fan then yeah.
hardware,3axz0l,Dommy73,1 point,Thu Jun 25 00:31:17 2015 UTC,Actually usually it has to do with both resonance from radiator and pump noise IMO.
hardware,3axz0l,LRed,0,Thu Jun 25 12:29:25 2015 UTC,Probably explains why the NDA was lifted on release date. :/
hardware,3axz0l,iPlayRealDotA,5,Wed Jun 24 20:10:54 2015 UTC,"No? Considering the 980TI also had reviews on the same day as release. This card is a 550 dollar card with a 100 dollar cooler. It is being put up against a card it's not really supposed to compete with. This was supposed to be the answer to the 980, not the 980 TI. When the aircooled version comes out at 550 a lot of people will be here saying how great of a deal it is. 5% performance loss for 100 dollars."
hardware,3axz0l,atriax,-18,Wed Jun 24 20:44:38 2015 UTC,The 295x2 seems better and cheaper
hardware,3axz0l,rempred,18,Wed Jun 24 14:09:05 2015 UTC,"The 295x2 has its own problems, including unoptimal Crossfire support in many games, power usage and worse frame times compared to single cards."
hardware,3axz0l,bphase,11,Wed Jun 24 14:14:45 2015 UTC,"500W power consumption, CF scaling issues in every second game, way hotter, way louder, 4GB GDDR5 vs the Fury's 4GB HBM...   And in most places it's roughly the same price.  So no, the 295X2 is not a better deal. Not by a long shot."
hardware,3axz0l,HavocInferno,-11,Wed Jun 24 14:41:10 2015 UTC,8GB
hardware,3axz0l,rempred,15,Wed Jun 24 14:58:11 2015 UTC,"Not 8GB. It's more like 2x4GB, which is really 4GB effective."
hardware,3axz0l,SeaJayCJ,14,Wed Jun 24 15:01:17 2015 UTC,4GB per GPU. VRAM doesnt stack in CFX/SLI.
hardware,3axz0l,HavocInferno,3,Wed Jun 24 15:00:35 2015 UTC,Not in my country.
hardware,3axz0l,Dommy73,-19,Wed Jun 24 14:34:37 2015 UTC,"The Fury x has way more ability to overclock than the Titan X. You can beat Titan x performance if you just overclock it, meanwhile there's no reason to OC a Titan."
hardware,3axz0l,skilliard4,6,Wed Jun 24 17:38:17 2015 UTC,980 Ti OC'd was beating overclocked Fury X too. Plus there's no way to change voltage yet.
hardware,3axz0l,Dommy73,0,Wed Jun 24 18:25:14 2015 UTC,"SHHH YOU KNOW NOTHING JON SNOW, Siriusly though the 980ti cant beat the titan and it overclocks like a beast and its better stock then the fury. AMD is not known to overclock 500+ and thats besides the fact that no one knows if AMD will unlock the voltage."
hardware,3axz0l,liquidh20,-1,Wed Jun 24 18:10:45 2015 UTC,"Um, what?  Dont post."
hardware,3axz0l,abacabbmk,23,Wed Jun 24 18:30:08 2015 UTC,"I have no idea why one would take the Fury X over the 980ti then, and that saddens me because I want competition to be tight.  The 980ti has better performance and significantly lower power use while taking up less slots in your case."
hardware,3axz0l,MumrikDK,14,Wed Jun 24 15:34:42 2015 UTC,The 980ti is the better choice you're right. All amd can hope to sell the Fury X on is brand loyalty. You also forgot the 980ti overclocks very well.
hardware,3axz0l,DHFearnot,4,Wed Jun 24 15:52:32 2015 UTC,"if you play anything below 4k resolution the fury x is a bad choice considering the other cards. If you only play at 4k, the fury x is very competitive."
hardware,3axz0l,Rotaryknight,9,Wed Jun 24 16:00:48 2015 UTC,the fury x is very competitive.   That's the point. It's competitive - not better.  http://www.techpowerup.com/reviews/AMD/R9_Fury_X/31.html
hardware,3axz0l,MumrikDK,-6,Wed Jun 24 17:27:09 2015 UTC,If it was better it wouldn't be priced at 550 dollars. Stop being ignorant.
hardware,3axz0l,atriax,1 point,Wed Jun 24 20:45:11 2015 UTC,You need a mirror and some time by yourself...  http://www.techpowerup.com/reviews/AMD/R9_Fury_X/33.html  Competitive but worse performance per dollar at all resolutions - on top of that it takes more slots in your case AND uses significantly more power.  That's the point. There's no upside to it. For the price it is slower and has more secondary expenses to it.
hardware,3axz0l,MumrikDK,-1,Thu Jun 25 02:34:07 2015 UTC,"At 4K the Fury is going to instantly hit memory bottlenecks with future games, as well as mods. It's already hitting memory bottlenecks on Dying Light, and that's on 1444p."
hardware,3axz0l,forestcollector,-20,Wed Jun 24 19:17:10 2015 UTC,The Fury X overclocks to Titan x levels. If you don't wanna OC go with the Fury for $100 cheaper.
hardware,3axz0l,skilliard4,5,Wed Jun 24 17:40:46 2015 UTC,Where? There's literally no proof for that statement.
hardware,3axz0l,ZeM3D,0,Wed Jun 24 17:58:28 2015 UTC,shut up please.
hardware,3axz0l,abacabbmk,7,Wed Jun 24 18:32:26 2015 UTC,Anyone know why (or if it's even significant) the interposer is fabbed on a 65nm process compared to the GPU being TSMC 28nm?
hardware,3axz0l,LevLev,8,Wed Jun 24 12:22:36 2015 UTC,It's cheaper and easier to do.
hardware,3axz0l,buildzoid,2,Wed Jun 24 12:39:19 2015 UTC,"Because the single largest cost in making these things is the cost per mm2 of a high-end foundry process for the gpu, and the cost per mm2 of older processes is much lower."
hardware,3axz0l,Tuna-Fish2,18,Wed Jun 24 15:56:33 2015 UTC,Pricing on par with the 980ti with less performance is going to rely on people wanting a cooler case temp via the water cooler.
hardware,3axz0l,spicypixel,7,Wed Jun 24 12:24:05 2015 UTC,"Honestly it all comes to how well Fury X is going to overclock. 980 Ti overclocks like a champ, and we have no idea how well Fury X will do."
hardware,3axz0l,ShoutingDani,5,Wed Jun 24 14:14:04 2015 UTC,Unless it overclocks 50 percent I don't see it matching an overclocked 980 ti.
hardware,3axz0l,w00t692,5,Wed Jun 24 14:37:06 2015 UTC,;/ I don't see that happening sadly.
hardware,3axz0l,ShoutingDani,1 point,Wed Jun 24 17:16:52 2015 UTC,"Well a big part of overclocking is keeping it cool, and it's rare to have a card that is this cool out of the box.   Still people need to remember temps aren't everything. My 290X is water cooled and never breaks 53C, with voltage and power increases. Still can't make it past 1180MHz stably. I could foresee instability arising from the sheer number of shaders, AMD packed a lot in and it really hurt them that they had to stick to 28nm this gen. Overall it may be a good year to sit out on the red team, last year was great with the 200 series, and next year will be much better with HBM 2 and the new fab process."
hardware,3axz0l,pb7280,1 point,Wed Jun 24 17:21:26 2015 UTC,Nobody really cares about the fury x being slightly higher in a handful of games at 4k.
hardware,3axz0l,abacabbmk,3,Thu Jun 25 13:13:11 2015 UTC,"There's so many benchmarks out there that vary quite a lot sometimes, is there some site that shows aggregate benchmarks of all the sites out there? So you could select the sites to view and it would show all of them in one graph side by side."
hardware,3axz0l,Cueball61,8,Wed Jun 24 18:31:40 2015 UTC,"If Fury is the same core, but $100 cheaper then AMD got themselves a serious 980 Ti competitor. This is a good Ti alternative if you don't want Nvidia."
hardware,3axz0l,AndreyATGB,26,Wed Jun 24 19:40:56 2015 UTC,"This is a good Ti alternative if you don't want Nvidia.   Which is stupid, because you shouldn't be buying based on childish fanboyisms.   You should be buying based off performance numbers, which show that the Fury X uses more power, performs consistently worse, and requires a bulky water-cooling unit, all while costing the same."
hardware,3axz0l,Whats_a_narwhal,34,Wed Jun 24 12:34:40 2015 UTC,"You are correct, but I'm thinking the Titan X is a preview of what it will be like without competition, and the 980 TI is what happens while there still is.  No doubt more people will be buying the 980 TI now and AMD will continue to lose marketshare. I'm actually surprised they're still around.  Is it stupid to buy a good enough product to support a struggling AMD, or is it more stupid to punish them everytime they come up a couple of frames short of a winner until they go under? Not to mention AMD supports open technologies while Nvidia always locks theirs down."
hardware,3axz0l,thesiscamper,5,Wed Jun 24 12:43:54 2015 UTC,"Well, there's not much one person can do to support a huge company sadly. I'm not gonna call it stupid, but you punishing yourself by buying an inferior product isn't going to save them. It is very disappointing really, since we definitely need more competition and less shady practises from nVidia."
hardware,3axz0l,bphase,8,Wed Jun 24 13:06:20 2015 UTC,"I actually gave up on ""saving"" them, switched to Intel 2 years ago and switched to Nvidia last month.  Anyway, the Fury X is more competitive in 4k it seems, and I hope the Fury becomes a much better choice against the 980 (also the Nano against the 970)."
hardware,3axz0l,thesiscamper,13,Wed Jun 24 13:37:27 2015 UTC,So you are playing on the PC and would like to see exclusiveness? Because that's what Nvidia is doing. Making exclusive things that fuck up competition while not even working properly on their own cards!
hardware,3axz0l,Zlojeb,15,Wed Jun 24 13:55:12 2015 UTC,"Indeed, after reading up on Freesync and G-Sync there seems to be no clear winner yet. Now I'm not sure how anyone can dismiss OpenCL, OpenGL, Mantle and all the open technologies that people can use without being tied to a brand.  edit: After checking /u/Whats_a_narwhal 's history, it has become clear that he's not the most objective person when it comes to GPUs."
hardware,3axz0l,thesiscamper,3,Wed Jun 24 13:53:19 2015 UTC,Plus he deleted the post. Must have been downvoted to the ground.
hardware,3axz0l,Zlojeb,5,Wed Jun 24 14:19:03 2015 UTC,"I'd say there are other reasons other than performance. For a start, it is the quieter card. You probably also want to pair it with an adaptive sync display and we know freesync monitors are generally cheaper."
hardware,3axz0l,Undead_Dog,6,Wed Jun 24 14:33:41 2015 UTC,"Which is stupid, because you shouldn't be buying based on childish fanboyisms.   consumer ethics is a thing.  i try to avoid Nestle because they actively work to increase world hunger.  I avoid Wal-Mart because they have shitty business practices and are a powerful force in keeping people poor and driving the economy down.  I (plan to) avoid any diamond that can be traced to deBeers because they support war and slavery.  I avoid Intel because they actively practice anti-competitive behaviour and have yet to be punished for it.  I avoid nVidia because they keep attempting to create closed platforms and consumer lock-in instead of just trying to compete with open standards."
hardware,3axz0l,willyolio,7,Wed Jun 24 18:35:55 2015 UTC,"AMD supports Freesync, so if you want a ""cheap"" monitor with variable refresh rate, you can save ~150 $ on the monitor by choosing the Fury."
hardware,3axz0l,rndnum123,9,Wed Jun 24 14:36:14 2015 UTC,"I think AMD offers more performance per value in this case (when you look at both combined, GPU and monitor)."
hardware,3axz0l,rndnum123,4,Wed Jun 24 17:56:41 2015 UTC,Did you read the same comment as me? Clearly rndnum123 was talking about a combined GPU + monitor.
hardware,3axz0l,umfk,1 point,Wed Jun 24 14:13:07 2015 UTC,"Yes, updated my comment."
hardware,3axz0l,rndnum123,1 point,Wed Jun 24 14:17:13 2015 UTC,"It performs worse at some games on the current drivers, though new drivers might not change this, we will have to wait and see. It does NOT require a watercooler, it just has one to run more silent and offer better temps for possible OC. We will have to wait how the air version (the Fury), performs when it gets released. It appears to have quite an OC potential, the current drivers are just still locking it somehow. Will have to see when they are ""unlocked"" and compare than.  I can't promise you that the OC will get better, but chances are it will AMD AMD themselves stated huge OC potential for this card IRC.  We will simply have to wait and see."
hardware,3axz0l,rndnum123,7,Wed Jun 24 14:18:33 2015 UTC,"There are many people who try to get the best performance/dollar, though you are right there are also people who have enough money to buy just the best, I don't know which group is bigger, I would guess the performance per value buyers."
hardware,3axz0l,rndnum123,1 point,Wed Jun 24 14:21:27 2015 UTC,"I don't know which group is bigger, I would guess the performance per value buyers.   There's no question about that. People who buy the top-end cards are by far a minority."
hardware,3axz0l,bphase,1 point,Wed Jun 24 14:38:26 2015 UTC,"Or I could not compromise on performance   Says the guy who dismissed the cheaper 295X that performed consistently better than the Titan X, GTFO."
hardware,3axz0l,thesiscamper,3,Wed Jun 24 14:54:05 2015 UTC,currently nvidia drivers are causing tons of problems.. just go to their sub.  It's an easy choice for the money right now.
hardware,3axz0l,imoblivioustothis,3,Wed Jun 24 15:03:03 2015 UTC,then you are in the minority. /r/nvidia has been a mess the last couple months with driver problems.
hardware,3axz0l,imoblivioustothis,3,Wed Jun 24 14:41:11 2015 UTC,"I don't want nVidia based on their business practices.  Call me stupid if you want, but I don't like their closed/proprietary model, patent trolling, etc.  I will gladly vote with my dollar on more than pure performance specs."
hardware,3axz0l,spiker611,5,Wed Jun 24 14:25:22 2015 UTC,"Which is stupid, because you shouldn't be buying based on childish fanboyisms.   Easy there. Maybe someone prefers AMD because of their software?  When it comes to software I personally prefer Nvidia, but maybe someone prefers AMD."
hardware,3axz0l,Yearlaren,4,Wed Jun 24 14:50:39 2015 UTC,"But Fury promises to be the same thing but on air for $550, which would make it a very good deal for near 980 Ti performance. The thing that baffles me is they said this card is ""an overclockers dream"" and all they could get is +100MHz? Apparently voltage control doesn't work at the moment."
hardware,3axz0l,AndreyATGB,12,Wed Jun 24 16:30:48 2015 UTC,"Fury isn't same, at least Tech Report is saying it's cut down.  Honestly, I'm very disappointed. I was hoping for Fury X to slightly beat the 980 Ti, but that's not happening, it doesn't seem to overclock well and at least Tech Report is measuring bad frametimes from it."
hardware,3axz0l,bphase,0,Wed Jun 24 14:41:53 2015 UTC,"It sounds like they chose not to increase voltage at all, not that they couldn't."
hardware,3axz0l,CarVac,1 point,Wed Jun 24 18:42:13 2015 UTC,I prefer not to feed Nvidia's anti-competitive tactics that hurt PC gaming in general. Especially since they do things like hurt their own top end cards from less than a year ago as collateral damage (780Ti was Nvidia's top end until October of last year).
hardware,3axz0l,TBradley,3,Wed Jun 24 18:47:45 2015 UTC,"It's not about childish fanboyism, it's about supporting a company that doesn't use anti-competitive business tactics.  http://www.reddit.com/r/amd/wiki/sabotage"
hardware,3axz0l,SnakeSnakeSnakeSna,2,Wed Jun 24 19:57:08 2015 UTC,"2GB less ram too, and many recent games are optimized for nVidia. Very disappointing, I was hoping for much more. Doesn't really make any sense to buy the Fury X currently."
hardware,3axz0l,bphase,0,Wed Jun 24 23:34:33 2015 UTC,"Not fanboyisms, no, but some people disagree with NVIDIA's business practices. Whether it's rightly so or not I'm not sure, but sticking with a company who's treated you right and that you trust is important to some people, important enough to stay away from competitors."
hardware,3axz0l,pb7280,3,Wed Jun 24 17:50:09 2015 UTC,Yes but the air cooled one is going to be a bit slower for the price difference.
hardware,3axz0l,bloodspore,0,Wed Jun 24 12:45:30 2015 UTC,Rumors rumors rumors rumors. Please don't say bullshit like this. You have absolutely no idea if this is true or not. The only people that do are AMD.
hardware,3axz0l,atriax,-3,Wed Jun 24 13:00:04 2015 UTC,I thought it was explicitly announced at e3?  http://wccftech.com/amd-radeon-fury-x-specs-fiji/  The specs for the fury air look to be about 10% lower than the X
hardware,3axz0l,ebonlance,11,Wed Jun 24 13:25:38 2015 UTC,wccftech    I'll believe it when anandtech confirm it.
hardware,3axz0l,TeutorixAleria,3,Fri Jun 26 20:01:23 2015 UTC,"They didn't say anything other than ""air cooled""   And WCCftech is well known garbage. Might as well link a kotaku article."
hardware,3axz0l,atriax,-2,Wed Jun 24 14:37:09 2015 UTC,"If Fury is the same core   Nope, it's going to be even weaker than the Fury X. Prepare for more disappointment. (I'm not even talking about cooling)"
hardware,3axz0l,spyder256,1 point,Wed Jun 24 13:01:09 2015 UTC,He said without proving anything.
hardware,3axz0l,Kaghuros,3,Thu Jun 25 13:16:47 2015 UTC,Props to them for not including project cars in the performance summary. Looks to be a 290X repeat but now it's equal at 4k.
hardware,3axz0l,namae_nanka,1 point,Wed Jun 24 13:19:56 2015 UTC,I feel the Fury X  is a redundant card but this shows that the Fury will br very hood at $100 cheaper then the Fury X with  a slight performance drop.
hardware,3axz0l,Eruntien,-14,Wed Jun 24 13:36:03 2015 UTC,"Got some minor issues with the conclusions.  From the ""Con"" section:   Could be much quieter in idle   I agree. Noise level is important to me. But it is not mentioned if the fan speed can be controlled, and if not, is there any way to hook it up to a fan controller or LNA?   4 GB of VRAM   Still not sold on the idea that this is a negative. Show me 1 example where a game constantly demands more than 4gb, and show me where the games performance is verifiably hindered in cases of 100% vram usage. Even if you did show me that example, that would be such an extreme edge-case.....   Lack of HDMI 2.0   HDMI 2.0 is bleeding edge..... This is literally only a negative for people who already own a 4k Television who want to game on it at >30hz, which is a very small minority. And adapters are coming. And 4k TVs with Display Port are coming, in fact I think there are some available already. And it's not impossible a 3rd party Fury will get HDMI 2.0. Why not complain about missing Thunderbolt 3.0 or USB type-c as well?   No memory overclocking   Not officially supported by AMD utilities yet, no. But they may unlock it in the future, and I think it's likely enthusiasts will find their own way to OC.   Radiator takes up extra space   It doesn't take up much more space than a standard intake/exhaust fan would. It's just thicker.   No DVI / analog VGA outputs    This is a more fair knock against it than HDMI 2.0. An extremely large amount of people actually use DVI, unlike HDMI 2.0... But again, at some point a line needs to be drawn and support for old tech needs to be cut. VGA? Really? You knock it for not having HDMI 2.0 AND knock it for not having VGA? We're criticizing extremes now and ignoring the happy middle.  In the end, I'm glad they directly listed many of the topics that were hot discussions online..... but at the same time I felt like many of the ""cons"" were unfair and over-reaching."
hardware,3axz0l,defiance158,15,Wed Jun 24 13:54:42 2015 UTC,"HDMI 2.0 is bleeding edge..... This is literally only a negative for people who already own a 4k Television who want to game on it at >30hz, which is a very small minority. And adapters are coming. And 4k TVs with Display Port are coming, in fact I think there are some available already. And it's not impossible a 3rd party Fury will get HDMI 2.0. Why not complain about missing Thunderbolt 3.0 or USB type-c as well?   The card is aimed at 4K gaming though, so not supporting a common 4K technology is worth mentioning as a con IMO. There will also never be any third-party Fury X, just as there are no third-party Titan X."
hardware,3axz0l,exscape,4,Wed Jun 24 13:59:31 2015 UTC,Note: EVGA made a 3rd party Titan X; albeit limited to the cooling element.  http://www.evga.com/articles/00935/EVGA-GeForce-GTX-TITAN-X-HYBRID/
hardware,3axz0l,dracaXL,1 point,Wed Jun 24 20:46:35 2015 UTC,i thought the pcb was reference for all titan x?
hardware,3axz0l,logged_n_2_say,2,Wed Jun 24 17:42:21 2015 UTC,"agreed.  it has hdmi, just not the newest standard that has been out long enough to implement.  also no dp 1.3.  for a top tier card, that is slightly annoying."
hardware,3axz0l,logged_n_2_say,1 point,Wed Jun 24 21:06:38 2015 UTC,Not been out long enough to implement? My 980 gtx has hdmi 2.0 and it came out October 2014.
hardware,3axz0l,w00t692,1 point,Wed Jun 24 16:51:41 2015 UTC,"i wrote it ""has been out long enough to implement."""
hardware,3axz0l,logged_n_2_say,1 point,Thu Jun 25 03:06:18 2015 UTC,"It's a pretty confusing post.  It makes it sound like you're apologizing for them not implementing hdmi 2.0 simply because it does happen to have hdmi 1.4.  If that's not what you meant i apologize, just sounded that way."
hardware,3axz0l,w00t692,1 point,Wed Jun 24 12:38:27 2015 UTC,"i meant it as, they put hdmi on it so they think it's important connection, so why not use hdmi 2.0 (which has been out a long time) which is backward compatible and the newest standard?  but i confuse myself sometimes, no biggie."
hardware,3axz0l,logged_n_2_say,8,Wed Jun 24 12:42:54 2015 UTC,"Maxwell launched 9 months ago with HDMI 2.0. In the tech industry, that is hardly bleeding edge."
hardware,3axz0l,mvnman,6,Wed Jun 24 14:01:47 2015 UTC,"Still not sold on the idea that this is a negative. Show me 1 example where a game constantly demands more than 4gb, and show me where the games performance is verifiably hindered in cases of 100% vram usage. Even if you did show me that example, that would be such an extreme edge-case.....   Have you read the few paragraphs before the pro/cons where he addressed exactly that?"
hardware,3axz0l,Mr_s3rius,1 point,Wed Jun 24 17:21:30 2015 UTC,"It sounds to me like the fan is the least of the noise problems though, compared to the pump."
hardware,3axz0l,CarVac,0,Wed Jun 24 13:18:51 2015 UTC,So the FURY X2 wait begins
hardware,3axz0l,WilllOfD,-7,Wed Jun 24 17:21:19 2015 UTC,A comma splice error in the first sentence. Tech writing needs better editors
hardware,3ay24z,My_6th_Throwaway,23,Wed Jun 24 12:36:00 2015 UTC,40% faster than the last generation 290X at lower power consumption and smaller size for that price is still very remarkable. It just can't quite take the crown from the 980 Ti.
hardware,3ay24z,barthw,8,Wed Jun 24 13:16:39 2015 UTC,"I keep seeing this repeated, but Tech Report's Scott Wasson reviewed the Fury X specifically with the ""15.5 Fury specific"" driver - see that in the comments.  If AMD had designed 15.15 for the Fury X cards, I believe they would have released that specifically to the reviewers before the NDA life. But after reading Scott's comments, the 15.15 seem more to be for the 3xx series cards."
hardware,3ay24z,terp02andrew,1 point,Wed Jun 24 18:46:56 2015 UTC,"Because he used the wrong drivers. 15.15 is for Fury X, not 15.5."
hardware,3ay24z,Hiryougan,1 point,Wed Jun 24 19:31:27 2015 UTC,"Scott already answered this, per my link:   I did use the Fury-specific Cat 15.5 with the Fury X. Sorry, AMD's labeling is a little shaky there."
hardware,3ay24z,terp02andrew,1 point,Thu Jun 25 15:48:58 2015 UTC,Is there something wrong with him? There is no such thing. 15.5 is for 2xx
hardware,3ay24z,Hiryougan,1 point,Thu Jun 25 15:52:38 2015 UTC,Haha feel free to ask Scott that question.
hardware,3ay24z,terp02andrew,-2,Thu Jun 25 15:58:09 2015 UTC,Hey is that Ryan Shrout with a Fury X to review? They give those out to whiny children now?
hardware,3ay24z,0pyrophosphate0,-5,Thu Jun 25 16:32:05 2015 UTC,My thoughts exactly. They guy whined like a baby before we even knew when the card would hit shelves. IMO very unprofessional.
hardware,3ay24z,MrBeastshaw,-8,Wed Jun 24 12:56:38 2015 UTC,Hah! I was wondering the same thing.
hardware,3ayapc,logged_n_2_say,16,Wed Jun 24 13:59:06 2015 UTC,Who cares about torture power consumption?  Unless you're mining bit coins or staring at fuzzy donut all day.  The actual difference between the 980 Ti and FX is about 250W vs 280W.
hardware,3ayapc,TaintedSquirrel,-6,Wed Jun 24 16:22:18 2015 UTC,"i'm thinking more of what it says of the future.  i was hoping with the power savings of hbm that amd would close the gap, since many outside of north america do care about power consumption, but seems pretty typical watt/performance generation shift for amd and still behind nvidia even with the benefit of hbm.  under gaming max load, they had the fury pulling 454 watt card only which is about 20w more than the reference 980ti.  better news, the average is lower on the fury for the ""gaming loop"", they just only gave us one graph with the fury.  here is the 980ti average gaming loop. (the fury is at 221w)  to me the max should tell us more about actual consumption, since people will be pushing them to get the most out of them."
hardware,3ayapc,bphase,12,Wed Jun 24 16:27:08 2015 UTC,"No, torture power consumption is useless. It doesn't tell anything about future games' usage. Besides, nVidia limits torture power usage artifically, while the Fury X has plenty of power headroom available."
hardware,3ayapc,hans_ober,-6,Wed Jun 24 17:26:02 2015 UTC,"see below it's the only comparative chart toms did for the fury x, but again the gaming max is still higher by ~20 watts than a 980ti.  and tpu's  EDIT: nvm, i found it. i'll update.  http://media.bestofmicro.com/J/Q/506150/original/32-Overview-Gaming.png"
hardware,3ayapc,Hiryougan,7,Wed Jun 24 17:30:03 2015 UTC,"More power, less performance. Sure, they've made improvements to efficiency, but Nvidia is still better without using HBM (they spoke so much about power saving ~50W).   Was hoping that the Fury X actually turned out to be ~10% faster than the 980Ti."
hardware,3ayapc,JarvisBasknight,2,Wed Jun 24 17:04:16 2015 UTC,"Oh. So it actually has lower ""typical"" power consumption than 980 Ti/Titan X."
hardware,3ayapc,cantbebothered67835,2,Thu Jun 25 14:15:46 2015 UTC,"For the love of god, please tell me this is the end of the tired argument of ""uses more power, runs hot"" when comparing sports cars."
hardware,3ayapc,playingwithfire,2,Wed Jun 24 20:28:24 2015 UTC,Runs at 50c though...
hardware,3ayapc,tbonanno,1 point,Thu Jun 25 13:39:44 2015 UTC,Somewhat related question. How well does passive Displayport to Dvi adapter work for those?
hardware,3ayapc,playingwithfire,1 point,Wed Jun 24 17:28:30 2015 UTC,They work fine. But I think you're limited to 1200p if I remember correctly.
hardware,3ayapc,tbonanno,1 point,Wed Jun 24 19:36:45 2015 UTC,I will believe it when they actually ship.
hardware,3ayapc,playingwithfire,1 point,Wed Jun 24 20:10:36 2015 UTC,Passive adapters work already. What are you waiting for Fury X's to ship for?
hardware,3b0zn9,_osprey,2,Thu Jun 25 02:28:17 2015 UTC,Both the all black model and all white model are available for $139.99 on amazon from ID Cooling. I have also been quite interested in the case and contacted ID Cooling when I was having trouble finding it for sale as well. They said they were sold out for a few weeks and linked me to their amazon page. Unfortunately $140 is a little out of my price range... Now I'm looking at the cougar qbx
hardware,3b0zn9,kodiakbear32,1 point,Thu Jun 25 05:23:39 2015 UTC,Wow the price really jumped. All the reviews list its MSRP at $79.99.
hardware,3b0zn9,TrptJim,2,Thu Jun 25 20:47:50 2015 UTC,"I know.. It's pretty disappointing, I really wanted to do a build in the case but I can't justify spending that much on the case"
hardware,3ay2ci,zmeul,9,Wed Jun 24 12:38:21 2015 UTC,"Well, this wasn't what I was hoping for.   But I agree with their conclusion, particularly around marketing. Selling a $429 card with 8GB VRAM and then selling a $649 card with 4GB VRAM is confusing. It makes me think that the only reason they included the 8GB of RAM on the 390 series is to sell it to the uninformed, because those cards are not powerful enough to use the extra VRAM. It would have been much smarter - in my opinion - to sell those cards with 4GB at a lower price and lower power consumption. It would have benefited end users, especially when the competition also has only 4GB at that price point.   HBM should have sat out a round until 2016. The tech is extremely promising but 4GB isn't enough when you have this level of performance and can actually drive high settings at >1440p."
hardware,3ay2ci,Benbulthuis,2,Wed Jun 24 15:25:17 2015 UTC,"The only cases where the Fury X can actually beat the Ti is high resolutions, mainly 4K though. So that seems a bit strange. Maybe it's only games where 4GB is still enough to handle the load in 4K and the GPU can really work it's magic without bottlenecks."
hardware,3ay2ci,barthw,3,Wed Jun 24 16:32:21 2015 UTC,"I feel like there is a disconnect between the shader design, the memory design, and the intent of this product. Sticking with 64 ROPs seems like a mistake."
hardware,3ay2ci,Benbulthuis,8,Wed Jun 24 16:38:36 2015 UTC,"http://www.hardocp.com/images/articles/14351085919S0HOOZkGA_6_2.gif   Before we begin looking at this game it should be noted that this game definitely can exceed 4GB of VRAM at 1440p. We saw up to 5GB of usage when the VRAM capacity was there to support it. This game is bottlenecked on every 4GB video card at 1440p with maximum in-game settings.  The AMD Radeon R9 290X always struggled in this video game from day one. We were never able to achieve a playable setting using the highest in-game settings. Instead we had to use the built in preset of ""Best Quality"" which sets the shadow map size to ""High"" and the view distance to ""Half.""  The AMD Radeon R9 Fury X is the first video card from AMD to allow us to play at the highest in-game settings at 1440p. We were able to turn up the shadow map size to ""Very High"" and max out the view distance. The game was playable, but you will notice it does have a lot lower minimum framerate on the Fury X compared to the GTX 980 Ti. There was some stutter here and there as it loaded new game assets in different areas, which did not occur on the GTX 980 Ti.  The GeForce GTX 980 Ti was also playable at the highest in-game settings but was 42% faster than the AMD Radeon R9 Fury X. We think the difference is so great because the AMD Radeon R9 Fury X is being held back in total performance due to not having enough VRAM for the video card to accelerate the game at its fastest.  Let's elaborate a bit more on the point above. We can back this up by looking at the total system wattage as we played the game at both ""Maximum"" settings and at ""Best Quality"" settings which lessens the burden on VRAM capacity.  With the game running at ""Maximum"" settings on the AMD Radeon R9 Fury X we saw the system wattage only go up to the 340W range, and the performance was a lot lower as we see. When we bumped the game down to ""Best Quality"" settings the wattage actually increased up to 365W because the burden on VRAM capacity was lessened and the GPU was allowed to work harder.  The VRAM bottleneck is like a cap on performance, keeping the true performance of the GPU from shining through. This game is a shining example of how VRAM capacity bottlenecks can lower performance and power demand. Sorry AMD, 4GB isn't enough in this game at 1440p, what do you think is going to happen at 4K?   http://www.hardocp.com/images/articles/14351085919S0HOOZkGA_6_3.gif"
hardware,3ay2ci,fullofbones,10,Wed Jun 24 12:47:39 2015 UTC,"This part of the article is definitely the main point, for me. It's basically proof that 4GB isn't enough for demanding games with lots of textures. And these are current games, so this card is hardly future proof for the price.  Bad optimization or not, a Ti won't have this problem. I waited to give AMD the benefit of the doubt, but I think I've made my decision."
hardware,3ay2ci,forestcollector,3,Wed Jun 24 14:11:13 2015 UTC,"Right. That's just today. Texture sizes will increase down the road just as they always do. This was not future proofed whatsoever, which is completely ridiculous for a flagship GPU."
hardware,3ay2ci,Hiryougan,1 point,Wed Jun 24 18:58:54 2015 UTC,Too bad not all game devs optimize their games like CDPRED did with their Witcher 3. Welp.
hardware,3ay2ci,barthw,3,Thu Jun 25 07:49:54 2015 UTC,"Seems like HBM does not really bring a lot of real world performance to the table then as it's not even faster on games where there is no bottleneck. Generally though, the gap to the TI seems to be smaller in higher resolutions, even getting beaten by the Fury X in some 4K benchmarks."
hardware,3ay2ci,Darkstryke,3,Wed Jun 24 16:28:37 2015 UTC,"The apologists are already trotting out the 'well the real drivers don't come out till next month for the fury x' line.  Even if that's true, heh.. ""here's our flagship, best GPU, but it won't work properly for another month before our software is ready. We decided to release early to get a bunch of non-paid press to call our new baby 'average' instead of releasing it complete.""  I don't know how logic works in those minds, it's the same dementia that was making the rounds during the Fermi days on the green camp."
hardware,3ay2ci,Seclorum,1 point,Wed Jun 24 19:59:32 2015 UTC,It also seems like there is a serious driver bottleneck at lower resolutions.
hardware,3ay2ci,terp02andrew,2,Wed Jun 24 18:13:45 2015 UTC,"This was precisely what I was looking for, after [H] published memory usage figures from their 980Ti in-depth review.  Furthermore, it makes Macri's words to TR in May ring far less true - and quite hollow frankly. He made a big gesture about how memory efficiency was not a concern with modern GPUs and that it would now be addressed by AMD engineers in their implementation of HBM.   It is immediately clear that this is not something you can address with drivers :P"
hardware,3ay2ci,Darkstryke,5,Wed Jun 24 20:19:54 2015 UTC,Waiting for the boys to come here with pitch forks because someone's actually using the card and not just running time demos.
hardware,3ay2ci,klymen,4,Wed Jun 24 14:41:37 2015 UTC,"I'm disappointed. If this was priced lower as mentioned in the article, it would have been received in a much better way."
hardware,3ay2ci,Yearlaren,1 point,Wed Jun 24 15:08:21 2015 UTC,"The problem is that AMD said that they don't want to be ""the cheaper solution"". They needed the Fury X to be significantly faster than the GTX 980Ti, but both cards trade blows, and the Fury X uses more power, so they needed to give the card liquid cooling to make it cooler than the GTX 980Ti. That's the reason it doesn't overclock very well: it's power consumption is already quite high."
hardware,3ay2ci,Seclorum,2,Wed Jun 24 17:08:05 2015 UTC,We currently cannot touch voltages. The 3rd party tools are not up to date yet.
hardware,3ay2ci,klymen,1 point,Wed Jun 24 18:08:34 2015 UTC,"Agreed. I honestly didn't think about the liquid cooling aspect. I see it as a pro but now that you shed light in a different way, it furthers my disappointment. I really wanted them to win. I guess we will wait until later this year to see how it plays out."
hardware,3ay2ci,TaintedSquirrel,5,Wed Jun 24 18:59:59 2015 UTC,"Zmeul is gonna cash in on that anti-AMD karma this week, for sure.  Cha ching."
hardware,3ay2ci,Trollatopoulous,7,Wed Jun 24 15:15:29 2015 UTC,"I still expect my cigar from the 3xx series ;)  don't like to do it, but .. told you!    nVidia scratched their balls, cut some bits and pieces from the Titan X and sold it as 980Ti - and it does a better job just air cooled than Fury X  it's mind blowing that AMD's muscle flexing got them only this far - down the road, this might be bad for PC gaming    just wait for 2016 when ""if"" nVidia releases Pascal GPUs for desktop - maybe early 2017"
hardware,3ay2ci,Seclorum,1 point,Wed Jun 24 15:22:37 2015 UTC,Considering how many downvotes he usually gets it at least evens out lol.
hardware,3ay2ci,Seclorum,1 point,Wed Jun 24 21:19:37 2015 UTC,"There is a strange performance quirk I keep seeing in all these different benchmarks from different sites.  4k gaming.   @1080p or 1440p, the Fury X seems like it has bottlenecks somewhere, im betting drivers. But crank it to 4k with settings up and it doesn't take the massive performance drop that we would expect for a card hitting the 4gb vram wall."
hardware,3ay2ci,Seclorum,1 point,Wed Jun 24 15:47:31 2015 UTC,"one simple explanation: Fiji XT has a lot of shader cores to push those calculations just fast enough (or slow enough, depending how you look at it) so that system RAM has time to swap textures in VRAM - at UHD resolution"
hardware,3ay2ci,SimonReach,1 point,Wed Jun 24 16:11:32 2015 UTC,"So then why does the 980ti and Titan-X have more of a performance delta hit going to 4k, and in some cases so significantly that the Fury-X equals them in performance at that resolution.   Shouldn't their greater Vram pools allow them much greater performance at those resolutions?"
hardware,3ay2ci,Exist50,2,Wed Jun 24 16:20:45 2015 UTC,"because I don't think it's about the VRAM capacity, just yet - we still don't have games tailored specifically for UHD gaming, with UHD textures  quite interested in what's gonna happen when playing Mordor with the UHD texture pack - I haven't seen a test with this, or I missed it; one other game that uses a lot of VRAM, CoD: AW  look at the min FPS: http://cdn.overclock.net/7/75/900x900px-LL-75154006_advance.png"
hardware,3ay2ci,bentan77,0,Wed Jun 24 16:28:09 2015 UTC,"Yeah gotta love that image where it's one of the only benchmarks they decide to randomly toss some SLI configs to inflate how many configs are above the Fury.   But it goes to show that most games out there are not exceeding the 4gb vram limit, and it takes very specific settings to approach that limit."
hardware,3ay2ci,Seclorum,-4,Wed Jun 24 17:48:30 2015 UTC,"Just had a brief look over a few of the reviews and HardOCP seems to be so much different than anyone else's.  Everyone has the Fury beating the 980Ti whereas HardOCP, the FuryX looks very disappointing.  Edit: seems quite a few review sites all agree with HardOCP so ignore me.  Not upgrading till September so will see how things improve till then, if not, 980Ti for me."
hardware,3ay2ci,Olissipo,7,Wed Jun 24 12:52:58 2015 UTC,"you should read them again, OC3D's TechPowerUP shows the Fury X trading blows with a reference 980Ti and getting stomped by the custom ones - Fury X is no winner  overcomplicated design, 4GB of HBM1 .. for what!??! even the power draw raises some serious eyebrows: http://www.overclock3d.net/gfx/articles/2015/06/23174943248l.jpg    100Mhz overclock on the core: https://www.techpowerup.com/reviews/AMD/R9_Fury_X/34.html WTF?!? that's it? why AMD presented this card as an overclocker's dream  and the really big problem: no custom designs for Fury X - what you see today is what you get"
hardware,3ay2ci,Olissipo,2,Wed Jun 24 13:07:21 2015 UTC,"Some sites show the power draw as lower than a 980ti. Also, I hardly can believe that 100mhz is the max overclock. Looks like it needs more voltage"
hardware,3ay2ci,w00t692,2,Wed Jun 24 13:39:02 2015 UTC,"AMD presented this card as an overclocker's dream   I'm hoping it's just a driver lock that gets removed later, but then again most AMD cards don't tend to overclock well, especially when compared to Maxwell based cards."
hardware,3axy0r,AverageNinja,20,Wed Jun 24 11:49:45 2015 UTC,Was this card this suppose to be the 980ti killer?
hardware,3axy0r,bbgun83,16,Wed Jun 24 12:16:24 2015 UTC,A lot of us hoped it would be.
hardware,3axy0r,lukeroge,21,Wed Jun 24 12:40:26 2015 UTC,"No, it was supposed to be AMD's way back into the market as a contender for enthusiasts."
hardware,3axy0r,thexsa,-2,Wed Jun 24 12:18:18 2015 UTC,"And it doesn't overclock (100MHz at most from the reviews I've read), really disappointed!"
hardware,3axy0r,Tancoll,-1,Wed Jun 24 13:57:25 2015 UTC,AMD isn't allowing OverClocks on it right now since according to them HBM is still too new.
hardware,3axy0r,FreddyFuego,8,Wed Jun 24 13:59:54 2015 UTC,"The memory is probably locked, but the voltage should get unlocked today"
hardware,3axy0r,RUGDelverOP,-1,Wed Jun 24 14:04:05 2015 UTC,"And there is probably a reason behind this decision.  I don't like to speculate, but personally I think they have an issue with stability at higher clocks."
hardware,3axy0r,Tancoll,0,Wed Jun 24 14:04:42 2015 UTC,I doubt theirs much benefit from OC the memory anyway. Haven't really seen much in GDDR5 cards so far.
hardware,3axy0r,spencer32320,1 point,Wed Jun 24 18:21:48 2015 UTC,I hoped it would match the Titan X... :( I thought the Fury (non X) would be similar to a 980Ti.
hardware,3axy0r,redditwentdownhill,33,Wed Jun 24 23:21:40 2015 UTC,"Their charts are soooo fuckin painful to read.  Edit: Also no synthetic benchmarks and it feels like I've read ""big improvement"" 50times in this article. Overall pretty bad review."
hardware,3axy0r,Cravem,6,Wed Jun 24 11:59:50 2015 UTC,I was wondering why it was at the bottom until I actually looked at the numbers.
hardware,3axy0r,Flabbyflamingo,5,Wed Jun 24 12:01:14 2015 UTC,Yea they put TEN 980s in there... to make the fury near the bottom?
hardware,3axy0r,rempred,1 point,Wed Jun 24 14:15:16 2015 UTC,I noticed that too. Seems to happen with oc3d
hardware,3axy0r,TehRoot,3,Wed Jun 24 12:01:00 2015 UTC,"It feels like I'm just reading a transcript of whoever does the videos. There's a lot of repetition, or at least seems to be."
hardware,3axy0r,TheRealGecko,12,Wed Jun 24 13:20:49 2015 UTC,I was expecting a bit better. It's a good card but no homerun imo! That being said I shouldn't forget that it's competing against a just as awesome card(s).  Basically both companies have achieved the results.
hardware,3axy0r,Schmich,41,Wed Jun 24 12:37:44 2015 UTC,So the 980ti is stellar right?
hardware,3axy0r,jpwns93,25,Wed Jun 24 12:12:15 2015 UTC,Stellarer.
hardware,3axy0r,BlayneTX,1 point,Wed Jun 24 12:12:34 2015 UTC,Stelara?
hardware,3axy0r,bad-r0bot,2,Wed Jun 24 14:40:36 2015 UTC,STELLLLAAA
hardware,3axy0r,namae_nanka,5,Wed Jun 24 16:18:28 2015 UTC,"Yep... The aftermarket ones are like 10-15% better for the same price. When you factor in the water cooling hassle, it is not that good of a card... I still don't know what to do... Might just endure that 670 for a while"
hardware,3axy0r,ElDubardo,1 point,Wed Jun 24 12:35:18 2015 UTC,Water cooling is far better at that small factor dude wtf is wrong with you. You don't like keeping your gpu's and cpu's heat separate?
hardware,3axy0r,pabloe168,1 point,Wed Jun 24 12:46:50 2015 UTC,"You still have fan noise from the radiator.  You also have to deal with the pipes and mounting that thick ass radiator somewhere in your case.  Many builders are already using a 120-240mm radiator for the CPU which doesn't leave mush room for running the GPU radiator.  It would be nice as an option, but as it is required right now it is a drawback."
hardware,3axy0r,Thunder_Bastard,-9,Wed Jun 24 14:33:45 2015 UTC,"Have you heard how much the Fury X sounds? The pump is loud (test examples) and coilwhine. A Fury X at idle sounds like a 980Ti at full load.  Sure the watercooling is nice, but for us that uses a custom loop it doesn't matter at all. Rather get something that is easier to remove then a pre-installed waterblock.  Edit, getting downvoted for telling the truth. As usual.. Typical reddit -.-"
hardware,3axy0r,Tancoll,8,Wed Jun 24 13:55:45 2015 UTC,I think you're getting downvoted for saying a Fury X at idle sounds like a 980Ti at load which is just objectively wrong no matter the performance of the two cards...  http://www.techpowerup.com/reviews/AMD/R9_Fury_X/30.html
hardware,3axy0r,DowneySyndromey,2,Wed Jun 24 14:41:12 2015 UTC,"reference 980 ti not so much, but aftermarket yes.  this review also has actual recordings of idle and peak, against a 980 ti reference.  http://www.sweclockers.com/test/20730-amd-radeon-r9-fury-x/18#content"
hardware,3axy0r,logged_n_2_say,0,Wed Jun 24 15:31:41 2015 UTC,"Here, listen for your self.  You cant tell how loud it is, but what type of sound it makes and how irritating it is...  http://www.sweclockers.com/test/20730-amd-radeon-r9-fury-x/18#content  I rather take a 980Ti at load then a Fury X at idle with that sound..."
hardware,3axy0r,Tancoll,-1,Wed Jun 24 17:30:53 2015 UTC,"There is some hardcore AMD circlejerking going on lately in this sub, they have basically taken it over.  Every post that comes up about Nvidia or about both AMD/Nvidia they will trash it.  Any Nvidia positive posts or negative AMD posts will be downvoted.  It happened so suddenly and so effectively it almost looks like a brigade of hired social media spammers are doing it on purpose.  It wouldn't surprise me considering AMD has stopped sending review cards out to any site that ever said Nvidia makes a better card than AMD....  they are trying to forcibly manage their image."
hardware,3axy0r,Thunder_Bastard,3,Wed Jun 24 14:36:56 2015 UTC,Yep I have noticed this too
hardware,3axy0r,dracaXL,-2,Wed Jun 24 15:24:56 2015 UTC,"Yeah... notice how the kid saying ""WTF is wrong with you"" regarding a required rad on a video being a good thing gets upvotes... yet posts explaining why it is bad are all getting downvoted.  Think it is just a small group of people with multiple accounts nailing each post."
hardware,3axy0r,Thunder_Bastard,3,Wed Jun 24 15:36:39 2015 UTC,Uhhh maybe because liquid cooling is pretty much the best way to cool your components and having it included is a HUGE feature. It allows for super cool temps WHILE keeping all of the heat outside of the case and not being vented into the case heating up the other parts.
hardware,3axy0r,Teethpasta,2,Wed Jun 24 19:03:39 2015 UTC,"It is a HUGE feature if you have room to mount it.  Standard cases typically will not be able to mount it along with a CPU closed loop system because it just won't fit.  Even if it does it creates another point of failure.  You don't control the quality of the pump or loop, you just get what AMD gives you.  You also can't change it because of the custom copper pipe running inside of it.  There are also plenty of setups where this simply will not fit, like ITX systems made to fit a full air cooled card.  Maybe as a custom add-on it would be ok, but being required on this top-end card is kind of dumb.  What it says to me is that the chip is running at the utmost extremes and couldn't be air cooled...  meaning you aren't going to squeeze any more out of it.  Plus the fact that if the pump fails then entire card can go...  if fans fail on a normal air cooler they still have passive cooling.  Nvidia aftermarket parts have had simple add-on water cooling solutions for a while now.  Buy a plate, select your choice of cooler and add it in.  It has NOT been popular, meaning people are not going after water cooling for a GPU.  But again no one is going to listen because they are far too busy circlejerking each other and downvoting anyone that doesn't think this is the coolest thing ever (even though you can do this on multiple cards for years)."
hardware,3axy0r,Thunder_Bastard,2,Thu Jun 25 00:13:12 2015 UTC,"It's there because AMD went waaaay overboard on their shader counts. You see that 4096 shader count? That's around 50% more than the 290X shader count, which we all remember as a train wreck with reference cooling. The main problem is Fury and HBM were designed to run on a fabrication process lower than 28nm, in fact it was supposed to be 20nm. Their chip company cancelled 20nm and announced 16nm for later this year, so AMD was forced to rework their architecture to get it to run on 28nm. This means it produces much more heat than it was originally designed too, it's really a miracle they were able to do it at that TDP. Maybe since HBM uses less power."
hardware,3axy0r,pb7280,1 point,Thu Jun 25 13:46:48 2015 UTC,Good thing  AMD has the regular fury for people who don't want this feature. Maybe it hasn't been popular because the work wasn't done for you like AMD has done. Because people aren't morons and recognize how great this is and how small it allows the card to be
hardware,3axy0r,Teethpasta,0,Thu Jun 25 03:16:44 2015 UTC,"Well, I kinda expected Fury X not to be as fast as most people thought.   Heard some rumors, read some stuff and guess the early numbers about 5 - 10% behind Titan X was right.  Its sad that people downvote because it doesn't fit their opinion rather then keep it to facts.  The fact is that Fury X got a really annoying sound, at least the review example Sweclockers got. You can listen to it here if you want.  http://www.sweclockers.com/test/20730-amd-radeon-r9-fury-x/18#content  Don't take the volume for real, but what type of sound Fury X does. For me its really annoying and disturbing."
hardware,3axy0r,Tancoll,1 point,Wed Jun 24 17:35:42 2015 UTC,Yeah if I were you I'd just hold out until 16nm and hmb 2.0 and get a massive performance boost as well as full dx12 and whatever else new tech is coming out.  The 670 is still a good card.
hardware,3axy0r,TheRealRacker,2,Wed Jun 24 19:19:00 2015 UTC,Im thinking about a 970 for the next 3 years could also be a good move.
hardware,3axy0r,ElDubardo,1 point,Wed Jun 24 19:46:13 2015 UTC,"It only has 3.5gb of vram though(not circlejerking here), and in 2 years I could see that being a big problem, even on 1080p and especially on 1440p+.  And higher resolutions are becoming more and more common, and those need a lot of GPU horsepower as well as vram, and 28nm is end of life.  I personally think that waiting until Pascal would be the smart move and that it'll have a lot more long term staying power than a 970.  That's just me though, I have a 280x now and nothing new out now impresses me much, especially now with the Fury X being so mediocre.  But if you want a 970 go ahead, it's a great card for current games but I can't help but feel like it'll be crap in comparison to 16nm HBM 2.0 cards that'll be coming out in a year."
hardware,3axy0r,TheRealRacker,2,Wed Jun 24 19:53:14 2015 UTC,"I don't think the 3,5 will be a issue for me. And I think the 1440p+ is not ready in GPUs offers. Im heading for an ultrawide setup in the future and right now i thinks its too soon. HBM is too soon as well."
hardware,3axy0r,ElDubardo,1 point,Wed Jun 24 20:01:08 2015 UTC,"All of your issues about graphics cards not being ready or it being ""too soon"" for something are fixed with waiting until the second generation of HMB and AMD's and Nvidia's new architectures coming out in 2016."
hardware,3axy0r,TheRealRacker,1 point,Wed Jun 24 20:10:00 2015 UTC,Yup
hardware,3axy0r,ElDubardo,1 point,Wed Jun 24 20:42:52 2015 UTC,Might just endure that 670 for a while   I would say this is wise unless you can afford to lay out another £300 in a year's time. I've been saying for a while that the architecture of the consoles will lead to minimum requirements of 4GB of VRAM on PC and necessitate 8GB for high settings and resolutions. Of course I was usually downvoted because wishful thinking (that devs will build magical ports of these games that only require 2GB of VRAM) trumps cruel reality. So I am holding off until the next round of nVidia cards and will aim to get hold of something affordable with at least 6GB of VRAM as that should last 2-3 years.
hardware,3axy0r,LazyGit,2,Thu Jun 25 13:36:15 2015 UTC,"Except consoles don't have the full 8gb. Don't remember how much is left, but it's more then Windows... So i think 6gb is the new norm. Also consoles ONLY has 8gb as for pc we have RAM and VRAM so again, 6gb should be enough for this gen."
hardware,3axy0r,ElDubardo,1 point,Thu Jun 25 13:42:00 2015 UTC,"They don't have the full 8GB but they aren't going to be running games at 1440p or UHD either. So if a game needs 4GB at 1080p just to run, you're probably going to need more for QHD and UHD."
hardware,3axy0r,LazyGit,1 point,Thu Jun 25 13:48:51 2015 UTC,Right... Forgot about that one... lol damn consoles...
hardware,3axy0r,ElDubardo,2,Thu Jun 25 14:32:40 2015 UTC,"When voltage controls are updated it'll overclock past the 980ti. I mean, it runs at 50° under load, the head room is enormous."
hardware,3axy0r,Dragon_Fisting,5,Wed Jun 24 13:23:33 2015 UTC,"Overclocking is rarely limited by temps. Look at the 980ti for example, you dont get more that 1550 core, No matter what cooler you use. You can get a 1500 core on a nice air cooler on the ti. Same performance, only difference is the water cooled card will sit at nicer temps."
hardware,3axy0r,steinarsen44,1 point,Wed Jun 24 13:34:43 2015 UTC,At this temp aio watercooled 980ti is going on 1500Mhz lol (JayzTwocents review)
hardware,3axy0r,3HunnaBurritos,1 point,Wed Jun 24 18:54:11 2015 UTC,Temps are not going to play a big factor in having bigger overclock capability against anything but reference cards as aftermarket coolers will be cool regardless of the OC they're able to achieve without crashing.  I'm running 1520 MHz on my 980 Ti G1 and it's not going above 65 celcius.
hardware,3axy0r,YinKuza,-10,Wed Jun 24 13:39:20 2015 UTC,Lol watercooling hassle. Maybe if you are retarded.
hardware,3axy0r,wwoodi,3,Wed Jun 24 12:43:11 2015 UTC,"My 150$ h220 pump died in less then a year. Also the fluids in it was pretty full of particle and really had to be cleaned up before that year. Every 6 month I'd says. For me, it is a big hassle, considering it's embed on the card."
hardware,3axy0r,ElDubardo,3,Wed Jun 24 12:49:00 2015 UTC,"one of my $60 air coolers failed but my h100i has been going strong for two years.  there, my meaningless sample of 1 anecdotal experience cancels out your meanings sample of 1 anecdotal experience"
hardware,3axy0r,Hariooo,1 point,Wed Jun 24 14:30:22 2015 UTC,My $100 H100i has been working 24/7 for 3 years.
hardware,3axy0r,Stingray88,1 point,Wed Jun 24 20:02:30 2015 UTC,"Good, still my experience with water is not overwhelming like everyone says it is. I saw barely any pro's with my h220. As i said, it was a hassle I'm glad I got rid off."
hardware,3axy0r,ElDubardo,-5,Wed Jun 24 20:19:27 2015 UTC,If it failed you will get a free replacement. The same as if your GPU failed. I run a custom loop and i've never had to clean it out. I only had one pump fail and thats when i tried to cheap out on the AIO units like yourself.  The amount of fanboys in /r/hardware is really high :(
hardware,3axy0r,wwoodi,21,Wed Jun 24 12:53:30 2015 UTC,Am I missing something? The 980 Ti beats it most of the time; unless driver optimization manages to squeeze more out of the card I'd go with NV were I in the marked for a $650 card.
hardware,3axy0r,Reporting4Booty,2,Wed Jun 24 12:19:12 2015 UTC,"Other reviews are confirming hte same thing. You're not missing anything. AMD lost this time around. The biggest issue is the 4 gigs of RAM. HardOCP showed the Fury X chokcing in Dying Light at 1440p. Not only is this card slower, but it's also way slower at any game that goes beyond 4 gigs of VRAM, and we will be seeing more of those as time goes on."
hardware,3axy0r,forestcollector,2,Wed Jun 24 19:04:58 2015 UTC,And yet AMD refused to send samples to a few sites for being to harsh with their products... I really start to wonder what this card is worth.
hardware,3axy0r,u-r-silly,77,Thu Jun 25 16:28:43 2015 UTC,"""Stellar"".   Not really, no. It's worse than a 980 Ti, requires a water-cooler, uses more power, and doesn't overclock well at all."
hardware,3axy0r,Whats_a_narwhal,19,Wed Jun 24 12:35:00 2015 UTC,"Yeah, what the hell?  Averaging 10 fps less than a 980 Ti on 1080p, but gets close/matches a 980 Ti at 4K.  Runs cooler, at least.  And then you have project Cars. 30 FPS lead by the 980 Ti over the Fury X at 1080p."
hardware,3axy0r,Colorfag,5,Wed Jun 24 12:57:03 2015 UTC,No cards should be compared at the 4k benchmark unless you can average 60fps. No one wants to play at 4k 30 fps with micro-stuttering.
hardware,3axy0r,LiberDeOpp,7,Wed Jun 24 16:07:48 2015 UTC,"I disagree. If you don't push the cards enough you risk running into CPU bottlenecks. There's a reason benchmarks almost always use an i7. If you used a Pentium all video cards would show the same performance, just like if you test the cards at 1024x768 with a good CPU. At 1080p and 1440p you will not see the same performance across the board but you also won't see the differences between cards that you'd see at 4K."
hardware,3axy0r,Yearlaren,-1,Wed Jun 24 16:56:17 2015 UTC,"The CPU has little to do with resolution changes. Resolution is directly impacted by video memory performance and thus the Fury x has an  advantage. However, the Fury x GPU cannot process 4k games well enough to make them playable."
hardware,3axy0r,LiberDeOpp,1 point,Wed Jun 24 17:04:07 2015 UTC,"The CPU has little to do with resolution changes.   Exactly, that's why testing the cards at low resolutions means that you will be bottlenecked by the CPU."
hardware,3axy0r,Yearlaren,3,Wed Jun 24 18:06:24 2015 UTC,If they were bottle necked they would show the same fps regardless of the gpu.
hardware,3axy0r,LiberDeOpp,2,Wed Jun 24 18:09:04 2015 UTC,"Yes. That's what I said on my first post: if you test all the cards at a low resolution, you will get that all the video cards have the same performance."
hardware,3axy0r,Yearlaren,1 point,Wed Jun 24 18:17:04 2015 UTC,"I don't want to play with microstuttering but dips into the 30s are not a problem, especially if you have Freesync (though I can't remember if it actually goes that low).  It's absolutely lusicrous that people are pretending that sub 60 fps is 'unplayable'. 'Less than ideal' sure, but not unplayable."
hardware,3axy0r,LazyGit,2,Thu Jun 25 14:04:04 2015 UTC,That's how previous AMD cards have behaved too.  The 290X was ~15% behind the 980 non-Ti at 1080p but got close/tied at 4K.  I'd really hoped that they would solve that behavior with the Fury X but I guess that was too much to hope for given that it's based on a GCN architecture.  Even if they did step up to GCN 1.2.
hardware,3axy0r,capn_hector,1 point,Wed Jun 24 13:35:30 2015 UTC,"It's because AMD has had more memory bandwidth, which becomes more crucial at higher resolutions. It's not that the AMD cards perform worse than they should at 1080p, it's that NVIDIA cards perform worse than they should at UHD. Overall the AMD cards perform lower but surpass the NVIDIA ones in memory intensive situations."
hardware,3axy0r,pb7280,-1,Thu Jun 25 13:29:01 2015 UTC,Runs cooler on a water cooler... Imagine how how it would be with a standard air cooler.
hardware,3axy0r,MichaelArnold,8,Wed Jun 24 13:45:05 2015 UTC,"I think its voltage locked, so there is that. It will be unlocked soon I think. However, the question is if its going to match 980 Ti OC ability."
hardware,3axy0r,ShoutingDani,5,Wed Jun 24 13:01:17 2015 UTC,"doesn't overclock well at all   I am very disappointed by that. It seems that even watercooled, most of the reviewers are stuck at <1155Mhz (+10%). We should have most info about the OC capability while overvolted later but I'm hesitant."
hardware,3axy0r,benb4ss,11,Wed Jun 24 12:53:32 2015 UTC,"There's no voltage tweaks available, AMD has locked it.  You'll be waiting at least a week for a community mod or for AMD to unlock it."
hardware,3axy0r,Le_rebbit_account,4,Wed Jun 24 13:15:59 2015 UTC,"What? It was supposed to be an ""overclocker's dream""."
hardware,3axy0r,benb4ss,9,Wed Jun 24 13:20:27 2015 UTC,"All their cards come with locked voltage at launch, there's always an update shortly after that unlocks it."
hardware,3axy0r,s4in7,7,Wed Jun 24 13:49:41 2015 UTC,why not directly at launch?
hardware,3axy0r,HavocInferno,8,Wed Jun 24 14:43:51 2015 UTC,Good question.
hardware,3axy0r,s4in7,2,Wed Jun 24 15:05:43 2015 UTC,"doesn't overclock well at all.   Do you have anything to back this up? I have not seen any of the reviews talk about overclocking performance.  I mean it theory it should be a great overclocker, since the temps are so low."
hardware,3axy0r,sebofdoom,4,Wed Jun 24 13:00:52 2015 UTC,"Locked voltage, all the reviewers barely got 10% out of it."
hardware,3axy0r,Le_rebbit_account,1 point,Wed Jun 24 13:16:38 2015 UTC,They will hopefully unlock that soon.
hardware,3axy0r,Eruntien,1 point,Thu Jun 25 03:14:21 2015 UTC,"Yeah, I can see that being a problem.  Hopefully someone will make a bios with unlocked voltage, that can be flashed on it."
hardware,3axy0r,sebofdoom,2,Wed Jun 24 13:21:25 2015 UTC,"There's so many shaders on the chip, that'll be the problem. Temps aren't everything, and AMD was really putting their eggs into the 20nm basket, as opposed to the current 28nm they were forced to work with. Packing 4096 shaders was no easy task, and instability arises from that (leading to less overclocks). The chip company that supplies both AMD and NVIDIA had a problem with 20nm, but they're releasing 16nm later this year which will unlock the potential for what Fury was supposed to be."
hardware,3axy0r,pb7280,0,Thu Jun 25 13:33:55 2015 UTC,Requires a cooler? No it includes one as a BONUS. That's a plus. And you don't know how well it will over clock no one has control over the voltage yet.
hardware,3axy0r,Teethpasta,1 point,Wed Jun 24 19:06:45 2015 UTC,It's a radiator. That's the point. That's a good thing. It keeps it cool.
hardware,3axy0r,Teethpasta,4,Wed Jun 24 19:34:43 2015 UTC,"Probably because the 980 Ti wasn't expected to be that good. It being almost as fast as the Titan X was a surprise.  The Fury, on the other hand, was hyped to heaven by AMD and people alike.  Now we can see that its performance is good but somewhat inconsistent. It fell short of the expectations."
hardware,3axy0r,Mr_s3rius,20,Wed Jun 24 20:19:01 2015 UTC,I'm honestly sort of disappointed.  Not saying it isn't a great card; it is! And if you like the prospect of a water cooled card then I'd say it's for you; but I was hoping for more of a smack down than just being very close to 980Ti. That coupled with no HDMI 2.0 sorta killed it for me personally. I sorta missed AMD xD
hardware,3axy0r,marcoalexander,12,Wed Jun 24 14:12:18 2015 UTC,"No DVI ports, as well. That renders 2 of my monitors useless"
hardware,3axy0r,InfiniteZr0,2,Wed Jun 24 14:18:18 2015 UTC,DP adapts back down to anything old with a 5 dollar adapter.
hardware,3axy0r,squigiliwams,10,Wed Jun 24 14:14:26 2015 UTC,Not at higher resolutions or refresh rates.  You have to pay $70+ for one and even then you can only achieve ~90fps max at 1440.
hardware,3axy0r,seishi,3,Wed Jun 24 12:02:16 2015 UTC,Only at 1080p 60hz  If you want 1440p 120hz or something you're screwed unless you shell out $100+ for an adapter
hardware,3axy0r,wolfcry0,2,Wed Jun 24 12:21:39 2015 UTC,Understood! My bad.
hardware,3axy0r,squigiliwams,1 point,Wed Jun 24 12:34:42 2015 UTC,Do high resolution monitors come with DVI?
hardware,3axy0r,Yearlaren,2,Wed Jun 24 13:01:22 2015 UTC,"Most do, yes  The reason is it can handle 1440p at higher refresh rates like 96hz or 120hz"
hardware,3axy0r,wolfcry0,1 point,Wed Jun 24 18:28:09 2015 UTC,This seems to be a bigger issue than AMD anticipated; a lot of people needed these ports.
hardware,3axy0r,marcoalexander,1 point,Wed Jun 24 18:47:10 2015 UTC,"I don't get why HDMI is an issue now? Displayport is far superior  EDIT: The Sapphire Fury X has an HDMI output, whether it's 2.0 or not is unknown"
hardware,3axy0r,TehRoot,13,Wed Jun 24 17:10:58 2015 UTC,4K TVs normally don't have DP.
hardware,3axy0r,SLNC,3,Wed Jun 24 18:28:47 2015 UTC,4K TV'S all use HDMI 2.0. Superior or not only 2 TV'S that I know of will come out with display port. It just seems odd that they wouldn't include it considering they advertised a home theater setup too.
hardware,3axy0r,marcoalexander,4,Wed Jun 24 18:42:41 2015 UTC,"it's just kinda silly to have a top tier card in 2015 without the highest connection standards.  it would have been nice to see dp 1.3 and even hdmi 2.0.  yes, i know they are pushing the adapters, but the fact they have to push them means they somewhat missed the mark.  Regarding the edit: if it's reference it will still be hdmi 1.4.  there were photoshops with hdmi 2.o on the sapphire but the were quickly debunked."
hardware,3axy0r,logged_n_2_say,1 point,Wed Jun 24 12:03:18 2015 UTC,"AMD has said they'll be pushing adapters since the 5xxx series but they're quite MIA imo! Similar with DP on monitors, they're not abundant at all."
hardware,3axy0r,Schmich,1 point,Wed Jun 24 12:04:29 2015 UTC,DP to HDMI 2.0 active converters are coming soon.
hardware,3axy0r,SeaJayCJ,1 point,Wed Jun 24 12:07:41 2015 UTC,Sadly not until the end of the year now; they were supposed to come out a month ago but that didn't happen :/
hardware,3axy0r,marcoalexander,-1,Wed Jun 24 12:09:58 2015 UTC,gaming on a 4k tv   why.jpg
hardware,3axy0r,Le_rebbit_account,10,Wed Jun 24 12:34:49 2015 UTC,Imagine you have your normal desktop and then already have a 4K TV. You will want to connect to that TV as well and play on a large screen (depending on the game).
hardware,3axy0r,Schmich,1 point,Wed Jun 24 12:20:44 2015 UTC,"I'm talking about the TV part, not the 4k 40"" part.  TV's typically don't give a fuck about latency, most TV tuners induce a significant amount of lag making any kind of gaming experience pretty shit."
hardware,3axy0r,Le_rebbit_account,1 point,Wed Jun 24 18:41:57 2015 UTC,do that many people really own a 4K TV?
hardware,3axy0r,TehRoot,8,Wed Jun 24 12:27:13 2015 UTC,the type of people that would buy a $650 gpu are also more likely to own a UHD tv.
hardware,3axy0r,logged_n_2_say,2,Wed Jun 24 12:33:36 2015 UTC,"In general no, but the group of people looking to spend over $600 on a graphics card are significantly more likely to be the people that have them."
hardware,3axy0r,mvnman,1 point,Thu Jun 25 02:22:36 2015 UTC,I have noticed recently that they have become more common. I went and checked out current prices. They have really come down apparently. Now I want one!
hardware,3axy0r,CNUSubie07,1 point,Wed Jun 24 12:41:12 2015 UTC,"this is exactly what I've got, and wanted to do.  Also, Display port is much more sensitive about cable length than HDMI."
hardware,3axy0r,SilmarilSE,1 point,Wed Jun 24 13:43:50 2015 UTC,Why not?
hardware,3axy0r,shadowofashadow,1 point,Wed Jun 24 13:47:59 2015 UTC,Latency
hardware,3axy0r,Le_rebbit_account,1 point,Wed Jun 24 12:48:31 2015 UTC,The next round of 4K TV's run 4K 60Hz.... but at the same time they should also run 1080p 120Hz/240Hz.  All for the same price as a 1080p TV.  Hell by next year yuou should be able to get a 40-50 inch TV that will run beautiful 4K and also game at extreme refresh rates.  If you haven't tried a true 120Hz+ monitor they make a huge difference in games.
hardware,3axy0r,Thunder_Bastard,0,Wed Jun 24 12:41:30 2015 UTC,"I'm talking about the TV part, not the 4k 40"" part.  TV's typically don't give a fuck about latency, most TV tuners induce a significant amount of lag making any kind of gaming experience pretty shit."
hardware,3axy0r,Le_rebbit_account,1 point,Wed Jun 24 12:59:03 2015 UTC,I like bigger screens? :3
hardware,3axy0r,marcoalexander,1 point,Thu Jun 25 02:22:51 2015 UTC,"So do I, I also like a responsive panel when I pay mu gaemz"
hardware,3axy0r,Le_rebbit_account,1 point,Wed Jun 24 14:45:33 2015 UTC,My panel isn't too bad with response :P
hardware,3axy0r,marcoalexander,15,Thu Jun 25 02:22:06 2015 UTC,This review has the 980ti beating it pretty much every time. Even the nvidia reference stock card. Didn't expect that.  edit: relevant to op
hardware,3axy0r,Hidden__Troll,9,Wed Jun 24 18:40:43 2015 UTC,Tomshardware has both cards going backand forth with the Fury X generally winning in 4k by a decent margin.
hardware,3axy0r,Citizen_Snip,2,Thu Jun 25 02:23:37 2015 UTC,Where can you get 980 Ti for less? :)
hardware,3axy0r,Andrej_ID,2,Thu Jun 25 06:11:20 2015 UTC,http://geizhals.de has several 980 Ti models for €677.
hardware,3axy0r,LFKhael,1 point,Wed Jun 24 12:14:10 2015 UTC,"If what bit-tech said in their review is correct re: pricing, in the UK it'll be around £510. At that price, a 980TI is only £30 more expensive. It seems so far that (generally, depending on the site) the 980 TI beats it at 1440P. Even if it's only a few frames difference, if you're going for the best, it would appear the TI is the way to go, especially given it's overclocking potential vs. the Fury X.   It'll be interesting to see whether this is changed when (if?) voltage is unlocked for the Fury."
hardware,3axy0r,TheRealGecko,5,Wed Jun 24 12:19:12 2015 UTC,"So it only pulls 24 more watts than a 980ti, which is perfectly happy to run on a single fan blower cooler, yet they went with an AIO water loop on the reference model?   Seems to me that if they'd built it with a blower they'd be able to sell it significantly cheaper than the 980ti, which is what they need to be doing when the benchmarks show it's up to ~15% slower depending on the game. But for $650USD (price of the Fury X according to guru3d) you can get a Gigabyte 980ti from newegg which is faster out of the box.  As much of a step forward as this is for AMD, it seems that the only hope for this card to be competitive is if its water loop allows for some insane overclocks once all the GPU overclocking suites properly support it. And they'll have to be some properly bonkers clock bumps, given that guru3d showed their reference design 980ti going from 1002base/1076boost/7012ram to 1252base/1477boost/7818ram with only an 87mV core voltage bump."
hardware,3axy0r,mildr3d,6,Wed Jun 24 12:22:40 2015 UTC,"They did build it with a blower, but that one comes out next month for $100 less"
hardware,3axy0r,Colorfag,3,Wed Jun 24 12:43:08 2015 UTC,"we should wait a little, the fury x might be oc crippled for now  as i understand you can't tweak the core voltage. Perhaps when they'll unlock that, the card will oc better and the AIO will make more sense"
hardware,3axy0r,ytsoc,2,Wed Jun 24 13:16:27 2015 UTC,"I'm pretty sure the Fury non-X is a worse card, not just a Fury X without water cooling."
hardware,3axy0r,LazyGit,1 point,Wed Jun 24 14:08:53 2015 UTC,"The Fury  is coming out in a month or so and is $100 cheaper at a slight performance reduction, this card is also designed to OC and AMD has locked voltage for some reason so when it is unlocked it will warrant the card more."
hardware,3axy0r,Eruntien,9,Wed Jun 24 13:21:48 2015 UTC,Techpowerup has the Fury being best by the 980 in some tests.  Not what I expected at all.
hardware,3axy0r,thejshep,0,Wed Jun 24 13:28:04 2015 UTC,"Tweakers aswell, I was really hoping for it to be a little bit better. It's not a bad card at all but definitely not as good as it was made out to be. I'm a sad panda. I wanted to get one for 1080p 144hz gaming. Guess I'll get a ti."
hardware,3axy0r,phx_,1 point,Wed Jun 24 15:41:46 2015 UTC,You'll be happy with the Ti.
hardware,3axy0r,Haxican,0,Thu Jun 25 14:12:39 2015 UTC,Fury is on average 2% slower than 980ti at 4k   (average performance summary from 22 popular games)  Source here
hardware,3axy0r,anon1821,1 point,Thu Jun 25 03:22:25 2015 UTC,"Yes buy on GTAV at least, the 980 beats it at 1080 - it's still a great card but not the game changer I expected."
hardware,3axy0r,thejshep,0,Wed Jun 24 12:21:12 2015 UTC,Do you play your games at 4K?  Do you own a 4K monitor?  Do the games you play even support 4K?  Do the games you play at 4K scale correctly on the UI at 4K?  1080p is still the core of the gaming world and at that resolution the 980ti is still a clear winner and doesn't require a water cooler.  Plus there is the point that 1080p at 150fps+ to support 144Hz monitors makes a huge difference while 4K struggling at 40-60fps is not going to be that impressive.
hardware,3axy0r,Thunder_Bastard,3,Wed Jun 24 13:24:11 2015 UTC,I understand your point but many people render their games at 4k and downscale at their native resolution(via gpu DSR VRS or using their monitor scaler)
hardware,3axy0r,anon1821,5,Wed Jun 24 15:03:58 2015 UTC,"In the short term I'm disappointed, I like amd, and like others I welcome the competition because in the end it benefits us, the consumers.   Maybe I'm in the minority here but I haven't got a 4k display, and I don't feel the need to get one anytime soon, aiming for 4k performance while appearing costly to lower resolutions seems strange, I don't know how that works so excuse my ignorance if it's just the way it is, and I wouldn't mind if we were at a point where 4k gaming is in reach, but it really isn't, especially when there's a huge number of people playing at 144hz, these people aren't going to settle for sub 60 frames, I think it's too early to make it a selling point, other than being solely for dick measuring.  While I'm disappointed now, I'm happy to see AMD innovate, I really think their next card that uses HBM will be a strong competitor."
hardware,3axy0r,GetYourGoujOn,3,Wed Jun 24 13:38:50 2015 UTC,Is it normal for TitanX to lose to 980ti in many games? I would expect to be other way around.
hardware,3axy0r,Tuczniak,1 point,Wed Jun 24 14:30:26 2015 UTC,The custom pcb 980 Ti variants are typically faster than the Titan X.
hardware,3axy0r,EnsoZero,3,Wed Jun 24 15:43:27 2015 UTC,This is about what I expected. At least AMD is knocking on nvidia's door. I'll be interested to see benchmarks in a couple weeks with unlocked voltages and new drivers.
hardware,3axy0r,billthedozer,4,Wed Jun 24 17:14:35 2015 UTC,"i'm going to somewhat chalk up these disappointing uhd results to drivers (per usual, for a new card release) but also wish they had used a reference 980ti.  the msi looks to be ~10% better than reference, in their other graphs.  http://www.overclock3d.net/gfx/articles/2015/06/23140914497l.jpg"
hardware,3axy0r,logged_n_2_say,2,Wed Jun 24 16:11:25 2015 UTC,Only 5 fps in Crysis 3? Is that a VRAM issue? Or just a fluke?
hardware,3axy0r,aziridine86,3,Wed Jun 24 14:09:37 2015 UTC,"i noticed that too, but i dont ever remember a vram issue with crysis even at UHD.  again, i chalked up to driver issue, but it could also be reviewer error (remembering those oc3d gtav 390x benches)  in tpu it's leading the pack  http://www.techpowerup.com/reviews/AMD/R9_Fury_X/15.html"
hardware,3axy0r,logged_n_2_say,2,Wed Jun 24 16:31:26 2015 UTC,Ah maybe they just left off a digit or something then.
hardware,3axy0r,aziridine86,1 point,Wed Jun 24 18:43:54 2015 UTC,Why use a reference 980 Ti if the custom pcb versions are roughly the same price and typically better?
hardware,3axy0r,EnsoZero,1 point,Wed Jun 24 12:14:56 2015 UTC,"msrp is higher, but yes sale price they come down.  it's usually just easier to compare across difference review sites if you use reference, since everyone does."
hardware,3axy0r,logged_n_2_say,5,Wed Jun 24 13:36:00 2015 UTC,So around a 980ti. Sweet.   I will hope to get one
hardware,3axy0r,lechechico,2,Wed Jun 24 13:40:36 2015 UTC,"I wonder how non-UK people deal with Tom Logan...   ...like the MSI 980 Ti Gaming which comes out of the box absolutely bob-on, ready to rock off"
hardware,3axy0r,phigo50,11,Wed Jun 24 13:44:47 2015 UTC,"Fury X. It's a short 980ti that's slightly weaker (overall), has less VRAM, uses more power, and can't really overclock.  :("
hardware,3axy0r,lukeroge,2,Wed Jun 24 16:28:01 2015 UTC,Here's their video review: https://www.youtube.com/watch?v=YZ3co5a-k1s
hardware,3axy0r,SKEW_YOU,4,Wed Jun 24 16:37:41 2015 UTC,Looking good! Inbetween 980 and 980ti performance-wise. Shines at higher resolutions.  Let's just hope the price tag is reasonable once it's in stores over here...
hardware,3axy0r,nemmera,1 point,Wed Jun 24 12:00:35 2015 UTC,"699€, USD retail is $650 so sounds about right for euros"
hardware,3axy0r,TehRoot,5,Wed Jun 24 16:26:43 2015 UTC,"699€, USD retail is $650 so sounds about right for euros   Yeah, true. I read a few reviews and seems it has a constant high-pitched whine/noise (from the pump?) though, which is a dealbreaker for me. Performance seems okay though."
hardware,3axy0r,nemmera,1 point,Wed Jun 24 12:26:40 2015 UTC,"They also said it's typically only noticeable in their open air benches, and that Maxwell's infamous coil whine is more pervasive.   Still, I expected more from Big Red :("
hardware,3axy0r,s4in7,1 point,Wed Jun 24 11:57:43 2015 UTC,"Supposedly that whine will be fixed on the consumer end, but I'm going to wait until early adopters get their cards and do reviews on them as well."
hardware,3axy0r,reynardtfox,1 point,Wed Jun 24 11:58:47 2015 UTC,"The AIO they used is cooler master iirc, which are louder then corsair"
hardware,3axy0r,TehRoot,4,Wed Jun 24 11:59:55 2015 UTC,I expected more. the locked voltage is kinda weird.
hardware,3axy0r,Riodian,11,Wed Jun 24 12:28:00 2015 UTC,All cards have locked voltage until there's an update for the most part..
hardware,3axy0r,TehRoot,1 point,Wed Jun 24 13:55:12 2015 UTC,Sitting here mashing the F5 button on Amazon..  Can't wait to see how well the oc'ing goes. It competes with the 980ti even at a decent oc so I'm excited.
hardware,3axy0r,TehRoot,3,Wed Jun 24 16:04:06 2015 UTC,Available in Germany :D  http://www.mindfactory.de/product_info.php/4096MB-PowerColor-Radeon-R9-FURY-X-Hybrid-PCIe-3-0-x16--Retail-_1007340.html
hardware,3axy0r,SLNC,1 point,Wed Jun 24 12:29:42 2015 UTC,Ah but I need US :(
hardware,3axy0r,TehRoot,4,Wed Jun 24 11:57:43 2015 UTC,"Well, it's a good price for a good card. I, myself, have gone for a 980 Ti. But to each his own. The Fury X looks like a good card."
hardware,3axy0r,SLNC,2,Wed Jun 24 11:58:40 2015 UTC,They're both great cards and I wish people would recognize that instead trying to say one's shit and the other's great.  They trade blows fairly well and depending on your preferences either can easily be better than the other.
hardware,3axy0r,reynardtfox,2,Wed Jun 24 11:57:02 2015 UTC,True.
hardware,3axy0r,SLNC,1 point,Wed Jun 24 11:57:41 2015 UTC,what site?
hardware,3axy0r,TJ_Schoost,1 point,Wed Jun 24 11:59:01 2015 UTC,Amazon.  Bought mine.
hardware,3axy0r,TehRoot,1 point,Wed Jun 24 12:10:24 2015 UTC,dammit out of stock already.
hardware,3axy0r,kungfujedis,2,Wed Jun 24 12:16:42 2015 UTC,"I'm team green but was looking forward to Fury X smashing the 980ti just to justify HBM being a significant break through in GPU technology. While it's a good performing card, it seems HBM at this point is still lack luster. Now that the excitement is over, now who knows how long it will be until the next big thing that changes the game again."
hardware,3axy0r,cyberd0rk,3,Wed Jun 24 16:05:52 2015 UTC,"According to Digital Storm, the Fury X lands in between the Titan X and the 980ti."
hardware,3axy0r,fr33andcl34r,3,Wed Jun 24 18:29:52 2015 UTC,HBM is significantly superior to GDDR5 in every aspect.  Nvidia will also use HBM(rev 2.) first chance they can (which is 2016).
hardware,3axy0r,HavocInferno,1 point,Wed Jun 24 12:02:30 2015 UTC,"HBM2 is where it'll really deliver its potential. HBM is an improvement, but it's not like memory bandwidth has been the most critical issue in AMD GPUs before."
hardware,3axy0r,bphase,1 point,Wed Jun 24 12:17:42 2015 UTC,I'm pretty sure HBM isn't the reason the Fury X doesn't perform a lot better than the 980Ti. It's the GPU what's not good enough.
hardware,3axy0r,Yearlaren,2,Wed Jun 24 12:25:49 2015 UTC,Most reviewers are reporting very limited overclocking potential despite being told otherwise.  What's the point of that radiator and sub-60C temperature under load if I can't push the card any harder?
hardware,3axy0r,Oafah,6,Wed Jun 24 13:00:04 2015 UTC,"I'd assume for some reason that those OC settings are locked for now, to be unlocked at some date in the near future.  There's no chance that AMD would have opted for a stock watercooled option while talking up their OC potential for such pitiable gains."
hardware,3axy0r,xhytdr,5,Wed Jun 24 12:26:16 2015 UTC,"There's no chance that AMD would have opted for a stock watercooled option while talking up their OC potential for such pitiable gains.   Until we see otherwise, that's exactly what they've done."
hardware,3axy0r,Oafah,3,Wed Jun 24 12:53:34 2015 UTC,"There's no memory control at all(somewhat expected), and there's no voltage adjustments yet.  Have to wait for an update in the coming weeks"
hardware,3axy0r,TehRoot,1 point,Wed Jun 24 14:42:53 2015 UTC,"Damn, I dont know what to do now. I wanted to jump on the Fury X, but given the benchmarks, Im hesitant to do so."
hardware,3axy0r,Colorfag,3,Wed Jun 24 13:19:05 2015 UTC,I'd wait a few weeks to see what the overclocking potential looks like once the gpu voltage is unlocked.  AMD called the Fury X an overclocker's dream.  We'll have to wait and see if it is true.  If it even overclocks nearly as well the 980ti I think I'd call that a win in my book.
hardware,3axy0r,reynardtfox,1 point,Wed Jun 24 17:16:25 2015 UTC,Wait till next year when we can see HBM2 and 16nm cards.
hardware,3axy0r,EnsoZero,2,Wed Jun 24 12:10:12 2015 UTC,Not even close to stellar. 980ti beats it. I am kind of disappointed with this card.
hardware,3axy0r,pnevroq,4,Wed Jun 24 12:15:28 2015 UTC,"Read more reviews. Digital Storm's benches show it consistently landing between the 980 Ti and Titan X. Tom's Hardware shows the Fury X matching or leading the 980 Ti in several more games.   Anyway you cut it, the Fury X offers 980 Ti-level performance with cooler temps and less noise--however it does so by using more power and with a bulkier cooling kit. So depending on your priorities in an enthusiast graphics card, each has their strengths and weaknesses."
hardware,3axy0r,s4in7,1 point,Wed Jun 24 12:38:11 2015 UTC,"I've read a few, digital storm too (which is the only reviewer with such results). We have waited this card for too long to see it performs below/on par with 980ti. That is why it is not stellar. Check this out http://www.techpowerup.com/reviews/AMD/R9_Fury_X/31.html. I like this card but it is not what I expected."
hardware,3axy0r,pnevroq,2,Wed Jun 24 12:25:14 2015 UTC,"AMD never branded it as a 980 Ti killer. It was in development for a loooong time and was always a 980 killer--which it does swimmingly. The fact it can even approach 980 Ti levels is amazing. I too wished it would be the new flagship graphics card, but that was always an extremely hopeful wish :/"
hardware,3axy0r,s4in7,2,Wed Jun 24 12:58:49 2015 UTC,The gap is only 2% at 4k
hardware,3axy0r,anon1821,-1,Wed Jun 24 16:07:28 2015 UTC,"Exactly, the gap."
hardware,3b4pbh,bohemia,18,Thu Jun 25 23:11:13 2015 UTC,"This always happens when AMD/Nvidia/Intel release a new GPU/CPU, it should be expected"
hardware,3b4pbh,wasdzxc963,-13,Thu Jun 25 23:55:30 2015 UTC,"This doesn't always happen. I've been following /r/hardware for years. It is a much larger subreddit than even this time last year. It has certainly grown, but into what?"
hardware,3b4pbh,wasdzxc963,11,Fri Jun 26 00:04:59 2015 UTC,"It also could be because of Fury having HBM, which is the next major step for video memory  Maybe it has grown, I haven't been following /r/hardware for as, maybe just over a year  Maybe /r/hardware does have a slight AMD bias, but most because AMD is the underdog and we want to see AMD keep up with Intel and Nvidia since it benefits everyone"
hardware,3b4pbh,hilo8914,8,Fri Jun 26 00:50:45 2015 UTC,"AMD's flagship just came out, what do you expect?"
hardware,3b4pbh,BlackTriStar,5,Fri Jun 26 00:34:48 2015 UTC,It's died down a lot now that the Fury X isn't the second coming everyone was hoping it'd be. Still got the air-cooled Fury and the Nano launches coming up.
hardware,3b4pbh,SupremeGunman,8,Thu Jun 25 23:21:13 2015 UTC,"Right now AMD is the hot stuff, in about a week or so things will return to normal until someone else releases their hot stuff.  Then we will be stuck with them.  et cetera"
hardware,3b4pbh,Seclorum,3,Thu Jun 25 23:14:33 2015 UTC,It always swings one way or another when they release a new product.
hardware,3b4pbh,maybachsonbachs,8,Fri Jun 26 00:03:06 2015 UTC,AMD posts are better than this low effort off topic complaining.
hardware,3b4pbh,stapler8,2,Fri Jun 26 01:13:11 2015 UTC,"No. AMD's been everywhere lately mostly because of the new GPU lineup, HBM/Fiji, and the Nano."
hardware,3b4pbh,MerryLane,-10,Thu Jun 25 23:27:50 2015 UTC,"AMD is like Pepsi, each time you see an advert from that brand, you think mostly to the ""real stuff""."
hardware,3awq64,Platinumjsi,28,Wed Jun 24 02:56:50 2015 UTC,"Would kind of make sense, they were jumping right up to 10nm awfully fast otherwise.  give me Skylake-E Intel!"
hardware,3awq64,JaketheAlmighty,16,Wed Jun 24 03:00:56 2015 UTC,"Yeah they had so many problems with broadwell, I didn't think they could turn 10nm so quickly"
hardware,3awq64,Jakeattack77,5,Wed Jun 24 03:42:00 2015 UTC,Yeah I want this soooon. Looking like January is the earliest... Have you seen otherwise?
hardware,3awq64,Imidazole0,9,Wed Jun 24 03:06:23 2015 UTC,I would think next summer.
hardware,3awq64,PhilipK_Dick,2,Wed Jun 24 06:40:34 2015 UTC,"Yea, I was wondering what will happen in 5-10 years."
hardware,3awq64,yuhong,3,Wed Jun 24 04:52:25 2015 UTC,"I think we won't see '10nm' from Intel for a long time, and this Skylake refresh may not be the last."
hardware,3awq64,spiker611,1 point,Wed Jun 24 04:45:22 2015 UTC,"kaby lake doesn't make much sense to me, Perhaps it is Skylake-E but spread out across multiple formfactors"
hardware,3awq64,spellstrike,21,Wed Jun 24 15:34:53 2015 UTC,The successor to Kaby Lake is Babycakes
hardware,3awq64,omega_fucking_cunt,5,Wed Jun 24 10:03:55 2015 UTC,1.6 GHz single cores for maximum efficiency
hardware,3awq64,JimJamJamie,14,Wed Jun 24 14:09:13 2015 UTC,"I was just expecting a simple Skylake refresh with solder instead of paste and a small clock boost. An upgraded I-gpu and extra L4 is nice in comparison. Might be getting to the GTX760/270 performance level.   Overall it sounds like the same story of Ivy, Haswell & Broadwell though. Hopefully Zen shakes things up a bit and we start seeing more 6-core/8-core aggressively priced."
hardware,3awq64,Scrabo,12,Wed Jun 24 03:59:35 2015 UTC,Admittedly the 5820 is amazingly priced.
hardware,3awq64,Seclorum,7,Wed Jun 24 06:59:31 2015 UTC,Especially if you live near a microcenter!
hardware,3awq64,reynardtfox,5,Wed Jun 24 13:30:40 2015 UTC,And DDR4 dropped significantly in the last two months.
hardware,3awq64,Smagjus,3,Wed Jun 24 19:43:51 2015 UTC,"The motherboards are still expensive, but the processor is reasonably priced and with cheaper and cheaper memory it's becoming more and more an option for some people."
hardware,3awq64,Seclorum,3,Wed Jun 24 19:46:31 2015 UTC,For gaming is there any benefit of a 5820 over the 4790? Noob here
hardware,3awq64,Abipolarbears,5,Wed Jun 24 18:18:19 2015 UTC,"For the game itself, basically none. Almost no engines truly scale past 4 cores.   the 5820 gives you 6 natural cores. So it starts to benefit more by loading more than just the game.   Toss a video encode, record gameplay, stream, it all adds up when you try and do it all at the same time."
hardware,3awq64,Seclorum,3,Wed Jun 24 18:41:32 2015 UTC,"Not significantly, no."
hardware,3awq64,skrillcon,0,Wed Jun 24 18:50:07 2015 UTC,Depends on the game.
hardware,3awq64,Teethpasta,48,Wed Jun 24 18:39:30 2015 UTC,"I wish Intel would focus on improving actual CPU performance instead of investing all of this die space into more advanced integrated graphics:  -Businesses that require the ability to render 3d for CAD/simulation will be using workstation GPUs anyways, making the IGP worthless to them.  -Consumers/employees that don't use computers for 3d tasks like gaming, animation, or simulation don't really need advanced graphics. Basic integrated graphics can already handle HD video, there's no need to throw in a complex IGP that gamers and businesses replace.  It's a shame what lack of competition does. AMD is so far behind so Intel has no reason to improve performance more than 10% per generation."
hardware,3awq64,skilliard4,70,Wed Jun 24 04:45:39 2015 UTC,"From the perspective of providing better performance to consumers, Intel's not doing very much. But from their perspective of screwing AMD and NVidia, they're doing great: they're making great progress toward shutting them out of the laptop market by raising the bar for IGPs to the point that discrete GPUs have to be the absolute cream of the crop in order to be competitive in any ordinary sort of laptop. Intel's content for now to leave the discrete desktop GPU market alone, but they're trying pretty hard to monopolize the laptop GPU market."
hardware,3awq64,wtallis,30,Wed Jun 24 04:58:28 2015 UTC,Linux user here.   Fuck AMD! Fuck Nvidia! Long live Intel Graphics!
hardware,3awq64,Jew_Fucker_69,11,Wed Jun 24 10:31:59 2015 UTC,intel contribute a lot to Linux not just good graphics drivers. I respect them for that
hardware,3awq64,TeutorixAleria,3,Wed Jun 24 14:06:37 2015 UTC,Well this can't be the way to go. Are drivers such a big issue? I thought it is just s lack of support from game devs.
hardware,3awq64,Klorel,8,Wed Jun 24 15:35:37 2015 UTC,"Game support has improved dramatically since Valve has campaigned to make all Steam games Ubuntu compatible.   But the AMD and Nvidia drivers are still a cramp to install IMO. Especially since they're the only drivers that need to be installed. All other hardware runs out of the box on Linux systems. For example I have an Intel Graphics based PC, and an Ubuntu installation takes 10 minutes (SSD), after which I only have to download some programs (e.g. Chrome). If I had a dedicated GPU from Nvidia / AMD, the system installation would take more like 4 hours, and I would need to google how to do it for each new GPU and each new Ubuntu version."
hardware,3awq64,Jew_Fucker_69,4,Wed Jun 24 15:40:45 2015 UTC,"Most distros include the open-source drivers for AMD and nVidia with a base install.  Good distros will detect your card and ask if you would like to install the proprietary drivers instead.  Minus weird laptop configurations, I've never had any issues running any distro with a dedicated GPU.  Can't speak for CF or SLI though"
hardware,3awq64,Petrieiticus,4,Wed Jun 24 16:25:31 2015 UTC,"That might be true, but it's absolutely not my experience. Even when I had the GeForce 8800 GT - which was one of the best supported cards on Linux - I ran into lots of problems."
hardware,3awq64,Jew_Fucker_69,1 point,Wed Jun 24 18:59:28 2015 UTC,"I feel sorry for you my friend. Same graphics, less problems for me."
hardware,3awq64,spiral6,1 point,Sat Jun 27 05:57:14 2015 UTC,"I have literally never run into an issue with nVidia cards in Linux.  The Catalyst driver is a fucking nightmare, but nVidia has always been great.  (Optimus aside, but I don't have that on any of my systems.)"
hardware,3awq64,Floppie7th,1 point,Wed Jun 24 23:17:08 2015 UTC,"GTX 780 here, I have to install a custom driver in every distro, plus modify it more to make it run. It's a pain in the ass."
hardware,3awq64,hellishhk117,2,Thu Jun 25 02:58:35 2015 UTC,"There's still no way to get even OpenGL 4 on Intel's Linux driver, though. It's been over five years."
hardware,3awq64,pinumbernumber,1 point,Thu Jun 25 00:43:37 2015 UTC,That's true. The drivers for my i5-4670k only support up to OpenGL 3.3.
hardware,3awq64,Jew_Fucker_69,8,Thu Jun 25 06:50:48 2015 UTC,Why does it help Intel to shut discrete graphics cards out of laptops? I thought improving the graphics and power efficiency of their processors was all working towards the goal of taking the tablet market and eventually the cell phone market from ARM.
hardware,3awq64,jecowa,8,Wed Jun 24 06:52:16 2015 UTC,"If they can offer something to OEMs that lets them have a smaller bill of materials, less components to integrate on the motherboard, then they're more likely to use intel."
hardware,3awq64,plank_,2,Wed Jun 24 09:25:16 2015 UTC,Not only that but more and more % of the BOM will be going to Intel. They used to only get the cpu. Now so much is integrated that o ly a few things aren't going to intel
hardware,3awq64,dylan522p,4,Wed Jun 24 18:59:24 2015 UTC,Intel believes they can easily provide a complete solution that could also allow for lower prices and longer battery life.  I wonder if it will be possible for the integrated GPUs to work in conjunction with a dedicated GPU to handle some aspect of processing.  That would be awesome for end users.
hardware,3awq64,thekeanu,3,Wed Jun 24 09:59:11 2015 UTC,"Yeah, Iris Pro brought Nvidia GT 630M performance to iGPUs in processors that were only 47W. A 630M is 35W by itself.  The recent i5/i7 -C releases on the desktop have performance in the level of the GT 740. 65W, too, which is the TDP of just a 740."
hardware,3awq64,LFKhael,2,Wed Jun 24 14:42:48 2015 UTC,"Intel's strategy of taking over the laptop graphics market dates from before the tablet market mattered. Their first big move was abusing patents to prevent NVidia from making chipsets for post-Core 2 processors. NVidia had been dominant in the integrated graphics space, but Intel completely locked them out of that market, forcing laptop manufacturers to choose between Intel integrated graphics (which was still shit back then) or squeezing a large discrete graphics chip onto an already crowded logic board. Since then all of their improvements to the IGP have been clearly aimed at the laptop market; their newer larger IGP configurations are too big and power-hungry now for tablets and their desktop strategy for IGPs has been completely nonsensical for years. It's only been with Broadwell that they've had anything that's even close to being a credible tablet product."
hardware,3awq64,wtallis,5,Wed Jun 24 16:50:37 2015 UTC,"The IGPs for laptops are good, but idk why they need to do it for desktops."
hardware,3awq64,skilliard4,27,Wed Jun 24 05:02:57 2015 UTC,Plenty of systems run with Core i5s and i7s without a discrete video card.
hardware,3awq64,Anaron,11,Wed Jun 24 05:25:39 2015 UTC,I built one a couple years ago for an old grandmotherly type who the most intensive thing she runs is watch the occasional youtube video.   No sense gouging her for a discrete card she would never use.
hardware,3awq64,Seclorum,14,Wed Jun 24 06:58:23 2015 UTC,Yet you gouged her for an i5 or i7. Surely an i3 would've done the job easily?
hardware,3awq64,dotdrew,9,Wed Jun 24 09:34:08 2015 UTC,"Occasional Youtube video is Celeron territory. Intel has had 4k video playback hardware since Intel HD Ivy Bridge, which is what you get on Celerons/Pentiums with just ""Intel HD"" for their GPU."
hardware,3awq64,LFKhael,3,Wed Jun 24 11:30:11 2015 UTC,"It was an i5, nonK not even top of the line. Cant remember which one exactly because it was a few years ago.   I'm just happy that she is happy with the machine and doesn't experience a million problems like her old machine.   She would seriously make a sandwich after turning her old machine on.  And when she tried to run her accounting software, half the time the machine would pack in and just crash. No such issues since then."
hardware,3awq64,Seclorum,-1,Wed Jun 24 15:25:01 2015 UTC,pre built trash at wallsmart  i build my own PC even then i refuse to buy any processors with integrated filth on them
hardware,3awq64,Battle_Sparks,1 point,Wed Jun 24 21:56:23 2015 UTC,I build my own gaming PCs too. I don't care if my CPU has integrated graphics or not as long as the performance isn't affected. Are there Core i5/i7 models without integrated graphics that also perform better because of it?
hardware,3awq64,Anaron,1 point,Wed Jun 24 22:06:56 2015 UTC,think about it  all that space that the iGPU is wasting could of been put to better use like giving us more than 10% performance and not stat padding there transistor budget on stupid shit like the iGPU  thus i have forced myself into buying xeons or -E CPUs
hardware,3awq64,Battle_Sparks,6,Wed Jun 24 23:29:57 2015 UTC,They do it for desktops because making a separate set of chips for desktops isn't worth the expense of setting up another production line.
hardware,3awq64,wtallis,1 point,Wed Jun 24 05:54:17 2015 UTC,Intel is a massive corporation whose responsibility is to its shareholders.  They seem to be playing the long game of gaining market share over ARM and NVidia.  Not screwing anyone.  It's not like they make bad processors that we all don't buy.
hardware,3awq64,PhilipK_Dick,2,Wed Jun 24 15:50:17 2015 UTC,That only makes sense if you ignore their history of anticompetitive behavior
hardware,3awq64,Farnso,-5,Wed Jun 24 19:22:54 2015 UTC,"When you say anticompetitive, do you mean competitive?  They are a corporation.  Their job is to compete with other companies.  Are you blaming them for being a more competitive company than AMD?"
hardware,3awq64,PhilipK_Dick,5,Wed Jun 24 19:42:17 2015 UTC,"No, I mean anticompetitive. There is a big difference between being better at competition and being anticompetitive. Intel has used anticompetitive means to maintain their edge many times."
hardware,3awq64,Farnso,-1,Wed Jun 24 21:41:48 2015 UTC,"If it gets bad enough, the US will force them to pump money into ARM or AMD to avoid anti-trust laws.    If AMD didn't suck as bad at business - and I mean when they made the decision to go into the foundry business and trying to be competitive in APU, CPU and GPU businesses with only a fraction of the R&D budget of any of their competitors in any of those segments - they could execute on one of those.  Then there is their awful marketing and launch of products.  Not to mention poor product support.  It is like their business plan is to suck long enough to force the government to make Intel bail them out.  How could you root for a business that is run this poorly?  Because they encourage open source even to their own detriment?"
hardware,3awq64,PhilipK_Dick,3,Wed Jun 24 22:04:59 2015 UTC,"Lol, your biases are showing and you completely dodged my point. I don't know why you are assuming I'm rooting for AMD at all. The only thing I have brought up at all is that Intel has engaged in anticompetitive practices in order to maintain their lead and avoid actual competition.  How can you root for a business that cheats in order to win? Intel is so afraid of ""a business that is run this poorly"" that they paid other businesses to not use their products. It's ridiculous."
hardware,3awq64,Farnso,0,Wed Jun 24 22:28:49 2015 UTC,"I'm bummed by AMD in general.  I buy the best products for my dollar.  Frankly, I don't care how a business is run and if you own anything by Apple or Intel, or consume anything produced by General Mills, or drink Tropicana juice, or chew any major brand of gum, or eat any commercially available candy bar - - - you do the same.  What makes you think Intel is afraid of AMD?  By the time Zen comes out, Intel will be 2 generations ahead.    I wish AMD would just focus on one thing instead of losing money on everything.  Do you think they will gain market share with Fury?  Even if they go up a tick or two, they can't sustain a decent R&D budget losing money.    I'd say if you believe in AMD, buy stock in the company.  It went up by a cent today with ""the big news"" and is currently $2.62 per share.  At least if Qualcomm buys AMDs server business, the stock may go up a bit.  If Qualcomm buys the whole shebang, they will hopefully set the company straight and help them to finally become profitable.  Actually - looking at it, if Qualcomm buys AMD, it would be the best thing for everyone involved.  Qualcomm will go up in value significantly and AMD will get an influx of cash that might help make Zen worth the wait..."
hardware,3awq64,PhilipK_Dick,2,Wed Jun 24 22:52:38 2015 UTC,"""What makes you think Intel is afraid of AMD?""  .....They have been caught paying their customers to not buy from AMD, or to delay releases of products with AMD parts.  They also had to be banned from purposefully redesigning their CPU/Compilers's to harm the performance of their competitors GPU's.  Here's one example:  http://www.wired.com/2009/12/ftc-sues-intel-for-anti-competitive-practices/  The rest of your points are entirely off topic and I couldn't care less about any of them in the context of this discussion."
hardware,3awq64,Farnso,1 point,Wed Jun 24 23:09:26 2015 UTC,"The European Commission seems to think that they've been anticompetitive, so does the US Federal Trade Commission."
hardware,3awq64,Dr_professional,-1,Wed Jun 24 20:21:43 2015 UTC,Then stop buying their products.
hardware,3awq64,PhilipK_Dick,2,Wed Jun 24 21:12:55 2015 UTC,You presume that I do.
hardware,3awq64,Dr_professional,0,Thu Jun 25 00:15:22 2015 UTC,I don't really care how you build your computers or for what tasks you use them.  I buy the best product for the best value at the time I am buying.  Intel has had that product every time I have done a build since my Commodore 64 - and I don't see that changing in the near future.  I could care less about how they kept AMD down last decade.
hardware,3awq64,PhilipK_Dick,2,Thu Jun 25 02:55:26 2015 UTC,"I don't really care how you build your computers or for what tasks you use them.   Then why did you tell me what to buy?   I buy the best product for the best value at the time I am buying.  Intel has had that product every time I have done a build since my Commodore 64 - and I don't see that changing in the near future.  I could care less about how they kept AMD down last decade.   I buy AMD out of necessity, not ideology. I, too, do not care much about what others buy, but you claimed that Intel was not anticompetitive, which was wrong."
hardware,3awq64,Dr_professional,8,Thu Jun 25 03:59:25 2015 UTC,Hopefully AMD has something to show with zen.
hardware,3awq64,Rraymond123,4,Wed Jun 24 06:43:16 2015 UTC,"If you look at Knights Landing, you'll see that they are definitely doing some innovative work while still meeting the demands (in this case, laptops) of the mainstream computing market with their normal line of CPUs."
hardware,3awq64,salgat,1 point,Wed Jun 24 13:43:09 2015 UTC,I've seen it and it's pretty cool.
hardware,3awq64,spellstrike,1 point,Thu Jun 25 23:03:48 2015 UTC,"Knights Landing is extremely cool, and something I'd love to get my hands on to program for!"
hardware,3awq64,salgat,1 point,Thu Jun 25 23:49:26 2015 UTC,"The on package memory is ridiculous, I'm surprised it's not getting more attention. http://www.theplatform.net/2015/03/25/more-knights-landing-xeon-phi-secrets-unveiled/  The die is absolutely massive as well, dwarfs haswell xeons."
hardware,3awq64,spellstrike,1 point,Fri Jun 26 01:39:24 2015 UTC,"Yeah I was surprised also, but I guess it's necessary to deal with the inevitable thread caching issues that come with so many processors (similar to GPU)."
hardware,3awq64,salgat,3,Fri Jun 26 01:59:57 2015 UTC,They are doing this to push for the mobile/ultra portable space.
hardware,3awq64,Dark_Crystal,2,Wed Jun 24 19:08:52 2015 UTC,"A lot of people use their portable computer for school, work, browsing, etc primarily - but still want to be able to play games (at modest settings). I've got a desktop gaming PC as well as a Surface Pro - it is nice to be able to play games on it when I'm out of the house."
hardware,3awq64,Kocidius,6,Wed Jun 24 12:41:05 2015 UTC,"They have that, its the -E brand as well as Xeons."
hardware,3awq64,Seclorum,4,Wed Jun 24 06:56:40 2015 UTC,"That's nice and all, but I have a couple problems with those CPUs.   Low single core performance. The $200 i5 4690K outperforms the $1000 i7 5960x on individual cores. Considering a lot of software(especially games) are reliant on individual core performance, this makes the -e series a poor choice. The pricing. $500-1000 is a lot for a CPU. I get that it's high end, but they would never get away with this if they had competition. I really wish they had desktop i5s with no IGP and good single core performance.   Overall, I wish they would make a good mid budget processor based on single threaded performance. Right now the i5 4690K is the go to, but with most of the rewards of the 14nm manufacturing process going into the graphics die, we aren't seeing many gains with newer technology.  If Intel made a 14nm quad core CPU with no IGP and 30-40% faster single threaded performance than the i5 4690K, I would buy it in a heartbeat."
hardware,3awq64,skilliard4,7,Wed Jun 24 07:03:37 2015 UTC,You do realize the point of the -E series is for workstations right?  They're not gaming CPUs.  There's a reason they don't have iGPUs in them and that's because they're meant to be paired with a power GPU that will be utilized for numbercrunching like a  FirePro or a Quadro.  Where the -E series really shines is its ability to multitask.  Having 6 cores and hyperthreading allows it to do a lot more than a 4690k or even a 4970k.  And as /u/MaloWlol points out below pushing individual core stats beyond 4Ghz on silicon is still a bit of a ways away.  What Intel is doing now makes sense (even if their pricing could probably be a bit laxer).
hardware,3awq64,reynardtfox,9,Wed Jun 24 13:46:35 2015 UTC,"If Intel made a 14nm quad core CPU with no IGP and 30-40% faster single threaded performance than the i5 4690K, I would buy it in a heartbeat.   Afaik Intel has kinda reached the max single-threaded performance possible on silicon already. Sure as we see every iteration we get a 5% performance increase because IPC gets improved thanks to new architectures designed a little smarter, but increasing IPC gets exponentially harder as it gets better. The only other thing they can do to increase single-core performance is increase clock frequency, which has proved difficult to do beyond 4Ghz-ish on silicon. That's why pretty much all Intel's CPU's performs the same in single-threaded scenarios, with a little variance due to the varying clock speeds as they cherry-pick the best performing chips and push their clocks for the high-end CPUs.  So just dropping the IGP is not gonna let them make a faster CPU, just maybe let them squeeze in another core or two instead. The way forward is therefor either to add more cores and increase utilization in applications of more cores, which is pretty hard to do, or to move away from silicon, which is pretty hard to do as well and we're probably a while away from that happening. I've read that Graphite might be a good alternative that is capable of running at 100+Ghz."
hardware,3awq64,MaloWlol,4,Wed Jun 24 07:59:09 2015 UTC,"More cache might make a decent difference though, especially some substantial amount of L4/EDRAM.  This is pushing it and I'm a bit hazy on the details, but perhaps speculative execution of multiple code branches on multiple cores could get a nice speedup for single threaded only code. Ideal CPU dynamically manages the tradeoff between allocating resources to multiple cores vs boosting a single core. It would only be worth it if only 1 core is being used, but you might be able to trade 4 cores for 1 core operating up to 50% higher IPC on average."
hardware,3awq64,sifnt,1 point,Wed Jun 24 10:09:54 2015 UTC,If you're already achieving a ~95% hit rate then simply spending more die space on cache probably won't yield very much.
hardware,3awq64,hughJ-,5,Wed Jun 24 18:00:55 2015 UTC,"Low single core performance. The $200 i5 4690K outperforms the $1000 i7 5960x on individual cores. Considering a lot of software(especially games) are reliant on individual core performance, this makes the -e series a poor choice.   Those CPUs are not really targeted at gamers through. i5/i7 on the 1150 socket are the sweet spot and marketed as such.   Enthusiast class chips really excel at media and content creation and in those markets are awesome for the price."
hardware,3awq64,Jalkaine,2,Wed Jun 24 11:06:28 2015 UTC,Those CPUs are not really targeted at gamers through.   The original complaint was that there was no affordable chip tailored to gaming that didn't include an IGP.
hardware,3awq64,longshot2025,1 point,Wed Jun 24 16:42:21 2015 UTC,The original complaint hinges on the premise that the IGP incurs a single threaded performance hit from what could have otherwise existed - as if one could simply have allocated die space to use make-threads-go-faster transistors or something.  If single threaded performance actually scaled with complexity then the industry would have stuck with hyperthreaded single core chips.
hardware,3awq64,hughJ-,1 point,Wed Jun 24 17:57:32 2015 UTC,"I read it as the IGP is taking space that could go to more equally powerful cores, not necessarily better performing cores. Something like the 5820k without the -E specific features and maintaining the 4790k single-threaded performance would be amazing."
hardware,3awq64,longshot2025,2,Wed Jun 24 18:37:32 2015 UTC,"Maintaining 4790k single-threaded performance would necessitate maintaining 4790k clocks.  Unless Intel is planning on doubling their TDP and packaging a 3lb heatsink with it, this isn't a reasonable expectation."
hardware,3awq64,hughJ-,1 point,Wed Jun 24 19:23:28 2015 UTC,Fair enough. But that leaves open the issue that there remains a gap of top tier single-thread performance without an IGP. The -E series and E3 Xeons don't fully fill that gap.
hardware,3awq64,longshot2025,2,Wed Jun 24 19:45:43 2015 UTC,"Not having a product line of unique silicon to serve the subset of enthusiast users that don't like the idea of having a vestigial IGP sitting under the heat spreader isn't an ""issue"".  Removing the IGP doesn't make the chips faster.  Replacing the IGP with more cores makes your clocks (single-threaded perf) worse.  Adding a brand new line of silicon that fragments Intel's product lineup, complicates their OEM contracts, and cannibalizes their retail sales would cost Intel (and the consumers) more money."
hardware,3awq64,hughJ-,7,Wed Jun 24 22:39:01 2015 UTC,"If Intel made a...   While they're making your chip, I would like one of those 18core Xeon dies, albeit have 30meg and 14cores taken off and spend that empty space on a 4x boost in thread performance."
hardware,3awq64,hughJ-,0,Wed Jun 24 10:24:52 2015 UTC,How the fuck re they gonna boost a thread by 4x?
hardware,3awq64,dylan522p,3,Wed Jun 24 13:42:39 2015 UTC,gypsy magic obviously
hardware,3awq64,reynardtfox,2,Wed Jun 24 13:47:13 2015 UTC,Woosh? :P
hardware,3awq64,hughJ-,1 point,Wed Jun 24 18:00:43 2015 UTC,Yeah I dumb lol
hardware,3awq64,dylan522p,2,Wed Jun 24 19:03:39 2015 UTC,"I've heard many people get their Haswell-E chips up past 4.4ghz.   Unless your 4690k can exceed that, it's moot.   As for pricing, the 5820 is around 350, and the 4970k is around 330. 20 bucks for 2 more logical cores?   The motherboards and memory carry more premiums over the cpu, but memory is coming down and will go even lower with Skylake."
hardware,3awq64,Seclorum,0,Wed Jun 24 15:30:10 2015 UTC,"I've seen people get the i5 4690K past 5 ghz, so there's that."
hardware,3awq64,skilliard4,2,Wed Jun 24 17:23:55 2015 UTC,And how much extra performance will you really get out of an extra 600mhz?   I've equally heard of some -E processors going upwards of 4.8-5ghz but most can be guaranteed 4.4 at least.
hardware,3awq64,Seclorum,2,Wed Jun 24 17:52:07 2015 UTC,"The $200 i5 4690K outperforms the $1000 i7 5960x on individual cores.   I'm guessing that's because you didn't account for the 4690K's base clock of 3.5Ghz vs. 3.0 for the 5960X, and that's because intel steps down the speeds on the high-core chips to keep the TDP sane. Intel has 18 core Xeons but the absolute fastest one you can get is only 2.4GHz, and unlike the 5960X, it's not unlocked/overclockable. Overclock that 5960 to 4690 speeds and try again (which is very easy, especially since the 5960 has a soldered heat spreader which dissipates heat much faster than the garbage TIM underneath the heatspreader on ivy/haswell mainstream chips.) If you're looking for single core performance, I think you can get dual core haswells that come out of the box even faster than the 4690. All you can say is the 4690 is a good middle-ground chip, and intel wants IGPs on all its middle-ground CPUs just so the capability is there. If you want to pay for nothing but raw CPU at this point, you'll have to look at the workstation options."
hardware,3awq64,therealab,1 point,Wed Jun 24 13:46:18 2015 UTC,X5650 reporting in at 4.2ghz on 12 threads.  Running 975 on cinebench and plowing through all modern games for $80.
hardware,3awq64,PhilipK_Dick,3,Wed Jun 24 15:41:19 2015 UTC,"Using the GPU for accelerating the rendering of normal Desktop UI, Websites, Processing tasks (eg. Video decoding) etc. is becoming more common place and we are also moving quickly to high resolution displays that require a decent GPU. So it makes sense to improve GPU performance as well so that CPU/GPU on one chip is basically all you need for a good Desktop experience, excluding playing intensive games of course."
hardware,3awq64,barthw,1 point,Wed Jun 24 07:37:04 2015 UTC,"GPGPU for desktop tasks is a hot topic for research and everybody's working to get the infrastructure in place to make it possible, but it's not actually happening in the wild. Your GPU is still just getting used as a GPU by almost every piece of typical desktop software. It's a long ways away from IGPs pulling their weight by accelerating GPGPU tasks."
hardware,3awq64,wtallis,1 point,Wed Jun 24 17:45:32 2015 UTC,"Video decode is also handled by the GPU or iGPU. If you want to decode that sweet 4k video, your GPU or iGPU needs to be up to the task."
hardware,3awq64,Dark_Crystal,1 point,Wed Jun 24 19:12:46 2015 UTC,No. Video decode is in practice almost never handled as a GPGPU task; it's done by fixed-function hardware that doesn't scale across the product line.
hardware,3awq64,wtallis,2,Wed Jun 24 21:11:29 2015 UTC,"did I say GPGPU? No, I did not. I said GPU/iGPU."
hardware,3awq64,Dark_Crystal,2,Wed Jun 24 22:00:19 2015 UTC,"Meanwhile, everyone else in the thread was talking about GPGPU, and you implied that bigger GPUs have an advantage for video decoding, which is only true in a GPGPU context."
hardware,3awq64,wtallis,3,Wed Jun 24 22:22:26 2015 UTC,"Everyone else? no. Some people are trying to make the point that any advancement (or even the presence of) an iGPU is somehow pointless, or less worthy than a 5th or 6th core (despite the fact that that would require a higher TDP or cutting singe core speeds). There are also some that seem to think that you can just throw more transistors at a core to make it faster somehow."
hardware,3awq64,Dark_Crystal,0,Wed Jun 24 23:19:13 2015 UTC,Even new Celerons can hardware decode 4k video at this point. So your point doesn't hold water.
hardware,3awq64,quadrahelix,1 point,Wed Jun 24 20:40:49 2015 UTC,"Sure if you want to tie up the CPU doing something it is comparatively terrible at. Oh, but wait, newish Celerons have an iGPU, which is what does the [de|en]coding. Try playing 4K video on a xeon with a worthless (for 3d) video device typically used for remote management and basic local use."
hardware,3awq64,Dark_Crystal,0,Wed Jun 24 21:59:03 2015 UTC,"Not talking about GPGPU specifically. Look at the Retina Macbooks for example. For the resolutions they are running at, the iGPU is hardly up to the task especially if you start using external monitors. OSX also handles UI scaling really well by rendering the target resolution x2 and then downscaling (it's effectively supersampling) back to the native resolution. This looks super crisp and avoids all the problems Windows has with UI scaling but it also taxes the GPU a lot which is the reason why many Retina owners (same Problem on the 5K iMac) complain about UI lag and dropped frames.  It's a main reason i moved from a Macbook Pro to a Macbook Air which has the low resolution but no lag problems. With the dawn of 4K external Monitors i can't wait for the iGPUs to get more powerful because it's the only way to not sacrifice on battery life.  I obviously do not care about it on my gaming rig, but things like the Intel NUC are becoming more commonplace as Desktop PC's and such a tiny box handling dual 4K screens would be quite remarkable."
hardware,3awq64,barthw,5,Thu Jun 25 06:22:22 2015 UTC,is this related to the end of moore's law in any way
hardware,3awq64,courtjesters,4,Wed Jun 24 11:02:30 2015 UTC,"Sort of, it'll come to an end soon unless we get some big paradigm changes (switching away from silicon basically) working. If we do, this'll be just a hiccup and we could well catch up later."
hardware,3awq64,bphase,4,Wed Jun 24 15:30:02 2015 UTC,"For mainstream CPUs yes, for GPUs and Xeons it holds up pretty well.  On the other hand, if Intel decided to pack more cores in there instead of the iGPU i think moores law  would still hold up. you would still see nearly the same performance increase as before CPUS hit the 4-5Ghz wall.  If we look at the top of the line xeons, they used the free space to get more cores on there so you still get the improvements.  Edit: forgot what Moores Law was really about.  tl;dr  Single cores don't get faster now as they did earlier"
hardware,3awq64,XorFish,3,Wed Jun 24 16:53:19 2015 UTC,"No. Moore's law is about transistor density, it has nothing to do with single core performance. That is an another issue entirely, and basically needs higher clock speeds to improve significantly.  It's just that after 7/5 nm or so, it just really won't be possible to scale current technology down any more."
hardware,3awq64,bphase,1 point,Wed Jun 24 17:22:18 2015 UTC,"Correct me if I'm wrong, but Moore's Law is actually that transistor density doubles every two years, not the performance. Moore's Law doesn't discriminate between how many cores there are, or even whether the transistors are part of the CPU or GPU - they're still transistors.  An interesting fact is that Moore's Law is a self-fulfilling prophecy. Originally Moore noticed a correlation between the increase in transistor counts and stated his law. Since then, manufacturer's efforts have been focused on sticking as close as they can to the Moore's Law slope. As time goes on it takes more and more R&D resources for increasingly meagre improvements."
hardware,3awq64,ShinyCyril,3,Wed Jun 24 17:27:09 2015 UTC,Is Kaby Lake essentially going to be another Haswell Refresh? Do we know if any features Kaby Lake will have that Skylake will not?
hardware,3awq64,sk9592,7,Wed Jun 24 14:42:31 2015 UTC,I wonder how the folks at AMD feel about this announcement.
hardware,3awq64,ummokaysoon,7,Wed Jun 24 04:18:00 2015 UTC,"It's good news for AMD. If Intel truly has hit a wall when it comes to die-shrinking, AMD might actually reach parity for once in their existence. The closest they ever came was in 2009 or so, and they've been steadily a step behind ever since."
hardware,3awq64,Oafah,6,Wed Jun 24 11:16:04 2015 UTC,"The closest they ever came was in 2009 or so   So not the Athlon era, then?"
hardware,3awq64,DiscoYou,6,Wed Jun 24 13:30:00 2015 UTC,"The Phenom II is the most recent example I can recall of AMD pushing Intel to their creative limits, and that was around 2009."
hardware,3awq64,Oafah,8,Wed Jun 24 13:36:06 2015 UTC,"Your post suggested to me that AMD has never been equal or ahead, whereas the Athlons were ahead, I thought."
hardware,3awq64,DiscoYou,0,Wed Jun 24 13:54:31 2015 UTC,"The Athlon x2 series was certainly better than the Pentium D, but at a much higher cost at the time. I don't really call that ""parity"", much is the same way I wouldn't even begin to compare Intel's 5960X to the 9590. Totally different animals."
hardware,3awq64,Oafah,5,Wed Jun 24 13:58:30 2015 UTC,"I was thinking more of the Athlon 64 3200+ era, where a 3200+ Athlon was cheaper than a 3.2 ghz Pentium 4, whilst offering similar/ more performance."
hardware,3awq64,DiscoYou,4,Wed Jun 24 14:03:15 2015 UTC,"And before that, the early Athlon XPs were very competitive against the RDRAM-based P4 platform.  Really, every major architectural change AMD has done dating back to the NexGen acquisition except for Bulldozer has been a big win that has allowed them to temporarily erase Intel's fab advantage."
hardware,3awq64,wtallis,-1,Wed Jun 24 17:49:59 2015 UTC,"You might be right, but ten years is a long time. I can't honestly remember that far back."
hardware,3awq64,Oafah,4,Wed Jun 24 14:10:10 2015 UTC,My opty 165 remembers. Socket 939 was awesome.
hardware,3awq64,markatto,1 point,Wed Jun 24 15:10:50 2015 UTC,I'm pretty sure the Athlon era was the only time AMD actually exceeded Intel in sales/performance.
hardware,3awq64,salgat,1 point,Thu Jun 25 12:08:08 2015 UTC,I didn't suggest the Phenom II series exceed Intel's offerings. It's simply the last time they were even comparable.
hardware,3awq64,Oafah,1 point,Thu Jun 25 12:14:14 2015 UTC,"You said they closest they've ever come, not the most recent time they came close."
hardware,3awq64,salgat,3,Thu Jun 25 12:26:10 2015 UTC,"That name doesn't ring true. Maybe a bad translation or speculative rumour, but it's a shit name for a product.  Edit: It's now being referred to as Kabi Lake - which sounds a bit better. There's a Kabinakagami Lake in Ontario, Canada - Also a Broadwell Lake, and a Blue Sky Lake."
hardware,3awq64,bondiben,7,Wed Jun 24 09:37:32 2015 UTC,It's not the product name though.
hardware,3awq64,Phantom_Absolute,3,Wed Jun 24 11:19:14 2015 UTC,"Should have called something like ""Drylake"" or ""Greenlake"" or ""Clearlake"" or ""Springlake"".   Something along those lines.   Compared to Sandy Bridge, Ivy Bridge, Haswell, Broadwell, Skylake, and Cannonlake, this code name sounds very out of place."
hardware,3awq64,aziridine86,2,Thu Jun 25 04:24:07 2015 UTC,Great...
hardware,3awq64,III-V,3,Wed Jun 24 03:44:20 2015 UTC,"Wasn't ""cannonlake"" supposed to be the name of the next tick/tock?"
hardware,3awq64,JimSkills,10,Wed Jun 24 05:50:21 2015 UTC,"Cannonlake is the die shrink of Skylake. But now Intel announced that there would be a revision of Skylake that is not a die shrink before Cannonlake. So basically Intel just went Tock(Skylake)-Tock(Kaby Lake)-Tick(Cannonlake), except the second tock is not a new architecture either. In the end, this means Intel is not following their Tick-Tock approach anymore."
hardware,3awq64,kimmitanto,9,Wed Jun 24 05:59:31 2015 UTC,Isn't that exactly what happened with devils canyon?
hardware,3awq64,ZeM3D,2,Wed Jun 24 07:52:12 2015 UTC,"Sort of. It's more like the Haswell Refresh as a whole, really. Devil's Canyon was just 3 processors."
hardware,3awq64,Oafah,3,Wed Jun 24 11:17:09 2015 UTC,Wait so what is Kaby Lake supposed to be compared to Skylake then?  What improvements does it offer (on mobile right now)?
hardware,3awq64,reynardtfox,2,Wed Jun 24 13:49:12 2015 UTC,"Skylake, not Sky Bridge"
hardware,3awq64,everyZig,3,Wed Jun 24 06:08:49 2015 UTC,"Duh right, dunno why I had bridge in mind..."
hardware,3awq64,kimmitanto,8,Wed Jun 24 06:14:36 2015 UTC,"Because in order to reach the Skylake, you need a Skybridge."
hardware,3awq64,Rediterorista,3,Wed Jun 24 06:49:09 2015 UTC,"Did you open the article?   Intel Corp. has changed its roadmap once again and delayed its code-named “Cannonlake” processors to an unknown date. Next year the company will introduce “Kaby Lake” processors made using 14nm FinFET process technology.  While the reasons why Intel decided to delay or even cancel “Cannonlake” processors are unknown, it is highly likely that motive behind the change of plans is the postponement of Intel’s 10nm fabrication process."
hardware,3awq64,JimSkills,3,Wed Jun 24 06:01:19 2015 UTC,Wouldn't open on my phone sorry - thanks for the info!
hardware,3awq64,bigboy678,1 point,Wed Jun 24 07:21:32 2015 UTC,hmm with the amount of issues Intel had with 14nm cpus and now 10nm cpus something tells me we are reaching the limits of what silicon can do sooner than the industry expected. I mean without a question intel has the most advanced fabs in the world with some of the best engineers and they are struggling with this. silicon was suppose to scale down easily to 7nm they said. something tells me thats not the case
hardware,3awq64,PhilipK_Dick,2,Wed Jun 24 15:32:45 2015 UTC,I don't think anyone thought things were going to get easy by this point.  They have said they will be able to scale down to 7nm on silicon but that shit is hard and getting harder...
hardware,3awq64,Ikinhaszkarmakplx2,0,Wed Jun 24 15:47:58 2015 UTC,So what...Ima wait instead of upgrading my 2600k? Goes still well at 4.5 Ghz....for now. Limiting factor most likely is my GPU gtx670.
hardware,3awq64,Oafah,8,Wed Jun 24 07:38:58 2015 UTC,A 2600K @ 4.5GHz is nowhere near a bottleneck for your GTX 670. It's not even worth a second thought.
hardware,3awq64,everyZig,5,Wed Jun 24 11:17:55 2015 UTC,"FYI, the 670 is indeed the bottleneck in the scenario, he will not experience CPU bottlenecking, therefore the GPU is the bottleneck.  People are really starting to miss-use the term bottleneck badly on here."
hardware,3awq64,Oafah,2,Wed Jun 24 11:35:48 2015 UTC,I understand the concept of bottlenecking. I just misspoke and changed my sentence mid-thought. I'll edit it.
hardware,3awq64,everyZig,1 point,Wed Jun 24 11:39:44 2015 UTC,"Ah fine, i see a lot of people mixing it up in /r/buildapc these days.  Have a good day!"
hardware,3awq64,Hariooo,1 point,Wed Jun 24 11:42:18 2015 UTC,if you only play DOTA or something it kinda is but that's mostly moot unless you're trying to run those games 144hz at 1440p
hardware,3awq64,Oafah,0,Wed Jun 24 14:43:08 2015 UTC,"Totally baseless comment, unsupported by evidence.  A GTX 670 does not have the graphical horsepower to demand excess instruction from an i7 2600K. Period."
hardware,3awq64,Hariooo,2,Wed Jun 24 14:45:11 2015 UTC,DOTA is pretty well known to be CPU bottlenecked for minimum framerates. this is not in dispute lol
hardware,3awq64,Oafah,0,Wed Jun 24 14:52:46 2015 UTC,"That's a limitation of the CPU, and has nothing to do with its relationship to the GPU. His question was about his particular GPU being bottlenecked by his i7 2600K. He made no query about the performance value of his CPU independent of it.  I think you really need to understand the definition of the term ""bottlenecking"" before you contribute here."
hardware,3awq64,Hariooo,3,Wed Jun 24 14:57:03 2015 UTC,"??? in the niche case of ""i only want more dota fps"", upgrading the CPU will increase your minimum framerates. i'm not saying your original answer was wrong i'm just making note of a specific scenario  that's all you need to know."
hardware,3awq64,Oafah,0,Wed Jun 24 15:02:03 2015 UTC,"That's not what he was asking about. Obviously there are better CPUs for CPU-intensive tasks (like some multiplayer games) than an i7 2600K. He was remarking on whether his GTX 670 would be bottlenecked by it, and the answer is a resounding ""no"", in all scenarios. The GTX 670 is not being limited by the i7 2600K in DOTA2. DOTA2 might be limited by the i7 2600K in regards where graphical performance is not a factor, but the GPU itself is not serving as a stress-point for the CPU. At all."
hardware,3awq64,sk9592,0,Wed Jun 24 15:16:00 2015 UTC,"Haha, yep. Still sitting on my i5-2500k@5.0GHz. I'm really trying to look for a reason to upgrade and can't find one. I would like to give Intel or AMD my money, they just need to give me a reason. They can't seem to come up with one."
hardware,3awq64,ptd163,0,Wed Jun 24 14:45:57 2015 UTC,I thought Canonlake was Skylake's successor.
hardware,3awq64,is_this_4chon,-23,Wed Jun 24 18:23:55 2015 UTC,By then Apple will have gone fully in house or acquired AMD for a bag of skittles.  Desktop is dead and mobile is going to rt embedded.
hardware,3awq64,tdub2112,12,Wed Jun 24 05:49:10 2015 UTC,I'd gladly pay two bags of skittles for AMD.
hardware,3awq64,Pesceman3,9,Wed Jun 24 06:23:27 2015 UTC,Well for the price of two bags of skittles you can buy one share of AMD.  Close enough
hardware,3awq64,Teethpasta,2,Wed Jun 24 06:34:07 2015 UTC,LOL desktop is dead. Have fun editing videos and doing actual work on your ARM tablet.
hardware,3awq64,is_this_4chon,0,Wed Jun 24 18:53:41 2015 UTC,Implying cpu bound tasks aren't going to cloud executed.   Stay pleb
hardware,3awq64,Teethpasta,1 point,Thu Jun 25 03:39:21 2015 UTC,"LOL cloud executed. You realize the cloud is not some magic force? What is going to be powering the ""cloud?"""
hardware,3awq64,is_this_4chon,-2,Thu Jun 25 04:55:14 2015 UTC,"Certainly no OC'd Extreme Edition chip that you Intel fanboys get raging boners for.   Instead it will be a commodity like RAM or disk space.    Fluctuating on demand and only network I/O bound.    Did you think desktop procs were going to eventually get to 0.4nm @ 8ghz and 24 cores in your HP Pavillion, my child?  If you don't understand this, then you are 12.   Please respond if you'd like to further your education."
hardware,3awq64,Teethpasta,2,Thu Jun 25 14:49:53 2015 UTC,Lol have fun with that data plan. You're clueless. That makes no sense at all. And Intel already powers most servers and that doesn't seem to be changing does it?
hardware,3azeh9,zmeul,4,Wed Jun 24 18:55:37 2015 UTC,Too bad. Had several of their boards and they worked extremely well.
hardware,3azeh9,stapler8,1 point,Wed Jun 24 19:10:29 2015 UTC,"I can sort of recall when I opened my first crappy PC at work (to steal a CPU for another machine, or maybe add memory), and it had an ECS mobo.  That seemed to scream ""Yup, it's official, Compaq/HP has reached rock bottom"" to me.  I can recall when the only place you found ECS boards was the worst of the worst of white box systems.  It wasn't even that the boards were bad (they were though) but they always tended to be a little zany (weird chipsets, strange design compromises, and nothing resembling support)"
hardware,3azeh9,Bounty1Berry,1 point,Thu Jun 25 03:21:28 2015 UTC,"I dunno. Never had any issues with my boards. Had one of their high end models and two of their budget ones, and they worked flawlessly."
hardware,3azeh9,stapler8,1 point,Thu Jun 25 03:23:00 2015 UTC,"This story isn't true, you can thank Dodgy Times for the rumor mongering."
hardware,3azeh9,xtothemess,1 point,Thu Jun 25 13:28:15 2015 UTC,"My story or his story?  If you're talking about mine, I still have the boards as proof."
hardware,3azeh9,stapler8,1 point,Thu Jun 25 15:41:26 2015 UTC,"Oh my bad I should have specified, ECS today sent out a press release stating that the story isn't true at all, and that they are still in the business. The letter is signed by the President of ECS."
hardware,3azeh9,xtothemess,1 point,Thu Jun 25 23:49:03 2015 UTC,"Oh, okay. Awesome, I'll look forward to an ECS board for Cannonlake."
hardware,3azeh9,stapler8,1 point,Fri Jun 26 02:03:39 2015 UTC,I've only heard bad things about them to be honest.
hardware,3azeh9,39th_Step,1 point,Thu Jun 25 18:24:46 2015 UTC,"Ahh the old k7s5a... It was the best of times, it was the worst of times."
hardware,3axv1t,4980347698,1 point,Wed Jun 24 11:11:27 2015 UTC,Worst codename ever.
hardware,3axv1t,RAIDguy,1 point,Thu Jun 25 06:36:24 2015 UTC,Wat.  Skylake is a 14nm process. It was never going to be 10nm until the die shrink in Cannonlake.
hardware,3ay7bz,TheRealGecko,2,Wed Jun 24 13:28:47 2015 UTC,"Extracts from the conclusion:    At the time of writing, the latest information we have is that the card will launch for $649 USD, the same as the GTX 980 Ti, and will be available in the UK for around £510, which leaves the GTX 980 Ti at around £30, or 6 percent, more expensive. It also makes Fury X about a third more expensive than a GTX 980, prices for which typically start at £380.  If you're gaming at 1080p or 1440p, the GTX 980 Ti offers better value for money. The two cards have price parity but Nvidia's is faster and significantly better at overclocking, too. With pre-overclocked versions readily available, the performance difference will be even higher. There's some argument for UK customers to opt for the Fury X at 1440p, especially if you're really concerned about card length or noise. That said, third-party coolers for Nvidia's hardware are often very quiet too, and many mini-ITX cases can still house long cards and you won't need to mount a radiator (though that won't be an issue in many small cases either). The Fury X is an awesome card in many ways, and we certainly commend AMD for tackling the demands of 4K head-on. the trouble is it lacks that killer feature or performance that many were probably hoping for. Even if it ends up being a bit more than £510, say £540 like the GTX 980 Ti, it gives Nvidia's card a run for its money and does an all-round good job.   We're thus just about satisfied enough to give it an Approved award, with the caveat that it's only really worth considering for 4K gamers. With 4K screens dropping in price all the time, this isn't necessarily a bad thing. The card is clearly competitive in this scenario, but even so a pre-overclocked, custom-cooled GTX 980 Ti is, in most instances, still going to be the better option."
hardware,3ay7bz,Salsadips,3,Wed Jun 24 13:31:33 2015 UTC,I live in the UK and i was dreading this.  US cost is 649 USD  UK cost is the equivalent of 800 USD. What the fuck.
hardware,3ay7bz,musef1,4,Wed Jun 24 14:40:22 2015 UTC,US cost probably doesn't include tax.
hardware,3ay7bz,waiting4myteeth,2,Wed Jun 24 15:21:08 2015 UTC,On the upside the UK prices of AMD cards are usually available at steep discounts a few months after release - unlike nVidia cards.  That's not what anyone thinking of buying now wants to hear though.
hardware,3ay7bz,bloodspore,1 point,Wed Jun 24 15:31:29 2015 UTC,"Yep, cheapest (Sapphire) Fury X on overclockers is £509.99. Since it's overclockers, add a £10 delivery charge on top of that, so it's basically £519.99 for a Fury X.  The most expensive is £649.99.   UK resident here, too. It's not looking good."
hardware,3ay7bz,Thunder_Bastard,1 point,Wed Jun 24 14:50:51 2015 UTC,"What? That is so fucked up, why is the powercolor card £150 more when the only difference is the sticker in the fan. Lol"
hardware,3b39bx,noobas4urus,21,Thu Jun 25 16:44:00 2015 UTC,"Kitguru is literally making this all up; notice how there are no sources, how Kitguru just speculates on how nVidia could boost sales because it's logical, and how nVidia had no comment on this same article? What a terrible piece of click-bait."
hardware,3b39bx,Alarchy,3,Thu Jun 25 17:00:26 2015 UTC,"Lets see, clickbaity article title...  Completely baseless speculation and rumor...  No verifiable facts...  No good reasons given...  Yeah that's Kitguru."
hardware,3b39bx,Seclorum,2,Thu Jun 25 17:57:44 2015 UTC,That site is terrible. I swear every non-review article ive seen at KitGuru is baseless speculation. No wonder AMD yanked their Fury X sample
hardware,3b39bx,dravell,3,Thu Jun 25 22:15:36 2015 UTC,"yeah, so the US won't get repricing, but they might lower prices in other countries, but the odds of that happening are low given that Fury X is basically sold out everywhere already..."
hardware,3b39bx,TehRoot,-2,Thu Jun 25 16:48:18 2015 UTC,C'mon slight cut to the 970!
hardware,3b39bx,FitnessRegiment,4,Thu Jun 25 16:53:06 2015 UTC,I highly doubt it... the Fury X isn't any danger to the 980 Ti and is already sold out everywhere. Why would they decrease price if it's constantly out of stock?
hardware,3b39bx,Stingray88,2,Thu Jun 25 16:47:11 2015 UTC,"They 980Ti has sold out three times since launch. Every shipment sells out within half a day. You're completely right, nVidia isn't going to lower their price when they're still selling out."
hardware,3b39bx,TaintedSquirrel,1 point,Thu Jun 25 18:39:15 2015 UTC,"How are GM200 yields?  Titan X was selling out since its launch, too.  I don't think the card is that popular.  GTX 970 is probably selling more units and those have no stock problems whatsoever."
hardware,3b39bx,Stingray88,1 point,Thu Jun 25 20:00:41 2015 UTC,"The 970 did have stock issues when it first released. It's not remotely new anymore, so naturally it has plenty of stock by now. There's also many versions available by now, where as some of the 980 Ti versions aren't even out yet.   It takes a while for the supply to catch up with the demand, even if the yield is good.  Regardless it doesn't matter what the yields are or how popular something is. If it's still selling out, there's no reason to lower the price."
hardware,3b39bx,jforce321,1 point,Thu Jun 25 20:06:58 2015 UTC,"from what I can read of that I'd say its just for the foreign markets, as the us prices are more than competitive enough with amds offerings in all honesty."
hardware,3b39bx,Meanest_Phlebotomist,1 point,Thu Jun 25 17:27:29 2015 UTC,I think their prices are pretty good (in the US). 980Ti certainly doesn't need a price cut at this point (pending the Fury release in mid July).
hardware,3b39bx,Eruntien,1 point,Thu Jun 25 17:48:31 2015 UTC,There are no sources :/.  Even if we had some sources woulsnt it be more logical to cut the price of AMDs cards considering there performance per dollar is much less.
hardware,3b39bx,tedlasman,1 point,Mon Jun 29 14:41:50 2015 UTC,Yay!
hardware,3axijy,Devo7ion,6,Wed Jun 24 08:11:02 2015 UTC,"I think you are confusing TDP with Power Draw. The Sapphire one permits up to 375w, so that you can overclock it more... the others probably don't permit the same thing."
hardware,3axijy,VanayadGaming,3,Wed Jun 24 10:05:54 2015 UTC,So what you're saying is the effective power draw should be about the same but the Sapphire one would theoretically allow you to overclock/overvolt higher?
hardware,3axijy,VanayadGaming,2,Wed Jun 24 11:31:13 2015 UTC,"Yes, exactly."
hardware,3axijy,AMW1011,5,Wed Jun 24 11:42:40 2015 UTC,"Depends on how they're setup, but essentially some cards come with higher voltage for better clockspeeds and overclockimg out if the box."
hardware,3axijy,HavocInferno,2,Wed Jun 24 08:16:11 2015 UTC,"Overclocks exponentially (I think?) increase power draw.   Overvolting increases it too.  The Tri-X is likely overclocked compared to the reference 390X, thus it draws more power."
hardware,3axijy,bphase,3,Wed Jun 24 08:21:02 2015 UTC,"Overclocks exponentially (I think?) increase power draw.  Overvolting increases it too.   Overclocking increases it linearly, but voltage increases power draw squared. Meaning 10% more voltage is 21% more power draw etc."
hardware,3axijy,Liam2349,1 point,Wed Jun 24 11:02:23 2015 UTC,"They likely just run on a higher voltage.  Power = current * voltage, so running an overclock with a higher voltage increases the power by that relationship.  I think my Sapphire Vapor-X 290 draws ~375W at the maximum voltage. It also uses higher-quality components to allow the higher overclocks (higher voltage)."
hardware,3axijy,everyZig,2,Wed Jun 24 10:49:01 2015 UTC,"Power = current * voltage   And current = voltage / resistance, which leads to  Power = (voltage/Resistance) * voltage  So if we assume a GPU behaves like a typical Ohmic resistor, upping the voltage by 1,1x, the power will go up a factor 1.12 which is why you try to keep Vcore as low as possible when OCing, power/heat scales linearly with clockspeed, but quadraticly with voltage"
hardware,3axijy,Liam2349,1 point,Wed Jun 24 12:00:58 2015 UTC,"That depends on the current or the resistance, depending on which equation you're following.  As you overclock, temperatures rise, and this changes the resistance of the components. You know what the new voltage is, but you are not considering how much the resistance has changed by this temperature increase, so it's not quite so straight-forward. If it were a voltage increase at constant resistance, you would be correct, but that's not the case.  Likewise in the example I gave, you need to know the current at the new voltage.  Forgive me if any of this seems incoherent - I've just completed my second year of Physics so do have a decent understanding of these concepts, but did not sleep last night.  EDIT: See here for a little more information, if you're into that. http://hyperphysics.phy-astr.gsu.edu/hbase/electric/restmp.html"
hardware,3axijy,everyZig,1 point,Wed Jun 24 12:17:40 2015 UTC,"Forgive me if any of this seems incoherent - I've just completed my second year of Physics so do have a decent understanding of these concepts, but did not sleep last night.   No worries, ive done eletrical engineering in college, so yeah, i realize there are some assumptions at the base of my formula, if the GPU throttles itself down, or the materials change resistance because of their temperature all of this will obviously change, but the premise that a 10% rise in voltage will produce more then a 10% increase in power/heat is still valid imho"
hardware,3axijy,Liam2349,1 point,Wed Jun 24 12:24:25 2015 UTC,"Maybe. If I can find some relevant specs for resistance I might actually do some calculations when I've slept. I'm curious now.  EDIT: Actually just remembered GPU-Z gives you the power, so no need for any brainwork. Still, I'm probably going to test the difference when I've slept."
hardware,3axijy,namae_nanka,1 point,Wed Jun 24 12:33:54 2015 UTC,"I have read that it rises as the cube of the voltage, at least for cpu."
hardware,3axijy,logged_n_2_say,1 point,Wed Jun 24 16:26:46 2015 UTC,"Regarding your edit: it likely can consume more power when fully pushed, but there can be small variations even in the same model. You would want to look at more in depth reviews for more accurate consumption, but we know its going to be high compared to other cards in the market."
hardware,3avo2q,TaintedSquirrel,14,Tue Jun 23 21:39:25 2015 UTC,"Am i the only person that thinks the active adapters excuse is a cop out?  They have been  using ""active dongles"" as an excuse for years. Has anyone ever seen these DisplayPort dongles?  ""It doesn't matter that our latest flagship GPU doesn't support the latest display standards, because you can just buy a dongle."""
hardware,3avo2q,PinetreeRoad930,6,Wed Jun 24 02:41:02 2015 UTC,I think they were shown at CES: http://img.clubic.com/07848475-photo-displayport-hdmi-2-0-bizlink.jpg
hardware,3avo2q,Exist50,6,Wed Jun 24 03:26:31 2015 UTC,"Yeah, and they aren't out yet. And if they follow the pricing trend for high end active adapters (of which will be required in this case), expect $80+ for an adapter. Which they aren't even including in the box at all."
hardware,3avo2q,Kinaestheticsz,0,Wed Jun 24 06:42:58 2015 UTC,AMD said they are working with companies to push DP-HDMI2.0 adapters. Might be they are also pushing for lower prices.
hardware,3avo2q,HavocInferno,2,Wed Jun 24 07:26:38 2015 UTC,"What an ugly display. Which part is even the adapter? The round black thing, one of the myriad of wires, or the entirety of what's photographed?"
hardware,3avo2q,leafthegreen,2,Wed Jun 24 22:30:36 2015 UTC,The round black thing is a Mac Pro
hardware,3avo2q,Exist50,6,Thu Jun 25 05:00:13 2015 UTC,"I havent seen any prices for those active dongles, but i expect them to be rather expensive, to which i respond with ""fuck that shit"" same ass with apple and their USB 3.1c only laptop, being forced to buy an expensive adapter with your already premium product just sucks."
hardware,3avo2q,everyZig,1 point,Wed Jun 24 06:18:29 2015 UTC,"Apple, come for the shiny new device, stay because you spent 800 dollars on dongles and adapters and don't want them to be a waste."
hardware,3avo2q,TeutorixAleria,1 point,Wed Jun 24 11:15:55 2015 UTC,Only to be met with a new interface standard and having to rebuy all your cables and dongles shortly after buying a new shiny device
hardware,3avo2q,everyZig,2,Wed Jun 24 11:24:08 2015 UTC,"Exactly! Active adapters are a bullshit copout!  People in the market for a high end video card don't want to (nor should they need to) compromise when buying a $500+ video card.  Active adapters increase latency, making them not ideal for gamers. Not to mention, they are also more expensive than a native port or passive adaptor."
hardware,3avo2q,sk9592,1 point,Wed Jun 24 15:06:44 2015 UTC,i think it tends to be price that's usually a bigger issue or some lag that adapters have. DP is on the card so it really should not be an issue with most people anyway
hardware,3avo2q,dinos22,0,Thu Jun 25 04:35:06 2015 UTC,But displayPort is better than hdmi...
hardware,3avo2q,VanayadGaming,1 point,Wed Jun 24 08:50:28 2015 UTC,"That is not at all what i was talking about....  I commented about the dongles because the product guy on the sidelines said something about it being a ""non-issue"" simply because they are making a dongle for people to buy. If they were giving away the dongle it would be a ""non-issue"".  AMD has been touting how great these active dongles are for years, yet they only became a commercially available product a year or so ago. Now they are claiming that they are working on new dongles, and they expect us to believe they will be out this summer?"
hardware,3avo2q,PinetreeRoad930,1 point,Wed Jun 24 12:39:37 2015 UTC,"I don't know if they are good or not, I know for one I am not going to use one. Why limit myself to HDMI 2.0 ?"
hardware,3avo2q,VanayadGaming,2,Wed Jun 24 14:04:22 2015 UTC,"Still missing the point.   It doesn't matter if they are good or not. People who just bought a 600$ GPU that came out less than 24 hours ago, shouldn't be forced to buy anything else to support the latest display standards. Perhaps people want to use HDMI 2.0 for a specific reason."
hardware,3avo2q,PinetreeRoad930,23,Wed Jun 24 23:36:14 2015 UTC,"HDMI 2.0 was a time-to-market decision and they are pushing the active adapters.  4-way CrossFire is supported, you might want to use a custom loop when exceeding 2 or 3 cards.  Fury sounds more like a full Fiji XT based on their response: ""the fury x is the name that we've attached to that set up"" --  NO INFO until we get closer to mid-July.  They repeated old info we already knew.  Can't talk about 8 GB HBM2 yet."
hardware,3avo2q,AssCrackBanditHunter,12,Tue Jun 23 21:46:12 2015 UTC,Really hoping fury is the full Fiji. I will shit myself if there's a card in the 980 price range.pulling.off 980ti performance
hardware,3avo2q,TrptJim,16,Tue Jun 23 22:21:05 2015 UTC,They don't want you to know that right now because it would eat into Fury X sales.  :o)
hardware,3avo2q,TRD099,2,Tue Jun 23 22:23:00 2015 UTC,"I wouldn't see why. The cooling solution is amazing. EVGA's 980Ti Hybrid is about $100 above regular models as well and is sold out. 50c with silence is so worth it to me. A waterblock by it self is $100 or more, but with this you get the whole shebang."
hardware,3avo2q,Beakface,1 point,Wed Jun 24 07:17:22 2015 UTC,"It's all about timing, hopefully they get it right or they might have another r9 290 and 290x thing going on where almost no one buys the 290x because the 290 is almost as good for so much cheaper."
hardware,3avo2q,xandergod,1 point,Wed Jun 24 11:07:54 2015 UTC,If you want (imo) ugly 120mm radiators etc instead of a neat and tidy custom loop.
hardware,3avo2q,YakyPeanut,-2,Wed Jun 24 23:14:00 2015 UTC,"Who wants to bet that partner cards are going to be faster than the X. They shot for high end, missed a bit, but the 980 is dead in the water."
hardware,3avo2q,ZeM3D,4,Tue Jun 23 22:32:07 2015 UTC,"All Fury X's come at 1050 stock core, the non-ref Fury models will probably come at 1100+ so yes they will be faster... Assuming they are Fiji XT."
hardware,3avo2q,YakyPeanut,0,Tue Jun 23 22:33:25 2015 UTC,So... I should wait for Fury? :/
hardware,3avo2q,FredFS456,5,Tue Jun 23 22:47:36 2015 UTC,"With the fury x's cooler, you can hit much better overclocks than what manufacturers are going to ship out their cards at."
hardware,3avo2q,tarheel91,1 point,Tue Jun 23 22:51:18 2015 UTC,Could they make tweaks to the set up that actually make it run better?
hardware,3avo2q,capn_hector,5,Tue Jun 23 23:18:29 2015 UTC,"Well, they could conceivably bin the chips such that they get higher quality and better overclocking performance, but fundamentally, if Fury is Fiji XT, then no - it's the same silicon."
hardware,3avo2q,dylan522p,3,Tue Jun 23 23:25:35 2015 UTC,I'd add that whatever cooler they put on it will be worse than the reference solution on the Fury X.
hardware,3avo2q,YellowCBR,1 point,Wed Jun 24 02:07:17 2015 UTC,"Why couldn't a Fury ship with the X's reference cooler, or a near cousin installed?  There have been plenty of rumors about a water-cooled Fury.  The fact that it's the sole cooler available on the X doesn't preclude it from being used on the Pro.  Those rumors are also the reason I kinda think it will be a cut-down."
hardware,3avo2q,TeutorixAleria,10,Wed Jun 24 20:18:50 2015 UTC,"If you want the fastest GPU with the best cooling, you're better off buying a Fury X and overclocking it yourself.  The non-ref Fury models will be hotter, louder, and have lower OC headroom guaranteed.  BUT you will save $100."
hardware,3avo2q,AMW1011,-3,Tue Jun 23 22:48:30 2015 UTC,"No, what. They would be just as good at worst"
hardware,3avo2q,dinos22,7,Wed Jun 24 01:49:15 2015 UTC,"Except one is liquid cooled with even liquid cooled VRMs, which will surely overclock better. And Im sure AMD already binned the chips for Fury vs Fury X."
hardware,3avo2q,xandergod,1 point,Wed Jun 24 05:14:40 2015 UTC,"Still playing the silicon lottery with the x. Sure better cooling helps but you could buy a fury x and a fury at random and have them both hit the same limit, I'd rather take the $100 discount and live with the extra heat and noise."
hardware,3avo2q,tarheel91,2,Wed Jun 24 11:14:00 2015 UTC,"As long as AMD supplies the Fury X with very good voltage regulation and backend, then no non-ref Fury cards should be able to stand up to the Fury X once both are overclocked.  Watercooling keeps temperatures absurdly low, and the lower the temperature the less leaky the processor meaning more stability at higher voltages/same voltages."
hardware,3avo2q,163941,1 point,Tue Jun 23 23:18:10 2015 UTC,"There won't be non - reference FuryX cards, watch the video"
hardware,3avo2q,klymen,1 point,Wed Jun 24 11:55:25 2015 UTC,I didn't say there would be.
hardware,3avo2q,sk9592,1 point,Wed Jun 24 11:57:53 2015 UTC,"They literally can't be faster on air.  An air cooler is an inherently inferior cooling solution to the water cooling solution on the Fury X.  What you might see by third parties, though, is binning so that they can guarantee a certain clock."
hardware,3avo2q,dinos22,3,Wed Jun 24 02:06:37 2015 UTC,"HDMI 2.0 was a time-to-market decision and they are pushing the active adapters.   But NVIDIA have had HDMI 2.0 support for nine months now. How was this a ""time-to-market"" decision?  Active adapters are not going to enable the HDCP 2.2 support required for UHD video playback, are an additional expense, and are likely to introduce latency.  It's not a solution."
hardware,3avo2q,Logun0,3,Wed Jun 24 15:46:48 2015 UTC,"I felt that someone is his position would be able to speak more confidently about the product? I looked him up but based on this interview, I would have thought he was some random VP."
hardware,3avo2q,nazzo,3,Wed Jun 24 03:13:21 2015 UTC,"The lack of 4-way doesn't bother me and shouldn't bother many other people. 99.9999% of people have no business running more than two video cards.  The 4GB limitation is a bit irksome, but reasonable considering it is the first generation in a major technological change. The ""low"" amount of vram is somewhat mitigated by the fact that it so much faster than GDDR5. 4GB is fine for 1440p gaming, and will still do decently well at 4k.  The lack of HDMI 2.0 is the most glaring oversight. Nvidia has had it in their cards for almost a year now. Don't tell me that you were blindsided by a new standard and had to rush it to the market. HDMI 2.0 is essentially your only option is you want to make a 4K gaming HTPC.  Active adapters are a no go for people who want the best response rates or picture accuracy. Not to mention they are expensive."
hardware,3avo2q,Logun0,1 point,Wed Jun 24 14:50:39 2015 UTC,thanks for sharing my video TaintedSquirrel
hardware,3avo2q,nazzo,-1,Thu Jun 25 00:15:45 2015 UTC,They needed to leak information about that new DP to HDMI adapter before they leaked the FuryX IMO. I've already ordered a 980 Ti when I heard about the lack of HDMI 2.0
hardware,3avo2q,makar1,3,Wed Jun 24 00:03:33 2015 UTC,Are you using a TV as a monitor? I don't understand how that is an issue when almost all modern monitors support Display Port.
hardware,3avo2q,sk9592,10,Wed Jun 24 01:42:56 2015 UTC,"Yes - due to poor eyesight I had to balance screen size with resolution in order to be able to read onscreen text. Then I just said fuck it and bought a 55""LG 4K TV.    This was back in Jan so it has HDMI 2.0 and was ready to rock at 60Hz. I'm not butt hurt about the Fury but I just do not understand why. Hudy said it was a time to market issue but that doesn't make sense to me - HDMI 2.0 has been available for a LONG time.    I still have 2 true monitors (Dell U3011) but I knew that scaling wasn't good enough to be surfing reading and coding at 4K to my liking. So I went with the TV to satisfy multiple requirements both for regular TV watching (yes in 1080p)  and big screen gaming.     In the end I know I'm almost a non-existent market but it's worked out phenomenally for me as I play a lot of older titles. Most everything I play maxes at 30Hz so it will be interesting to see what happens with the 980Ti. For me the trade of 4K resolutions for 30Hz was worth it - but mainly because I knew there was a 60Hz light at the end of the tunnel."
hardware,3awu5q,RTomassi,9,Wed Jun 24 03:31:38 2015 UTC,And here I am with 3 GB DDR2 RAM on an old Thinkpad with a Core 2 Duo. Damn I need to upgrade.
hardware,3awu5q,ummokaysoon,2,Wed Jun 24 04:10:31 2015 UTC,"Greets from my e8400. At least it is a desktop, upping the ram helped a good bit."
hardware,3awu5q,Klorel,2,Wed Jun 24 09:28:20 2015 UTC,3 GB of DDR2 and a dual core turion in my gateway. I feel the pain.
hardware,3awu5q,Sethual,1 point,Wed Jun 24 09:34:35 2015 UTC,Work machine has a Core 2 Duo and 2GBs DDR2. It's blazing fast and has to cache anything over three tabs in Chrome to the page file.
hardware,3awu5q,Purp3L,3,Thu Jun 25 01:18:06 2015 UTC,"Pfft, amateurs.  You're kind of an idiot if you put this amount of memory in a machine and don't get ECC.  All that silicon's a nice easy target for cosmic rays, and it doesn't take much of a defect to make one out of 2.1 trillion components quietly flip a 0 to a 1 occasionally.  I mean, you're probably not getting all that memory because you're doing nothing of value with the machine, right?  There's a reason even small servers almost universally have ECC RAM.  You pay a fairly modest premium and effectively eliminate a whole class of problems.  You also get to feel smug every time a correction event is logged.  I felt smug about 60 times last year."
hardware,3awu5q,Freeky,2,Wed Jun 24 16:14:10 2015 UTC,"At this point the author said it best, there really isn't a whole lot of use for consumers with that much ram. He just took a chunk of it and made a ramdisk in software which works but it's not for everyone.   Although I shudder to think how many tabs you can have open in chrome with that much ram."
hardware,3awu5q,Seclorum,7,Wed Jun 24 05:09:56 2015 UTC,16 if you leave it for an hour.
hardware,3awu5q,Trollatopoulous,2,Wed Jun 24 06:40:21 2015 UTC,"Oh man, I would love a Ramdisk for Rome 2 w/ DeI mod, the loading times are INSANE even with an SSD."
hardware,3awu5q,Freeky,2,Wed Jun 24 12:15:58 2015 UTC,"Like many games it's probably more CPU limited than anything.  A RAM disk probably isn't going to help all that much.  You effectively get that already just with OS caching.  With 32GB there's plenty of games that basically do no reads/writes the second time you start them - they execute straight out of RAM, and they still take ages to start."
hardware,3awu5q,Trollatopoulous,1 point,Wed Jun 24 16:00:01 2015 UTC,"I'm pretty sure the loading times have to do with where you keep the game, not the CPU."
hardware,3awu5q,Freeky,1 point,Wed Jun 24 17:59:06 2015 UTC,"Easily demonstrated otherwise.  If you move a game from an SSD to a HDD the load times don't generally quadruple, and a cached game running directly out of memory doesn't start 10x quicker.  You run into diminishing returns because they're rapidly CPU bound - uncompressing, parsing and compiling things, running scripts, building data structures etc."
hardware,3awu5q,Trollatopoulous,1 point,Wed Jun 24 19:12:59 2015 UTC,"If it's even twice faster, that's really good. But since we don't have any raw numbers it's speculation anyway."
hardware,3awu5q,Freeky,1 point,Wed Jun 24 21:10:39 2015 UTC,"Well, I've been loading games from HD's, SSDs and out of memory for years, and have pretty detailed system monitoring available at all times, so I can see when loading stuff is CPU limited.  Even out of cache (so, zero physical IO), the typical experience is pegged cores, and load times only a little bit better than usual.  If it's slow loading off an SSD, it's probably going to be slow loading out of a RAM disk too."
hardware,3awu5q,Anaron,2,Wed Jun 24 21:43:45 2015 UTC,"Few people need more than 16GB of RAM, and some can even get by with 8.   I don't have any issues with 8GB DDR3-2400 RAM. I think 16GB is overkill for gaming unless you're doing specialized tasks (e.g. photoshop work).  128GB though. Damn."
hardware,3awu5q,rnawky,4,Wed Jun 24 05:08:24 2015 UTC,I have 32GB on my desktop and it's still not enough.  Virtual Machines =/
hardware,3awu5q,dustofnations,4,Wed Jun 24 05:36:07 2015 UTC,You need some containers in your life :o)
hardware,3awu5q,kingpool,2,Wed Jun 24 06:14:35 2015 UTC,for tears?
hardware,3awu5q,rnawky,1 point,Wed Jun 24 09:55:32 2015 UTC,I do for Linux. Windows not so much.
hardware,3awu5q,dustofnations,1 point,Wed Jun 24 13:02:29 2015 UTC,Rumour has it that such a thing will arrive for windows
hardware,3awu5q,poematik,2,Wed Jun 24 13:28:39 2015 UTC,leave 32 for OS and the rest goes to a 4500MB/s ramdisk partition lol
hardware,3awu5q,Anaron,2,Wed Jun 24 06:04:54 2015 UTC,The speed is nice but say goodbye to your data if the power goes out.
hardware,3awu5q,Anaron,5,Wed Jun 24 06:27:45 2015 UTC,It's 2015 we have 8 wH cells for 20 cents why is it that nobody seems to have UPS's?
hardware,3awu5q,anon1821,1 point,Wed Jun 24 06:42:58 2015 UTC,"Clearly, I'm not ready for a power outage. :O"
hardware,3awu5q,anon1821,1 point,Wed Jun 24 06:44:33 2015 UTC,"Most consumer stuff can handle an outage fairly gracefully, but we have in-house UPS's at each computer and a building wide UPS. It's not worth the potential down time."
hardware,3awu5q,LightShadow,1 point,Wed Jun 24 06:59:04 2015 UTC,How often should one replace batteries inside their UPS ?  My friend hasn't serviced his UPS in like 5 years with 24/7 operation and one day he went to his job and the place smelled burnt plastic from within the UPS and also the PC would not turn on
hardware,3awu5q,poematik,2,Wed Jun 24 07:24:58 2015 UTC,"We have a monitoring system that let's the batteries drain and recharge about 20 percent. If they drain to fast we swap it out. Under normal use they last around 2-3 years, but these are lithium based not lead."
hardware,3awu5q,Walter_Bishop_PhD,1 point,Wed Jun 24 09:49:57 2015 UTC,thanks for answering
hardware,3awu5q,Anaron,1 point,Wed Jun 24 09:50:57 2015 UTC,"One of the better purchases I've made.  I have a beefier one on my desktop that can handle 30 minutes on a single monitor easily. I was so impressed I threw another cheaper one on the server box that can last ~5 minutes without a monitor, which gives it plenty of time to shutdown everything gracefully and start back up automatically when power is restored.  Loving the future."
hardware,3awu5q,Anaron,1 point,Wed Jun 24 17:37:35 2015 UTC,"well yeah, its only for temporary projects/stuff."
hardware,3awu5q,Beckneard,1 point,Wed Jun 24 06:29:56 2015 UTC,Awesome as a premiere scratch disk!
hardware,3awu5q,Smagjus,2,Wed Jun 24 15:34:45 2015 UTC,Our video marketing guys use 64 on their editing rigs. Those things are beastly though.
hardware,3awu5q,Beckneard,2,Wed Jun 24 06:41:36 2015 UTC,64 is already a lot and 128 is double that. I didn't even know 128 was possible until I learned about 16GB DDR4 RAM sticks.
hardware,3awu5q,Blowmewhileiplaycod,1 point,Wed Jun 24 06:43:39 2015 UTC,I'd like 256. They're currently running with quad raid 0 ssd's so the ability to push that to a RAMdisk seems wonderful.
hardware,3awu5q,Anaron,1 point,Wed Jun 24 06:56:42 2015 UTC,Damn.
hardware,3awu5q,Blowmewhileiplaycod,1 point,Wed Jun 24 10:35:02 2015 UTC,I have 16GB and have actually made it past the 8GB mark on occasion when having a shitload of tabs in Firefox open while running Arma 3.
hardware,3awu5q,Anaron,2,Wed Jun 24 17:31:26 2015 UTC,I have 32GB and Windows happily uses 13GB alone for caching. I don't mind that.
hardware,3awu5q,Blowmewhileiplaycod,1 point,Wed Jun 24 12:33:39 2015 UTC,"No this wasn't caching, it was actual RAM usage from all the processes running."
hardware,3awu5q,Anaron,1 point,Wed Jun 24 13:25:36 2015 UTC,some people I've seen are saying arkham knight hits 9.5 on its own... however that may be a memory leak or something so for right now I'd say you are right.  Still happy I went with 16 gigs.
hardware,3awu5q,wolfcry0,2,Wed Jun 24 13:36:01 2015 UTC,Arkham Knight is a broken steaming pile of shit. I'll never play that joke of a port until they fix the issues.
hardware,3awu5q,Jakeattack77,2,Wed Jun 24 16:56:15 2015 UTC,"if you haven't played it, you don't know that.  On my rig it's running rather well, although I have a titan x, 16gb ram, and 4790k"
hardware,3awu5q,TheImmortalLS,3,Wed Jun 24 17:30:51 2015 UTC,"I don't need to play it myself in order to verify people's claims as well as the claims of reputable reviewers.  Also, think of how many people have a Titan X. It's a beast of a card."
hardware,3awu5q,poematik,1 point,Wed Jun 24 19:45:56 2015 UTC,What I don't understand is the apparently huge performance gap between the titan x and the 980ti with this game when in most other scenarios they are comparable. My guess is that this is a major driver issue in addition to the pc optimization stuff
hardware,3awu5q,nonameowns,2,Wed Jun 24 19:50:42 2015 UTC,It's a really bad port. The Titan X probably has enough raw performance to get reasonable FPS despite the poor optimization. That means performance will likely go up once they fix the issues.
hardware,3awu5q,clickwir,1 point,Wed Jun 24 20:12:53 2015 UTC,Arma 3 + Firefox with a lot of tabs will chew through 8GB in no time
hardware,3awu5q,TheImmortalLS,2,Wed Jun 24 23:23:26 2015 UTC,http://24.media.tumblr.com/74e6faa44adf2a006591521dafb3f87e/tumblr_miaq1nhrVs1qjbsm8o1_500.gif
hardware,3ay1h3,knowledgestack,2,Wed Jun 24 12:28:26 2015 UTC,Ah two weeks difference! Thanks. I guess they are just trying to cash in more.
hardware,3ay1h3,Roph,5,Wed Jun 24 13:04:41 2015 UTC,Why would you buy from OCUK? They apply an idiot tax to everything they sell.
hardware,3ay1h3,Le_rebbit_account,3,Wed Jun 24 13:07:10 2015 UTC,They apply an idiot tax   What's wrong with OCUK? Get some good deals there.
hardware,3ay1h3,makar1,1 point,Wed Jun 24 13:13:49 2015 UTC,"Yea I dont get his comment, they are cheaper than scan."
hardware,3ay1h3,Le_rebbit_account,1 point,Wed Jun 24 13:19:15 2015 UTC,Cheaper than Scan. If it comes up on amazon next few weeks ill flubit it though.   I never said I was actually buying one.
hardware,3ay1h3,ExcessNeo,0,Wed Jun 24 16:09:40 2015 UTC,Their 980Tis are cheapest in the UK atm
hardware,3aylyy,aik3n,4,Wed Jun 24 15:29:37 2015 UTC,"Fury X is identical for any of the AIB partners - it's a reference design that AMD doesn't allow customization  might put a sticker here and there, but that's pretty much it"
hardware,3aylyy,zmeul,1 point,Wed Jun 24 15:37:49 2015 UTC,"head to /r/buildapc. Yes, different brands are different. Very different cooling, noise, temps, actually being overclocked different amounts, different backplates, look different, different customer support, warranties, etc.   Can't say whats different about the Fury X, only thing I've heard is that the sapphire has HDMI port"
hardware,3aylyy,Zarknox,7,Wed Jun 24 15:33:14 2015 UTC,"Normally, this is correct - different brands have different coolers, different board designs, different VRM blocks for better power delivery, etc.  However, Fury X is a reference-only design - all cards are the same, the only difference is who assembles it and the stickers that go on it.  Same as Titan X was until recently."
hardware,3aylyy,capn_hector,1 point,Wed Jun 24 15:59:08 2015 UTC,"I was answering the post title,    Why are there different brands of the same card, and do they each bring something different to the table?   and then I answered the information inside the post.   I ask because I'm seeing multiple brands of the new R9 Fury X  Can't say whats different about the Fury X, only thing I've heard is that the sapphire has HDMI port"
hardware,3aylyy,Zarknox,3,Wed Jun 24 16:02:07 2015 UTC,"Right, I'm just clarifying what's different about models of the Fury X since you said you couldn't say.  The answer is nothing, because it's a reference-only model.  All brands have 3xDP + 1xHDMI because that's the reference design.  Just trying to clarify both your cases a bit - both what's normal and Fury X."
hardware,3aylyy,capn_hector,1 point,Wed Jun 24 16:33:08 2015 UTC,"fair enough. I actually thought the AMD version had no HDMI at all, because somebody seemed shocked on a post of the Sapphire at the HDMI and they said it might be 2.0."
hardware,3aylyy,Zarknox,3,Wed Jun 24 16:35:19 2015 UTC,"Reference is 1.4 and AFAIK there's no variation from reference.  People are actually fairly upset about that, along with the lack of a DVI.  I would have said ""2xDP, 1xHDMI 2.0, and 1x DVI-D"" would have been better myself.  If not 2x DVI-Ds.    DP's not THAT dominant yet, and there's no way to drive high-refresh DVI/HDMI displays with this ~flagship~ card."
hardware,3aylyy,capn_hector,1 point,Wed Jun 24 18:13:01 2015 UTC,"Titan x allows board partners now?  Edit: no, they still don't, just non reference coolers, which I think they have always allowed, just not altered pcbs"
hardware,3aylyy,Blowmewhileiplaycod,0,Wed Jun 24 16:58:15 2015 UTC,EVGA is shipping their Hybrid model with the AIO liquid cooler now. They sell whole GPUs as well as a retrofit kit.
hardware,3aylyy,capn_hector,0,Wed Jun 24 17:26:20 2015 UTC,"that's not  an altered pcb, that's just an aftermarket cooler...."
hardware,3aylyy,Blowmewhileiplaycod,1 point,Wed Jun 24 19:44:26 2015 UTC,"""Board partners"" refers to the companies assembling/modding the boards, not the boards themselves.  Previously, board partners were not allowed to sell anything other than bone-stock reference boards with blower coolers installed, i.e. a 100% reference design - now they're allowed to sell reference boards with aftermarket coolers installed.  I never said the boards were different, you inferred that (incorrectly).  What I said was that the Fury X was a reference-only design, as was the Titan X until recently.  An analogous reference-only product is AMD's previous flagship, the 295x2 - every product labeled as a 295x2 must ship with the exact reference board design and cooler.  You can make a custom-board/custom-cooler GPU with a pair of 290x's on it, like the Devil 13, but you can't call it a 295x2.  The Fury X is the same way, and that's distinct from how board partners are now allowed to sell custom coolers on their Titans X."
hardware,3aylyy,capn_hector,1 point,Wed Jun 24 20:01:43 2015 UTC,when were they not allowed to put custom coolers on it?  I was under the impression that like evga had hydrocoppers available shortly after release.  I know I got mine like a month ago.
hardware,3aylyy,Blowmewhileiplaycod,1 point,Wed Jun 24 20:15:11 2015 UTC,"Release date for the reference card was March 17th, the Hydrocoppers hit the street about May 8th.  So about 7 weeks after launch?  So you got your card like a week or two after they came out.  The Hydrocopper is basically an EK waterblock, which shipped immediately on launch.  You've always been able to rip your card apart and install a waterblock by yourself, it's just that they couldn't ship them like that.  It was a fairly well-discussed thing at the time."
hardware,3aylyy,capn_hector,1 point,Wed Jun 24 20:34:09 2015 UTC,"oh ok, looks like I pretty much got into looking at parts right after the hydros were released."
hardware,3aylyy,Blowmewhileiplaycod,-2,Wed Jun 24 20:52:45 2015 UTC,"Words present in your post indicate that this might be a question or a request for help. If you review our rules you will notice that most questions don't belong here. This includes PC building questions and requests for tech support. If this is the case, please delete this post and resubmit it to either /r/buildapc or /r/techsupport. If this isn't the case, feel free to ignore this post.  I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns."
hardware,3aylyy,ShoutingDani,1 point,Wed Jun 24 15:29:37 2015 UTC,PLS STAPH
hardware,3aup2v,bankkopf,22,Tue Jun 23 17:35:13 2015 UTC,"Nothing new, same stuff we knew already thus far.   Still good to have more confirmation."
hardware,3aup2v,Imidazole0,10,Tue Jun 23 17:55:29 2015 UTC,"my hopes are on the i5 6600k. lots of overclocking potential and not overpriced please. (come on, let me dream a bit.)  cpu launches really seem to be very anticlimatic. so much buzz about GPUs on r/hardware and for CPUs everbody is like ""yeah, same procedure as every year"". gogo amd, shake things up in ~12month."
hardware,3aup2v,Klorel,2,Tue Jun 23 21:33:56 2015 UTC,It's probably not going to cost any more than existing Haswell MSRPs.
hardware,3aup2v,III-V,1 point,Wed Jun 24 00:40:19 2015 UTC,If AMD can deliver a competitive CPU in Zen we may see decent pricing.
hardware,3aup2v,335is,10,Wed Jun 24 13:19:14 2015 UTC,still waiting for a reason to upgrade my >5 year old i7 980x..
hardware,3aup2v,Raising,5,Wed Jun 24 01:17:37 2015 UTC,"Heh, Intel came full circle. I have a Core 2 Quad Q6600 laying around."
hardware,3aup2v,spellstrike,1 point,Wed Jun 24 11:44:13 2015 UTC,i'm still using mine... planning on replacing the q6600 with the 6700k
hardware,3aup2v,TorontosaurusHex,5,Fri Jun 26 02:22:51 2015 UTC,Just curious -- haven't followed desktop CPUs in years: any tangible benefits over Core iX 5xxx line? I mean 4690 still rocks...
hardware,3aup2v,Fnarley,9,Tue Jun 23 18:54:15 2015 UTC,"Not really, small performance and efficiency gains the main benefit is the new socket - the motherboards for skylake will bring things like USB C and better ddr4 support"
hardware,3aup2v,Durinthal,6,Tue Jun 23 19:16:40 2015 UTC,From a couple days ago.
hardware,3aup2v,animeman59,5,Tue Jun 23 19:27:59 2015 UTC,"If you currently have a Haswell based PC, then there's absolutely no need to upgrade to Skylake.  And I'm saying this based upon several years of following Intel's performance increases from Sandy Bridge.  Each iteration had about a 5% increase from the last one, at best.  It's still a valid argument to say that if you're on a Sandy Bridge PC, then you still might be good on the performance front.  The only reason that you would need to upgrade is for the new features set that comes with LGA 1151.  My own upgrade path was from an i5-2500K to an AMD FX-8350, and then currently to Intel's i7-4790K Devil's Canyon.  And the only reason I went with Intel was for a smaller micro-ATX build that sadly AMD has no decent motherboards for.  Overall performance between all of them wasn't really noticeable unless you really paid attention to performance metrics."
hardware,3aup2v,pb7280,3,Wed Jun 24 00:19:37 2015 UTC,"You notice much difference from your 2500k for gaming? I have one now at 4.8GHz, and have been putting off an upgrade for years since it has yet to bottleneck me in a game (2x290Xs, MMOs excepted). I was thinking of getting Skylake for DDR4 and PCIe 3.0 (my board can only do PCIe 2.0 x8 with CrossFire), but I really am happy with this 2500k. Maybe Cannonlake will be the one I'll go for."
hardware,3aup2v,animeman59,2,Wed Jun 24 03:35:20 2015 UTC,"No notice at all in gaming.  Biggest difference was in upgrading the GPUs.  I had a CFX 6770 on the Sandy Bridge and AMD builds.  No difference in gaming performance.  Going from the 6770 to GTX 570 to 780 to 980 was the biggest performance increase each time.  If you're still not seeing a performance drop with your CFX 290X, then upgrading the processor won't make a lick of difference.  I wouldn't worry about DDR4 performance.  Wait until the price for DDR4 memory comes down to DDR3 levels, then upgrade.  And don't even worry about PCIe bottlenecking.  PCIe 2.0 still has plenty of headroom for bandwidth.  I don't even think going with Cannonlake is going to make that big of a difference.  Yeah, you're still good for a long while."
hardware,3aup2v,pb7280,2,Wed Jun 24 03:45:10 2015 UTC,"Yeah I'm pretty sure I'm skipping Skylake. I was thinking maybe Cannonlake since it's a die shrink, it'll be less than half the fab process of Sandy Bridge IIRC. Maybe I'll wait and see what AMD cooks up with Zen, or if they pursue HBM in processor lineups ever."
hardware,3aup2v,ominouschaos,2,Wed Jun 24 03:49:33 2015 UTC,"If they have 20 lanes of PCIe, and 16 lanes go to the 16x slot, where are the other 4 lanes routed to? Maybe an M.2 slot?"
hardware,3aup2v,imtheproof,2,Tue Jun 23 19:34:45 2015 UTC,"16 dont go to the first slot unless there is a single graphics card there (or anything else that would use 16, cant think of anything right now). If you have 2 graphics cards in 8 will go to the first slot and 8 to the other.  what 20 allows for is having a single gpu at 16 and an m2 at 4, or 2 gpus at 8/8 and an m2 at 4, etc."
hardware,3aup2v,ominouschaos,3,Tue Jun 23 20:41:11 2015 UTC,Well where else would they go if the GPU wasn't there?! They simply wouldn't be used. I was mainly asking where the extra 4 went. I know about the 16/0 and 8/8 configurations.
hardware,3aup2v,imtheproof,2,Tue Jun 23 21:52:13 2015 UTC,"sorry, you made it sounds like you thought the lanes were 'hardwired' to a certain slot or something.  Yea, the extra 4 are intended for stuff like pcie ssds that take up 4 lanes."
hardware,3aup2v,ominouschaos,1 point,Wed Jun 24 01:38:11 2015 UTC,"No big deal, bro. And well, they kind of are hardwired to the first slot. Then 8 of those are also wired to the 2nd slot.   Ahhh -- according to this page, the CPU only has 16 lanes... it's the Z170 chipset that has 20 lanes. This page shows the same thing, but also goes on to show in the graphic that there's a max of 3 RST for PCIe storage, being x4 M.2 or x2 SATA express.  Although I'm not entirely sure how true it is."
hardware,3aup2v,wtallis,2,Wed Jun 24 02:18:37 2015 UTC,"In past generations there has been the occasional publication referring to 20 lanes by counting the 4 lanes of the DMI link, which is perfectly reasonable provided that they're consistent."
hardware,3aup2v,PhilipK_Dick,2,Wed Jun 24 01:57:31 2015 UTC,Here we have Skylake noted as 6th Gen Core CPU meaning they will most likely be released as Core iX 6xxx. I posted a link to the Product Change Notification by Intel (the source provided by the news) where Core i5 6xxx and Core i7 6xxx are listed for order. Now tell me how Intel can't confirm its upcoming CPUs in official releases to its customers.
hardware,3aup2v,TheImmortalLS,6,Tue Jun 23 18:12:22 2015 UTC,Intel can release Skylake however they want.  It's not like they have pressure from competitors....
hardware,3atujk,TaintedSquirrel,8,Tue Jun 23 13:40:10 2015 UTC,I kinda wanna replace that center plate with an all copper version.
hardware,3atujk,dasiffy,9,Tue Jun 23 15:17:56 2015 UTC,"That would be very heavy, and expensive."
hardware,3atujk,Exist50,3,Tue Jun 23 15:24:20 2015 UTC,It'd just be a rather thick sheet of copper. I can't imagine it'd be the most expensive thing in the world compared to the the card itself.
hardware,3atujk,Exist50,1 point,Tue Jun 23 22:23:33 2015 UTC,"Dunno how useful it would be, the VRMs are already under copper.  The HBM chips go directly into the CPU block."
hardware,3atujk,fzed1199,2,Tue Jun 23 22:24:21 2015 UTC,"I think the idea would be to do it purely for aesthetics. Might look cool, especially if etched/engraved."
hardware,3atujk,Hiryougan,1 point,Tue Jun 23 22:31:11 2015 UTC,"A common design ethic is to stain the entire cabinet, not just the parts you see. It's speaks to a level of craftsmanship that's not as common as it used to be. Is it necessary? No, not at all. Sometimes it's just nice to know that it's finished and presentable through and through."
hardware,3atujk,nemmera,-3,Wed Jun 24 02:52:09 2015 UTC,Have fun with an overheating gpu.
hardware,3atujk,FatS4cks,38,Wed Jun 24 06:50:02 2015 UTC,I don't need it.
hardware,3atujk,BrockYXE,10,Wed Jun 24 03:55:58 2015 UTC,Keep telling myself that as well. I only play games that my 780 can handle anyways... No need for something new... So glorious... Agh.
hardware,3atujk,Hiryougan,5,Tue Jun 23 14:57:49 2015 UTC,I'm thinking I'll probably get the second gen Fury once Zen is out and just build a whole new system with that. 8150 and 7970 should be good for another year hopefully...
hardware,3atujk,nemmera,4,Tue Jun 23 15:13:56 2015 UTC,I-I can just add a second 680...
hardware,3atujk,epsys,0,Tue Jun 23 21:54:33 2015 UTC,I'm sorry how nvidia treats you guys lately btw. I would be pissed like hell.
hardware,3atujk,Hiryougan,7,Tue Jun 23 16:09:09 2015 UTC,"I don't really feel mistreated though, I'm just a sucker for new shit.  Some misleading info/hype about the Fury X also, so I'll wait for reviews."
hardware,3atujk,dylan522p,6,Tue Jun 23 15:18:59 2015 UTC,"Some misleading info/hype    oh, which?"
hardware,3atujk,Hiryougan,2,Tue Jun 23 15:25:12 2015 UTC,"I was talking about recent issues were 960 is on the 780 level.  But back to the topic. Daaamn, it's the most beautiful, badass looking card right now. Cool looking Titan blower has nothing on this :D too bad it's impossible for me to buy. Where i live (Poland) it costs my 3 months salary worth. Full."
hardware,3atujk,dylan522p,1 point,Tue Jun 23 16:45:08 2015 UTC,Where? Nearly every benchmark I'm looking at I so the rise but they aren't recent
hardware,3atujk,Mighty-Tsu,1 point,Tue Jun 23 15:33:09 2015 UTC,I mean the recent ones. Like Project CARS or Witcher 3.  780Ti is slower than 970 on these. It's ridiculous.
hardware,3atujk,Tuna-Fish2,3,Tue Jun 23 16:15:31 2015 UTC,Wow wasn't it about 970 level too
hardware,3atujk,SirCrest_YT,4,Tue Jun 23 17:37:32 2015 UTC,Orrr Gameworks is gimping old architectures to make the newest cards seem more tempting. The difference in compute performance doesn't match up. :3
hardware,3atujk,aznanimality,1 point,Tue Jun 23 18:12:02 2015 UTC,"It's not just that. The gap between the old and new architecture keeps growing because nVidia apparently stopped all driver optimization work for Kepler the minute they had a full lineup of Maxwells out.  In a way, that makes sense -- Maxwell uarch is sufficiently different that the compiler optimization work that helps Kepler does nothing to help Maxwell, and with nVidia controlling >60% of the market, their primary competition is their own older cards. Why make sales harder?  The reason people are so pissed about it is that it certainly doesn't bode well for Maxwell performance once Pascal is out, or for Pascal once Volta is out."
hardware,3atujk,Chidwick089,1 point,Tue Jun 23 18:39:08 2015 UTC,"I don't really feel mistreated though, I'm just a sucker for new shit.   If I'm mistreated, I don't feel it. There is certainly a lot of badpr news from Nvidia but for a single user, it doesn't really materialize."
hardware,3atujk,feladirr,1 point,Wed Jun 24 00:21:14 2015 UTC,"I tell myself that too, but I did once see my FPS drop to 58 for a few seconds in Witcher 3"
hardware,3atujk,Call_Me_ZeeKay,3,Tue Jun 23 23:56:18 2015 UTC,My excuse is definitely VR lol
hardware,3atujk,Call_Me_ZeeKay,13,Wed Jun 24 04:46:10 2015 UTC,That looks good.
hardware,3atujk,WYLD_STALLYNS,3,Wed Jun 24 01:41:32 2015 UTC,Anyone got any ideas or info on how to integrate this into an existing loop? I'm guessing wait for the normal Fury and add an aftermarket block?
hardware,3atujk,TheDreamerofWorlds,5,Tue Jun 23 16:41:01 2015 UTC,There will be aftermarket blocks for the Fury X.
hardware,3atujk,Seclorum,-4,Tue Jun 23 13:41:38 2015 UTC,But what's the point in buying the $100 more expensive WC version and basically throwing that away?
hardware,3atujk,Call_Me_ZeeKay,24,Tue Jun 23 15:44:34 2015 UTC,"I don't know, you just asked if it could be integrated into existing loops."
hardware,3atujk,Seclorum,3,Tue Jun 23 15:45:22 2015 UTC,Checkmate.
hardware,3atujk,MrPoletski,1 point,Tue Jun 23 15:56:15 2015 UTC,Any idea if the radiator can be replaced with a 240mm?
hardware,3atujk,heratic666,3,Tue Jun 23 15:58:38 2015 UTC,Because the Fury X has a fully unlocked Fiji core.
hardware,3atujk,ZeM3D,6,Tue Jun 23 21:55:36 2015 UTC,Has that been proven that the Fury is cut down? From the AMD presentation they made it seem its just the air cooled version (potentially just downclocked?). The only things I've seen that point to a cut down Fury are rumors. Also the cost difference of $100 is pretty much the cost of a AIO cooler.
hardware,3atujk,heratic666,2,Tue Jun 23 21:57:28 2015 UTC,"That is just the thing. There have been rumors going back and forth on the straight Fury and AMD hasn't revealed just what if any difference there will be.   So my post really should be taken in the context, ""The Fury X has the only fully unlocked Fiji core that we know of at this time."""
hardware,3atujk,Boy1998,5,Tue Jun 23 16:05:40 2015 UTC,I heard a rumour that the Fury X will have a built in time machine.
hardware,3atujk,heratic666,6,Tue Jun 23 16:11:42 2015 UTC,when can we expect to see some benchmarks? it's meant to be launching soon yeah?
hardware,3atujk,bphase,11,Tue Jun 23 16:15:47 2015 UTC,"NDA for reviews and sales lifts at 8:00 AM EST tomorrow, according to OcUK.  This is the same time the 300-series NDA lifted.  We might see reviews at midnight, OcUK is just a vendor not a reviewer.  The cards are already on sale in Aus/NZ as well as a few Asian countries."
hardware,3atujk,AndreyATGB,1 point,Tue Jun 23 18:52:06 2015 UTC,Embargo ends at midnight
hardware,3atujk,Killmeplsok,2,Tue Jun 23 14:32:37 2015 UTC,midnight where? its nearly 1am in Australia lol
hardware,3atujk,1Bravo,1 point,Tue Jun 23 15:23:11 2015 UTC,I would assume 12 on the west coast of the U.S. since that's where their Company is located.
hardware,3atujk,lnvincibleVase,1 point,Tue Jun 23 14:37:29 2015 UTC,Thanks. hope the fury is at least as good as the benchmarks they showed at E3
hardware,3atujk,Lag-Switch,1 point,Tue Jun 23 14:42:47 2015 UTC,"Unlikely, those were all cherry-picked examples most likely. Like the Fury Nano having 2x perf/watt, turns out it was measured from Unigine which is a Tesselation benchmark, where GCN 1.2 is greatly improved compared to 290X."
hardware,3atujk,YellowCBR,1 point,Tue Jun 23 14:46:29 2015 UTC,"Tomorrow most likely, some Chinese sites may release early though."
hardware,3atujk,Lag-Switch,4,Tue Jun 23 14:57:36 2015 UTC,The Chinese author in the link says that his NDA ends on 24th 2000 hours GMT+8 which is when this post is 20 hours old.
hardware,3atujk,YellowCBR,4,Tue Jun 23 15:33:24 2015 UTC,Is that pump any good? I love the brand for cases but not familiar with their water cooling quality.
hardware,3atujk,djfakey,5,Tue Jun 23 14:34:22 2015 UTC,"What I've seen is that they are really performant, but can be a tad noisy."
hardware,3atujk,krista_,1 point,Tue Jun 23 16:06:10 2015 UTC,"I also have heard any mention it being sold with a larger radiator, which is a little disappointing."
hardware,3atujk,CarVac,1 point,Tue Jun 23 14:13:27 2015 UTC,"The 295x2 radiator did its job fairly well, and this has a lower power consumption and a thicker rad and a better fan."
hardware,3atujk,krista_,1 point,Tue Jun 23 14:18:32 2015 UTC,"IIRC, the 295x2 wasn't actually that much cooler than air cards.  Sure it had more heat output, but I'd bet a lot of people would rather have a GPU that is actually cool and can handle full load with fans on low.  Maybe I'm just being picky. We'll have to wait for more reviews and I hope you're right."
hardware,3atujk,CarVac,1 point,Tue Jun 23 18:47:05 2015 UTC,295x2 at load: 60C and 41dB  Gigabyte G1 980 Ti: 70C and 40dB.  From TPU
hardware,3atujk,swimming-bird,4,Wed Jun 24 05:18:28 2015 UTC,Cool I was under the assumption it was going to be the same asetek supplied water pump found in the R9 295x2.
hardware,3atujk,anotheraznguy,2,Wed Jun 24 05:32:38 2015 UTC,I'm curious why they didn't make this 1 slot deep...
hardware,3atujk,snowball666,4,Wed Jun 24 05:40:17 2015 UTC,The pump needs thickness.
hardware,3atujk,KMKtwo-four,1 point,Tue Jun 23 14:16:56 2015 UTC,They could have put the pump on the reservoir/radiator side
hardware,3at4ik,yuhong,58,Tue Jun 23 08:18:14 2015 UTC,First SSD now this? My money....
hardware,3at4ik,LvL99OnionKnight,39,Tue Jun 23 09:42:02 2015 UTC,"Ha, that will teach you. I have saved money by not having any money! Wait..."
hardware,3at4ik,DomUK89,15,Tue Jun 23 12:53:32 2015 UTC,It's always like this. Each new tech comes in and drops in price. My first cd-rw drive was ~$200. The price game isn't changing :D Just what the relevant tech is.
hardware,3at4ik,Virtualization_Freak,3,Tue Jun 23 13:47:41 2015 UTC,Ha I remember getting my first CD-RW drive for christmas. I think it was my 'main' present because they were so expensive at the time.
hardware,3at4ik,glr123,3,Tue Jun 23 16:08:35 2015 UTC,"and the cd-rs were too! I was afraid to burn something, and afraid to leave even 30MB unused"
hardware,3at4ik,epsys,6,Tue Jun 23 16:35:48 2015 UTC,The 970 is getting more and more relevant to my wallet by the day.
hardware,3at4ik,skullshark54,10,Tue Jun 23 13:50:11 2015 UTC,"By the time it is, you'll be drooling over the Nvidia FX Force 27's!"
hardware,3at4ik,Virtualization_Freak,5,Tue Jun 23 14:49:31 2015 UTC,Oh god please no more GeForce FX
hardware,3at4ik,MoreFeeYouS,4,Tue Jun 23 17:16:56 2015 UTC,"Wait! We can go with the MX line. Wait no. Even better...  nVidia TNT 1000 series. The ""titan"" will be the nVidia TNT 1337."
hardware,3at4ik,Virtualization_Freak,2,Tue Jun 23 18:15:13 2015 UTC,"man nVidia Riva TNT 32 was my first graphics card, even overclocked it using Rivatuner, good times.."
hardware,3at4ik,iamjamir,1 point,Tue Jun 23 22:38:11 2015 UTC,MX with the 'M' for 1000
hardware,3at4ik,TheImmortalLS,3,Thu Jun 25 08:55:52 2015 UTC,I know I know. I have been looking at the 970 from a 650 to boost and now I am looking at the 980 ti it's a vicious cycle.
hardware,3at4ik,skullshark54,12,Tue Jun 23 15:21:57 2015 UTC,2016! 16nm and HBM2 will be crazy. We've been stuck on 28nm for so long... Three years is a long time spent on a single process. The 2016-equivalent of the GTX 970 (1070?) and R9 390 (R9 490?) will most probably amaze and impress. +40% performance-per-watt due to the process change alone.
hardware,3at4ik,flyafar,7,Tue Jun 23 16:34:15 2015 UTC,"And then you factor in the 20% standard boost that comes with architecture change, and next gen gpus are looking at approximately a 68% boost in processing power. It gets my dick WET son"
hardware,3at4ik,AssCrackBanditHunter,1 point,Tue Jun 23 20:26:17 2015 UTC,"Add in 8GB of HBM 2.0 and I already came. I've been saving up for months now, and will continue until the 16nm cards are released. I can't wait.  Wonder if my 4790k will bottleneck it lol"
hardware,3at4ik,flyafar,1 point,Tue Jun 23 20:31:05 2015 UTC,"Unless dx12 saves the day, I can totally see a future where our gpus are bashing through games at ultra settings, but our cpus are weeping."
hardware,3at4ik,AssCrackBanditHunter,1 point,Tue Jun 23 20:53:03 2015 UTC,Ray tracing commence
hardware,3at4ik,TheImmortalLS,1 point,Thu Jun 25 09:06:30 2015 UTC,Sounds like I shouldn't sell my 970 for a 980ti then...? better to wait?
hardware,3at4ik,HeliconPath,2,Wed Jun 24 01:21:55 2015 UTC,"Depends. The 980ti has decent price for performance. It's double the cost of the 970, but gives a 50-70% or so boost in performance depending on the game.It's a good card. But pascal's equivalent of the 970 should be in the price range of the 970 while giving 980ti or better performance. So you can get the 980ti now to get a taste of the future, or you can wait and get the future equivalent of the 970 and get true future performance (lower power consumption, lower temps, hbm 2)  You can't really go wrong either way tbh"
hardware,3at4ik,AssCrackBanditHunter,1 point,Wed Jun 24 01:45:03 2015 UTC,"I think at this point based on games launching waiting might be the best bet. Work wise, 4gb has been enough VRAM for my scenes though 6 would be nice...."
hardware,3at4ik,HeliconPath,0,Wed Jun 24 01:50:02 2015 UTC,"I find the idea of a fully HSA compliant APU with 14nm Zen cores, 16nm GCN cores and 4 GB of HBM 2 far more intriguing. It would be rather interesting to see how the shared memory performed in such a setup."
hardware,3at4ik,MarcusOrlyius,2,Wed Jun 24 07:08:53 2015 UTC,"That's cool too but I just spent ~$600 a couple month ago on a 4790K/mobo/ram upgrade (new case as well).  I'm definitely excited for AMD to compete in the desktop ""CPU"" space again, but it's not on my radar the same way Pascal and Greenland (Is that what AMD's next gen will be called?) are."
hardware,3at4ik,flyafar,1 point,Wed Jun 24 07:16:03 2015 UTC,Why would you want a 970? Do you actually support the anti-consumer drivel nvidia pulled?
hardware,3at4ik,skullshark54,5,Tue Jun 23 20:27:55 2015 UTC,I just want an upgrade from a 650 ti boost... Didnt realize I had to be political about it.
hardware,3at4ik,skilliard4,4,Wed Jun 24 00:03:51 2015 UTC,"get an r9 390 instead, way better performance for your money."
hardware,3at4ik,skullshark54,1 point,Wed Jun 24 02:51:46 2015 UTC,I don't know. My plan for now is to sit and wait. And see what goes on sale. Or what is coming out next.
hardware,3at4ik,skilliard4,2,Wed Jun 24 02:56:58 2015 UTC,"wait for the fury to come out(not just the x, wait for the fury mid july @ $549). Even if you can't afford it, it might cause NVIDIA to push down prices on their 980, which means it may be close in price to their 970, which means that they will have to drop 970 prices too."
hardware,3at4ik,skullshark54,1 point,Wed Jun 24 03:08:18 2015 UTC,Yea my sights are set on the 980. Now I play the waiting game
hardware,3at4ik,column5,2,Sat Jun 27 06:19:48 2015 UTC,Now if only the NVME SSDs would do the same :D
hardware,3at4ik,sfacets,1 point,Tue Jun 23 18:09:39 2015 UTC,"Give it a couple of days, and this too will be debunked."
hardware,3at4ik,Klorel,24,Thu Jun 25 00:03:26 2015 UTC,"Indeed affordable. 16gb seem to start at around 110-120€.  But i noticed that timings seem to be much higher. These moduls are all running at cl15 or something like that. Why is that so, does it even matter?"
hardware,3at4ik,Timiniel,93,Tue Jun 23 09:55:59 2015 UTC,"Well, you've been given some explanations, but they didn't go into much detail. I'm not sure you want more, but here we go. The timings you usually see (9-9-9-24, 15-15-15-45) refer to the number of clocks required for the memory to access something. the first one (CAS latency, column address strobe in length) refers to the time it takes to get data when you already have the right memory row open. Let's look at this concept first.   WARNING: All values should be divided by two, but that gets us some large fractional numbers and rounding issues. A 1600 MHz memory is actually running at 800 MHz, but does operations on both changes of the clock ""wave"". This is a simplification, I hope it doesn't confuse you more than it clarifies.  This means that for a 1600 MHz 9-9-9-24 ddr3 module of memory, it will take 0.625 (1/1600 MHz as the period/cycle is the inverse of the frequency) ns * 9 cycles of the clock to get that information. This is approximately 5.625 ns. These are commonly found.  For a 2133 MHz 15-15-15-35 ddr4 module, it will take 0.468 ns * 15 cycles of the clock to to get that information. This amounts to approximately 7.02 ns. These are also commonly found.  In this situation, DDR3 is faster, but let's instead look at another module, this one 2400 MHz at 14-14-14-34. These are less commonly found, but some companies have them. I know G.Skill has their ripjaws 4 F4-2400C14Q-16GRK model in this configuration, for example. Here we would see it would take 0.414 ns * 14 cycles to get the info. This now is 5.824 ns, or really close to the DDR3 but not yet better in terms of performance.  Now, a break to speak of the other concepts of the timings. The 2nd number in 9-9-9-24 is what is known as row to column delay. This is the additional (As in, you need to add this number to the previous one) time needed to get to the information when you don't have the right row open. The third number is the row precharge. This is the additional time it takes when you have the wrong row open. The final time, row address strobe (RAS), is basically the time to get a complete operation in total, meaning getting the complete operation done.  You might see that this last concept means the first module takes 24 * 0.625 = 15 ns to complete, while the third module takes 34 * 0.414 = 14.07 ns, meaning the third one is now slightly faster in this situation. You begin to see how this can be complex, eh?  But let's go big. Let's look at a fourth module, this one 3333 MHz at 16-16-16-36. This means CAS is 0.3 ns * 16 cycles = 4.8 ns. Faster than the DDR3, and the RAS is now 0.3 ns * 36 cycles = 10.8 ns, quite a bit faster than the DDR3.  Hope this helps and I was clear enough."
hardware,3at4ik,Flayum,6,Tue Jun 23 12:39:33 2015 UTC,"This was an extremely clear explanation, thank you!"
hardware,3at4ik,Mr_Ignorant,6,Tue Jun 23 14:30:07 2015 UTC,For the first time I actually understood what those timings meant. Thanks for taking your time to explain that.
hardware,3at4ik,wanking_furiously,2,Tue Jun 23 17:00:37 2015 UTC,Isn't it better to use time to first word anyway? No cpu starts working on the first bit.
hardware,3at4ik,Timiniel,1 point,Tue Jun 23 19:52:45 2015 UTC,"Tell that to the memory manufacturers, but honestly? No. The difference in read speeds like that is nearly negligible between brands and models. What differentiates the various modules are the above concepts, frankly. That one is mostly constant..."
hardware,3at4ik,wanking_furiously,1 point,Wed Jun 24 10:41:24 2015 UTC,"What? Time to nth word is based off frequency and cas latency.   I did get the concept a bit confused though. What you were calculating was time to first word, but as you read more sequential words the effect of latency becomes negligible. So, on average, faster ram with higher real latency will have better performance than your calculations show."
hardware,3at4ik,Timiniel,1 point,Wed Jun 24 11:00:57 2015 UTC,"but as you read more sequential words the effect of latency becomes negligible   Until you have to swap row. That's the point here. Hits on the current open row on the memory are a minority of the operations in most commonly seen applications. You're going to have to do a lot of swapping, and then row precharge becomes very important. Remember that the prefetch buffer usually is only 2048 bits wide... While accessing any of the 256 words in the same row is quick, and streaming those words is as you say more affected by faster ram, how often do you think you will stick with only one row open of the memory? I mean, it is literally 2048 bits."
hardware,3at4ik,wanking_furiously,1 point,Wed Jun 24 11:11:10 2015 UTC,"https://en.m.wikipedia.org/wiki/CAS_latency  Pipelining. If you happen to know the addresses in advance the RAM can keep reading without pause, so the row doesn't matter.   Anyway even if you compare 1600 MHz 9-9-9-24 to 2133 MHz 15-15-15-35, the second of which is pretty shitty, you only have to read 9 words in a row for the latter to be faster. And I really don't think that sequential reads are rare enough in CPU applications to e considered negligable."
hardware,3at4ik,Timiniel,1 point,Wed Jun 24 11:30:01 2015 UTC,"You are aware that the article you link itself shows that only applies in a certain condition, right? :)"
hardware,3at4ik,wanking_furiously,1 point,Wed Jun 24 11:33:52 2015 UTC,Obviously. There is only one condition and I stated it.
hardware,3at4ik,Timiniel,1 point,Wed Jun 24 11:43:03 2015 UTC,"Well, then the point is what is more common: Is that condition satisfied commonly, or not?   If it isn't, then the precharge delay becomes very important in distinguishing memory modules...  (But all in all, I think we're wasting time here. I purposefully ignored bandwidth in my initial explanation as the guy I was replying to didn't mention it and just seemed confused by the timings. Without focusing on that, this is pretty much moot.)"
hardware,3at4ik,autowikibot,1 point,Wed Jun 24 11:48:25 2015 UTC,"CAS latency:       Column Access Strobe (CAS) latency, or CL, is the delay time between the moment a memory controller tells the memory module to access a particular memory column on a RAM module, and the moment the data from the given array location is available on the module's output pins.  In general, the lower the CL, the better.  In asynchronous DRAM, the interval is specified in nanoseconds (absolute time). In synchronous DRAM, the interval is specified in clock cycles. Because the latency is dependent upon a number of clock ticks instead of absolute time, the actual time for an SDRAM module to respond to a CAS event might vary between uses of the same module if the clock rate differs.     Relevant: Serial presence detect | Synchronous dynamic random-access memory | DDR2 SDRAM   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
hardware,3at4ik,softawre,1 point,Wed Jun 24 11:30:01 2015 UTC,Team Group has amazingly fast latency.  I'm using their 2400 C10 today.
hardware,3at4ik,Timiniel,3,Wed Jun 24 11:30:15 2015 UTC,"That's some incredibly low timings. I have to admit I haven't done much testing on DDR4, not having access to a 2011 chipset CPU at the moment, but I'll be looking into it when I do."
hardware,3at4ik,softawre,1 point,Tue Jun 23 17:09:02 2015 UTC,"It's DDR3, not DDR4.  Guess I should have mentioned that :)"
hardware,3at4ik,Timiniel,2,Tue Jun 23 17:19:11 2015 UTC,Ah the overclocked memory? Then it's not as impressive. Thank you for the clarification :)
hardware,3at4ik,clickwir,1 point,Tue Jun 23 17:21:07 2015 UTC,That's just a horrible name for a company though. Sounds like AllWinner. I just can't buy something that sounds like Engrish.
hardware,3at4ik,softawre,1 point,Tue Jun 23 17:32:05 2015 UTC,LOL.  But really does it matter?  All that mattered to me were the specs and the wonderful reviews over on Tom's Hardware from the ram experts.
hardware,3at4ik,Vynlovanth,1 point,Wed Jun 24 05:02:55 2015 UTC,Thank you for posting this.  I've posted similar explanations before in /r/buildapc when recommending a higher frequency RAM for someone who has a build that can utilize it but it's such a common misconception that the CAS Latency number must always be lower.
hardware,3at4ik,Timiniel,1 point,Wed Jun 24 13:23:08 2015 UTC,"Yeah, unfortunately a lot of people don't understand that most often, we do not have the correct row open in the memory, and as such, we need to consider all situations.  There was, a few years ago, a very nice analysis of the cache hits/ram row hits when running a complex weather simulation software, and the vast majority of these were TLB/cache misses as well as memory row misses. I wonder if I could still find that..."
hardware,3at4ik,epsys,-3,Tue Jun 23 23:48:36 2015 UTC,"I think you made a small mistake. DDR4-1600 runs at twice the frequency / bandwidth of DDR3-1600-- so when you have a 15-15-15-45 you're already at 7.5-7.5-7.5-22.5 in DDR3-1600 terms. For example, from DDR2-800 to DDR3-800, bandwidth doubled, but latencies didn't-- they slightly less than doubled."
hardware,3at4ik,Timiniel,8,Wed Jun 24 10:44:33 2015 UTC,"What? I'm sorry but I don't understand what you are talking about. What you've just said makes no sense, can you explain it better?   Bandwidth capacity doesn't matter when we're talking about timings. Frequency isn't directly related to the width of the bus, nor is it true that ddr4 1600 runs at twice the frequency OR the bandwidth of ddr3-1600. If they had the same timings, the only difference between the two would be the voltage on the former being lower (1.2v).  I have a feeling you are getting concepts mixed up there."
hardware,3at4ik,epsys,-2,Tue Jun 23 16:38:47 2015 UTC,"had to edit the last bit-- latencies didn't double, they slightly less than doubled.  Bandwidth capacity has every bearing when we're talking about timings, because those cycles are counted in the frequency they're operating at. My DDR2 was 5-5-5-15 at ddr2-800 and many early DDR3-1600's that came out were 9-9-9-27-- which is even lower latency in real-world-seconds (the only thing that matters) than DDR2-800. So at DDR3-1600, not only are you getting twice the bandwidth of DDR2-800, but you're getting it at effectively DDR2-800 latencies of 4.5-4.5-4.5-13.5--getting it in even fewer nanoseconds than DDR2-800's 5-5-5-15   width of the bus    that's different--didn't say anything about that"
hardware,3at4ik,Timiniel,2,Tue Jun 23 17:22:40 2015 UTC,"Sir, I'm sorry, but your knowledge of this is lacking, and you really shouldn't be speaking of what you do not know."
hardware,3at4ik,epsys,-1,Tue Jun 23 19:04:05 2015 UTC,It's not. This is basic Wikipedia knowledge. You're going to have yo do better than that
hardware,3at4ik,Timiniel,1 point,Wed Jun 24 10:30:27 2015 UTC,"""basic wikipedia knowledge"" ? What are you talking about? Sir, please, this is beyond ridiculous. It might be a question of english (it's not my mother language), but what you said made no sense. If you think you are so correct, please rephrase because as it stands, you were incorrect."
hardware,3at4ik,epsys,0,Wed Jun 24 10:57:47 2015 UTC,"you know how to google and use wikipedia, go see for yourself  DDR2  DDR3  also read about how the length of a 'cycle' is determined by the frequency of operation. At twice the frequency, each cycle you count is going to take half-as-long in a frequency-agnostic measurement like 'seconds'  to reiterate,   My DDR2 was 5-5-5-15 at ddr2-800 and many early DDR3-1600's that came out were 9-9-9-27-- which is even lower latency in real-world-seconds (the only thing that matters) than DDR2-800. So at DDR3-1600, not only are you getting twice the bandwidth of DDR2-800, but you're getting it at effectively DDR2-800 latencies of 4.5-4.5-4.5-13.5--getting it in even fewer nanoseconds than DDR2-800's 5-5-5-15"
hardware,3at4ik,Timiniel,2,Wed Jun 24 11:12:43 2015 UTC,"sigh...  Ok, I'm going to be even more blunt here:  What is the difference between an hypothetical ddr2 1066 module and a ddr3 1066 module if they have the same timings?  Answer me that."
hardware,3at4ik,StevenSeagull_,15,Wed Jun 24 18:50:12 2015 UTC,"The frequency in Hertz means ""cycles per second"". This determines the bandwidth (together with the bus width). The CL timing indicates the latency of the memory. CL15 means the memory needs 15 cycles to respond. So 3000Mhz with a CL 15 timing means 15/3000=0,005s response time. 2133 with CL11 timing: 11/2133=0,0051s. In this case the latency is basicly the same but the 3000Mhz Ram has a higer bandwidth. Different applications are more sensitive to bandwidth(7-Zip, Winrar) and some are more sensitive to latency"
hardware,3at4ik,bphase,10,Thu Jun 25 10:33:05 2015 UTC,"3000MHz is actually 3GHz or 1/3 nanosecond per cycle, so those response times are only in the several nanoseconds range (10-9 s).  Also, just to clarify, DDR4-3000 runs at 1500MHz Double Data Rate (two transfers per clock cycle), so CL15 would give us a latency of 10 ns."
hardware,3at4ik,jder208,1 point,Tue Jun 23 11:12:32 2015 UTC,So more latency than 2133 with CL11 (im guessing would be 5ns)?
hardware,3at4ik,bphase,7,Tue Jun 23 11:36:57 2015 UTC,"No, slightly less. 2133 runs at 1066MHz, which means 0.94ns per cycle or 10.3ns for 11 cycles."
hardware,3at4ik,jder208,6,Tue Jun 23 11:59:23 2015 UTC,"Ah, I just realised what DDR stood for"
hardware,3at4ik,aaron552,3,Tue Jun 23 12:07:51 2015 UTC,"CL15 means the memory needs 15 cycles to respond.   Not entirely accurate. It's a minimum of 15 cycles to respond (outside of burst transfers). CL15 RAM is typically 15-15-15-45, with the last number (45 cycles) being the typical maximum latency."
hardware,3at4ik,Tuna-Fish2,7,Tue Jun 23 12:16:21 2015 UTC,"Specifically, cas latency is the time it takes to respond if the correct row is already open.  In general, unless you are streaming the correct row is never open, as rows are small enough that if you hit the same ones often your requests will also fit the CPU cache. Because of this, cas latency mostly determines how much buffering is needed for streaming loads. Actual memory latency is Trcd + CL when very little bandwidth is being used, Trp + Trcd + CL when some bandwidth is being used and near Tras- Trcd + Trp + Trcd + CL when the interface is going full tilt on random accesses, with some cycles possibly shaved away from the Tras by banking."
hardware,3at4ik,Klorel,1 point,Tue Jun 23 11:42:37 2015 UTC,thanks for explaining.
hardware,3at4ik,wpm,3,Tue Jun 23 13:17:31 2015 UTC,The timings are higher but the frequencies are higher too. Overall a net increase in bandwidth over DDR3.
hardware,3at4ik,CJKay93,1 point,Tue Jun 23 11:14:12 2015 UTC,"Given that my current DDR3 16GB for 110-120€ is CL9 I imagine the DDR4 frequencies are yet to really outperform the lower DDR3 timings for the same price, but I don't know enough of the math to figure it out."
hardware,3at4ik,skottydoesntknow,1 point,Tue Jun 23 10:06:41 2015 UTC,"It's an increase, but likely not anything you would notice. I picked up a 2400mhz kit on sale a while back that is cl11 and it is faster than 1600mhz cl9, did out the math.  On mobile so I don't have the link, I believe it was from anandtech, but they gave the formula to apply to each part of the timings. So if my 2400 cl11 ddr3 is faster, ddr4 is as well"
hardware,3at4ik,__thiscall,6,Tue Jun 23 10:09:38 2015 UTC,"This is how to do the math, but let's keep it really short:  CL / Freq * 2000 = CL time in nanoseconds     Specs Calculation Result    1600 MHz CL9 9 / 1600 * 2000 11.25 ns   2400 MHz CL11 11 / 2400 * 2000 9.17 ns"
hardware,3at4ik,cowbutt6,1 point,Tue Jun 23 10:52:33 2015 UTC,Why are you multiplying by 2000 rather than 1000?
hardware,3at4ik,__thiscall,2,Tue Jun 23 13:10:41 2015 UTC,"(remember frequency is half the chips label frequency)   CL / (Freq / 2) * 1000 is more reasonable, but people who are interested in reasons will read the linked answer, others who are not are happier with a formula that is as simple as possible."
hardware,3at4ik,Dommy73,8,Mon Jun 29 15:46:10 2015 UTC,"Good, just in time for Skylake then."
hardware,3at4ik,dexter311,5,Mon Jun 29 16:18:22 2015 UTC,"More like, just in time for Skylake to come out and drive prices back up again?"
hardware,3at4ik,Dommy73,5,Tue Jun 23 12:16:53 2015 UTC,"Not necesarilly. Sure, it's one of the possible outcomes, but I think lots of gamers, used to DDR3 prices would keep the price down."
hardware,3at4ik,Teethpasta,3,Tue Jun 23 13:26:07 2015 UTC,No more like skylake is the cause of this price lowering. Everyone is making more in preparation for skylake so the prices are dropping due to the increase in supply.
hardware,3at4ik,fuzzycuffs,8,Tue Jun 23 13:31:04 2015 UTC,"And, as always, prices in Japan are still very high."
hardware,3at4ik,TheSiskoIsCoporeal,5,Tue Jun 23 21:24:57 2015 UTC,"Singapore too man, I feel you"
hardware,3at4ik,foxtrot1_1,3,Tue Jun 23 11:33:57 2015 UTC,Aren't the prices for most things a lot higher in Singapore?
hardware,3at4ik,TheSiskoIsCoporeal,4,Tue Jun 23 13:32:42 2015 UTC,Yes :( Except food. Eating out is much cheaper.
hardware,3at4ik,dylan522p,3,Tue Jun 23 13:47:00 2015 UTC,Its not slowing server demands it's that they expected skylake to be out and it isn't. They pushed it back to allow Broadwell a little room
hardware,3at4ik,Vince789,6,Tue Jun 23 14:53:10 2015 UTC,"Noob here with a somewhat related question, last year Samsung released their 850 Pro and 850 Evo SSDs with 3D NAND, I believe several other companies are close to producing 3D NAND, will we see 3D NAND used in RAM soon or in a few years?"
hardware,3at4ik,AlchemicalDuckk,14,Tue Jun 23 16:11:26 2015 UTC,"DRAM is not the same as NAND flash. They are constructed using completely different electronics structures. DRAM is uses capacitors and transistors to store a bit, flash uses NAND gates. The way they are constructed also defines how they operate. DRAM is faster than NAND flash, so you're not going to be using flash as system memory."
hardware,3at4ik,Vince789,1 point,Tue Jun 23 10:53:24 2015 UTC,Oh never knew DRAM was different from NAND  Is there any 3D DRAM coming soon?
hardware,3at4ik,MINIMAN10000,3,Tue Jun 23 11:53:47 2015 UTC,"Inside the AMD Fury cards ""High Bandwidth Memory (HBM) is a high-performance RAM interface for through-silicon vias (TSV)-based stacked DRAM"" and a competing standard Hybrid Memory Cube HMC) is a high-performance RAM interface for through-silicon vias (TSV)-based stacked DRAM memory and as both pages say they are incompatible with the other."
hardware,3at4ik,Exist50,7,Tue Jun 23 21:17:08 2015 UTC,"There are two main 3D RAM technologies in the works, HBM (high bandwidth memory) by Hynix and AMD and HMC (hybrid memory cube) by Intel and Micron. The former is launching tomorrow in AMD's Fiji GPU."
hardware,3at4ik,dylan522p,3,Wed Jun 24 03:51:58 2015 UTC,The other is launching with xeon phi Knights landing which should be happening at the end of this year.
hardware,3at4ik,themangeraaad,4,Tue Jun 23 12:42:10 2015 UTC,"Not likely. As fast as NAND memory is,  it is still quite slow compared to DDR memory.   That said Sandisk released an enterprise SSD that runs in a DRAM DIMM slot.  This provides a latency benefit (compared to traditional SSDs) since the drive is right on the memory  bus rather than connected to the CPU through some other controller. You might see NAND used on a DIMM unit like this in the future but I'd suspect it would only be accessed as a SSD rather than as CPU memory."
hardware,3at4ik,spydr101,1 point,Tue Jun 23 16:10:43 2015 UTC,I just bought 32GB of DDR4 not even a month ago :(  At least I can double up for 64GB and not feel as bad
hardware,3at4ik,Mildcorma,-9,Tue Jun 23 12:00:02 2015 UTC,Yeah because that's not a pointless amount of ram... I bet you have an i7 as well but you only game? Sounds about right tbh
hardware,3at4ik,HeliconPath,8,Tue Jun 23 13:31:15 2015 UTC,"You are making quite a few assumptions based on nothing there, matey."
hardware,3at4ik,Syliss1,1 point,Tue Jun 23 22:43:28 2015 UTC,"Sweet! I'm upgrading my pc to x99, and I haven't gotten the RAM yet."
hardware,3at4ik,u-r-silly,1 point,Wed Jun 24 01:26:05 2015 UTC,Buy now before Skylake is out.
hardware,3at4ik,fuzzycuffs,10,Tue Jun 23 17:17:09 2015 UTC,"That's what I was wondering. When Skylake comes out, prices continue to fall or sudden demand pushes them back up?"
hardware,3at4ik,bphase,7,Tue Jun 23 10:56:58 2015 UTC,"Hard to say. Maybe they'll keep falling for now since production is ramping up fast but demand isn't there yet, with demand picking up after SKylake is released. So it indeed could be a good idea to buy them in now or in a month or so.  The price seems to be pretty reasonable by now, only about 20-30% more expensive than DDR3?"
hardware,3at4ik,cowbutt6,1 point,Tue Jun 23 11:31:29 2015 UTC,"Well, I think Skylake should support both."
hardware,3at4ik,damnshoes,1 point,Tue Jun 23 11:45:07 2015 UTC,"My expectation is that there'll be a short uptick shortly after Skylake is available to buy (as a result of increased demand, but supply not picking up immediately), but then the increasing mainstreaming of DDR4 will result in further falls to the retail price. Meanwhile, DDR3 production will be ramped down and prices will increase."
hardware,3ax2jx,k3mic,3,Wed Jun 24 04:55:12 2015 UTC,"I'll allow this post, because it could be very helpful to some people, but this isn't the kind of thing that's usually posted here.  Please x-post this to /r/TechSupport, you will probably help more people there."
hardware,3ax2jx,piovocsic,1 point,Wed Jun 24 04:57:34 2015 UTC,holy crap this is amazing! I thought it was a question and was going to ask you if you have access to TAC but then........... damn!!!  Great job dude. Please post this on the Cisco support forums if you didn't already.
hardware,3avoeh,CubemonkeyNYC,11,Tue Jun 23 21:41:59 2015 UTC,"How will you concretely see the benefits of improvements to a single component of a large system when the whole system has been improved on tests that are designed to assess the system as whole?  You can't.  You can try to isolate memory bandwidth, but that's just going to show what we already know: memory bandwidth has gone up a lot."
hardware,3avoeh,tarheel91,-8,Tue Jun 23 21:49:22 2015 UTC,"What aspects of fps, frame time, etc. will it affect?  It's a tent pole feature with few obvious benefits."
hardware,3avoeh,tarheel91,11,Tue Jun 23 21:56:53 2015 UTC,"It doesn't work like that.  All the different parts of a GPU work together to render a frame, and they all have to be taken into account when assessing performance.  I've never heard the term tent pole feature, but ""few obvious benefits"" is hilariously misinformed.  Memory bandwidth is absolutely critical in a bunch of different applications, especially games and even more so at higher resolutions.  Memory bandwidth goes from 320GB/sec on the 290x and 384GB/sec on the 390x to 512GB/sec on the Fury X.  That is a massive jump."
hardware,3avoeh,III-V,-13,Tue Jun 23 22:07:07 2015 UTC,"Well show me the obvious benefits then. Don't just mock me. At least mock me and show me what it'll do.  I've owned AMD cards since the 5870 and I'm interested in a fury, but I want to know why hbm matters when 4gb of it is compared to 6gb+ of GDDR5."
hardware,3avoeh,aziridine86,7,Tue Jun 23 22:15:49 2015 UTC,"Okay, so this is a horribly bastardized ELI5 version, because this is all way more complex (and this is still a monumentally massive mouthful -- 3M, get it? Ha, monumentally massive mouthful... that sounds dirty), but here goes:  Imagine that GPUs are just a Lego factory. This lego factory has four production lines, but they have to rely on a supplier for their acyrlonitrile butadiene styrene (say that 10 times fast) plastic that the bricks are made out. The ABS plastic is your memory bandwidth, and the four production lines each spit out a different color brick. Red bricks are compute, yellow bricks are textures, green bricks are pixels, and blue bricks are geometry. The red production line is called a shader, the yellow production line is called a texture mapping unit (TMU), the green production line is called a ROP, and the blue production line is called a geometry unit.  These bricks are sold to their customers in kits (frames), which have different quantities of each brick in them (different workloads). For example, one kit may need five of each brick, another may need a hundred of each. Most of the time though, the factory's customers want slightly different ratios of bricks (one frame may be slightly pixel-heavy, the slightly next geometry-heavy. Also applies to different games -- one may be more texture-heavy than another). Their needs are constantly changing (60-120 times a second, they want different ratios of bricks), and those colors are going in and out of fashion on occasion (new game engines and technologies can drastically change the ratios of ""bricks"" needed -- like, say 4K becomes a thing, now you need more ROPs to fill those pixels) so it's very difficult to optimize production to meet those needs.  Originally, the factory had equally-capable production lines, but they want to get a lego up on their gross Mega Blocks competition. They do an analysis, and see that green bricks are highest in demand right now. So they decide that they want to add more green brick production (ROPs), only to find out that they can't because their plastic supplier can't make enough plastic to feed them (not enough memory bandwidth). Not only that, but their plastic supplier literally cannot supply any more plastic, because they get a fixed amount of raw materials every day from recycling unwanted plastic (GDDR5 is basically maxed out).  So the Lego factory decides to cooperate with the plastic supplier, and figures out how they can work together to produce more plastic from less material (therefore breaking the laws of the universe, but I need to break them because I didn't think this far ahead). They spend a ton of money, and somehow figure out a new technology that creates double the plastic from the same amount of matter (HBM). As a reward for their hard work and investment, the Lego factory gets exclusive access to the extra plastic production for a while, thus enabling them to get an edge on their competition. Or so they'd hoped.  As it turns out, Mega Blocks figured out a way to use less plastic (bandwidth) in their green brick line (ROPs) without affecting their quality. Since they had more plastic freed up for use, they then expanded their green brick production (ROPs) as well. Oh, and they did this a year before Lego did, because breaking the laws of physics was a massive undertaking, and was really hard for Lego to do.  Somewhat back to reality, this is basically how things went down, and how Nvidia and AMD optimize their hardware (and yes, I did just call Nvidia Mega Blocks and AMD Lego -- deal with it; I'm usually really hard on AMD). They have a fixed amount of memory bandwidth available, because they can only do as much with their hardware as their memory subsystem can keep up with. The demands of games are constantly changing, so the AMD/Nvidia are constantly having to re-predict the future and optimize their GPUs the way they think is optimal when they finally hit the market 2 years or so later. ROPs are notoriously heavy on bandwidth, and Nvidia managed to add more ROPs by being able to use memory compression. Without that compression, Nvidia wouldn't have made any progress, because they would have been bottlenecked by their bandwidth requirements.  AMD chose to go a different route, the route they've always had a hand in (ATI/AMD has had a hand in pretty much all of the GDDRs, IIRC), and worked with Hynix to create a higher bandwidth solution. However, simply because they've solved their bandwidth problems does not mean that they can just explode ahead of their competition -- the new technology is expensive, and adding more shaders, ROPs and TMUs that can even take advantage of that seemingly unlimited bandwidth just is too expensive for them to do.  Make no mistake though, bandwidth is critical. You can't do anything without it. The ""problem"" is more of along the lines of:   How do I utilize that bandwidth best? Do I build more TMUs, ROPs, shaders, or geometry engines? How do I utilize all of this bandwidth while staying within budget (keeping die size within reason)?   And there you have a really dumbed-down, but still ridiculously long explanation of what bandwidth does, why it's important, and some history too."
hardware,3avoeh,III-V,1 point,Wed Jun 24 02:27:00 2015 UTC,"So is there no quantitative way to measure the impact of memory bandwidth on performance?  It seems like that is what Cubemonkey is asking.   Tarheel responding saying:   If you're expecting something like +10fps, that's not really something you can answer because frame draw time is so dependent on so many different things.   Basically suggesting that it is impossible to evaluate the impact of memory bandwidth independently from other factors, but least in cards where we have control over the memory clock speed, that shouldn't be the case, should it?  If the Fury X does have a locked memory clock, then we can't evaluate the performance impact of memory bandwidth via under or overclocking, but if someone does find a way to unclock the memory clock, shouldn't it be possible to evaluate the impact of HBM on the card's performance?  In other words, if the memory was clocked down to give bandwidth down to the level of the 290x or Titan X, how significantly would the performance of ROP-heavy workloads be affected?  Maybe those kinds of tests would be pointless because we can trust that AMD would not be investing in HBM (and Nvidia as well) if there was not a clear need for higher bandwidth.  Or perhaps is it the case that the move to HBM is prompted as much by is ability to reduce memory power consumption and to save space as it is by increased bandwidth?  But either way it does sound like he was asking for a more quantitative picture of the positive performance impact of HBM, and I'm not sure he should be downvoted to -10 for that."
hardware,3avoeh,aziridine86,2,Wed Jun 24 12:49:29 2015 UTC,"So is there no quantitative way to measure the impact of memory bandwidth on performance?  Basically suggesting that it is impossible to evaluate the impact of memory bandwidth independently from other factors, but least in cards where we have control over the memory clock speed, that shouldn't be the case, should it?   Right, you could underclock the memory and observe its effects.    If the Fury X does have a locked memory clock, then we can't evaluate the performance impact of memory bandwidth via under or overclocking, but if someone does find a way to unclock the memory clock, shouldn't it be possible to evaluate the impact of HBM on the card's performance?   Yes. I'd imagine it'd be possible to do it through BIOS editing, but I'm not educated on that procedure.    In other words, if the memory was clocked down to give bandwidth down to the level of the 290x or Titan X, how significantly would the performance of ROP-heavy workloads be affected?   You could get a rough idea by checking reviews of past cards on AnandTech, which have pixel fill rate benches.    Maybe those kinds of tests would be pointless because we can trust that AMD would not be investing in HBM (and Nvidia as well) if there was not a clear need for higher bandwidth. Or perhaps is it the case that the move to HBM is prompted as much by is ability to reduce memory power consumption and to save space as it is by increased bandwidth?   The purpose of HBM is two-fold. First is higher bandwidth, second is lower power. HBM drastically reduces the power that the memory subsystem uses, allowing that power to be used to increase core clocks.    But either way it does sound like he was asking for a more quantitative picture of the positive performance impact of HBM, and I'm not sure he should be downvoted to -10 for that.   Welcome to hardware forums, where anything that can be misconstrued as an attack on AMD will be subject to heavy disapproval. Wish the more blatant shills would get banned... Would be nice if every thread wasn't ""but muh AMD""  around here and elsewhere."
hardware,3avoeh,tarheel91,1 point,Wed Jun 24 14:17:02 2015 UTC,Thanks for the reply.
hardware,3avoeh,aziridine86,1 point,Wed Jun 24 14:18:40 2015 UTC,I really appreciate the effort it took to write all of that. Very informative. Thank you.
hardware,3avoeh,namae_nanka,9,Wed Jun 24 13:57:33 2015 UTC,"I just told you.  You're getting 384GB/sec of bandwidth on the 390X and 512GB/sec on Fury X.  That's the advantage.  It also takes a lot less power to use it.  If you're expecting something like +10fps, that's not really something you can answer because frame draw time is so dependent on so many different things.  It's just like with cars.  Let's say I told you I added a turbo to my engine and you asked me ""Well, how much faster are you going to go around a track?"" I couldn't really answer.  It's going to depend on what kind of tires I'm using, what the track is like, what ratios are used in my gearbox, how do the turbo and engine interact, etc.  If I'm using shitty tires you won't see any improvement, because I won't have any grip to put the extra power down."
hardware,3avoeh,aziridine86,-1,Tue Jun 23 22:23:02 2015 UTC,"512 GB/sec bandwidth on its own is just a number.  If they could show that the Fury X would have been significantly bottlenecked by <400 GB/sec of bandwidth, that would demonstrate that HBM was required to get full performance from this chip and that the switch from GDDR5 to HBM was clearly warranted.   On the other hand that begs the question of how the Titan X achieves its performance with only 336 GB/sec of bandwidth, unless it turns out that the Fury X is significantly faster than the Titan X.  Obviously the severity of the memory bottleneck is going to depend on the game in question and other factors like resolution and settings, but it would be nice data to see nonetheless."
hardware,3avoeh,Seclorum,1 point,Tue Jun 23 22:55:01 2015 UTC,"The situation has gone upside down since kepler vs. hawaii where hawaii had 64ROPs to kepler's 48, however kepler had more bandwidth for them.   Now Fury has 64 of them while Titan X has 96 and the bandwidth situation is now in AMD's favor. Massively. It most probably played a factor in AMD going along with 64ROPs and saving die area where they were almost at the limit of what's possible."
hardware,3avoeh,MerryLane,1 point,Wed Jun 24 01:37:53 2015 UTC,"Are you saying Hawaii already had a significant amount of excess memory bandwidth (i.e. headroom) in the 290x?  EDIT: Or do we just not have the data to quantify that in a concrete way? (e.g. benchmarks showing how a R9 290x performs in several different games at several different memory clock speeds, both under and overclocked)"
hardware,3avoeh,namae_nanka,-1,Wed Jun 24 01:51:41 2015 UTC,"You're getting 384GB/sec of bandwidth on the 390X and 512GB/sec on Fury X. That's the advantage.    Sticking to your car analogy, this is as helpful as somebody saying I'll have less backpressure on my exhaust.   I suppose we'll just see. I doubt that the bandwidth will make up for the lower total memory compared to the 980 Ti."
hardware,3avoeh,dylan522p,3,Wed Jun 24 03:55:53 2015 UTC,"I suppose we'll just see. I doubt that the bandwidth will make up for the lower total memory compared to the 980 Ti.   It might, but it really depends on just how well optimized AMD's memory compression tech is, if they even bothered to look into it. As well as how fast the Fiji core can chew through data to free up space in the buffer."
hardware,3avoeh,namae_nanka,1 point,Wed Jun 24 06:05:30 2015 UTC,"It's more about the power budget as of now.  => Less TDP for the memory = more TDP for the chip. That's the only benefit for now.  Else, it solves problems that only need to be solved in a few years. Bandwith is irrelevant because the textures can be compacted. Developpers for both AMD and NVIDIA barely worked on that problem actually. Generally card makers scale the strength of their chip to its bandwith, too, so it's never/almost never an issue, and then you can also ask for memories that can run at a higher frequency to solve the bandwith issue.  That's actually the sole (or biggest reason) of why AMD went for HBM now. If they don't use it, they wouldn't be able to go for 500+nm chips, unlike green team that is, here, limited by yields issues of such a high surface, too much chips would be unperfect. I mean, they could, but their card would produce so much heat they'd have to put more fans going faster, which would be detrimental to the user (noise, noise everywhere).  If you spose that some side puts a lot of effort (money and talent and time) into improving their energy efficiency, by reducing power leakage and turning off unused transistor, then by going on smaller nodes, this would bypass the need for this new kind of memory before years. That's actually what happened.  TL;DR : Cardmakers would be better off focusing on improving their architectures, reducing power consumption, AND MOSTLY rushing newer nodes ASAP, before going for HBM. Going for HBM is only cost efficient if you get a good bargain (kind of beta-tester price reduction), because if they put the real price into improving the architectures up to a certain point (hyperbolic yields), they'd have more benefits."
hardware,3avoeh,Seclorum,1 point,Tue Jun 23 22:19:47 2015 UTC,Going for HBM is only cost efficient if you get a good bargain (kind of beta-tester price reduction)   or if you helped develop it in the first place. Which is why it's comical to see folks bringing up nvidia's CEO calling HBM expensive.
hardware,3avoeh,MerryLane,2,Wed Jun 24 01:44:28 2015 UTC,"HBM is extremely expensive, are you kidding me?"
hardware,3avoeh,troublegoats,1 point,Wed Jun 24 01:58:39 2015 UTC,are you kidding me?   That's my line.
hardware,3avoeh,TaintedSquirrel,1 point,Wed Jun 24 02:17:18 2015 UTC,"There are other reasons for HBM instead of just power and just bandwidth.    Massively Decreased PCB space needed HBM gen 2 will increase the memory density on the board enormously, upwards of 32gb onboard."
hardware,3avoeh,Seclorum,0,Wed Jun 24 05:59:37 2015 UTC,"These are two reasons that don't need to be solved within a few years. Damn, either AMD fans can't read, or their brain just doesn't allow them to make sense.  The only reason, to increase NOW ""performance data"" like OP said, is the TDP, for AMD. Any other issue is a non issue before years.  AMD just didn't have enough money and people working for them to improve their architecture that is roughly 30% worse than their competitors in performance/transistor and power/transistor. So they couldn't make a big chip with big memory without requiring a 400W PSU for it alone, and 5 fans at 3500 RPM at idle.  That's why they went sooner for HBM, and when they say they developped it, they either paid it cheap as beta testers, either paid a lot and they sell fury X at loss. Which is why they'll sell 30k.  Kinda exagerating, but that point of view kinda sticks to the reality"
hardware,3avoeh,kennai,2,Wed Jun 24 10:59:28 2015 UTC,Underclock the memory to gddr5 speed and see what happens
hardware,3avoeh,Seclorum,2,Tue Jun 23 21:55:45 2015 UTC,"Memory clocks are locked.  Maybe AMD will unlock it later, maybe some custom software/mods will unlock it for us."
hardware,3avoeh,Seclorum,3,Tue Jun 23 22:04:36 2015 UTC,It already IS clocked lower than GDDR5. It's only running 500mhz.   This is because the bus is so wide the available bandwidth remains higher even when it's clocked way lower.
hardware,3avoeh,kennai,3,Tue Jun 23 23:28:32 2015 UTC,He's saying simulate GDDR5 bandwidth on HBM/Fury X and see what happens.
hardware,3avoeh,Seclorum,1 point,Wed Jun 24 03:57:08 2015 UTC,"Well to start with your going to have to clock it way way down. So from 500mhz down to around 250mhz.   But beyond that I dont really see the point to clocking it down against GDDR5.   The power savings from the memory are significant, and were invested into the monster die.   The massive bus width just means you dont need to clock the memory as high to achieve the same overall bandwidth. This is advantageous with HBM because you dont really have a way to effectively cool the whole stack so you need to keep the clocks within a certain thermal range."
hardware,3avoeh,R403Q,1 point,Wed Jun 24 05:01:44 2015 UTC,Addendum to last post:   Is the objective to see just how much bandwidth the Fiji needs?   Because really massive bandwidth is one of the last things the setup gives.    Decreased Vram Voltages and overall Vram power consumption.  Massively decreased PCB space needed. HBM gen 2 will drastically increase Vram densities on boards in excess of 32gb of onboard Vram.  The massive bus and bandwidth will feed hungry GPU's without having to clock memory sky high.
hardware,3avoeh,HavocInferno,2,Wed Jun 24 05:55:47 2015 UTC,"The title of this topic is ""Regarding AMD's HBM, how will we be able to concretely see its benefits in performance data?""  To answer the question he's saying to simulate GDDR5 bandwidth with the Fury X/HBM and see what the bandwidth increase will do. Of course there's other improvements but that's a real easy way to do it."
hardware,3avoeh,spellstrike,1 point,Wed Jun 24 06:08:48 2015 UTC,Yeah the benefits in terms of performance will really depend on just how much bandwidth Fiji needs. HBM doesn't really give a performance boost to actual processing so unless the GPU is bandwidth starved it's not going to show up at all. But as you clock the memory down to choke off bandwidth you will see a difference when it gets bottlenecked.
hardware,3avoeh,spellstrike,2,Wed Jun 24 06:17:49 2015 UTC,It will be tough to do a conclusive test on that because there is no GDDR5 fury card to use as a reference.
hardware,3aur0m,JustineCase69,2,Tue Jun 23 17:48:25 2015 UTC,So much in such a little package.  ( ͡° ͜ʖ ͡°)
hardware,3aur0m,HumanistGeek,1 point,Tue Jun 23 22:22:12 2015 UTC,"I really wanted to get one of the Intel ones when they came out but the reviews are less than steller. The Atom proc, Z3735F, in there is more than a year old and i guess using wifi and bt at the same time yields poor connections since they are both on the same chip  wait till the next model with the new rounds of atom procs"
hardware,3aur0m,longlive289s,1 point,Tue Jun 23 23:25:12 2015 UTC,Long time in development and finally coming to market.  Expect a revised version within a year with a better CPU.  Now that they have the design and manufacturing down they can upgrade quickly.
hardware,3aur0m,Thunder_Bastard,1 point,Wed Jun 24 05:12:08 2015 UTC,"Another 2gb ram 32gb ssd micro PC. Just like all the other ones on the market for the last 6 months. Nothing really new, just made known company."
hardware,3axu89,Jayce_Pulsefire,4,Wed Jun 24 11:00:37 2015 UTC,it is entirely possible and highly likely. Intel always puts those damn iGPUs in their i7s(excluding extreme edition).
hardware,3axu89,haekuh,1 point,Wed Jun 24 13:21:40 2015 UTC,AFAIK they plan to put those mid-tier GT2 into already announced CPUs.. I'm curious if/when they plan to release higher-end iGPUs into unlocked CPUs..
hardware,3axu89,sk9592,1 point,Wed Jun 24 13:36:01 2015 UTC,You mean excluding LGA 2011. The i7-5820k and i7-5930k are not extreme editions and do not have iGPUs.
hardware,3axu89,haekuh,1 point,Wed Jun 24 15:11:00 2015 UTC,yea sorry
hardware,3axu89,communist_llama,2,Wed Jun 24 20:32:37 2015 UTC,I certainly hope so. Quick Sync has been Incredibly useful.
hardware,3axu89,JohnyTucker,1 point,Wed Jun 24 13:50:58 2015 UTC,"IDK man, it's not clear to me if the ""K"" versions will have GT2e or GT4e igpu. Look at here for example: http://latestworldtechnology.com/intel-skylake-s-announced-idf-2015/ According this there should be GT4e in K version, or am i wrong? And i read everywhere that ""Skylake igpu will be faster than Broadwell igpu"" Well, we'll see. One thing's for sure, it has to be better than Haswell igpu. That's good enough for me :-)"
hardware,3axu89,leafthegreen,1 point,Wed Jun 24 21:09:08 2015 UTC,"I was hoping to get GT4e, because I plan to run on iGPU untill pascal arrives, as I won't pay 500+ bucks for 4 year old technology in form of 28nm GPUs. We shall see and hope though..."
