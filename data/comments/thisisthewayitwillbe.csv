thisisthewayitwillbe,3e4hf2,starspawn0,4,Tue Jul 21 21:58:35 2015 UTC,"Funny, I feel the same way about most managers. However, just a software program will suffice in their case!"
thisisthewayitwillbe,3e36tz,starspawn0,1 point,Tue Jul 21 16:38:20 2015 UTC,"@Noahpinion:   2015-07-20 14:46:29 UTC  Guess what? Prizes for innovation just don't work that well. There's no escaping the ""optimal patent"" question. nber.org     [Mistake?] [Suggestion] [FAQ] [Code] [Issues]"
thisisthewayitwillbe,3e0l7k,starspawn0,5,Tue Jul 21 01:05:35 2015 UTC,This is incredible. I've had an ongoing argument with someone for a long time that neural networks are stupid because they can't recognize symbols and drawings.
thisisthewayitwillbe,3e0l7k,Noncomment,3,Tue Jul 21 02:07:34 2015 UTC,Here is the arxiv paper:  http://arxiv.org/abs/1501.07873
thisisthewayitwillbe,3e0l7k,queerMTFchicago,2,Tue Jul 21 12:36:35 2015 UTC,"Any technical information here on how they achieved this? Training information, neural net set up, etc"
thisisthewayitwillbe,3e079g,neuromorphics,3,Mon Jul 20 23:12:37 2015 UTC,Might turn up the heat on students (and maybe some authors)
thisisthewayitwillbe,3dwqka,starspawn0,1 point,Mon Jul 20 04:16:16 2015 UTC,Here is a non-mobile link: https://twitter.com/pmarca/status/622873772685529088  Sourcecode | Feedback?
thisisthewayitwillbe,3dwqka,untouchedURL,1 point,Mon Jul 20 04:16:30 2015 UTC,@pmarca:   2015-07-19 21:00:54 UTC  An entire generation of economic experts/investors anticipating inflation that never comes! üòÆ blogs.ft.com pic.twitter.com [Imgur]     [Mistake?] [Suggestion] [FAQ] [Code] [Issues]
thisisthewayitwillbe,3dw2g9,starspawn0,2,Mon Jul 20 00:35:11 2015 UTC,"Cool. I wonder if this can be applied to NPCs in future video games, so as to give them a broader range of responses."
thisisthewayitwillbe,3dvbz8,brylevkirill,2,Sun Jul 19 20:48:10 2015 UTC,"Policy learning for partially observed control tasks requires policies that have the ability to store information from past observations. In this paper, we present a method for learning policies with memory for high-dimensional, continuous systems. Our approach does not assume a known state representation and does not attempt to explicitly model the belief over the unobserved state. Instead, we directly learn a policy that can read and write from an internal continuous-valued memory. This type of policy can be interpreted as a type of recurrent neural network. However, our approach avoids many of the common problems that plague RNNs, such as the vanishing and exploding gradient issues, by instead representing the memory as state variables. The policy is then optimized by using a guided policy search algorithm that alternates between optimizing trajectories through state space (including both physical and memory states), and training a policy with supervised learning to match these trajectories. We evaluate our method on tasks involving continuous control in manipulation and navigation settings, and show that our method can learn complex policies that successfully complete a range of tasks that require memory.   ...   In direct comparisons, we find that our approach outperforms a method where the neural network in guided policy search is na¬®ƒ±vely replaced with a recurrent network using backpropagation through time, as well as a purely feedforward policy with no memory.   ...   The memory states are added to the state of the system, and the policy is tasked both with choosing the action and modifying the memory states. Although the resulting policy can be viewed as an RNN, we do not need to perform backpropagation through time to train the recurrent connections inside the policy. Instead, the memory states are optimized by the trajectory optimization algorithm, which intuitively seeks to set the memory states to values that will allow the policy to take the appropriate action at each time step, and the policy then attempts to mimic this behavior in the supervised learning phase.   ...   However, when viewed together with the memory states, the policy is endowed with memory, and can be regarded as a recurrent neural network. Our experimental results show that our method can be used to learn policies for a variety of simulated robotic tasks that require maintaining internal memory to succeed. Part of the motivation for our approach came from the observation that even fully feed-forward neural network policies could often complete tricky tasks that seemed to require memory by using the physical state of the robot to ‚Äústore‚Äù information, similarly to how a person might ‚Äúremember‚Äù a number while counting by using their fingers. In our approach, we exploit this capability of reactive feedforward policies by providing extra state variables that do not have a physical analog, and exist only for the sake of memory.   ...   One interesting direction for follow-up work is to apply our approach for training recurrent networks for general supervised learning tasks, rather than just robotic control. In this case, the memory state comprises the entire state of the system, and the cost function is simply the supervised learning loss. Since the hidden memory state activations are optimized separately from the network weights, such an approach could in principle be more effective at training networks that perform complex reasoning over temporally extended intervals. Furthermore, since our method trains stochastic policies, it would also be able to train stochastic recurrent neural networks, where the transition dynamics are non-deterministic. These types of networks are typically quite challenging to train, and exploring this further is an exciting direction for future work."
thisisthewayitwillbe,3dtzfe,johnnd,2,Sun Jul 19 13:17:50 2015 UTC,See this article on Jack Andraka:  http://scholarlykitchen.sspnet.org/2014/01/03/the-jack-andraka-story-uncovering-the-hidden-contradictions-of-an-oa-paragon/
thisisthewayitwillbe,3dtzfe,starspawn0,2,Sun Jul 19 13:31:44 2015 UTC,Thanks. It does all seem a bit too Disney-y to be true.
thisisthewayitwillbe,3dtzfe,RushAndAPush,2,Sun Jul 19 17:25:38 2015 UTC,This kid seems to be taking credit for things that he didn't create.
thisisthewayitwillbe,3dpvra,MissKaioshin,-2,Sat Jul 18 07:57:16 2015 UTC,"Here's something particularly damning in the comments section:   Let me tell after working for a company for 5 years leading efforts to develop wearable sensors. Almost every company that is out there now marketing these wearable products, including Apple, is exaggerating and covering up serious flaws. A adoring media that is in love with the idea of wearables rarely questions the companies claims. Indeed a lot of stories read like they were planted by the companies themselves. Broadly after 5 years of development, my team found there were a few classes of sensors in the market.   Accelerometer based ones. There are by far the most popular, because theses are the only ones that don't require actual contact with the skin and work regardless of sweat, tattoos, a slight loose strap etc. Almost all of them use accelerometer data to count steps, calculate calories, measure ""resting"" heart and respiration (by parsing small repetitive movements). These accelerometer based sensors are OK at counting steps but pretty much suck at finer movements like breathing and heart rate, since they have no way of isolating those from other vibrations that are all around us. Optical sensors - these are used for temperature, heart rate, oxygen levels. These suck, because they detect only surface temperatures and not core body temperature ( I wil address the ""correction factor"" in a bit""). And these have be strapped tight to prevent leakage of ambient radiation into the sensor (yeah, please don't tell me you will place the sensor in a recessed well to restrict aperture or its subtended angle of lines of sight, because we already tried that, and it doesn't work). These also get dirty and fouled up easily. And the ""correction factor"" to convert surface to corebody temperature is a joke. We tried experiments in various external temperatures with animals who had temperatures and found no statistically significant co-relation. Some genius here will point out we could have a temperature sensor pointing away from the body and into the ambient and incoporate that in a feed back loopwhen applying the ""correction factor"". Yes, we tried that too. That didn't work. The optical sensors in a given room depending on whether they were facing a window, a hvac duct, a plasma TV, a door or a hot car, whatever, would supply garbage data. Anyway optical sensors were a joke. Patches, and swallowable sensors - The swallowable temperature sensors, while accurate, are excreted. No one wants to swallow a sensor each day. The patches, rarely worked as claimed because of temperature changes, sweat, and just plain variance between human to human. Plus not too many people we surveyed we excited about putting a sticker on themselves and peeling it off other than for a very short period of time. And yeah, that whole lie about measuring blood glucose levels from sweat? Well let me tell you that the handheld glucometers (we used several brands) were far more in agreement with each other than the patch and far more closer to the actual lab results we did in a hospital.  Most of these sensors work ""roughly"" about 50-70% the time. All of them are quite useless when compared to actual medical sensors you would find in dedicated equipment in hospitals. Think of it this way. Would you black out your car windshield and drive the roads with only your google maps showing the road and turns? That is about the precision of today's wearables. They are useless but no one wants to say so. I've wasted 5 years of my life going from university to university paying learned faculty fools and underpaid graduate students to test sensors and produce reproducible, statistically valid, ""clinically tested"" studies and it was all a colossal waste of time and money. My take is this - if a instrument doesn't use reagents (wet dry) or doesn't invade the body (either physically via a probe, or needle, or electromagnetically - xray, MRI, ultrasound)) then it produces garbage data. Non invasive, passive sensors with no reagents are garbage pure and simple    So yeah, it looks like wearables are pure hype, too."
thisisthewayitwillbe,3dmeeb,johnnd,0,Fri Jul 17 13:22:21 2015 UTC,"Repeat after me: Moore's law has nothing to do with benchmark performance.  Repeat 2 after me: the only relevant benchmark is your code.  Particularly looking at extremely unbalanced GPGPU computation in this context is extremely misleading. Just look at random access latency of GDDR5 -- perhaps HBM will be flatter here, we'll see.  Far more interesting observations: how well does your code map to a 3d torus of nodes on a high-bandwidth low-latency signalling fabric, with no shared memory? Particularly, small-footprint nodes, since cluster on a chip.  So look at Parallella Epiphany or Xeon Phi today for hints of the future."
thisisthewayitwillbe,3dmeeb,eleitl,2,Fri Jul 17 16:03:11 2015 UTC,So look at Parallella Epiphany or Xeon Phi today for hints of the future   That's so adorable!  In the GPU programming model access latency is almost never an issue. Prefetching is deterministic rather than probabilistic/predictive. Compute blocks are determined ahead of time and the only latency evident to the ALU is in the worst case global cache (L2 ATM).
thisisthewayitwillbe,3dmeeb,NanoStuff,0,Fri Jul 17 16:19:18 2015 UTC,"In the GPU programming model access latency is almost never an issue.   Yeah, worst case = best case. Sure.   Prefetching is deterministic   So you have to tailor your accesses to achieve useful bandwidth. The opposite of flat access of embedded memories.   the worst case global cache (L2 ATM).   Riddle me this Batman: how does global sharing work where no two bits can occupy the same space, and the only way to effect state change is by relativistic signalling?"
thisisthewayitwillbe,3dmeeb,eleitl,1 point,Fri Jul 17 16:27:10 2015 UTC,"So you have to tailor your accesses to achieve useful bandwidth   Efficiency over abstraction. I never found this to be particularly difficult, rather intuitive actually.   how does global sharing work where no two bits can occupy the same space   I'm unclear what the issue here is. It is low latency memory, which could even be accessed efficiently speculatively if needed."
thisisthewayitwillbe,3dmutj,starspawn0,2,Fri Jul 17 15:38:50 2015 UTC,"Ok, there might be a paywall, depending on where you are reading this from (I'm reading it from my office computer).  Let me at least give you a bit of the conclusion:   Many times during the past 50 years, enthusiastic researchers have had high hopes that the language-understanding ability of robots in science fiction movies was just around the corner.  However, in reality, speech and language understanding did not work well enough at that time to power mainstream applications. The situation has been changing dramatically over the past five years. Huge improvements in speech recognition have made talking to your phone a commonplace activity, especially for young people. Web search engines are increasingly successful in understanding complex queries, and MT can at least yield the gist of material in another language, even if it cannot yet produce human-quality translations. Computer systems trade stocks and futures automatically, based on the sentiment of reports about companies. As a result, there is now great commercial interest in the deployment of human language technology, especially because natural language represents such a natural interface when interacting with mobile phones. In the short term, we feel confident that more data and computation, in addition to recent advances in ML and deep learning, will lead to further substantial progress in NLP. However, the truly difficult problems of semantics, context, and knowledge will probably require new discoveries in linguistics and inference."
thisisthewayitwillbe,3dn9x7,Yuli-Ban,2,Fri Jul 17 17:32:54 2015 UTC,"I've seen the article all over the internet.  I think it's mostly hype.  Yes, there's a real discovery here; but it's not really a new particle -- it's what is known as a ""quasi-particle"" or ""particle analogue"" (which is not a fundamental particle at all).  It's a little like saying that, ""We've discovered a black hole here on earth!""; when, in fact, what was discovered might be a tornado whose vortices behave similarly to a black hole (even though they don't suck in matter and light the way a real black hole does)."
thisisthewayitwillbe,3dksyc,starspawn0,2,Fri Jul 17 02:01:45 2015 UTC,"To have an machine learning day and night working on your research project with you, working things out as you sleep. Checking in the next day to find great idea's it brings to you, from it's night long working, with the area's you were working on the prior day.   Yes He sounds like he's got the right idea."
thisisthewayitwillbe,3dktt4,starspawn0,-3,Fri Jul 17 02:09:16 2015 UTC,Nice to have rock-solid confirmation that technology is slowing down. He says that 2045 is going to look more like today than Kurzweil and his followers will like to admit. I wonder what their response will be when they hear that. Call Markoff a troll?
thisisthewayitwillbe,3dktt4,MissKaioshin,2,Fri Jul 17 04:31:37 2015 UTC,"His opinion is just as valid as Kurzweil's one, or the Starbucks guy I met this morning. No one can predict the future."
thisisthewayitwillbe,3dktt4,Fab527,1 point,Fri Jul 17 15:29:15 2015 UTC,"Transistor count isn't the only way to measure performance. Its like judging a CPU on Hertz. Within the last generation of chips there is more going on than transistor count. There is a plethora of computation innovations going on. Besides GPU trends are still solid.   Even if the the current computation paradigm follows the typical computational s-curve trajectory that doesn't mean technology is slowing down.   God damn he isn't nearly as gloomy as you, MissThingsare going to be devastatingly bad. In fact he isn't really gloomy at all.    Is it going to be a science-fiction utopia or a science-fiction nightmare? It's going to be a little bit of both.                                I was pretty early to understanding that robots and robotics, automation technology, and artificial intelligence were going to have a Renaissance. Now, all of a sudden, it's the white-hot center and I've started to look around for other things that are interesting, that are on the edge, if you will.   Some things he is clearly underestimating.                                     so it's not clear that you'll be able to turn these machines loose in the environment to be waiters or flip hamburgers    There are tons of examples. Seriously, god damn.   Another example he is underestimating.    There are a number of small and large companies who are basely arguing and or trying to develop this technology that causes the mouse to disappear, the keyboard to disappear, the smart phone disappear. You will interact with the computing resources that are all around you in the Cloud and wherever else by just speaking and looking through your glasses. I thought that was entirely science-fiction, and more recently I've seen HoloLens from Microsoft."
thisisthewayitwillbe,3dj9ms,johnnd,4,Thu Jul 16 18:45:30 2015 UTC,"Some ASML customer had already exposed over 1000 wafers in a 24-hour period. System availability time should be 70% by year-end.  The new drive laser from Trumpf will provide a source power of 125W (130W already demonstrated internally at ASML), the level deemed necessary for true volume production. Source power was lagging for a long time, but has been improving really nicely over the last year or so.  We're not too far away from 1500 wafers a day, which is when EUV will really become a commercial reality."
thisisthewayitwillbe,3dilm9,starspawn0,3,Thu Jul 16 15:51:42 2015 UTC,Bookmarked. Let's see how this works out.
thisisthewayitwillbe,3dilm9,brihamedit,0,Fri Jul 17 00:26:48 2015 UTC,"I've already got this place, its' doing all the sifting, and showing all the most way to kewl stuff I want to see. Why download there stuff? Ok maybe later..."
thisisthewayitwillbe,3dg5h4,starspawn0,3,Thu Jul 16 00:31:40 2015 UTC,Gorila takes a further step towards fulfilling the promise of deep learning in RL: a scalable architecture that performs better and better with increased computation and memory.
thisisthewayitwillbe,3df4ak,starspawn0,4,Wed Jul 15 19:52:54 2015 UTC,paywall reported
thisisthewayitwillbe,3df4ak,sasuke2490,1 point,Wed Jul 15 22:42:11 2015 UTC,Hmmm... it wasn't there earlier.  I'll look for alternate sources.
thisisthewayitwillbe,3df4ak,NuScorpii,1 point,Thu Jul 16 00:37:30 2015 UTC,"It's not a paywall, you just need to register for free."
thisisthewayitwillbe,3dem77,mirror_truth,1 point,Wed Jul 15 17:49:28 2015 UTC,"As machines become more human-like, questions like this will become more uncomfortable for most people.  Here's a question I considered the other day:  using an fmri scanner and some machine learning software, a computer can predict what you will do a few seconds from now.  In a sense, that program is running a very crude simulation of the brain -- good enough to make acurate predictions.    Can one say that, at some  crude level, the program is ""thinking""?"
thisisthewayitwillbe,3d8pai,johnnd,2,Tue Jul 14 11:12:20 2015 UTC,http://i.imgur.com/Hth8MdG.jpg  Higher-res pics to come.
thisisthewayitwillbe,3d8pai,andmar74,1 point,Tue Jul 14 13:32:25 2015 UTC,"Beautiful !  The definition of a planet is a bit shaky, when Earth placed in a Pluto orbit would not be a planet."
thisisthewayitwillbe,3d5izb,starspawn0,2,Mon Jul 13 18:12:08 2015 UTC,"Makes sense. While learning new things, we also learn how to learn related things."
thisisthewayitwillbe,3d43sy,SnowLong,5,Mon Jul 13 10:50:59 2015 UTC,"Incredible. My first thought was they look like real photographs. Then I noticed the weird distortions. But they still look quite real, just like they were edited in a very unsettling way."
thisisthewayitwillbe,3d43sy,Noncomment,1 point,Mon Jul 13 12:17:36 2015 UTC,@AlecRad:   2015-07-12 21:14:11 UTC  Slowly progress on deep GANs... #creepypeople pic.twitter.com [Imgur]     [Mistake?] [Suggestion] [FAQ] [Code] [Issues]
thisisthewayitwillbe,3d43sy,TweetPoster,2,Mon Jul 13 10:51:34 2015 UTC,"Are these computer generated faces or a snapshot of what the computer sees when it sees faces, I'm not entirely sure what this is. What is GAN?"
thisisthewayitwillbe,3d43sy,superexistence,2,Mon Jul 13 14:58:50 2015 UTC,"This is generated from scratch images. GAN stands for generative adversarial networks. Here is one that was trained on faces: http://vision.stanford.edu/teaching/cs231n/reports/jgauthie_final_report.pdf  Essentially researchers train GAN so it generates images that main neural net think is real. After GAN learned to fool main model reliably, the discriminative model learn to distinguish between real images and generated one. Rinse/repeat, train next generation GAN to fool new main model and then adjust model so it again could sort out fake images.  Idea is that one will get a good generative model at the end of the process and main model will learn to look for better features to discriminate images."
thisisthewayitwillbe,3d40xw,HD125823,1 point,Mon Jul 13 10:12:19 2015 UTC,That's excellent!  Very detailed and informative.
thisisthewayitwillbe,3d133m,starspawn0,2,Sun Jul 12 17:19:57 2015 UTC,Loved how LeCun said that Facebook's AI group shares source code and ideas with DeepMind group. And sure Yann used that moment to advertise that both groups use Torch as expected LOL.
thisisthewayitwillbe,3d133m,SnowLong,2,Mon Jul 13 05:32:57 2015 UTC,Turns out Neil Lawrence was the panelist who said it.  It would be interesting to see what the response was from the other panelists.
thisisthewayitwillbe,3d133m,RushAndAPush,3,Sun Jul 12 17:31:44 2015 UTC,Could you explain what exactly the quote means?
thisisthewayitwillbe,3d133m,RushAndAPush,3,Sun Jul 12 18:05:05 2015 UTC,"It's a long story; but, basically, many in the NLP / conputational linguistics are suspicious of deep learning.  This quote is surely to be taken in that context, as if to say, ""No... it will do to NLP what it did to computer vision.  It's not hype!""  NLPers' suspicion about deep learning goes back to the criticisms of Fodor et al of connectionism, and whether ""distributed representations"" are adequate structures / ""the right representation"" for processing ""meaning"".    There is also criticism around the use of strong baselines, not considering realistic (not ""toy"") problems, and not openly sharing of code / algorithmic secrets."
thisisthewayitwillbe,3d133m,untouchedURL,4,Sun Jul 12 18:30:38 2015 UTC,Thanks.
thisisthewayitwillbe,3d133m,TweetPoster,0,Sun Jul 12 18:33:37 2015 UTC,Here is a non-mobile link: https://twitter.com/sgouws/status/619880524329414656  Sourcecode | Feedback?
thisisthewayitwillbe,3cyj86,starspawn0,1 point,Sat Jul 11 23:03:13 2015 UTC,@shakir_za:   2015-07-11 14:43:25 UTC  Juergen schmidhuber: within next 10 years we'll have a small monkey-like in a simulated world. #ICML2015     [Mistake?] [Suggestion] [FAQ] [Code] [Issues]
thisisthewayitwillbe,3cyj86,TweetPoster,1 point,Sat Jul 11 23:04:14 2015 UTC,Here is a non-mobile link: https://twitter.com/shakir_za/status/619879673502298112  Sourcecode | Feedback?
thisisthewayitwillbe,3cx3p9,starspawn0,2,Sat Jul 11 15:26:16 2015 UTC,"Bouchard's talk ""Programming by  Teaching"" is fascinating."
thisisthewayitwillbe,3cx3p9,brylevkirill,2,Sat Jul 11 17:46:39 2015 UTC,"Totally agree with you!  Another somewhat similar initiative as part of Language Understanding Intelligent Service in Microsoft's Project Oxford with participation of Chris Bishop. This service is very probably built using Probabilistic Programming (and Infer.NET lead by Chris) - information extraction was the goal of one ambitious project (also lead by Chris) in Microsoft Research Cambridge I told you about earlier.  Probabilistic Programming is indeed a natural solution for teaching machine using natural language and minimal data. And it is already in use by the industry (although for simpler tasks), see applications of Infer.NET in Microsoft Office 365 and Microsoft Excel."
thisisthewayitwillbe,3cx3p9,brylevkirill,2,Sun Jul 12 14:13:04 2015 UTC,"Guillaume Bouchard also works on embedding logic into vector space in collaboration with Sebastian Riedel, Sameer Singh and Tim Rockt√§schel (currently at DeepMind) whose publications (including ""Embedding Probabilistic Logic for Machine Reading"") I referred to.  Good luck to him. Insanely huge amount of effort is spent on developing business logic by programmers instead of domain experts, and  software for enterprises is often impossible or unpleasant to use for knowledge workers without training courses."
thisisthewayitwillbe,3cuias,starspawn0,5,Fri Jul 10 21:53:34 2015 UTC,PC sales are declining because people are keeping them longer. If there were a death of the PC the way to show this would be to post statistics on what percent of the population uses one (the same way you could talk about the death of the dot-matrix printer as everyone uses other types of printers--laser or ink jet...).
thisisthewayitwillbe,3ct6so,brylevkirill,3,Fri Jul 10 16:00:48 2015 UTC,"Ok, having listened to this now, here are some comments:   I think Chris Bishop said the most sensible things of all the panelists.  What he said about combining neural network discriminative approaches with generative approaches a la probabilistic programming, was very interesting (and the projects that Microsoft has ongoing).  He said many things that just sounded ""right"". Oren Etzioni mentioned Hinton's ""thought vectors"" as an example of ""hype"".  We'll see whether he and people like Raymond Mooney are right, or whether Hinton and the Deep Learning people are right.  My money would be on Deep Learning completely transforming NLP in the next few years. Josh Tenenbaum has done some really interesting work on the ""Bayesian Brain"" and probabilistic programming.  From what I've read, however, it's still far too slow to be practical (as hinted at by Chris Bishop), except in certain specific cases (e.g. to train classifiers).  Meanwhile, purely data-driven approaches continue to advance. Fei Fei Li:  I didn't really get much out of what she said.   Michael Littman didn't say much."
thisisthewayitwillbe,3ct6so,starspawn0,2,Fri Jul 10 20:48:16 2015 UTC,"I consider logic (including probabilistic logic and probabilistic programs as extensions) to be a kind of knowledge representation (another one is distributed continuous representations used in Deep Learning) with several possible approaches for inference (proof procedures for classical logic, ILP or SAT solver for probabilistic logic, neural network variational inference for graphical models). Similarly to embedding of entities and relations in recent Deep Learning research, it could be possible to map constructs of probabilistic logic and programs to distributed continuous representations (thought vectors) and probably back using RNN or more mathematically explicit procedures (such as ones in this line of research: 1, 2).  Such representations empower RNNs as a tool for inference with some unique properties (not easily shared by inference methods in GOFAI): they can memorize (and even generalize from) huge amount of heuristics, can find nontrivial connections and similarities between entities (reasoning by analogy), can visualize entities (sampling from memory), can potentially come up with new concepts on their own (abstracting from memory), can potentially perform meta-thinking (reflecting on its processes). Though RNN inference converges to logical inference in many cases, especially when we would like to also get an interpretable explanation and not just statistical guess.  RNN inference over natural language texts is more powerful than inference provided by probabilistic knowledge bases in the limit (the former subsumes the latter and is not a closed system). Embedding spaces used in Deep Learning and their representational power are not yet understood (even their structure for word2vec, let alone for thought vectors of RNN). Research to represent logic (and not just a set of activations corresponding to entities/concepts in picture or sentence) as thought vectors will continue. Memory Networks operate on primitive embeddings of simple sentences as facts and can be expected to operate later also on rich embeddings of logical statements, especially considering that the simplest way to bootstrap reasoning by RNN is to train it on data generated using GOFAI methods.  Learning from data (probabilistic knowledge bases also do that) is orthogonal to knowledge representation (and to inference), and different methods can be interchanged and combined to some degree. So I don't see a true competition between approaches focused on learning (roughly, Deep Learning research) and focused on theories (roughly, GOFAI research). Both are must-haves and complementary for most efficient machines and for most complex tasks (such as artificial scientists). Learning to do anything from scratch in neural network (for example, inventing mathematics as human civilization did, driven by physical world phenomena) is the most powerful solution. But it could turn out to be way harder to invent (although solution itself may be simple) than to train neural network to effectively use already existing tools (probabilistic logic as knowledge representation and GOFAI methods for inference with it as curriculum to learn from)."
thisisthewayitwillbe,3ct6so,starspawn0,2,Sat Jul 11 07:29:17 2015 UTC,"I think distancing AI2 from Deep Learning research by Oren Etzioni would be detrimental to their progress.  I consider Pedro Domingos, Andrew McCallum and William Cohen to be the leaders in NLU based on probabilistic knowledge bases. And Microsoft Research Cambridge is indeed currently doing a lot of work in that area lead by Chris Bishop."
thisisthewayitwillbe,3ct6so,starspawn0,2,Sat Jul 11 07:39:57 2015 UTC,"Chris Manning is at least on the board of advisers to AI2, last I checked.  I'm not sure in what capacity; I'm guessing that he meets with Etzioni and other people there on occasion.    AI2 has a few people (a postdoc or two, I think) working on deep learning, but I think mostly for computer vision.  I would have to look again at their staff.  I seem to recall Etzioni mentioning it once in a talk."
thisisthewayitwillbe,3cosbf,starspawn0,2,Thu Jul 9 15:51:44 2015 UTC,Here's another article on this by the New York Times  http://www.nytimes.com/2015/07/14/science/scientists-demonstrate-animal-mind-melds.html?_r=2
thisisthewayitwillbe,3cosbf,Yuli-Ban,1 point,Thu Jul 9 20:51:06 2015 UTC,"Cool science. I wonder if brain-computer interfaces, brain-to-brain interfaces, etc. won't advance more quickly than AGI? I mean, everyone who knows anything about AI research will tell you that human-level AGI is, at best, decades away. But here we at least have early proof-of-concepts of these ""brainets"". It will be interesting to see how the next 50 years plays out."
thisisthewayitwillbe,3cn2x6,starspawn0,-5,Thu Jul 9 04:39:47 2015 UTC,"Interesting. But I thought that Moore's Law stopped about a decade ago? Or at least performance stopped improving at the same rate in about 2005 or so? It's cool that they made this chip, but by how much does it make up for lost ground?"
thisisthewayitwillbe,3cn2x6,MissKaioshin,5,Thu Jul 9 06:13:49 2015 UTC,"Don't be thick. Performance gains have slowed down, frequency is not improving, heat dissipation is a problem, the cost of transistors is not going down at the same rate.  But Moore's Law--which has squat to do with any of those--is still alive."
thisisthewayitwillbe,3cn2x6,johnnd,-3,Thu Jul 9 10:38:00 2015 UTC,Economic scaling of Moore has ended at 28 nm http://electroiq.com/blog/2014/03/moores-law-has-stopped-at-28nm/  Physical limits are not far behind.
thisisthewayitwillbe,3clgbh,wisintel,1 point,Wed Jul 8 20:47:36 2015 UTC,Encouraging to see performance continue scaling with size.
thisisthewayitwillbe,3clhru,starspawn0,2,Wed Jul 8 20:57:37 2015 UTC,"Suppose you want to test a SAT-solver algorithm.  What you might do is feed it a randomly-generated k-CNF, and look to see how long it takes to find a satisfying assignment.  The trouble is that if the number of clauses in the k-CNF formula is beyond a certain threshold, with very high probability there will be no solution to find!    So what you instead want to do, in the case where the number of clauses is large, is artificially construct a set of formulas that you know in advance have a solution.  One way to do this is to start with a choice for the variables, and then generate a formula for which it is a satisfying assignment  -- and then look to see whether your algorithm will find that solution (or any solution!).  This is the idea behind ""planting"":  you ""plant"" the solution, and then see if the algorithm can find it.  The paper is about determining which distributions over formulas with a planting are ""hard"" for a very wide class of algorithms to solve -- basically, ""statistical algorithms""."
thisisthewayitwillbe,3chgs1,starspawn0,5,Tue Jul 7 23:03:48 2015 UTC,"I don't agree with this. I did comment a few days ago that I felt the field was moving slower. But I think that is more because I've gotten familiar with everything and so things seem less novel, not because innovation is drying up.  Just decreasing performance on benchmarks doesn't mean anything. This happens on all benchmarks. It means you are approaching the limits of the dataset. Where each additional percentage point is exponentially harder than the last. At some point it's impossible to do any better. But that doesn't mean the methods themselves aren't improving. Perhaps improving much more than tiny differences on benchmarks would make it seem like.  I expect there to be more massive breakthroughs in the future. Like figuring out how to learn very structured or very sparse neural networks Figuring out how to get memory and fast weights on RNNs. Using less hardware and getting much faster/larger models to work on FPGAs.Getting parallelism right. The problem seems to be bandwidth between GPUs. Some recent research showed that gradients can be reduced to a single bit and it still works fine. Harnessing unsupervised learning right. And we are just starting to revisit recurrent vision models with attention.  I think there is a ton of innovation to be had in vision - we've basically got the dumbest thing that can possibly work, just feeding pixels into big NNs. Same with audio and text and other domains.  We have barely even touched bayesian learning. I bet there are significant gains to be made there, wherever overfitting is a problem."
thisisthewayitwillbe,3chgs1,Noncomment,2,Wed Jul 8 01:09:50 2015 UTC,"I think there is a ton of innovation to be had in vision - we've basically got the dumbest thing that can possibly work, just feeding pixels into big NNs. Same with audio and text and other domains.   Image question-answering is one application that seems to be showing amazing progress, for example.  Also graphics generation like Facebook's Project EyeScream; Google's recent work on extrapolating new views based on a small sample of images; and also model compression and more efficient models:  https://www.re-work.co/blog/deep-learning-christian-szegedy-google   Current deep learning algorithms and neural networks are far from their theoretically possible performance. Today, we can design vision networks that are 5-10 times cheaper and use 15 times less parameters while outperforming their much more expensive counterparts from one year ago, solely by the virtue of improved network architectures and better training methodologies. I am convinced that this is just the start: deep learning algorithms will become so efficient that they will be able to run on cheap mobile devices, even without extra hardware support or prohibitive memory overhead."
thisisthewayitwillbe,3chgs1,RushAndAPush,3,Wed Jul 8 01:37:22 2015 UTC,There is still low-hanging fruit in NLU using massive datasets and multi-modal data; commonsense reasoning; machine reading; dialog systems; machine translation; robotics; image and video generation; image and video question-answering.  The article is misleading.
thisisthewayitwillbe,3cgk96,starspawn0,3,Tue Jul 7 19:11:46 2015 UTC,I wrote something on the theoretical justification for deep learning: http://houshalter.tumblr.com/post/120134087595/approximating-solomonoff-induction
thisisthewayitwillbe,3cgk96,Noncomment,3,Wed Jul 8 07:14:36 2015 UTC,"Hi starspawn0, I am the article's author. I could retort with the lament [It's amazing how poor reading and writing skills are among redditors]. I do not speak on behalf of any community as a whole. This very notion of speaking on behalf of a community is strangely cult-like and non-academic, and not something I'd imagine Geoff Hinton, Yann Lecun, Yoshua Bengio, or Jurgen Schmidhuber would favor. The piece is a conversation-starter that, if anything, is very supportive of deep learning research. I'd recommend that you read more than the title before firing a misinformed response."
thisisthewayitwillbe,3cgk96,zackchase,3,Wed Jul 8 17:32:41 2015 UTC,"Ok, I read it again. Yes, my comment seems wrong and harsh.  I was probably in a bad mood or something."
thisisthewayitwillbe,3cbrjn,starspawn0,5,Mon Jul 6 18:00:59 2015 UTC,"I personally have a pretty good idea of where the field of AI is at present, as well as what it could become in a few years.  And I don't need Horvitz or Hassabis to tell me.    I am impressed with progress in some areas (like success at speech and image recognition)... and less impressed with others (like ""deep reasoning"").    I think Deep Learning (and maybe Reinforcement Learning) will eventually get us the intelligent machines of sci-fi.  But don't expect that anytime soon.  It will take at least a decade -- if not two -- to become really impressive to us now (we might be less impressed with it, then, when it arrives).    I'm also hopeful that other areas of AI will develop.  I have a soft-spot for techniques like SAT-solvers, ILP, submodularity, probabilistic logic, etc."
thisisthewayitwillbe,3cbrjn,MissKaioshin,-1,Tue Jul 7 01:45:21 2015 UTC,"The problem is hype. Futurists and the media are hyping the heck out of A.I. right now. People seriously think Skynet or Ultron are right around the corner. When it becomes clear that we're still many decades away, if not a century, from general human-like AI, there's going to be a backlash. Star0, i know you've said that we'll never again have another AI winter, but I'm not so sure."
thisisthewayitwillbe,3cbrjn,mirror_truth,3,Mon Jul 6 22:07:03 2015 UTC,"The question is how to separate hype from the more realistic and grounded predictions. The best way to do that is to simply focus on the nearer term predictions, rather than speculating endlessly about long term outcomes."
thisisthewayitwillbe,3cbrjn,RushAndAPush,1 point,Mon Jul 6 22:49:19 2015 UTC,Demis Hassabis would disagree. In private company he says that Deepmind will solve everything in regards to intelligence in 10 years.   Here's the link for proof: https://www.youtube.com/watch?v=Z1ytzW3Icig#t=36m0s
thisisthewayitwillbe,3cbs8q,Yuli-Ban,3,Mon Jul 6 18:05:51 2015 UTC,Depends on whose estimate you use.
thisisthewayitwillbe,3cbs8q,starspawn0,2,Mon Jul 6 18:09:19 2015 UTC,low hanging fruit  desktops can correlate more data than all humans combined  solving the secret of deductive reasoning will bring a hard start  i try to tell kursweil to bring me on as a consultant for a few weeks but he hasnt called
thisisthewayitwillbe,3cb1a6,starspawn0,2,Mon Jul 6 14:42:40 2015 UTC,"Some comments:   It only did marginally better than Glove and word2vec on the ""semantic"" portion of the test.   And given that the model uses morphology, of course it will do better on the syntactic portion; but this part of the test is less interesting, in my opinion.  In fact, the whole point of including those syntactic questions in the original dataset was surely to see whether one could predict syntactic relations just using contextual information alone (and not morphology). It's unfair to compare the number of parameters in this ""neural network"" to the 10 billion or so that Google used in image recognition.  They are different networks and different tasks! -- it's comparing apples to oranges."
thisisthewayitwillbe,3c7fh4,starspawn0,4,Sun Jul 5 16:20:22 2015 UTC,The comments on that article demonstrate why Britain will continue to have a lack of Billion dollar technology companies.
thisisthewayitwillbe,3c5poc,RushAndAPush,3,Sun Jul 5 01:40:58 2015 UTC,"Researchers in various fields are becoming more angry with Google and other companies who give the impression of ""openness""; but then write papers that don't give important details, and patent algorithms and other research ideas that were assumed to be public domain.    For example, I was just reading the other day Google's ""chatbot"" paper again, trying to find out what kind of representation they used for tokens, but could not find it anywhere.  It seems, based on my reading, that they didn't use 1-hot representations; so it must be a word embedding representation (they mention word vectors in passing in the article, without outright saying that that's what they used).  The reason I wanted to know the specifics is I wanted to see whether they used multi-modal embeddings like Baroni describes here:  http://arxiv.org/abs/1501.02598  My thought was that if they didn't use such embeddings, then that could be another way to improve their model (adding some symbol-grounding to the mix).    But there's no clue in the paper as to what they used... unless I overlooked it!"
thisisthewayitwillbe,3c5poc,starspawn0,2,Sun Jul 5 03:54:46 2015 UTC,I noticed the same thing.
thisisthewayitwillbe,3c5poc,010011000111,1 point,Sun Jul 5 15:02:26 2015 UTC,"I guess a problem with using word-vectors is that the model could treat them as just another representation of 1-of-k vectors, by learning the inverse-matrix associated to the word vectors (that would be true in the case where vocabulary size = embedding dimension, which is not the case; but it could learn some kind of pseudoinverse of something).  In other words, there would have to be some mechanism for the model to make use of the information contained in them, and not treat them as random vectors.      One possible mechanism is that if the encoding is actual word vectors (and not 1-of-k representations), there will be simple linear relations or near-relations between them; and the gradient descent optimization might try to make use of those in predicting future responses.  The gradient descent should be biased towards learning simpler processes to predict future responses.  ....  The best way to ensure that the model makes use of the extra information in the vectors might be to somehow use them to expand the size of the training dataset (augmentation).  I'm not sure how to do that exactly.  Maybe some synthetic conversations could be created that capture a lot of the information in the vectors."
thisisthewayitwillbe,3c5poc,starspawn0,3,Sun Jul 5 15:24:46 2015 UTC,"""Further applications of the techniques we describe, which are merely given by way of example, include: robot control (such as bipedal or quadrupedal walking or running, navigation, grasping, and other control skills); vehicle control (autonomous vehicle control, steering control, airborne vehicle control such as helicopter or plane control, autonomous mobile robot control); machine control; control of wired or wireless communication systems; control of laboratory or industrial equipment; control or real or virtual resources (such as memory management, inventory management and the like); drug discovery (where the controlled action is, say, the definition or DNA sequence of a drug and the states are defined by states of a living entity to which the drug is applied); application to a system in which the state of or output from the system is defined by words (text and/or audio and/or image), such as a system employing natural language; application to a trading system such as a stock market (although the actions taken may have little effect on such a system, very small effects can be sufficient to achieve useful overall rewards); and others.  No doubt many other effective alternatives will occur to the skilled person. It will be understood that the invention is not limited to the described embodiments and encompasses modifications apparent to those skilled in the art lying within the spirit and scope of the claims appended hereto."""
thisisthewayitwillbe,3c25zk,starspawn0,3,Sat Jul 4 01:48:57 2015 UTC,"SIGGRAPH is without a doubt one of the coolest conferences ever. It's like the Great expo in 1850, except every year!"
thisisthewayitwillbe,3c25zk,apmechev,2,Sat Jul 4 06:58:52 2015 UTC,You are the coolest redditor ever!
thisisthewayitwillbe,3c25zk,acmsiggraph,1 point,Mon Jul 6 13:23:14 2015 UTC,<3
thisisthewayitwillbe,3c11nm,HD125823,3,Fri Jul 3 19:59:43 2015 UTC,"Looks like a deepmind  clone except that the company isn't owned by one of the major players and is still searching for investors. I'm curious if he and his team will make some achievements in the near future cause the best experts are now with deepmind(some of them actually are old students from schmidhuber, like alex graves, tom schaul, daan wierstra and also cofounder of deepmind shane legg)"
thisisthewayitwillbe,3c11nm,corebiz,2,Fri Jul 3 20:05:56 2015 UTC,"i think you got your causality wrong, if anything, deepmind is a clone of schmidhuber's swiss ai lab, not the other way round, deepmind even cloned methods by other members of their former team in switzerland, methods that reinforcement-learn directly from large scale vision input http://people.idsia.ch/~juergen/naturedeepmind.html and apparently the best guys (except for graves) are still in switzerland"
thisisthewayitwillbe,3c11nm,brylevkirill,1 point,Sun Jul 5 19:26:39 2015 UTC,"What I meant by clone, is that the company ""nnaisense"" seems to have the exact same goal as deepmind (building general purpose learning algorithms) and because the company was founded in late 2014 (at least to this source: http://www.moneyhouse.ch/en/u/v/nnaisense_sa_CH-501.3.019.165-6.htm) 3 years after deepmind.  What I didn't want to say is that his team is bad or something like that, the opposite is true. So, I guess I should have posted this right away in my original message to avoid confusion.  Out of interest, could you tell us something about the rest of his team? You seem to know something about them?"
thisisthewayitwillbe,3c11nm,starspawn0,3,Sun Jul 5 21:27:51 2015 UTC,It seems to me that schmidhuber is desperately trying to stay on top of the field with this start up. The deep learning field has changed so much during the last 3 years that it isn't enough anymore to work in this field at a university lab because so many people are using deep learning for some really lucrative business opportunities. If he doesn't want to be vanished he has to do something against deepmind & co. I also don't believe that it will be enough especially because the start up scene in switzerland ca't be seriously compared to what's going on in london or silicon valley....it's kind of sad how fast things change. A few years ago he was on top...but now things are the other way around and he can't compete with the likes of google and facebook
thisisthewayitwillbe,3c11nm,Noncomment,2,Sat Jul 4 01:32:10 2015 UTC,"Nando de Freitas said that a right cognitive architecture (how to put components to work together) is a key challenge for DeepMind, so as for NNAISENSE probably."
thisisthewayitwillbe,3c11nm,starspawn0,2,Fri Jul 3 20:28:08 2015 UTC,"His team is too small.  What he's doing won't be a match for Google, Facebook and Microsoft and their army of Stanford-Berkeley-NYU-Toronto-Montreal ph.d.'s.  Mat Kelcey once wrote something that I am thinking more and more is the truth:  https://twitter.com/mat_kelcey/status/599682764254883840   i think the success of deep learning is less about neural networks specifically & more about gradient descent methods with a lot of data   So long as you can produce a model that isn't guaranteed to fail because of some issue like vanishing or exploding gradients, and that doesn't have any other obvious deficiencies, and so long as it is possible to train it using SGD, it will be about as good as any other method out there trained with SGD (including LSTMs).   e.g. LSTM, GRU and iRNN perform about the same (or so it is claimed); some better than others for different tasks.  So, if they are going to beat everyone else, they will need to come up with some truly novel algorithm."
thisisthewayitwillbe,3c11nm,brylevkirill,3,Fri Jul 3 20:56:55 2015 UTC,SGD is a very general optimization algorithm. But the model being optimized is still incredibly important. Minor hyperparameters make a huge difference in the performance of big neural networks. Stuff like dropout vastly helps fight against overfitting. Or the recent methods that try to approximate bayesian methods.  There is a huge amount of flexibility that you can do with just making models that will then be fit with SGD.
thisisthewayitwillbe,3c24xt,Yuli-Ban,2,Sat Jul 4 01:38:30 2015 UTC,"Nope.  It looks bogus -- so bogus that I went to see whether somebody like Scott Aaronson had already refuted it.  Sure enough, he had:  http://www.scottaaronson.com/blog/?p=2212  Quantum computers, graphene computers, memcomputers, photonic computers, nuclear spin blah blah blah computers -- they're all decades away from being practical."
thisisthewayitwillbe,3c24xt,starspawn0,1 point,Sat Jul 4 02:02:17 2015 UTC,"Apparently, there's a reason this is so funky sounding‚Äî the guy who wrote the article doesn't know a thing about memristors. Or basic English.  https://www.reddit.com/r/science/comments/3c0qbz/scientists_create_the_first_functional/?limit=500  At least I now know where to buy some memristors  http://www.bioinspired.net/products-1.html"
thisisthewayitwillbe,3c24xt,starspawn0,1 point,Sat Jul 4 03:11:35 2015 UTC,"You want to know what is the near-future?  Better GPUs and the chips like Nervana is developing.  They will enable better versions of applications like this:  http://www.futuretimeline.net/forum/topic/13011-neural-net-based-conversational-computers/  (Note that the discussion was about Google's ""chatbot"", but I wrote about it several months before it ever appeared all over the web.)"
thisisthewayitwillbe,3c24xt,MissKaioshin,-2,Sat Jul 4 04:16:00 2015 UTC,"Why did you bother posting this, then?"
thisisthewayitwillbe,3c24xt,MissKaioshin,-5,Sat Jul 4 03:28:37 2015 UTC,"Wow starspawn0, you sound like a TROLL. How dare you doubt the Law of Accelerating Returns? Don't you know that technology is accelerating and graphene will make everything magical? Memristors will usher in the Singularity by 2020. Please don't contradict this, i have a lot invested emotionally in this being true. Troll!"
thisisthewayitwillbe,3c1p1w,starspawn0,1 point,Fri Jul 3 23:09:50 2015 UTC,"The video is packed with some info (though, I think a few of his facts are off -- like about the number of bank tellers is off).  But here's one little nugget:  around 55:15 or so into the video, Markoff talks about how when he heard that Magic Leap has plans to destroy the Asian screen manufacturing industry, he was skeptical.  They had said that if you wanted to use a screen, you just do a gesture in the air, and a screen will pop up in your field of view to use -- all from an AR system the size of a pair of glasses.  But then he got the chance to actually try it out, and he now thinks that this is very possible."
thisisthewayitwillbe,3by2qv,starspawn0,2,Fri Jul 3 02:44:26 2015 UTC,I'm surprised that he's not sold on the idea that intelligence is solvable.
thisisthewayitwillbe,3by2qv,RushAndAPush,2,Fri Jul 3 04:09:41 2015 UTC,"I don't know... I found his take pretty positive.  He basically says that:   Solving intelligence is a moving target, since as soon as you solve one part of it, people say, ""Well, that's not intelligence (anymore)!""; Every time they work on a problem related to intelligence and make progress, they find that the problem was much simpler than they thought; He has come to realize that the problem of solving intelligence is more about demystifying it -- each little advance demystifies it more and more, until they can see it all.   I get the picture of people walking through a dark mansion with candles, illuminating room by room, until there are no more dark rooms left."
thisisthewayitwillbe,3by2qv,Noncomment,2,Fri Jul 3 04:19:42 2015 UTC,The download link is missing. The url of the mp3 file is here.
thisisthewayitwillbe,3bvtd9,starspawn0,2,Thu Jul 2 15:46:13 2015 UTC,A better analogy would be nuclear weapons before they were invented. They were entirely theoretical. Few would have believed a single bomb could destroy an entire city. Only a few scientists thought they were possible.
thisisthewayitwillbe,3bu1k3,starspawn0,5,Thu Jul 2 03:46:04 2015 UTC,"Eventually all such questions end up devolving into semantics. After all, who's to say that human sentience isn't a biological ""trick""? Is a reduction in serotonin in the brain all that different from ai.mood.happiness-=1?  Personally, I believe that if an AI can converse in an indistinguishable way from a human, then for all intents and purposes it has at least equal to human-level sentience in that task."
thisisthewayitwillbe,3bu1k3,BerickCook,3,Thu Jul 2 04:43:48 2015 UTC,"And I do think that the human brain is much more of a ""trick"" than many probably want to believe.  However, it's probably not too simple a trick -- e.g. it can't just be a giant lookup table, or simple Markov Model (it might be equivalent to a Hierarchical Hidden Markov Model, however)."
thisisthewayitwillbe,3bu1k3,Noncomment,1 point,Thu Jul 2 05:00:49 2015 UTC,Well people say that seretonin causes the experience of happiness. Not that it is happiness. Somehow I feel that this is an important distinction.
thisisthewayitwillbe,3bu1k3,Noncomment,3,Thu Jul 2 16:23:48 2015 UTC,"As a practical matter, who cares? If somehow you had a GLUT that could do anything a human could do, then you could just use that as an AI. Its internal properties make no difference in practical applications."
thisisthewayitwillbe,3bu1k3,Noncomment,2,Thu Jul 2 04:30:06 2015 UTC,"Right, as a practical matter it's not important (if problems occur so rarely as to be safely ignored).  But if the programming really were a trick, and if you knew the trick, you could come up with queries nobody else would think of that would prove that the machine is not sentient after all (similar to the ""adversarial examples"" in computer vision); that might bother people who thought a machine was their ""friend""."
thisisthewayitwillbe,3bu1k3,Noncomment,2,Thu Jul 2 04:57:18 2015 UTC,Well that's probably true of humans too. There are tons of biases humans have that can be exploited.
thisisthewayitwillbe,3bucvk,starspawn0,1 point,Thu Jul 2 05:44:55 2015 UTC,"This video punches holes in the idea that we have free will.  The people discussed in the video are perfectly rational, except for the fact that they make up elaborate confabulations to disguise their paralysis.  They simply can't not do it; because that is what their unconscious mind is making them think and say."
thisisthewayitwillbe,3btgmo,starspawn0,3,Thu Jul 2 00:32:34 2015 UTC,"Amazing. Even more so in an ipython notebook.   I will be testing this, and playing with it."
thisisthewayitwillbe,3bsl31,starspawn0,1 point,Wed Jul 1 20:25:36 2015 UTC,"I would go with a purely machine learning-based approach, and then try to address any problems that spring up as they arise.  As a first step, one could start by deploying the AI in fairly low-risk situations, and then gradually increase the number of contexts to include those with a higher level of risk, if nothing goes wrong.  For example, robots that remind people to take pills might start by doing so for drugs that won't seriously negatively impact people if they aren't taken on time; and then the range of drugs could be expanded to more serious ones, after the robot is shown to be safe."
thisisthewayitwillbe,3br6ul,starspawn0,3,Wed Jul 1 14:20:04 2015 UTC,"You shouldn't come away with the impression that Yoav Goldberg is part of the ""old guard""; he adapts quickly to new methods from what I've seen.  He's just someone who wants to be sure about what the new methods can and cannot do.  Other critics of machine learning applied to NLP seem to be motivated by ideology, however."
thisisthewayitwillbe,3bow9a,starspawn0,3,Tue Jun 30 23:17:17 2015 UTC,But the rush to the front ranks of science may come at a price: Some experts worry that medical researchers in China are stepping over ethical boundaries long accepted in the West.   Thank goodness!
thisisthewayitwillbe,3bow9a,Fab527,3,Wed Jul 1 12:42:28 2015 UTC,"Chinese scientists are generally poorly paid, he said, but may receive a bonus of up to $32,000 per article from the state for publishing in international scientific journals, providing financial incentives for pushing the boundaries.   Holy shit!"
thisisthewayitwillbe,3bow9a,untouchedURL,1 point,Tue Jun 30 23:59:06 2015 UTC,Here is a non-mobile link: http://nytimes.com/2015/06/30/science/a-scientific-ethical-divide-between-china-and-west.html  Sourcecode | Feedback?
thisisthewayitwillbe,3bnhik,brylevkirill,3,Tue Jun 30 17:09:47 2015 UTC,"Well, I was just waiting for something like this.  He has given hints right along about that seems to suggest he feels deeply, deeply wronged;  but he has never said it outright in public before.  Now that he has, you can be sure that there will be a little war in machine learning circles!  Hinton-Bengio-Lecun have held off doing the kind of ""historical perspective"" work (i.e. making sure everyone knows who invented what) that Schmidhuber has done.  I think after this you might see them give their version of ""history""."
thisisthewayitwillbe,3bnhik,starspawn0,2,Tue Jun 30 17:13:31 2015 UTC,"Why is ""history"" in quotes?  This is actually the kind of work that historians do."
thisisthewayitwillbe,3bnhik,radarsat1,2,Wed Jul 1 01:38:51 2015 UTC,"Because the facts are not all there is to it.  There is also the interpretation of the facts.  Schmidhuber may have the facts right; but his interpretation may be different from Lecun-Bengio-Hinton.  When two countries go to war, there is often one version of history as told by one country and another history as told by the other.  Both can tell exactly the same facts; but interpret them differently.  ""A History"" versus ""The History""."
thisisthewayitwillbe,3bnhik,starspawn0,2,Wed Jul 1 01:53:52 2015 UTC,"Ah ok, that's what I meant.  ""History"" as a study, not as a single individual story.  Of course, perhaps historians will argue about it for decades :)  That's I guess why Schmidhuber felt obliged to tell his side of the story."
thisisthewayitwillbe,3bnhik,radarsat1,2,Wed Jul 1 04:07:14 2015 UTC,"Yann LeCun's answer: (https://plus.google.com/100849856540000067209/posts/9BDtGwCDL7D)  ""I'm not going to go through your points one by one, which would be pointless, but I'll make a few general remarks.  Yes lots and lots of people have used chain rule before [Rumelhart et al. 1986], lots of people figured you could multiply Jacobians in reverse order in a multi-step function (perhaps even going back to Gauss, Leibniz, Newton, and Lagrange). But did they all ""invent backprop?"" No! They did not realize how this could be used for machine learning and they sure didn't implement it and made it work for that. Many people were looking for a multi-layer learning algorithm in the 60s (and a few in the 70s). If the backprop idea had been so obvious, many people would have found it and demonstrated it. The point is that no one managed to demonstrate a working instantiation until the mid 80s. Perhaps this idea is obvious in hindsight, but that's a feature of many good ideas: they are obvious in hindsight. The same is true for ConvNets. It's a pretty obvious idea in hindsight.  Yes, a few people actually figured out early on that you could use chain rule for training a machine (including Rumelhart by the way. It took him and Geoff Hinton several years to get it to work). Some people had the intuition that you could use backward signals to train a multi-stage system (e.g. system theorist A.M. Andrews in the early 70s). But did they reduce it to practice and did they manage to make it work? No. that didn't really happen until the mid-1980s.  I actually was the one who originally made the connection between backprop and the adjoint method used in control theory (the Bryson, Kelly, Dreyfus methods). In my 1987 PhD thesis, I have a derivation of backprop using a Lagrangian formulation which was inspired by control theory methods. I eventually wrote a couple papers about this connection [""a theoretical framework for backpropagation"" 1988].  How should we attribute credit? Let's use an analogy, lots of people tried to build airplanes and helicopters in the late 19th century and early 20th century.  Many people had the right sort of ideas. An airplane even took on its own power in 1890 (Cl√©ment Ader's Eole), quite a few airplanes flew more or less well before 1903. But the Wright Brothers get most of the credit because they were the first ones to build a fully controllable airplane.  Same for the helicopter. Lots of people tried to build helicopters in the early 20th century, and several took off. But the idea didn't become practical until Sikorski's refinement of the cyclic control and tail rotor in the late 30s and early 40s. Who should get credit? Leonardo da Vinci?  Also, ConvNets were indeed ""largely forsaken by the mainstream computer-vision and machine-learning communities until the ImageNet competition in 2012."" I'm rather well positioned to speak about this, having had countless ConvNet papers rejected, published and ignored, and occasionally paid attention to, for over 15 years before your group even started playing with the idea. We did have quite a few successful vision applications of ConvNets before 2012, but they really did not gather much interest from the vision and learning communities, and certainly didn't trigger the revolution that came after the 2012 ImageNet results.  Krizhevski, Sutskever and Hinton get a lot of credit for their work, and it's well deserved. They used many of my ideas (and added a few), but you don't see me complain about it. That's how science and technology make progress.Ôªø"""
thisisthewayitwillbe,3bnhik,HD125823,1 point,Thu Jul 2 21:30:09 2015 UTC,That's about what I expected he would write.
thisisthewayitwillbe,3bnhik,starspawn0,2,Thu Jul 2 21:48:17 2015 UTC,"Here is what Soumith Chintala had to say on Schmidhuber's Google+ page:   The article looks great, lots of references. Thanks.  ""The system was twice better than humans, and three times better than the nearest non-human competitor (co-authored by LeCun of LBH).""  I dont think you did yourself any favors there by quoting the 98.98% vs 98.97% (a difference of 1 sample) in a hyped up fashion.  Reference:  http://benchmark.ini.rub.de/?section=gtsrb&subsection=results&subsubsection=ijcnn  I also took part in the competition.  Edit: I see the phase 2, or ""final"" results here:  http://benchmark.ini.rub.de/index.php?section=gtsrb&subsection=results&subsubsection=official  99.46% vs 98.84% vs 98.31%. Claiming a twice better or thrice better is kinda still not great, because the last 1 percentage is essentially a numbers game and making nice ensembles of models. Practically it is hard to see your claims of twice better than humans.  Also, as part of being part of the ConvNet bandwagon pre-2012, I disagree with your point 6. ConvNets were shunned by the mainstream computer vision community. Period. You showcase a couple of results here and there of winning competitions, but neither were the most significant computer challenges beaten using ConvNets, nor were any of the pioneer CV research labs using CovnNets, even after these results were showcased. It was very hard to get CV papers using CovnNets in the top-tier CVPR or ICCV. It was a running joke among researchers even.Ôªø"
thisisthewayitwillbe,3bnhik,starspawn0,2,Wed Jul 1 03:39:12 2015 UTC,Here's the link to the Google+ post.
thisisthewayitwillbe,3bnhik,chetannaik,2,Wed Jul 1 05:01:02 2015 UTC,I just think that if a lot more researchers paid attention to Schmidhuber's prior work on AI and read through all his papers there would most certainly be more progress in the field. His ideas such as Learning to Learn (one of numerous unexplored yet) can potentially lead to new incredible breakthroughs like it was with LSTM.
thisisthewayitwillbe,3bomxr,starspawn0,2,Tue Jun 30 22:05:07 2015 UTC,"I think this question gets at the reason AI and machine learning-based systems are anthropomorphized:   When asked by IEEE Spectrum‚Äôs Lee Gomes ‚Äúif you were a reporter covering a Deep Learning announcement, and had just eight words to describe it, which is usually all a newspaper reporter might get, what would you say?‚Äù    The problem is that we don't have words in common language to describe something that behaves in thought-like ways, but isn't actually thinking.  It's simply easier to use words like ""understand"" and ""think"".  Lecun's answer doesn't help the average reader, for example, since they might not grasp what he means by ""represent"".    Also, while neural nets currently don't think in the full human sense of the term, if one believes they will be capable of doing so one day, then maybe they are thinking a little bit already (as Hinton seems to be suggesting) -- maybe there is a spectrum of ""thinking"" and ""understanding"".  On the other extreme from people like Hinton, who think machines are already thinking a little bit, are the angry skeptics.  They pounce on any mention of the word ""understand"", and express incredulity at the ""stupidity"" of journalists for using such language.  Some of these probably are just afraid of being dehumanized, as I wrote before (or, they have philosophical objections to various computational theories of mind).  And some probably are sincere, and really do think the journalists are stupid hype-monger."
thisisthewayitwillbe,3bmiii,starspawn0,2,Tue Jun 30 12:34:05 2015 UTC,"Once they come to realize that the ""beef"" is in the data, then the criticism will shift to, ""But that's that not science!  That's not understanding!"""
thisisthewayitwillbe,3bmiii,zombiefood1,1 point,Tue Jun 30 12:55:11 2015 UTC,thanks for that.  in both cases it seems logical to me that they will both reach a tipping point where they can digest all information that arrives and make a determination what conclusions can be drawn and which problems have insufficient data (anywhere on earth) to make a conclusion and will call for measurements
thisisthewayitwillbe,3bn7yp,zombiefood1,2,Tue Jun 30 16:03:07 2015 UTC,"The title makes it seem like the program is sentient.  It's not.    The article is about what I posted here a week or two ago:  https://www.reddit.com/r/thisisthewayitwillbe/comments/3anpmt/a_neural_conversational_model_whoomp_there_it_is/  I also posted the original Google Research Blog posting and Wired's take on the work.  ....  I am convinced, however, that with the right training algorithm and the right training data it might be possible to use machine learning to build a system that convinces people it is sentient a large percent of the time.  I don't think there's any limit to what you can do with machine learning!"
thisisthewayitwillbe,3bn7yp,starspawn0,2,Tue Jun 30 16:10:58 2015 UTC,my first question is how fast can that happen
thisisthewayitwillbe,3bn7yp,starspawn0,2,Tue Jun 30 18:10:05 2015 UTC,"Pretty fast, if you had the data.  Within a year, maybe?  The trick is how to do it with much less data, or even low-quality varied types of data (boosting... multimodal data... etc.)."
thisisthewayitwillbe,3bmeiw,starspawn0,2,Tue Jun 30 11:49:20 2015 UTC,"Funny:   Can you share some of the lessons of pain and failure from your early career?   My basic theory in my twenties is that IQ was fungible. I would hire a great physicists, biologists, someone who was smart, and I would assign them some task, and they would figure out how to do because they have a high IQ.  I basically thought that I should never ask somebody to work for somebody who is not smarter than them. We‚Äôll just have this IQ hierarchy. Well that didn‚Äôt work for very long. By age 25, I realised IQ comes in different forms. These guys who understand sales and management, that seems to come negatively correlated with IQ. That was befuddling to me.   Negatively correlated?"
thisisthewayitwillbe,3bmeiw,johnnd,4,Tue Jun 30 11:54:23 2015 UTC,He's saying that MBAs are dumb.
thisisthewayitwillbe,3bkx3l,starspawn0,2,Tue Jun 30 01:41:16 2015 UTC,That disclaimer is horrible...
thisisthewayitwillbe,3bkutw,starspawn0,2,Tue Jun 30 01:22:15 2015 UTC,"That's good, then the hotel prices should go down, if I ever come to Paris again :-)"
thisisthewayitwillbe,3bkutw,andmar74,1 point,Tue Jun 30 11:20:07 2015 UTC,Here is a non-mobile link: https://twitter.com/amcafee/status/615189481520500736  Sourcecode | Feedback?
thisisthewayitwillbe,3bkutw,untouchedURL,1 point,Tue Jun 30 01:22:51 2015 UTC,"@amcafee:   2015-06-28 16:06:16 UTC  WOW. Last summer, over 10% of Paris's visitors used Airbnb wsj.com #2MA pic.twitter.com [Imgur]     [Mistake?] [Suggestion] [FAQ] [Code] [Issues]"
thisisthewayitwillbe,3bis38,starspawn0,2,Mon Jun 29 15:54:05 2015 UTC,YES!!
thisisthewayitwillbe,3bik18,starspawn0,2,Mon Jun 29 14:50:29 2015 UTC,"I think the IoT will profoundly change daily life, and I've written before about some of the ways:  https://www.reddit.com/r/thisisthewayitwillbe/comments/2n6a6p/some_thoughts_on_what_the_future_of_the_internet/  A few of the things in that post now seem a little less futuristic than they did when I wrote it.  Among other things, Google Maps now lets you experience various mapped locations in VR:  http://www.theverge.com/2014/12/18/7414451/google-maps-streret-view-virtual-reality  That's not about experiencing live web-streams of a location in VR; but it's a step in that direction."
thisisthewayitwillbe,3bgzpd,starspawn0,3,Mon Jun 29 03:33:45 2015 UTC,"Actually, it ties with the humans; but I consider that a win, especially as it took a whole team of humans versus only one machine!  The nice thing about this system is that it only took a single grad student to develop it, whereas Watson took a whole team of programmers and $100 millions to develop.    Watson can do a few things that this model can't do (like answer the ""frothy pie topping"" problem that Kurzweil likes to repeat); but I wouldn't be surprised if another grad student somewhere could find a way to train a recurrent net to even handle these kinds of questions (given mountains of training data)."
thisisthewayitwillbe,3bic3l,starspawn0,0,Mon Jun 29 13:40:37 2015 UTC,@CompSciFact:   2015-06-22 15:45:53 UTC  'The hardest problem in computer science is not being an opinionated jerk about everything.' -- @ntakayama     [Mistake?] [Suggestion] [FAQ] [Code] [Issues]
thisisthewayitwillbe,3bic3l,TweetPoster,0,Mon Jun 29 13:40:52 2015 UTC,Here is a non-mobile link: https://twitter.com/CompSciFact/status/613010021887148032  Sourcecode | Feedback?
thisisthewayitwillbe,3belrc,johnnd,2,Sun Jun 28 14:43:25 2015 UTC,This is a huge setback for SpaceX. Damn.
thisisthewayitwillbe,3belrc,starspawn0,1 point,Sun Jun 28 14:44:25 2015 UTC,"Well, I personally am not that ""into"" Space X, since it will be decades before it impacts me directly -- except maybe for its delivery of internet satellites."
thisisthewayitwillbe,3belrc,Yuli-Ban,4,Sun Jun 28 15:53:22 2015 UTC,"I am ""into"" it for humanity in general and for being a blatant Elon Musk fanboy. :P"
thisisthewayitwillbe,3belrc,Chainedfei,1 point,Sun Jun 28 16:37:39 2015 UTC,"This should bring people back to Earth (no pun intended). Elon Musk is aiming for great things, but we shouldn't believe he'll win every time."
thisisthewayitwillbe,3belrc,someday69,1 point,Sun Jun 28 21:32:13 2015 UTC,"There is no gain without risk.  Failure is a lesson, not an end.  We all must fail in order to improve."
thisisthewayitwillbe,3belrc,someday69,1 point,Mon Jun 29 03:13:49 2015 UTC,Well I hope they can learn form this misfortune. It is not a given that they will find what went wrong...
thisisthewayitwillbe,3befir,starspawn0,3,Sun Jun 28 13:33:49 2015 UTC,"Much of the stuff I've read about Natural Language Generation leads me to think that it's a bunch of hacks that work remarkably well.  One of the more interesting such hacks I've read about pertrains to fiction-generation:  I would have to look it up, but I recall seeing a paper a few months back about how to generate new stories by applying anaologies to existing ones.  (For example, you could start with a story about the Vietnam War, and end up with a story about a war on an alien planet, mapping Vietnam --> Solar City Colony, Ho Chi Minh --> Evilarz, the enemy of the Federation; etc.)  Then, applying some paraphrase and expansion techniques iteratively, and techniques to change the style and color of the work, you might end up with a wholly new story that looks nothing like the original on which it was based.    This is why I think it won't be long before we have computers that can generate seemingly original works of short fiction -- and perhaps even whole novels -- even very thought-provoking ones (all the interesting ideas will be contained in the original work on which the new work is based; and this is true also of most current ""original works""; there's nothing new under the sun, as they say!)."
thisisthewayitwillbe,3bcsde,starspawn0,3,Sun Jun 28 00:12:15 2015 UTC,I've heard the story about Watson and cancer more times than I can count.  But it does look like they are continuing to make progress.  This article is fairly in-depth (as far as such articles go).
thisisthewayitwillbe,3bcsde,andmar74,2,Sun Jun 28 00:13:18 2015 UTC,"Yes there are many articles out there, but most of them are low on details. This one is as you say more comprehensive, though I still want more to know. How exactly does Watson improve treatment?  The bosses want Watson to take on other cancer forms, and they know more than we do, so that's a good sign."
thisisthewayitwillbe,3bcsde,vincentrevelations,3,Sun Jun 28 05:32:00 2015 UTC,"She regularly tells Watson about journal articles she‚Äôs read that might be helpful, by inputting a citation and highlighting key passages, and Watson helps her delve into patient records much faster than she could on her own.   I like how this sounds. So natural."
thisisthewayitwillbe,3bc239,starspawn0,2,Sat Jun 27 20:05:19 2015 UTC,"I fast-read the article. I don't get it, what is he talking about? You can access Uber from other programs than the Uber app? That's great ! ??"
thisisthewayitwillbe,3bc239,andmar74,2,Sun Jun 28 06:42:26 2015 UTC,"Yes, such as integration with Sound Hound's virtual assistant. You would be able to ask it to order you a cab to take you somewhere via somewhere else. The example in the article is order a cab to his brother's house and stop off on the way to get a bottle of wine to go with lasagna."
thisisthewayitwillbe,3bc239,NuScorpii,1 point,Sun Jun 28 10:49:52 2015 UTC,He seems to really like Uber's chutzpah.  It almost looks like @ss-kissing.
thisisthewayitwillbe,3bb843,starspawn0,3,Sat Jun 27 15:39:16 2015 UTC,"From what I've read, market crashes have a nice side-effect, as far as accelerating change is concerned:  they force businesses to use the latest technologies to cut costs.  Unfortunately, this often leads to mass unemployment, as some of these technologies are labor-substituting."
thisisthewayitwillbe,3b8kqc,starspawn0,2,Fri Jun 26 21:06:13 2015 UTC,"Wow! It looks like Google has solved almost all the problems that the cars will handle, including things like how to interpret hand signals from cops; when to pull over (when seeing a cop car); handling pedestrians, people in wheelchairs chasing ducks, and weird pedestrian behavior.  The skeptics might just be wrong.  Badly wrong!"
thisisthewayitwillbe,3b8kqc,MissKaioshin,2,Fri Jun 26 21:23:41 2015 UTC,"That was a cool talk, thanks for sharing. I remember watching a presentation of Amnon Shashua talking about computer vision and self-driving cars, and it looked like many of the traditional problems of self-driving cars were close to being solved, such as driving on a snowy road and not being able to ""see"" the lanes but still staying on the road. Can Google's car do that? Chris Urmson didn't mention how it might handle different types of weather."
thisisthewayitwillbe,3b6vek,starspawn0,3,Fri Jun 26 13:23:28 2015 UTC,"My plan to investigate this consistency problem is to train on my personal responses only, so it will have more than a flavor of my personality. :D I don't have corpus nearly as big as OpenSubtitles, so I might go from word-level to syllables or even characters. I bet it won't produce anything like the philosophical debate from paper, but it will be fun to play with it. :)"
thisisthewayitwillbe,3b5cnk,starspawn0,1 point,Fri Jun 26 02:36:07 2015 UTC,Here is a non-mobile link: https://twitter.com/karpathy/status/613657354563899392  Sourcecode | Feedback?
thisisthewayitwillbe,3b5cnk,untouchedURL,1 point,Fri Jun 26 02:36:47 2015 UTC,if true then hard start
thisisthewayitwillbe,3b5cnk,zombiefood1,1 point,Fri Jun 26 14:14:59 2015 UTC,"I have the exact opposite problem. After catching up with much of the current research and absorbing it, everything feels less novel and slower than it did before.  Now that half the year has gone by, I can't think of what to put in my AI advancements list. At least stuff that is as significant as last year's list. Which in turn included some stuff that really happened in late 2013."
thisisthewayitwillbe,3b5cnk,Noncomment,3,Fri Jun 26 03:58:36 2015 UTC,"Sutskever once said that Deep Learning is ""easy the same way that rocket science is easy"".  And he explained that there isn't much to it from the theory point-of-view; you can learn the basic math fairly quickly (it doesn't even crack undergraduate-level).  On the other hand, it's difficult from the engineering point-of-view.  Most of the innovations seem to involve tweaks to existing architectures; or, faster ways to train models; or, new ways to generate large training datasets; or, applications of existing architectures to problems in areas never attempted with these methods."
thisisthewayitwillbe,3b4mwd,starspawn0,1 point,Thu Jun 25 22:50:54 2015 UTC,business owners of big data have only one interest and it is not betterment of society
thisisthewayitwillbe,3b4rpi,starspawn0,1 point,Thu Jun 25 23:32:19 2015 UTC,Yeah its just starting to get some eyes on this.  Wait until people realize Tesla motors is doing this as you drive.
thisisthewayitwillbe,3b4j4y,starspawn0,2,Thu Jun 25 22:20:48 2015 UTC,"This fits remarkably well with Numenta's theory of ""bursting columns"" which relay more signal to higher level areas."
thisisthewayitwillbe,3b2srs,starspawn0,4,Thu Jun 25 14:40:53 2015 UTC,"This is even true of some researchers.  They don't think there is anything special about the algorithms they use -- just some function iteration is all.  Yet, they think of the human mind as different; and find it a little silly to even suggest that recurrent nets are some kind of little brain.    Geoff Hinton wants to change this view with the use of the new term ""thought vector""."
thisisthewayitwillbe,3b2srs,Noncomment,4,Thu Jun 25 14:49:12 2015 UTC,"This works both ways. Once we get AI, we may no longer see humans as intelligent agents. The mysterious properties will be gone."
thisisthewayitwillbe,3b2srs,RandomFlotsam,1 point,Fri Jun 26 03:48:50 2015 UTC,"We may have to yield citizenship to the AI's.  After all, they will probably be making better decisions."
thisisthewayitwillbe,3b2srs,andmar74,3,Fri Jun 26 15:27:10 2015 UTC,"This is similar to what Kurzweil is saying. Computer beating a grandmaster in chess must be intelligent. No not anymore, that was simple. Computer driving a car, impossible ! No, that was also easy..Computer beating humans at Jeopardy! Can't be done. The areas were humans are superior to computers will diminish until there are none left. Why would the human brain be special (out of this world?) when it has gradually evolved over millions of years on Earth?  It surely is fascinating that atoms can come together and form such a complex being as a human, but what stops us from doing the same? We have the advantage that we can think and put some intelligence behind a design, Nature is more like trial and error, although in a more random way than humans would do it."
thisisthewayitwillbe,3b3obt,starspawn0,1 point,Thu Jun 25 18:30:42 2015 UTC,Here is a non-mobile link: https://twitter.com/jasonbaldridge/status/613275204484243456  Sourcecode | Feedback?
thisisthewayitwillbe,3b3obt,untouchedURL,1 point,Thu Jun 25 18:31:10 2015 UTC,@jasonbaldridge:   2015-06-23 09:19:37 UTC  . @timberners_lee just mentioned @karpathy's post on unreasonable effectiveness of RNNs: karpathy.github.io #CannesLions     [Mistake?] [Suggestion] [FAQ] [Code] [Issues]
thisisthewayitwillbe,3awynl,starspawn0,2,Wed Jun 24 04:15:11 2015 UTC,Clearly this is a working imagination similar to the recent images from label+noise.  The computers can't imagine argument falls. This is a big one.
thisisthewayitwillbe,3avnl9,starspawn0,2,Tue Jun 23 21:35:53 2015 UTC,"And where will we be in 20 years? A scenario like that of the movie Her, in which the lead character falls in love with his computer's Siri-like operating system? ""That's science fiction,"" says Legg, 41. ""Will we get there one day? I hope so. But most AIs may not be so human-like. Language is quite a sophisticated thing. Think of it as we're trying to build insects. Years from now, you might get to a mouse. Our systems are very good at Space Invaders, they can play Breakout, but they're struggling with Pac-Man. There's a long gap from here to having a system where you can sit and debate philosophy.""¬†Hassabis agrees. ""We're decades away from anything that's nearing human-level general intelligence.   Shane Legg sounded more optimistic 3 years ago:   I‚Äôve decided to once again leave my prediction for when human level AGI will arrive unchanged. ¬†That is, I give it a log-normal distribution with a mean of 2028 and a mode of 2025, under the assumption that nothing crazy happens like a nuclear war. ¬†I‚Äôd also like to add to this prediction that I expect to see an impressive proto-AGI within the next 8 years. ¬†By this I mean a system with basic vision, basic sound processing, basic movement control, and basic language abilities, with all of these things being essentially learnt rather than preprogrammed. ¬†It will also be able to solve a range of simple problems, including novel ones.   http://www.vetta.org/2011/12/goodbye-2011-hello-2012/"
thisisthewayitwillbe,3avnl9,FractalHeretic,1 point,Tue Jun 23 22:35:37 2015 UTC,"In 20 years virtual assistants will be able to have pretty convincing conversations most of the time.  They won't be like Samantha in Her... but they will be close enough that people will enjoy their company.    If you picked a random person off the street, it might take them 30 minutes to an hour of conversation before they noticed anything unnatural.  I disagree with Hassabis's claim that it will take many decades before machines can write novels.  I think hacks and tricks (templates, script knowledge) like Narrative Science uses will get us there a lot sooner.  They'll be pretty ordinary novels, but will still be passable.  Machines that create movies?  Pretty difficult.  As a first step, try:  programs that can generate decent movie scipts; and programs that can generate the movie, given the script.  The former might be solvable with hacks; the latter might be solvable with further advances in deep learning.  10 second film clips might be possible in a few years; and if that works, then several years after that it should be possible to do much longer clips (script2movie)."
thisisthewayitwillbe,3avnl9,RushAndAPush,2,Tue Jun 23 22:45:31 2015 UTC,Why is Hinton so optimistic? He claims that within ten years we will have human-like reasoning and logic. It makes absolutely no sense to me why Deepmind is so pessimistic. With the talent they have they should be able to crack the transfer learning problem within 10-20 years and continue to improve their learning algorithm so that it become even more general.  Why did Google even buy this company?
thisisthewayitwillbe,3avnl9,FractalHeretic,3,Tue Jun 23 23:52:03 2015 UTC,I almost wonder if Deepmind is downplaying it to avoid creating a panic.
thisisthewayitwillbe,3avnl9,RushAndAPush,4,Wed Jun 24 01:19:33 2015 UTC,"""panic"" is maybe too strong a word.  I wouldn't be surprised, though, that they've had a ""message control"" talk with the people at Google public relations, where they were told to be more ""sober sounding"" when talking with the press.  This is to protect Google's image, and also to keep the voices calling for AI safety restrictions from making trouble.  (Kurzweil has remarked about how the Google PR people have told him to say that Google doen't know when driverless cars will be viable.  That's evidence that they do have people who keep the hype from getting out of control.)  Here is a piece of evidence in support of your claim:  https://www.youtube.com/watch?v=Z1ytzW3Icig#t=36m0s  It's an interview with Robin Hanson from just a few months ago (starts around 36 minutes in), where he says that Demis Hassabis basically told him that in 10 years they would have solved everything regarding AI.  Hanson could be exaggerating here.  Or, he could have been referring to a meeting from far in the past -- I don't think so, though... I think he is referring to the Puerto Rico AI summit from earlier this year (the secret summit)."
thisisthewayitwillbe,3avnl9,FractalHeretic,2,Wed Jun 24 02:29:01 2015 UTC,Wasn't there a rule at the Puerto Rico AI summit that said that nobody could attribute anything said by an individual so that people could speak freely. Robin Hanson would be violating the terms if I'm correct.
thisisthewayitwillbe,3avnl9,RushAndAPush,2,Wed Jun 24 21:56:34 2015 UTC,"He didn't say they said that at the meeting, nor did he mention Hassabis by name; Adam Ford did.  You're referring to Chatham House Rules, which are standard for many private meetings."
thisisthewayitwillbe,3avnl9,RushAndAPush,1 point,Wed Jun 24 21:59:22 2015 UTC,"That's the same conference Sam Harris was talking about recently. He said experts there were concerned about radical advances in AI coming in the next 5-10 years, as opposed to the 50-100 year timeframe you hear in public.  http://youtu.be/qwDnvA7mQMQ"
thisisthewayitwillbe,3avnl9,andmar74,1 point,Wed Jun 24 16:20:23 2015 UTC,"Commonsense reasoning is only one part of the spectrum of human intelligence.  There is also creativity, motivation, emotion, motor control (robots), vision, and so on.    I think what Hinton is talking about here is that machines will have enough cognition and reasoning ability to be able to read documents and answer questions about them at a reasonably deep level; and they will be able to generate compelling conversations.  This doesn't mean they're going to go write 1,000 page (novels) meditations on the human condition."
thisisthewayitwillbe,3askys,andmar74,4,Tue Jun 23 04:17:53 2015 UTC,"Ehh, I remain skeptical that they will produce anything with their contraptions.  Last I checked, classical algorithms running on a good desktop computer can do just as well."
thisisthewayitwillbe,3askys,starspawn0,2,Tue Jun 23 04:31:24 2015 UTC,"It will be revolutionary... maybe in 10 years.  I think the technology will have its purpose, however currently we're still stabbing in the dark. In my opinion, it's like the first computers; to use them to their full extent a whole discipline will have to be created, and that will take the most part of a decade.   Of course if D-Wave stays afloat that long."
thisisthewayitwillbe,3askys,apmechev,2,Tue Jun 23 10:00:01 2015 UTC,It would be nice to see new results with new hardware. For GPUs it is clear to see how the increase in speed has increased realism. It would be nice to see the same kind of progress in a DWave problem. Maybe I am just not following them as closely to see the effect of the increased power.
thisisthewayitwillbe,3askys,neuromorphics,3,Tue Jun 23 13:34:43 2015 UTC,"Is it faster than the universe already?  Sorry, Freudian slip. I meant to ask, does it still do absolutely nothing?"
thisisthewayitwillbe,3askys,johnnd,2,Tue Jun 23 16:20:34 2015 UTC,So they are doubling number of qubits every 25 months or so.
thisisthewayitwillbe,3askys,AsIAm,1 point,Tue Jun 23 07:01:29 2015 UTC,"They even coined the term ""Rose's Law"" for it."
thisisthewayitwillbe,3askys,Yuli-Ban,1 point,Tue Jun 23 16:00:07 2015 UTC,"Well, it's doubling every two years, not one year. But it doesn't matter since they are much more capable than von Neumann style computers."
thisisthewayitwillbe,3asp61,mirror_truth,3,Tue Jun 23 05:01:10 2015 UTC,"He mentions this study in his talk:  http://www.epi.org/publication/how-unequal-is-your-state-the-ratio-between-the-average-incomes-of-the-top-1-percent-and-the-bottom-99-percent-in-each-state/   And in 17 of these states, the top 1 percent captured 100 percent of income growth.    I'll have to remember that stat."
thisisthewayitwillbe,3asmek,mirror_truth,1 point,Tue Jun 23 04:32:31 2015 UTC,"Hmm I can't login to IBM Chef Watson site to access it, maybe it's not up yet?"
thisisthewayitwillbe,3aryvd,starspawn0,1 point,Tue Jun 23 01:01:12 2015 UTC,"The full quote:   The visualizations also reveal a new twist in an ongoing story. Previous studies have shown that discriminative networks can easily be fooled or hacked by the addition of certain structured noise in image space (Szegedy et al., 2013; Nguyen et al., 2014).  An oft-cited reason for this property is that discriminative training leads networks to ignore non-discriminative information in their input, e.g. learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs. For this reason it has been seen as a hopeless endeavor to create a generative model in which one randomly samples an x from a broad distribution on the space of all possible images and then iteratively transforms x into a recognizable image by moving it to a region that satisfies both a prior p(x) and posterior p(y|x) for some class label y. Past attempts have largely supported this view by producing unrealistic images using this method (Nguyen et al., 2014; Simonyan et al., 2013).  However, the results presented here suggest an alternate possibility: the previously used priors may simply have been too weak.   Hahaha!  (Of course, we already knew that.)"
thisisthewayitwillbe,3apnsw,Yuli-Ban,1 point,Mon Jun 22 14:12:39 2015 UTC,"10 um   So, not nanorobots. This isn't a detail, the difficults with these things come at the nanometer level, when you start dealing with individual atoms."
thisisthewayitwillbe,3aopyk,BenRayfield,2,Mon Jun 22 06:51:45 2015 UTC,Here is the online viewer if you don't want download the image : https://www.google.com/culturalinstitute/asset-viewer/the-starry-night/bgEuwDxel93-Pg?exhibitId=DAJiOwFKTTbQLQ&userGallery&hl=en  And all the gigapixel images: https://www.google.com/culturalinstitute/exhibit/art-project-gigapixels/DAJiOwFKTTbQLQ?hl=en
thisisthewayitwillbe,3aopyk,Conchylicultor,1 point,Mon Jun 22 16:17:08 2015 UTC,"I wanted to say that's way to much. But zooming all the way in, it doesn't seem like it's that much. Like I can see objects in that high of resolution if I get close enough."
thisisthewayitwillbe,3ao6xp,mirror_truth,2,Mon Jun 22 03:15:49 2015 UTC,"Not that I believe it, but just thinking hypothetically:  the emergence of true AI in neural nets might come about as a phase transition, like in Erdos-Reyni graphs when the probability parameter is gradually tuned.  Perhaps, the ""mind"" of an AI can somehow be modelled as a random graph, whereby if the connection density is just below a certain threshold, you get an ok virtual assistant with little pockets of isolated capabilities... but then if you cross the threshold, you suddenly get a complete mind.  ""Sharp Threshold"" phenomena occur all over the place.  I've seen a few talks by Benny Sudakov where he mentioned how certain general classes of problems pertaining to ""monotone boolean functions"" have the ""sharp threshold"" property, for example."
thisisthewayitwillbe,3anpmt,starspawn0,5,Mon Jun 22 00:30:51 2015 UTC,The Jeff Dean conference (GPU Technology Conference 2015): http://www.ustream.tv/recorded/60071572  There is nothing really new but he speak of that at about 43:30
thisisthewayitwillbe,3anpmt,Conchylicultor,2,Mon Jun 22 16:04:16 2015 UTC,I wonder if a dataset with speaker information would have allowed the net to reason about more complex things. The results they have obtained are surely impressive though...
thisisthewayitwillbe,3anpmt,theotherhiveking,2,Mon Jun 22 06:05:59 2015 UTC,"Click on the paper to see some of the example conversations.  I was very surprised that even the simple LSTM models (one-layer) did decently.  Obviously, larger, more complex networks will work better.  From the paper:   We find it encouraging that the model can remember facts, understand contexts, perform common sense reasoning without the complexity in traditional pipelines. What surprises us is that the model does so without any explicit knowledge representation component except for the parameters in the word vectors.  Perhaps most practically significant is the fact that the model can generalize to new questions. In other words, it does not simply look up for an answer by matching the question with the existing database. In fact, most of the questions presented above, except for the first conversation, do not appear in the training set.   One of the drawbacks they mention is that the system doesn't seem to have a consistent personality.  That's probably due -- at least partly -- to the fact that many different conversations by many different people were used as training data.    Give this a couple years, and it will be amazing -- it will put all the other chatbots to shame!"
thisisthewayitwillbe,3anpmt,Noncomment,1 point,Mon Jun 22 00:42:20 2015 UTC,Is there any way to go beyond just mimicking the training data? This doesn't seem useful for anything else other than being a chatbot.
thisisthewayitwillbe,3akapg,starspawn0,7,Sun Jun 21 01:03:11 2015 UTC,"I like this last line:   Will machine learning, powered by neural networks, simply transform everything? ‚ÄúYes,‚Äù Hinton says. ‚ÄúAnd quite soon.‚Äù   This paragraph is also nice:   And with that philosophical hurdle (consciousness) out of the way, there‚Äôs nothing to stop us from constructing truly intelligent machines, Hinton says. To be sure, with today‚Äôs technology, no machine can perform as well, at so many different kinds of cognitive tasks, as a real live person with a fully functional brain. But a machine that‚Äôs modelled on the brain ‚Äì a machine that can recognize patterns, and learn from its mistakes, just like people do ‚Äì can think, too. And it won‚Äôt be mere illusion. It‚Äôs not just that they‚Äôll look or sound as though they‚Äôre being smart; Hinton believes they‚Äôll actually be smart. The first signs of the coming sea change are already here, from advances in computer vision to speech recognition to the self-driving car ‚Äì and Hinton is confident that the revolution in machine intelligence is only just beginning."
thisisthewayitwillbe,3akapg,andmar74,3,Sun Jun 21 01:14:55 2015 UTC,"Well written article. When is ""quite soon"" ? :-) Hinton seems rather certain that neural networks can be developed into something more sophisticated, maybe human level AI (?). With what confidence can we say that neural networks is not a dead end ?"
thisisthewayitwillbe,3akapg,zombiefood1,6,Sun Jun 21 06:44:19 2015 UTC,"Neural nets can be made to run any program that can be run on a digital computer; so, to say that they can't reach human-level AI is to say that they digital computers can't reach human-level AI.  This point figured prominently in Chalmers's criticism of Fodor (and it's really an obvious point -- shows that Fodor and Pylyshyn, and even many of their critics who never mentioned this point, didn't understand basic computational principles):  http://consc.net/papers/f-and-p.pdf  ....  Hinton thinks commonsense reasoning will be solved in the next decade by neural nets; and that human-like (maybe not human-level) AI will follow soon after -- he thinks we will have virtual assistants that behave in human-like ways, including the ability to understand natural language at a deep level."
thisisthewayitwillbe,3akapg,zombiefood1,2,Sun Jun 21 12:16:21 2015 UTC,you will have to agree you have witnessed the fact i have been harping on this for quite awhile now and it has been tough sledding without a phd and tenure  and this will happen in much less than ten years. it's coming like water through a broken down dam
thisisthewayitwillbe,3akapg,Noncomment,1 point,Sun Jun 21 19:42:15 2015 UTC,"Well, you thought 2015.  That's too optimistic.  You have said, yes, that there is nothing magical about consciousness, like Hinton.  I think there probably is a mystery to consciousness; but, it is not a mystery we are anywhere near to solving, and I doubt it has to do with quantum physics.  Furthermore, I see no reason to doubt machines will have inner experience like ours -- if there is no reason to suspect their experiences are different, might as well accept they are conscious.  At any rate, debating consciousness is a waste of time, unless one has a genuinely new argument or piece of insight to add to the discussion."
thisisthewayitwillbe,3allei,starspawn0,1 point,Sun Jun 21 12:18:54 2015 UTC,"An argument from complexity would have been a stronger one for Fodor and Pylyshyn to make; unfortunately, people continue to ignore it (complexity).  Specifically, a much more defensible ""main thrust"" argument might begin with, ""Yes, connectionist architectures are universal; but that doesn't mean they are easy to train!... etc."""
thisisthewayitwillbe,3aktqw,starspawn0,2,Sun Jun 21 04:34:14 2015 UTC,"Sounds mostly like a problem with having the right training dataset.  I noticed what Karpathy had to say on Hacker News about Convnets.  I don't think Yann Lecun would agree with that; we might find out if he chooses to write another of his Facebook AI blog postings about it.  (In general, one should be careful about taking the opinion of a grad student -- even a sufficiently advanced one -- as the final word on a subject.)   The fact that Convnets contain a lot of information about the structure of objects they are asked to classify should make one pause, and consider that they don't just do texture matching -- or are only capable of texture matching.   Also, you can compute any bounded-depth circuit using a sufficiently deep convnet (and approximate any function using a shallow-but-sufficiently-wide regular neural net); so, in principle, if you have the data to train one, you can get it to learn any such function whatsoever.  The space of bounded-depth functions is quite complex, and even includes anything the human brain can do in an arbitrary -- but bounded -- number of time-steps.  The issue, then, is whether one can train the network to learn the desired function.    And if the network doesn't get enough examples to figure out the rules for how to label objects (e.g. whether a couch can be made out of leopard skin covering), it has no chance of succeeding, even assuming access to an oracle to solve NP-hard problems."
thisisthewayitwillbe,3ak24b,neuromorphics,1 point,Sat Jun 20 23:35:15 2015 UTC,Some more videos from the same conference:  IBM SyNAPSE Deep Dive Part 1  IBM SyNAPSE Deep Dive Part 2  IBM SyNAPSE Deep Dive Part 3  IBM SyNAPSE Deep Dive Part 4  (talks about portable and neural supercomputers)  Visual Cortex on Silicon
thisisthewayitwillbe,3ak24b,theotherhiveking,1 point,Sun Jun 21 20:49:18 2015 UTC,This is seriosly impressive. 1000X faster and 400000x  more efficient.  I wonder what fab process they are using...
thisisthewayitwillbe,3ak24b,theotherhiveking,1 point,Mon Jun 22 06:46:04 2015 UTC,Found out. It's Samsung's 28nm.
thisisthewayitwillbe,3aihi4,starspawn0,3,Sat Jun 20 14:39:50 2015 UTC,"Could the fear of robots have a similar basis? I'm reminded of the scene from Automata where the villain tells the robot it's just a machine, and the robot replies ""'Just a machine'? That's like saying you're just an ape."" And then he shoots the robot."
thisisthewayitwillbe,3aihi4,FractalHeretic,5,Sat Jun 20 15:30:02 2015 UTC,"I think there are many fears of robots.  One that I think is growing in importance is the fear of dehumanization.  People are afraid that AI and robots will reveal human nature to be nothing more than the product of various mathematical tricks and feedback loops.  They want to believe that there is something special going on there in the brain, that can never be replicated by a formula.    So, you see people writing things like:    ""That's not art.  Machines can't create; they can only execute commands.""   ""No, stupid, machines don't ""understand"" (and never will!).""  ""There will always be a role for humans to play.  Machines don't have judgment."" ""There are more things in heaven and earth, Horatio, than are dreamt of in your philosophy!"""
thisisthewayitwillbe,3ahgch,starspawn0,3,Sat Jun 20 05:28:03 2015 UTC,"Beautiful article.  Reminds me of Ramez Naam's Novel ""Nexus"" where the monks have reached a ""perfect"" state of mind through decade long meditation and with the nanotech Nexus, ""normal"" people can accomplish this same state of mind in a few seconds.  Hopefully AGI will have a similar effect on us and change how we deal with each other..."
thisisthewayitwillbe,3ahgch,HD125823,3,Sat Jun 20 12:00:08 2015 UTC,"Yawn. Give me a hedonistic paradise over this hippie crap any day. Excitement is more important than happiness, anyway."
thisisthewayitwillbe,3af7uv,starspawn0,2,Fri Jun 19 17:17:33 2015 UTC,"I'm sure the Evo-Psych crowd have a rebuttal  -- or at least a set of clever arguments for why this doesn't contradict the ""modular mind"" hyporhesis in the slightest.  They are good with words and argument.  (Of course, as I've said before:  just because you have a good argument for something, doesn't make it true.)"
thisisthewayitwillbe,3af7uv,skynet2013,2,Fri Jun 19 17:36:41 2015 UTC,Isn't this kind of what Ray K says in How To Create a Mind?
thisisthewayitwillbe,3aclk6,starspawn0,3,Fri Jun 19 00:37:04 2015 UTC,"I imagine that, in a couple years, complex deep neural nets with lots of training data will be able to do something like the following:  you say to the network a description of what you would like to see -- say, ""A purple cow running around in circles on a soccer field."" -- and it will produce a little video clip (maybe 20 seconds long) for which that is a perfect caption.    That kind of capability would be extremely useful for presentations and amateur artists. It also would be useful for VR aficionados."
thisisthewayitwillbe,3aclk6,alexmlamb,2,Fri Jun 19 01:22:31 2015 UTC,I'm interested in building something like it.  That's a neat idea for an application.
thisisthewayitwillbe,3aafus,starspawn0,0,Thu Jun 18 14:41:35 2015 UTC,we only need one computer. there is no restriction on size as there would be if we were to  wish to be as smart  we are brain computer interfaced through our eyes and ears  do we need more?
thisisthewayitwillbe,3aaq9f,Yuli-Ban,2,Thu Jun 18 16:02:06 2015 UTC,I posted that a few days ago.
thisisthewayitwillbe,3aaq9f,starspawn0,3,Thu Jun 18 16:11:13 2015 UTC,"Are you serious...!  Again, I swear I didn't know... things are moving so fast... I love it..."
thisisthewayitwillbe,3aaq9f,starspawn0,2,Thu Jun 18 16:13:23 2015 UTC,http://www.reddit.com/r/thisisthewayitwillbe/comments/39ycic/speech_recognition_from_brain_activity/
thisisthewayitwillbe,3aaes0,fricken,2,Thu Jun 18 14:32:44 2015 UTC,Duplicate of my post.  Look at the posts on the forum.
thisisthewayitwillbe,3aa335,starspawn0,2,Thu Jun 18 12:45:53 2015 UTC,"Not a very in-depth article.    Will intelligent agents be useful?  Probably.  So far the biggest players are the ones with huge pockets and plenty of user data to play with.  That won't prevent other companies from coming up with something better, but it does show you that the barrier to entry in this space, at least right now, is costly.  It won't be a ""garage invention"" any time soon."
thisisthewayitwillbe,3a9y37,starspawn0,2,Thu Jun 18 11:55:04 2015 UTC,"...FAA, says that Amazon plans to speed up the regulation process...   If that doesn't sound like money talks, I don't know what else does."
thisisthewayitwillbe,3a91yn,starspawn0,1 point,Thu Jun 18 04:50:05 2015 UTC,"I had long predicted that we would see something like this before 2020 -- in fact, I predicted it for video, rather than images.  I claimed that the ""dreams"" we would see might lack some coherence, but would still have a ""plot""."
thisisthewayitwillbe,3a6ajk,reworksophie,3,Wed Jun 17 15:49:10 2015 UTC,We're in the process of uploading but there's already 9 viewable :)
thisisthewayitwillbe,3a6ajk,starspawn0,2,Wed Jun 17 15:50:59 2015 UTC,"See, especially, Andrew Ng's fireside chat.  He discusses Baidu's image question-answering."
thisisthewayitwillbe,3a6dei,starspawn0,3,Wed Jun 17 16:09:29 2015 UTC,"That's reaffirming. What types do art schools cultivate? Creative, entrepeneurial longshot systems thinkers who utilize unconventional problem solving and have a keen instincts about  how to leave an impact on people, and are comfortable obsessively pursuing goals that no one else understands or believes until they see it."
thisisthewayitwillbe,3a6jnw,starspawn0,1 point,Wed Jun 17 16:54:07 2015 UTC,"Federal Reserve economists have also taken a closer look.  I don't recall the source, but they considered PR for people in the 16 to 24 age group range.  Taking a fiiner spilt from 16 to 19, and then 20 to 24, both groups were found to see a significant decline.  The 16 to 19 figures are understandable, as it could mean more people going to college (newer high-paying jobs require more education than they did two decades ago).    ....  At any rate, the 25-to-54 LFPR  graph McAfee cites all but elimates the contribution of retiring Baby Boomers and young people going to college in greater numbers, and probably many goung back to college.  Yet, there still is a significant decline, starting around the year 2000; and that trend has accelerated since 2009."
thisisthewayitwillbe,3a6e47,starspawn0,2,Wed Jun 17 16:14:24 2015 UTC,Here is a company that just released a complete end to end IIOT solution http://www.divelbiss.com/VersaCloud/
thisisthewayitwillbe,3a4ybv,mirror_truth,3,Wed Jun 17 07:00:46 2015 UTC,"Personally, I disagree, but I was interested to read the opinions of those in this subreddit."
thisisthewayitwillbe,3a4ybv,Fab527,2,Wed Jun 17 07:02:21 2015 UTC,"He repeatedly insist on the lack of social and economical impact of new technologies, but he also repeatedly avoid talking about what AI will do to the world.  Yes, humans are more or less the same they have ever been (and that's the reason planes didn't change much etc...). Nope, AI is not something that has already happened. When you put it into the picture, your appeals to history will instantly cease to have any meaning.   Also...   But are we likely to experience such profound changes over the next decade or two? The answer is no.   No s**t sherlock. It doesn't have to happen right now, I thought we were talking about the future."
thisisthewayitwillbe,3a4ybv,starspawn0,2,Wed Jun 17 10:17:00 2015 UTC,"20 years into the future is a long, long time, and virtually impossible to predict.  Imagine trying to predict what would happen here in 2015 based only on what you knew in 1995.  Even 10 years into the future is impossible to predict -- here, for example, is how bad a job the Bureau of Labor Statistics did:  http://cdn.theatlantic.com/assets/media/img/posts/bls%20indsutry%20employment%20check-1.jpg"
thisisthewayitwillbe,3a4ybv,andmar74,2,Wed Jun 17 12:09:33 2015 UTC,"Even 10 years into the future is impossible to predict   I think it's worse than that. Now that AI is getting more sophisticated and powerful, the  prediction range goes down. Who saw IBM Watson Jeopardy! victory coming? No one. And now IBM is betting heavily on Watson, which could lead to a revolution or not so much, hard to predict."
thisisthewayitwillbe,3a28a3,starspawn0,2,Tue Jun 16 17:35:19 2015 UTC,if you read the whole think it states that darpa had this thing under wraps since 2013  no telling what they have going on  here is the video  https://www.youtube.com/watch?v=HyqT9Bdamt8
thisisthewayitwillbe,3a28a3,zombiefood1,2,Tue Jun 16 22:34:24 2015 UTC,no telling what they have going on   Likely nothing secret.  DARPA isn't the NSA.  It's more like NSF.
thisisthewayitwillbe,3a28a3,zombiefood1,1 point,Wed Jun 17 00:01:36 2015 UTC,the article you posted says they have been keeping this walking robot secret till now. my experience with the govt is that by the time they show something they have something much better  http://www.wired.com/2011/07/darpas-secret-spy-machine/
thisisthewayitwillbe,3a1vsx,starspawn0,1 point,Tue Jun 16 16:09:35 2015 UTC,so will robot brains
thisisthewayitwillbe,3a16p8,starspawn0,1 point,Tue Jun 16 12:40:37 2015 UTC,"you go first,  no you go first,   ok we'll go first,  ok we will be behind but we will see what happpens to you first,  good, if we make it work you will never catch up  sure and if it doesnt you are fucked  wonder if you will get a return of the luddites"
thisisthewayitwillbe,39xd3i,johnnd,2,Mon Jun 15 15:58:28 2015 UTC,I think it's hype.  Too many safety issues; lots of money needed; not enough political momentum to get it done anytime soon.
thisisthewayitwillbe,39xd3i,starspawn0,3,Mon Jun 15 16:02:07 2015 UTC,"It may be hype, but it's the prototype for vactrains, so... I dunno. Give it a little love."
thisisthewayitwillbe,39xd3i,Yuli-Ban,1 point,Mon Jun 15 19:09:22 2015 UTC,"I am just excited to see the test tracks that the Hyperloop companies will build. If it can be shown to work there, I think the political will will change. I am too optimistic."
thisisthewayitwillbe,39x2tx,starspawn0,1 point,Mon Jun 15 14:37:48 2015 UTC,"I noticed Ernest Davis wrote a reply:   Yann -- I'm having trouble squaring your question of ""why ML/AI people aren't starting to work on deep learning faster and in greater numbers"" with your post that CVPR was wall-to-wall CNN's and Heng Ji's similar post about ACL. My impression was that everyone and his grandmother was starting work on DL.   That kind of comment sounds like a hidden agenda.  Perhaps along the lines of:  ""I and Gary Marcus knew, just knew, there wasn't a Deep Learning revolution!""    Reading between the lines, the part about Bayesian models also could be a slight -- i.e. something like, ""But Deep Learning doesn't explain like Bayesian models do.  People working on Bayesian models are careful about the assumptions (independence and otherwise) and theory.  Deep Learning is all just a bunch of engineering and empirical stuff (i.e., not respectable among intellectuals)!"""
thisisthewayitwillbe,39xhlg,starspawn0,1 point,Mon Jun 15 16:31:07 2015 UTC,https://quantumsausagemachine.wordpress.com/2013/07/23/convert-to-truth-app/  my  short sci fi story goes with this science fact story
thisisthewayitwillbe,39uzu3,starspawn0,1 point,Mon Jun 15 00:44:30 2015 UTC,thinking?
thisisthewayitwillbe,39vkfv,starspawn0,1 point,Mon Jun 15 03:51:16 2015 UTC,the more it learns the more it understands and the more it understands the faster it can learn  sounds like a hard start
thisisthewayitwillbe,39vkfv,zombiefood1,2,Mon Jun 15 19:07:19 2015 UTC,Hard start is not coming anytime soon; but some amazing technology is.
thisisthewayitwillbe,39vkfv,Yuli-Ban,1 point,Mon Jun 15 19:09:23 2015 UTC,"Quick question‚Äî deep learning... is this new, or am I confusing it with neural networks? I recall deep learning undergoing a grunge moment in 2011, 2012, but with recent news of thought vectors, it sounds as if it's an old and tried technique. Where does deep learning stand when it comes to thought vectors?"
thisisthewayitwillbe,39vkfv,zombiefood1,2,Mon Jun 15 19:11:15 2015 UTC,"Deep Learning is about learning ""feature hierachies""; though. mostly the way people use the term is identical with ""neural networks"".  ""Thought vectors"" is a new term that Hinton invented.  It refers to a snapshot of the state values of a neural net as it is ""thinking"" or ""processing"" some input.  Hinton argues that thought simply boils down to transforming a vector into others.  This contrasts with the ""symbolic"" view of AI, which argues for processing of symbols as the basis for thought / intelligence."
thisisthewayitwillbe,39sb10,apmechev,2,Sun Jun 14 07:22:16 2015 UTC,"Interesting comment from Jurvetson:  he said that ""Travis"" (Kalanick?) said that if Tesla makes fully autonomous cars by 2020, that he would buy them all (for Uber?)."
thisisthewayitwillbe,39qbid,johnnd,2,Sat Jun 13 18:43:13 2015 UTC,"Imagine what computer graphics capability in 20 years will be, and how more advanced VR will be, this is the future"
thisisthewayitwillbe,39qox8,starspawn0,-1,Sat Jun 13 20:42:14 2015 UTC,"Here's why I'm skeptical of VR: for the foreseeable future, the only form of VR that will be possible is the kind that involves headsets. These headsets will be immersive for our vision and hearing, but we still won't be able to feel or touch anything in the VR environment.   Perhaps a little later, haptic devices will come into play, but even these will only allow you to feel textures. If a VR wall is right in front of you and you reach up to touch it, you may feel a texture but you'll also feel no resistance to your arm as you extend it outward. You'll also feel no resistance if you tried to walk right through a wall, or a cliff face, or a tree. If you tried to pick up a VR ball, the ball will be weightless and your fingers will be able to penetrate its surface effortlessly.  Similarly, you won't be able to experience different levels of gravity unless you had some kind of expensive harness. If you're wearing an Oculus Rift in your living room, and you wanted to experience what jumping around on the Moon is like, you wouldn't be able to. And finally, you wouldn't experience having a different body. You can change the appearance of your avatar, but its only visually cosmetic: you won't feel any different.   And all of this is still leaving out the fact that we won't be able to taste or smell anything, either. Because of all of this, VR is going to be quite limited. It's going to be purely ""Look and hear, but don't touch, feel, taste, or smell"". People will quickly realize how limiting it really is. Perhaps in the distant future, these problems will be dealt with but that is a long way off."
thisisthewayitwillbe,39qox8,MissKaioshin,2,Sun Jun 14 01:52:32 2015 UTC,"And if you had all those things, then there would still be people who complain about the even finer aspects of the experience that haptics and harnesses and other things don't provide.    Cronin has tried it; I've tried it; many people have tried it -- we're not talking about some hypothetical technology here.  This already exists.  Some got quickly bored; others didn't.  I personally could care less if people call it a ""flop"" -- because I will still keep enjoying it.  I say there's about a 30% it will flop; 70% chance it will take off like wildfire."
thisisthewayitwillbe,39qox8,johnnd,1 point,Sun Jun 14 02:07:26 2015 UTC,"Where you Miss the boat is that even with just vision, hearing and hand presence the experience is still highly compelling. It won't feel like reality in those other aspects, but it will still be the most immersive form of consuming 3D graphics, architectural visualizations, porn, etc. And so it will be fairly successful.  One solution to the haptic problem is fitting ordinary objects or sex dolls with markers, then modeling them 1-to-1 and in effect importing them to the VR world. Add animation and you will, again, have a very compelling experience."
thisisthewayitwillbe,39owm2,johnnd,1 point,Sat Jun 13 09:48:43 2015 UTC,Funny why?
thisisthewayitwillbe,39owm2,andmar74,3,Sat Jun 13 13:56:58 2015 UTC,"Because a five-order-of-magnitude jump and a doubling time of 9 months isn't going to happen in that time frame. This is significantly more optimistic than Kurzweil, who himself is wrong."
thisisthewayitwillbe,39owm2,andmar74,1 point,Sat Jun 13 14:09:11 2015 UTC,"From 2000 to 2013, the number one supercomputer went from 2.4Tflop/s to 33.9Pflop/s, which is 14125 faster, or 1.4*104 times faster. http://www.top500.org/statistics/perfdevel/"
thisisthewayitwillbe,39owm2,sasuke2490,3,Sat Jun 13 14:24:12 2015 UTC,"Which is still an order of magnitude less than what Diamandis proposes--not a trivial error!  Secondly, processing power was increasing faster in the early 2000s than it is now.  Thirdly, superomputers are generally on a different slope than discrete processors (i.e. relevant to you and me). This can be explained through better interconnects, better parallel architectures, more financing, increasing the thermal envelope, increasing the number of nodes, etc.  Fourthly, supercomputer progression is starting to falter. You can see it in the chart you linked to by following the projected performance vs. the actual trend. Sure, not enough data yet. But the situation isn't going to get any better in the near future, trust me.  Getting the ""joke"" presupposes understanding these basic premises."
thisisthewayitwillbe,39owm2,NanoStuff,1 point,Sat Jun 13 14:56:48 2015 UTC,there are various emerging technologies like optical computing and memristors that should come out in the next several years. perhaps by the early 2020s we will see a new computing architecture.
thisisthewayitwillbe,39p3nk,starspawn0,1 point,Sat Jun 13 11:29:21 2015 UTC,"The people who run Imagenet are probably shocked by how much attention this has gotten; and how a simple violation of rules has led to the downfall of a distinguished engineer.  This is what happens when there are billions of dollars at stake -- all the money flowing into CS, and deep learning especially, is warping things all out of proportion.  That said, based on Wu's response here, it looks like a ""Clinton lie"" to me (instead of being about what the meaning of ""is"" is, it's about the meaning of ""you"")."
