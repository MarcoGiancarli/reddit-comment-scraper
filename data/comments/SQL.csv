SQL,3e73ps,HoS_CaptObvious,1 point,Wed Jul 22 13:42:33 2015 UTC,"I deal with this kind of problem pretty often and as painful as is to accept, using a loop is a hell of a lot easier.  You can do it set based, but when you have these kind of rules, it starts getting messy in a hurry and you end with these monstrous queries that are very difficult to understand.  Adding more rules causes the complexity to go up exponentially.  The real problem shows up when some one wants you to tweak the rules.  Maintenance of these queries is a nightmare.    In my case, I've found trading a whole bunch of performance is worthwhile because things changes enough to justify the hit.  Every once in a while there will be something that's performance critical and then I'll go through the hassle of doing it set based, but the majority of the time, it's a loop where I evaluate the which bucket to put the current item in against all the rules and what I've already done (to keep the buckets as balanced as possible)."
SQL,3e73ps,lukeatron,1 point,Wed Jul 22 14:23:36 2015 UTC,What do you mean by using a loop?
SQL,3e73ps,lukeatron,1 point,Wed Jul 22 15:07:30 2015 UTC,"A WHILE loop.  You iterate one at a time over each individual item you're sorting and apply your logic to that item to determine which buckets it's allowed to go in then you decide which of those to pick.  Usually what I do is keep track of the results in a temp table or a table variable and after all the items have been placed, create or update the actual records from that temp table.  This is going to be a lot slower than querying out the answer and is not really a SQL way of doing things, but for the reasons I described above, I generally prefer this approach."
SQL,3e73ps,lukeatron,1 point,Wed Jul 22 15:13:54 2015 UTC,"Hmm, not sure if this feasible unless I'm not correctly understanding the way the WHILE loop works since the data set contains almost 300 stores and 150,000 customers"
SQL,3e36d8,Byrd952,14,Tue Jul 21 16:35:10 2015 UTC,"MSSQL wants you to alias all subselects, even simple ones.  Add ""AS AccountEmails"" or something similar to the very end of the query."
SQL,3e36d8,arfrazor,3,Tue Jul 21 16:39:56 2015 UTC,"That did it, thanks!"
SQL,3e36d8,Thriven,2,Tue Jul 21 17:16:34 2015 UTC,Whats the like '%' '%' trying to do exactly? I've never seen that before.
SQL,3e36d8,Elfman72,3,Tue Jul 21 17:05:27 2015 UTC,"Looking for ' with wildcards.  Inside the string, the second single quote is an escape character for the first one."
SQL,3e36d8,Thriven,1 point,Tue Jul 21 17:14:10 2015 UTC,"Yeah, that is odd.  Maybe looking for e-mails that are blank as opposed to null?"
SQL,3e36d8,slickwombat,1 point,Tue Jul 21 17:11:45 2015 UTC,Just looking for email addresses that contain single quotes.
SQL,3e095y,polyglotdev,8,Mon Jul 20 23:27:34 2015 UTC,Test on a Dev server?
SQL,3e095y,juitar,2,Tue Jul 21 00:43:25 2015 UTC,And set a sql profiler to see what's happening
SQL,3e095y,gruffi,3,Tue Jul 21 08:30:44 2015 UTC,"the script just inserts a report(200K rows) into a pre-existing table and then runs an UPDATE Statement over the table, but the process didn't complete and then the log file ballooned to 400GB.   Yeah, this does not make any sense unless you have nested triggers enabled or an infinite loop in your script or some kind of SQL injection going on or some serious failure in logic like running UPDATE Table SET Field = 'Value' WHERE 1 = 1 every step of a loop.  It sounds like you're using Python for ETL.  I would probably not do that.  I would probably use SSIS because that's very easy once you know how to author a package.  If I couldn't do that, I'd probably use Powershell.  There's nothing wrong with Python, but .Net interfaces very well with SQL Server and the provider has been tested to death.  If you know SqlDataAdapter and SqlCommandBuilder it's pretty straightforward to do, but even parameterized statements in a loop with SqlCommand is pretty easy.  In any case, whenever I do data imports I always use a staging table.  Usually I create an actual table and not a temp table.  First because then you can see the data after import but before you merge without cracking open the source file.  Second because you can write a view that does all the necessary transformations.  All you need to do is remember to start off with a TRUNCATE or DELETE on the table (use the latter if you want the possibility of a rollback, the former will save log space and reset AUTOINCREMENTs).  Now all your logic is stored in the DB in a nice, deterministic format.  Then you start a transaction and do your INSERTs and UPDATEs.  I tend to avoid MERGE because it has some rare concurrency issues, but if you do use it be sure to specify HOLDLOCK/SERIALIZABLE locking hints or the SERIALIZABLE transaction isolation level to avoid most of the badness.  [Short version is that MERGE unrolls into a bunch of UPDATE and INSERT statements, and doesn't entirely make sure nothing steps on it's feet... including itself.]  And as /u/juitar says, develop on a Dev server!"
SQL,3e095y,da_chicken,1 point,Tue Jul 21 02:31:17 2015 UTC,"I'm not sure if this is relevant, but the last command in the sql script is db.exectute(""UPDATE Table SET Last_Update_Date = SYSDATETIME();"")  It's not executed inside of a loop and no error is thrown. However, the code seems to freeze on running this command."
SQL,3e095y,cl0ckt0wer,1 point,Tue Jul 21 03:11:05 2015 UTC,I don't like using functions inside big statements like that. Create a variable of the same data type as LAST_UPDATE_DATE then set the column equal to that variable.
SQL,3e095y,da_chicken,2,Tue Jul 21 05:57:00 2015 UTC,"This doesn't matter.  SYSDATETIME(), like GETDATE(), is a runtime constant function.  While different instances of the function in the same query might have different values, a single instance will always have the same value because it's only evaluated once at the start of the statement's execution.  Try running this:  SET SHOWPLAN_XML ON; GO  SELECT SYSDATETIME() FROM SomeTable; GO   You'll see this in the plan:  <ScalarOperator ScalarString=""sysdatetime()"">     <Identifier>         <ColumnReference Column=""ConstExpr1005"">             <ScalarOperator>                 <Intrinsic FunctionName=""sysdatetime""/>             </ScalarOperator>         </ColumnReference>     </Identifier> </ScalarOperator>   The ""ConstExpr"" tells you it's a runtime constant.  See also here."
SQL,3e095y,da_chicken,1 point,Tue Jul 21 11:56:22 2015 UTC,"So you have a 200K row report that you're inserting, and after every row you insert you're updating the entire table's Last_Update_Date?  That seems very excessive.  On an empty table inserting one row at a time followed by the table-wide update that's (200000*200001)/2 = 20,000,100,000 updates!  If you're using a datetime2 or datetime2(7), that's 8 bytes of data for every row.  160,000,800,000 bytes ~= 150 GB just in datetime2 values.  If you've got your DB set to Full recovery, your transaction log has to remember all of those changes for point-in-time recovery.  Even if it's set to Simple recovery, there probably is too much activity for a periodic CHECKPOINT to flush the logs.  Even skipping that, I can't imagine needing to track a single date value on every single record.  It's essentially Table_Last_Update_Date.  You're sure that's right?  It's not supposed to be the record's Last_Update_Date?  Why not just update the value once at the end of the loop?  I mean, how many millions of rows is the table if you're inserting or updating 200K rows for a single report?    And is there a trigger on the table?  Many systems with this type of field have an AFTER INSERT, UPDATE trigger to automatically update the field when the record is changed."
SQL,3e095y,liveMonkeyBeware,1 point,Tue Jul 21 11:37:39 2015 UTC,"I use https://pypi.python.org/pypi/pyodbc/, just FYI.  Why do you want to use Python for this?  I'm always down for learning new things, but when you start doing this in production there are other concerns.  Scheduling. If you used SSIS or Powershell, you could use a SQL job to schedule it.  Using python on windows, the easiest way to schedule it would be a scheduled task, but that functionality has always sucked and it's better to have everything in one place.  You could still use a job with python, but then you'd have to get into remote commands because I doubt your DBA would want python on the production db server.  Maintenance. It sounds like you're the only one using Python in the office.  This causes problems if you're out and something breaks.  Python's not hard, but if the script is critical and whoever gets to fix it has to learn Python on the fly, you'll probably get chewed out when you get back."
SQL,3e095y,pug_subterfuge,2,Tue Jul 21 02:07:13 2015 UTC,"pyodbc is slow when inserting a large number of records. It's a performance issue related to how it executes the executemany method, which prompted me to switch to the ceODBC module"
SQL,3e095y,jaynoj,1 point,Tue Jul 21 03:15:19 2015 UTC,Always open your connections like this.    with ceodbc.connect(.....) as c:     cur = c.cursor()
SQL,3e095y,whileoneequalsone,1 point,Tue Jul 21 11:42:48 2015 UTC,If you're doing ETL use SSIS as it's specifically designed and optimized for what you need.
SQL,3dzfr1,throwawayjdn,2,Mon Jul 20 19:53:01 2015 UTC,"If I understand your question correctly.   SELECT  accountid  , cycleendingbalance  , purchaseamount_a  from [dbo].[table]   where at_creditlimitdelta <0   and   at_datelastlimitchange BETWEEN dateadd(m,-3,GETDATE()) AND dateadd(m,3,GETDATE())"
SQL,3dzfr1,notBenstar,1 point,Mon Jul 20 20:19:53 2015 UTC,"Shouldn't the dateadd be   dateadd(m,-3,getdate())   You want at_datelastlimitchange three months before today rather than three months after.   Some context about the table would help - is there just one record for each accountid or a record per accountid per day or what?"
SQL,3dzfr1,fauxmosexual,1 point,Mon Jul 20 20:23:46 2015 UTC,"*yes -3 for before. the problem is i dont want the balance, and spend 3 months before Today's Date. i want it 3 months before the 'at_datelastlimitchange'"
SQL,3dzfr1,fauxmosexual,1 point,Mon Jul 20 20:35:42 2015 UTC,"What does that table have in it, one record per account or one record per account per day or what?"
SQL,3dzfr1,fauxmosexual,1 point,Mon Jul 20 20:37:23 2015 UTC,sorry. the table has one record per account per month.
SQL,3dzfr1,jamsy,1 point,Mon Jul 20 20:44:42 2015 UTC,"select accountid, at_datelastlimitchange,  sum (case when at_datelastlimitchange = dateadd(m,-3,at_datelastlimitchange) then cycleendingbalance else 0.0 end) as Balance3MonthsBeforeChange from [dbo].[table] where at_creditlimitdelta <0 group by accountid, at_datelastlimitchange"
SQL,3dzfak,yellising,1 point,Mon Jul 20 19:50:04 2015 UTC,Use IF EXISTS(select SCENARIO) BEGIN  -- do something  END
SQL,3dzfak,LagWagon,1 point,Mon Jul 20 21:08:32 2015 UTC,"Thanks, but I still fail to see how I can implement this on the entire team's schedule in one go. I understand I can use this if I'm trying to check and insert one row at a time. Can your suggestion be used to an entire selection in one go?"
SQL,3dzfak,LagWagon,2,Mon Jul 20 22:34:01 2015 UTC,"Even without the exists:  INSERT INTO TABLE(StartDay,EndDay) SELECT GETDATE(), NULL FROM Table1 WHere (new agent criteria)  UPDATE T SET ENDDATE = getdate() FROM Table1 T WHERE Agent already exists  INSERT INTO Table T WHERE Agent already exists and EndDate = getdate()"
SQL,3dzfak,HoustonWHOO,1 point,Mon Jul 20 23:29:10 2015 UTC,use case statements and break em apart.
SQL,3dyjwe,admin_password,2,Mon Jul 20 16:08:56 2015 UTC,"The biggest question I have is whether or not each user will have their own login to access the data or if the user interface would be going through a proxy or service account.  My fear is there'd be some way for User A to see User B's data if the account both are using technically has access to both sets of data.  So security would be my first concern.  If you setup a different schema/owner for each user's data with completely different accounts tied to each schema that may be a good way to segment security, but that would need to be setup from the beginning.  Also how many users are you talking about here?  If just a few dozen it may not be a big deal to split into separate databases, but if thousands that could be impractical.  You could also mix it up by having a limit by creating a new database with your latest database reaches a preset limit, so multiple users per database plus multiple databases.  Again just be sure your security is in place.  The only way I could see issues with shared data on the same database are locks if each user will have access to common tables.   Also don't give users the ability to run ad hoc queries or these can bring a database and server to its knees.    I could go on, but if you could give more details (number of users, security layout, which database system are you using, etc) that would help."
SQL,3dyjwe,samalex01,1 point,Mon Jul 20 17:12:25 2015 UTC,"Each user would have their own login to access the data, which got me into thinking of this whole 'database per user' scenario where I could also provide them with read SQL access to their database.   As for how many users I can't really answer that myself, the idea could be a total failure and then sure a few dozen, or it could explode and I suddenly have 1000s of users. I would guess initially all going well to be dealing with 100 or so active users and maybe 200-300 more ""log on once or twice and use it"" users.  I wasn't really too worried on the security aspects of doing something like this, I think I understand pretty well on how to keep separate it per user whether its on the same database or different databases.  I'm much more concerned about performance and my question is really I guess ""how many rows is too many rows in a table"". I'm really just trying to design a way of storing this data that will scale cheaply if the idea gains popularity.   As for which DB I guess either mySQL or PostgreSQL with leaning more toward PostgreSQL at the moment."
SQL,3dyjwe,FIuffyRabbit,1 point,Mon Jul 20 18:36:25 2015 UTC,"Several years ago, Reddit used to only use two tables. Now it uses two tables per thing (users, posts, etc). So each table can have millions of rows. It's pretty typical for a heavily used table to have millions of rows.   At work on our test server we have one table with 4m rows, one with 1.5m rows, and it looks like 10, or so, more with 500k+ rows. That isn't even counting what is actually on the production server that clients use. You need to remember how efficient computers actually are at processing data. Especially SQL."
SQL,3dyjwe,el_chief,2,Tue Jul 21 15:54:15 2015 UTC,Read this then come back https://msdn.microsoft.com/en-us/library/Aa479086.aspx
SQL,3dyjwe,Thriven,1 point,Mon Jul 20 18:21:40 2015 UTC,"Thanks, I just gave it a quick scan through but looks very helpful I'll read it fully later."
SQL,3dy0oo,notasqlstar,1 point,Mon Jul 20 13:41:57 2015 UTC,72000 rows to sort through :(
SQL,3dy0oo,Grachuus,1 point,Mon Jul 20 13:56:13 2015 UTC,"If you're indexed on the sort row and not using other parameters OR you're group indexed including the fields with additional parameters the process load is not likely to be intense barring the data pull is larger.  72,000 rows is not significant at all."
SQL,3dy0oo,Coldchaos,1 point,Mon Jul 20 14:02:25 2015 UTC,"You could try something like this if you are using SQL Server(there are equivalences for others):  -- Note: Change datatype to match that of cs-uri-query DECLARE @tempstr VARCHAR(MAX) DECLARE findErrorCrsr CURSOR FOR     SELECT cs-uri-query     FROM yourTable  IF OBJECT_ID('tempdb..#failedToConvert') IS NOT NULL      DROP TABLE #failedToConvert CREATE TABLE #failedToConvert (dataz VARCHAR(MAX), datamsg VARCHAR(MAX))   OPEN findErrorCrsr FETCH NEXT FROM findErrorCrsr     INTO @tempstr   WHILE @@FETCH_STATUS = 0 BEGIN BEGIN TRY     CASE WHEN  Sign(CAST(dbo.ParseWebLog([@tempstr],'WT.vt_f_tlv=') AS NUMERIC(14,0)))= -1         THEN NULL     WHEN  dbo.ParseWebLog([@tempstr],'WT.vt_f_tlv=') = '0'          THEN NULL      ELSE          dateadd(ms,abs(CAST(dbo.ParseWebLog([@tempstr],'WT.vt_f_tlv=') AS UMERIC(14,0)))%86400000,dateadd(dd,floor(abs(CAST(dbo.ParseWebLog([@tempstr],'wt.vtvs=') AS NUMERIC(14,0)))/86400000),'01-01-1970'))      END AS LastTimeVisit_Standard END TRY BEGIN CATCH     INSERT INTO #failedToConvert(dataz, datamsg)         SELECT @tempstr, ERROR_MESSAGE() END CATCH END  CLOSE findErrorCrsr DEALLOCATE findErrorCrsr  SELECT * FROM #failedToConvert"
SQL,3dy0oo,Coldchaos,1 point,Mon Jul 20 16:24:57 2015 UTC,"Msg 156, Level 15, State 1, Line 18 Incorrect syntax near the keyword 'CASE'."
SQL,3dy0oo,ziptime,1 point,Mon Jul 20 14:37:19 2015 UTC,Put a SELECT in front of it.
SQL,3dy0oo,ziptime,1 point,Mon Jul 20 14:43:53 2015 UTC,"Thanks, got it to run with a little playing. Letting it go for now. Hate parse tables."
SQL,3dmv9i,plshelpsql,4,Fri Jul 17 15:42:25 2015 UTC,a bunch of 'invalid naming errors   It's hard to find fault in this. The result set column names are terribad indeed.
SQL,3dmv9i,ichp,1 point,Fri Jul 17 16:49:00 2015 UTC,I mean any help or an article you can point me to that I could learn how to fix this?
SQL,3dmv9i,ichp,2,Fri Jul 17 17:04:57 2015 UTC,"https://msdn.microsoft.com/en-us/library/ms175874.aspx   The names of variables, functions, and stored procedures must comply with the following rules for Transact-SQL identifiers. The first character must be one of the following: A letter as defined by the Unicode Standard 3.2. The Unicode definition of letters includes Latin characters from a through z, from A through Z, and also letter characters from other languages. The underscore (_), at sign (@), or number sign (#). Certain symbols at the beginning of an identifier have special meaning in SQL Server. A regular identifier that starts with the at sign always denotes a local variable or parameter and cannot be used as the name of any other type of object. An identifier that starts with a number sign denotes a temporary table or procedure. An identifier that starts with double number signs (##) denotes a global temporary object. Although the number sign or double number sign characters can be used to begin the names of other types of objects, we do not recommend this practice. Some Transact-SQL functions have names that start with double at signs (@@). To avoid confusion with these functions, you should not use names that start with @@"
SQL,3dmv9i,farhil,1 point,Fri Jul 17 18:36:30 2015 UTC,"In addition to ichp's comment, it's better to never use anything other than alphanumeric characters, and to avoid using numeric characters in database object names. There is a balance between clarity and brevity with names. You don't want people that read your code to be overwhelmed by the length of your names, but you also don't want them wondering what the names represent."
SQL,3dmv9i,farhil,2,Fri Jul 17 19:23:44 2015 UTC,"The issue you're asking about  lies with [# Of Exiting Seniors]. You assign the value SUM(CASE pe.GradeLevel WHEN 13 THEN 1 ELSE 0 END) the alias [# Of Exiting Seniors], which is fine, however you then try to use [# Of Exiting Seniors] further down in your calculations, which is not possible as at the time of evaluation SQL won't know what [# Of Exiting Seniors] is as the aliasing hasn't taken place yet. This probably occurs in other parts of your query but I haven't found them yet. That same line (SUM(CASE pe.GradeLevel WHEN 13 THEN 1 ELSE 0 END)) is what's causing you to require a GROUP BY clause, not the subquery.  E* This is how I would have written your procedure. I'm not sure if this will work or not because I don't have your tables or data, so no guarantees. Also I used different column names for my own convenience  Edit 2: Now that I think through it again, this code won't work as you're looking at the number of seniors per school. I'd go back through and correct it, but I honestly don't have time (I'm at work). Sorry  DECLARE @NumberOfSeniors DECIMAL(5,2); DECLARE @DiverseLearners DECIMAL(5,2);  SELECT  @NumberOfSeniors = COUNT(pe.gradelevel) FROM    etccommon.dbo.tblstudentinfo_prev pe WHERE   pe.gradelevel = 13  SELECT  @DiverseLearners = COUNT(DISTiNCT studentid) FROM    etccommon.dbo.tblstudentinfo_prev AS e         LEFT JOIN etccommon.dbo.ods_student AS o ON o.studentid = e.[sid] WHERE   e.gradelevel = 12         AND spedindicator = 'y'  ;WITH info AS (     SELECT               d.schoolid,             CAST(COUNT(d.chk13_01) AS DECIMAL(5,2)) AS SeniorsOrParentsContacted,             CAST(COUNT(d.rdb13_02_y) AS DECIMAL(5,2)) AS SummerSeniors,             CAST(COUNT(d.rdb13_03_y) AS DECIMAL(5,2)) AS CreditRecoveryPlans,             CAST(COUNT(d.rdb13_04_y) AS DECIMAL(5,2)) AS CompletedIepTransitionPlans,             CAST(SUM(d.txt13_05) AS DECIMAL(5,2)) AS AssistedSummerCollegeApps,             CAST(SUM(d.txt13_06) AS DECIMAL(5,2)) AS CollegeAcceptances,             CAST(COUNT(d.rdb13_07_y) AS DECIMAL(5,2)) AS CompletedFafsas,             CAST(SUM(d.txt13_08) AS DECIMAL(5,2)) AS CompletedScholarships     FROM                 datacollection.dbo.stc_transitiontool_data AS d     GROUP BY                 d.schoolid ) SELECT           net.network,         s.school,         s.schoolid,         @NumberOfSeniors,         info.SeniorsOrParentsContacted,         info.SeniorsOrParentsContacted / NULLIF(@NumberOfSeniors, 0) AS PercentOfSeniorsOrParentsContacted,         info.SummerSeniors,         info.CreditRecoveryPlans,         info.CreditRecoveryPlans / NULLIF(@NumberOfSeniors, 0) AS PercentOfCreditRecoveryPlans,         @DiverseLearners,         info.CompletedIepTransitionPlans,         info.CompletedIepTransitionPlans / NULLIF(@DiverseLearners, 0) PercentCompletedIepTransitionPlans,         info.AssistedSummerCollegeApps,         info.CollegeAcceptances,         info.CompletedFafsas,         info.CompletedScholarships FROM             #schools AS s         LEFT JOIN info ON s.schoolid = info.schoolid         LEFT JOIN etccommon.dbo.tblschoolnetwork AS net ON s.schoolid = net.schoolid WHERE            AND info.schoolid IS NOT NULL"
SQL,3dmv9i,Skreex,1 point,Fri Jul 17 18:37:49 2015 UTC,"It's extremely difficult to debug someone else's SQL, especially without the tables handy, but here are a few things I noticed.   SELECT net.[Network] ,[School]   [School] is unaliased here, which table are you pulling it from? I would prefix its table/alias here to avoid confusion and errors.    Further down:   FROM #Schools s    LEFT OUTER JOIN    Where # implies that #Schools is a temporary table. Is it populated outside of this statement? Or is it supposed to be a regular table that's been populated already? If it is a temp table, pay no heed to this.  Furthermore, your naming schemes could use a lot of work. I would try and cut down the column name sizes significantly to ones more manageable. You can always rename the columns in whatever report or application you're providing the data for (though that isn't ideal, it's a better solution to use short-hand names in the stored proc (at least for debugging purposes here).  Good luck!"
SQL,3dnmoa,winningsince1337,3,Fri Jul 17 19:07:37 2015 UTC,"Having used Microsoft SQL and MySQL, I can say of the two that Microsoft SQL had better tools for building stored procedures. (Which is how I communicated with the database so as to avoid ORM weirdness/SQL injection.)  (And depending on your scale/scope/feature set needed of project, Azure hosted SQL databases can be an awesome option. Just remember that you are effectively sharing the server with others at that point, and while security isn't as big an issue, you at least need to be good performance wise about playing in the sandbox together. Otherwise the connection governor can become your mortal enemy.)"
SQL,3dnmoa,cyong,1 point,Fri Jul 17 19:23:19 2015 UTC,"Okay, sounds like Microsoft would be a better choice with the added bonus of Azure as an option for the future.  The scope is really no more than 12 clients accessing the database which contains xrays, lots of notes and specialized forms, along with lots of patient and other information. Typically this scenario runs off of a closed network w/o internet access for security reasons. (Having all of peoples medical records along with their personal information is a HUGE liability) However, I know one of the software options that currently exists runs off of a cloud based solution that stores all information at the developer's HQ, allowing remote access. However, Azure seems like a better platform with better security.  Realistically ~85% of its use would be small text files and the remaining would be .jpg files of xrays."
SQL,3dnmoa,da_chicken,3,Fri Jul 17 19:41:50 2015 UTC,"Honestly I would probably side with MS SQL Server simply because there's a lot more resources out there relating to HIPAA security and auditing compliance especially if you're looking at Azure.  Having worked in healthcare IT some, I can say that Oracle and MS SQL Server are much more common than MySQL for medium and large offerings, too.  Hell, I think we had more apps use Informix more than MySQL.  Personally, as a Systems Analyst/DBA, I am pretty biased against MySQL.  I perceive it as easy to develop for primarily because it hasn't historically been a well-behaved RDBMS.  No RDBMS is perfect, by MySQL has historically had problems with data integrity.  My experience with MySQL 4.0 and earlier left me with the impression that the more important my data, the less interested I should be in MySQL.  Even now that many of the issues are fixed, I'd choose PostgreSQL over MySQL any day.  That said, there is a much steeper learning curve for PostgreSQL and MS SQL Server than there is for MySQL.  MySQL is very simple for an RDBMS.  PostgreSQL and MS SQL Server are fairly robust.  You'll likely need to spend some time learning how the RDBMS works to get a really good app going."
SQL,3dnmoa,TNReb,1 point,Fri Jul 17 21:43:19 2015 UTC,"I agree with most of this post. I would choose SQL Server of the two you mentioned. I've grown to appreciate Oracle myself.   As a DBA for a medical software development company, I am curious what you are working on."
SQL,3dnmoa,da_chicken,0,Sat Jul 18 03:56:38 2015 UTC,"Currently I'm in K-12 public education as an Analyst/DBA.  I one of the guys who runs the student information system, finance system, and other systems for a largish school district (~8,000 students).  I left healthcare IT about 7-8 years ago when I moved home to help take care of family.  I was more of a pure Analyst in healthcare, mostly writing reports and supporting apps from the technical side (at least half the group was former nurses).  I also did the workstation patch management because the desktop services team couldn't do it.  I worked at a larger hospital with ~400 beds.  I only worked there about a year and a half, though.  Just long enough to get the culture and everything figured out.  Oh, and to not spell it ""HIPPA"" anymore.  At the time we were still doing endless meetings about HIPPA compliance and auditing, and this post made me think of it.  Up until a couple years ago, I was trying to get back into healthcare IT so I was paying attention to it.  Now I'm kind of stuck where I'm at, and I don't pay much attention.  I can't imagine the culture has shifted that much, however.  In between I did some dev and sysadmin work at an engineering firm on their in-house ERP which ran on MySQL 4.0.  That's where I learned to be a lot more skeptical about MySQL, but 4.0 was very stupid.  It would silently truncate or modify data, allow invalid dates like February 30th, and just generally make itself known as a nuisance.  It was, however, very easy to use and manage... at least by appearance because it often just didn't tell you when there was a problem."
SQL,3dnmoa,davik2001,2,Sat Jul 18 11:37:53 2015 UTC,Although I would reco ms sql just like everyone else I would keep away for azure because a) Pesonally identifiable medical information is nothing to fuck with and why risk a potential breach (also CDAs may need to be established between Microsoft and the company you work for) and b). Since your user limit is low - your stand alone licensing cost will be pretty low too.  Lastly - I suggest consulting with Microsoft that your student license keeps you compliant.  MS licensing is very thick and is worth a double check.
SQL,3dnmoa,rvdginste,2,Sat Jul 18 01:39:09 2015 UTC,"If you go for Microsoft SQL server, then take into account you'll not be able to use Dreamspark forever, and if you create a product and sell it, who will have to pay for the SQL server license? Depending on which version you need, SQL server is not cheap.  Just checked: http://www.microsoft.com/en-us/server-cloud/products/sql-server/purchasing.aspx According to that price, you pay minimum 7434usd for a standard edition.  I would check on PostgreSQL and on Firebird. Both are open source and very mature and capable database servers. Firebird can also be embedded in applications.  If I understand it right, you will store big/huge files inside that database (xrays). I think you should definitely check out the support and performance with that kind of data in the database servers you want to chose from. That should be a major point in your decision.  I suppose you will use an ORM for your application. If you do it right, it should definitely be doable to switch to another database at some later point. Of course, it's possible you won't be able to use an ORM for everything if you want to take advantage of special support for big files in those database servers.  Personally, I'm not a fan of MySQL and would never use it. And the suggestion of using SQLite is... well, no comment."
SQL,3dnmoa,rvdginste,1 point,Sat Jul 18 09:07:20 2015 UTC,"That's the biggest porblem with Microsoft because to sell it, I'd need a commercial license which is very expense. The question is could I use the others and still be HIPPA compliant. I'll look up PostgreSQL and Firebird and read into them. As for file sizes and performance I will definitely look into that on the solution I go with as a backbone. Thank you for your help!"
SQL,3dnmoa,trotsky90,1 point,Sat Jul 18 16:48:50 2015 UTC,"I'm not from the US, but from what I can see online, I think you will probably have to have your complete software setup audited for HIPAA compliance by a certified, independent auditor. I don't think that is gonna be cheap.  There seem to be some technical guidelines available online for HIPAA compliance (http://www.hhs.gov/ocr/privacy/hipaa/understanding/srsummary.html): * access controls * audit controls * integrity controls * transmission security  In the software I work on, I also deal with a lot of personal information. We always implement access control (who can view the data), audit control (who changed what when), and transmission security (data between services is encrypted). (Software is also only available on-site.) Implementing those is not really difficult, but it's best to take those things into account right from the beginning. The only thing we don't have right now, is integrity control, but I assume that could be fullfilled by using some kind of checksum so that one can verify whether someone tampered with the data."
SQL,3dnmoa,BowserKoopa,1 point,Sun Jul 19 08:25:14 2015 UTC,This is going to get me a kicking here.   Have you thought about Access?  The front end/custom forms are easy to set up (little coding needed) and you can put the back end onto MS-SQL and use pass through queries later.
SQL,3dnmoa,readitour,1 point,Sat Jul 18 08:38:25 2015 UTC,Postgres
SQL,3dnmoa,el_chief,1 point,Sat Jul 18 16:40:08 2015 UTC,"Use Postgresql! It's the best database lang. Honestly, they're all good. But pg is the best."
SQL,3dnnyi,Sebastian69,3,Fri Jul 17 19:16:50 2015 UTC,SELECT project.details    FROM project  INNER   JOIN employees AS teamleader     ON teamleader.id = project.teamleaderid    AND teamleader.specialty = 'database'    WHERE YEAR(project.startdate) = 2014    that's as much of this homework question as i care to provide  the at-least-2-team-members wrinkle would depend on how normalized the tables are
SQL,3dnnyi,r3pr0b8,2,Fri Jul 17 19:48:52 2015 UTC,"Well, I can't test anything because you haven't given names of tables and fields, but you could try something like this:  Select * from ProjectDetailsTable Join TeamLeaderTable  on ProjectDetailsTable.ProjectID = TeamLeaderTable.ProjectID Join TeamMemberTable on ProjectDetailsTable.ProjectID = TeamMemberTable.ProjectID where ProjectDetailsTable.StartDate between '01/01/2014' and '12/31/2014' and TeamLeaderTable.Specialty = 'Database' And count(TeamMemberTable.Specialty) > 1"
SQL,3dnnyi,Craig,2,Fri Jul 17 19:47:07 2015 UTC,"These are the tables that would be required I believe  Project (TABLE),          PNo           EstCost          CompDate          LeadCId          PDesc          StartDate  Consultant (TABLE),          CId          Specialty          CName          HireDate          Salary  ProjectTeam (TABLE),          PNo          CId          EstHrs  Specialty (TABLE),         Specialty         BillingRate  Assigned_Task (TABLE),          PNo          CId          TaskNo          HoursBilled"
SQL,3dm2og,pddonnelly,4,Fri Jul 17 11:04:36 2015 UTC,"select trunc(to_date ('2015-07-16 12:12:04', 'YYYY-MM-DD HH24:MI:SS')) + 3/24 from dual;  This converts  the string to a date, then truncates the time portion, then adds three hours."
SQL,3dm2og,Tharagleb,1 point,Fri Jul 17 12:26:37 2015 UTC,"Beautiful!!! works like a charm!  select to_char(trunc(to_date (minexec, 'YYYY-MM-DD""T""HH24:MI:SS""Z""')) + 3/24,'YYYY-MM-DD HH24:MI:SS') from Dual;   I had to do some tweaking on format, but it seems to work. You're a king amongst men."
SQL,3dm2og,follier,2,Fri Jul 17 12:57:02 2015 UTC,"Take it one step at a time, friend.  These things can get hairy.  So first thing, timestamp format is easier to add/subtract time from than date format.  So that'll save you some time :)  I don't understand what the T and Z is there, and it certainly is a pain since you need to play around with the quotation marks.  Lucky for you, I've done this before!  Try it in dual first:  select to_timestamp('2015-07-16T12:12:04Z','YYYY-MM-DD""T""HH24:MI:SS""Z""') from dual;   2015-07-16 15:12:04.0  Success.  Now let's add three hours.    select to_timestamp('2015-07-16T12:12:04Z','YYYY-MM-DD""T""HH24:MI:SS""Z""') + interval '3' hour from dual;     2015-07-16 15:12:04.0  Yay.  But now we're confused, because you need it back in that same format?  You said you need it as a date, but specific formats only make sense as a string.  So I'm going to assume that we're turning it back into a varchar.  For that you'll need to_char.    select to_char(to_timestamp('2015-07-16T12:12:04Z',                 'YYYY-MM-DD""T""HH24:MI:SS""Z""')                 + interval '3' hour,                'YYYY-MM-DD""T""HH24:MI:SS""Z""'               )  from dual;   2015-07-16T15:12:04Z  Hoo boy, did that get ugly.  But it works.  Now just use that syntax with your table/column.  select to_char(to_timestamp(            MINEXEC,'YYYY-MM-DD""T""HH24:MI:SS""Z""')             + interval '3' hour,            'YYYY-MM-DD""T""HH24:MI:SS""Z""'            )  from YOUR_TABLE;     Edit:  But you don't want it in 24 hour format, and you do want it truncated to the day before you add three hours, so:  select to_char(trunc(to_timestamp('2015-07-16T12:12:04Z',             'YYYY-MM-DD""T""HH:MI:SS""Z""'))             + interval '3' hour,            'YYYY-MM-DD""T""HH:MI:SS""Z""'           )  from dual   2015-07-16T03:00:00Z    edit:  Never mind any of that - it will always be 0300.  I'm dumb!  select substr(MINEXEC,0,11) || '03:00:00Z' from table;  If you must convert it to a date:  select to_date(substr(MINEXEC,0,11) || '03:00:00Z','YYYY-MM-DD""T""HH24:MI:SS""Z""') from table;"
SQL,3dm2og,BossHogg88,2,Fri Jul 17 12:28:11 2015 UTC,Thats a huge help! Thanks a million; makes so much sense. One small element that's missing.   I'm trying to get the date + 3am. The code above seems to return the date + time + 3hrs i.e. 2015-07-16T17:22:00Z. Would it be possible to leverage the code above to return that?
