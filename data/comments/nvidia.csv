nvidia,3e31c1,Zintoatree,8,Tue Jul 21 15:59:59 2015 UTC,Nice work putting this together.  It's really interesting to see how much value you can get out of the 290 around that $185-200 price point.  What were your temps like for the 290 when you overclocked it? I've seen some get 90+.
nvidia,3e31c1,cousineddie58,4,Tue Jul 21 16:16:59 2015 UTC,The 290 really is a good value if you can grab one used. Around 85 on the Core and 100ish on the VRM. I had to have my fans running full blast while overclocked that much.
nvidia,3e31c1,cousineddie58,4,Tue Jul 21 16:20:06 2015 UTC,Not too bad all things considered. I'd be interesting to see how it does with a nice aftermarket cooler or waterblock.
nvidia,3e31c1,Probate_Judge,3,Tue Jul 21 16:36:45 2015 UTC,The Tri-X cooler is pretty good. I think you would have to go water cooling.  It works perfectly fine and might even be considered a bonus in winter times.
nvidia,3e31c1,Probate_Judge,5,Tue Jul 21 16:39:40 2015 UTC,"I know this really isn't the best test seeing as Pcars and WoW lean quite a bit to the Green team, but I was curious and figured someone might want to see it.   A lot of people don't admit that when it lies in their favor.  A couple of runaway outliers really throw off an average and can be misleading.  This is part of what is known as ""lying with statistics"" when people insist that such an all-encompassing number would be a good rule that is applicable to everyone.  Very large props for including a disclaimer with that piece of data.  A lot of reviewers do not and why I call a lot of them bias.  You often do not see that kind of awareness on reddit hardware subs.   You can find the 290 for around $200 while the 970 is usually around $50+ more.   At new prices you can get 290/290x cheaper than 970's as well, though the prices have spiked some since the 3xx release, though the 2xx are seeming to come back down and go on sale more often.  As even nvidia leaning reviewers state, the 970 is NOT budging in price at all aside from small short lived merchant level sales.  I got a new xfx 290x DD black edition for $305 recently on amazon(I had gift cards that brought it down to ~240), and newegg commonly has sales placing similar cards(Sapphire) in the ~$275 range. As where all the better 970's tend to ride mid-300(all USD pricing) which is why even the 390 is attractive to many.  [the 2xx xfx isn't really that bad of a cooler, the fans by default are just set incredibly low, something like 45% max]  Anyways...I was going to say, that is an awfully cheap 970!, had to have been a helluva sale....and only later saw it was bstock, because of course I skipped straight to the benchmarks!  : P   Still, whatever works.  I don't read much of nvidia(a lot of tech support stuff it seems, a lul in the new-release hype and tech-news that I'm interested in) but this popped up on my front page, and it was an interesting read.  Wish I could upvote more : )"
nvidia,3e31c1,Probate_Judge,1 point,Tue Jul 21 18:39:52 2015 UTC,"I was originally planning on testing out Far Cry 4 but I ended up changing my mind because I was getting a little burnt out with all the benchmarks. If I would have I might have left the disclaimer out because it would more than likely have evened it up.   When it comes to what card to choose, it is kind of tough to answer. When I bought my 290 I got it for 239 after MIR. A week after that it was $30-$40 more expensive. The prices shift so much in the world of PC parts, that it basically comes down to grabbing the lowest priced part, at that time, that will work for your intended uses. Some people only see Red and Green so their choices gets narrowed drastically and they might be losing out.  One of the main reason's I wanted to do this comparison was because I didn't see very many reviews that pitted Aftermarket cards vs other aftermarket cards, let alone OC'ed vs OC'ed. But after testing I realized that both cards are fantastic cards and both have strengths the other one does not. I would recommend either one to someone looking, it all would depend on price."
nvidia,3e31c1,Emophia,2,Tue Jul 21 18:57:15 2015 UTC,"Aftermarket cards vs other aftermarket cards   Heh, I mentioned that in the /amd thread, lol. That's where I spend more time, with the newer card as well as my recent purchase(as well as something I've typed below, you'll see why it's a better fit).  They sometimes don't outright say it's a reference card(s), they just say ""GTX 970""  so was it reference, or was it MSI or some other?  I saw one review for the AMD 3xx cards where that was the descriptor, something like ""Radeon 390x"" in the benchmark lists, but there wasn't a reference card, and the MSI 390x one was the one being reviewed and had its own separate entry in the bar chart....WTF?  I find more and more problems with ""professional"" reviewers that piss me off any more.  Lack of transparency / adequate information for the reader is certainly one of them.  When it comes to what card to choose, it is kind of tough to answer.  Not for me, but I may be in the wrong sub to say something about Nvidia's business practices, I don't want to pick a fight. : P  When giving advice to others though, I tell people to not listen to others.  More often than not that leaves the buyer sitting around going, ""Why did I listen to that idiot?""  I'm all for people obtaining to as much information and making the best decision for themselves instead of just going on the direction of someone else.  That's why people like you who do a good run of benchmarks are great.  It's those individual game performances that should influence choices the most after what the buyer can afford.  Who cares how Pcars performs if you're never going to play it?  That is why averages are bad things to base decisions off of after all.  Anyhow, thanks again, even though I don't need it, it is good to know and something to point to for people who want that info.  I'll be saving the thread for that purpose."
nvidia,3e4b0r,TERAFLOPPER,13,Tue Jul 21 21:13:05 2015 UTC,wccftech
nvidia,3e4b0r,computeBuild,1 point,Tue Jul 21 22:50:05 2015 UTC,I don't know how that site is still considered a reliable source of info for distant products.
nvidia,3e4b0r,seta8967,3,Tue Jul 21 23:26:22 2015 UTC,All these big numbers and words sound cool to have.  Can someone eli5 this ? Is this big ?
nvidia,3e4b0r,SzyjeCzapki,11,Tue Jul 21 21:51:08 2015 UTC,It will have better performance than your current card
nvidia,3e4b0r,Fxck,1 point,Tue Jul 21 21:53:55 2015 UTC,"A lot better ?   I got a 980ti, will it be big enough to sell it and get a flagship Pascal ? Everyone is saying that Pascal is going to be huge."
nvidia,3e4b0r,SzyjeCzapki,1 point,Tue Jul 21 23:00:48 2015 UTC,i thought the next gpus would be maxwell's tok?
nvidia,3e4b0r,aleschusta,1 point,Wed Jul 22 02:58:50 2015 UTC,This isn't coming till Q2-16 or so.  God I'm sick of this 3 years old 28nm garbage. When are we finally getting proper new GPUs?
nvidia,3e4b0r,58592825866,-7,Tue Jul 21 21:39:23 2015 UTC,Not to mention the prices are way inflated for the performance they offer.  Can't even get 60fps in modern games for 700$ and I'm supposed to be impressed.
nvidia,3e4b0r,arikv2,6,Tue Jul 21 22:21:21 2015 UTC,Lolwut.
nvidia,3e4b0r,Trrixx,4,Wed Jul 22 00:45:41 2015 UTC,60fps At 4K? Maybe. At 1440p and lower? definitely Yes.
nvidia,3e5ad3,SzyjeCzapki,4,Wed Jul 22 01:48:08 2015 UTC,"That old powerpoint slide that claimed Pascal had ""10x the performance"" of Maxwell was referring exclusively to the HBM 2.0 memory. Pascal will (supposedly) use HBM 2.0 instead of the current GDDR5 standard, which is capable of ""up to"" 10x faster data transfer speeds.   Thing is, memory is far from the only thing that matters in cards. Sure it matters, but as we've seen with the Fury X, a card needs more than fast memory to be leaps and bounds ahead of its competition. As for the size of the chip, the smaller the die size and the individual transistors are, the less power required. A 14nm chip would require less power draw than a 20nm for example, which leads to lower temperatures and better efficiency. Who knows if Pascal will be 14nm though...manufacturers have been having trouble getting defect-free chips at that size.   Basically, faster memory is nice, but unless the actual chip is a miracle chip, it'll be just another ~15-25% performance upgrade cycle. Until we have more info, it's all speculation anyway."
nvidia,3e5ad3,Reddit_Is_So_Bad,1 point,Wed Jul 22 02:57:17 2015 UTC,"So it's been rumored above that Nvidia will tap out its chips on the 16nm technology, which, while bigger, is a significant step up from the 28nm tech we see in Maxwell. Practically speaking, this should sum it up well.    TSMC’s 16FF+ (FinFET Plus) technology can provide above 65 percent higher speed, around 2 times the density, or 70 percent less power than its 28HPM technology. Comparing with 20SoC technology, 16FF+ provides extra 40% higher speed and 60% power saving.    So, basically, it's a whole lot faster, and uses less power (less HEAT!!!) to do the same amount of calculations as you would need on a 28nm chip. And since it's double the density, basically, you can get double the power/performance for the same size chip and same heat/power output."
nvidia,3e2s6m,Polypropylen,9,Tue Jul 21 14:52:11 2015 UTC,Parvum if im not mistaken   http://www.parvumsystems.com/
nvidia,3e2s6m,baskinmygreatness,2,Tue Jul 21 15:10:05 2015 UTC,This is correct.  Obviously the OP is a modded case.
nvidia,3e2s6m,kendirect,1 point,Tue Jul 21 19:32:20 2015 UTC,I take it they're not available in the US?
nvidia,3e2s6m,xanthurus,1 point,Wed Jul 22 03:27:34 2015 UTC,Performance pcs has them. I think their main site also ship but I'm not sure   http://www.parvumsystems.com/where-to-buy
nvidia,3e2s6m,baskinmygreatness,1 point,Wed Jul 22 03:37:52 2015 UTC,I hoped it'd be a regular ATX case :(
nvidia,3e2s6m,SirDPP,1 point,Tue Jul 21 20:32:36 2015 UTC,Out of curiosity when was this posted?
nvidia,3e2s6m,Atrocitusrex,1 point,Wed Jul 22 00:59:03 2015 UTC,Parvum. I'm planning on building a LAN gaming rig with them
nvidia,3e2s6m,crj3012,1 point,Wed Jul 22 00:56:26 2015 UTC,Looks similar to a modded Corsair Air 240.
nvidia,3e5664,GravyMix,1 point,Wed Jul 22 01:13:30 2015 UTC,As someone who switches audio devices a lot I have to load up GFE to turn shadowplay off and on again for it to change audio inputs. Being able to turn shadowplay on/off just from the taskbar would make things a lot quicker.
nvidia,3e4z66,85218523,2,Wed Jul 22 00:16:00 2015 UTC,They weren't meant to be a product line. It was an option for early adopters to try G-Sync before it properly hit the market.
nvidia,3e4z66,fredwilsonn,2,Wed Jul 22 00:30:48 2015 UTC,"Yeah, any monitors that support it, come with it now."
nvidia,3e5406,Echelonaz,1 point,Wed Jul 22 00:55:35 2015 UTC,"have you changed your BIOS settings, back them up and maybe try a reset?  Have you messed with Nvidia CP power management settings? Or maybe even Windows Power Management settings. Not sure just some stuff you can look at. GL man."
nvidia,3e3cnr,rB0rlax,3,Tue Jul 21 17:17:43 2015 UTC,I want to hijack your thread with a similar question:   How much performance do I gain if I add a GTX 960 just for PhysX to my rig? I currently have a 980 and a Xeon e3 1230v3....
nvidia,3e3cnr,Polypropylen,2,Tue Jul 21 18:01:47 2015 UTC,Very little if any. PhysX cards were good a few generations ago. Your 980 can handle it all without choking and the 960 will be of little benefit.
nvidia,3e3cnr,NCEMTP,1 point,Tue Jul 21 19:37:18 2015 UTC,I would love to know this as well.
nvidia,3e3cnr,Neumayer23,1 point,Tue Jul 21 18:08:59 2015 UTC,"Didn't try yet with my current card, but with a last gen card (780ti) I found myself wishing for a physx dedicated card in every game that made use of it. I'll tell you my experience with the ones I've tested:  -Batman Arkham asylum: Everytime I walked into paper stacks or objects on the ground, the game would often stutter for a split second before going back to 60 fps. The scarecrow sequences that used physx more heavily, brought the FPS down to single digit, in a 2009 game ran on what was the top single gpu card at the time.  -Arkham Origins: Fps drop from 60 when there was lots of smoke, the worst offender though was the deadshot fight, FPS was stable in the low 20ies there.  -Assassin's creed 4: No, just no.  So at least until last gen of nvidia cards, a dedicated physx card would have provided significant benefits."
nvidia,3e3cnr,miningdroid,3,Tue Jul 21 20:54:51 2015 UTC,"Ok as I'm an inpatient man I ended up contacting nVidia's chat support about it, and got fantastic support, thanks Sonal!  TL:DR    Yes it works as long as both cards are supported by the same drivers.  It won't drain performance form the main card. The secondary card will be able to help with PhysX, if set to do so, and thus might increase performance of the main card.   Here is the answers I got, for anyone that is interested:  Name changed for obvious reasons  Sonal: Hi, my name is Sonal. How may I help you?   rB0rlax: Hi   Sonal: Hello rB0rlax   rB0rlax: I currently use a GTX 980 TI and wonder if I would be able to use it together with an older nvidia card so I can use more than 4 monitors? Not using SLI obviously.   rB0rlax: I have a GTX 260 laying around and if that one is to old? I only intend to use the extra monitors for non-3d stuff   rB0rlax: I really need some extra space but don't feel like getting another 980 TI just for adding more monitors, that won't be used for games anyway.  Sonal: Please give me a moment, let me check   rB0rlax: Thank you   Sonal: Thank you for your patience rB0rlax   Sonal: GTX 260 is an older card as compared to GTX 980ti. In order to use both the cards in the same PC, you need to have a driver which is supported for both the cards, the latest driver for GTX 260 is 341.44 version which is not supported for GTX 980ti. So unfortunately, both the cards cannot work together.   Sonal: You can get any other midrange card like GT 620, Gt 720 etc in order to run two more monitors and you can use it along with GTX 980ti   rB0rlax: I see. I thought that might be the case   rB0rlax: How long will a 600-series card be compatible with latest drivers for a 980 TI? So I don't end up with not being able to update drivers later on because the older card gets dropped   Sonal: The 600-700 series is a newer series. The GTX 980ti can be used along with the cards from 400 series and above. So its good if you get 700 series because its a new series , it will be lasting longer.   rB0rlax: Ok, is there any roadmap for when support for the different series will be dropped? Just so I have an idea of how long they will both work with the latest drivers   Sonal: Unfortunately we do not have any such roadmap. It depends on the manufacturers what card they will release , but I am sure, 700 series is going to last longer as its a new series   rB0rlax: ok thank you!   Sonal: No problem rB0rlax   rB0rlax: So there is no performance loss for the main card (the 980 TI) when using together with an older card? It will run as it did without it?   rB0rlax: just want to make sure so I won't take performance from the 980 TI   Sonal: It depends which monitor is connected to which GPU. If the monitor that you will be gaming on is connected to GTX 980ti, the performance will remain intact   rB0rlax: great! thank you   Sonal: :)   rB0rlax: One last question if you don't mind? Let's say I get a 750 as secondary card, would the 750 be able to give a significant performance gain if I let it help the main card with PhysX?   Sonal: Certainly rB0rlax, if you set the other GPU for physx, the load from GTX 980 ti will get transferred to the physx GPU and the performance will get enhanced.   rB0rlax: Great :) I'm surprised so few have written about this online. I could hardly find any information at all when searching around.   rB0rlax: Thank you for everything Sonal! You've been really helpful :)   Sonal: My pleasure rB0rlax :)   Sonal: Is there anything else I can help you with today?   rB0rlax: If there is any way to rate the support, please let me know and I'll give a great rating   rB0rlax: Nothing else, thank you!   Sonal: Thank you so much :)"
nvidia,3e3h5k,Ianborg,4,Tue Jul 21 17:48:08 2015 UTC,"Yes. I bought a reference card when they came out. I don't at all regret my decision.  I bought NZXT Kraken and a couple packs of copper heatsinks for the vram. I had a H100 (or something) cooler that was compatible.  I've got my card OC'd +250/+500 memory. I hit 30-32 degrees idle and about 55-60 under load.  That's down from 40-50 idle and 78-85 under load. Totally worth the extra investment, and installation was a breeze.  p.s.  - sorry--I'm not a EVGA GTX 980ti hybrid owner, but I figured you and I could still relate."
nvidia,3e3h5k,schoff,2,Tue Jul 21 18:40:36 2015 UTC,"Did you install the copper heatsinks at the top or bottom? Directly on the vram? Could you possibly link the heatsinks you used? I really want to keep my 980 Ti's cool, sorry xD"
nvidia,3e3h5k,Kirb-,1 point,Tue Jul 21 18:50:04 2015 UTC,"These are the heatsinks I have. On the front  side of the card (where the processor is) you'll see the vram. Apply a dab of your favorite thermal compound on each, and stick on those heatsinks.   You need to do it before you do install the AIO cooler.  I would get 2-3 packages: http://www.amazon.com/gp/product/B00637X42A?psc=1&redirect=true&ref_=oh_aui_detailpage_o05_s00"
nvidia,3e3h5k,schoff,1 point,Tue Jul 21 21:31:04 2015 UTC,"Sweet I have these, my main problem being... Will it stick?"
nvidia,3e3h5k,Kirb-,1 point,Tue Jul 21 21:33:32 2015 UTC,"Oh, one more question. Do you have a fan pointed to the heatsinks or such? Because everytime I read someone doing something like this, they said they had a fan for it as well."
nvidia,3e3h5k,Kirb-,1 point,Tue Jul 21 22:47:11 2015 UTC,"Feel free to check out my imgur:  http://imgur.com/a/VQjNi  It will give you an idea of what to expect.. I'm running this on a first-gen i7-970 processor (3.2ghz x6 cores), so my 3DMark (overall) score is a little low.. Here is my latest 3DMark:  http://www.3dmark.com/fs/5466297  After some significant overclocking and modification in my case (it now exhausts through the back of the PC using 'warm' air from my CPU) I top out at 58 degrees C.  The largest advantage is that it is whisper-quiet and you don't really have to worry about overheating causing any sort of throttling any longer (though the power % is still limited to 110%)"
nvidia,3e3h5k,dragoonjefy,1 point,Tue Jul 21 19:31:46 2015 UTC,"When I installed my hybrid cooler on my 980 TI, I got an amazing temperature drop! My idle (144 hz display) temps were 60'c with the reference cooler, but now sit at about 35'c. My load temps went from 84'c with some slight overclocking to 54'c with as high overclocking as my card could take before I got crashes / artifacting.   My only complaint about the kit is the radiator fan that came with it is a little bit loud and is a 2 pin fan, so its just always blowing. It's honestly not even that loud, I'm just a quiet freak. To fix this, I ordered a Noctua static pressure fan and I'm going to connect it to my mobo, and just set up a custom fan curve there. I'll let you know if that does the trick if you're interested.   Overall, I'd say if you are down to learn how to take apart your card and get your hands dirty, it is a very rewarding and interesting process with great results."
nvidia,3e2rfe,NightmareP69,4,Tue Jul 21 14:46:22 2015 UTC,"353.49 (latest) is worth trying as it resolved problems for a lot of people - but it still kills my OC capability.  The GTA V release driver (not sure on version) is still decent, although not great if you're planning on playing TW3."
nvidia,3e2rfe,I_Will_be_Nice,1 point,Tue Jul 21 14:48:26 2015 UTC,"Oh i didn't know there was a hotfix driver update, I'll get it once Witcher 3 is done Dling for me on Steam. Thanks for the info."
nvidia,3e2rfe,lolygagging,3,Tue Jul 21 14:50:50 2015 UTC,I use 347.88 because of Witcher 3
nvidia,3e2rfe,sluflyer06,1 point,Tue Jul 21 15:07:21 2015 UTC,what advantage is that? I use 353.38 for Witcher 3 and its amazing.
nvidia,3e2rfe,lolygagging,2,Tue Jul 21 16:29:12 2015 UTC,the advantage of no crashes? The hotfix driver gaveme even worse ones.
nvidia,3e2rfe,sluflyer06,1 point,Tue Jul 21 18:16:52 2015 UTC,"ah ok. I'm still lost on the ""best"" driver right now, usually its just the newest :(.  I've yet to get a crash in any game since I had the 980Ti's but I think I've been on this driver the entire time."
nvidia,3e2rfe,Ur_favourite_psycho,1 point,Tue Jul 21 19:14:25 2015 UTC,I've updated mine and it's killed my pc
nvidia,3e2rfe,wanderjahr,3,Tue Jul 21 19:20:13 2015 UTC,"I have a 770 and 353.06 has been the most stable / best performance for me on TW3. 353.30 would only be better if you ran sli. The hotfix drivers are mostly for maxwell cards.  Not matter what you go with, OC'ing is practically a no-go with TW3."
nvidia,3e2rfe,thock2,2,Tue Jul 21 15:22:12 2015 UTC,I have a 660 and 353.06 has been the best for me
nvidia,3e2rfe,xTommy2016x,2,Tue Jul 21 16:29:34 2015 UTC,Seems like any sort of OC on my 970 causes driver crashes
nvidia,3e2rfe,StayFrostyZ,1 point,Tue Jul 21 18:14:38 2015 UTC,"From mine and a few redditors experiences on this subreddit, avoid 353.38 because it massively slows down your startup apps (antivirus, etc). Luckily rolling back did the trick and fixed this issue"
nvidia,3e2rfe,sluflyer06,2,Tue Jul 21 14:52:27 2015 UTC,353.38 has been best for me.
nvidia,3e2rfe,wanderjahr,0,Tue Jul 21 16:29:49 2015 UTC,Because you have a Maxwell card. Kepler cards didn't really need the hotfix.
nvidia,3e2rfe,xk4l1br3,1 point,Tue Jul 21 17:15:44 2015 UTC,"Installed 353.06, seems to be fine so far.  Running Witcher 3 with everything set to high, except for Shadow Quality and Foliage View Distance, those two are set to medium.  Ambient Occlusion set to SSAO, all types of blur turned off, no hairworks, no AA, Bloom ON, Light Shafts on and running in 1920 x1080.  Frame rate on average is 35-40, which surprisingly feels okay. Usually most games when they go under 60 feel terrible but Witcher 3 still feels good for me even at that FPS. Max FPS is 60(locked) and the lowest it ever got so far is 30.  GPU temperatures are fine, they're at 80c with 99% of the GPU being used, i know some of you would get a heart attack if they saw a temperature like that but my GPU has a regular reference design to it, so just one fan and unimpressive looking heatsink (it's from Gainward), it's also like 36-40c (95-104f) here all though i do have my air conditioner on which is helping a lot.  My CPU's temperature are fantastic 35-45c while playing, yes i know it's not related to a GPU's Driver but damn, i was impressed by the low temps i have on it ever since i finally installed an EVO 212 cooler on it and dumped by old stock cooler which would hit 50-60c while under heavy load.  In in all, happy so far."
nvidia,3e2rfe,Syn246,1 point,Tue Jul 21 17:41:02 2015 UTC,353.49 Hot Fix works perfectly fine for me My MSI 980Ti 6G overclocks okay as well.
nvidia,3e4s7w,reyllo,1 point,Tue Jul 21 23:19:46 2015 UTC,To be honest man they're all the damn same... At the end of the day a 980ti is a 980ti.. Get whatever you can get your hands on.. I have the SC.. Great card
nvidia,3e4s7w,Escopy,2,Wed Jul 22 00:30:22 2015 UTC,"Yup pretty much this. No matter which brand you go with, you're gonna end up with a beast of a card.  Each brand has their slight differences in OC, cooling, and aesthetics so just choose which one appeals to you. I'm sure you'll be happy with whichever one you decide on.  That being said...go for the G1 since I work for GIGAYBTE :D"
nvidia,3e4s7w,GBT_Van,1 point,Wed Jul 22 02:19:03 2015 UTC,From what I've seen the g1 is the best bet for overclocking and maybe cooling but at the cost of a bit noise at load. The SC is quieter I think. I'm not 100% sure but you really can't go wrong and after spending a few hours myself researching I went with the g1 and love it.
nvidia,3e4s7w,Zarknox,1 point,Wed Jul 22 00:51:35 2015 UTC,The choice is do you want a card with a reference PCB or a aftermarket? Do you plan on OCing? My G1's boost to 1342 with no additional OCing and default power target.  I'll be water cooling as soon as EK has a block so I wanted something with 2 8pin.  Also in stock...no need to wait. http://www.bhphotovideo.com/c/product/1160257-REG/gigabyte_gv_n98tg1_gaming_6gd_geforce_gtx_980_ti.html  http://www.newegg.com/Product/Product.aspx?Item=N82E16814125787&nm_mc=AFC-C8Junction&cm_mmc=AFC-C8Junction-_-na-_-na-_-na&cm_sp=&AID=10446076&PID=6202798&SID=
nvidia,3e4l02,itsToTheMAX,1 point,Tue Jul 21 22:24:25 2015 UTC,Did it freeze?
nvidia,3e4l02,the_unusual_suspect,1 point,Wed Jul 22 00:05:59 2015 UTC,"run a few games, do you have fan spiking issues?"
nvidia,3e4l02,hkgrx8,1 point,Wed Jul 22 00:24:04 2015 UTC,I've not noticed anything but I've only had it one night.  I'll check the graph after playing something tonight and post it if you want.
nvidia,3e4l02,hkgrx8,1 point,Wed Jul 22 00:30:24 2015 UTC,A lot of us have been having issues. I've contacted Zotac support and they are bringing out a new VBIOS this week.
nvidia,3e3j95,sven784230,2,Tue Jul 21 18:02:14 2015 UTC,Normally it should be like 50 at most since the fans turn off. Is your ambient really hot or bad airflow in the case?
nvidia,3e3j95,hdshatter,1 point,Tue Jul 21 18:08:18 2015 UTC,"Yes, ambient temp is pretty high right now (it has 34 degrees outside......)"
nvidia,3e3j95,scarecrowman175,2,Tue Jul 21 18:15:26 2015 UTC,Do you have any sort of AC in your room? If it's the same temperature outside as it is in your room then that'll of course lead to higher temperatures. What are your temps at load? Ideally you want your GPU to idle anywhere below 50c so 60c at idle is pretty warm. I had the same model 970 in my build and it idled at about 30c in my 20c room.
nvidia,3e3j95,tekwarfare,1 point,Tue Jul 21 18:27:11 2015 UTC,it goes up to ~79 degrees under load.. i have no AC sadly  I'll probably mount an intake fan on the side of my case.  Thats my OC btw: http://imgur.com/J8PIqlM
nvidia,3e3j95,scarecrowman175,1 point,Tue Jul 21 18:33:25 2015 UTC,I've disconnected all case fans and shoved my tower under my bed. My temps idle between 50 and 60°C. My load temps are 80°C tops. You can use MSI Afterburner to set the fans to turn on at 70°C if you want.
nvidia,3e3j95,scarecrowman175,1 point,Tue Jul 21 18:44:38 2015 UTC,"I'd recommend setting a custom fan profile for your 970 so the fans run earlier to try and keep temps. down a bit. I'd also recommend you get an AC, fan, or something to try to cool your room down as that'd greatly help the cooling of your PC and yourself. You're not exactly at risk of frying your GPU with those temps (I believe the 970 is safe up to 98c) but it will shorten the life span if those high temps stay consistent throughout its lifespan. Which, by the time it gets worn down by those temps it'd be 5 years from now in which case newer and better cards will be out."
nvidia,3e3j95,RainbowTurt,1 point,Tue Jul 21 18:46:10 2015 UTC,sooo... i guess there is no reason for custom fan curves?  It's pretty annoying/time consuming to always start Afterburner with Windows
nvidia,3e3j95,mrnewb2121,1 point,Tue Jul 21 18:53:24 2015 UTC,"There is a reason for custom fan curves; to make your GPU temps more stable. Most people who have cooler ambient temperatures won't ever need to use them, but people in your situation can benefit from them. As for it being annoying to start up afterburner, you can get it to start up at launch. Just add it to the Start Menu > All Programs > Startup directory if you have Win 7."
nvidia,3e3j95,mrnewb2121,1 point,Tue Jul 21 18:58:19 2015 UTC,"Out of curiosity, how is it annoying/time consuming? Afterburner can triggered to run at start-up in a minimized state."
nvidia,3e3j95,mrnewb2121,2,Wed Jul 22 03:23:12 2015 UTC,Do you have the power settings set to 'Prefer Maximum Performance' in the Global Settings?
nvidia,3e3j95,kittah,1 point,Tue Jul 21 18:13:11 2015 UTC,"uhm, no?   Never heard of that setting actually :§"
nvidia,3e3j95,kittah,2,Tue Jul 21 18:16:02 2015 UTC,i just checked it: its on adaptive
nvidia,3e3j95,Zarknox,1 point,Tue Jul 21 18:17:11 2015 UTC,Right click desktop > Nvidia control panel > Manage 3d settings > global settings tab
nvidia,3e3j95,TheSW1FT,1 point,Tue Jul 21 18:24:57 2015 UTC,should i set it to 'Prefer Maximum Performance'?
nvidia,3e3j95,danesnick,1 point,Tue Jul 21 18:21:58 2015 UTC,"na keep the global on adaptive imo. 50 seems high for idle, I normally idle around 40-35."
nvidia,3e4ez7,r0b3r71,1 point,Tue Jul 21 21:41:27 2015 UTC,Every DOA thing I've ever gotten was from Newegg. I avoid them as much as possible.
nvidia,3e4ez7,hdshatter,5,Tue Jul 21 21:49:33 2015 UTC,"They can't open up every single thing in their warehouse and test it. I 100% doubt they ask manufacturers for faulty items so they can sell them, and then overnight a replacement."
nvidia,3e4ez7,big_three,2,Tue Jul 21 22:30:57 2015 UTC,"At least their support is good. This is the only bad experience I've had with them, and I can't place any blame on them really for the gpu being defective."
nvidia,3e4ez7,Archmagnance,1 point,Tue Jul 21 22:00:41 2015 UTC,"They are just as much a middleman as amazon, I doubt they are paying for defective products on purpose"
nvidia,3e49l3,Suyneej,1 point,Tue Jul 21 21:02:55 2015 UTC,"Of those three, the MSI is the best."
nvidia,3e49l3,djisadud,1 point,Tue Jul 21 23:02:57 2015 UTC,"What makes it the best? Looking strictly at clock speeds the Strix seems the best. Are you just going off customer support, warranty and such? These things im not worried about because I have an in product replacement plan with memory express where is coil whine is too loud or the card fails I can exchange it no questions asked."
nvidia,3e49l3,djisadud,1 point,Tue Jul 21 23:08:26 2015 UTC,Temps and noise based on reviews.  The STRIX cooler is sub par on the 980 Ti.  I believe it doesn't even cover the VRAM and only has two or three of the five heat pipes touching the GPU.
nvidia,3e49l3,RainbowTurt,1 point,Tue Jul 21 23:14:49 2015 UTC,"Gotcha, thanks. Besides the msi, is there another one I should consider? I see a lot about the gigabyte g1."
nvidia,3e49l3,jtredd,1 point,Tue Jul 21 23:25:45 2015 UTC,"The G1 is a very popular card, so you are going to hear a great mix of people talking about noise level being both high and low. Coil whine is also a mixed bag, so just know no matter the option you choose - there's always the chance you get that.  That being said, I'm definitely one of the G1 owners who think the card is pretty quiet even under a 75% fan load. In idle, I can't even hear it over my normal fans, and with the 75% it blows my previous Gigabyte 290x out of the water in terms of silence.  When I was doing the research myself I saw a general consensus of the MSI being great for the gamer - offering silence and performance, while the Gigabyte was for those wanting to squeeze a little more out of their OCs (which, in the grand scheme of things is very, very small and luck-of-the-draw compared to the MSI - which already has great OC capability). Other than that, people love EVGA for their warranty services and Step-Up Program.   You'd be happy with any choice really, it's just the little perks that make the decision for many. (I also liked the custom LED on the G1 for my setup)."
nvidia,3e49l3,p3dr0maz,1 point,Wed Jul 22 03:16:27 2015 UTC,Just installed  https://imgur.com/a/iFLR1
nvidia,3e49l3,Princess709,1 point,Tue Jul 21 23:35:08 2015 UTC,All of the GM200's run hotter than the GM204 chips of the 980/970's so get whichever has the best cooling cause the boost vdrops kick in like around 46c/56c/66c/76c. Temps are the limiting factor on these for OC as the vdrop is a big hit in Mhz.
nvidia,3e49l3,omnicious,1 point,Wed Jul 22 00:01:05 2015 UTC,"I think you should consider gigabyte 980Ti, i saw the video review from Jayztwocents, and the card is absolute SILENT!!!  https://www.youtube.com/watch?v=B2-gCXscH04"
nvidia,3e49l3,Shyguythrowaway2,1 point,Wed Jul 22 00:45:41 2015 UTC,"Thing is, those vary a lot video by video. I saw another one where the g1 was very loud."
nvidia,3e2lfs,FreelancerGamer,1 point,Tue Jul 21 13:57:38 2015 UTC,"Yep, both versions of the SHIELD ATV have Shadowplay"
nvidia,3e3zei,D2Cj,1 point,Tue Jul 21 19:51:49 2015 UTC,The only way to check if its the PSU or the motherboard is to try it on a different one unless it's something like a lose cable or bad bios.
nvidia,3e3zei,hdshatter,1 point,Tue Jul 21 20:13:28 2015 UTC,You can do the paperclip test for the psu. Steps outlined here: http://support.antec.com/support/solutions/articles/1000015319-is-my-power-supply-dead-the-paperclip-test
nvidia,3e26sa,GravyMix,2,Tue Jul 21 11:32:57 2015 UTC,I believe it's leaking
nvidia,3e26sa,ElitelyGamersRay,1 point,Tue Jul 21 12:12:56 2015 UTC,This exactly has happened to me twice now. Had to end task. Both times occurred soon after starting shadowplay to record my gameplay.
nvidia,3e26sa,Buttermilkman,1 point,Tue Jul 21 12:47:18 2015 UTC,What version of GFE are you running?
nvidia,3e26sa,Trikle,1 point,Tue Jul 21 17:57:00 2015 UTC,I started having the same issue with the 2.5.11 GFE update.
nvidia,3e26sa,Razor505,1 point,Tue Jul 21 21:45:38 2015 UTC,Latest stable release. I always keep it up to date.
nvidia,3e26sa,DiorGalore,1 point,Wed Jul 22 01:14:37 2015 UTC,I have the same problem since updating to the newest version of GFE. I can't use shadowplay at all without the capture server taking up all my ram.
nvidia,3e070o,RenegadeXG,26,Mon Jul 20 23:10:36 2015 UTC,Considering how rabid the target audience for these cards are to have the highest ASIC score possible... it's fantastic marketing.   They'll be sold out in minutes.
nvidia,3e070o,Munkypoo7,3,Mon Jul 20 23:22:39 2015 UTC,Yeah i woulda opted in for higher bin too. I got a 71% asic card and it has just an avg OC ability. I might end up flipping it for $50/$75 loss to go bleeding edge with one that's higher than 80%.  Those seem to be hitting 1500+ easily.  So random though i still dont know. Im on the fence.  I've had an MSI 970 that was a 78 and it was solid and I had an EVGA 980 SC  ACX 2.0 that was a 69 and it acted just like this card but with a BIOS flash it was fine once the voltages were stabilized.  The problem with the TI's and Titan X's is even with a BIOS flash unless you get into LN2 temperatures you cant get around the vdrops and these chips dont respond to more voltage as well as the GM204 chips.
nvidia,3e070o,p3dr0maz,2,Mon Jul 20 23:29:11 2015 UTC,My 2 GTX 980's are 65 and 67 percent. Both have clocked over 1500. Hell at stock voltage I set it to +250 core and +450 on memory. Boost 2.0 took them to 1505 right out the gate. Asic score I feel doesn't have a whole lot to do with over clocking in these cards.
nvidia,3e070o,FunktasticLucky,1 point,Tue Jul 21 07:13:53 2015 UTC,"I have a 980ti SC with a low ASIC score (65 if I recall correctly) and I've managed to go up to 1450 boost OC stable with no overvolting, on a reference cooler. Soon I'll be going liquid to see how much higher I can go!"
nvidia,3e070o,juandemarco,2,Tue Jul 21 06:32:36 2015 UTC,From what I read from kingpin higher asics benefit water and air:  http://kingpincooling.com/forum/showpost.php?p=30273&postcount=57  Basically sounds like Maxwell just has it's own voltage behavior that doesn't follow usual conventions when it comes to asic scores.
nvidia,3e070o,p3dr0maz,1 point,Tue Jul 21 00:28:14 2015 UTC,"Well, since it's not even guaranteed that a high asic will deliver a good OC , this is mostly a marketting stun"
nvidia,3e070o,KoreanGirlLoL,1 point,Tue Jul 21 00:33:11 2015 UTC,"True, im willing to risk it, but idk if I'll have success reselling. We will see what the future holds."
nvidia,3e070o,p3dr0maz,1 point,Tue Jul 21 00:34:17 2015 UTC,I would be willing to wager there would be someone out there that would want it. But you would be hitting a pin head from 100 yards trying to find someone that would pay extra in the used market for it unless it's still a new to market card.
nvidia,3e070o,qazme,1 point,Tue Jul 21 00:35:24 2015 UTC,Yeah I'm talking about selling my current 71% ASIC MSI I have now.  I really don't think it could get much worse and it's not  a particularly bad card it's just not the best ofc.
nvidia,3e070o,p3dr0maz,1 point,Tue Jul 21 13:23:58 2015 UTC,"Well and to someone not 'in the know' about ASIC and what it even means. ""The quality of the card? Well wouldn't I want a 100% card or at least a high 90's on a used one? 75% means it's not gonna last much longer right?"" I see this becoming common lol."
nvidia,3e070o,qazme,1 point,Tue Jul 21 13:27:35 2015 UTC,"ASIC doesnt mean it won't last as long.  The ""grade""  it gets usually higher means the chip is ""better"".  Meaning it requires less voltage to achieve max performance and/or they have less ""power leak""  and have a lower ""thermal""  footprint which in general all adds up to higher /stabler clock speeds."
nvidia,3e070o,p3dr0maz,-6,Tue Jul 21 13:33:24 2015 UTC,"I have 2 evga 970 ssc one is 75 the other is 76, you got fucked."
nvidia,3e070o,LiberDeOpp,21,Tue Jul 21 13:42:49 2015 UTC,"I feel like this is mostly the customer's fault. People started latching onto ASIC like if they didn't have a good score they had a ""broken"" card. Just look at the low egg/star reviews people would leave for a perfectly functioning card that just didn't OC ""enough"". It's uninformed whining and outrage.  Well, now the GPU manufacturer is looking at this number instead of leaving it to the lottery, which means increased overhead and justification to enlarge their profit margins. Congratulations! Maybe this will stay within Kingpin sales, maybe within EVGA. Or maybe it'll spread like a virus to the other brands and people will finally settle for lower ASIC scores like they should've in the first place."
nvidia,3e070o,Aerundel,3,Tue Jul 21 05:42:03 2015 UTC,"I'm not even sure if this score is useful, I haven't heard much about it before.  How is it measured? What exactly does it mean? If the score system makes logical sense then a higher score means a card is better, but if that isn't true 100% of the time then the system is flawed and no one should care about it to start with."
nvidia,3e070o,Popingheads,1 point,Tue Jul 21 01:34:31 2015 UTC,If it makes them money then it will spread all over to every company.
nvidia,3e070o,RiffyDivine2,2,Tue Jul 21 12:24:50 2015 UTC,"I think that some of the other companies would first need to ""Kingpin-ize"" their offerings, also. Gigabyte would need something more premium than G1. MSI would have to stop being so aloof about whether or not a new Lightning will be introduced, etc. Some warranties will need to be altered, also.  Maybe I shouldn't even be typing that. Don't want to give them ideas. Shit..."
nvidia,3e070o,Aerundel,1 point,Tue Jul 21 13:13:22 2015 UTC,You got more faith then I do. I expect them to just slap a new name on it based just off the ASIC and charge a lot more.
nvidia,3e070o,RiffyDivine2,1 point,Tue Jul 21 13:34:08 2015 UTC,"They gotta put on that third power connector, at least =P"
nvidia,3e070o,Aerundel,1 point,Tue Jul 21 14:00:22 2015 UTC,"Why not four, let them overclock it to the moon."
nvidia,3e070o,RiffyDivine2,1 point,Tue Jul 21 14:52:46 2015 UTC,"This. As someone who hasn't been into gaming since the early 2000's, I looked at my 70% ASIC score on my SC+ and thought I've been screwed and contemplated returning it.  I've since realized how insane a thought that was."
nvidia,3e070o,big_three,9,Tue Jul 21 14:56:30 2015 UTC,"Can I pay, like, $100 for a 10% ASIC?"
nvidia,3e070o,wanderjahr,8,Tue Jul 21 22:34:58 2015 UTC,Yup. gt 910  ;)
nvidia,3e070o,solidnitrogen,1 point,Tue Jul 21 01:48:54 2015 UTC,would it even work?
nvidia,3e070o,RiffyDivine2,9,Tue Jul 21 02:01:06 2015 UTC,http://i.imgur.com/9Ozl01p.png  Taking offers on my SC+ 980ti
nvidia,3e070o,pj530i,4,Tue Jul 21 13:08:29 2015 UTC,$1300
nvidia,3e070o,Zintoatree,10,Mon Jul 20 23:55:05 2015 UTC,"$1300.00 I added 2 zeros, and can add more if you like."
nvidia,3e070o,Fearm0nger,7,Tue Jul 21 00:04:59 2015 UTC,I would go with this guy. I can't match that many zeroes.
nvidia,3e070o,Zintoatree,1 point,Tue Jul 21 01:04:10 2015 UTC,Add a zero to that and you've got a deal!
nvidia,3e070o,pj530i,9,Tue Jul 21 01:58:21 2015 UTC,$01300
nvidia,3e070o,Zintoatree,1 point,Tue Jul 21 00:24:04 2015 UTC,Bender reference?
nvidia,3e070o,slykrysis,1 point,Tue Jul 21 01:59:21 2015 UTC,What kind of OC have you gotten with that beast?!
nvidia,3e070o,TaintedSquirrel,3,Tue Jul 21 13:18:56 2015 UTC,"1520mhz in games, +450 on vram, +18mv   Nothing special really given the amount of fuss over asic score"
nvidia,3e070o,pj530i,4,Tue Jul 21 04:00:19 2015 UTC,lets hope evga is the only manufacturer  checking their asic scores
nvidia,3e070o,Derpydabs,2,Tue Jul 21 13:19:44 2015 UTC,EVGA has always checked them but this item is aimed at a very narrow group of people and not basic users. But if it takes off expect to see people like asus follow.
nvidia,3e070o,RiffyDivine2,4,Tue Jul 21 02:04:06 2015 UTC,First time hearing about ASIC -- still no clue what it is.
nvidia,3e070o,ChikNoods,4,Tue Jul 21 13:10:00 2015 UTC,"Magical go faster number, kind of like how the red ones go faster."
nvidia,3e070o,RiffyDivine2,2,Tue Jul 21 02:08:31 2015 UTC,"""DA RED WUNZ ARE FASTA!!!! BLU WUNZ ARE CULA!!!!"" -Ork    ""DA ASIC WUNZ ARE BEDDA!"" -random users."
nvidia,3e070o,Salt_Lake,3,Tue Jul 21 13:06:56 2015 UTC,"basically, there are... qualities to a card, like with a CPU and the K series CPU are higher quality, and as a result they are unlocked. you can make 1500 sheets of CPUs but only about 150 CPUs off those sheets are top tier where others might have bad cores and as a result be sold as duals or triples rather than quads, or would be slowed down and move to laptop CPUs well, the imperfections affect the quality and there for ""the binning"". This is basically, for the first time ever, assuring you get the quality/bin you want.    its less of a gamble this way, there are people who will buy 4 or mroe to get a high tier bin then sell the spares because a better bin means less imperfections and as a result you would have less heat leaking and better voltage response, so in theory, a better overclock. However, the issue is still a gamble, though less so. Now you are gambling that the few imperfections that the chip has are not in a vital location. so you know LESS imperfections, but still dont know the locations that they are located. you could have a 75+ bin, and it still have a critical imperfection that limits you past a certain point, but a 70 might have more imperfections (hence the lower score) but they might be in less vital locations and as a result that chip still could overclock better than the 75"
nvidia,3e070o,TidusJames,4,Tue Jul 21 21:02:43 2015 UTC,"Interesting, never heard of this ASIC score until now.  Apparently one of my 780s is 87.4%.  I only ever underclock though."
nvidia,3e070o,ralgha,2,Tue Jul 21 15:46:06 2015 UTC,"Still good for you, Higher ASIC means lower voltage and power consumption = Do not need to underclock as much!"
nvidia,3e070o,Sapass1,1 point,Mon Jul 20 23:37:05 2015 UTC,"GTX 780 SC ACX and I have a score of 82.8%. I overclock hard, too."
nvidia,3e070o,L33tMasta,5,Tue Jul 21 00:36:33 2015 UTC,They also charge extra if it doesn't have coil whine.
nvidia,3e070o,the_unusual_suspect,6,Tue Jul 21 07:07:40 2015 UTC,"This is a shit move by EVGA btw , now they will probably target their high ASIC cards to the most expensive chips , goodbye to your chances of getting a high ASIC on a reference evga card"
nvidia,3e070o,KoreanGirlLoL,2,Tue Jul 21 00:47:59 2015 UTC,Well I have an 83.9% asic SC+ and it didn't cost me $1050
nvidia,3e070o,pj530i,7,Tue Jul 21 00:33:28 2015 UTC,"yup, you got it before this new marketting strategy started trending"
nvidia,3e070o,KoreanGirlLoL,0,Tue Jul 21 00:38:08 2015 UTC,"That doesn't make sense. They had kingpin cards at whatever event before the launch of the 980ti so they've been building/binning them for a while. With this one your paying extra for the guarantee of a better binned chip but you can still get lucky on the siliocon lottery.  It all makes sense. The only reason you should be getting this card is for L2N, otherwise your paying $300 extra for maybe 50mhz over what a G1 or classified can do which is gonna make the card 1-2% faster over other oc'd cards. From what I can tell, the pricing for the Kingpin cards is either for A) l2N OCers who need the extra headroom because apparently Maxwell operates differently than older cards and higher binning looks like its always better or B) idiots. For both of those target markets the up charge seems reasonable."
nvidia,3e070o,BKachur,1 point,Tue Jul 21 00:43:59 2015 UTC,Sadly this may become the case and it will drive some of there customer base away from them. But it will take awhile to prove they are doing it.
nvidia,3e070o,RiffyDivine2,2,Tue Jul 21 13:05:55 2015 UTC,"I did some original research with regard to ASIC, basically, besides all the known stuff, for every ~5% higher or lower quality than about 70%, the 980 tis / titans with dynamically clock about 1 tick higher or lower usually (but no more than 2). 1 tick on this architecture is 13 mhz. Also, for all 980 tis / titans, the dynamic boost clock is 214 mhz over stock clock speed, for an ASIC of ~70%. So, if you were to buy an ASIC 72% 980ti Kingpin, your max dynamic clock will be 1203 mhz+214 mhz=1417 mhz, probably. If you were to get an 80% one, you can expect a max dynamic clock of 1203 mhz + 214 mhz + 26/39 mhz = 1443mhz/1456mhz. So, if you are okay with paying 200 bucks more for a 2-3% faster card, go for it, if not, don't waste your money. Note, this does not take into consideration things like the GPU hitting power/temp/other limits..."
nvidia,3e070o,Merpmerp1618,1 point,Tue Jul 21 13:16:40 2015 UTC,I would like to see how many extra fps this translates to in a game like Witcher 3.
nvidia,3e070o,wanderjahr,1 point,Tue Jul 21 00:54:43 2015 UTC,I kinda half ass tested this when overclocking my 980ti. Just loaded a couple save games with and without oc in witcher 3 and saw about 15 fps difference stock vs +250 mhz. So about 1 fps for every 17 mhz I guess based on my thoroughly unscientific testing.
nvidia,3e070o,kittah,1 point,Tue Jul 21 01:51:42 2015 UTC,"Interesting figures - coincides with my own experience, my 70.7% card won't go past 1417 on stock volts without crashing."
nvidia,3e070o,Orcworm,3,Tue Jul 21 02:17:50 2015 UTC,"$1050 for a card that might overclock what 5% better than you average? And thats a big might, theres more to OC than ASIC scores."
nvidia,3e070o,hdshatter,2,Tue Jul 21 02:12:12 2015 UTC,Mines 76.3 is that good?
nvidia,3e070o,Goblicon,2,Tue Jul 21 01:24:38 2015 UTC,Could be worse but at the end of the day you could out do someone with an 80+. The whole thing really is just luck with the card.
nvidia,3e070o,RiffyDivine2,2,Tue Jul 21 03:31:30 2015 UTC,"My MSI 980ti (reference) has an asics of 67, Thing can hit 1472 without overvolting. I believe there is way more to overclocking then just ASIC scores, I've had a card in the high 80's that was shit at overclocking gtx (770)."
nvidia,3e070o,drummerdude41,2,Tue Jul 21 13:06:36 2015 UTC,I never got the point of this ASIC crap. I've had cards with 80+ ASIC score that OC'ed worse than cards with 70+ ASIC.  But hell if people think a score system that no one really knows what is will make their cards better they can go pay overprice for them all they want to.
nvidia,3e070o,YinKuza,1 point,Tue Jul 21 10:58:33 2015 UTC,"ASIC isn't a direct mark of how well something will overclock, and that's the problem with people. Everyone now thinks it's the mark of how well you can push the card and like you I've had cards in the low 70's do better. The whole thing is just luck."
nvidia,3e070o,RiffyDivine2,1 point,Tue Jul 21 12:31:10 2015 UTC,The saddest part is that companies are charging money for it now.
nvidia,3e070o,YinKuza,1 point,Tue Jul 21 13:03:02 2015 UTC,Yes it is but hopefully it's just the kingpin cards which are aimed at a very small market of insane overclockers. The problem will be if people who don't understand shit start to buy them and this becomes mainstream.
nvidia,3e070o,RiffyDivine2,6,Tue Jul 21 15:31:12 2015 UTC,Not sure if I should laugh or cry about this....
nvidia,3e070o,qazme,3,Tue Jul 21 16:47:55 2015 UTC,I did a little bit of both
nvidia,3e070o,qazme,5,Mon Jul 20 23:32:57 2015 UTC,"I mean, the way I understand it anyways, behind it is lower ASIC has the same chance of overclock except you may have to put more voltage on it. To someone under water this means nadda. To someone on air it could be the difference between a stable 1500mhz and a stable 1470mhz. Whew HUGE difference. /sarcasm  Is that really worth an additional $200? This is my laugh.....the crying comes in when people start bragging about paying extra for it and how they have a 80% ASIC and talk about it like it's a big deal. This reminds me of the days when they first started introducing the SC cards.  Now there will be questions popping up - ""hey guys should I get the ASIC 80% SC or the ASIC 70% SSC? Which one will be faster they are both the same price?"".....UGH - laughable and crying at the same time. :-/"
nvidia,3e070o,p3dr0maz,3,Mon Jul 20 23:36:38 2015 UTC,I tried more volts on my maxwell TI and it didnt really make a diff. If anything it was more unstable.  Mid range voltage was the sweet spot.  What i have noticed from other forum members is that the higher ASIC's seem to be way more reliable with the power delivery and experience less voltage drops on temps than some others with a lower score (for example a higher binned GPU on the OC thread  upvolted to 1.230V vs my lower 1.224V on the same BIOS).  So more reliable power means stabler /higher OC's.  My voltage goes like this:  1.224 < 45c  1.205 < 55c  1.187 < 65c  so between those three voltages i fluctuate from 1468.6Mhz to 1455.5Mhz to 1442Mhz.  I can lock my voltage at 1.255 and ill be floating on the edge of stability at 1481Mhz. Most games it's fine but some of the more intense games it will artifact eventually.  This is on AIO water so i rarely pass 55c but before I was passing 75c on air so with the Vdrop hit being worse I experienced drops down to around 1400Mhz.
nvidia,3e070o,kittah,2,Mon Jul 20 23:47:24 2015 UTC,"Yeah, apparently Maxwell is an oddball in that it doesn't really respond well to increased voltage except at sub ambient (0C and colder) temps. In the past it was always more V = more clock with the ability to dissipate heat being the limiting factor. This is why you could always OC higher under water than on air.  However it appears with maxwell that you will typically hit your highest stable air/water clock at stock voltage. To benefit from increasing V you need phase change cooling or LN2.  Can read more about it from Kinpin's statement here: http://kingpincooling.com/forum/showpost.php?p=30273&postcount=57  My reference 980 Ti w/ ACX cooler is stable at 1480 Mhz at the stock 1.18V. It will pass firstrike at 1520Mhz and some games at 1502 but 1480Mhz is rock solid for daily use, about 100 hrs in witcher 3 with 0 crashes. This is with 75.4% asic quality."
nvidia,3e070o,p3dr0maz,2,Mon Jul 20 23:54:01 2015 UTC,Ok that read is good. thx man. gonna share it over at oc.net  That explains why my best clocks are right around 1.255V
nvidia,3e070o,RiffyDivine2,1 point,Tue Jul 21 00:17:01 2015 UTC,A bit off topic but is there any truth to leaving cards overvolted for normal use will damage them or wear out faster?
nvidia,3e070o,kittah,2,Tue Jul 21 00:25:31 2015 UTC,It is true that increasing voltage speeds up the process of electromigration but the minor amount of overvolting you can do with a stock bios is not going to have an appreciable effect.   The card will be obsolete long before this effect kills it. Pretty hard to kill a card these days with the thermal & voltage limits baked in.
nvidia,3e070o,RiffyDivine2,2,Tue Jul 21 13:25:55 2015 UTC,Thank you. I wasn't sure just how massive maxing the overvolt slider in X really was since it visually looks like a lot. I did move each power connector to it's own spot on the PSU I got so one line for each power connector. Thought it may help.
nvidia,3e070o,p3dr0maz,1 point,Tue Jul 21 17:21:45 2015 UTC,I mean yes but probably not in any significant or noticeable amount as long as your temps are fine. In general the extra voltage isn't what shortens the life it's the extra heat. As long as your temps are under control it should be negligible.
nvidia,3e070o,RiffyDivine2,1 point,Tue Jul 21 19:34:16 2015 UTC,I got my 980s on hybrid coolers so they are 45c under max load well overclocked pretty hard. But I kept being told if I leave them that overclocked the card was going to die which didn't make sense unless I blew a cap on it or something.
nvidia,3e070o,p3dr0maz,1 point,Tue Jul 21 13:31:19 2015 UTC,Overvolted  and overclocked are not the same thing  and you just used the term interchangeably.  Are you OV or OC'd or both?  Either way if your temps are that low it really shouldn't matter IMO.
nvidia,3e070o,RiffyDivine2,1 point,Tue Jul 21 14:09:25 2015 UTC,"Both and yes I did and didn't catch myself, sorry. I overclocked the card and was impressed before it began to crash if I pushed it more and was told to overvolt the card to fix that which it did. Currently when I play a game that can be demanding like the witcher 3 at max I will both overclock and overvolt the card for three or four hours at a time."
nvidia,3e070o,p3dr0maz,1 point,Tue Jul 21 14:14:31 2015 UTC,"Yeah Witcher 3 if you read around is kinda a pita.  Lot of games will work at certain clocks but Witcher 3, Project Cars,  metro: LL,  and Titan fall  are some of the more demanding games that might give you issues even though you think you're stable."
nvidia,3e070o,RiffyDivine2,1 point,Tue Jul 21 14:36:48 2015 UTC,Remember that this is aimed at a small market of people who will piss money up the wall for that number. I'd be more worried if clueless fucks start to buy them because big number better and then we get a trend going.
nvidia,3e070o,qazme,1 point,Tue Jul 21 16:53:53 2015 UTC,Yeah same here. o.0
nvidia,3e070o,RiffyDivine2,1 point,Tue Jul 21 13:24:43 2015 UTC,Which sadly I can see happening with the upswing of PC gamers who have zero understanding of the tech they are using.
nvidia,3e070o,kendirect,2,Tue Jul 21 13:31:40 2015 UTC,"Seems like we got hit by the downvote fairy for w/e reason, so I'll +1 your comment =)"
nvidia,3e070o,joshruffdotcom,1 point,Tue Jul 21 14:01:00 2015 UTC,I don't frown on this at all..
nvidia,3e070o,RiffyDivine2,1 point,Tue Jul 21 00:43:49 2015 UTC,$200 premium for ~10% higher ASIC score?
nvidia,3e070o,I_Will_be_Nice,3,Tue Jul 21 00:23:55 2015 UTC,It's aimed at a market of people who don't care about the price and not the normal user base.
nvidia,3e070o,sluflyer06,0,Tue Jul 21 04:12:55 2015 UTC,"Well, that's a bit shitty from EVGA, especially considering that ASIC scores don't appear to be the be all and end all of overclocking capability.  I'm disappointed in them."
nvidia,3e070o,Ianborg,1 point,Tue Jul 21 13:05:53 2015 UTC,"Seems legit to me, selling binned hardware at a premium price has been around for over a decade. It's worth it to people looking for the last 1%."
nvidia,3e070o,revjdm,1 point,Tue Jul 21 07:19:15 2015 UTC,Haha I feel fortunate. Mine $649 reference card has a 84% asic rating.
nvidia,3e070o,DexRogue,1 point,Tue Jul 21 14:08:12 2015 UTC,That's a crazy mark-up...
nvidia,3e070o,steak4take,1 point,Tue Jul 21 15:20:05 2015 UTC,78.1% ASIC G1 and I didn't pay more. Suck it EVGA.
nvidia,3e3ptf,Kirb-,1 point,Tue Jul 21 18:47:07 2015 UTC,"The same as any 980 Ti which is basically 1400 minimum, 1450-1500Mhz average @ stock voltage depending on asic quality & silicon lottery.   Maxwell is a bit odd when it comes to overclocking in that it doesn't really respond well to increasing voltage at ambient temps unlike previous generations. In the past it was always more voltage = more clock speed with ability to dissipate heat being the limiting factor which is why historically you could hit higher overclocks under water than on air.  With maxwell to see benefit from overvolting you need to be at sub ambient temps (0C or colder) so you don't really see a whole lot of benefit of a custom PCB with extra voltage & power phases unless you are using phase change cooling or LN2.  You can read more about this from Kingpin's statement here: http://kingpincooling.com/forum/showpost.php?p=30273&postcount=57  My reference EVGA 980 Ti ACX 2.0 is game stable at +250Mhz core (1480Mhz boost) at stock voltage & can pass firestrike at +290 (1520Mhz boost) with 75.4% asic quality."
nvidia,3e3ptf,kittah,1 point,Tue Jul 21 19:55:11 2015 UTC,"Hm, alright what are your temps like under load with ACX 2.0?"
nvidia,3e3ptf,kittah,2,Tue Jul 21 20:10:52 2015 UTC,With 1 card in a HAF XM case I see about 71C max in Witcher 3 which sits at a constant 99% GPU usage. That's at about 70% fan speed.
nvidia,3e3lr3,xTommy2016x,1 point,Tue Jul 21 18:19:12 2015 UTC,The only solution is to lower my overclock or just have it at stock settings   Maybe your card just sucks at overclocking and its not the driver.
nvidia,3e3lr3,hdshatter,1 point,Tue Jul 21 20:19:08 2015 UTC,"Doubt it, when I first got my card a few months ago it over clocked like a champ. Now I'm on a lower clock and still getting crashes. I'm going to roll back to a really old driver to confirm this"
nvidia,3e3lr3,hdshatter,1 point,Tue Jul 21 21:04:14 2015 UTC,Have you tried 353.49? No issues for me on it.  Maybe their first real windows 10 WHQL will fix all the BS.
nvidia,3e3lr3,SzyjeCzapki,0,Tue Jul 21 21:09:00 2015 UTC,wait for nvidia to not be shit.
nvidia,3e3ks2,deumetomnia,1 point,Tue Jul 21 18:12:36 2015 UTC,"Did you have any programs open in the background? That may drop your score a bit if your CPU is busy doing other tasks in the background. You may want to scan your PC for malware and other viruses as well as that would also impact performance.   I have an i7 4790k at stock speeds (4.0), 16GB RAM, and the same model 980 Ti with the stock OC mode enabled and I score about 14800 on average."
nvidia,3e3ks2,scarecrowman175,1 point,Tue Jul 21 19:12:13 2015 UTC,Ok I figured it out. For some reason my fans were locked at 25 percent speed so my card was at 800MHz. I got it running normally and I'm going to run another test now. Should get more reasonable results.
nvidia,3e3ks2,Syn246,2,Tue Jul 21 20:00:12 2015 UTC,"If that is your total score, please also post your graphics and physics scores (smaller numbers below the total after the benchmark run).  The 3dmark total score is not good for comparing your graphics performance relative to others because it includes the physics test which is 100% CPU-bound. You'll always come up short against i7 users unless you compare graphics-to-graphics scores."
nvidia,3e3ks2,andrew5161,1 point,Tue Jul 21 19:14:57 2015 UTC,"There is nearly no difference in 3d mark score for 4690k vs 4790k btw, its less than 500."
nvidia,3e3ks2,Syn246,2,Tue Jul 21 19:25:00 2015 UTC,"This is true for the graphics-only score, but not the total score. A given i5 will score substantially lower on the physics test than its i7 counterpart with Hyperthreading enabled, assuming all other things are equal such as clock speeds.  A 4690k will usually score somewhere in the 8-9k range on the physics test while the 4790k will score more like 13k+. This does manifest itself in the total score as well, even if the two machines' graphics scores are the same."
nvidia,3e3ks2,andrew5161,0,Tue Jul 21 19:50:03 2015 UTC,"That is not really true. My friends 5820k and my 4690k (both with large overclocks) only score about 2000 diff in total score, a 5820k is a much more powerful chip than a 4790k so the difference has to be even less than those two when comparing a 4790k to a 4690k. And the graphics score is not effected by your cpu except a tiny bit in the combined test score."
nvidia,3e3ks2,Syn246,2,Tue Jul 21 20:35:17 2015 UTC,"This is simply incorrect and it's misleading to others who are reading it. The total score is a function of both the graphics and physics scores. The physics score is directly related to the number of threads on the CPU. If you have the exact same GPU setup but different CPU thread count between two PCs, the machine with more threads will score higher both on the physics score and the total score (assuming all else is equal between the systems). The perceived degree or severity of the score discrepancy is subjective--the quantitative result is that one scores higher than the other every time. Thus, comparing graphics score to graphics score is the most valid choice when attempting to determine how your video card(s) are performing relative to others."
nvidia,3e3ks2,kittah,2,Tue Jul 21 21:55:39 2015 UTC,This is 110% correct. Here is a great example using a benchmark I found for a 980ti + 3570k vs my 980ti + 3770k with similar graphics scores.  Graphics score difference: 0.5% (19406 vs 19499) Physics score difference: 50.6% (8377 vs 12617) Total score difference:    8.2% (14356 vs 15540)  http://www.3dmark.com/compare/fs/5063515/fs/5343339
nvidia,3e3ks2,kittah,1 point,Tue Jul 21 22:05:45 2015 UTC,Here's even better proof. Ran a test with 980 Ti @ 1480mhz boost with a 3770k @ 3.7ghz stock vs 4.8ghz overclocked. Graphics score difference was within margin of error (0.2%)  http://www.3dmark.com/compare/fs/5469958/fs/5469899  980ti @ 1480Mhz boost  i7-3770k @ 3.7ghz stock vs i7-3770k @ 4.8ghz OC  Overall score: 14691 vs 16463 (+12.1%) Graphics score: 21133 vs 21097 (-0.2%) Physics score: 9847 vs 12621 (+28.2%) Graphics test 1: 101.95 fps vs 101.86 fps (-0.1%) Graphics test 2: 83.63 fps vs 83.43 fps (-0.1%) Physics test: 31.26 fps vs 40.07 fps (+28.2%) Combined test: 26.82 fps vs 34.96 fps (+30.4%)
nvidia,3e3ks2,andrew5161,0,Wed Jul 22 01:00:34 2015 UTC,"Actually you are still wrong, cpu vs cpu is only valid when talking in reference to applications that can use all threads to their max potential, a application that uses 4 threads, preforms exactly the same at the same clock with 4 threads as it does with 8 or 12 or 16 or 64. Also there is no direct relationship between the number off cores and the score for physics, so even suggest that is plainly ignorant. If this was even remotely true, why does a 8350 (8 core) score lower than a 4690k (4 core), twice as many cores. ""The physics score is directly related to the number of threads on the CPU."" << Wrong.  To expound further, the combined score in firestrike also effects the overall graphics score, the combined score is derived from cpu + gpu, so unless you have the exact same cpu even then its not a direct comparison between gpus to use the graphics score. You really should learn more about how firestrike works before you try to lecture on it.  ""--the quantitative result is that one scores higher than the other every time. "" Not true at all, a 4790k running at .1 ghz will score lower than a 4690k running at 4.5 ghz. You are making overriding statements about things you apparently know nothing about, the physics score is nowhere near as directly tied to core counts as you make it out to be, there are tons of other things to take into account such as clock speed and manufacturing process."
nvidia,3e3ks2,Syn246,2,Tue Jul 21 22:22:21 2015 UTC,"I'm actually right but you're trying to pick apart my words to turn them into something they're not. I stand by my statement that the physics score is directly related to the number of threads, because your terrible attempt at refuting it is entirely flawed--you are trying to compare an 8-thread AMD that's got a totally different architecture to a 4-thread Intel chip. It is improtant to note that I never said that core count is the only function of physics scoring in 3dmark. To support my prior statement which implies that all else is equal besides core count between two given CPUs, you should compare a Haswell i5 to its Haswell i7 counterpart, or Ivybridge i5 to Ivybridge i7, or Sandybridge i5 to Sandybridge i7, etc. You will see that two chips with the same architecture and clock speeds will generate vastly different 3dmark physics scores (and thus total scores) due to the core count difference between the two. Sure, there are other factors as well, but the thread count is by far the most impactful.  All I am saying is that quite simply, the ""total score"" in 3dmark--a.k.a. the giant orange one at the top that many people mistakenly post without also including the detailed graphics-only and physics-only scores--is tied directly to both the graphics and the physics scores. It would be invalid to surmise that a given i5 machine's graphics performance is significantly worse than the same machine with an i7 due to looking only at the two total scores.  Here's another example to add to the person who replied earlier. Before I post the details, this is not a perfect example because the machines are not identical and the CPUs are clocked differently, however the graphics scores are almost exactly the same while the physics scores are roughly 50% apart (which is typical for a 4-thread i5 vs. an 8-thread i7) and you can see that the total score is also different. This is all I'm saying--don't compare total score to total score when trying to determine graphics performance between machines with vastly different CPUs.   4690k: http://www.3dmark.com/fs/5324346 4790k: http://www.3dmark.com/fs/5459409 Graphics score difference: 0.1% (39799 vs 39850) -- nearly identical Physics score difference: 51.3% (8967 vs 13564) Total score difference: 14.3% (21119 vs 24139)"
nvidia,3e3ks2,andrew5161,1 point,Tue Jul 21 22:41:33 2015 UTC,"Lets just list some of the factors you didn't take into account at all, CORE CLOCK for the umptenth time. Next is motherboard, then ram amount and speed, next is you are dealing with 2 gpus in sli which you CAN NOT predict how they will interact. You are also comparing evga stock cards to g1 gaming cards from gigabyte. Jesus I give up, if you think this is an even remotely relevant test you are far beyond saving from your own stupidity."
nvidia,3e3ks2,Syn246,1 point,Tue Jul 21 22:48:50 2015 UTC,"So again for whatever reason you are trying to run away with your own fabricated argument. I never said thread count was the only factor in physics scoring. I wholly agree that there are other factors in play--of course there are! Thread count is just the primary driver of the discrepancy.  My point is simply to not compare 3dmark total scores between two machines with vastly different CPUs, otherwise you'll go mad if you're on the shorter end of the CPU stick.   You can set # of cores/threads aside entirely--if any two 3dmark results' graphics scores are identical but their physics scores differ by a significant percentage (i.e. 30% or more), the total scores will also be skewed in favor of the machine scoring significantly higher on the physics portion. Period.  If you disagree with this point, find me a pair of validated Fire Strike results where their graphics scores are within 1%, physics scores are 50% apart or more, AND their total scores are also still within 1% (or hell even 2%). It's not going to happen, and this is my entire point. I see people with i5s chasing i7 HT 3dmark total scores when they should instead focus only on the graphics scores, and that's what I hope people will understand from this discussion."
nvidia,3e3ks2,andrew5161,1 point,Tue Jul 21 23:09:11 2015 UTC,"No, you actually said physics score is wholey dependant on core count, which is idiotic. I am sure you've edited by now, you can not compared 2 cpus based on their core counts when they have entire different core speeds, but as i said before, ignorance is bliss, go on living in your fantasy world of made up numbers and ""facts"" based on completely different systems where you base your ""core count"" argument on test beds of completely different factors. NO idea how you ever made it through an science class if that is how you interpreted the scientific method.  I am done with this convo, please do not respond, go post your thoughts in some other thread where other people will believe you bullshit made up numbers."
nvidia,3e3ks2,kittah,1 point,Tue Jul 21 23:15:54 2015 UTC,"Graphics score measures graphics performance, 3dmark score measures system performance including CPU. When comparing graphics cards graphics score is BY FAR the most important since the physics test is 100% cpu bound and makes a significant difference to overall system score. Unlike most applications 3dmark DOES take full advantage of a hyperthreaded CPU which means the results get skewed in favor of i7s over i5s when in real world game performance the difference is minimal.  When comparing graphics cards it makes most sense to compare graphics score since that takes most of the other variables out of the equation."
nvidia,3e3ks2,andrew5161,1 point,Tue Jul 21 23:13:03 2015 UTC,"Go do some research, the graphics score in 3d mark is massively based on the cpu performance. The ""combined"" test is taken into account when calculating the graphics score, by virtue of cpu being half of the combined score the graphics score can not possibly be unbias toward the gpu without the cpu being taken into account."
nvidia,3e3ks2,Syn246,1 point,Tue Jul 21 23:24:47 2015 UTC,Right. I got a 7044 physics score and a 18867 graphics score. It is only an i5 so I suppose that explains it.
nvidia,3e3ks2,Syn246,2,Tue Jul 21 22:04:27 2015 UTC,"Your graphics score is fairly healthy, but as you can see the physics score is pulling your total score down compared to others with similar setups.   What is your CPU running at? It sounds like it's not overclocked or perhaps a mild OC compared to many of the results around here.  Using the Advanced Search feature on the 3dmark site, it appears most 4690k & single 980 Ti users are scoring a total score in the 15000+ range, but their physics scores are more in the 9000 neighborhood or ~30% higher than yours. If you can OC your CPU to the 4.3ghz+ range (assuming you're not already there based on the physics score) you'll probably see a substantial total score improvement."
nvidia,3e3ks2,p3dr0maz,1 point,Tue Jul 21 22:27:48 2015 UTC,My CPU OC is a at 4.44GHz. I have no idea what's going wrong.
nvidia,3e3ks2,andrew5161,1 point,Tue Jul 21 23:06:44 2015 UTC,How are the CPU temps? Any programs running in the background during the benches? (Sorry if that's been covered already)
nvidia,3e3ks2,hdshatter,1 point,Tue Jul 21 23:11:00 2015 UTC,All my temps are good. I'll end absolutely everything and try it once more... All I had was a text document open.
nvidia,3e3ks2,fastcar25,1 point,Tue Jul 21 23:12:52 2015 UTC,No problems with temps. I ran it again and it got up to 8880 physics score and around 15000 total. Not sure why but something was crippling my CPU performance. Seems OK now.
nvidia,3e3ks2,fastcar25,1 point,Tue Jul 21 23:28:55 2015 UTC,"Your GPU score is most important. Most TI's are getting around 20,750-21k after a little overclock action. Yours seems right in line with that. I wouldnt worry too much about your phsycis score unless you're trying to bench your CPU."
nvidia,3e3ks2,mrnewb2121,1 point,Wed Jul 22 00:03:06 2015 UTC,"Right. I really only use this machine for gaming so I don't need a ton of CPU performance. So far it handles everything I throw at it, even Witcher 3, just fine."
nvidia,3e312f,TaZTeD,2,Tue Jul 21 15:57:53 2015 UTC,You're fine. The 970 draws relatively little for a GPU and the rest of your system adds up to not much either.
nvidia,3e312f,sluflyer06,1 point,Tue Jul 21 16:41:33 2015 UTC,"Yep. Got a similar build with an OC 4690k, watercooling etc and it sits around 380w."
nvidia,3e312f,PCGA,1 point,Tue Jul 21 17:35:56 2015 UTC,"""A GeForce GTX 970 - needs 28 amps on the 12v + rail and a 500 watt power supply with 1 - 6 pin and 1 - 8 pin PCIE power connection.""  Your PSU is a CX430, which has: +3.3V@20A, +5V@20A, +12V@32A, -12V@0.8A, +5VSB@3.0A  You should be safe provided you don't OC much."
nvidia,3e312f,slykrysis,1 point,Tue Jul 21 19:07:17 2015 UTC,"Thanks for the information, I won't OC it at all..  Could you eli5 how those rails work? like do HDD's use a lower voltage rail ?"
nvidia,3e1jrn,Rate_My_Build,2,Tue Jul 21 06:25:15 2015 UTC,"23 this month,with mgsv:tpp"
nvidia,3e1jrn,RikoudoX,1 point,Tue Jul 21 06:43:33 2015 UTC,"Sick, do you happen to know where this was confirmed?"
nvidia,3e1jrn,RikoudoX,2,Tue Jul 21 06:58:50 2015 UTC,confirmed by newegg support on reddit just wait until i get the link and i'll edit https://www.reddit.com/r/pcgaming/comments/3di9uq/newegg_is_now_bunding_mgsvphantom_pain_with/
nvidia,3e1jrn,RikoudoX,1 point,Tue Jul 21 07:05:38 2015 UTC,"I don't see where it says the 23rd, but I will take your word for it. Thanks"
nvidia,3e0qgj,ineedhelpwiththiscar,3,Tue Jul 21 01:48:42 2015 UTC,"My 980ti was stuttering when I first got it, looked into it myself and found the problem almost immediately.  In GPU-Z my motherboard was running the card at PCI 2.0 x8 speed, so I went into bios and set PCI to force gen 3 and bam noticeable FPS boost and stutter gone.  Reported the bug to Asus, no response but they should know it exist."
nvidia,3e0qgj,hdshatter,3,Tue Jul 21 02:11:46 2015 UTC,I haven't heard that one yet. I'll see if I can get that working.
nvidia,3e0qgj,hdshatter,2,Tue Jul 21 03:37:29 2015 UTC,"Download GPU-Z, if your PCI is 3.0 on your motherboard it should say PCI-E 3.0 x16"
nvidia,3e0qgj,chrisdok,2,Tue Jul 21 03:41:51 2015 UTC,"Yeah, I've been using GPU-Z to monitor things such as GPU load, and  I do have an ASUS motherboard that PCI 3.0 capable at least. And on GPU-Z it's saying my bus interface is PCI-E 3.0 x16 as you described"
nvidia,3e0qgj,ExplosiveNutsack69,1 point,Tue Jul 21 04:33:21 2015 UTC,"The 900-series has problems with GPU usage in low demanding games like mobas and CSGO. There's a big thread on the nvidia forums about the issue. No games should have ""unplayable fps"" however, and most often it seems to bottleneck in the 150+ fps range."
nvidia,3e0qgj,Kameezie,1 point,Tue Jul 21 09:34:06 2015 UTC,Need more info dude.  What CPU have you got?  Did you install the 970 into the best available PCIe slot on your motherboard?  Is it getting too hot for some reason?
nvidia,3e0qgj,solidnitrogen,2,Tue Jul 21 02:56:28 2015 UTC,"i7 3770k sandy bridge. yes, and no."
nvidia,3e0qgj,solidnitrogen,1 point,Tue Jul 21 03:35:57 2015 UTC,That's an Ivy Bridge.
nvidia,3e0qgj,solidnitrogen,1 point,Tue Jul 21 05:12:55 2015 UTC,"Aren't there 3770's of both models? tbh I bought that processor three years ago and haven't looked since, so you're probably right.  EDIT: Sigh, 'tis Ivy indeed. My bad."
nvidia,3e0qgj,solidnitrogen,1 point,Tue Jul 21 05:14:29 2015 UTC,Did you use ddu when installing the new driver?
nvidia,3e0qgj,solidnitrogen,1 point,Tue Jul 21 03:22:13 2015 UTC,"yeah, to get rid of the old drivers"
nvidia,3e0qgj,Kameezie,1 point,Tue Jul 21 03:36:45 2015 UTC,Weird. Did you check your bios settings for PCI e lane speed?
nvidia,3e0qgj,veRxy,1 point,Tue Jul 21 04:04:45 2015 UTC,"Well, on GPU-Z, my bus interface reads PCI-E 3.0 x16. Does that answer anything?"
nvidia,3e0qgj,Sapass1,1 point,Tue Jul 21 04:36:44 2015 UTC,"yeah, that what it should be, honestly I have no clue what your issue actually is. Maybe your PSU isnt powerful enough? worst case scenario, reinstall windows. :("
nvidia,3e0qgj,Sapass1,1 point,Tue Jul 21 05:15:14 2015 UTC,"Psu is 750, so I donnttt think that's the issue. Sorry to ask, but what might reinstalling windows do?"
nvidia,3e0qgj,Sapass1,1 point,Tue Jul 21 17:11:13 2015 UTC,"Sometimes you end up having a root lot or some malware or ever registry errors that can mess with performance.  If there's something wrong with my pc that I can't undo after a few days I usually wipe and restart, but I have all my files and programs on a separate disk so it doesn't affect me too much.  IF you have an extra disk is clone your windows install onto it just to be safe if you do go for the nuclear option.  Then if it doesn't invoice performance you can always just go back to your previous setup knowing that it's a hardware issue not software."
nvidia,3e0qgj,p3dr0maz,1 point,Tue Jul 21 17:33:18 2015 UTC,"Hmmm, seems like something I would easily mess up, but it might be worth a shot.."
nvidia,3e0qgj,kraM1t,1 point,Tue Jul 21 17:59:12 2015 UTC,"I mean, this is if you can't get it working properly by normal means, If you end up needing to do that I can help you out with a couple of links and tutorials so you don't mess up"
nvidia,3e0qgj,Kryptickzz,1 point,Tue Jul 21 18:03:47 2015 UTC,"that would be incredibly appreciated if it does come to that. I feel so bad. This card was a birthday gift from my girlfriend and I know she hates seeing so many problems coming from something as simple as replacing a graphics card  EDIT: I do have two hard drives. An SSD which currently holds my OS, and another normal 7500RPM."
nvidia,3e0qgj,BlitzcrankBot,1 point,Tue Jul 21 18:20:01 2015 UTC,"Osu and CSGO aren't really GPU demanding games. Monitors are bottle necked by the hertz/refresh rate it can handle and going over to a certain amount will cause frame stutters. If you're over 150 FPS in those 2 games, it's fine."
nvidia,3e0qgj,stealer0517,1 point,Tue Jul 21 05:36:01 2015 UTC,"See I want to agree with you, but seeing friends run that at 300+ frames makes it hard to believe. Also my monitor is 144hz with 1 ms response, so I don't know if it will be hindered. The old card would push from 180 to 250 fps, while this new one can be from 90-150 at best"
nvidia,3e0qgj,BlitzcrankBot,1 point,Tue Jul 21 17:12:13 2015 UTC,My old AMD 7770 have me better FPS in csgo than my 970..
nvidia,3e0wvl,bilobob,4,Tue Jul 21 02:41:18 2015 UTC,"Raising voltage on Maxwell doesn't help much, it can even make your GPU unstable. Historically you could just add voltage and be pretty much guranteed better overclocks, not on Maxwell."
nvidia,3e0wvl,hdshatter,1 point,Tue Jul 21 02:57:00 2015 UTC,"You nailed it I don't raise voltage on Maxwell, in fact, undervolting yields almost better results for me."
nvidia,3e0wvl,LiberDeOpp,2,Tue Jul 21 05:49:24 2015 UTC,You have a 100hz monitor?
nvidia,3e0wvl,DrexelDragon93,1 point,Tue Jul 21 12:03:21 2015 UTC,It's a QNIX QX2710 overclocked to 100hz
nvidia,3dyxql,ariyapl,24,Mon Jul 20 17:45:14 2015 UTC,"EVGA hands down. Easy to RMA stuff, offer that step up program, can pay for option of cross shipping on RMA, etc."
nvidia,3dyxql,kittah,3,Mon Jul 20 17:55:32 2015 UTC,What's the step up program ?
nvidia,3dyxql,kittah,4,Mon Jul 20 17:57:08 2015 UTC,"If you buy an EVGA card and register it online within 14 days you have 90 days from the time of your purchase to ""step up"" to a better card. You just have to pay the difference (Price of new card - Price of old card = step up price). I just used it to step up my GTX 980 to a 980 Ti for $60 + shipping.   Can read about it here: http://www.evga.com/support/stepup/"
nvidia,3dyxql,kittah,2,Mon Jul 20 18:03:12 2015 UTC,"That's pretty awesome, didn't know about that. Does it work for the same line of products? Like changing reference EVGA into KINGPIN?   Thanks man."
nvidia,3dyxql,kendirect,3,Mon Jul 20 18:08:52 2015 UTC,"Sometimes but not always. Usually it's only reference models available, currently only 980 Ti they offer is the 980 Ti reference with ACX 2.0 cooler. However I have seen in the past where they offered ability to step up from like 2GB version of a card to the 4GB version.  On that page I linked they maintain a list of products available to step up to."
nvidia,3dyxql,Neumayer23,2,Mon Jul 20 18:15:24 2015 UTC,"Like Kittah said, sometimes - but generally no.  Just catch the right CSR feeling good that day."
nvidia,3dyxql,PrimaryLupine,9,Mon Jul 20 18:51:12 2015 UTC,"EVGA and I have a story about this to share. Late may, my 780ti died, so I sent it to EVGA and then they sent me a 980 FTW back, great! However this 980 FTW was having problems where it would randomly blackscreen and lock up the pc so that reset was the only thing to do.  When I went to the forum I found that the issue was fairly common and there was at least 8 or 9 people that were complaining about this. To add to the mistery, the card would work absolutely fine in firestrike extreme and could loop it as long as you wanted, however it could not finish a run of skydiver. It became apparent that voltage fluctuations were crashing the card as somehow if you ran with DSR enabled, that would stop the crashes.  Couple of days later in the thread chimes in not a forum mod, not some EVGA PR guy, but the product manager who made the card in the first place, he communicated directly with us and quickly they found out there was indeed an issue that affected quite some 980 FTWs. Once they identified it, it took them less than two weeks to deploy a fix.   Meanwhile, they were offering free returns to people affected and, I returned myself my card just to get another one with the same issue, I held onto this one though as the fix was being developed and once the fix was out, I sent it back one last time (still for free) and got another fixed 980 FTW back that is problems free.  That's a prime example of how customer support should be done, there was a problem with the card and rather than running circles around you the company investigated, ADMITTED the issue and fixed the problem quickly.   In the end everyone makes a mistake, what matters is how you own up to it and no one does that better than EVGA.  Yes this post is awfully pro EVGA, but they worked so hard on this and so quick that they deserve to have praises sung, they set an example for every other video card maker out there."
nvidia,3dyxql,Kyitrai,3,Mon Jul 20 19:04:29 2015 UTC,"I have an old EVGA GTX 260. I bought it like 6-7 years ago. It's still alive and kickin'. I opened it up recently, cleaned and changed the thermal compound. Temps went down and it's much quieter. Maybe not the quietest card on the market (blower cooler) but definitely very durable."
nvidia,3dyxql,Neumayer23,1 point,Mon Jul 20 19:14:44 2015 UTC,"Have a 260 and a 460 that are still trucking along. The 460 even got a decent score in Fire Strike Ultra. Cleaned the cooler, added a high-flow bracket, and redid the thermal paste with Arctic Silver. Card barely tops 75°C."
nvidia,3dyxql,Kyitrai,1 point,Tue Jul 21 14:11:56 2015 UTC,"I've got a GTX980 I'm having this problem with and have been through 2 RMAs (so this is the third card). Did they shed any light on what the issue was at all? I'm getting pretty frustrated, just hoping to get any info on the cause, as I've eliminated everything else I possibly can (clean installs, PSU, etc.). I have a GTX760 that has no problems, which I've fell back on whenever waiting for the replacement."
nvidia,3dyxql,kendirect,1 point,Tue Jul 21 02:42:55 2015 UTC,"The issue was confirmed only on the 980FTW. Classy, sc and ssc are trouble free. If you got a FTW it is likely you've got the same problem otherwise it might be something else."
nvidia,3dyxql,maxiimus1,1 point,Tue Jul 21 08:31:12 2015 UTC,Thanks for the reply. No such luck for me that way then; mine's a SC.
nvidia,3dyxql,slykrysis,6,Tue Jul 21 08:32:48 2015 UTC,EVGA in my experience.  I won't buy from MSI because of their CSRs.
nvidia,3dyxql,kendirect,1 point,Mon Jul 20 18:18:21 2015 UTC,"CSRs? Just bought MSI R9 390 Gaming, should I be worried?"
nvidia,3dyxql,djisadud,2,Mon Jul 20 22:42:14 2015 UTC,Customer service reps
nvidia,3dyxql,djisadud,1 point,Mon Jul 20 22:50:22 2015 UTC,"No, don't be worried about the quality of the product.  It's generally top of the line.  Be worried if you have to RMA something, it's actually unlikely you will get your RMA completed.  MSI really is that bad when it comes to customer service.  My mate bought has been on the runaround to replace his 980 ti for a month (bricked DOA)"
nvidia,3dyxql,djisadud,5,Mon Jul 20 23:55:56 2015 UTC,"Evga. I signed up for the advanced RMA plan on my 980 Ti classified. It came with coil whine. I called them, spoke with a tech, tech filed the RMA, and my replacement card will be here by the end of the week. The RMA was easier than returning the card to Newegg."
nvidia,3dyxql,djisadud,2,Mon Jul 20 18:40:36 2015 UTC,"Sounds awesome. When I'll be building my Pascal machine I'll definitely pick EVGA. Now it's a bit too late but hell, I hope my G1 comes without coil whine now."
nvidia,3dyxql,Nasaku7,3,Mon Jul 20 19:03:04 2015 UTC,"I went through two G1s until I landed on the classified. Both had coil whine, and my classified did too. If my replacement has coil whine, then I'm thinking it's my PSU. I hope you have better luck than I have had."
nvidia,3dyxql,iNoToRi0uS,1 point,Mon Jul 20 19:07:58 2015 UTC,What PSU you've got ?
nvidia,3dyxql,big_three,2,Mon Jul 20 19:15:28 2015 UTC,"I have an Evga 1600w P2. I got it for 200 dollars on sale from EVGA's website, new not B-stock. It's completely overkill, but I needed a PSU above 850w for my previous SLI setup and this was the cheapest option."
nvidia,3dyxql,Aldarro,2,Mon Jul 20 19:18:55 2015 UTC,"I also have EVGA. Might be the PSU as well. We'll see when I get the new card. Let me know when you get yours if it has coil whine or not, ok?"
nvidia,3dyxql,Aldarro,2,Mon Jul 20 19:27:07 2015 UTC,Will do!
nvidia,3dyxql,Aldarro,2,Mon Jul 20 19:49:43 2015 UTC,Thx m8
nvidia,3dyxql,Aldarro,5,Mon Jul 20 21:37:49 2015 UTC,Sounds like my next card will be from EVGA ;)
nvidia,3dyxql,BKachur,3,Mon Jul 20 19:41:27 2015 UTC,Evga
nvidia,3dyxql,Krypty,3,Mon Jul 20 18:22:40 2015 UTC,EVGA
nvidia,3dyxql,JaketheAlmighty,3,Mon Jul 20 18:23:46 2015 UTC,"EVGA for life! Always a pleasure to deal with their staff. Incredible customer service and great warranty policies. You can pretty much do anything you like, as long as you can get the card back with the original BIOS and no physical damage."
nvidia,3dyxql,SirCrest_YT,1 point,Mon Jul 20 21:39:07 2015 UTC,Can you put a waterblock on your card without voiding the warranty?
nvidia,3dyxql,TaintedSquirrel,2,Mon Jul 20 22:01:46 2015 UTC,"I believe so, yes. You're free to change the cooler/paste etc. without voiding it. They just don't cover damage, understandably, although they're the type of company to make exceptions to help customers (not guaranteed, but likely).  I recommend sending them an email with any queries, as they'll obviously be the best place for answers:  http://www.evga.com/about/ContactUs/"
nvidia,3dyxql,iamapizza,1 point,Mon Jul 20 22:05:31 2015 UTC,I've sent them an e-mail. Will see what they say.
nvidia,3dyf26,Pksayshey,4,Mon Jul 20 15:33:56 2015 UTC,"352.86, 353.06 & 353.30 have a problem that is fixed by 353.38 hotfix driver.  353.38 hotfix thread: https://forums.geforce.com/default/topic/849203/geforce-drivers/announcing-geforce-hotfix-driver-353-38/  352.86, 353.06 & 353.30 crashing thread: https://forums.geforce.com/default/topic/833693/geforce-drivers/nvidia-driver-352-86-353-06-stops-responding-and-kernel-reloads/"
nvidia,3dyf26,kittah,3,Mon Jul 20 16:02:52 2015 UTC,I find that 353.38 does not completely fix the crash it only made it happen less frequently
nvidia,3dyf26,arghsinic,3,Mon Jul 20 17:10:46 2015 UTC,Yep still crashing...
nvidia,3dyf26,xTommy2016x,1 point,Tue Jul 21 00:57:38 2015 UTC,"Same, but much less frequently for me"
nvidia,3dyf26,attomsk,2,Tue Jul 21 02:53:04 2015 UTC,"awesome, thank you for the reply."
nvidia,3dyf26,ahh_real_monsters_pc,2,Mon Jul 20 16:09:30 2015 UTC,Thanks. I have no had any crashes since I updated this (total of about 5 hours since upgrade).  Edit: The hotfix seems to slow down windows startup by several minutes. May be better to go back to an old driver if I can find a good one.
nvidia,3dyf26,kittah,1 point,Tue Jul 21 14:35:13 2015 UTC,I read in another thread that the latest version of Geforce experience is causing a slow boot for some people. If you have that installed it may be the culprit.
nvidia,3dyf26,ahh_real_monsters_pc,1 point,Tue Jul 21 15:42:55 2015 UTC,Thanks! Can you link me that thread?
nvidia,3dyf26,kittah,2,Tue Jul 21 15:55:48 2015 UTC,I think it was this one: https://www.reddit.com/r/nvidia/comments/3dpzsy/slow_ssd_boot_time_after_installing_new_gpu/  Found another one on the geforce forums with a supposed workaround: https://forums.geforce.com/default/topic/857772/geforce-experience/latest-geforce-experience-slow-windows-startup-bug-fix-/
nvidia,3dyf26,ahh_real_monsters_pc,2,Tue Jul 21 16:08:06 2015 UTC,Thank you!
nvidia,3dyf26,ahh_real_monsters_pc,3,Tue Jul 21 16:30:09 2015 UTC,"Yeah, I get random brief freezes and then the driver crashes. This is making it really hard to overclock =/."
nvidia,3dyf26,StayFrostyZ,2,Mon Jul 20 20:33:05 2015 UTC,"With 353.38, does anyone have a massive slow down of their applications loading right after a restart? (i.e. Antivirus starting up, etc) After installing this hotfix all of my startup apps seem to take a while to load up. This only happened today after installing this driver and so I tested my theory by running DDU. Afterwards my PC loaded up super fast as usual (Samsung 850 Evo). I guess I'm rolling back but not sure which one to roll back to"
nvidia,3dyf26,Lamella,1 point,Tue Jul 21 02:49:39 2015 UTC,"Weird that you mention that, since I suddenly got slow downs on login too and wasn't sure what the deal was. Takes about 30 seconds to log in to my account after entering my password when it used to be instantaneous."
nvidia,3dyf26,StayFrostyZ,1 point,Tue Jul 21 13:11:08 2015 UTC,Yeah I rolled back to the one before the hotfix and everything works fine now. I tried it again with 353.38 and that seems to be the cause of my massive slow down. Usually Nvidia seems to be ontop of drivers but for the past few months.. Man these drivers have been the bane of my PC life haha
nvidia,3dyf26,ahh_real_monsters_pc,1 point,Tue Jul 21 13:39:01 2015 UTC,Which version are you running now? I do not want to run 353.38 or 353.30 anymore.
nvidia,3dyf26,StayFrostyZ,1 point,Tue Jul 21 14:40:13 2015 UTC,Right now I'm running 353.30 while researching which would be the most stable driver to use right now.. It's so limited because some of the ones before 353.30 would make GTA V crash randomly.
nvidia,3dyf26,ahh_real_monsters_pc,1 point,Tue Jul 21 14:50:56 2015 UTC,Any reason not to try drivers before 353.30?
nvidia,3dyf26,StayFrostyZ,1 point,Tue Jul 21 15:05:00 2015 UTC,I noticed that on 353.30 there would be random crashing in general like while browsing the web or watching YouTube videos (I've tried Chrome and FireFox).
nvidia,3dyf26,ahh_real_monsters_pc,1 point,Tue Jul 21 15:06:12 2015 UTC,I said before 353.30. I know 353.30 sucks. :P
nvidia,3dyf26,StayFrostyZ,1 point,Tue Jul 21 15:30:36 2015 UTC,Ahh sorry! I misread. I read so far that people are having success with 347.08 (something close to that number) but I'm not sure how it runs on GTA V which has annoying crashing issues
nvidia,3dyf26,ahh_real_monsters_pc,1 point,Tue Jul 21 15:32:11 2015 UTC,I noticed that about half my desktop icons would take forever to load as well as every other startup app after Nvidia icon appeared. Thanks for confirming it for me.   Did your system seem very responsive yet it would take a while for some apps took a while to load before a certain period?
nvidia,3dyf26,StayFrostyZ,1 point,Tue Jul 21 14:39:26 2015 UTC,Yes! My system was pretty responsive besides that the start menu button would freeze up once in a while. It was just that the apps took forever to load and it led me to think my SSD was malfunctioning but that wasn't the case after running DDU haha
nvidia,3dyf26,ahh_real_monsters_pc,1 point,Tue Jul 21 14:49:12 2015 UTC,"I jumped to my old SSD, too! Haha. I ran a ""SMART"" utility to check the SSD and it was fine. I checked its speed, and it was blazing, especially with rapid mode. I also remembered issues with NVidia drivers stuttering my system and crashing, so I jumped on here to check. Again, really glad we could confirm it. Let's hope NVidia fixes this soon."
nvidia,3dyf26,Jjerot,1 point,Tue Jul 21 15:00:11 2015 UTC,"I'm running 353.06, just upgraded to a G-sync monitor and I started having similar problems. Updating to 353.30 or 353.38 causes my Titan X to Code 43 in the device manager and I have to deal with integrated graphics until I roll back. On windows 8.1 x64.  My solution for now is to disable G-sync and hope for the best, anyone else having similar issues?"
nvidia,3dyf26,Laffet,1 point,Mon Jul 20 19:00:18 2015 UTC,Latest Experience just dont working with me :) It wont open anyway.
nvidia,3dyf26,HeitorHCGomes,1 point,Mon Jul 20 21:02:18 2015 UTC,"I have a titan X and i'm still on 347.88 due to it's stability...Have already read that 353.38 doesn't completely fix the crashes...At least, not on titan x"
nvidia,3dyf26,Rasral123,1 point,Mon Jul 20 21:26:55 2015 UTC,"Having the same issues but only in The Witcher 3 and Serious Sam 3. It seems to be fine everywhere else. GTAV, FC4 ect all run fine but in Witcher 3 and Serious Sam 3 i can't play more than 30-40 mins without a TDR crash :/"
nvidia,3dyf26,spin_kick,1 point,Tue Jul 21 02:56:01 2015 UTC,"This new driver is running the cards too high, look at your boost in game and youll see why we are crashing so much."
nvidia,3dyf26,Neumayer23,0,Tue Jul 21 05:50:23 2015 UTC,"Stick with 353.38, there have been a couple of other release that in my experience, only downgraded the performance. 353.38 seems to be the best driver right now. Guess nvidia hired some of the AMD driver writers."
nvidia,3dyf26,no3y3h4nd,1 point,Mon Jul 20 16:54:03 2015 UTC,I'm on the 353.49 drivers currently - the 38 drivers tanked FPS in witcher 3 for me.
nvidia,3dyf26,ahh_real_monsters_pc,1 point,Tue Jul 21 05:44:40 2015 UTC,How is the 353.49 working out as far as games FPS and windows performance/crashes?
nvidia,3dyf26,no3y3h4nd,2,Tue Jul 21 19:26:48 2015 UTC,Its solid apart from lag in my email client. Its been solid for witcher 3 gives c. 80 fps in all ultra @ 1080p gsync
nvidia,3e0clh,turdking,2,Mon Jul 20 23:55:22 2015 UTC,"Try to reset any configuration the panel has, check you didn't check 24 hz in Nvidia's custom resolution panel, if you're on win 10 you may be bugged.  If it's a softbug unplug your monitor for 10 mins , or ultimately upgrade its firmware"
nvidia,3e0clh,KoreanGirlLoL,1 point,Tue Jul 21 00:21:41 2015 UTC,"I'm running Windows 7, so it it isn't a Win 10 thing.  First place I checked was the Nvidia Control Panel.   I'll unplug it for a while to see what takes place.   Thank you."
nvidia,3e0clh,sn1313,2,Tue Jul 21 00:52:52 2015 UTC,"Have you gone into NVIDIA control panel and checked the setting that lets you toggle the refresh rate? I know this is a ""no brainer"" but thats the only place I am aware of that will allow you to change it...unless Windows 7 has refresh rate settings in the built in ""appearance"" section in control panel on the OS."
nvidia,3e0clh,sn1313,1 point,Tue Jul 21 00:22:48 2015 UTC,"Yeah, that was the first place I went.  Running Windows 7 and also checked the advanced refresh rate setting as well.  The only option I have is 24hz in the Nvidia Control Panel.  Thanks for the help."
nvidia,3e136m,pixelqq,4,Tue Jul 21 03:35:09 2015 UTC,"Just 3d mark your system, then look at the graphics score, anything from 19.5 k to 22 k is fine. There is a lot of variance with binning."
nvidia,3e136m,andrew5161,1 point,Tue Jul 21 04:52:36 2015 UTC,"Tfw I can only get +105 core, +450 memory and about 16,500 in Firestrike. 75% asic"
nvidia,3e136m,shatteredhalo0,2,Tue Jul 21 11:45:10 2015 UTC,"I am running +103 +495 and get 21000k graphics and 16,5K combined.  So your score seems fine if that's a combined score."
nvidia,3e136m,p3dr0maz,2,Tue Jul 21 16:59:15 2015 UTC,"Yeah, that's combined, I'll have to check the graphics score when I get home from work.  I've been following this thread: http://www.overclock.net/t/1544574/gigabyte-gtx-9xx-g1-gaming-h2o-air-bios-tweaking/840  where user Laithan and some brave 980ti G1 owners have been testing out a modded bios with the goal of removing all perfcaps and maximizing overclock capability without any throttling.  It's exciting stuff, I may jump in and try out the beta modded bios to see how much farther I can go. : )  Btw, is that just a Firestrike/Heaven/Valley stable clock you listed, or a Witcher 3 stable clock?  Lol"
nvidia,3e136m,shatteredhalo0,1 point,Tue Jul 21 19:21:21 2015 UTC,"Yeah it's Witcher 3 stable. +103Mhz puts me at 1468.1Mhz @ 1.224V up until around 46c when it vDrops to 1.205V that puts me at 1455.5Mhz that's USUALLY where i bottom out, the next threshold at 56c (i rarely get this hot but on Witcher 3 vsync off it does) it vDrops again to 1.187V putting me at 1442Mhz.  The BIOS i have is vanilla with boost enabled only difference is I raised the power limit. If I drop in a vlocked boost disabled BIOS it will stay at 1468 all day long just idk idc that much.  I tried locking it in at 1.255V 1481Mhz and that was ALMOST stable it ran almost everything but Witcher 3 and Project cars was really putting it to its limit and I got some red artifacts after about 10/15 minutes.  ASIC 71% btw."
nvidia,3e136m,p3dr0maz,1 point,Tue Jul 21 19:30:42 2015 UTC,"um add me on steam and maybe we can trouble shoot your chip, thous numbers are super super low."
nvidia,3e136m,andrew5161,1 point,Tue Jul 21 19:19:03 2015 UTC,"I'm definitely running into Vrel perfcaps pretty much full time at boost clock of 1,506, and when it hits around 67C+, it downclocks the boost to 1,499.  Weird.  I've been half too busy playing Witcher 3 to do TOO much tweaking yet.  Edit:  Also getting a pretty bad coil whine from my PSU (and some from the GPU, but I think the PSU is worse, and maybe the PSU sucking is causing the GPU to suck as well), and a few times now my PC has just restarted without warning while under heavy gaming load.  Think my PSU is failing, need to open an RMA with Firepower (formerly OCZ) to see if they will replace my Fatal1ty 1000w (yeah, I fell for the gaymer meme), but I need to run to my local Mircrocenter to see if they can reprint my receipt from 2013.. I can't find it. :s"
nvidia,3e136m,shatteredhalo0,1 point,Tue Jul 21 19:32:51 2015 UTC,"Yea, a 1506 boost is a tad on the low side for our gpu, mine is stable at 1585 and my 24/7 clock is 1565. You should like to see around a 1540-1550 max boost clock. At least that's from the 6-8 that I have helped people overclock on reddit so far."
nvidia,3e136m,andrew5161,1 point,Tue Jul 21 20:37:21 2015 UTC,"Slightly different topic, what's binning? I've seen it talked about a couple of times but not understood what it is. Is it when the manufacturer picks the best chips and bins the 'bad' ones? So you end up with top grade chips, each one being slightly different?"
nvidia,3e136m,PorkAmbassador,3,Tue Jul 21 13:46:21 2015 UTC,"That's not binning. Binning is a process that happens after the manufacturing is done and its what gpu manufacturers uses to estabilish what will become of the chips. Let's say they just got done producing some GM 200, they will be checked for defects. Some will be perfect and have everything working and match the specification of a full GM200 GPU. Those will end up into Titan X cards. Some might have some slight defects, let's say rather than having all 24 SMM working, some have only 23. What nvidia does in this case is to disable another one and those chips will become 980Ti.  The downside to this is that any chip that comes out with less than 22SMM working, will have to be trashed as nvidia didn't create a product that uses a further reduced version of the GM200, which could partly explain the high costs.  What you are referring to (picking the best chips) happens when you have already a bunch of chips that you know are either Titan X or 980tis and you specifically select the ones that overclock more, have less leakage. That is called ""cherry-picking"""
nvidia,3e136m,Neumayer23,1 point,Tue Jul 21 14:22:37 2015 UTC,"TIL, Thanks very much, that makes sense."
nvidia,3e136m,PorkAmbassador,1 point,Tue Jul 21 15:57:09 2015 UTC,"So when you produce chips, you have literally bins of chips. You sort them by grade into I.E. low medium and high overclockers. The low ones go into reference pcbs the med go into cards like the sc and the 6g and the high binned go into cards like the msi lightning and the g1 and cards like the kingpin. Only cards i can think of that i know for a fact get binned chips."
nvidia,3e136m,andrew5161,1 point,Tue Jul 21 19:21:10 2015 UTC,"I get 1545 at max settings 1440p, Extreme tesselation and 8x AA."
nvidia,3e136m,hdshatter,1 point,Tue Jul 21 03:45:11 2015 UTC,1080p.
nvidia,3e136m,LiberDeOpp,1 point,Tue Jul 21 03:47:08 2015 UTC,What settings are you running at? what are your system specs?
nvidia,3e0a0j,pixelqq,2,Mon Jul 20 23:34:10 2015 UTC,What are you using to measure the clock speed?
nvidia,3e0a0j,Captain_Bear,1 point,Mon Jul 20 23:46:58 2015 UTC,Afterburner.
nvidia,3e0a0j,Captain_Bear,1 point,Mon Jul 20 23:49:40 2015 UTC,Mine will say that sometimes in afterburner (it OCs to 1480MHz normally (i have an EVGA SC+ ACX)) l0nk. If it isnt clocking higher in games try turning prefer maximum performance on under manage 3D settings>power management in NVidia control panel.  P.S. if you alt tab out of a fullscreen game it will clock down to a lower speed because it isn't rendering stuff so if you want to see real speeds use windowed/borderless mode and my favorite app for reading GPU things is GPU-Z
nvidia,3e0a0j,Captain_Bear,1 point,Mon Jul 20 23:57:26 2015 UTC,"Now, when I idle, my core clock is at 135, and my memory clock is at 405. When I benchmark, my core clock hits 1380, and my memory clock hits 3506. Is this normal?"
nvidia,3e0a0j,Captain_Bear,2,Tue Jul 21 00:17:19 2015 UTC,"Yes, turning the power management to max performance will make it idle around 1100MHz, which will make it run hotter but reach higher clock speeds in games (more fps). If you want you can probably overclock it more and get maybe 10+ more fps out of it."
nvidia,3e0a0j,p3dr0maz,1 point,Tue Jul 21 00:20:45 2015 UTC,I have no idea how to overclock... Think you could help? :3
nvidia,3e0a0j,Lobrauski,2,Tue Jul 21 00:21:38 2015 UTC,"Yeah, first start by downloading a benchmarking software like 3DMark or Heaven benchmark. Then turn the power limit slider all the way up and then increase the core clock slider by +25 and +50 on the mem slider and run the benchmark, of it runs without any problems and artifacting (weird colors, flashing textures, black spots) increase it again until it crashes (screen goes black and an error message appears and says the driver stopped and recovered) then turn the core and mem clock down by around -35 and -60 and there you go! A stable overclock. A good game that really pushes unstable overclocks is battlefield 4 (if it crashes in that you should lower the clocks around the same amount)"
nvidia,3e0a0j,arikv2,1 point,Tue Jul 21 02:23:57 2015 UTC,"595 core clock is the GPU ""safe clock"". Are you OC'ing? If it becomes unstable sometimes it will lock itself at this clock until you either a) restart the PC or b) enable/disable the GPU in device manager.    But yeah if the card's still stock that's kinda weird.  It could also be caused by a driver bug so not necesarilly a card issue. Make sure you're on the latest and greatest driver."
nvidia,3e0xf6,pixelqq,1 point,Tue Jul 21 02:45:53 2015 UTC,Run occt gpu bench with max memory and error checking enabled.  Would be surprised if you were error free for over a minute at that memory clock
nvidia,3e0xf6,attomsk,1 point,Tue Jul 21 02:51:28 2015 UTC,"I downloaded it, but how do I set that up?"
nvidia,3e0xf6,hdshatter,1 point,Tue Jul 21 02:54:57 2015 UTC,"I did it and it lasted 10 minutes, played Rust and it worked flawlessly."
nvidia,3e0xf6,hdshatter,1 point,Tue Jul 21 03:09:40 2015 UTC,"Ran it on mine too, no errors.  Memory running at 8312mhz"
nvidia,3e0xf6,attomsk,1 point,Tue Jul 21 03:31:47 2015 UTC,Wait wat.
nvidia,3e0xf6,attomsk,2,Tue Jul 21 03:32:39 2015 UTC,"8312mhz is the effective speed. In GPU-Z its 2078.  I've seen people who could do +700, have not tried it myself since I've heard after 650ish it starts to lower FPS rather than raise it."
nvidia,3e0xf6,hdshatter,1 point,Tue Jul 21 03:33:50 2015 UTC,That's pretty surprising.  Cool.  Any reason you are wanting to OC your mem so high without a core overclock?
nvidia,3e0xf6,hdshatter,1 point,Tue Jul 21 03:53:16 2015 UTC,I'm new to overclocking.. Would lowering the amount of my mem OC allow me to further OC my core clock?
nvidia,3e0xf6,JaketheAlmighty,1 point,Tue Jul 21 03:59:29 2015 UTC,"It might.  Most people recommend going for core clock over memory clock, I tend to believe a good balance of both is best.  I would monitor your power % when overclocked.  Overclocking GDDR5 can draw a surprising amount of power, this is one reason that lowering the memory clock could help achieve higher core clocks.  I would consider putting memory clocks down to +495 (~8000Mhz effective) and see what you could get from the core, start with something reasonable like 1450MHz and go from there.  For instance, My G1 seems the happiest at 1492MHz core and 8002 Mhz Mem  In MSI afterburner this is:  +87mv 130 power% +100 core +495 mem  I also use a custom fan curve that is a bit more aggressive to keep the gpu under 73 when I am using my overclocked settings."
nvidia,3e0xf6,JaketheAlmighty,1 point,Tue Jul 21 04:05:14 2015 UTC,"Been running mine at +650mhz, haven't seen any artifacts even in Valley/Heaven."
nvidia,3e0xf6,JaketheAlmighty,1 point,Tue Jul 21 02:55:34 2015 UTC,Have you altered you core clock?
nvidia,3e0xf6,attomsk,1 point,Tue Jul 21 02:55:50 2015 UTC,"+135mhz core, it gets me to 1500mhz with GPU boost.  Voltage set to +30, anything lower I get TDRs."
nvidia,3e0xf6,jim2point0,1 point,Tue Jul 21 03:01:11 2015 UTC,Isn't adding voltage bad for the GPU?
nvidia,3e0xf6,andrew5161,1 point,Tue Jul 21 03:02:28 2015 UTC,"well, it's not good for it.  but.. you want to use your card? or let it sit there and grow old never having stretched its legs.  your card will be fine."
nvidia,3e0xf6,LiberDeOpp,1 point,Tue Jul 21 03:36:30 2015 UTC,"I'm currently at +650 MHz memory clock, +150 core clock with factory voltage. No problems."
nvidia,3e0xf6,dcdead,1 point,Tue Jul 21 03:38:10 2015 UTC,sure. now crank it higher. increasing voltage will stabilize the card on higher OCs that are crashing otherwise
nvidia,3e0uoa,the_only_luke,1 point,Tue Jul 21 02:22:05 2015 UTC,"Use Wagnard's tool DDU from Geforce.com to thoroughly clean out the Nvidia drivers and afterwards install the latest driver version on 8.1 which supports your GPU Geforce.com/Drivers. As for your monitors, see if you can find the drivers from their respective manufactures site and install them after installing the GPU driver."
nvidia,3e02kq,pixelqq,1 point,Mon Jul 20 22:36:10 2015 UTC,"Does it clock down when in idle? The fans stop below 60-65c somewhere, which can lead to high idle temps if it doesn't clock down. 75c under full load is to be expected with any custom 980ti due to the higher TDP."
nvidia,3e02kq,keviig,1 point,Mon Jul 20 22:38:59 2015 UTC,"What do you mean by clocking down? Sorry, I'm new to GPU temps."
nvidia,3e02kq,AlphaSentry,1 point,Mon Jul 20 22:39:52 2015 UTC,"I now understand what you mean, but how would I know if it is clocking down?"
nvidia,3e02kq,AlphaSentry,1 point,Mon Jul 20 22:44:34 2015 UTC,"download MSI afterburner and watch the core clock graphs, they'll clock down when under low load and then kick up into full power when the need arises."
nvidia,3e02kq,AlphaSentry,1 point,Mon Jul 20 22:53:24 2015 UTC,"At idle my core clock is at 1152, and my memory clock is at 3506."
nvidia,3e02kq,kittah,1 point,Mon Jul 20 22:54:19 2015 UTC,"That seems excessively high for idle clocks, do you use a 144hz or higher then 1080p monitor?"
nvidia,3e02kq,hdshatter,1 point,Mon Jul 20 22:55:42 2015 UTC,1080p 60hz.
nvidia,3e02kq,hdshatter,1 point,Mon Jul 20 22:56:22 2015 UTC,"When running a benchmark my memory clock stays the same, but the core clock goes up to 1380."
nvidia,3e02kq,kmarsara,1 point,Mon Jul 20 22:56:08 2015 UTC,Do the clocks change at all when nothing is open except for Afterburner?
nvidia,3e02kq,attomsk,1 point,Mon Jul 20 23:00:13 2015 UTC,No.
nvidia,3e02kq,andrew5161,1 point,Mon Jul 20 23:01:56 2015 UTC,"Did you install that rivatuner server that is bundled with Afterburner? If so click that little i (pic) and look under ""active 3d processes"". Is anything listed?   I don't have rivatuner running in that screenshot which is why it says 'server not loaded' but if it is running it will tell you which process is preventing downclocking.  Having Chrome open or one of any # of programs can prevent the GPU from ever going into idle mode & downclocking."
nvidia,3e02kq,YinKuza,1 point,Tue Jul 21 00:36:45 2015 UTC,Is your power plan in windows set it balanced?
nvidia,3dy9j3,OlSom,1 point,Mon Jul 20 14:52:40 2015 UTC,"Happens every time I play a YouTube video and click another one or when I close Firefox.  Update: Downgraded to the previous version, now the bug is there as well..."
nvidia,3dy9j3,dayofmone,-1,Mon Jul 20 15:14:04 2015 UTC,I know this sound.To solve it you 1) set in control panel performance to max performance.2) uninstall GeForce experience.
nvidia,3dy9j3,myhtconex,-1,Mon Jul 20 22:23:04 2015 UTC,I know this sound.To solve it you 1) set in control panel performance to max performance.2) uninstall GeForce experience.
nvidia,3dy9j3,myhtconex,-1,Mon Jul 20 22:21:29 2015 UTC,Tell me if my solutions solved your problem
nvidia,3dz3rt,Syn246,3,Mon Jul 20 18:26:59 2015 UTC,"Just so you know that you aren't crazy, I have an X99 SLI setup as well and the cards show up backwards in Nvidia Control Panel for me. I have my monitor hooked up to the top card (closest to CPU), and it shows the monitor attached to card #2."
nvidia,3dz3rt,drkow19,1 point,Mon Jul 20 20:32:25 2015 UTC,"Thanks--that at least confirms I'm not experiencing some crazy/new scenario. I wonder if it impacts performance though, because like I said it's a little bit of a catch-22: if I put the stronger (& thus cooler/lower voltage) card in the top slot then it gets detected as GPU2 and does less of the work (again assuming the primary GPU does in fact carry more of the load), but if I put it in the bottom slot to make it GPU1 then the weaker/hotter card is in the top position and will reach its thermal limit sooner...  The main reason I'm so interested in chasing this down is because my cards now seem to be unstable at their old 1506/8000 OC. I've got them down to ~1470/8000 right now. Of course, being that I just changed the motherboard/processor/memory, the cause of the OC instability could be any number of things."
nvidia,3dz3rt,fuguemaster,3,Mon Jul 20 22:50:59 2015 UTC,"FWIW the NVIDIA control panel says the GPU closest to the CPU is #2 for my Z97 motherboard. I have my monitors hooked up to the card closest to the CPU, and it shows the monitors attached to card #2."
nvidia,3dz3rt,Sourdeez2014,1 point,Mon Jul 20 22:00:16 2015 UTC,I had a previous z77 board that showed them in reverse but after it died my new z77 motherboard shows them in the correct order.
nvidia,3e0fr0,kmarsara,2,Tue Jul 21 00:21:03 2015 UTC,Damn! What sucks the most is when it affects audio chips with less than adequate shielding.
nvidia,3e0fr0,cyberpunk7t9,2,Tue Jul 21 00:33:18 2015 UTC,i had the evga reference models before and now i regret returning them for these.  the evga ones had minor coil whine but it was just a slight buzz
nvidia,3e0fr0,hdshatter,1 point,Tue Jul 21 00:42:09 2015 UTC,If its doing it during normal gameplay and not just on high FPS menus/stress test its defective.
nvidia,3e0fr0,KoreanGirlLoL,1 point,Tue Jul 21 01:15:51 2015 UTC,"well this is during heaven benchmark, then again i do have a 144hz gsync monitor so it pushes the cards pretty hard"
nvidia,3e0fr0,DexRogue,1 point,Tue Jul 21 03:22:13 2015 UTC,"You can totally rma, those are faulty working cards, the assembly went wrong, in video card production , any powerspike in the machinery affects the final ending of the card, this is still a limitation of big PLC found in those industries"
nvidia,3e0fr0,fuccboi9000,1 point,Tue Jul 21 01:38:07 2015 UTC,"good thing i got them from retail, ill prolly take them back and replaced when they get more back in stock.  but out of the 3 sets of card i had with the 980ti evga ref evga acx and this g1 all of them had some sort of whine.. the ref ones had a hummmm sound and the acx had high pitch.. then these g1s have the buzz"
nvidia,3e0bgj,Barner2,1 point,Mon Jul 20 23:45:56 2015 UTC,"Make sure the PCI-E power connectors are on good, test another PCI slot if you can, reset your BIOS if that doesn't work.   Do the LED's come on for like where it says MSI? Last thing would be to try another rig if available.   If none of that works, RMA take it back."
nvidia,3e0bgj,p3dr0maz,1 point,Mon Jul 20 23:49:13 2015 UTC,"I'll try another PCI slot, how would I reset my BIOS? The fan is going and the lights on the card turn on."
nvidia,3e0bgj,p3dr0maz,1 point,Mon Jul 20 23:59:00 2015 UTC,Just to be sure make sure you're connecting to the card and not to your mobo graphics connectors.  To reset your bios you have to go into it when you start the computer  usually this means hitting F2 or something right in the beginning.  If you are seeing lights come on then double check the card is seated properly on the board.  All else fails it's probably a bad card.
nvidia,3e0bgj,kraM1t,1 point,Tue Jul 21 00:03:44 2015 UTC,"This happened to my bro's PC when he got a new GPU, it just kept using the integrated and ignoring his GPU. I fixed it by clearing CMOS on the motherboard.  Google it, it's easy, different for each manufacturer."
nvidia,3dxxoy,SaphiDaphi,1 point,Mon Jul 20 13:14:07 2015 UTC,"I was having a similar problem, check your volume mixer for nvidia capture server. If it isn't open try running GeForce Experience again."
nvidia,3dxxoy,Jjerot,1 point,Mon Jul 20 18:58:28 2015 UTC,yes that is it. But it doesnt come if i turn it on again.  http://puu.sh/j6q2T/81735a3b76.png
nvidia,3dxxoy,Jjerot,1 point,Mon Jul 20 20:04:06 2015 UTC,"Check your task manager to see if its running in the background, might have locked up or something. Force it to close and relaunch/toggle shadow play should hopefully fix it?"
nvidia,3dxxoy,Jjerot,1 point,Mon Jul 20 20:27:02 2015 UTC,what is the name of the prozess i might turned it off during start  http://puu.sh/j6tft/de5cc48662.png
nvidia,3dxxoy,Jjerot,1 point,Mon Jul 20 20:47:40 2015 UTC,"I'm on 8.1 so mine just shows as Nvidia capture server, but I think its  something like nvspcaps64.exe   If you open up GeForce Experience, click shadowplay at the top right where the login is, and toggle it on/off, does it show up then?"
nvidia,3dzv0j,AY099d0h9h0d,1 point,Mon Jul 20 21:39:49 2015 UTC,yes it will work. but dont expect to play across all 3 screens. a single 780 can do it but you will have to lower the settings. if you plan on sing the 1440p screen is will work as well but you wont get 144hz on any of the latest screens. i also have a 144hz 1440p screen but i have SLI 780s. and i still dont hit a full 144hz on the latest games.    so long story short...  1920x1200 screens = expect full frames.  1440p screen = you wont hit 144hz on graphic heavy games unless you lower the settings if you want speed over quality.   all 3 screens = expect medium graphic settings  with average 30-60(60 depending on the game)
nvidia,3dzv0j,Vinnyk84,1 point,Mon Jul 20 22:06:04 2015 UTC,"I'd expect to only use the one screen @144hz to play the game. With that being said, am I still going to lose frames because the displays are active? In other words, will I need to disconnect the other two screens, or are you talking about the case where somebody would span the game across all three screens?"
nvidia,3dzv0j,kittah,1 point,Mon Jul 20 22:10:34 2015 UTC,"Nah, no trouble in games at all. I'm just taking about the behavior of the GPU at idle. Instead of downclocking all the way to 135 Mhz it will idle at around 8-900Mhz which will cause higher idle temps than you might be used to.  This is a side effect of the higher pixel clock required to drive a 144hz display or to drive three displays.  Really no big deal just something you might notice and wonder why it's happening."
nvidia,3dzv0j,Vinnyk84,1 point,Mon Jul 20 22:20:04 2015 UTC,"if you are using just the 1440p and the others are just gonna be connected, you will maybe lose...1% of the power. its nothing big enough to notice. but a single gtx 780 will be tough to hit 144hz @ 1440p. if you give me some games you plan on playing. i can help you as we pretty much have the same setup except that i have SLI 780s."
nvidia,3dzv0j,fuguemaster,1 point,Mon Jul 20 23:08:20 2015 UTC,Should be no problem. The GTX 780 supports up to 4 displays. http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-780/features
nvidia,3dzv0j,kittah,1 point,Mon Jul 20 22:07:18 2015 UTC,"Yeah, you can run 3 monitors off 1 780 no worries.   Just be aware that you will most likely run into the issue of your GPU not downclocking when not in use. Normally at idle GPUs will clock down to ~150Mhz but if you have a single monitor @ 144hz or 3 monitors total it will only clock down to around ~800-900Mhz.  Workaround for single 144hz monitor is to set monitor to 120hz at desktop. However there really isn't a good way to get it to downclock with 3 monitors. You can google about forcing P states through nvidia inspector but it's more trouble than it's worth IMO."
nvidia,3dzs1j,TheBossPineapple,1 point,Mon Jul 20 21:18:31 2015 UTC,Bought Evga gtx 970 SSc and it's rocking! Running witcher on ultra 60 fps also playing Swtor with huge improvement from gtx 760. Didnt try the Gta V but i'm sure it will kill it too. So in conclusion i'm very happy with it so far.
nvidia,3dzs1j,Laffet,1 point,Mon Jul 20 21:22:15 2015 UTC,"Hm... I'm looking for a slightly cheaper GPU, pretty much all I play right now is League of Legends and some steam games such as CS:GO and Rocket League. The main reason that I want a new GPU is because I can only run rocket league at lowest settings and I only get 30 fps, lol. Would your old GTX 760 work with those types of games?"
nvidia,3dzs1j,MegaMaluco,1 point,Mon Jul 20 21:24:27 2015 UTC,GTX 960 should be enough for CS:GO and LoL
nvidia,3dzs1j,enano9314,1 point,Mon Jul 20 21:38:18 2015 UTC,"Alright, thanks a ton"
nvidia,3dzs1j,Randomness6894,1 point,Mon Jul 20 21:41:07 2015 UTC,"I'm going to be selling my old 760TI (probably) if you want it, name a price.   Good luck with whatever you decide, though!"
nvidia,3dzr8e,foxtrot1_1,2,Mon Jul 20 21:12:54 2015 UTC,I've been playing Witcher 3 on 670 sli just fine since the driver with the sli profile. Which problem(s) are you referring to? Any issue in particular? Maybe I haven't noticed it.
nvidia,3dzr8e,Peacecrafts,1 point,Mon Jul 20 21:44:51 2015 UTC,"Thanks. I think the negative reaction I saw was from before the profile was released, so obviously people were pissed.   What kind of frame rates are you getting? And do you have an OC on the cards?"
nvidia,3dzr8e,Peacecrafts,1 point,Tue Jul 21 14:18:46 2015 UTC,"My 670s are running at 1215 MHz boost and 1778 Mhz memory.  I don't usually play at this setting but for benchmark purposes it was set at 1080p, ultra, hairworks OFF, patch 1.07, latest? beta driver 353.49. I get avg around 53-54 running around in Novigrad and avg around 60-61 running in the forest.  If you tweak the settings like the useless ultra shadow, you can get solid 60 everywhere.  Also, if there's a specific place you want me to test, let me know."
nvidia,3dzr8e,ariyapl,1 point,Tue Jul 21 18:09:34 2015 UTC,"Thanks, that's really helpful. Will investigate how my own setup works this weekend. I run at similar clocks on my single 670, hopefully two will be twice the fun."
nvidia,3dzr8e,no3y3h4nd,1 point,Tue Jul 21 18:15:44 2015 UTC,I had 970 SLI. Just don't run Batman drivers. I had constant crashes when I updated to that version. I rolled back to HoTS version and everything was fine.
nvidia,3dzq9e,Laffet,3,Mon Jul 20 21:05:51 2015 UTC,"Try hotfix driver 353.38. It was released to address the crashing issues in 352.86, 352.06 & 353.30. If that doesn't work try rolling back to a driver that came out before 352.86 like 350.12 or 347.88.  Thread about the 353.38 hotfix here: https://forums.geforce.com/default/topic/849203/geforce-drivers/announcing-geforce-hotfix-driver-353-38/  Also how high are you trying to overclock your card? Crashing & artifacts can be caused by too high of a clock even on good drivers. How high you can push any given card is up to the silicon lottery, some chips just OC better than others."
nvidia,3dzq9e,kittah,1 point,Mon Jul 20 21:33:23 2015 UTC,"Well i don't think im overclocking that much, i installed the new driver too but problem not solved. While playing witcher 3 it crushes again and screen goes black have to res the Pc. So dunno what the problem is look like card cant pull all the performance well? Overclock + Witcher 3."
nvidia,3dzq9e,kittah,1 point,Mon Jul 20 21:45:21 2015 UTC,Just to be clear you installed 353.38 hot fix and not the latest driver available through GeForce experience right?  If so then try 350.12 or 347.88. If you still get crashes on those it's not a driver issue then. If reverting to stock clocks fixes it then it's definitely just too high of an overclock.   Hell I had a pair of superclocked 770s that would artifact and crash with as little as +20 mhz overclock. Current 980 ti can handle +251. So it goes.
nvidia,3dzq9e,kittah,2,Mon Jul 20 21:53:57 2015 UTC,Hmm i see. Ok mate i'll try those options:) Thanks a lot.
nvidia,3dzq9e,Racoonslikepuzzles,1 point,Mon Jul 20 21:57:59 2015 UTC,"Good luck man, Nvidia drivers have been causing so many headaches for so many people for the past few months. Hope you get it sorted!"
nvidia,3dzq9e,Racoonslikepuzzles,2,Mon Jul 20 21:59:46 2015 UTC,Does your driver crash and window will say that the kernel has crashed or something of the sort?
nvidia,3dzq9e,an_angry_Moose,1 point,Mon Jul 20 21:14:07 2015 UTC,Exactly that!
nvidia,3dzq9e,an_angry_Moose,1 point,Mon Jul 20 21:18:20 2015 UTC,"I used to have that problem too, I couldn't overclock at all. I recently reinstalled the drivers using DDU and also uninstalled geforce experience and now I can overclock my 970 g1 to +150mhz."
nvidia,3dx3nw,Sminkiottone,8,Mon Jul 20 06:46:18 2015 UTC,at such high fps it's pretty normal. Just limit your fps with nvidia inspector and you'll be fine
nvidia,3dx3nw,dcdead,6,Mon Jul 20 07:23:44 2015 UTC,"Thanks for the advise, first time I hear about nvidia inspector. will download it ASAP."
nvidia,3dx3nw,jim2point0,1 point,Mon Jul 20 07:37:54 2015 UTC,"I prefer to use Rivatuner as you can set the framerate cap while the game is running and it will update in real-time.  That said, My gigabytes have no coil whine even at 144FPS. So... if coil whine is normal at that framerate the I guess I'm lucky."
nvidia,3dx3nw,frenchpan,1 point,Mon Jul 20 14:19:35 2015 UTC,"I'm no expert, but I don't think coil whine at 144FPS is normal, I'm not even sure how FPS can effect coil whine, but here we are. If someone could enlight me it would be awesome."
nvidia,3dx3nw,flame03,2,Mon Jul 20 14:57:03 2015 UTC,"https://youtu.be/T7HsXHqtxrI  Your gpu is working harder at higher fps, which increases the likely hood the conditions will be met that causes the componets to vibrate, causing coil whine.   Some cards have it worst or are more likely to whine than others."
nvidia,3dx3nw,ariyapl,0,Mon Jul 20 17:17:00 2015 UTC,"It's about the game, if it's a heavy game, it requires a lot of GPU power, which results in a lot of cooling, which results in high fan speeds."
nvidia,3dx3nw,maxbarnyard,1 point,Mon Jul 20 15:32:22 2015 UTC,lucky you! :D mine had at 60
nvidia,3dx3nw,ariyapl,5,Mon Jul 20 17:26:39 2015 UTC,"I had the same issue, only in Far Cry 4's menus & intro screens where it can get up to 2-3k fps. I was able to correct it without having to resort to vsync fps drops by forcing adaptive vsync for the game in the GeForce Control Center. Just make sure you're playing in fullscreen and not borderless and it should prevent the coil whine without impacting gameplay."
nvidia,3dx3nw,p3dr0maz,1 point,Mon Jul 20 10:51:39 2015 UTC,"Your card's fine. Limit your maximum FPS in games with additional software or, if the game supports it, turn v-sync on.   I think you can do it in MSI Afterburner:  https://www.youtube.com/watch?v=1CvoHXzaHKY"
nvidia,3dzj1a,D2Cj,1 point,Mon Jul 20 20:15:38 2015 UTC,"350.12, 353.06 drivers have been giving a lot of people the same issues. .38 fixed it for me, as you said you just started trying it.  Report back!  Edit: Some sources http://forums.overclockers.co.uk/showthread.php?t=18675135"
nvidia,3dzj1a,ChrisJSY,1 point,Mon Jul 20 21:20:48 2015 UTC,"that's good to hear, so far so good at the moment.. I managed to have Netflix on this last hour and I'll leave my PC on tonight and check for BSOD logs in the morning."
nvidia,3dzccm,Strawpedro,2,Mon Jul 20 19:27:56 2015 UTC,"Do you have a 144hz monitor? If so set it to 120hz at the desktop and the gpu will downclock all the way to 135Mhz. To get it to go back to 144hz in games just set the refresh rate option in Nvidia control panel from ""application controlled"" to ""prefer maximum refresh rate"".  Edit: Since I can't read, this can also be caused by high res monitor or 3 screens connected. It has something to do with the pixel clock needed to drive 144hz or multiple displays."
nvidia,3dzccm,kittah,3,Mon Jul 20 19:39:07 2015 UTC,Do you know what the work around is? Can he use the pixel clock driver patcher to lower the clocks or is it only meant to enable custom resolutions?  http://www.monitortests.com/forum/Thread-NVIDIA-Pixel-Clock-Patcher
nvidia,3dzccm,p3dr0maz,2,Mon Jul 20 19:50:52 2015 UTC,"As far as I know the only work around if you have 3 screens is to use Nvidia Inspector to force the power states to toggle which is a huge pain in the ass. Something like this: http://windowedup.blogspot.com/2014/05/how-to-force-specific-p-state-using.html   It's a pain cause you have to click the shortcut every time you want to force the toggle, its not automated.  That pixel clock patcher is designed for increasing the pixel clock to allow you to drive higher resolutions at higher refresh rates than the original clock would allow. Like for people trying to create custom resolutions for downsampling. Honestly not sure if you could use it for forcing downclocking."
nvidia,3dzccm,kittah,1 point,Mon Jul 20 20:00:01 2015 UTC,sorry about the false information the gpu downclocked after i set the refresh rate to 60 hz (120 hz is crazy dark for me idk why)
nvidia,3dzccm,haltdef,2,Mon Jul 20 19:55:37 2015 UTC,ULMB on your monitor perhaps? Should be able to disable it.
nvidia,3dzccm,dragoonjefy,1 point,Mon Jul 20 20:14:09 2015 UTC,"In your NVidia Control Panel, under ""Manage 3D Settings"", what ""Power Management Mode"" are you using? Adaptive, or Prefer Maximum Performance?"
nvidia,3dzccm,StaplerToast420,1 point,Mon Jul 20 20:12:33 2015 UTC,adaptive
nvidia,3dzccm,StaplerToast420,1 point,Mon Jul 20 23:01:21 2015 UTC,"Get Nvidia inspector, activate multi monitor power saving mode."
nvidia,3dz625,minimumfear,1 point,Mon Jul 20 18:42:45 2015 UTC,might just be contrast setting to high.
nvidia,3dz31l,Vince419,1 point,Mon Jul 20 18:22:09 2015 UTC,"BEFORE screenshot: 353.06 drivers  AFTER screenshot: 353.30 drivers  Are both screenshots using the exact same card?  if so, the card ain't your problem."
nvidia,3dz31l,dragoonjefy,1 point,Mon Jul 20 20:10:14 2015 UTC,Then what is?
nvidia,3dz31l,omnicious,1 point,Mon Jul 20 21:13:18 2015 UTC,Seems like it would be the driver.
nvidia,3dz31l,p3dr0maz,1 point,Tue Jul 21 01:51:53 2015 UTC,But I've tried using the older drivers and the new drivers and there is no difference so it's most likely something else.
nvidia,3dz31l,Nyshan,1 point,Tue Jul 21 03:50:05 2015 UTC,"Enjoy your card, man. Don't get so caught up in the numbers as long as it's close. Also the standard for card benching now a days is probably 3D Mark's FireStrike. If you insist on using Unigine's benchmark's then Heaven is superior to Valley."
nvidia,3dz31l,p3dr0maz,2,Mon Jul 20 20:28:31 2015 UTC,"A 7 frame drop in the minimum fps range is definitely a number to worry about. It's basically a 17% drop in performance in the that high intensity spectrum. Altough, as dragoonjefy said, drivers are most likely the problem here."
nvidia,3dz31l,Nyshan,1 point,Mon Jul 20 21:12:56 2015 UTC,I've tried going back to the older drivers. Didn't make any difference.
nvidia,3dz31l,Nyshan,1 point,Mon Jul 20 21:13:55 2015 UTC,I would try a different bench at this point to see if it responds similarly.
nvidia,3dz31l,Syn246,1 point,Mon Jul 20 21:17:57 2015 UTC,"It won't make a difference unless he has prior record of performance in those benchmarks, though."
nvidia,3dz31l,Syn246,1 point,Mon Jul 20 21:22:47 2015 UTC,Did you completely uninstall all your old drivers? Anything and everything related to nVidia?
nvidia,3dz2q7,greg35greg,5,Mon Jul 20 18:19:57 2015 UTC,Any of the EVGA ACX 2.0 or the Gigabyte G1 then. There aren't a million options here.
nvidia,3dz2q7,big_three,1 point,Mon Jul 20 18:25:34 2015 UTC,What about the MSI variant?
nvidia,3dz2q7,p3dr0maz,2,Mon Jul 20 19:22:50 2015 UTC,You can read all the owner threads over at overclock.net for each version.    As far as silicon lottery its pretty random.  I think the G1 guys are getting better luck though for being able to get 1500 on air w/o throttling voltage cause of temps.  I think the concensus is the G1 fans do keep this chip the coolest.    MSI anecdote: I can get 1468.5Mhz @ 1.224V  but at 45c then 55c the stock BIOS Boost 2.0 downvolts (1.205V/1.186V respectively).   Keeping it under 65c is pretty hard on air. Mine went all the way to the mid 70's on Witcher 3 1440p/Vsync off before i threw a water cooler on (kraken g10) and most run about that hot.  With the water cooler i usually run in the mid 50's.
nvidia,3dz2q7,hdshatter,2,Mon Jul 20 19:56:28 2015 UTC,They run cooler if you have a case with good airflow.
nvidia,3dz2q7,p3dr0maz,1 point,Tue Jul 21 01:32:01 2015 UTC,I didnt say it wasn't. You will be throttling all day long above 55c then 65c then at 75c.   I've actually had instabilities at some higher clocks because of the GPU Boost 2.0 behavior dropping volts unexpectedly in some high GPU stressed games like Witcher.    My favorite BIOS is actually one w/o boost and constant forced voltage in 3d mode.  Either way im happy with my card. it didnt go over 1500 like my other last two maxwells but its all good.
nvidia,3dz2q7,hdshatter,1 point,Tue Jul 21 01:34:59 2015 UTC,"Thanks, this will definitely be useful."
nvidia,3dz2q7,djisadud,1 point,Mon Jul 20 22:49:14 2015 UTC,"Why 4790k and not a 5820k if your going to spend that much? The 4790k is in a really bad place right now, its not worth buying over an i5 for gaming and its not worth getting at all if you need performance because the 5820k is barley anymore money for 50% more cores."
nvidia,3dyepr,dayofmone,2,Mon Jul 20 15:31:23 2015 UTC,Same with VLC (670)
nvidia,3dxmv1,minimumfear,1 point,Mon Jul 20 11:22:07 2015 UTC,Re install drivers perhaps?
nvidia,3dxmv1,imadorkx,1 point,Mon Jul 20 11:28:07 2015 UTC,I'll try
nvidia,3dxmv1,imadorkx,1 point,Mon Jul 20 11:31:24 2015 UTC,Try the hotfix drivers. Use DDU too.
nvidia,3dxmv1,Tylerdurden516,1 point,Mon Jul 20 11:34:04 2015 UTC,it's ok now it's not gpu it's skyrim.thanks anyway :) and last can you help me with screen calibration it's just too saturated and the black level is too dark
nvidia,3dxmv1,r0b3r71,1 point,Mon Jul 20 12:13:01 2015 UTC,I cant run skyrim on my gtx 980 ti either. Its a buggy disaster.
nvidia,3dxmv1,Tylerdurden516,1 point,Mon Jul 20 13:43:44 2015 UTC,"Seriously? Oh shit, I just ordered my 980 TI to play skyrim..."
nvidia,3dxmv1,kittah,2,Mon Jul 20 17:43:32 2015 UTC,Well I hope it works for you.
nvidia,3dxmv1,r0b3r71,2,Mon Jul 20 18:03:39 2015 UTC,"Skyrim shits the bed at > 60 fps. So while possible to uncap the fps it's not a good idea since the physics go crazy.   This is a friend's vid showing what happens in skyrim at 144 fps: https://www.youtube.com/watch?v=rrQ_AyFniao  Stuff bouncing everywhere, water flashing in and out of existence, skydiving chickens, etc.   So long as you leave the cap in place it should work just fine with your 980 Ti."
nvidia,3dxmv1,AlphaSentry,1 point,Mon Jul 20 18:12:30 2015 UTC,that's so weird. Hopefully it will be patched...
nvidia,3dxmv1,r0b3r71,1 point,Mon Jul 20 20:31:08 2015 UTC,"It's been a known issue from launch, at this point it'll never be fixed.  Edit: I also got a 980 Ti for Skyrim and it runs flawlessly on mine with ENBs, and a ton of high res texture mods and such, so nice looking now"
nvidia,3dxmv1,hdshatter,1 point,Mon Jul 20 21:59:11 2015 UTC,"I can't wait to get mine, I ordered it yesterday with rush processing and 1 day shipping, so I should have it tomorrow (hopefully!) can't wait to try out all my games."
nvidia,3dxmv1,therusskiy,1 point,Mon Jul 20 22:16:45 2015 UTC,Delete renderinfo.txt and check the ini to make sure its set to a 980ti.
nvidia,3dwvvn,playingwithfire,2,Mon Jul 20 05:10:45 2015 UTC,"The only ""Optimal"" smoothness factor is the one you like. It's entirely personal taste. I personally like the smoothness factor a little higher as I don't mind a soft image for general enjoyment-play, but anything where I need to focus more like first person shooters, I'll dial it down to be clearer slightly. But the difference is pretty small typically I find."
nvidia,3dwvvn,SirCrest_YT,1 point,Mon Jul 20 05:18:54 2015 UTC,Good to know. Even at 0% smoothing I don't find the image to be unpleasant unless I'm looking at a static image. And it fixes most jaggies better than AA I think.
nvidia,3dwvvn,sn1313,2,Mon Jul 20 05:20:13 2015 UTC,"I keep mine right around 18-22%. I like a smooth/soft edge appearance, rather than a sharp one. It changes little for me."
nvidia,3dwvvn,slykrysis,1 point,Mon Jul 20 05:35:58 2015 UTC,I'm guessing the first pic in the album is the DSR one?
nvidia,3dwvvn,DonDalle,1 point,Mon Jul 20 13:04:37 2015 UTC,Yessir
nvidia,3dwvvn,DonDalle,1 point,Mon Jul 20 14:22:46 2015 UTC,"When using a 1440p Monitor, can I use DSR to downscale 4k or even 5k?"
nvidia,3dwvvn,DonDalle,1 point,Mon Jul 20 14:04:33 2015 UTC,Yes you set the scale. 4x would be ideal but you can't get good fps at that resolution. So 2x with some smoothing is probably fine.
nvidia,3dx1ha,minimumfear,4,Mon Jul 20 06:18:01 2015 UTC,nvidia inspector?
nvidia,3dx1ha,fuccboi9000,2,Mon Jul 20 06:41:38 2015 UTC,"I would say it would be Adaptive VSync. It's a vsync that turns it self on and off depending on the frame rate you are getting, if you are hitting the targeted limit. Adaptive VSync  There is also a frame rate limiter that can be set through nvidia inspector that will let you control the frame rate your card is displaying."
nvidia,3dx1ha,shuggatank,1 point,Mon Jul 20 08:13:43 2015 UTC,"Available through Nvidia Inspector, EVGA Precision and MSI Afterbuner.  Probably others, too."
nvidia,3dx1ha,I_Will_be_Nice,1 point,Mon Jul 20 08:20:19 2015 UTC,Frame rate target control is something they exposed to third parties like EVGA Precision. They introduced it back with the 680.
nvidia,3dx1ha,DRW_,1 point,Mon Jul 20 08:26:55 2015 UTC,"Nvidia already supports it, its just not in the NVCP. You have to use a 3rd party program to use it. Was usable on AMD before the driver for years with Radeon Pro. It is not a new feature."
nvidia,3dx1ha,hdshatter,1 point,Mon Jul 20 08:34:25 2015 UTC,"GPUTweak can do it too, there's a frame rate target option. I don't use it personally but I've tried it a few times and it seems to work as intended."
nvidia,3dxzv1,danhufc,1 point,Mon Jul 20 13:34:31 2015 UTC,Is it stock clocks?  Sounds like what happens when you try a OC that's not stable.
nvidia,3dxzv1,p3dr0maz,1 point,Mon Jul 20 14:14:41 2015 UTC,The Zotac comes OC'd by default. I haven't changed anything but running HDMI with the same clocks doesn't have any problems.
nvidia,3dxzv1,p3dr0maz,1 point,Mon Jul 20 14:18:28 2015 UTC,"Strange,  You'll have to try some stuff.  Maybe Try AnotheR cable and adapter"
nvidia,3dxwdy,spark_soul,6,Mon Jul 20 13:02:01 2015 UTC,"No issues for me, EVGA 980 Ti SC with backplate. 353.30 driver, no TDR's, overclocked +150 on core and +300 on memory."
nvidia,3dxwdy,slykrysis,3,Mon Jul 20 13:05:28 2015 UTC,No issues on 353.49.
nvidia,3dxwdy,hdshatter,2,Mon Jul 20 13:06:51 2015 UTC,"acx 2.0 arrived yesterday and works perfectly, silently in fact."
nvidia,3dxwdy,slakex,2,Mon Jul 20 14:38:56 2015 UTC,"Besides coil whine I've heard no issues on the hardware side, infact I've heard such good things. Drivers though seem iffy.  My G1 arrives today though. Stoked."
nvidia,3dxwdy,SirCrest_YT,2,Mon Jul 20 14:48:27 2015 UTC,"Mine arrives tomorrow, how the hell do i sleep :("
nvidia,3dxwdy,ariyapl,3,Mon Jul 20 14:56:46 2015 UTC,Don't. Sleep is for the weak.
nvidia,3dxwdy,SirCrest_YT,1 point,Mon Jul 20 18:05:47 2015 UTC,I'M AWAKE.
nvidia,3dxwdy,SirCrest_YT,1 point,Mon Jul 20 18:35:54 2015 UTC,Did it arrive btw?
nvidia,3dxwdy,SirCrest_YT,1 point,Mon Jul 20 18:36:08 2015 UTC,"Did around 1pm, but I'm still stuck at work for another hour.   It's... agony."
nvidia,3dxwdy,sluflyer06,1 point,Mon Jul 20 21:30:05 2015 UTC,I'm still awake....i still have hours to go :(  Post pics? :D
nvidia,3dxwdy,Ohmps_,2,Mon Jul 20 22:24:40 2015 UTC,"I'll post some pictures when I get home, and maybe on PCmasterrace for that sweet karma.  ""I have ascended to another level"" or something. I'm coming from a pair of MSI GTX580 3GB Lightning Extreme Editions. And when I got the pair of those for $1200 it felt fucking incredible to play everything on highest settings easily, excited to experience now double that in one card basically."
nvidia,3dxwdy,Ohmps_,1 point,Mon Jul 20 22:31:51 2015 UTC,Do let me know !
nvidia,3dxwdy,bp0017,2,Mon Jul 20 22:36:04 2015 UTC,"OK first impressions, no coil whine (Thank God). It's basically hidden behind your computer sound below 60% fan speed.   I just threw it to 110% Power Limit, +65 Mhz core, and that was it and I'm boosting to 1400 consistently in BF4. The lowest the FPS ever got was 95fps on 2560x1440, Ultra Preset, 90Fov on a 64person Lockers Conquest. Often it was sitting around maybe 130-150fps, which fucking blows my mind.  Never went over 70c, fan hit 55% speed and I barely hear it (if that's even the card at all) over my extremely quiet computer. I'm honestly CPU imited in this case with my 3770k.  So far, extremely satisfied. Amazing how it replaced two of the most powerful cards on the market at their time. Annnndddd it uses much less power.  And FYI, I got a 67.5% ASIC, bit low compared to many, but I'm at 1.4ghz, which is still pretty decent."
nvidia,3dxwdy,solidnitrogen,1 point,Tue Jul 21 01:47:38 2015 UTC,What would even give you the impression it was flawed? I haven't seen a single widespread issue from any vendor.  I have 2 980ti's and both are perfect.
nvidia,3dxwdy,solidnitrogen,1 point,Mon Jul 20 13:24:31 2015 UTC,"I got a 970, and later we found out the video ram issue."
nvidia,3dxwdy,solidnitrogen,2,Mon Jul 20 13:35:20 2015 UTC,"Jayztwocents looked it up directly,full 6gb"
nvidia,3dxwdy,solidnitrogen,1 point,Mon Jul 20 14:01:03 2015 UTC,link?
nvidia,3dxwdy,solidnitrogen,1 point,Mon Jul 20 14:04:18 2015 UTC,was in one of his 980ti videos on youtube first or second
nvidia,3dxwdy,Aaldenberg,1 point,Mon Jul 20 15:09:21 2015 UTC,"Nope, acx 2.0 runs perfect"
nvidia,3dxwdy,Neumayer23,1 point,Mon Jul 20 13:25:23 2015 UTC,"Had some cool whine on my first gtx 980ti sc reference. Had to rma it , new one is perfect!"
nvidia,3dxwdy,durin86,1 point,Mon Jul 20 13:31:33 2015 UTC,"So would you recommend a reference?  I have a bitfenix prodigy m as a case, my older 970 used to run hot at times. http://imgur.com/a/MMnGy  this is the previous GPU and how it sits in the case."
nvidia,3dxwdy,ariyapl,2,Mon Jul 20 13:37:56 2015 UTC,"Sweet setup man. Your 970 had a cooler that didn't vent out of the case.  I have an even smaller case (evga hadron)  and my reference 980ti runs well within limits, I usually keep my fans at 60% and its stays in the low 70's at full load overclocked.  I've been running reference collets since I got my hadron, my 780ti got way hotter but I had no issues, the 980 was significantly cooler and my 980ti is in between the two.  I used an acx 980 for a week before I decided to swap for a reference because of the way the heat builds up."
nvidia,3dxwdy,ariyapl,1 point,Mon Jul 20 13:45:47 2015 UTC,So you'd prefer a reference if you had my kinda setup?
nvidia,3dxwdy,MadCuzBadThusSad,2,Mon Jul 20 13:47:48 2015 UTC,Definitely!
nvidia,3dxwdy,p3dr0maz,1 point,Mon Jul 20 13:51:57 2015 UTC,Thank you ! :)
nvidia,3dxwdy,p3dr0maz,1 point,Mon Jul 20 14:04:33 2015 UTC,Welcome!  I couldn't recommend this card enough. And evga has excellent service for replacments and everything.
nvidia,3dxwdy,p3dr0maz,1 point,Mon Jul 20 14:08:40 2015 UTC,"Well, im getting a zotac reference card, but all reference are the same yeah?"
nvidia,3dxwdy,p3dr0maz,2,Mon Jul 20 14:11:54 2015 UTC,Yup.
nvidia,3dxwdy,p3dr0maz,1 point,Mon Jul 20 16:18:44 2015 UTC,Thank you !
nvidia,3dxwdy,Icanhaswatur,1 point,Mon Jul 20 16:49:07 2015 UTC,"I heard some people had issues, but I didn't try to look out to know more about it (so I'm not sure if it was the driver or something else)"
nvidia,3dxwdy,Zinthros,1 point,Mon Jul 20 13:36:26 2015 UTC,"MSI 980 Ti Gaming, looks great, performs even better, silent as death itself."
nvidia,3dxwdy,kmarsara,1 point,Mon Jul 20 17:00:48 2015 UTC,Same here. OC'd to +120 core +300mem with no voltage increase
nvidia,3dxwdy,shatteredhalo0,1 point,Mon Jul 20 22:08:03 2015 UTC,"Gigabyte G1 Gaming - terrible coil whine. RMA'ed it, waiting for a new one."
nvidia,3dwxi7,Troll_Trollington_,5,Mon Jul 20 05:29:50 2015 UTC,"You will find few games that will max out your VRAM.  The heaviest game for me is GTA V, with a mixture of Very High/Ultra. I used Afterburner/Rivatuner to monitor the usage and when I was driving really fast, on the highways. It will go up to around 3400 max.  Most other games stay well below 3GB VRAM. Some games, such as Alien Isolation (a very optimized, yet beautiful game), barely hit 1.5 GB VRAM.  One thing to consider though, is if you are installing huge texture mods or similar. These will eat into your VRAM. That is not to say though, that you can't have Skyrim looking absoltely sexy.  I've never used AMD/Radeon cards, so I cannot say how good or bad that they are. I can only echo what reviewers and users have said. Most have said that the 390 is a great card and trades blows with the 970. Sometimes it even beats it. Everything points to it being, with the 970, the best card you can get for the money.  They are both good cards and you cannot go wrong with either. 390 has the advantage because of more VRAM but most games don't go above 3.5GB and when they do, it is mainly because you have a monitor with a very high resolution."
nvidia,3dwxi7,CaptSkunk,1 point,Mon Jul 20 07:00:05 2015 UTC,Basically. Unless you plan on running skyrim with all sorts of crazy 4K texture.
nvidia,3dwxi7,StillAzure,3,Mon Jul 20 11:37:32 2015 UTC,"Well, there are some people out there that could bring a Titan X to it's knees, with their crazy mods."
nvidia,3dwxi7,CaptSkunk,1 point,Mon Jul 20 12:09:32 2015 UTC,"Speaking of, I'm actually very curious about the VRAM usage when using tons of 4K texture."
nvidia,3dwxi7,StillAzure,2,Mon Jul 20 16:53:20 2015 UTC,I'm not much for modding so I think I will be ok with the 970 then. I overclocked mine to 1530 mhz and by the time I get a 1440p monitor I will probably buy a new gpu anyway. Also the lower heat output and power draw is a plus as well
nvidia,3dwxi7,StillAzure,1 point,Mon Jul 20 14:16:04 2015 UTC,"Perhaps so, although 1440p will still be playable with a 970."
nvidia,3dwxi7,Finalwingz,1 point,Mon Jul 20 16:49:26 2015 UTC,R9 390 is a better card for a better price.
nvidia,3dwxi7,CaptSkunk,2,Mon Jul 20 15:39:12 2015 UTC,"They are both great cards. They, along with the 980ti and Fury X, are the best cards you can get for their respective price/performance ratios.   If I did not use Linux (and game on it), I'd look at a 390. Sadly though, the drivers are just plain horrible on Linux. The Windows drivers are good though and the cards from AMD are great.   I'll be looking at GPU's in another two years. Just bought a 970."
nvidia,3dwxi7,Finalwingz,1 point,Mon Jul 20 16:04:19 2015 UTC,"I agree, they're both great but it's just not logical to buy a 970 anymore unless you can get it for around 300."
nvidia,3dwxi7,Kohvwezd,1 point,Mon Jul 20 23:01:45 2015 UTC,"No, probably not. I myself have a 970, haven't seen any games really push it to the limits. I'll probably upgrade to the Pascal flagship as soon as it's out, so it won't become a problem in the future either. Unless, you know, Nvidia fucks up twice."
nvidia,3dwxi7,Dommy73,1 point,Mon Jul 20 10:45:08 2015 UTC,Firggin' CoD: AW dips into it on some maps.
nvidia,3dwxi7,vector_control,0,Mon Jul 20 12:29:43 2015 UTC,"Just upgraded to the 970 last week (EVGA SSC Edition) from a MSI 660. The VRAM does affect some games, as mentioned a heavily modded skyrim can slow down a bit. It doesn't really hinder the game play experience, IMO. But the FPS does drop down and you will notice it in scenes with lots of buildings and textures. Plays SC2 on ultra, company of heroes 2 on very high/ultra, GTA V on very high. It is still a beast of a card, even with the vram issue. Editing multiple 4k photoshop files at once and video editing in 4k.  But, yes, it will hinder some newer games on ultra. Still a very solid card, IMO.  edit: This is all at 1080p"
nvidia,3dwxi7,fortean,1 point,Mon Jul 20 14:10:00 2015 UTC,"For your information, your fps dips are because of your underpowered CPU, and not because of any limtiation on the part of the gpu."
nvidia,3dwxi7,vector_control,1 point,Mon Jul 20 19:58:49 2015 UTC,Do you think getting an 8350 would get rid of that problem? That will probably be my next upgrade.
nvidia,3dwxi7,fortean,1 point,Mon Jul 20 20:11:36 2015 UTC,"I had a 8350 before buying this i5, and before that I had a 6300. The difference between the 8350 and the 6300 was obvious, but not earthshattering. I was only playing wow at the time, I got like 15% more fps. I only used amd up to a month ago when I switched to intel, and the performance difference is enormous.   Having said that, if you can get a 8350 (or a 8320, very decent cpu) for cheap go for it, you'll definitely see a difference."
nvidia,3dwxi7,vector_control,1 point,Mon Jul 20 20:17:47 2015 UTC,"Cool, thanks for the info. 8320 looks like a great deal."
nvidia,3dwxi7,fortean,1 point,Mon Jul 20 20:22:15 2015 UTC,"No worries. Keep in mind that the 4690k is an absolutely insane upgrade, so only go for a 8320 or 8350 if you aren't changing motherboard. It really pains me to say this, since I only used amd since the k6, but it's the truth unfortunatley."
nvidia,3dwxi7,SpaceCowboy26,1 point,Mon Jul 20 20:56:50 2015 UTC,Get an unlocked Intel processor and smile as it outperforms AMD's offerings for many years to come. I have an i5-2500k from early 2011 overclocked to 4.5GHz and it still performs better (in games) than anything AMD can currently offer. I expect it will still be sufficient and not bottleneck the rest of my rig for a few more years yet.
nvidia,3dwxi7,SpaceCowboy26,2,Mon Jul 20 23:37:26 2015 UTC,"I bought a 970 about two and a half months ago. I also have a 1080p monitor. The two most demanding games I'm playing at the moment are GTA V and Dying Light.  I can turn all of the settings in GTA V up to max, excluding:   MSAA, which I leave at 2x and combine with FXAA, instead of a max of 8x Post processing, which I leave at high instead of very high All advanced graphics options, which I leave off   This gives me a solid 60fps in the city but when I go out near the mountains it fairly consistently dips below that (probably because of grass, but I don't want to turn it down because it looks so good). I hate screen tearing so deal with vsync and only having 30fps sometimes.  I have looked at the memory usage a few times while playing GTA V and have seen it go above 3500MB a few times, but didn't really notice any stuttering.  Dying Light, I'm able to max out completely (apart from shadow map size being one notch below max) and maintain a constant 60fps everywhere, no performance hiccups at all.  Overall, I'm very satisfied with how the card is performing at the moment. I knew I wasn't going to be able to max GTA completely with it but didn't want to fork out extra dosh to buy the 980 as it didn't represent as much value for money as the 970 and is probably slight overkill for 1080p in most games anyway.  I'm not sure if it will still be doing as well in a couple of years even at the same resolution; the memory requirement for games only seems to be going up and I'm think a few big releases will push that 3.5GB barrier a bit more.  I don't regret my purchase one bit and the 970 is a great card but  I think the 390 is a better choice for most people. As long as you don't care about it running hotter and consuming more power I think it really is the smarter choice. For that exact reason I would personally not choose it (I prefer having a cooler and more power efficient card because Australian summers get a bit hot). 8GB of VRAM is plenty and in a few years if the memory is not what is holding it back you could easily crossfire to regain some performance."
nvidia,3dwxi7,ChikNoods,1 point,Mon Jul 20 14:28:17 2015 UTC,1080p@144 never seen one game come close on my 970. I play everything on highest possible graphics. All blizzard games mostly. Metro franchise and popular valve games
nvidia,3dwxi7,S00L0NG,1 point,Mon Jul 20 15:02:18 2015 UTC,"If your going to upgrade to a 1440p eventually and will be keeping the same card then get the 390 otherwise there is not much in it  but i would go with the 390 anyway because you never know the future and it is not like you can add more Ram to a GPU. I have had troubles with AMD drivers in the past so switched to Nvidia but there drivers since 350 seemed to be just as bad, so you can not win."
nvidia,3dwxi7,parasemic,-2,Mon Jul 20 06:09:28 2015 UTC,"A few games may max out today (GTAV, shadow of mordor, skyrim modded etc) and future is a huge questionmark. Biggest question is if DX12 will be widely adopted and SLI 970 becomes viable option.   Thank god all of those games are boring trash.. :D  Either way, 970 isnt a good long term card without SLI and 390 is better in every way  E: I own a 970 but will upgrade within next 6 months"
nvidia,3dxtnb,Twixes3D,4,Mon Jul 20 12:34:32 2015 UTC,Simple answer to the title of post: No.  Are you going to stare at the inside of your case often? Then Yes get a backplate.
nvidia,3dxtnb,Kameezie,0,Mon Jul 20 12:52:24 2015 UTC,"Meh, I like the look of the motherboard and graphics card PCBs meshing."
nvidia,3dxtnb,ExplosiveNutsack69,1 point,Mon Jul 20 17:20:03 2015 UTC,Example?
nvidia,3dxtnb,hdshatter,1 point,Mon Jul 20 12:57:36 2015 UTC,iGame or a Strix.
nvidia,3dxtnb,Racoonslikepuzzles,1 point,Mon Jul 20 12:58:41 2015 UTC,"I don't think you need one, but it definitely looks nice."
nvidia,3dxtnb,Vocalifir,1 point,Mon Jul 20 13:00:55 2015 UTC,no
nvidia,3dxtnb,Cereaza,1 point,Mon Jul 20 13:06:54 2015 UTC,"Backplates are aesthetic. They're not heatsinks or stabilizers or anything. Unless someone is attacking the back of your GPU with a knife, the backplate's only real purpose is ""ooooh"". It hides the PCB."
nvidia,3dxtnb,Leonick91,1 point,Mon Jul 20 13:31:30 2015 UTC,"No. For the most part they are just aesthetic as people have pointed out. Now, backplates can help hold the card up and prevent the sag you often see on long cards but on the MSI GTX 970 Gaming 4G that's solved by a piece of metal between the card and the heatsinks. Not sure if that's the primary purpose of that piece or if it's part of the cooling but it makes the card quite sturdy and eliminates any need for a backplate, besides aesthetics of course."
nvidia,3dxtnb,p3dr0maz,1 point,Mon Jul 20 18:15:52 2015 UTC,Its easier to wipe dust off a backplate than it is to do it to a PCB with all the little connectors on there. Plus it protects it even if just a little bit.
nvidia,3dxtnb,StayFrostyZ,0,Mon Jul 20 19:51:21 2015 UTC,Do note to route your PCIe cables so that it lifts the card upward since the MSI GTX 970 4G Gaming does sag quite a bit
nvidia,3dxtnb,ExplosiveNutsack69,0,Mon Jul 20 20:05:32 2015 UTC,No it doesn't.
nvidia,3dxtnb,StayFrostyZ,0,Mon Jul 20 14:25:21 2015 UTC,Congrats. You're one of the lucky ones who doesn't have this issue while many of us do. Don't believe me? Plenty of others have the same complaint on google and YouTube.
nvidia,3dxtnb,ExplosiveNutsack69,-1,Mon Jul 20 17:21:00 2015 UTC,"Considering how difficult it was for me to find anyone complaining about it on YouTube, and the fact that the only complaints I could find on Google were just a few extremely random tech forums, and since most of those results weren't even MSI cards, I'm inclined to say either you installed it incorrectly or you are very unlucky* (oops)."
nvidia,3dw2qt,floydian32,2,Mon Jul 20 00:37:40 2015 UTC,Can't wait for it's implementation to Killing Floor 2
nvidia,3dw2qt,HawkEye0,1 point,Mon Jul 20 01:27:20 2015 UTC,Does this launch without an NVIDIA GPU and run off the CPU?
nvidia,3dw2qt,TheDark1105,1 point,Mon Jul 20 04:18:38 2015 UTC,"Parts of this slow my 970 to a crawl. If you're trying to run it off your CPU, good fucking luck"
nvidia,3dw2qt,JJSCHMITTY,1 point,Mon Jul 20 05:13:55 2015 UTC,"Nah, I have a 770 (at least until my Fury arrives) so it runs pretty good. Was just curious if it runs at all without an NVIDIA GPU. I would guess no."
nvidia,3dw2qt,TheDark1105,1 point,Mon Jul 20 14:20:55 2015 UTC,Knowing the current state of nvidia libraries.... No
nvidia,3dw2qt,Archmagnance,1 point,Mon Jul 20 19:58:54 2015 UTC,I didn't have any problems with any of them on my Titan x. They were all very smooth.
nvidia,3dx4xo,killerkid745,5,Mon Jul 20 07:02:49 2015 UTC,"You are using 2 different driver versions. Also, you might have had some malware on your PC before reinstall."
nvidia,3dx4xo,scarystuff,1 point,Mon Jul 20 08:09:58 2015 UTC,"What low usage issues? I pulled out my 780, popped in 2 980tis and hit power, didn't even need to install new drivers. :)"
nvidia,3dx4xo,sluflyer06,1 point,Mon Jul 20 13:07:34 2015 UTC,You should always reinstall your OS before saying that something is broken.
nvidia,3dv2to,8sdh0ah80dash80asd0h,2,Sun Jul 19 19:30:19 2015 UTC,If you want to keep a constant 144 frames the 980 ti would be your best bet. The 980 will keep you above 60 though most likely. Evga makes some nice models with closed loop water cooling systems if that's your forte.
nvidia,3dv2to,mynis,1 point,Mon Jul 20 04:54:56 2015 UTC,"If I were you, I'd stick with your current setup until pascal cards (GTX 1k series or w/e) is released if you really want to go for 2k (1440p) gaming.    I myself don't see the point of going beyond 1080p until GPU technology advances to the point where I can get high fps on a 2k/4k monitor as I do on my 1080p monitor and not have to upgrade to a new card until several years. Perhaps Pascal will do that since it's supposedly 10x better than the current series. A GTX 980 is pretty OP right now for a 1080p which is awesome because you can skip a couple of series.   Btw, I have a GTX 780 SC from EVGA with a 1080p 144hz G-Sync monitor. The G-Sync monitor helps with the waiting.    EDIT: Take this with a grain of salt because I never witness a 2k or 4k resolution monitor/tv in person but I'm sure there would be a noticeable difference between 1080p and 4k but not 1080p and 2k. Stick with a 1080p or your current resolution, or head straight to 4k."
nvidia,3dv2to,auraofjason,5,Sun Jul 19 19:57:40 2015 UTC,2560x1440 is almost twice the pixels of 1920x1080.. (~3.7m vs ~2m). There's a pretty huge difference there.
nvidia,3dv2to,sluflyer06,-7,Sun Jul 19 21:55:28 2015 UTC,"On paper, yes. Visually, doubt it.  EDIT: We're talking about monitors, not TVs... EDIT: I guess I was wrong and was being an ignorant, apologies."
nvidia,3dv2to,420LaysIt,7,Mon Jul 20 00:05:07 2015 UTC,"Wrong, night and day 1080 vs 1440. Like really really instantly noticeable difference.   Everyone I've ever known that's made the jump is extremely happy they didnand said they would never go back.  I've used both extensively."
nvidia,3dv2to,420LaysIt,4,Mon Jul 20 03:01:36 2015 UTC,"Then clearly you've never used beyond 1080p. Yes, your frame rate will be marginally lower depending on your GPU but assuming you're using a 60Hz monitor, anything beyond 60FPS makes no difference. A GTX 980 should fairly easily be able to achieve at least 60 FPS in 1440p in most modern titles. That being said, your point stands if you were speaking of an ROG Swift-esque monitor, with both 144Hz and 1440p as most cards will not achieve 144FPS in 1440p on ultra settings."
nvidia,3dv2to,steak4take,1 point,Mon Jul 20 01:09:13 2015 UTC,"You're right, I never used beyond 1080p. Games will become more demanding which is why you can't keep your current card forever. The point is, in the long run, the card will last you longer if you were to choose 1080p over QuadHD. Reason I said 4k because that would be more noticeable in sharpness compared to buying a 2k. Also, lets add the fact that you don't notice the fps with G-Sync until it hits 30 and below."
nvidia,3dv2to,Cereaza,1 point,Mon Jul 20 02:54:00 2015 UTC,"Then buy a 4K monitor, use it until your card can no longer handle it then adjust the output resolution. It is adjustable, you know-the resolution given when purchasing a monitor is the maximum resolution, not the only resolution. Yes, true a card will ""last longer"" but if you have the money for a 4K monitor in the first place, chances are you will have the money to upgrade your GPU down the line."
nvidia,3dv2to,steak4take,1 point,Mon Jul 20 03:22:37 2015 UTC,"It all comes down to his budget and whether or not he prefers visualization, performance or both."
nvidia,3dv2to,steak4take,4,Mon Jul 20 03:26:40 2015 UTC,"""1080p is twice the pixels of 720p.  That's a pretty huge difference there.""  ""On paper, yes. Visually, doubt it"""
nvidia,3dv2to,Cereaza,3,Mon Jul 20 01:37:22 2015 UTC,"720p is twice the pixels of 480p. That's a preyyt huge difference there.   On paper, yes. Visually, doubt it"
nvidia,3dv2to,Cereaza,0,Mon Jul 20 01:53:41 2015 UTC,"Let me put it to you this way, the higher the value, the less noticeable it seems, for example, fps. You look at 30 fps vs 60 fps and you can tell the difference, however, you notice less the higher the value goes to the point where it doesn't really matter anymore. The same can be said about resolution; the increments in sharpness will only be low from this point on that you will barely notice it. Also, let me remind you that we are talking about monitors and not TVs."
nvidia,3dv2to,mynis,4,Mon Jul 20 02:49:10 2015 UTC,Get glasses.
nvidia,3dv2to,sluflyer06,0,Mon Jul 20 02:54:25 2015 UTC,Apparently you play with your face against the screen.
nvidia,3dv2to,Galactic_Nightmare,6,Mon Jul 20 03:07:00 2015 UTC,Apparently you comment with your head up your arse.
nvidia,3dvm8z,64Bit_Is_Da_Shit,24,Sun Jul 19 22:15:04 2015 UTC,Be careful that you don't pick wrong. An 0.5 FPS difference on average is the difference between a great experience and UNPLAYABLE UTTER SHIT
nvidia,3dvm8z,masturbateAndSwitch,6,Sun Jul 19 22:58:34 2015 UTC,I know your joking but things like frame times and low minimum fps can make a big difference between cards that bench the same avg fps.
nvidia,3dvm8z,BlayneTX,2,Sun Jul 19 23:42:19 2015 UTC,LMAO!  Literally unplayable
nvidia,3dvm8z,flashen,15,Mon Jul 20 06:57:47 2015 UTC,Fury X in crossfire beats even the Titan X in SLI. The 980ti is better as a single card though.  Source
nvidia,3dvm8z,Tinyds,12,Mon Jul 20 00:57:27 2015 UTC,The problem with the Fury Crossfire tests is that most were done without the newly released 15.7 driver which included a lot of Crossfire improvements. This one is no exception.   So the Fury Crossfire should be even further ahead now.
nvidia,3dvm8z,Smagjus,3,Mon Jul 20 01:32:47 2015 UTC,Generally what I have seen is stock the Fury X in Crossfire beats the 980ti however when you overclock the 980 ti is very close to the Fury X since generally crossfire scales better but the 980ti's have fewer dips in Fps which gives a smoother exoerience.
nvidia,3dvm8z,jcgill,1 point,Sun Jul 19 23:09:57 2015 UTC,"Personally, for me the decision came down to knowing the 980Ti had a high overclock ceiling, and the FuryX had an Unknown ceiling (and right now it's very low in OC performance). Also, G-sync capable monitors in my spec range (1440@144 IPS) were more available.   On top of that, I have Terrific airflow in my case (four 200mm fans, with zero interior cage drives), so I know i can keep my blower cards cool.   So, I went with the 980Ti. I'll be getting another 4K G-sync IPS 60hz panel when one comes out thats big enough to justify 4K for me (30+ inches)."
nvidia,3dvm8z,Cereaza,2,Mon Jul 20 01:29:23 2015 UTC,"Crossfire scales better but they are still dealing with worse frametime numbers, last I heard. In that department, SLI is much more stable. That is another thing to take into consideration."
nvidia,3dvm8z,Sethos88,-4,Mon Jul 20 00:46:29 2015 UTC,"Yep with their worse frame pacing, crappier driver support, and only having 4GB VRAM per card? I wouldn't be choosing a Fury X right now.   Edit: I made the AMD crowd mad...ooops"
nvidia,3dvm8z,Soulshot96,5,Mon Jul 20 01:12:40 2015 UTC,"For the record, have a read through /r/advancedmicrodevices and /r/nvidia, then tell me who is dealing with more driver issues."
nvidia,3dvm8z,an_angry_Moose,-2,Mon Jul 20 02:37:17 2015 UTC,"Well, you see, I use Nvidia drivers, and 3 of my friends use them too. We all stay up to date with driver updates, and the worst we have had to deal with in the last month was a few TDR's in Chrome. Which can be resolved by disabling GPU acceleration, downloading the hotfix, or dealing with 1-2 TDR's on your desktop a month. Its a fine tradeoff imo versus waiting weeks or months after a game releases for AMD to update their drivers for it. And in the case of games like GTA V, still not fix launch bugs like MSAA causing artifacts. But lets ignore all that, and let you reading through an echo chamber decide. also, keep in mind that their are over 3000 more users here than on AMD's sub, so more people to complain, and more echo."
nvidia,3dvm8z,Soulshot96,4,Mon Jul 20 02:41:44 2015 UTC,N=4
nvidia,3dvm8z,an_angry_Moose,1 point,Mon Jul 20 03:40:43 2015 UTC,Actually Nvidia drivers have been quite crappy for me for the past three months while AMD's 15.5 & 15.7 drivers were excellent in the short time I had the Fury X. I also hear that AMD has been stepping up their driver game and releasing faster drivers. I am installing the 353.38 hotfix as we speak to stop this stupid crashing.
nvidia,3dvm8z,StayFrostyZ,2,Mon Jul 20 12:12:41 2015 UTC,"Idk, hasn't been my or my friend's experience."
nvidia,3dvm8z,Soulshot96,-8,Mon Jul 20 12:25:37 2015 UTC,"Obv hasn't read anything about how hbm works, its = to 6-8 gigs of gddr5 which puts it even or ahead of the 980 ti. Second you must not own a 980 ti or not even read /r/nvidia but our driver support is total shit also atm. I read multiple threads ranging from game crashes from alt tabing to bsod for no reason. Before you call me a amd fanboy, I own the only current 980 ti overclocking thread on the sub reddit and also bought a 980 ti g1 gaming the day it came out. Please get informed before you make posts like yours, they just make your look stupid ""omg it only has 4 gigs of ram its total shit"" paraphrased."
nvidia,3dvm8z,andrew5161,1 point,Mon Jul 20 01:52:59 2015 UTC,"can people who have no idea how hardware functions stop spreading this  4gb hbm = 6-8 gb ggdr5 thing jesus. one gb is one gb, regardless of how you access it and regardless of how you optimize memory paging."
nvidia,3dvm8z,methcurd,1 point,Mon Jul 20 09:48:46 2015 UTC,"Ok, I keep hearing this bullshit about how HBM is somehow used less than GDDR5, yet I have already seen a review for the Fury X where if runs out of VRAM. Specifically in GTA V, where my card does not. And I would not call Nvidia's driver support shit, sure there is a single TDR bug with chrome, but there are 3 workarounds to that, that I know of alone. Hardly a big issue. Not only that, but they are actively working on that problem, working on Windows 10 drivers, and updating the current set for new games and adding SLI profiles. Far cry better than AMD's current driver situation in my book, as I have already found plenty of people who can't even get the new AMD drivers to install."
nvidia,3dvm8z,Soulshot96,-1,Mon Jul 20 02:03:25 2015 UTC,"Please go read read and get informed, literally everything you said is untrue. Some how amd and nvidia both having driver issues that their working on comes out to amd being satan and nvidia being ""satanized by the amd fanboys"". Every company has their flaws, but blowing them out of proportion is dumb. Speaking of which, everyone was pissed about amd having cards that run hot, the fury x runs 10-15 c lower then the x/980 ti, where is all the rage about nvidia having hot cards? Oh no amd used a water cooler, why didn't nvidia their cards are the same price point and very similar performance. +/- 10 % depending on settings and source.  Edit: If hmb isn't better at higher res, why is nvidia using it on their pascal?"
nvidia,3dvm8z,andrew5161,3,Mon Jul 20 02:15:24 2015 UTC,"Sir you are either a troll or a so misinformed it's sad. Read not skim because everything you are saying is horse shit. Every review I have read points out the simple fact that. HBM memory although faster does not mean you need less. Many reviews have stated not only that but also this. GDDR5 is not this generations bottleneck, Remember read don't skim, skimming is bad."
nvidia,3dvm8z,Griffith1984,3,Mon Jul 20 02:57:07 2015 UTC,"What I said about frame pacing is perfectly true, what I said about driver support, even now, is still true. And as far as VRAM is concerned, I don't give two fluffy fucking shits about how efficient HBM is, when a 980Ti can play a game with higher settings than a Fury X, without running out of VRAM already, what the fuck does that say for the future? Games are going to start using more, and if 4GB of magical HBM is already not enough for some games, that doesn't bode well. Now, on to you, what the fuck is your problem? You talk like a fucking dumbass fanboy, yet claim not to be. Spout out BS like this: ""its = to 6-8 gigs of gddr5 which puts it even or ahead of the 980 ti"", A. where the fuck did you get that? Got source? No? Hmm, from my readings, yes, AMD did focus on optimizing VRAM use for the Fury X, because even they know 4GB is not really enough. And no matter what they do, its going to come to a point, where it isn't going to be(it already has in games like GTA V, like I said). So get off your high horse, stop ignoring pieces of my argument that you don't want to address, and stop spouting bullshit."
nvidia,3dvm8z,Soulshot96,0,Mon Jul 20 02:27:37 2015 UTC,"And did I fucking say HBM was not better at higher resolution? NO. You're just a fuckwit that doesn't read anything, including his 'sources' correctly. The reason Nvidia is using it on Pascal is because HBM2 should be available by then, and will have a capacity of 8GB. Jesus fucking christ."
nvidia,3dvm8z,Soulshot96,-2,Mon Jul 20 02:32:50 2015 UTC,"The entire premise of this argument was over 4 k res, which you said it was worse at, go back and reread this post."
nvidia,3dvm8z,andrew5161,-1,Mon Jul 20 02:38:56 2015 UTC,Shut up just shut up you are so ignorant it is sad please for your own sake shut up.
nvidia,3dvm8z,Griffith1984,1 point,Mon Jul 20 02:58:43 2015 UTC,Thank you.
nvidia,3dvm8z,Soulshot96,0,Mon Jul 20 03:27:38 2015 UTC,"Find me the point where I said 4k, even ONCE. True the OP is talking about 4k, but I am not talking about it specifically. If you want to go there, my argument is even stronger, as games use even more VRAM at 4k, and they will continue to use more and more. Also, when did I say the Fury X was worse at 4k? It no doubt will be when it hits its VRAM limit, but I didn't say it was, not once. I said it and AMD cards in general, are hampered by their overall worse frame pacing(especially bad in CF), slow driver updates, and the Fury X in general, 4GB VRAM capacity."
nvidia,3dvm8z,Soulshot96,1 point,Mon Jul 20 02:45:33 2015 UTC,"if you get reference 980tis and dont have issues with applying a small overclock, go for 980tis  if you get g1s or any other 980ti release with massive factory overclocks, go for them   otherwise fury x maybe. though it also depends on how diligent amd is with cf profiles, i had a lot of trouble during the 7xxx days.  but for what its worth, i dont really get this ""crossfire scales better"" argument, have any tests been done on hybrid/water cooled 980tis/tx vs fury x? i have a feeling its just thermal throttling making the nvidia cards scale worse."
nvidia,3dvm8z,methcurd,1 point,Mon Jul 20 09:43:58 2015 UTC,As far as I've been able to research the difference is more pronounced between different games than CrossFire and SLI.
nvidia,3dvm8z,Monosodium_Glutamate,-2,Mon Jul 20 11:03:14 2015 UTC,"I wouldn't go with a Crossfire setup right now.  I think AMD are generally OK, but I felt pretty burned coming from a 295x2 card. It just did not have good enough support for me."
nvidia,3dvm8z,boundedwum,1 point,Sun Jul 19 23:54:24 2015 UTC,"Really? Maybe I'm just impressed because of how utterly awful the support for the 7990 and tri/quadfire was, but over the last year X-fire support has been fantastic as far as I can tell. I'm left waiting for x-fire profiles maybe a month at most, but usually way less (Dragon Age: Inquisition was actually the only game I had any trouble with and it was taken care of pretty quickly and was never unplayable)"
nvidia,3dvhq8,GoldieEmu,3,Sun Jul 19 21:36:45 2015 UTC,"My personal experience is to look last at the card clocks and look. Customer support is the first thing you should look for when purchasing a video card, especially if the company is a taiwanese/chinese one. Down there, the companies seem to largely ignore the CS standards we have in europe/Usa and it takes A LOT to get something out of them should a problem appear. Make sure to purchase the card through a retailer with an AMAZING CS such as amazon. Lack of which, I'd gladly settle for a card that looks like a brick  and has 100mhz lower clocks but, comes from a company that has stellar CS. Just my $ 0.02"
nvidia,3dvhq8,Neumayer23,3,Mon Jul 20 16:25:41 2015 UTC,"I've had one of these on order for the past two weeks. I've had one other Inno3D card (a GTX 770) and it was a beast of a card at the time. I'm getting a little impatient waiting for it, so I may just end up getting the Gigabyte G1 instead."
nvidia,3dvhq8,BnanaRepublic,2,Sun Jul 19 23:52:16 2015 UTC,I'd take that over the EVGA Hybrid.  Looks like they put more effort into the design than EVGA did.
nvidia,3dvhq8,floydian32,2,Mon Jul 20 03:08:47 2015 UTC,"Aye, I just placed my order, hopefully I won't have to wait too long."
nvidia,3dvhq8,Jamolas,1 point,Mon Jul 20 09:37:50 2015 UTC,"Yeah, because how many jagged lines and angles a card has on it really reflects its performance."
nvidia,3dvhq8,floydian32,1 point,Mon Jul 20 12:41:25 2015 UTC,"Well, to me, looks of a card are important.  All EVGA did by the looks of it is take a the reference cooler shroud and fan a put a water block in it.  Not that it looks like shit, but it's nothing spectacular either."
nvidia,3dvhq8,Archmagnance,2,Mon Jul 20 19:16:37 2015 UTC,I'd rather put a brick with fans in my pc but is higher binned and from a company with better customer service  rather than one that looks cooler but loses in everything else because i dont think epeen matters....But that's just me
nvidia,3dvhq8,floydian32,1 point,Mon Jul 20 20:12:46 2015 UTC,"OK,well to each their own.."
nvidia,3dvhq8,MasterfulWonga,2,Tue Jul 21 01:19:15 2015 UTC,"I was looking hard at this product and can't see a reason what I shouldn't get it. Except for I have no idea if Inno3D are any good now?  I had a card from them quite some time ago and they weren't too good.  The O.C does look very good, esp the memory, maybe too good?"
nvidia,3dvumg,blbjtb,1 point,Sun Jul 19 23:26:24 2015 UTC,"I haven't read anything saying it's on the way or not, but I'd be inclined to guess they're working on it.  That feature didn't even launch right away with Maxwell so they're known to add features down the road.  I'd be way more happy with full VRAM stacking with Sli in Windows 10 DX12 over DSR though."
nvidia,3dvumg,floydian32,-2,Mon Jul 20 03:26:53 2015 UTC,"hopefully not, dsr is dumbest ""feature"" ever. If people want to run high resolution than use a high resolution monitor"
nvidia,3dvkoo,crawlerz2468,3,Sun Jul 19 22:01:52 2015 UTC,Nothing official but that's the rumor.
nvidia,3dvkoo,antonyourkeyboard,1 point,Sun Jul 19 23:46:29 2015 UTC,is that the reason these are out of stock everywhere? or am I missing something?
nvidia,3dvkoo,floydian32,2,Mon Jul 20 00:05:58 2015 UTC,They're probably coming out with a new version and have cut back or stopped production on the original to clear inventory.
nvidia,3dvhcy,CausingNirvana,1 point,Sun Jul 19 21:33:39 2015 UTC,"OBS does support using the nvidia NVENC encoder (what shadowplay is), for just about any window. The biggest downside with NVENC is if you're doing livestreaming at typical bitrates (1500-3500), the quality is pretty bad compared to x264 encoding at the same bitrates; but it's really great for local recording.  MSI afterburner also has NVENC for local recording but it's limited to 50Mbps max bitrate, even though newer cards support up to 130Mbps NVENC recording."
nvidia,3dvhcy,rewrite,1 point,Sun Jul 19 23:00:06 2015 UTC,yeah but the best part about shadowplay is the actual shadow mode that records the x amount of minutes of gameplay. Ive used the Nvec encoder on OBS and it just looks grainy.
nvidia,3dvhcy,rewrite,1 point,Sun Jul 19 23:35:26 2015 UTC,"ahh yeah, OBS with some form of shadow mode would be great.  I usually do around 60-75Mbps on ""high quality"" preset with my 970 and haven't noticed any quality issues so far. Sure eats through disk space though which is where shadow mode would really help :p"
nvidia,3dvhcy,SirCrest_YT,1 point,Sun Jul 19 23:44:59 2015 UTC,"OBS does have a loop function. You set the replay length, and then setup a hotkey to dump the buffer whenever you want. I use it all the time."
nvidia,3dvhcy,SirCrest_YT,1 point,Mon Jul 20 00:55:09 2015 UTC,how do you enable this feature? i dont see a replay length anywhere
nvidia,3dvpmp,SJtheFox,3,Sun Jul 19 22:43:08 2015 UTC,Make sure you've selected PC from HDMI source on the TV.
nvidia,3dvpmp,hawkens85,2,Mon Jul 20 03:16:39 2015 UTC,"I don't know if this will help, but I always used this program to correct my colors.  http://blog.metaclassofnil.com/?p=83"
nvidia,3duah1,himmatsj,3,Sun Jul 19 15:21:50 2015 UTC,"There were a few reports last week circulating that due to AMD's dominance of the HBM2 supply, Nvidia is likely to have to delay Pascal significantly."
nvidia,3duah1,condraj2,1 point,Sun Jul 19 20:54:08 2015 UTC,"I've only seen that 1 place and it wasn't based on any actual data, just pure speculation.  The reality is that money talks and Nvidia has a 76% market share in the GPU world...I'm pretty sure Hynix is in the business of making $$$."
nvidia,3duah1,sluflyer06,1 point,Mon Jul 20 13:28:13 2015 UTC,"That's fair, and yes I'd agree that it's speculation/rumors. It was reported in a few places last week, but that doesn't mean it's accurate.  And I'm sure they're in the business of making money, but that doesn't get them out of contracts they make with other vendors. If they committed to supplying AMD first and their contract excludes them doing business with Nvidia for X amount of time, they'd be stuck. Granted, I'd be very surprised if that were the case."
nvidia,3duah1,condraj2,1 point,Mon Jul 20 18:56:26 2015 UTC,AMD will already have something around a year of exclusivity with HBM..also I think its worthwhile to mention Nvidia already said (IIRC) HBM would be part of Pascal and they wouldn't say that unless the deal was in place.
nvidia,3duah1,sluflyer06,0,Mon Jul 20 20:07:41 2015 UTC,"The good news is that Maxwell GPUs would still be ""bleeding edge"" and AMD have a chance to be competitive. Not everyone has the upgrade itch."
nvidia,3duah1,bach99,2,Mon Jul 20 05:22:59 2015 UTC,Won't anything think that we might see a surprise launch of at least something until then?  Around the Fall or Spring?  Or is the 980Ti going to be the go-to for premium until then?
nvidia,3duah1,verycoolguySmash,3,Mon Jul 20 05:38:33 2015 UTC,I think early 2016?
nvidia,3duah1,Rickyplayer6,7,Sun Jul 19 15:24:32 2015 UTC,"you're very optimistic  my guess, very late 2016 or early 2017"
nvidia,3duah1,zmeul,-1,Sun Jul 19 17:59:46 2015 UTC,Thanks for explaining your logic and reasoning behind that random guess.
nvidia,3duah1,JackVS1,8,Sun Jul 19 18:44:45 2015 UTC,"it's not random at all  1st, Pascal just taped out less than a month ago - from design to production nVidivia has a log road ahead, approx 1y  2nd, nVidia has hinted quite a few times that Pascal has priority for their Tesla accelerators, deep learning and other applications  so, if Pascal goes to HPC market 1st, that leaves the desktop market for very late 2016 or even 2017  is it clear now?    also, to note: there is no news from SK-Hynix front on HBM2"
nvidia,3duah1,zmeul,8,Sun Jul 19 18:57:27 2015 UTC,"Yeah, I didn't pick up any of that from your original post. Weird."
nvidia,3duah1,JackVS1,2,Sun Jul 19 19:18:19 2015 UTC,"I realize he didn't type any of that at first, but if you're following HBM news, what he's saying is pretty accurate and it's the topic of most conversation."
nvidia,3duah1,an_angry_Moose,1 point,Sun Jul 19 23:43:53 2015 UTC,Should he back his conjecture up with some of the hard data available on the topic?
nvidia,3duah1,zkDredrick,1 point,Mon Jul 20 08:33:31 2015 UTC,Q2 2016 from what I've seen.
nvidia,3duah1,GoldieEmu,1 point,Sun Jul 19 21:34:43 2015 UTC,Its all guesswork. But id say q3-q4 2016 and the first consumer card to utilize it from nvidia will be reference titans. As it will be there way of limiting the demand on the supply constrained hbm2 and possible issues. Then we'll get a 980 card later down the line.
nvidia,3duah1,Put_It_All_On_Blck,1 point,Sun Jul 19 22:08:19 2015 UTC,I would say 16 months.
nvidia,3duah1,Cereaza,1 point,Mon Jul 20 01:58:26 2015 UTC,the 980ti's just came out and already wanting next generation...lol.
nvidia,3duah1,sluflyer06,0,Mon Jul 20 13:26:11 2015 UTC,"Maxwell is already a 1.5 year old architecure, so..."
nvidia,3duah1,sluflyer06,1 point,Mon Jul 20 13:33:17 2015 UTC,I don't think Nvidia has ever released more than 1 flagship card in a given year...at least not in recent history. (Titan's are a separate product line so I don't count 780ti/titan / 9xx/titan as separate releases since they are basically the same card targeted towards 2 different customers)
nvidia,3dumo3,Erisi,1 point,Sun Jul 19 17:14:40 2015 UTC,I can confirm this is happening to me as well. If I enable shadowplay my memory instantly rockets to 100% usage (16GB) and then my system eventually crashes. I installed the last driver version and it fixed it. However the first time you launch GFE it forces you to update to the newest broken memory leak version. I haven't been able to find anything on nVidia's end even addressing this as an issue but it's happening to me and several of my friends.
nvidia,3dtstw,taranpawa,11,Sun Jul 19 11:46:16 2015 UTC,Anything EVGA is my personal preference.
nvidia,3dtstw,Knight-of-Black,1 point,Sun Jul 19 18:39:44 2015 UTC,"part of the appeal of evga is they're an american company and have great warranties, will cross-ship, will accept coilwhine returns, etc. Also the step-up program"
nvidia,3dtstw,fuccboi9000,3,Mon Jul 20 01:06:08 2015 UTC,http://hexus.net/tech/reviews/graphics/84005-zotac-geforce-gtx-980-ti-amp/
nvidia,3dtstw,orairwolf,9,Sun Jul 19 13:02:02 2015 UTC,I think the Gigabyte G1 is the best of the vanilla aftermarket cards.
nvidia,3dtstw,BlayneTX,12,Sun Jul 19 13:18:52 2015 UTC,Isn't a custom PCB cust cooler card the opposite of vanilla?
nvidia,3dtstw,sluflyer06,4,Sun Jul 19 14:03:03 2015 UTC,Beware of coil whine though. Many people have had bad luck with the G1s. But on the other hand some work fine. It's just prone to coil whine.
nvidia,3dtstw,deumetomnia,3,Sun Jul 19 17:56:45 2015 UTC,"Mine does 1500/2078 with a small voltage boost and stays under 75c on stock fan curve. Huge FPS boost over the reference 980ti, often 20%+ more fps.   Very happy with this card, runs cool and quiet with a great overclock and it has no problem pushing games on Ultra at 1440p well above 60fps."
nvidia,3dtstw,hdshatter,2,Sun Jul 19 13:49:31 2015 UTC,Yeah decided I'm gonna go with this 980 ti especially after the bios update they did to the g1 fans run much quieter than before under full load
nvidia,3dtstw,TILfuck,1 point,Sun Jul 19 19:44:16 2015 UTC,Ah really? Where can I get this?  Edit: Here it seems
nvidia,3dtstw,zWeApOnz,1 point,Sun Jul 19 23:06:27 2015 UTC,Also curious about this bios update.
nvidia,3dtstw,Luckyduck1337,2,Mon Jul 20 12:13:05 2015 UTC,My g1 is a dream overclocker and runs cooler than most the benchmarks Ive seen on my overclocking thread at the same clocks. I did receive a great bin but its a great card imo.
nvidia,3dtstw,andrew5161,2,Sun Jul 19 20:48:22 2015 UTC,My vote goes to the EVGA 980 Ti Classified. It's an awesome card and EVGA's customer support and warranty are the best of the bunch.
nvidia,3dtstw,Aldarro,2,Sun Jul 19 17:39:55 2015 UTC,"I still like the Inno3D GeForce GTX 980 Ti iChill X3 Ultra  Like most other 980ti's it isnt really that special, they all seem to roughly do the same with a few exceptions of cards that you shouldnt buy (strix), the reason I would buy the ichill x3 ultra is because it is apparently 7db quiter than the g1 at load. Obviously it runs slower and hotter, just like the msi does, but 7db quieter is huge.  I wouldve bought this card immediately and said fuck the bundles, fuck the sales, fuck whatever else is coming out later, but sadly they dont sell in NA, so it is out of my reach unless i pay hundreds more than msrp on international ebay."
nvidia,3dtstw,Put_It_All_On_Blck,3,Sun Jul 19 22:20:29 2015 UTC,You might as well go with the msi version and match the rest of your build. It's very comparable to the other vendors cards.
nvidia,3dtstw,Xintros,3,Sun Jul 19 15:42:58 2015 UTC,"I'd also suggest this.  It's what I'm going to do with my build.  I think I'll love the black/red dragon look.  MSI is the ""safe choice"" in 980 Ti.  It is the middle ground option in terms of noise, warranty support, cooling, price, overclock, and power consumption.  You won't win in any of those categories but you're not losing in any of them either.  Only downside is I have no idea how aggressive MSI is in bidding on binned chips."
nvidia,3dtstw,omnicious,1 point,Sun Jul 19 16:56:57 2015 UTC,"Based on my own experiences, not as aggressive as gigabyte or even ASUS"
nvidia,3dtstw,mermaliens,2,Mon Jul 20 02:52:33 2015 UTC,"I was following the Galax 980TI Hall of Fame. Should be in stock tomorrow. Otherwise from what I can tell the Gigabyte G1 gaming and the EVGA SC ACX2.0 w/bp look great.   I'm buying one of these cards today or tomorrow.   The Galax card will look great with my build, the EVGA card is backed by great service, and the G1 is great at performance. Not sure which one to get."
nvidia,3dtstw,r0b3r71,2,Sun Jul 19 13:20:09 2015 UTC,In Stock where?
nvidia,3dtstw,DrexelDragon93,1 point,Sun Jul 19 18:00:19 2015 UTC,I haven't been able to find it anywhere in stock in EU. Some German sites list 24 july as the next shipment.
nvidia,3dtstw,ANCIENT-ALIEN,0,Sun Jul 19 18:17:19 2015 UTC,Galax website downvote me if you want but I messaged them on facebook and they said it will be in stock 7/20/15
nvidia,3dtstw,r0b3r71,1 point,Sun Jul 19 18:32:20 2015 UTC,Is this  Galax 980TI Hall of Fame better then G1 or acx2.0? Or probably is to early.....
nvidia,3dtstw,r0b3r71,2,Sun Jul 19 19:13:53 2015 UTC,"The clock is better than the EVGA acx2.0. Not sure about the g1. The HOF is more exotic than the other cards, so it will probably retain its value for longer. (So if you want identical cards for SLI in the future, buying a more mainstream card may be easier to find later) I think the HOF cards are binned. Any 980TI can be overclocked, but the silicon lottery will restrict you potentially. I read on overclock.net about a guy that bought and returned the HOF due to bad performance.  Ultimately, what card looks best to you? I liked the EVGA and the HOF, but the fact that EVGA has 24x7 phone technical support, and a better warranty was the tipping point which made me go EVGA. I like the peace of mind that support will bring.   The HOF and the G1 have two 8 pin connectors whereas the SC Acx 2.0 has one 8 pin and one 6 pin."
nvidia,3dtstw,r0b3r71,1 point,Sun Jul 19 19:29:30 2015 UTC,"I'm surely wrong...but...i never cared about phone support or others, i'm usually go with the best vale for money product or the most powerful product. I tend to put aside support and give priority to these 2 points"
nvidia,3dtstw,r0b3r71,2,Sun Jul 19 19:38:14 2015 UTC,"Your not wrong if that's what's important to you. The way I see it, if you get a good card, it can be overclocked to around 1500mhz. The guy with the HOF said that he returned his because he couldn't get past 1400 iirc. Some people have great experiences with the g1, while others complain of coil whine.    That's why support is important to me, just in case my card has some type of issue.  Based purely on cost, the g1 is the best option for what you want. Highest performance per dollar. Evga is in the middle, with similar cost with lower performance, and HOF is the highest cost, but you have the nice clean design (black sniper edition will be released in the next week or so) and similar performance to the g1."
nvidia,3dtstw,omnicious,2,Sun Jul 19 19:47:14 2015 UTC,Are there many difference in terms of performance from G1 and G6 (MSI)? Cause my setup is Black/red and the msi will match very well. Also msi price is highest...
nvidia,3dtstw,r0b3r71,1 point,Sun Jul 19 19:54:10 2015 UTC,"Honestly I am not sure. I only researched the G1, SC acx.20, and HOF. From what I've heard, MSI is a good brand. Check out the Asus Strix as well, I believe that card is red/black too."
nvidia,3dtstw,Soulshot96,1 point,Sun Jul 19 19:57:16 2015 UTC,A difference of 1-3 fps from what I've read. Where are you ordering from? G1 is the most expensive for me in the US.
nvidia,3dtstw,Salt_Lake,1 point,Sun Jul 19 20:00:33 2015 UTC,EU (italy)... that's the problem :\ G1--> 740 € lowest price G6--> 800€ lowest
nvidia,3dtstw,andrew5161,1 point,Sun Jul 19 20:04:46 2015 UTC,tell me what GPU u will buy :P
nvidia,3dtstw,Salt_Lake,2,Sun Jul 19 18:31:45 2015 UTC,"I bought the EVGA SC ACX2.0 w back plate! I really liked the support behind EVGA, and the low profile side of the card. Figure in a year or two, it might be easier to find another one of them for SLI. Would've bought the classified or hybrid but they were out of stock everywhere. I had a newegg discount and gift card so i got enough of a discount to get free overnight shipping and about $5 off the $679 MSRP.  So $675 w/ next day shipping out the door. I'm going to paint the backplate white to match my build."
nvidia,3dtstw,andrew5161,1 point,Sun Jul 19 18:39:58 2015 UTC,"I can't help ya much man, I have the ACX SC+ myself, and its great, but I know the G1 is great too."
nvidia,3dtstw,p3dr0maz,2,Mon Jul 20 01:45:17 2015 UTC,"Custom cooling or custom pcb? Custom PCB i would go e ga classified. Same price as G1 but better support IMO, plus more flexible warranty. If custom cooler the evga hybrid if you cant fit a AIO cooler i hear about MSI lately but no direct experience."
nvidia,3dtstw,andrew5161,1 point,Sun Jul 19 14:58:08 2015 UTC,"If you read kingpins post on /r/overclocking the cooler and overvolting makes no difference on maxwell after a certain point. Based off of what he says and benchmarks ive seen of 980 tis on my thread there is no point in buying a aio cooled card other then to put the heat outside your case. I score 21,690 on my graphics score with my card which is in line with any max overclock ive seen on custom loop and aio cooled cards."
nvidia,3dtstw,p3dr0maz,1 point,Sun Jul 19 20:50:57 2015 UTC,That would the point of any custom cooler is to put the heat outside the case. Even ACX/STRIX/TWINFROZR are designed to take heat off the card and use case fans to take it out of the case. If you dont remove it ypu are going to cook your card.
nvidia,3dtstw,Contact_lenses,1 point,Sun Jul 19 21:51:17 2015 UTC,"Air coolers can handle that job just fine, most people that put aio or custom loops on their cards are looking to either get really low temps or to get extra performance out of their card by having better cooling and therefore better thermal margins to overvolt their card, which according to kingpin is essentially pointless with maxwell."
nvidia,3dtstw,Wingnut10,1 point,Sun Jul 19 21:54:07 2015 UTC,"Anecdote: Agreed, I AIO'd my card but it didnt improve the OC head room. I tried a myriad of voltages and the mid range ones were most stable than the higher ones I tried at the same clocks.   Here's how my card goes though when I turn it on I get 1.224V, then at 45c it goes to 1.205V, then at 55c it goes to 1.187V on AIO'd it pretty much does'nt go above that. I am going to put a new BIOS on it though because i feel i shouldnt have to deal with that lol."
nvidia,3dtstw,xk4l1br3,1 point,Mon Jul 20 20:17:39 2015 UTC,you can plug your aio power and fan into pwm connectors on your mobo and control them to keep your temps down. Assuming your plugs are pwm.
nvidia,3dtstw,xk4l1br3,1 point,Mon Jul 20 23:22:55 2015 UTC,"The fan for the radiator i do have connected to the mobo, if im gaming it's on 100%, i pretty much game with all my fans at 100%.  The only thing I did different is the AIO power and the fan for the VRM's on the G10 bracket I have those running off of the PSU using molex to pwm adapters. I figured that just means they run at 100% all the time which is fine by me. If that is detrimental lmk."
nvidia,3dtstw,bmwmk3,-1,Mon Jul 20 23:31:58 2015 UTC,custom cooling is the way!  i also wanna know when all theese gpu coming: http://videocardz.com/56048/interesting-stuff-from-computex-2015-part-2  they have to be more powerful then actual VGA
nvidia,3dtstw,Jack_BE,2,Sun Jul 19 19:10:20 2015 UTC,"I just got the palit super jetstream version, came yesterday and its amazing, literally silent and cheaper too, if you look at the benchmarks it beats a lot of the other versions too. Id recomend"
nvidia,3dtstw,Jack_BE,2,Sun Jul 19 17:04:59 2015 UTC,"Me too - it's a very good card and cheaper than the others. It doesn't really beat most of the others (looking at all the site's reviews), but there's very little in it tbh."
nvidia,3dvwtn,anotheranotherother,3,Sun Jul 19 23:45:21 2015 UTC,"Set the Hysterises setting in Afterburner, it will make RPM changes less sensitive to heat drops."
nvidia,3dvwtn,Zinthros,1 point,Sun Jul 19 23:50:42 2015 UTC,"Set your own custom fan curve in Afterburner. If you haven't got a very noisy cooler, don't be afraid to ramp your fan up lower on the heat scale."
nvidia,3dvwtn,an_angry_Moose,1 point,Mon Jul 20 02:40:49 2015 UTC,You might want to check your temps. Last time i.had this I accidently left my fans set to like 30 percent and was playing assassin's creed on my gtx 470 sli. The temps were getting to 100C and it was ramping the fan to 100 % then back to 30% over and over again as the temps fluctuated.
nvidia,3dvwtn,FunktasticLucky,1 point,Mon Jul 20 11:00:15 2015 UTC,"For the past...3 to 5 years the only games I've played are dota2 and Minecraft, and this card works just fine for both.  And I've been planning to get a new system later this year."
nvidia,3dvvf4,HeitorHCGomes,1 point,Sun Jul 19 23:33:30 2015 UTC,"I was having the same problems with Witcher 3 in particular and this is what fixed it for me.   1) Remove Geforce experience or disable nVidia streamer service  2) Disable nVidia high definition audio under sound, video and game controllers in device manager   Just something to try before you go through the hassle of rolling back."
nvidia,3dvvf4,Paradox621,2,Mon Jul 20 02:31:15 2015 UTC,Hmm... Will try this tomorrow and see if it helps! Thank you!
nvidia,3dvvf4,luggagepls,2,Mon Jul 20 03:52:38 2015 UTC,"I gave up on fixing Nvidia on Windows 8. Both hot fixes did not fix my issue at all and I didn't want to stay downgraded (I did downgrade to the GTA V patch, people have reported success with that but I had frames dropping) nor disable/remove features just for Witcher 3.  Went for the Windows upgrade after seeing people reporting no issues on Windows 10. Am now on the Windows 10 Insider Build 10240 along with Nvidia's latest drivers (for Windows 10), have not crashed at all."
nvidia,3dvvf4,dcdead,1 point,Mon Jul 20 04:01:20 2015 UTC,Can't wait to get my win10 then... The crashes are becoming more constant... To the point I'm looking which driver is more stable
nvidia,3dvvf4,disdisdisengaged,2,Mon Jul 20 04:36:04 2015 UTC,Been using 353.38 since it was released and haven't had a single TDR
nvidia,3dvvf4,dcdead,2,Mon Jul 20 07:50:57 2015 UTC,I can vouch for this 353.38 has worked great
nvidia,3dvvf4,hdshatter,1 point,Mon Jul 20 10:51:34 2015 UTC,"Hm...What GPU are you using? I had a 970 and had no problems with TDR, just got a titan X and still bugs me the fact that is TDRing"
nvidia,3dvvf4,BrutalGoerge,2,Mon Jul 20 13:55:35 2015 UTC,I have a 970 and a 980 ti (in different PCs) and what I wrote holds true for both
nvidia,3dvvf4,Luckyduck1337,1 point,Mon Jul 20 16:57:51 2015 UTC,Hm...Maybe I'll try 353.38...Currently on 347.88 and seems pretty stable so far
nvidia,3duv7h,John-Smith-,3,Sun Jul 19 18:26:11 2015 UTC,"I have the same CPU, GPU, RAM and SSD as you are planning to get.   Its main advantage is noise and temperatures - mainly temperatures - get it if you prefer those factors to overclockability.  However note that it can be overclocked a bit, but I don't think you'll get speeds like the popular Gigabyte G1 980 Ti.  In my case I wanted the lower temperatures and a bit of overclocking.  As for warranty, it's EVGA, so they're a pretty safe bet.  I don't think these hoses are so fragile that there's a high chance of leakage; they're pretty flexible but sturdy."
nvidia,3duv7h,iamapizza,1 point,Sun Jul 19 18:45:32 2015 UTC,Yeah I figured It'd safe since EVGA covers it but the main problem here is the warranty of other damaged parts such as my Mb for example.  The MSI and Gigabyte variants of the GTX 980 Ti don't appeal as much to me as the hybrid. On top of that here in the EU the Hybrid is barely any more expensive than the other cards so that's also a plus for me.
nvidia,3duv7h,TheMastodan,1 point,Sun Jul 19 22:00:50 2015 UTC,"TPP is September 15 for PC  If it's anything like Ground Zeroes, it'll be capped at 60 fps, and even modest hardware can max it out"
nvidia,3duv7h,Love2Nurb,0,Sun Jul 19 20:03:30 2015 UTC,"Not worth it, only cools gpu. It does not cool the vram. Better to get a water block for it if any water cooling is wanted on the item."
nvidia,3duv7h,Love2Nurb,1 point,Sun Jul 19 20:58:36 2015 UTC,"I know, it gets cooled by a fan though so that is absolutely fine since vram doesn't get that hot anyway. Considering it's benchmarks I don't think it's not worth it.  The only problem here is the risk of those hoses leaking fluids. Which, as a person without any experience in water cooling, instills fear in me."
nvidia,3dw2z4,Cobra0702,1 point,Mon Jul 20 00:39:40 2015 UTC,"What have you played? What haven't you played?  If I had more time and a proper setup, I'd try Elite Dangerous. I might do that other one that looks less involved when it comes out. A ""new"" Star Control is supposedly in the works, so for nostalgic purposes, having played Star Control on a Tandy 1000, and Star Control II on the Panasonic 3DO, I will probably give it a go.  Simulation covers a wide range of games, so I'm wondering what you mean. Racing, atmospheric / space flight, cops and robbers, cowboys and indians, war (historic, modern, futuristic)... its a long list."
nvidia,3dw2z4,motleyguts,1 point,Mon Jul 20 03:29:03 2015 UTC,"I have played dayz and love it, but it sadly has lost its ""discovery"" aspect and I feel that it is too easy now. I guess I am looking for an action/adventure type simulation game with or without horror aspects to it."
nvidia,3dw2z4,BlayneTX,1 point,Mon Jul 20 23:19:43 2015 UTC,Arma 3 is fun if you join a group.
nvidia,3dw2z4,Rate_My_Build,1 point,Mon Jul 20 03:59:00 2015 UTC,"Critical Gaming role play is the most fun thing in that game. :DD  EDIT: but if you do not own it now, I would recommend waiting until it is on sale as it is ike $80 with the DLC. I got mine for $30 during the summer sale."
nvidia,3dw2z4,GameSyns,1 point,Mon Jul 20 04:35:08 2015 UTC,"I play older games, but if you like major simulation, check out flight simulator x, or prepar3d. There are some pretty cool addons you can get for those games. Maybe train simulator 2015."
nvidia,3dw2z4,GAZZY75,1 point,Mon Jul 20 08:38:39 2015 UTC,Elite Dangerous if you have a Hotas.
nvidia,3dvc4k,blacksabathiscool,2,Sun Jul 19 20:49:25 2015 UTC,"commenting ""I have the same issue"" or something similar keeps people from posting the same question as you and this has been answered.    Posting your current driver helps with the troubleshooting.    Only help I can offer is doing a clean install (use driver sweeper after you uninstall your current driver) of the hotfix.  It has not gone with WHQL testing but can be found here. Hopefully this helps."
nvidia,3dvc4k,MistralNiblick,1 point,Sun Jul 19 21:33:21 2015 UTC,"352.86, 353.06 & 353.30 have a problem that is fixed by 353.38 hotfix driver. If you are on one of those 3 drivers 353.38 should fix your problem.  353.38 hotfix thread: https://forums.geforce.com/default/topic/849203/geforce-drivers/announcing-geforce-hotfix-driver-353-38/  352.86, 353.06 & 353.30 crashing thread: https://forums.geforce.com/default/topic/833693/geforce-drivers/nvidia-driver-352-86-353-06-stops-responding-and-kernel-reloads/"
nvidia,3dvc4k,kittah,1 point,Mon Jul 20 16:14:10 2015 UTC,Thank you very much i'll be sure to check this out. Right now the crashing has stopped but i am not sure what will happen in the future.
nvidia,3dvc4k,isaacthebrick,1 point,Tue Jul 21 02:14:16 2015 UTC,"I have a gtx 770 too, I believe that they are all having the same issue, I even rma my card and they said that nothing was wrong. The drivers are shit"
nvidia,3dtg2d,Tungstencarbide1611,7,Sun Jul 19 07:51:28 2015 UTC,At 1080p 60hz the benefits of SLI would be minimal to non existent in most games.  Out of all of that your weakest hardware is your monitor.
nvidia,3dtg2d,hdshatter,5,Sun Jul 19 08:07:35 2015 UTC,In The Witcher 3 when I max the game out I get down to 25 fps in some situations. I would prefer to stay over 60fps.
nvidia,3dtg2d,Cereaza,6,Sun Jul 19 08:47:19 2015 UTC,"1080p, you hit 25fps on a 980?"
nvidia,3dtg2d,hofern,14,Sun Jul 19 08:53:11 2015 UTC,Probably with Hairworks.
nvidia,3dtg2d,p3dr0maz,1 point,Sun Jul 19 09:21:00 2015 UTC,"With a 980 on a 1080p/60Hz I could easily get 45+ with Hairworks,  without it I'd stay above 55+.  Dude must also be using DSR. Or his cards an ez  bake oven and is down clocking hard."
nvidia,3dtg2d,CameBackWrong,1 point,Sun Jul 19 15:21:06 2015 UTC,"No I am talking about minimum fps. This is a worst case scenario. I get average of 50 fps with everything maxed out (incl. hairworks) but there are certain areas/fights in the game, which get the fps down to 25 for a few milliseconds.  Oh and you got a ti."
nvidia,3dtg2d,NCEMTP,2,Sun Jul 19 19:43:53 2015 UTC,"I can confirm that it hits 25fps. I've noticed as low as 23fps a couple patches back. Being on a strix 980, I've noticed that this happens when its' VRM temperature hits the 83-85+c range, and when the environment visuals are fairly complex. My guess is the card starts throttling itself at that temperature. I should look at my core speeds sometime."
nvidia,3dtg2d,Shandlar,1 point,Sun Jul 19 12:35:07 2015 UTC,"Even with two 980s, you won't be constantly over 60fps with all settings maxed at 1080P on TW3."
nvidia,3dtg2d,Shandlar,1 point,Sun Jul 19 18:28:45 2015 UTC,damn. So should I wait until next year for whatever comes after the 980ti?
nvidia,3dtg2d,hdshatter,1 point,Sun Jul 19 19:45:51 2015 UTC,"I am personally, but I'm pretty patient.  Hynix announcing that AMD is getting preferred HBM2 supply is a big hit to Pascal tbh.  I doubt we'll see Pascal 'Titan' til end of April or into May now.  Flagship cut down '1080' probably not til June.  I'm still probably going to wait for Pascal, but there is definitely a strong argument to get a 980ti right now if you are struggling on your current GPU."
nvidia,3dtg2d,BlayneTX,1 point,Mon Jul 20 05:20:51 2015 UTC,Wouldn't a second 980 in sli be cheaper and more powerful?
nvidia,3dtg2d,FunktasticLucky,2,Mon Jul 20 05:32:16 2015 UTC,"Other option is to sell the 980 and get a 980Ti.  Much less improvement, but not as much money either, and you won't be beholden to SLI scaling with a single, more powerful card.  Considering your biggest issue is minimum framerates and not average frame rates I would do this instead.  Two SLI 980s will introduce even more frame rate drops.  That is the nature of two cards.  A single 980ti will allow for a much more stable framerate, as well as a good bit higher average and minimum frames.  A little bit of work to sell the 980 ofc, but skylake wont do anything for your framerate's because your 2600 cannot possible be bottlenecking your 980 by any appreciable amount."
nvidia,3dtg2d,orairwolf,1 point,Mon Jul 20 05:45:46 2015 UTC,This would be a great idea for anyone in my situation BUT I'm still paying for the 980. (payment by installments)
nvidia,3dtg2d,Zinthros,1 point,Mon Jul 20 07:29:54 2015 UTC,"It shouldn't be that low ever on a 980, even with hairworks on max.  Even if SLI was 100% scalingand you do for some reason go to 25 it still wouldn't be 60."
nvidia,3dtg2d,SoldMySoulToReddit,2,Sun Jul 19 11:19:46 2015 UTC,We don't know if an overclocked skylake can beat an overclocked 4790k yet.
nvidia,3dtg2d,StepZ082,0,Sun Jul 19 09:23:29 2015 UTC,I could have sworn I saw somewhere that skylake shows serious promise over 4790's. They were only clocked at 2.2GHz and matching performance or coming close if I recall. When I get back to my desktop I'll see if I can find it.
nvidia,3dtg2d,kittah,1 point,Sun Jul 19 18:03:20 2015 UTC,All the benchmarks I could find said a maximum of 20% (but that was comparing internal graphics. With a dedicated card it was only 2-10%)
nvidia,3dtg2d,kittah,2,Sun Jul 19 19:46:56 2015 UTC,"As an Intel employee, I recommend getting the second GTX 980."
nvidia,3dtg2d,Cereaza,2,Sun Jul 19 10:09:22 2015 UTC,"Second 980.  Intel CPUs are stagnant as fuck. My X5650 from 2010 (granted, 4.4GHZ OC) can beat modern stock-clocked Intel CPUs."
nvidia,3dtg2d,A1phaBetaGamma,1 point,Sun Jul 19 21:41:09 2015 UTC,Upgrade your CPU?
nvidia,3dtg2d,CaptSkunk,1 point,Sun Jul 19 13:00:08 2015 UTC,What's wrong with his cpu?
nvidia,3dtg2d,A1phaBetaGamma,2,Sun Jul 19 18:10:49 2015 UTC,"It's not the unlocked version so he can't overclock it. At stock speeds it MIGHT bottleneck the 980 in some CPU intensive games (BF4, GTAV, Witcher 3 around Novigrad area, etc). In most games it shouldn't be an issue though."
nvidia,3dtg2d,CaptSkunk,1 point,Sun Jul 19 18:17:26 2015 UTC,BF4 still gets me a minimum of 105 fps on Ultra and Witcher 3 is not CPU intensive at all. It can be played with a dual core perfectly fine.  I am looking forward to future games like The Division.
nvidia,3dtg2d,A1phaBetaGamma,1 point,Sun Jul 19 19:53:32 2015 UTC,"Most of the witcher 3 sits around 60-70% cpu usage but Novigrad hits 100% on all 8 threads on my 3770k @ 4.8 ghz when there are lots of NPCs. This causes GPU usage on my 980 Ti to drop from the locked 99% it maintains everywhere else in game & the fps to drop into the 70s.  Also in BF4 I see a pretty big difference in minimum & avg framerate between 3.5ghz stock clock on my CPU & 4.8ghz OC. Anything over about 4.2 seems plenty for 144hz though. This benchmark shows the impact of CPU for BF4 pretty well: http://gamegpu.ru/images/remote/http--www.gamegpu.ru-images-stories-Test_GPU-Action-Battlefield_4-test-bf4_proz_2.jpg  But yeah, IMO your system is pretty badass as is & your idea about upgrading monitors might be the way to go. You could definitely pick up a 144hz gsync monitor with that budget. Just be warned that it will totally ruin 60hz monitors for you and you'll be wondering how you lived without one for so long."
nvidia,3dtg2d,CaptSkunk,1 point,Sun Jul 19 22:27:38 2015 UTC,Oh wow I didn't know about that. Thank you for clearing that up.  Well that pretty much sums up my fear about upgrading to a 120hz+ monitor. What if I can't afford to upgrade my system the next year? I will be stuck with 40fps in games and I will miss the 120+ fps. Maybe I will only be able to play minecraft or newer games in lowest settings just to get the high fps?  Or is it not about the ingame fps but more about the monitor's capabilities? I never experienced a 120hz+ monitor. All my friends got 60hz just like me. And you can't watch a video about it on a 60hz monitor I guess haha.
nvidia,3dtg2d,HelperBot_,0,Mon Jul 20 03:09:54 2015 UTC,"For gaming, SLI.   But there's a lot to consider. Skylake will run on a new chipset, will likely not support that RAM. On the otherhand, you're runing an older CPU, not sure how many PCI-E lanes it has, which may be an issue in SLI if you use other extension slots of it the 2600 has fewer than 16 lanes."
nvidia,3dtg2d,Zinthros,0,Sun Jul 19 08:52:49 2015 UTC,If sli can work with the 2600 I don't see a reaskn why it wouldn't work with skylake. There's no reason they'd go down from 16 lanes. Also why shouldn't skylake support that RAM ? It should support both DRR3 and DDR4.
nvidia,3dtg2d,zkDredrick,2,Sun Jul 19 10:07:33 2015 UTC,"Skylake will support DDR3L RAM. That means that for most people, buying new RAM is still a must, in order to upgrade to Skylake/Z170. Most people now, do not have low-voltage RAM installed."
nvidia,3dtg2d,CaptSkunk,1 point,Sun Jul 19 12:00:54 2015 UTC,"I don't really understand the difference. I have normal 240 pin DDR3 CL11 1600MHz RAM, would I need to upgrade that ? I thought the L at the end for the laptop and sff ram. 204pin I think."
nvidia,3dtg2d,zkDredrick,4,Sun Jul 19 13:32:54 2015 UTC,"So, here is the gist of it.   DDR3L stands for double data rate 3 Low voltage random access memory.   What this means is that any DDR3L sticks, use a max of 1.35 volts. Normal DDR3, which is what a great majority of people have, uses 1.5 volts and even sometimes 1.65 volts.   DDR4 uses 1.2 volts. So, it uses between .30-.45 less volts than the most typical memory and still between .05-.15 less than DDR3L memory.   I'm not 100% sure but I think some processor models can only use one or the other (DDR3 or DDR3L) and it can also be implemented on the motherboard/chipset level.   Basically, low voltage RAM is mainly used for laptops or mobile devices but can also be found in some desktops or even servers. The whole point of it is to save energy or battery life. Also of note, is the fact that DDR4 is aiming to get down to a voltage use level of 1.05 volts but this will take a while."
nvidia,3dtg2d,Cereaza,2,Sun Jul 19 14:55:49 2015 UTC,"Okay that was a great explanation thank you. There's something I still don't  though. The motherboards and processors who would support 1.2  and 1.05, why should they have a problem with 1.5  volts ?  I remember haswell requires 1.5V yet 1.65v still somehow works. Would 1.5V and above not work if the motherboard supports  ddr4 ? Is this somehow confirmed or is this some how just logical and can't be any other way. I really appreciate your help thank you."
nvidia,3dtg2d,A1phaBetaGamma,3,Sun Jul 19 15:09:44 2015 UTC,"Because they are just not designed for so much voltage. Though, I'm not sure if that is strictly a limitation that is implemented through hardware or if it is implemented in the bios.   Basically though, over time, the voltage required for each piece of hardware sinks. Whatever it stabilizes at becomes sort of the standard, though there are variations   Let's look back at DDR2 RAM. It goes from 1.8 volts to 2.3 volts (2.3 volts is just in the specs but most common was 1.8-2.1 volts). DDR was 2.5-2.6 volts.   As I said above, over time, hardware is able to become more efficient; doing the same or more amount of work with less energy required. A perfect example of this are the Maxwell cards from Nvidia or the latest Haswell/Devils Canyon chips from Intel. My old GTX 260, from 2008, used something like 250-275 watts but my 970 uses maybe 200-225 max and delivers 5 times the performance of the old 260.   Here is a great article with links and specs of different RAM types.   https://en.m.wikipedia.org/wiki/DDR_SDRAM"
nvidia,3dtg2d,Cereaza,3,Sun Jul 19 15:37:58 2015 UTC,Non-Mobile link: https://en.wikipedia.org/wiki/DDR_SDRAM.    HelperBot_® v1.0 I am a bot. Please message /u/swim1929 with any feedback and/or hate. Counter: 40
nvidia,3dtg2d,Cereaza,1 point,Sun Jul 19 15:38:15 2015 UTC,"Too much RAM voltage can fry your CPU because it has a direct connection to the RAM since the first i7 / i5 CPUs (LGA1366), which would burn out their memory controller over ~1.7V, with Intel requiring motherboard vendors to display a scary warning over 1.65V."
nvidia,3dtg2d,Cereaza,-4,Sun Jul 19 21:44:24 2015 UTC,I do.
nvidia,3dtg2d,ziplock9000,2,Sun Jul 19 12:48:49 2015 UTC,"Are you most people? No, you are one person. If you happen to have DDR3L RAM for whatever reasons, then you should be set to go to Skylake."
nvidia,3duaob,minimumfear,1 point,Sun Jul 19 15:23:53 2015 UTC,"I just got the palit super jetstream yesterday and its awesome its so quiet and never goes abobe 70 degrees, it does take up 3 slots though."
nvidia,3duaob,Contact_lenses,1 point,Sun Jul 19 17:06:08 2015 UTC,That's good to hear.I am planning to buy one tommorrow and I guess Ill just go for palit.I dont mind the 3 slots anyway since my pc is only geared for 1 gpu only
nvidia,3duaob,Blokie13,1 point,Sun Jul 19 18:01:03 2015 UTC,I've got the 980ti Palit Super Jetstream too and it's a great card.  My 980ti replaced a Palit 970 which was also a great card.
nvidia,3dsi9l,rx512,11,Sun Jul 19 01:01:50 2015 UTC,"There might be a slight discount on 980tis between now and then, but moreover, the ""buy or wait"" dilemma comes up a lot around here . The basic answer is that there are always better things coming up than what you currently have, and there are always discounts coming up on the thing you already own.   Just buy whenever you need/want to."
nvidia,3dsi9l,phalange33,4,Sun Jul 19 01:19:02 2015 UTC,"Pascal is a year off, sitting on your hands until Q2 2016+ is going to be painful.  A transitional GPU might be a good idea, though.  Something like a 970."
nvidia,3dsi9l,TaintedSquirrel,13,Sun Jul 19 02:12:14 2015 UTC,"If transitional is all it will be. I don't see the point in getting a 970. Just get a very-similarly performing 290 for less, or a 390  which is just an overall better card. I didn't mean this to be a rant but honestly I don't see a reason why anyone should buy a 970 now unless it costs less than $300.  It's not just me who says this either, even Jayztwocents, the guy with 3 titan X's and who has been owning only nvidia cards for a very long time says the 390X is a better do.  Yes I know this is the nvidia sub,.but yes this is the truth. Unless you need something like cuda acceleration there's just no point.   I don't mean to say this directly to you but rather to all the people who recommend 970s. This is other's people's money you're playing with. How would you feel if someone helped make you buy something that costs that much then know there's a better alternative."
nvidia,3dsi9l,A1phaBetaGamma,2,Sun Jul 19 10:22:54 2015 UTC,This!
nvidia,3dsi9l,IceCoolTea,1 point,Sun Jul 19 14:14:18 2015 UTC,"970 could be an option for several reasons:  -Cheaper than the 390 in some markets -Lower power use / cooler operation -nVidia/Gameworks compatibility (contentious, I know) -Good old brand preference -Potentially better overclocker  At 1080, it's six of one, half dozen of the other. Above that, the extra memory of the 390 will make a difference for modern games."
nvidia,3dsi9l,quadraphonic,6,Sun Jul 19 19:24:38 2015 UTC,"I'm sorry but most of these points are rather weak, while yes they are probably true.    They both MSRP at $330 (I think, I don't suppose the 970 is officially cheaper) and it's normal if one of them is 10-20 dollars cheaper, even if a 970 costs $300 and a 390 costs $330 that's still just 10 percent of the price, and it's just as likeyy that a  can also be cheaper. Power consumption I can't argue about that, but the difference in the electricity bills is really negligible, the real problem with higher power consumption is my next point Heat. The 390 isn't considerably that much hotter than a 970, and a cooler like the msi TFV really does a great job at keeping the card both cool and quiet. I'd rather not talk about gameworks. I don't know much about the issues it causes and  most of the rants people claim about it is just speculation, yet there's definitely something shady there, but you can still count that as a point for the 970 I guess. Overclocking you can also count s a win for the 970. There's no arguing that.it overclocks like a beast, butnit everyone overclocks their gpu past 1400-1450MHz to significantly beat a 390.   What I get from this is that a person who would buy a 970 should be a person who doesn't mind upgrading that often (more often than a person who gets a 390), fond of gameworks and is really hardcore about overclocking and likes nvidia's apps and such more, but these are far from the majority, especially those only looking for a temporary card and wouldn't like to mess with overclocking.  Please excuse me if you've found my tone hostile or aggressive, I am only entertaining an argument and for all I know you're just a wonderful person who just likes to play games."
nvidia,3dsi9l,A1phaBetaGamma,2,Sun Jul 19 20:15:18 2015 UTC,I picked a 970 because I was impatient and couldn't get a 390 easily (they were also about -$40 more for me in the CDN market).   I was just playing Devil's advocate noting there are some reasons for choosing one over the other. I'm just excited to leave my 8800GT behind!
nvidia,3dsi9l,quadraphonic,2,Sun Jul 19 20:17:56 2015 UTC,"Yeah I think both of these cards are great and just a significant upgrade for anyone using a card that's a few years old, let alone an 8800GT! My friend upgraded from that to a 960 and he couldn't believe himself, I imagine a 970 would be more of a huge jump!"
nvidia,3dsi9l,A1phaBetaGamma,2,Sun Jul 19 20:22:07 2015 UTC,"Yeah, every game was starting to look like Minecraft!"
nvidia,3dsi9l,quadraphonic,6,Sun Jul 19 20:32:59 2015 UTC,"I'll tell you, I jumped on the Titan X in April, knowing the 980Ti and Fury X were right around the corner. Thing is, you're too busy enjoying your new product to worry about what comes later. It's different after you make the purchase and fire it up. I came from a trio of 580s and I and my electric bill are so much happier. I even friended some conservationists. Edit: Real world differences between the overclocks of various 980Tis is minuscule. It all really comes down to the sound they put out, and those differences are pretty small too."
nvidia,3dsi9l,motleyguts,3,Sun Jul 19 01:43:01 2015 UTC,"The only high end card coming between now and November is AMD's Nano. If you're not interested in form factor, you can safely buy the 980 TI now."
nvidia,3dsi9l,MorningYourLordship,4,Sun Jul 19 11:40:31 2015 UTC,"I think now is a good time to act. The way i look at it, right now the maxwell architecture in this card is in its prime. Pascal is going to be brand new and it will take a while for them to really take advantage of what it is capable of. So by the time they release the 2nd gen pascal cards with all the tweaks and upgrades, youll be ready to update since that will probably be 1.5 to 2 years from now."
nvidia,3dsi9l,Tylerdurden516,4,Sun Jul 19 01:15:49 2015 UTC,"I'm just going to say that a 980ti is over kill if you want to play fallout 4. Not that there is nothing wrong with over kill. But i would wait for the info on fallout 4 to come out before you make a move for a purchase so large. Unless you really want one then act on it, otherwise I think you should wait."
nvidia,3dsi9l,Kcoggin,2,Sun Jul 19 01:44:29 2015 UTC,OP didn't say anything about his monitor. Maybe it's 144Hz 4K?
nvidia,3dsi9l,boraca,2,Sun Jul 19 10:22:19 2015 UTC,I really wish that existed right now :). Nothing would be able to push that many pixels though.
nvidia,3dsi9l,dellaint,2,Sun Jul 19 18:30:32 2015 UTC,Agreed! My 980 has enough trouble with 1440p 60hz on some games!
nvidia,3dsi9l,FrozenBananaMan,1 point,Mon Jul 20 14:46:44 2015 UTC,What are the recommended system specs for Fallout 4?
nvidia,3dsi9l,NCEMTP,1 point,Sun Jul 19 02:45:33 2015 UTC,"They haven't been released as of yet, but I would say minimum requirements for a gpu would probably be no higher then a 750ti, and amd equivalent. With at least a 6300fx and 8 gigs of RAM. But that's all pure speculation"
nvidia,3dsi9l,Kcoggin,1 point,Sun Jul 19 02:52:12 2015 UTC,Yeah. Rumor mill says minimum specs will recommend a 660 or so.
nvidia,3dsi9l,NCEMTP,2,Sun Jul 19 02:56:26 2015 UTC,660 and 750ti would be close enough for a few frames difference. We will just have to wait for bethesda to come out with an anouncment
nvidia,3dsi9l,Kcoggin,2,Sun Jul 19 03:01:47 2015 UTC,"Don't know why you are being downvoted, you're absolutely right. Getting a 980 for Fallout 4 is like using napalm to like a cigar."
nvidia,3dsi9l,wanderjahr,1 point,Sun Jul 19 04:56:12 2015 UTC,"Without knowing what his monitor specs are, you can't possibly say that. 144 hz or 1440p/4K...nope."
nvidia,3dsi9l,thaumogenesis,1 point,Sun Jul 19 18:01:43 2015 UTC,Good point.
nvidia,3dsi9l,wanderjahr,1 point,Sun Jul 19 18:12:44 2015 UTC,"Also, just because Fallout 4 spurred him to buy a new card doesn't mean it's the only thing he'll be playing. Fallout 4 pushed me over the line to buy some new cards but I'll be playing Witcher 3 and other games on my new rig as well."
nvidia,3dsi9l,dellaint,0,Sun Jul 19 18:32:48 2015 UTC,Skyrim with mods brings a 980 to its knees I think Fallout 4 won't be that much of a difference.
nvidia,3dsi9l,Tywele,2,Sun Jul 19 08:06:46 2015 UTC,That's because the engine is poorly optimized.
nvidia,3dsi9l,hdshatter,2,Sun Jul 19 23:29:54 2015 UTC,From what I understand the graphics aren't going to be so great.
nvidia,3dsi9l,Colinski282,3,Sun Jul 19 03:10:44 2015 UTC,"You will not need a 980 TI for Fallout 4, Considering how old the engine is."
nvidia,3dsi9l,Brandonspikes,1 point,Sun Jul 19 06:04:07 2015 UTC,I'm in the exact same situation; need a new GPU for this Fall's big hits.  I'm spending the next few months watching for blockbuster sales.  I wouldn't pay $650+ for a 980 Ti right now if you don't actually need it for a few months.  By the time FO4 comes out we should see some sub-$600 sales.
nvidia,3dsi9l,TaintedSquirrel,1 point,Sun Jul 19 02:11:27 2015 UTC,"What you have now might be enough.  Fallout 4 looks to be a modified version of the engine they've been using since Elder Scrolls Oblivion.  It looks good, but not knock the shit out of your ass good.  So even if you had say an 660Ti you'd likely be fine.  If what you have now is really bad, like 400 series, then yeah I'd spring for a 980Ti and be done with it for a few years.  But if you have something still capable of running today's games then I'd wait for Pascal."
nvidia,3dsi9l,floydian32,1 point,Sun Jul 19 02:13:01 2015 UTC,"who knows, maybe pascal will be delayed til 2017. probably gonna be another maxwell refresh next year some how"
nvidia,3dsi9l,fuccboi9000,1 point,Sun Jul 19 03:41:34 2015 UTC,If AMD keeps disappointing they could delay it and get away with it.
nvidia,3dsi9l,hdshatter,2,Sun Jul 19 23:28:12 2015 UTC,"It's unlikely a new card will come out.  However if you have until Nov 11, save for sales.  Shit maybe wait for Black Friday//Cyber Monday, a more enjoyable experience is better than a lackluster sooner one."
nvidia,3dsi9l,kendirect,1 point,Sun Jul 19 04:15:47 2015 UTC,"The EVGA ACX 2.0 Classified is supposed to be really cool and quiet while overclocking well. If you want a good performer though, get one of their hybrid cooled 980Ti's. Benchmarks show that it'll outperform a Titan because of the closed water loop it has."
nvidia,3dsi9l,nd4spd1919,1 point,Sun Jul 19 04:45:27 2015 UTC,"Steam forums don't think that even the GTX 870M/GTX 965M could run Fallout 4. How do you feel about your GTX 760M running Fallout 4? Do you think you can run that honestly, or are you upgrading?"
nvidia,3dsi9l,bach99,1 point,Mon Jul 20 05:37:18 2015 UTC,"I'm fairly certain that I can run Fallout 4. The idea that an 870M wouldn't be capable of running FO4 is just an over-reaction. From all the trailers and gameplay vids we've seen, FO4 looks to be on par with a modestly modded Skyrim. My laptop only has a 1080p screen, so I think I'll still be able to go with medium-high setings and get around 40fps. As for upgrading, hell no. I've only had this laptop for a year and a half; it would be stupidly expensive to upgrade so soon. I'm probably going to have to stick with this until it's about 4 years old, so December 2017 is my targeted upgrade date."
nvidia,3dsi9l,nd4spd1919,1 point,Mon Jul 20 15:33:07 2015 UTC,"Reason why I'm asking is the lighting is different from Skyrim though - it's like a more well tuned ENB setting. As we know, ENB is pretty demanding. I'm worried not because I can't run it, but if it will have enough headroom to add mods on top of it. Not graphic mods, maybe a little texture mods here and there and quest mods. Good luck to you!"
nvidia,3dsi9l,bach99,1 point,Mon Jul 20 16:18:45 2015 UTC,"Get the EVGA GeForce GTX 960 4GB FTW ACX 2.0+ for $250 now, then start saving for that big upgrade later when Pascal comes out.   I am sure it will run what you need til then."
nvidia,3dsi9l,lordroy,1 point,Sun Jul 19 09:15:42 2015 UTC,Fallout 4 is Console Optimised TM  I don't think you'll even need something so powerful.
nvidia,3dsi9l,gaveasky,1 point,Sun Jul 19 10:36:42 2015 UTC,What kind of monitor do you have?
nvidia,3dsi9l,BigBlueApples,1 point,Sun Jul 19 13:08:53 2015 UTC,Get 2 980s.
nvidia,3dsi9l,tl2horse,-1,Sun Jul 19 13:13:02 2015 UTC,Falllout 4 looks like complete shit you could most likely get away with a 970. 980 Ti will be mooooore than enough.
nvidia,3dsi9l,Agent_McMuffin,3,Sun Jul 19 14:22:25 2015 UTC,Brilliant input.
nvidia,3dsi9l,thaumogenesis,1 point,Sun Jul 19 17:58:52 2015 UTC,"Hahaha, probably some merit there mind you."
nvidia,3dsi9l,GAZZY75,0,Mon Jul 20 13:42:54 2015 UTC,Wait! Always wait because prices could and do go down.
nvidia,3dsi9l,arikv2,-1,Sun Jul 19 13:29:50 2015 UTC,Promise u the game will not run good at all
nvidia,3dtw5n,casc1701,11,Sun Jul 19 12:33:18 2015 UTC,"The GPU is much more important, but 1gb might be an issue at your resolution. I reccomend 2 options for your price range.  1: Radeon R7 260x $120 - 2gb vram and a pretty good card.  2: Nvidia 750ti $150 - there is a 2gb vram version of the 750 ti and I reccomend it if you can go up to $150.  Don't do a 650, its too old and will underwhelm."
nvidia,3dtw5n,zkDredrick,4,Sun Jul 19 12:58:19 2015 UTC,"Got it, no 650, old and busted."
nvidia,3dtw5n,VLAD1M1R_PUT1N,2,Sun Jul 19 16:10:05 2015 UTC,"To be fair, there are 2GB models of the 750, but yeah you're right."
nvidia,3dtw5n,himmatsj,4,Sun Jul 19 14:47:20 2015 UTC,"If it is a direct choice between the two cards you listed - go for the 1 GB GTX 750.   However, I would recommend getting the GTX 750 Ti (2GB) one short. I have the 1GB 750 and it can be a struggle to play the best AAA games today at decent textures. Mind you, 1 GB VRAM still works (with games like The Witcher 3, Far Cry 4 and Shadow of Mordor for example), but 2GB should keep you good to go for the rest of this gen without getting too worried.   Then again, do you aim to play at 2560x1080? It's too high a res for these cards. You will need to play at a lower res than that according to your screen orientation.   Also, specifically for the games you mention, 1GB VRAM should be more than enough."
nvidia,3dtw5n,ralgha,3,Sun Jul 19 13:08:44 2015 UTC,Thanks!
nvidia,3dtw5n,HawkEye0,1 point,Sun Jul 19 16:09:19 2015 UTC,"It's a personal preference kind of decision.  Which bothers you more?  Lower res textures or a lower framerate?  You can always turn down texture resolution in the game settings if you're low on VRAM and the game needs more than you have.  But if your GPU is too slow, you'll have to turn down a lot more settings and maybe even run at a lower resolution to get a decent framerate.  I agree with the other comments here that you should get at least a 750 Ti."
nvidia,3dtusn,DonDalle,1 point,Sun Jul 19 12:14:48 2015 UTC,"Hello!  Feel free to search this subreddit as I've already complained about this card a few times :)  To summarize: My first unit had a loud/perpetual coil whine at all times (idle, gaming, etc). Certainly not all of the cards have this issue, but I've seen it reported enough that it's something to worry about. I returned the first card and got a replacement without this issue.  Beyond that, there are a few other issues that I experienced on both cards that I've corroborated with a few other users. These issues may be present on all of the cards as they currently exist and should be taken into account:  One: Fan behavior. On both of my cards and for at least a couple of other users, our fans don't behave well. They report RPM spikes in monitoring programs (which in itself isn't a big deal) but they also cannot maintain a constant speed. If you set a fixed speed of like ~80%, the fans will stay at that speed... for the most part. They will periodically slow down and speed up for no apparent reason. When the fans get loud enough, it can become annoying hearing the fans spin down and up again quickly.  Two: Power issues. I have yet to have anyone provide screenshot/video proof showing otherwise for this card, so I'm hesitant to perform another RMA until I confirm it isn't a design issue. Basically, this card ships with a firm TDP limit fixed by hardware. Unlike other Nvidia cards, the Power Limit slider/target value cannot be modified. It can be changed in say Afterburner, but the GPU will not honor any changes to the setting. The result is a card that is pretty crippled: try to overclock it a bit beyond stock and, depending on the game, you may immediately hit the TDP wall. This results in the GPU dropping Boost bins and your card ends up running a hampered voltage.   For example, my card defaults to 1430MHz and 1.193V, but likes to sit at 1415MHz and 1.168 V (one bin down). If I bump the core frequency by say 30MHz, the following might happen. (This is with a significant Memory overclock that makes the power issues worse)  Goes to 1460MHz and 1.193V initially, then immediately starts power throttling. It can throttle so bad that it gets down to the 1415MHz it used to settle at, but now the voltage may be 1.155 or 1.13X depening on how many Boost Bins it dropped. The end result is that the card dials back too far and lowers the voltage too much doesn't help with stability.  I'm still trying to gather more anecdotal reports - but for now I haven't seen anyone provide proof that this card has functional Power Limit modification and is hampered as a result."
nvidia,3dtusn,patriotsfan82,1 point,Sun Jul 19 15:14:28 2015 UTC,"thank you for the detailed response! That doesn't sound too great. Besides the coil whine, how loud is the card? I'm coming from an 7970 GHz which is pretty loud and hope for something more quiet."
nvidia,3dsrqt,-Rizhiy-,1 point,Sun Jul 19 02:39:50 2015 UTC,Try safe mode and ddu to uninstall all drivers.  Reinstall drivers.
nvidia,3dsrqt,Colinski282,1 point,Sun Jul 19 03:12:04 2015 UTC,Yea just DDU your driver and go back to the last hotfix and don't download the latest Gforce Experience update (might be a beta) and you'll be golden.
nvidia,3dr0ak,Kweetus,6,Sat Jul 18 16:52:10 2015 UTC,"Sensible recommendation would be to wait however... I was running 780s on a 4k monitor, most things ran ok with 780s, the only recent game I've had trouble with was Witcher 3 which I knocked down to 1440 and got solid frames. It depends on your situation, if I overclock my 980ti I get better scores in benchmarks but not necessarily higher frames BUT I do maintain higher lowest frames whilst gaming due to it being a single card. So a 980ti is roughly on par with 2 780s so you're dropping all that money for little if any gains, at 1440 I don't think you'll struggle to hit 60 but may struggle to maintain 96. I do plan on getting a 2nd maybe on my Birthday next month or may wait a while see if the price drops."
nvidia,3dr0ak,UnrepentantN00b,3,Sat Jul 18 17:06:20 2015 UTC,"You're response is excellent my friend, thank you. I think I will take your advice and hold off on the 980ti and wait for next gen. I don't think I would ever plan on 980ti SLI, so with the money saved from not upgrading now I may choose to run whatever the next gen card is in SLI and get a 4k monitor. Maybe by then SLI will be even more refined, or something better perhaps. Thanks again dude."
nvidia,3dr0ak,an_angry_Moose,1 point,Sat Jul 18 17:42:56 2015 UTC,"I think it's DEFINITELY in your best interest to wait. Two 780s is no slouch, and you already have them, so you're out no cash. Use that savings to hold off and pick up the next extreme class GPU. You know the next AMD/nvidia gpu's will both have HBM2. It could be a big upgrade by then."
nvidia,3dr0ak,BaNaNaKING42,1 point,Sun Jul 19 01:11:19 2015 UTC,"Yeah but performance really isn't all when you are going from SLI to a single-card solution. I had two overclocked 780's which score about 1000-2000 points lower than a 980 Ti seems to score on a similar system (don't have the 980 Ti, yet).  However, I had some issues with HDR rendering being too fast in SLI resulting in flickering in certain games. Also temps, noise and power consumption are higher with two cards. So you have to take those things in consideration, too."
nvidia,3dr0ak,UnrepentantN00b,2,Sat Jul 18 17:42:20 2015 UTC,"Valid points, but it wasn't something I really considered, I strapped a aio cooler using a Nzxt Kraken bracket to my 780s so noise and heat were never a problem for me. I still wouldn't consider it a worthwhile upgrade. I only went with it because I like shiny things and have plenty of disposable income. But I think mostly I miss the Gigabyte cooler, I had a 670 with it and hated my 780s with the titan shroud.  Let's be honest, unless you literally can't play the game you want then the sensible advise should always be to hold out, however the final decision is with the op's desires."
nvidia,3dr0ak,BaNaNaKING42,2,Sat Jul 18 18:05:40 2015 UTC,"True. To each his own. For me, SLI wasn't as good as I hoped it would be so I decided to go back to a single card for now. (And, of course: shiny new thing :D )  I want to buy the G1 myself. How do you like it? Do you also have it water cooled? And are you experiencing any coil whine? And what are your 3D Mark results (seeing that our setup would be similar)?"
nvidia,3dr0ak,UnrepentantN00b,1 point,Sat Jul 18 18:41:36 2015 UTC,"EDIT: I can't format on Reddit for shit but I it reads ok I think. I do get coil whine, mostly at high fps but its quiet enough or non-existent at 60 that its not a problem with my headset on (coil whine seems to be hit or miss with the G1), I think I got a good overclocker, I've had it boost to 1587 and crash but can hold 1560ish stable. Highest temp I've seen is 74deg using my fan curve it hit about 50% fans (start becoming audible at  45-50%) but that was during a stress test, gaming I've only seen mid to high 60s where my fan sits nicely at 35-40%.  Firestrike 1080 - 780 SLI score for reference This was at the height of my 780s, started getting much lower scores as time went by  Firestrike Ultra 4K"
nvidia,3dr0ak,BaNaNaKING42,1 point,Sat Jul 18 19:02:49 2015 UTC,"Yep. Those SLI scores are pretty much what I had, too. However, my CPU is overclocked to 4.4 GHz.  Those temps also look pretty good. I pretty excited to get mine. but I'm waiting for the new promo to start."
nvidia,3dr0ak,LordOfTheGiraffes,2,Sat Jul 18 19:08:35 2015 UTC,What do you guys think about the 1440p monitors with GSYNC? I think there are only two out at the moment and they look expensive. Would that be a more noticeable upgrade over the 980ti? I would be going from an OC'ed QNIX to one of the gsync bad boys. Haven't been able to see one in action myself so wondering what you guys think. I'm pretty happy with the QNIX but have heard good things about gsync. Right now i'm thinking to wait until there are more options/price drops but if its that much of a difference I may pull the trigger on one.
nvidia,3dr0ak,ant1248,2,Sat Jul 18 19:15:05 2015 UTC,"I have an ROG PG278Q, and I love that thing. I bought it in January, and it inspired an upgrading frenzy that replaced essentially every single component in my desktop over the last six months. I just had to make my PC more capable of utilizing its goodness!  It's got a TN panel so the viewing angles aren't fantastic, but when I'm gaming I'm the only one looking at it so that doesn't matter. The colors aren't as vibrant as IPS, but the contrast and response are excellent (and there weren't any 1440p, 144 Hz IPS displays with G-sync on the market when I bought it, though I think there are now).  I was really surprised at how amazing g-sync is, and can't recommend it enough. I didn't realize how distracting tearing can be until it was gone."
nvidia,3dr0ak,floydian32,1 point,Sat Jul 18 20:44:30 2015 UTC,Yup. Love my RoG swift.
nvidia,3dr0ak,Shamefur_Disgrace,1 point,Sun Jul 19 02:11:45 2015 UTC,"Variable refresh like G-Sync and FreeSync are the biggest innovation to come along in PC gaming since 3D graphics.  If you have the hardware to drive a 1440p display it makes no sense to buy one without out it.  Everything is so smooth that it makes gaming a whole new experience when you aren't seeing any tearing or stuttering.  There are only 2 1440p G-Sync displays ATM, the ROG Swift and the Acer XB270HU.  I have a Swift and I briefly had the Acer.  The Swift is miles better in build quality and feels like an $800 display.  The Acer feels cheap, has a glossy finish and is IPS with 4ms response.  The one I had had terrible light bleed, dead pixels and what appeared to be dirt on the panel.  If you research that display you'll find the QC on it is horse shit.  Even though I feel IPS has better colors and viewing angles, I like the Swift more due to the 1ms response and build quality.  If you wait a couple of months, Asus is going to be releasing an IPS version of the Swift."
nvidia,3dr0ak,skidkids,1 point,Sun Jul 19 13:44:34 2015 UTC,For overclocking your monitor http://www.gamersnexus.net/guides/1674-overclock-monitor-higher-refresh-rate. I have a QNIX as well and got to 96hz with this method. The Witcher 3 at 1440p is the only game where I notice a little stuttering. I also have SLI 780's and an i5 3570k at 4.4 GHz.
nvidia,3dr0ak,YouShouldKnowThis1,1 point,Sat Jul 18 19:45:44 2015 UTC,I did this exact upgrade and I'm very happy. The new Vram was worth it and the slight performance boost plus massive boost in SLI unfriendly games was worth ir
nvidia,3ds1fe,Nahxify,0,Sat Jul 18 22:19:19 2015 UTC,"The current update has a lot of shadowplay bugs, so it may not be an issue you can fix."
nvidia,3dvrqe,RudanZidane,4,Sun Jul 19 23:01:41 2015 UTC,"I'm a bot, bleep, bloop. Someone has linked to this thread from another place on reddit:   [/r/advancedmicrodevices] I thought eyefinity worked with 3 monitors at different resolutions?   If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads. (Info / Contact)"
nvidia,3dvrqe,TotesMessenger,4,Sun Jul 19 23:46:41 2015 UTC,I thought everyone knew that an active adapter was needed on AMD cards after the third monitor. Sorry to see you got caught off guard about it. Hope your RMA goes well on the 980 ti
nvidia,3dvrqe,d2_ricci,2,Mon Jul 20 05:53:43 2015 UTC,"I should have done more research, but the sales associate at Canada Computers was pretty adamant that it was plug and play 3 monitors in whatever configuration.  Shit happens I guess."
nvidia,3dvrqe,Nimelrian,3,Mon Jul 20 06:29:09 2015 UTC,"Strange, my R9 290 was handling 2 DVI and 1 HDMI screen perfectly, including plug&play"
nvidia,3dvrqe,Jealy,2,Mon Jul 20 07:01:00 2015 UTC,I bought 3 screens when I had a 6950 and didn't know I needed an active adapter (I even bought a passive). After figuring this out and buying an active adapter I ended up upgrading my GPU anyway to an R9 290 which runs 2x DVI 1x HDMI natively with no such adapters.  I guess it depends on the card.
nvidia,3dvrqe,d2_ricci,1 point,Mon Jul 20 11:11:08 2015 UTC,This is one thing I kind of wish AMD would remedy with the future 400 series
nvidia,3dvrqe,jorgp2,1 point,Mon Jul 20 13:08:42 2015 UTC,"Well they're dropping legacy connector support, so that should fix it."
nvidia,3dvrqe,MDK350,0,Mon Jul 20 21:39:06 2015 UTC,3 monitor eyefiniti works fine for me. 290x.
nvidia,3dtehl,lemonmancool,1 point,Sun Jul 19 07:23:37 2015 UTC,"I think you typed Google Chrome incorrectly.  Jokes aside, firstly, I'd monitor the GPU speed, and temperature, etc... Just so you know your overclocking was done correctly, since this is your first time.  Then, your next best option, albeit tedious, is to uninstall all video card drivers, turn off your computer, remove your GPU, and connect it again, and install its drivers again."
nvidia,3dtehl,amnuous,1 point,Sun Jul 19 07:48:35 2015 UTC,"oh god no i don't use IE, just that Skype does and if IE isn't working, then neither is Skype. Actually the way i fixed it is by simply making a new admin account and migrated to it, weird."
nvidia,3dsgqj,Mogito10,2,Sun Jul 19 00:46:18 2015 UTC,"No, but if you turn it on in the settings it will use it when possible."
nvidia,3dsgqj,hdshatter,2,Sun Jul 19 01:21:20 2015 UTC,For example I see a lot of people use it on Battlefield 4 but on their site its not listed as one of the supported games.
nvidia,3dsgqj,troublegoats,2,Sun Jul 19 18:42:31 2015 UTC,Mother Fuckin Anti-Aliasing?
nvidia,3dqr9h,matrixhaj,4,Sat Jul 18 15:27:27 2015 UTC,"I'm not knowledgeable enough on the differences to speak on the comparison, but it might be worth mentioning that the Skylake processors will be arriving in a month or two, likely driving the prices of previous generations down a bit. If you're making a purchasing decision and can bear to wait a little while you might save some cash."
nvidia,3dqr9h,RumAndCookies,3,Sat Jul 18 18:47:27 2015 UTC,The Xeon AFAIK is essentially the same as an i7 but without integrated graphics and support for ECC memory. There shouldn't be any difference in fps as most games don't take advantage of Hyperthreading. I would personally go with whatever's cheapest for you.
nvidia,3dqr9h,Jabjibjam,1 point,Sat Jul 18 20:06:12 2015 UTC,What does that have to do with hyper threading?   btw: the Xeon 1231v3 DOES support hyper threading.  Oh and also: BF4 (2013) is just one example of games that support hyper threading.
nvidia,3dqr9h,Tungstencarbide1611,1 point,Sun Jul 19 06:58:42 2015 UTC,"I was saying that the Xeon supported hyperthreading but the i5 does not. Battlefield, Crysis and the Civilization games are the only games that I know of which support it.  Even when hyperthreading is supported in games it usually only makes a small increase in fps. You would be better off buying a better graphics card if the Xeon is more expensive. Of course, if you frequently edit video or do rendering work it would definitely be beneficial."
nvidia,3dqr9h,Jabjibjam,1 point,Sun Jul 19 12:19:04 2015 UTC,"Oops sorry.  Well either that or if you are a huge fan of the gaming series which have engines that support HT. (like the ones you listed above) To me Battlefield is still life and I build my system around it. (and Battlefront, which will be based on the same or a newer version of the same engine)"
nvidia,3dqr9h,Tungstencarbide1611,2,Sun Jul 19 20:01:46 2015 UTC,Both CPUs are quite evenly matched in core performance so it comes down to features.  As others have said. Wait for the new stuff to come out if you can.   I myself don't have the desire for OC so when I was in your position I chose the 1231v3 due to HT.
nvidia,3dqr9h,theamunraaa,2,Sat Jul 18 21:12:06 2015 UTC,"@theamunraaa : Yes but frequency is not equal and some people say HT will boost games, some say that it wont. @Jabjibjam : both exacly same price(i5 and xeon (both like 300USD)) @Wolfie3101 : but everyone is saying that those cpu dont bottleneck/is not needed and it takes ages for skylake."
nvidia,3dqr9h,IvanKozlov,1 point,Sat Jul 18 21:20:50 2015 UTC,"Skylake is supposed to launch is August, unless you absolutely need it now, I'd advise waiting as you'll likely see a price decrease on current processors and skylake is suppose to be kind of a big deal."
nvidia,3dqr9h,IvanKozlov,1 point,Sat Jul 18 23:17:21 2015 UTC,"on current processors and skylake is suppose to be kind of a big deal.   yes, but another thing is: it will take some time for arriving to my country and it will have absolute ridiculous prices (high as hell). Thanks for info(august) trought"
nvidia,3dqr9h,Wolfie3101,2,Sun Jul 19 08:34:42 2015 UTC,"My mistake for assuming you live in the U.S. That's the second time I've made that mistake this week. As always, do what's best for your budget, but when skyline comes out, the prices of the haswell/broadwell line should drop."
nvidia,3dqr9h,Colonel_Shepard,-2,Sun Jul 19 19:24:22 2015 UTC,"None. wait for the new CPU's, then decide."
nvidia,3dqr9h,theamunraaa,-10,Sat Jul 18 19:23:30 2015 UTC,"Xeons are optimized for workstation loads and their resp ctive features. i3, i5, and i7 processors are all designed for gaming. They handle the processes much differently as games give them in a very different way from some like Blendr. Rendering a video or 3D animations is the staple of xeons, as they excel at it. i-series processors will almost always give you higher frame rates and more consistent fps's in games, even though cheaper, because they have been optimized for such conditions. And if I were you, go with an i7, the hyperthreading helps greatly with future-proofing and getting rid of a possible** bottleneck, though if you don't have the money i5's will do just fine. Hope this helps!  Edit: I am very open to criticism of my statement, and ask that you do so along with a refutation so that I may learn from my mistakes."
nvidia,3dqr9h,Colonel_Shepard,8,Sat Jul 18 18:33:42 2015 UTC,Please refrain from commenting if the knowledge you have about processors and their operation is only from public forums and word of mouth. Almost any statement you've written is wrong and can be disputed.
nvidia,3dqr9h,IvanKozlov,3,Sat Jul 18 21:06:40 2015 UTC,"Thank you for actually constructively criticizing me, the only thing thing you need to do now is dispute my statement. I, as well as the OP, would do much better with a wrong answer and its refutation than just a wrong answer and a statement of its invalidity."
nvidia,3dqr9h,Colonel_Shepard,6,Sat Jul 18 22:26:38 2015 UTC,"You were downvoted because the Xeon series is not much different from the i series. The main difference being that the Xeon series does not have integrated graphics on the chip while the i series does. Thus the Xeon series is generally cheaper. Next, the vast majority of games do not utilize the hyperthreading feature of the i7 series. Most games are actually just now starting to use more than two cores, actually utilizing something like hyperthreading in the mainstream probably won't happen anytime soon. As far as gaming is concerned, the performance difference between an i5 and an i7 is almost negligible, a few frames at most. Certainly not worth the price difference for purely gaming. Bottlenecking a gpu with an i5 nearly impossible at the moment and the foreseeable future. Hope this was helpful!"
nvidia,3dqr9h,IvanKozlov,1 point,Sat Jul 18 23:06:17 2015 UTC,Thank you very much! I had always been under the pretense that Xeon were directed more to workstations and i-series more to gamers. Is the integrated graphics (Iris if I remember correctly) a factor in gaming? Which $4p is better for gaming?
nvidia,3dqr9h,Colonel_Shepard,2,Sat Jul 18 23:46:42 2015 UTC,"The integrated graphics are only really relevant if you don't have a dedicated (discrete) gpu, otherwise it's not very useful. (You can use it to run a second monitor if you don't want to use gpu resources to do it though.) In fact, it's generally disabled if you have a dedicated gpu. As far as the i5/i7 and the Xeon are concerned, they'll preform equally for gaming. It's only when you get into multithreaded programs that the i7 will outshine the i5. The Xeon is the equivalent of an i7 performance wise."
nvidia,3dqr9h,CaptSkunk,3,Sun Jul 19 00:58:24 2015 UTC,"Thank you very much! I have learned a lot and will refrain from posting again without research. I must applaud you for not being an asshole and explaining things to me clearly and simply. Others seemed to just say I was wrong without explaining why, but you helped me. Apologies to OP for no research."
nvidia,3dr0z1,r0b3r71,3,Sat Jul 18 16:58:26 2015 UTC,Eyeinfinty ftw.
nvidia,3dr0z1,Seirf,4,Sat Jul 18 20:46:45 2015 UTC,"For huge resolutions and multimonitor, and cards tend to do better."
nvidia,3dr0z1,Scoob_ford,1 point,Sun Jul 19 02:53:37 2015 UTC,Which card is good from AMD?
nvidia,3dr0z1,zkDredrick,2,Sun Jul 19 06:46:19 2015 UTC,390X is a great card for just over $400. If thats your price range I wouldnt reccomend anything else.  FuryX is a nice card at $650 but Id reccomend a 980Ti over it.
nvidia,3dr0z1,zkDredrick,1 point,Sun Jul 19 13:01:01 2015 UTC,"My budget is around $750 right now. I heard that the fury x wasn't optimized for DX11 though, so maybe it will really come into it's own at launch of Win10 and DX12. Though, I would think that the 980 ti would also benefit from those platforms."
nvidia,3dr0z1,Scoob_ford,2,Sun Jul 19 13:14:07 2015 UTC,"You should be looking at a custom 980Ti. Most will reccomend the EVGA Classified, Gigabyte G1, or MSI Gaming."
nvidia,3dr0z1,andrew5161,1 point,Sun Jul 19 17:25:12 2015 UTC,"I ended up going with the EVGA superclock acx 2.0 with bp. Classified is out of stock everywhere. I like EVGA's support. I called them today, got an answer within 30 seconds, and a friendly agent talked me through all the different options. They have 24 hour phone support, and a good warranty. I figure that in a year or two, finding one of these will be easier than finding a custom or htf model for SLI. The g1 and msi looked good too, but I really liked the lower profile of the one I chose. I figure at 1080p this card can handle anything I throw at it."
nvidia,3dr0z1,maxiimus1,1 point,Sun Jul 19 18:07:06 2015 UTC,"The 280, 390, 390x, and fury x. I miss your budget but 280s can be found for $150 if that's your price bracket."
nvidia,3dr0z1,SolidCake,-3,Sun Jul 19 18:07:41 2015 UTC,"Yea, even if you are running 144hz panels you still should be already with a 980ti. For 1080 its just laughable overkill. I think on a full overclock I get 290 fps on tomb raider on max settings at 1080?  P.S. I have a gtx 980 ti g1 Gaming edition from gigabyte."
nvidia,3dr0z1,andrew5161,9,Sat Jul 18 17:30:59 2015 UTC,"But don't forget its 3 monitors, so it's not really 1080p"
nvidia,3dr0z1,andrew5161,1 point,Sat Jul 18 18:17:18 2015 UTC,If it's three monitors its actually 3/4ths of 4k.. So its pretty demanding
nvidia,3dr0z1,andrew5161,-1,Sun Jul 19 01:16:05 2015 UTC,980 ti overclocked handles 4 k just fine. People say it doesn't but you turn aa off which you don't even need on 4 k and your over 60 fps in all but 1 or 2 games. so at 3/4 of 4 k you are at 80 fps in the same games. 980 ti is plenty for that setup.
nvidia,3dr0z1,andrew5161,1 point,Sun Jul 19 01:27:04 2015 UTC,"Yeah, my thought process is that if I get an overkill card, (in theory) it will be good for a few generations. I was looking at the Galax 980 TI HOF. The Gigabyte card looks great too. If I wasn't able to get the Galax card, I'd go with either EVGA or Gigabyte. On one 1080p monitor, my intel integrated graphics (intel 4600 graphics) runs Arma 3 with mid settings at around 35ish FPS. I can't wait to see what a game looks like with settings on ultra, with smooth graphics. My last computer was a 6 year old laptop."
nvidia,3dr0z1,andrew5161,1 point,Sat Jul 18 18:18:39 2015 UTC,"Once you get to where the g1/classy are, your paying massive premium for very little real world gain. Once you are able to hit 1500 or so on your clock there is very little to gain after that. I can run most games at 90-100 fps average at 1440 p, so based upon pixel vs fps scaleing you should get 60-65 at 5.8k x 1080. Hell I can run full 5 k and get 25-30 in shadow of mordor on maxed settings and that's more than twice the height in pixels than 1080x3."
nvidia,3dqsag,raveya,3,Sat Jul 18 15:37:41 2015 UTC,Not too sure if this is your problem but I heard shadow play was leaking ram quite badly so with your ram being 8gb that could be it - I'm not entirely sure but try turning off shadow play and use something else to read your frame rate if that's what you're using
nvidia,3dqsag,ElitelyGamersRay,3,Sat Jul 18 16:08:51 2015 UTC,Not all games are crated equal when it comes to GPU use. I know some direct x 9 games won't even boost my clock.   Also on d9 depends on the game but if the game isn't very demanding then the Power Management mode Adaptive kicks in. The other day it had me in game on Path of Exike at like 900Mhz.  I changed anfew of these games to Power Management mode High Performance and that helped a lot.  That being said which game are you having issues with?
nvidia,3dqsag,p3dr0maz,1 point,Sat Jul 18 16:27:54 2015 UTC,"Could it be a driver problem? Check the drivers on both systems and use the one of the better system, if they are different."
nvidia,3dqsag,Randomness6894,1 point,Sat Jul 18 20:56:18 2015 UTC,"Turn off any extra programs running in the background and see if that helps. I have almost the same build except with two GPUs, and that along with disabling flux, Logitech gaming software, and Vsync all stabilized my system.   Maybe our mobos don't play well with these cards. I am using the Z77 Sabertooth. It is awfully strange. Might also want to try upping your fan speed profile with MSI afterburner or the like. I noticed, with just one card, that stuttering only began in earnest when my card hit 83C. Now I hover at that still, but the load is balanced between two and the stuttering is absent if I make the aforementioned tweaks."
nvidia,3dr1tq,Hondroids,6,Sat Jul 18 17:05:35 2015 UTC,Nothing wrong with your PSU. Some video cards just have coil whine. It will not damage the card. It just can be annoying sometimes.
nvidia,3dr1tq,FDisk80,3,Sat Jul 18 19:25:21 2015 UTC,"Burn the card for a few days under gaming load, and see if it clears up.  This is what EVGA recommends."
nvidia,3dr1tq,TaintedSquirrel,1 point,Sat Jul 18 21:41:14 2015 UTC,I've heard people talking about this method before. Exactly how do you do it though?
nvidia,3dr1tq,garrett_burns,2,Sat Jul 18 21:51:26 2015 UTC,Turn on something like Unigine heaven and let it loop.
nvidia,3dr1tq,TaintedSquirrel,2,Sat Jul 18 21:52:13 2015 UTC,Seems simple enough. Thanks for the answer. Do you know if its generally successful in helping with coil whine?
nvidia,3dr1tq,garrett_burns,3,Sat Jul 18 22:10:33 2015 UTC,"From personal experience, it does absolutely nothing."
nvidia,3dr1tq,FDisk80,2,Sat Jul 18 22:19:10 2015 UTC,"You'll want a new PSU regardless, so start with that."
nvidia,3dr1tq,mtn_dewgamefuel,2,Sat Jul 18 18:38:42 2015 UTC,Why?
nvidia,3dr1tq,FDisk80,3,Sat Jul 18 19:25:43 2015 UTC,"The Corsair builder series is notorious for its low quality. Here is a list of PSUs, ranked by quality. You will notice your PSU is in tier 3, the ""good enough"" tier. If you want another Corsair PSU, you will want an AX or HX series one."
nvidia,3dr1tq,mtn_dewgamefuel,-1,Sat Jul 18 19:37:00 2015 UTC,By that logic everyone should get rid of their Ford's because Bugatti's are better.
nvidia,3dr1tq,FDisk80,0,Sat Jul 18 22:23:20 2015 UTC,"No, it means everyone that owns a pinto should get rid of it and buy a better car."
nvidia,3dr1tq,mtn_dewgamefuel,1 point,Sat Jul 18 23:05:59 2015 UTC,I don't think his PSU is going to burst in to flames because of a little coil whine.
nvidia,3dr1tq,FDisk80,4,Sat Jul 18 23:24:49 2015 UTC,"With the CX series, you never know"
nvidia,3dr1tq,mtn_dewgamefuel,1 point,Sat Jul 18 23:38:45 2015 UTC,Poor quality hardware contained in the psu (capacitors mostly) and the safety features aren't as robust. You'll still get the rated wattage but that's about it.
nvidia,3dr1tq,Kautkto,1 point,Sun Jul 19 02:19:19 2015 UTC,"My original 980ti sc from evga had this issue, insanely loud above 60fps, I'm running a 500watt gold rated psu from evga, I decided to rma anyway, the new one has not made a peep.  Try to rma."
nvidia,3dr1tq,solidnitrogen,1 point,Sat Jul 18 21:00:27 2015 UTC,"I have the same composition as you. A EVGA 980 ti and a CX750M, I've found that it's both. In my situation, the PSU is louder."
nvidia,3dsxsl,Muffinfiend,2,Sun Jul 19 03:46:11 2015 UTC,Nope not in september. There will be pascal though sometime next year around spring time.
nvidia,3dsxsl,mater36,1 point,Sun Jul 19 05:23:27 2015 UTC,"Ah OK. I'm wondering what card to settle on then; getting another 970 seems silly, as i did experience stuttering on some games like Shadow of Mordor. I also feel the 980 is at too high of a price point for me, thus my pickle."
nvidia,3dsxsl,Neumayer23,1 point,Sun Jul 19 12:51:18 2015 UTC,"980 isn't realy enough for 1440p either unless you are willing to lower some graphic options, keep in mind that 980 is basically a 680 2.0 as in, it's a middle range gpu labeled and sold at a high end gpu price tag. Not to mention that the 4gb of vram might still not be enough in games like shadow of mordor with the ultra texture pack and, 1440p res.  The only good answer for 1440p right now is a 980ti that unfortunately, is way above what you feel comfortable in spending, so your best best would be to hunt for an used 980. Lots of people who upgraded to a 980ti are selling their old 980 so you might find a bargain."
nvidia,3dsxsl,mater36,1 point,Sun Jul 19 13:09:09 2015 UTC,Yeah that sounds like the best bet. If I buy a used card does the warranty transfer over to me?
nvidia,3dsxsl,hdshatter,1 point,Sun Jul 19 13:55:44 2015 UTC,Certain brands do have transferable warranties. I think evga does but not sure about the others.
nvidia,3dpyhl,thatnitai,4,Sat Jul 18 08:42:09 2015 UTC,"Yep, I have a few games i push at different resolutions. I mostly use 1440p but ffxiii-2 for example is buggy as hell at 1440p and some older games i can push 4k no problem.  Even with the same resolution smoothness looks different on each game."
nvidia,3dpyhl,Kurosov,3,Sat Jul 18 11:41:57 2015 UTC,Also the default of 33% is pretty awful.
nvidia,3dpyhl,finakechi,1 point,Sat Jul 18 17:16:22 2015 UTC,Yea lol the default should be 15% or something
nvidia,3dpyhl,Die4Ever,2,Sat Jul 18 23:51:08 2015 UTC,20-24% looks best to me
nvidia,3dpyhl,Balthalzarzo,1 point,Sun Jul 19 02:35:28 2015 UTC,"I'll have to try that out. I think I only tried the default of 33%, 0% (for the hell of it mostly, I knew it'd only look good at 4x), and when I tried 15% I was just like ok yea that's good. Especially since I didn't want to have to change it depending on the game for those I can run at 4x."
nvidia,3dpyhl,Die4Ever,2,Sun Jul 19 02:38:55 2015 UTC,I would use this feature all the time - it's especially useful to be able to turn it off for VR titles that I render in 4K.
nvidia,3dpyhl,StaticSignal,1 point,Sat Jul 18 20:27:55 2015 UTC,Yes please.
nvidia,3dpyhl,billyalt,1 point,Sun Jul 19 06:03:11 2015 UTC,I sure hope Nvidia maybe notices people want this and implement it soon... I daresay it's probably not that difficult :)
nvidia,3dsv0z,Colinski282,3,Sun Jul 19 03:15:54 2015 UTC,3840×2160
nvidia,3dsv0z,DrexelDragon93,2,Sun Jul 19 03:18:13 2015 UTC,Why? It has about 4K or 4000 pixels  horizontally.
nvidia,3dsv0z,touzainanboku,1 point,Sun Jul 19 03:19:25 2015 UTC,"The ""4"" refers to the horizontal resolution of 3840"
nvidia,3dsv0z,DrexelDragon93,1 point,Sun Jul 19 03:24:30 2015 UTC,I was unaware of this. TIL thanks!
nvidia,3dsv0z,Ibestrokin9,1 point,Sun Jul 19 03:25:41 2015 UTC,"That's differen, that's their vertical resolution. 1080p, 1440p, 2160p are referred to like that. But also as ""2k"", ""2.5k"" and ""4k"" when referring to their horizontal resolution."
nvidia,3dsv0z,DrexelDragon93,1 point,Sun Jul 19 04:19:40 2015 UTC,The confusion comes from 1080p not being commonly called 2k. 2k is a resolution used in Digital Cinema which is a slightly wider image 2048 × 1080
nvidia,3dsv0z,HawkEye0,-4,Sun Jul 19 04:21:30 2015 UTC,"Ummmmmm no, it's 4k because it's 4 times the amount of pixels in a 1080p display.  19202 = 3840 10802 = 2160"
nvidia,3dsv0z,tonn482,8,Sun Jul 19 12:48:11 2015 UTC,No 4k is the cinema standard where the resolution is actually 4096 x 2160 hence 4k horizontal pixels. Ultra HD is the condumer standard and that is 3840 horizontal pixels. Similarly 1080p or Full HD is 1920 horizontal pixels and 2k is the cinema standard where they have the same 1080 pixels tall display but a wider 2048 horizontal pixels. Source: https://en.wikipedia.org/wiki/4K_resolution
nvidia,3dsv0z,mater36,2,Mon Jul 20 01:36:57 2015 UTC,"Yes but when you purchase a 4K monitor the resolution it runs in is 3840x2160, so the fact that people are saying that the 4k is a different resolution to that which the average consumer uses, is irrelevant."
nvidia,3dsv0z,tonn482,2,Sun Jul 19 04:12:37 2015 UTC,Doesn't change the fact that it is wrong. 4k is a buzzword as far as the consumer should be considered. 3840x2160 is UHD and frankly I'm pretty insulted that even NVIDIA is calling it 4k when it isn't.
nvidia,3dsv0z,billyalt,1 point,Sun Jul 19 05:30:23 2015 UTC,"I was just saying that for the average person, they wouldn't be purchasing a 4096x2160 monitor and calling it 4k, they will be purchasing the 3840x2160 monitor and calling it 4k, It's not good to confuse the person you are trying to sell something to."
nvidia,3dsv0z,tonn482,2,Sun Jul 19 07:30:31 2015 UTC,Technically that is UHD though and not 4K (though commonly referred to as). The 4 is indeed the horizontal resolution.
nvidia,3dsv0z,ChrysisX,-5,Sun Jul 19 10:10:47 2015 UTC,Not sure why this is downvoted when it's correct.
nvidia,3dsv0z,iMILFbait,3,Sun Jul 19 19:56:18 2015 UTC,"Nope.  4K resolution, also called 4K, refers to a display device or content having horizontal resolution on the order of 4,000 pixels.  https://en.wikipedia.org/wiki/4K_resolution"
nvidia,3dsv0z,Shyguythrowaway2,-3,Sun Jul 19 07:27:58 2015 UTC,Just round 3840 up to 4000 its pretty much the same /s
nvidia,3dsv0z,tonn482,3,Sun Jul 19 04:46:49 2015 UTC,4K is 4096 x 2160  perhaps you're referring to UHD(1) - 3840 x 2160
nvidia,3dsv0z,zmeul,2,Sun Jul 19 05:06:46 2015 UTC,"This is the actual correct response, UHD is actually mistaken for true 4 k rather often.   4096 x 2160"
nvidia,3dsohq,Yvese,1 point,Sun Jul 19 02:06:23 2015 UTC,They should be more or less the same as the reference card with the blower since that is still what is used to cool the VRM's after the hybrid kit is installed.
nvidia,3dr6gn,ThatGuy97,1 point,Sat Jul 18 17:47:32 2015 UTC,Are you on Windows 10? I had this problem on my installation. For some reason changing the audio format in windows fixed it for me.
nvidia,3dr6gn,VLAD1M1R_PUT1N,1 point,Sat Jul 18 18:30:40 2015 UTC,"Windows 8.1, but I'll try that"
nvidia,3dr6gn,Codizzle0024,1 point,Sat Jul 18 18:36:15 2015 UTC,"You will need to go into Control Panel  > Sound and set the device connected  through HDMI as the default. If this doesn't work, make sure an  HDMI sound driver is installed."
nvidia,3dr6gn,Codizzle0024,1 point,Sat Jul 18 20:31:47 2015 UTC,Could you point me in the direction of drivers to download?
nvidia,3dr52u,MoneysaurusRex,1 point,Sat Jul 18 17:35:08 2015 UTC,"i'll throw a few forum links at you that i found while researching this. they dont specifically deal with OCing but they are a good read and will have some results in there. plus, look for links to clocking threads as you go through. http://www.overclock.net/t/1487012/official-nzxt-kraken-g10-owners-club/3930  http://www.overclock.net/t/1203528/official-nvidia-gpu-mod-club-aka-the-mod/7580  http://www.overclock.net/t/1558645/official-nvidia-gtx-980-ti-owners-club/4490  http://www.overclock.net/t/1479716/guide-closed-loop-aio-coolers-on-your-gpu-warning-picture-heavy"
nvidia,3dr52u,ddeuced,1 point,Sat Jul 18 18:26:16 2015 UTC,"Here is a review of EVGA's AIO cooled card from Jayztwocents (a reputable reviewer). I think any of the AIOs (including the NZXT G10, Corsair HG10, and EVGA Hybrid) would be good, with the latter two being slightly better IMO. All of them should help you get the highest possible overclock, if that's what you're looking for."
nvidia,3dr52u,VLAD1M1R_PUT1N,1 point,Sat Jul 18 18:41:25 2015 UTC,Thanks bot...
nvidia,3dr52u,VLAD1M1R_PUT1N,1 point,Sat Jul 18 18:41:42 2015 UTC,"If you have a reference card, go with the EVGA hybrid cooler, its specifically made for the reference card and even uses the fan that is already on there to cool the VRM's."
nvidia,3dr52u,BKachur,1 point,Sat Jul 18 18:44:27 2015 UTC,vram also
nvidia,3dr52u,Ohmps_,1 point,Sat Jul 18 20:33:04 2015 UTC,"Good Point. I like it because the reference fan is a HELL of a lot stronger than the fan that most 3rd party systems come with. That thing nice and looks really purty. Only kinda pain in the butt thing, is if your benchamarking you have to artificially force the fan to kick in too cool the VRAM and VRM. Since the card never breaks 50 under load, the fan on stock setting literally never turns up. This means that while the VRM is hurting from over-voltage the fan will never automatically increase."
nvidia,3dqhjc,sn1313,2,Sat Jul 18 13:42:37 2015 UTC,"I think it is because your GPU is ""asleep"", and then is jolted awake and cannot respond in time.  I'm currently playing Mass Effect 1. Very old game, easy on the hardware. I'm playing it in a GTX 750 at average clock rate of 900MHz (1359 max) with around 50% GPU usage, temps under 50C. VRAM usage at 500MB max.   Yet, I still see framerate drops when entering a new area especially. My only deduction is that my GPU is almost practically sleeping, that when it has to render more heavily it doesn't respond quickly enough. Not sure if yours is a similar experience.   Also, FPS drops have nothing to do with VRAM. It's related to GPU clocking power. If you run out of VRAM what you see is ugly stuttering."
nvidia,3dqhjc,himmatsj,3,Sat Jul 18 14:12:23 2015 UTC,"My only deduction is that my GPU is almost practically sleeping, that when it has to render more heavily it doesn't respond quickly enough.   If that is the case, you can change Energy management mode from 'adaptive' to 'prefer maximum' in the nvidia control panel."
nvidia,3dqhjc,boraca,1 point,Sat Jul 18 18:38:10 2015 UTC,Thank you for this.
nvidia,3dqhjc,p90nub,0,Sat Jul 18 21:23:00 2015 UTC,Your GPU is to weak to handle it faster.
nvidia,3dqhjc,Sapass1,2,Sat Jul 18 14:11:52 2015 UTC,"Really? I also noticed that gpu usage chart shows 0, and no data being shown at all. What's the deal here? How is a 980 classified too weak?"
nvidia,3dqhjc,Sapass1,1 point,Sat Jul 18 14:29:59 2015 UTC,"Depends, but its more like how much a single gpu can handle then being weak, dsr simulate 4k resolution and no single  gpu can handle that very good atm, so it is ""weak"" for 4k, but it is still one of the best"
nvidia,3dqhjc,himmatsj,1 point,Sat Jul 18 15:32:23 2015 UTC,Oh I don't even run 4k...usually 2715 or 1440p
nvidia,3dqhjc,himmatsj,2,Sat Jul 18 18:45:02 2015 UTC,Lol dude 2715 is above 4k! 4k is 2160p
nvidia,3dqh68,MisterMonacle,2,Sat Jul 18 13:38:34 2015 UTC,350.12 did my 580 wonders. Check that one out. Very stable on my machine
nvidia,3dqh68,JESUSgotNAIL3D,1 point,Sun Jul 19 00:56:09 2015 UTC,I've tried multiple drivers for my 560 SE and the newest that works for me is 347.88. 350.12 and anything over that including all the hotfixes just give me tdrs no matter what I try.
nvidia,3dqh68,MarowHD,1 point,Sun Jul 19 08:00:52 2015 UTC,"For my 560ti, 320"
nvidia,3dqh68,Psychotic_oreos,1 point,Sat Jul 18 16:22:06 2015 UTC,would this work for a 560? on with 350 or whatever is most recent it frequently crashes.
nvidia,3dqh68,toad_family,1 point,Sun Jul 19 02:15:36 2015 UTC,Yeah the latest i got my 560ti to go was 320
nvidia,3dsg8r,WhichGPUz,1 point,Sun Jul 19 00:41:09 2015 UTC,You'd be fine with the super clocked edition.
nvidia,3dsg8r,StillAzure,1 point,Sun Jul 19 05:06:57 2015 UTC,"Your question is incredibly vague - price aside, what are you looking for from a prospective card? Examples including desire to watercool, noise levels, overclocking potential etc."
nvidia,3dqtz7,playingwithfire,3,Sat Jul 18 15:54:03 2015 UTC,"Honestly, the GeForce Experience is a waste of valuable drive space and pretty useless, unless you use it for the streaming/video function.   9 times out of 10, it recommends settings that are either much too low or much too high. I just set them myself, turn on Rivatuner (for fps counter) and see how it plays. If needed, adjust accordingly.   Generally, the GeForce Experience is regarded as buggy and a resource hog. Unless you absolutely need it, just get rid of it."
nvidia,3dqtz7,CaptSkunk,2,Sat Jul 18 17:10:38 2015 UTC,You're gonna get frustrated with that software eventually . it's nice to see but 99% of the time was not really what I wanted quality wise . haven't installed it for a while.
nvidia,3dqtz7,p3dr0maz,1 point,Sat Jul 18 16:31:44 2015 UTC,You might have a point. It is really laggy too. Takes a couple seconds to open and to switch tab...
nvidia,3dqtz7,p3dr0maz,1 point,Sat Jul 18 16:36:22 2015 UTC,The other reason people install it is for ShadowPlay. So it still has some use. I ended up using OBS for game capture in the end though so that was the end of that software for me.
nvidia,3dqtz7,StaticSignal,2,Sat Jul 18 16:42:28 2015 UTC,"Does Shadowplay directly link to Twitch like AMD Raptr? I could never get Raptr to work right with Twitch.  But then again, OBS works fine for me and it's configured already..."
nvidia,3dqtz7,p3dr0maz,5,Sat Jul 18 16:43:24 2015 UTC,"ShadowPlay does indeed stream right to Twitch, with multiple quality metrics you can adjust. It works great and I use it often."
nvidia,3dqtz7,MaghnusBalogh,-1,Sat Jul 18 20:30:10 2015 UTC,Nah it doesn't.  For that OBS works best.  Grats on the new card.  Enjoy! I didnt know it had built in functionality. mb.
nvidia,3dqtz7,Fukui_San,1 point,Sat Jul 18 16:48:47 2015 UTC,The biggest problem with GeForce Experience is that it tends to optimize for 40 FPS. I can usually get better results with general mucking around and setting maximum pre-rendered frames as low as possible.
nvidia,3dqggl,maxbarnyard,9,Sat Jul 18 13:29:55 2015 UTC,"Temp seems fine... Your 450w power supply DOES NOT.  That's well below the safe minimum, I wouldn't even attempt to be running without at least a 550.  Running a 450 with a 980ti is frightening and asking for serious possible stability problems.  You don't need something big, like I said 550-600 is perfect."
nvidia,3dqggl,sluflyer06,1 point,Sat Jul 18 14:45:46 2015 UTC,"Thanks, my rig at total is drawing <450W but I've been meaning to replace my PSU anyway. EVGA has an offer from the card registration of 50% off a PSU and/or mouse so it looks like I could get a SuperNOVA 750 G2 for half off. I figure it's overkill but it's the lowest wattage G2 included in the offer, so I'll snap one up when they're back in stock on EVGA's site."
nvidia,3dqggl,sluflyer06,3,Sat Jul 18 14:48:27 2015 UTC,"Your card alone overclocked is eating 300w, leaving only 150w for the entire remaining balance of components is not enough.  A small amount of excess capacity is paramount, systems experience peak load spikes well above their average or high consumption times"
nvidia,3dqggl,sluflyer06,1 point,Sat Jul 18 15:02:18 2015 UTC,Is that accounting for the 80 Plus Bronze on the PSU? I'm not overclocking the card and I'm not increasing its power draw by the available 10% in Precision X so it shouldn't be drawing above 250W before accounting for efficiency.
nvidia,3dqggl,sluflyer06,1 point,Sat Jul 18 15:03:42 2015 UTC,It might  appear you have just enough capacity if you try to calculate everything out but your playing with fire and you can end up with odd system behavior and chasing ghost issues.  Also keep in mind capacitor aging and overall bad practice to run something as crucial as your PSU at essentially maximum output all the time which only serves to increase wear and tear on the components prematurely.
nvidia,3dqggl,CharmingJack,1 point,Sat Jul 18 15:09:20 2015 UTC,"That's a good point, thanks! Since it looks like EVGA doesn't have any 750 G2 units in stock I might swing by the nearby Micro Center to see if they have any 650 G2's or Corsair CS650M (a buddy of mine has one and loves it). Any others you think I should look at?"
nvidia,3dqggl,p3dr0maz,1 point,Sat Jul 18 15:16:50 2015 UTC,"Anything from seasonic, corsair, EVGA is best."
nvidia,3dqggl,NCEMTP,1 point,Sat Jul 18 17:44:22 2015 UTC,EVGA website recommends 600 watts minimum. I'm surprised you haven't had any other issues.
nvidia,3dqggl,p3dr0maz,2,Sat Jul 18 22:40:54 2015 UTC,Cards fine. I almost convinced myself something was wrong with my card for the same reason. If you check reviews though everyone's 980 TI cards under load are going 72c and above.  That was my max with fan @ 100%.  If you check the card owners clubs on overclock.net forums you will see a lot of people water cooling these. I did and now my max load temps are 59c.
nvidia,3dqggl,p3dr0maz,2,Sat Jul 18 13:58:25 2015 UTC,"Wow, I need to set up a liquid cooling loop for my cards. Temps max at 83 and 79 under load, but stock fan curve caps fan speed at like 60% I believe. Temps in the upper 70s at 100%."
nvidia,3dqggl,p3dr0maz,1 point,Sat Jul 18 14:43:03 2015 UTC,Yeah before I did this the card would throttle itself to try and stay cool.  I think the 1st one  starts around  65c. Luckily don't get that warm anymore. I also increased the power limit up to 127%/350w. That helped a lot too.
nvidia,3dqggl,p3dr0maz,1 point,Sat Jul 18 16:19:17 2015 UTC,"Thanks! I decided to mess with my fan curve, too, dropping the 60-7c range by about 10%. In Shadow of Mordor (though I only just started it) on Ultra I was holding ~60C with 50% fans. Does this suggest that my fan curve is kinda overdoing it since it was holding the same temps at lower fan speed?"
nvidia,3dqggl,ddeuced,2,Sat Jul 18 14:46:09 2015 UTC,"Probabuy yeah . if you're 1080p/60Hz gaming that's probably fine. If youre playing in 1440p/96-144Hz I'm guessing you'll need the extra fan. For example, my hottest running games are 1440p witcher 3 project cars when vsync is off. Gpu at 99% constant."
nvidia,3dqggl,p3dr0maz,1 point,Sat Jul 18 16:22:56 2015 UTC,"I'm doing 1080p/144Hz so if that makes a difference. Are you suggesting I install an extra case fan, then? I guess I can look into an off-mobo fan controller or just connect it to the PSU via molex."
nvidia,3dqggl,ddeuced,1 point,Sat Jul 18 16:28:44 2015 UTC,Oh no I meant the other 50% fan left in your curve. You should be fine.
nvidia,3dqggl,p3dr0maz,1 point,Sat Jul 18 16:33:14 2015 UTC,"Was I misinterpreting my results with SoM? It looked like it was able to hold the same temp at less fan than it was using before, so would it be a correct interpretation to think that my fan curve was using more than necessary? I suppose I'm trying to understand the thermodynamics behind the whole thing haha, seemed like it was only using 60% fan because my fan curve was telling it to."
nvidia,3ds58f,Jayden82,3,Sat Jul 18 22:54:31 2015 UTC,"Something is seriously wrong, looks like a scam, why does it have ddr3 and not gddr5?"
nvidia,3ds58f,andrew5161,1 point,Sat Jul 18 23:05:46 2015 UTC,"Good thing I asked here before buying it, haha. I don't know much about Nvidia gpu's but I knew that they weren't this cheap, guess I'll keep looking."
nvidia,3ds58f,andrew5161,2,Sat Jul 18 23:07:09 2015 UTC,"If you want further advice, add me on steam: banisherlord There are 2 but add both 1 is an old acc."
nvidia,3ds58f,PangoWango,3,Sat Jul 18 23:10:51 2015 UTC,"Yeah, these cards pop up from time to time. They're just usually rebranded lower end cards with bad knockoff shrouds and fans."
nvidia,3ds58f,Bickell,1 point,Sat Jul 18 23:48:42 2015 UTC,"That's too bad, was hoping I found a good deal!"
nvidia,3ds58f,Subtracting,2,Sun Jul 19 03:19:59 2015 UTC,If it's too good to be true then it's always some type of scam.
nvidia,3ds58f,floydian32,2,Sun Jul 19 03:02:58 2015 UTC,GTX 650's are lower tier cards so the price is believable but since it says only GDDR3 for video memory I'd purchase a better version of that card that actually has GDDR5 like it was marketed.
nvidia,3ds58f,shinozuka,1 point,Sun Jul 19 04:28:12 2015 UTC,"I actually ended up buying a 465, which apparently has similar specs if not better (from what I've seen atleast)."
nvidia,3dpfwy,the_unusual_suspect,1 point,Sat Jul 18 04:33:22 2015 UTC,Copper. Excellent conductor of electricity and a very quick temperature dissipate. Sexy.
nvidia,3dpfwy,hawkens85,1 point,Sat Jul 18 05:58:12 2015 UTC,"I believe it conducts it better, but dissipates worse. So it'll carry it quick but will hold it longer. Hence the combo nickel-copper most use."
nvidia,3dpfwy,homogenized,1 point,Sat Jul 18 21:43:23 2015 UTC,Probably the best looking card I've ever seen.  Looks like a pound of copper on there.
nvidia,3dpfwy,floydian32,1 point,Sun Jul 19 06:31:25 2015 UTC,I like how kingpin's avatar shows suggestive girls laying on the floor scantily clad. Great idea when you are essentially a brand name/marketer.
nvidia,3drxvt,covertskippy55,1 point,Sat Jul 18 21:46:18 2015 UTC,Are you using precision tuner? Maybe unlink the power limit and temp and make power limit the priority?
nvidia,3drxvt,watsaname,1 point,Sat Jul 18 23:07:15 2015 UTC,"I'm using afterburner, I was unaware precision X had that feature. I'll give it a try. Thanks!  EDIT: Actually does it do the same thing as setting the power limit to be the priority in afterburner?"
nvidia,3drxvt,watsaname,1 point,Sun Jul 19 00:21:38 2015 UTC,Probably? I  haven't tried afterburner with my 980ti yet. Just precision tuner because my friend recommended it.
nvidia,3drxvt,watsaname,1 point,Sun Jul 19 00:24:29 2015 UTC,"Ah gotcha, well I tried making the power limit the priority in afterburner and didnt have much luck. As soon as it hits beyond 70 itll start throttling and goes back and forth as the fans keep dropping it to 69 and the room temp keep raising it to beyond 70. Man I wish its the winter here, I could save so much money on heating costs."
nvidia,3dqsg8,Eddyboytoy,1 point,Sat Jul 18 15:39:11 2015 UTC,Did you check to see if your antivirus or firewall was preventing local connections ?
nvidia,3dqsg8,chuckydidly,1 point,Sat Jul 18 21:32:04 2015 UTC,how do i check if the firewall is preventing local connections?
nvidia,3dqryu,Capture4U,3,Sat Jul 18 15:34:32 2015 UTC,Huh? Running at full speed? How dlse woult it run? In slow motion?
nvidia,3dqryu,maxiimus1,1 point,Sat Jul 18 16:57:40 2015 UTC,"Excuse me, what exactly do you mean by dlse? May sound a little dumb.."
nvidia,3dqryu,maxiimus1,3,Sat Jul 18 17:01:27 2015 UTC,"Sorry, meant else.   Like, what is mesnt by 100% speed? Every game runs at 100% speed."
nvidia,3dqryu,maxiimus1,1 point,Sat Jul 18 17:16:05 2015 UTC,"Yeah you are right, but some nvidia graphic cards are not able to handle these games, thats why i wanted to show this."
nvidia,3dqryu,maxiimus1,3,Sat Jul 18 17:24:55 2015 UTC,"Wait do you mean frame rate? Like that it runs on 60 fps?  If so, your title is confusing, because speed isn't frame rate. The games speed dorsn't change the more fps you have. It just shows more frames per second."
nvidia,3dpes7,uberdice,7,Sat Jul 18 04:21:14 2015 UTC,"Folders have been removed without destroying all life as we know it. Thanks, Reddit!"
nvidia,3dpes7,ez117,6,Sat Jul 18 10:32:26 2015 UTC,"Try copying it off to a different folder, see if you have any issues, and then if not completely delete it."
nvidia,3dpes7,FDisk80,3,Sat Jul 18 07:39:52 2015 UTC,Yes you can. Nvidia never cleans their shit. After every driver installation it only grows.
nvidia,3dpes7,Yokurt,3,Sat Jul 18 08:09:24 2015 UTC,"I setup CCleaner to include the following directories (""Options"" -> ""Include"") and never had any problems:  %ALLUSERSPROFILE%\NVIDIA Corporation\NetService\  %ProgramFiles%\Nvidia Corporation\Installer2\"
nvidia,3dpes7,ChikNoods,2,Sat Jul 18 08:24:58 2015 UTC,had to enable -> delete files and sub-folders  could figure out why it wasn't working. Thanks!
nvidia,3dpes7,PM_ME_CHIMICHANGAS,2,Sat Jul 18 18:26:13 2015 UTC,"Thanks to both of you, I just cleared out a sweet 500mb on my SSD."
nvidia,3dpes7,mercurycc,1 point,Sun Jul 19 01:52:16 2015 UTC,"No idea, is it cached stuff?"
nvidia,3dpes7,PM_ME_CLOCK_PICS,5,Sat Jul 18 04:26:06 2015 UTC,"Each folder contains a single .exe which I assume is an installer, and the folder names suggest temporary files. I'm asking because my googling has only turned up guides that say it's safe to delete all the .exe files in \NetService but not the folders... except when I checked my directory, it had no .exe files apart from what's in the folders.  I have no reason to believe they're anything but temporary copies of installers, but I'm hoping someone can confirm this for me before I delete what looks like 2.93GB of wasted SSD space that turns out to be the only thing holding the fabric of space and time together."
nvidia,3dpes7,TheFallen837,2,Sat Jul 18 04:35:08 2015 UTC,Delete them all. That's what it means. Delete all the exes in that folder and its subdirectories.
nvidia,3dpes7,pcpietjeisl,1 point,Sat Jul 18 06:57:22 2015 UTC,Boom.
nvidia,3dpes7,jeremyeyyy,1 point,Sat Jul 18 06:15:01 2015 UTC,Better safe than making TIFU.
nvidia,3dpes7,notsure1235,1 point,Sat Jul 18 06:54:17 2015 UTC,"ehh, what's the worst thing that can happen?"
nvidia,3dpes7,Bfedorov91,1 point,Sat Jul 18 07:24:12 2015 UTC,AMD has a similar thing and I deleted them a while ago and nothing happened so I am assuming nothing will happen with nividia
nvidia,3dpes7,ChikNoods,1 point,Sat Jul 18 08:07:41 2015 UTC,"yes you can, these are just backup files. It never hurts to copy the folder to some external hdd first, but  i delete that trash regularly. I only have a NVNetService.log file in that dir."
nvidia,3dpes7,michigandiscusbball,1 point,Sat Jul 18 09:21:28 2015 UTC,I have noticed FPS increases in some games by disabling nvidia's netservice or stream service.. forget what it's called.  It is only needed if you use gamestream.
nvidia,3dq72g,elkaxd,2,Sat Jul 18 11:11:54 2015 UTC,"Tried the hotfix, underclocking my gpu. The driver still crashes after 30 mins of play time."
nvidia,3dq72g,doobydoo123,1 point,Sat Jul 18 13:39:27 2015 UTC,"They released a hotfix for it, I will find it, give me a second. Are you using Chrome while playing the game by any chance?"
nvidia,3dq72g,Sethos88,1 point,Sat Jul 18 11:12:58 2015 UTC,"No, I dont leave it open."
nvidia,3dq72g,sluflyer06,2,Sat Jul 18 11:13:59 2015 UTC,".51 popped up today, in case OP was interested  http://www.guru3d.com/files-details/geforce-353-51-driver-download.html"
nvidia,3dq72g,homeless_homo,1 point,Sat Jul 18 11:55:23 2015 UTC,"Try 353.38, has been flawless for my 780 and then equally as good for 980ti sli"
nvidia,3dq72g,xTommy2016x,1 point,Sat Jul 18 12:34:58 2015 UTC,"I tried everything in most troubleshooting threads and installed the 353.49 hotfix driver and the only 100% definitive solution for me so far is completely closing Afterburner. At first I would close steam, chrome and afterburner, but I would slowly open one by one while playing and the only one that would cause a kernel crash was Afterburner. Apparently this doesn't work for everyone but I've gone from crashing every 10 minutes to not crashing in the past 6 hours. I've been playing while chatting on steam and watching 3+ streams (evo time) on chrome and haven't crashed yet as long as I make sure not to open Afterburner."
nvidia,3dq72g,homeless_homo,1 point,Sat Jul 18 14:56:17 2015 UTC,If you close afterburner does you overclock profile remain? Because I noticed a few times my fan profile would not work with afterburner closed
nvidia,3dq6zn,doobydoo123,2,Sat Jul 18 11:10:20 2015 UTC,"Yeah, definitely a problem with the latest experience update. Lots of people reporting the same issue on Nvidia's forums: link. I got the same sluggish boot problem after updating and it was solved right away by uninstalling GFE and simply installing the most recent drivers but not GFE. If you absolutely need to have GFE, the thread on the Nvidia forums does mention a possible workaround (disabling the Nvidia streamer service)."
nvidia,3dq6zn,TokeAndPlay,2,Sun Jul 19 00:44:24 2015 UTC,Thanks a lot for that! I did the workaround and my boot times are back to normal. Quick and easy.
nvidia,3dpzsy,Wifi-rape,5,Sat Jul 18 09:04:06 2015 UTC,The latest update to GeForce Experience is causing your slow boot times. Uninstall it and you should be back to normal.
nvidia,3dpzsy,Empty_Kingdom,1 point,Sat Jul 18 09:24:32 2015 UTC,Already done that and it's still taking longer then when I had my 780. :(
nvidia,3dpzsy,IvanKozlov,1 point,Sat Jul 18 09:28:09 2015 UTC,"What he said may be true for some. It is not true for me. I don't think that's the issue as I'm running the latest GeForce experience on windows 10 and my boot time is about 7 seconds.  Edit: I now see the topic on this subreddit that's talking about these issues. I am not having these issues with the latest version. Odd. Doesn't mean it's not happening though, I suppose."
nvidia,3dpzsy,IvanKozlov,1 point,Sat Jul 18 23:19:25 2015 UTC,"How can that be true? I have a Samsung 850 Evo running Windows 10 with the latest version of GeForce experience installed. My total boot time is about 7 seconds. Just timed it.  Edit: I now see the topic on this subreddit that's talking about these issues. I am not having these issues with the latest version. Odd. Doesn't mean it's not happening though, I suppose."
nvidia,3dpzsy,ChrisJSY,1 point,Sat Jul 18 23:18:56 2015 UTC,"That's what sorted it for me, came on to check if others had the issue. I set the experience service to delayed automatic start as I still need it to run."
nvidia,3dpzsy,sluflyer06,1 point,Mon Jul 20 21:27:42 2015 UTC,UEFI windows install?
nvidia,3dpzsy,Leonick91,1 point,Sat Jul 18 14:55:23 2015 UTC,Yep. The boot up turns were all normal before I installed the GTX 980 ti.
nvidia,3dpzsy,FLU773RSHY,1 point,Sat Jul 18 16:04:05 2015 UTC,"Before when there was no GPU everything loaded fast, I never saw the windows 8 logo, it just booted straight into the desktop.    Are you seeing the windows logo for a long time? Or is the delay before that?  I'm having a similar experience with my new 970. PC sits at the UEFI/Post screen for over half a minute, after that Windows itself loads almost instantly though. If I put in the previous card, R9 280, it boots to the login screen in less than 10 seconds.  Annoying. Fortunately the only time I do a full boot is when a reboot is necessary, never do a full shut down besides that..."
nvidia,3dpzsy,Leonick91,1 point,Sat Jul 18 17:00:33 2015 UTC,Yep it stays on the Bios screen for a while then the windows 8 spinning logo shows up which never used to happen before.
nvidia,3dpzsy,FLU773RSHY,1 point,Sat Jul 18 17:08:38 2015 UTC,https://www.reddit.com/r/buildapc/comments/1ymui1/troubleshooting_my_windows_startup_is_very_slow/  I too am having the same troubles. From what I understand our old drivers are messing with the new ones. I went from a 7950 to a gtx 970 so the AMD vs Nvidia drivers are probably doing it for me.
nvidia,3dpzsy,FLU773RSHY,1 point,Sat Jul 18 18:45:43 2015 UTC,"Interesting. Some people in there saying Guru3D's Driver Sweeper fixed it. Guess I'll give that a go. Seems a bit odd that drivers would impact boot at such an early stage, but then I can't come up with a better explanation either..."
nvidia,3dpzsy,FLU773RSHY,1 point,Sat Jul 18 21:03:14 2015 UTC,I tried it and it didn't work. Still super slow.
nvidia,3dpzsy,Leonick91,1 point,Sat Jul 18 21:28:18 2015 UTC,s Yep i've tried DDU and it hasn't made a difference for me.
nvidia,3dpzsy,FLU773RSHY,1 point,Sun Jul 19 17:04:23 2015 UTC,"I'm going to be doing a clean install of windows in a little bit, I'll tell you if it works."
nvidia,3dpzsy,Leonick91,1 point,Sun Jul 19 17:08:59 2015 UTC,Thank you.
nvidia,3dpzsy,FLU773RSHY,1 point,Sun Jul 19 17:12:55 2015 UTC,"Ok finally done. My BIOS for my intel dz77bh55k was at least 2 years older than its newest update, that is all i had to do. It took me all of today to figure this out xD  I must've swapped cards at least 10 times, and did 2 fresh installs of windows!"
nvidia,3dop7y,djhax04,5,Sat Jul 18 00:19:02 2015 UTC,If you want to fix this open up services and stop NvStreamSvc and change it to manual start up.
nvidia,3dop7y,chadashcroft22,1 point,Sat Jul 18 02:21:21 2015 UTC,Thanks so much for the tip! Fixed the issue for me and boot times are back to normal now.
nvidia,3dop7y,Quality_Controller,2,Mon Jul 20 08:02:59 2015 UTC,"Aye broken as fuck. Remember vaguely installing a GFE update the night before and powering down PC.  Next day i power it on, and got served with an eternal grey screen. Had to go into safemode and nuke that shit off my PC.  I'm really dissapointed in Nvidia atm, the solid drivers used to be often a good argument for purchase.  But we've been getting garbage for a long while now on the driver side.  Also what-ever happened to those glorious Kepler era performance drivers. I remember seeying such big gains with big driver releases.  Seems all we get today is an update when a new GPU is out and those Game Ready drivers. Which basicly are the same except adding some support/updated sli profiles.  Meh :("
nvidia,3dop7y,Gutterblade,1 point,Sat Jul 18 10:38:07 2015 UTC,"You're not alone! I'm getting a very slow boot on my screaming fast computer ever since I did the last Experience + Driver updates. I did a boot trace, and I'm still learning about how to interpret this so take it with a grain of salt, but it looks like the two files ""OAWrapper.exe"" and ""conhost.exe"" keep getting called alternately over and over and over again during boot. OAWrapper.exe is an Nvidia file."
nvidia,3dop7y,Trichinobezoar,1 point,Sat Jul 18 00:45:13 2015 UTC,"I just recently switched to NVIDIA and was wondering why my computer seemed to take longer to connect to the internet at boot, taking longer to shutdown, and resulting in the driver crashing when I was doing stuff like using chrome. Which was weird, since it'd be fine when I was gaming.  I uninstalled Geforce Experience, and the issues have gone away. At least when it comes to booting and shutdowns. I haven't go any driver crashes yet when using chrome, so hopefully that issue is gone.  It's a shame since Geforce Experience seems really convenient. Is it just the recent update that has caused it to be buggy?"
nvidia,3dop7y,Yaytista,3,Sat Jul 18 03:21:07 2015 UTC,"So the chrome crashes are a known issue on drivers 352.86, 353.06 & 353.30 which were FINALLY fixed on hotfix driver 353.38. Not sure about the boot issues but at least they finally unfucked that one.  http://nvidia.custhelp.com/app/answers/detail/a_id/3694/~/geforce-hotfix-driver-353.38"
nvidia,3dop7y,kittah,1 point,Sat Jul 18 05:49:45 2015 UTC,"So THAT'S what was causing all those chrome crashes?? Dang, I thought I had totally screwed up building this computer, I guess not."
nvidia,3dop7y,Trichinobezoar,1 point,Sat Jul 18 07:41:37 2015 UTC,"Yeah WTF. I went to upgrade my driver for my Titan X. Screen goes blank and computer restarts on it's own. Now my rig isn't registering the Titan X at all (shows up as a Code 43 in Device Manager). It works fine if I downgrade back to 350.12, which was the last driver I was using due to all the Chrome-related issues the latest drivers were having. Infuriating."
nvidia,3dop7y,Sasquatchimo,3,Sat Jul 18 01:01:19 2015 UTC,"As someone who is currently doing some Google-Fu to try and fix Driver crashes, which may or may not be the Chrome related ones you mentioned, Did you notice any performance drops rolling back to 350.12?"
nvidia,3dop7y,SLiiDE101,1 point,Sat Jul 18 01:15:47 2015 UTC,Nothing major. All the enhancements after that were for recent releases like Witcher 3. I'm beyond irritated by the fact that I'm apparently stuck on that driver as my computer will give me a code 43 on my Titan X for any driver beyond that.
nvidia,3dop7y,Sasquatchimo,1 point,Sat Jul 18 01:21:32 2015 UTC,"Yeah it really sucks. I switched from an AMD card because I was fed up of the lack of driver support. First few months with my 970 were fine, But now I'm running into more issues than I did with AMD. Can only imagine how much more infuriating it is when you have $1k card."
nvidia,3dop7y,SLiiDE101,1 point,Sat Jul 18 01:29:04 2015 UTC,Guess im not updating and because im not updating im not using GEXP -- this is nice. T.T
nvidia,3dop7y,ChikNoods,1 point,Sat Jul 18 01:26:16 2015 UTC,"Yeah i just downloaded the new update and like everyone said, it slowed my computer down to a halt. Ain't dealing with that shit. I uninstalled GXEP immediately. Issues were gone after restart. Really sucks."
nvidia,3dop7y,lykanprince,1 point,Sat Jul 18 03:45:02 2015 UTC,Same thing here! slow ass boot after windows logon... fix ure thing nvidia
nvidia,3dop7y,tulextreme,1 point,Sat Jul 18 04:21:34 2015 UTC,seems like i'm not alone.
nvidia,3dop7y,DarklordDaniel,1 point,Sat Jul 18 09:03:35 2015 UTC,"Yes, the latest GFE is broken and causes a really long start up time."
nvidia,3dop7y,enemy2k,1 point,Sat Jul 18 09:24:15 2015 UTC,"I had the same problem with the last GFE update, immediately after installing it booting to windows became extremely sluggish (~ 1 minute rather than instant). Obvisouly completely unnaceptable so I uninstalled GFE and all the issues are gone. Pretty pathetic by Nvidia to put out such a piece of shit broken update."
nvidia,3dop7y,TokeAndPlay,1 point,Sat Jul 18 23:02:11 2015 UTC,"I just installed Windows on a new SSD. After getting everything to where I wanted and several hours of playing games I restarted my computer. After entering in my password at the login screen it took 40-50 seconds to get to my desktop.   I thought the fault was something that I installed or the Windows installation.   After reinstalling Windows again, and installing everything again it started happening again.  Finally after disabling and reenabling everything in msconfig I finally found it was NVIDIA related and I finally found this sub.  Thanks NVIDIA for causing me several hours of troubleshooting and hassle.   Uninstalled Experience and everything is back to under 10 second boots. :-/"
nvidia,3dprou,Doctor_Turkleton,2,Sat Jul 18 06:54:46 2015 UTC,"Have you ever heard of the ""silicon lottery""?  There's a phenomenon in pcb/ processor fab where some boards (with the exact same specifications) may be able to operate at lower temperatures and higher clocks than others before becoming unstable.  The ones that can get higher are ""binned"" higher.  Those are the ones that get factory over-clocked because they are more easily able to reach that clock.  You may get lucky and get a regular 980ti that can reach the clock speed of a SC card but it's definitely not a guarantee."
nvidia,3dprou,gsparx,1 point,Sat Jul 18 16:24:06 2015 UTC,"Interesting. I had heard of it, but didn't know that they specifically sought out the overachievers of the lot to use for the factory overclocked versions of the card. Thanks for the information. I've seen some pretty aggressive overclocks on the regular 980 Ti, so hopefully I'll get that lucky."
nvidia,3dprou,kittah,2,Sat Jul 18 16:50:36 2015 UTC,"They are physically identical, literally the only difference is the SC has a +100Mhz factory overclock. The PCB is identical, the cooler is identical & the chips for the SC aren't binned any higher. EVGA only bins chips for the flagship models like kingpin or classified.  An SC model does not guarantee more overclocking headroom, it ONLY guarantees your card will run at +100Mhz (about 1320 boost) compared to reference (about 1220 boost). However basically any 980 Ti you get regardless of make or model is going to be able to hit 1400-1500Mhz boost at stock voltage. The only thing that determines that is silicon lottery.  Maxwell chips are actually a bit weird when it comes to overclocking in that they don't respond well to increased voltage except at sub ambient temps (0C or colder) so on air or watercooling you don't really see much benefit from a custom PCB with extra power/voltage since it doesn't help. On previous generations basically more voltage = higher clock with the ability to cool the chip being the limiting factor.  You can read about how maxwell is different straight from Kingpin himself here: http://kingpincooling.com/forum/showpost.php?p=30273&postcount=57  But yeah, nothing to worry about your card is going to kick ass. I also got a 980 Ti through EVGA step up so I have the same card you're getting and it's rock solid at 1480 Mhz (+251 core). This benchmark shows it at stock clocks (1220 boost) vs overclocked (1480 boost). http://www.3dmark.com/compare/fs/5343306/fs/5413053"
nvidia,3dprou,Juan-Carlos-H,1 point,Sat Jul 18 17:30:07 2015 UTC,"Thank you, this definitely puts any doubts I had to rest. I love seeing my CPU in those benchmarks as well. Now I'm even more excited!"
nvidia,3dprou,BKachur,2,Sat Jul 18 17:42:19 2015 UTC,"If you purchase two cards which are identical, but one has a default overclock, there would be no difference whatsoever from temps if you overclocked them to the same speed. In the real world they might be a difference due to having a better manufactured chip in one, but that's the silicon lottery on which one you end up with. I am not sure if the cards you mention have different coolers, and you would have to investigate that yourself. But if the SC card had a larger cooling power, it would run cooler that the default card."
nvidia,3dnj5o,RooooooooooR,3,Fri Jul 17 18:41:16 2015 UTC,"Hey guys, this is my first time ever buying an Nvidia card, and also my first time ever doing any sort of overclocking. So far I'm seeing the above performance gains with:  Afterburner:  Core Voltage: +30mV  Power Limit: 110  Core Clock: +100  Memory Clock: +350MHz  Everything seems to be stable so far after a few hours gaming. Any advice on pushing it further would be awesome.  edit: formatting"
nvidia,3dnj5o,slykrysis,6,Fri Jul 17 18:43:31 2015 UTC,"If you're happy with the gains in performance, no reason to push it further just because.  It all looks about right, but I've seen online most cards can go ~+500Mhz on the memory before it starts to crap out. But I reiterate, unless you direly need those extra FPS, just leave it where it's at and then optimize for noise/temperature."
nvidia,3dnj5o,jaymobe07,2,Fri Jul 17 18:57:24 2015 UTC,Can't remember the offsets but I can run 1442 core and 8000Mhz memory.  With same voltage offset.  73C on reference cooler.  Only game I crash in is witcher and I'm pretty sure its driver related since it crashes at stock.  Edit:  that must be boost clock but it never goes down to the 1273 that firestrike recorded.  Using afterburner
nvidia,3dnj5o,sluflyer06,1 point,Fri Jul 17 21:08:44 2015 UTC,"I've never had a crash in Witcher 3, 353.38."
nvidia,3dnj5o,jim2point0,1 point,Sat Jul 18 14:50:06 2015 UTC,Those have been the best drivers for me as well
nvidia,3dnj5o,w00t692,1 point,Sat Jul 18 17:15:50 2015 UTC,"I need every frame i can get at all times.  Clockspeeds are going to boost your minimum framerates as well, i see no reason to run it well below it's capabilities."
nvidia,3dnj5o,Lobrauski,1 point,Fri Jul 17 22:48:43 2015 UTC,"same card as you, I was able to get +120 on the core and +500 on the memory. It's been rock solid stable for about a week now. I was able to go 130 to 135 on the core clock but some games weren't liking that and crashing. even 125 would eventually crash in crysis 3 after a couple hours. I added +87mV right away though don't seem to have any issues with heat with that at all."
nvidia,3dnj5o,billjanke,2,Fri Jul 17 19:31:15 2015 UTC,You shouldn't necessarily need to add any voltage right away when overlocking.  I've been able to add +135 to more core speeds on stock voltage with my EVGA SC.
nvidia,3dnj5o,SuperCoolEGangster,1 point,Fri Jul 17 19:50:37 2015 UTC,"You can try to dial down the voltage to zero, and still push it a little bit.  I'm on reference (SLI), and I currently have the following:  Power limit 110%  Core clock + 234 (1234 Mhz total)  Memory clock + 356 MHz  Voltage + 0mV"
nvidia,3dnj5o,deonisius,1 point,Fri Jul 17 20:09:15 2015 UTC,What are the temperatures are you seeing when playing with this OC? Do you know what temperature is considered to be good and above what it should not go?
nvidia,3dnj5o,Soulshot96,2,Fri Jul 17 20:09:54 2015 UTC,"At the highest during playing gta I hit 80c, normally around mid 70s."
nvidia,3dnj5o,schwat,1 point,Fri Jul 17 22:12:49 2015 UTC,"Hmm, I got too +100 on top of the SC+ clocks and +250 on the memory. Tried +500 on the memory, but it craps out after ~4 hours of gaming, and I am too lazy atm to dial it down slowly until its stable. I am quite sure my card is capable of ~+450 though."
nvidia,3dnj5o,covertskippy55,1 point,Sat Jul 18 03:29:05 2015 UTC,"Yeah, these cards overclock pretty damn well. I'm seeing about 20% increase in firestrike graphics score vs stock clocks with +251 core (1480Mhz boost) & +350 mem. Reference EVGA 980 Ti ACX (not SC).   http://www.3dmark.com/compare/fs/5343306/fs/5413053  This is translating to about 15-20 fps improvement in Witcher 3 tested by enabling & disabling OC & loading the same saved game."
nvidia,3dnj5o,sluflyer06,1 point,Fri Jul 17 19:26:20 2015 UTC,"As a point of reference I'm able to run 1455-1468 boost clock and 3800ish on memory with no voltage increase. I haven't really run a game for many hours though, just for like half an hour to an hour. Haven't really had time to stress test gaming wise, but it seems perfectly stable.   I was even thinking about flashing the BIOS with a custom BIOS since the card is running into TDP limit with about a +30 voltage increase. I'm gaming at 1440p and while most games are able to stick to 60 some do drop a bit below that every once in a while, and since I tend to run v-sync for tearing staying close to 60 as much as possible helps.   I might even DSR to 4k although witcher tends to run near 40 mostly at 4k with my current settings.   Also my room gets HOT AS FUCK with fan speed of like 65-70 and a card temp of about the same."
nvidia,3dnj5o,covertskippy55,1 point,Fri Jul 17 22:53:15 2015 UTC,"Fan speed has no bearing on your room temperature, the card is producing the same heat output regardless of what the fan is doing."
nvidia,3dnj5o,HoloKK,1 point,Sat Jul 18 14:52:58 2015 UTC,"That was my point, the fans are moving that heat quite well, its just doing it into the room, I wasn't expecting the heat to just dissappear :p"
nvidia,3dnj5o,Noirgheos,1 point,Sat Jul 18 17:38:53 2015 UTC,"Why not Windows 8.1? Isn't it better for performance, or just the same? //Build 9200 is Win 8.0"
nvidia,3dnj5o,joshruffdotcom,1 point,Sat Jul 18 01:47:35 2015 UTC,"No crashes in games, etc? I'm seeing a lot of issues with the 980Ti on this sub... really throwing me off."
nvidia,3dnj5o,djisadud,1 point,Sat Jul 18 23:46:49 2015 UTC,Not so far.  I haven't had a single crash on mine.
nvidia,3dnj5o,Soulshot96,1 point,Sun Jul 19 00:18:19 2015 UTC,Don't  be afraid to up the core clock to 200+ mhz. You should be able to reach that easily with a minimal voltage increase
nvidia,3dnj5o,BlunTman805,3,Fri Jul 17 21:44:23 2015 UTC,"He has the SC version, he can't get +200 unless his chip is godly."
nvidia,3dnj5o,Step1Mark,2,Sat Jul 18 01:50:31 2015 UTC,SC's core clock is already up by ~100. So going up by more than another 100 is getting into the you have a amazing fucking GPU territory.
nvidia,3dnj5o,sluflyer06,0,Sat Jul 18 03:30:31 2015 UTC,"I'm curious why there's a minimum fps drop of 9 or 30fps?   Benchmarks are great and all, and even benchmarking games but shouldn't having minimum fps be 60? And average >"
nvidia,3dpit8,Orilby,2,Sat Jul 18 05:04:30 2015 UTC,"Might be silly but electronics often come with a slim adhesive plastic on them, did you remove all of them on your GPU?"
nvidia,3dpit8,Chris3013,1 point,Sat Jul 18 06:36:03 2015 UTC,The one that said remove me? Of course not. My house is burning by the way..  /s  Edit: read online and people say it's normal for a week or two. So what about the driver s question
nvidia,3dpit8,VLAD1M1R_PUT1N,2,Sat Jul 18 07:54:04 2015 UTC,It's normal to have a bit of a weird smell on new components. Just make sure your temps stay reasonable and that it doesn't smell like burning. Always go with the newest drivers if possible.
nvidia,3dodjq,MatmanAlpha,2,Fri Jul 17 22:36:47 2015 UTC,"update....after reading online and seeing similar issues, I'm going along with the ""it's the graphics card"" line of thinking. Rather than chuck it for a new one, I'm trying one last ditch attempt and baking it. The method seems to work for others, hopefully it will work for me. Although there is the chance of ""frying"" (pardon the pun) it."
nvidia,3doqoe,MorningYourLordship,1 point,Sat Jul 18 00:31:38 2015 UTC,"I'm fairly certain that it's the exact same board minus the extra vram chips. So it should fit, I'm using my 980 backplate on my 980ti so maybe that helps?"
nvidia,3doqoe,solidnitrogen,1 point,Sat Jul 18 01:35:53 2015 UTC,Call or e-mail EVGA Tech Support. They're very quick to respond.
nvidia,3doqoe,hawkens85,1 point,Sat Jul 18 06:02:57 2015 UTC,"Any update on this, I did the same as you and got a reference 980 ti cos I couldn't wait."
nvidia,3doqoe,Smockron,1 point,Mon Jul 20 12:27:32 2015 UTC,I read a post on overclock.net saying it should work.
nvidia,3dopyh,kmarsara,3,Sat Jul 18 00:25:21 2015 UTC,out of stock that lasted a whole min and 30 secs
nvidia,3dopyh,maybe_just_one,1 point,Sat Jul 18 00:27:52 2015 UTC,"I got the notification and tried to check out on my phone, I just wasn't quick enough :("
nvidia,3dopyh,joshruffdotcom,4,Sat Jul 18 01:17:09 2015 UTC,supposedly they said on the evga forums that they are ramping up production and the supply problem could go away in last july early aug
nvidia,3dopyh,Stoffel31849,1 point,Sat Jul 18 03:01:26 2015 UTC,Same here. Right now you basically have to be sitting on the store page pressing F5 to get one.
nvidia,3dpnmn,GhostOfACyborg,1 point,Sat Jul 18 06:01:13 2015 UTC,Might be one of the inherent problems of SLI/CF.
nvidia,3dpnmn,Kameezie,1 point,Sat Jul 18 06:25:38 2015 UTC,"I'm not familiar with the problems of sli, this is my first time. Sofar it has been a poor and frustrating experience."
nvidia,3dpnmn,myhtconex,1 point,Sat Jul 18 06:29:20 2015 UTC,"I have 780 ghz sli.Last games i played are gta v and assassin creeds rouge.They both are awesome in sli with no problems.I play evolve too,but i have to turn off sli because the sli profile isnt perfect and casuses stutter.Over all,my sli experience is great and sucked only when the game's sli profile isnt great."
nvidia,3dpnmn,myhtconex,1 point,Sat Jul 18 06:40:26 2015 UTC,The two games I have had crash are BF4 and Witcher 3. They both run great until then suddenly freeze while on sli.
nvidia,3dpnmn,kapella,1 point,Sat Jul 18 06:46:26 2015 UTC,Could be the driver.I always tell ppl to try out their sli with Sniper elite 3.that game can be your sli benchmark.Because it has a great sli profile.Try it out and see.Dont froget to switch back to more stable drivers if you still have freezing problems
nvidia,3dpnmn,kapella,1 point,Sat Jul 18 06:50:50 2015 UTC,"I have 2x980tis and havent had crashing. Well in the way you have stated anyway. Typical random TDRs in witcher but other than that...I am at 4k not 1440 but I feel that is not relevant to crashing. What drivers are you on? Are you overclocking the cards? SLI does have its cons, but you should NOT be having these issues. Especially with BF4. The cards dont break a sweat in BF4 in 4k. Havent had a single crash running bf4. Like myhtconex said, could be the driver. If youre not on 353.38 (hotfix) give it a shot http://nvidia.custhelp.com/app/answers/detail/a_id/3694/~/geforce-hotfix-driver-353.38"
nvidia,3dpnmn,kapella,1 point,Sat Jul 18 07:11:26 2015 UTC,"I'm using the 353.30 drivers, I'll try that and see what happens. I am not overclocking and they both work fine individually.  On a side note, do you use the nvidia experience optimized game settings or edit them from the game menus?"
nvidia,3dpnmn,sluflyer06,1 point,Sat Jul 18 07:23:14 2015 UTC,I tried the updated drivers and my computer froze after playing bf4 for 1 minute.
nvidia,3dni7u,dng25,3,Fri Jul 17 18:34:26 2015 UTC,It already says auto-notify for me. Did it really go that quickly?
nvidia,3dni7u,Jackster1209,2,Fri Jul 17 18:46:30 2015 UTC,"Probably, it's a hot ticket item. All of the 980 Ti's were and now this being a newly released thing it'll go through the same stock flows."
nvidia,3dni7u,slykrysis,2,Fri Jul 17 18:52:48 2015 UTC,"Well darn. Was looking forward to getting this one in particular. Oh well, hopefully they get more in stock soon."
nvidia,3dni7u,Jackster1209,0,Fri Jul 17 18:56:11 2015 UTC,I think it sold out within 5 minutes.
nvidia,3dni7u,djisadud,3,Fri Jul 17 18:54:28 2015 UTC,Mine just arrived with a mediocre 67% ASIC score.  I guess it beats the 64% + coil whine of my G1.
nvidia,3dni7u,big_three,2,Sat Jul 18 01:10:22 2015 UTC,"I'm not sure ASIC means jack for these cards, people with barely above 60% ASIC scores are getting 1500Mhz OC's - mine's 70% and I can't get above 1425."
nvidia,3dni7u,djisadud,2,Sat Jul 18 01:16:04 2015 UTC,"Kingpin said that ASIC was a pretty good measure of GM200.  I've had 5 GTX 980 Ti's now.  Two reference in SLI, returned for one G1 with coil whine, returned for another G1 with coil whine, now a Classified.  So far, the Classified with 67% is benchmark stable at 1520, however, I am getting artifacts at that clock, so I have to back it down to 1475 until I don't get artifacts.  Good enough for me.  My 64% G1 would go to 1490 with fewer artifacts, but could not push 1500 without crashing.  My G1 with 69% could get 1505 with no artifacting and no crashes.  My reference cards could get ~1430 at 65% and 59%.  I'm happy with the Classified.  It's quieter than the G1 at the same fan curve and clocks just about the same.  I think I'm finally done returning 980 Tis, thank god.  The G1 coil whine was too much for me."
nvidia,3dni7u,sn1313,2,Sat Jul 18 01:33:19 2015 UTC,Makes me sad that in March or Feb I paid the same price for a regular 980 classified.
nvidia,3dni7u,homogenized,1 point,Fri Jul 17 19:17:51 2015 UTC,Can't you step up from march to june?
nvidia,3dni7u,sn1313,1 point,Sat Jul 18 21:45:17 2015 UTC,I punched in my info and I registered the day I got my card. No longer able to step up it said.
nvidia,3dni7u,homogenized,1 point,Sat Jul 18 22:06:13 2015 UTC,"Shit, sucks. I figured since the TI came out in june you woulda had a chance. Well, classy's and nvidia hold their value, so you could still sell. Or SLI on the cheap?   Really wish i sold my unused 970 months ago, lost at least a $100 waiting."
nvidia,3dni7u,sn1313,1 point,Sun Jul 19 00:00:51 2015 UTC,And it's july...I may have gotten the card the last week of feb.
nvidia,3dni7u,koolaidman04,2,Sat Jul 18 22:06:57 2015 UTC,"Does anyone have a review link for this? Specifically one that covers the sound that the card makes, preferably with video."
nvidia,3dni7u,TheRealGecko,1 point,Fri Jul 17 20:09:34 2015 UTC,"So far, no reviews are out yet."
nvidia,3dni7u,lebaje,1 point,Sat Jul 18 15:46:51 2015 UTC,Would it be a reason why people order Evga instead of Asus...or order an Asus instead of a Evga ?
nvidia,3dni7u,roastpuff,3,Fri Jul 17 20:40:23 2015 UTC,Warranty... looks... brand loyalty?
nvidia,3dni7u,FunktasticLucky,7,Fri Jul 17 21:13:36 2015 UTC,Asus horror stories with their warranty process.
nvidia,3dni7u,lebaje,1 point,Fri Jul 17 21:57:15 2015 UTC,"Never had to used their warranty, my asus 780 is working perfectly!"
nvidia,3dni7u,FunktasticLucky,1 point,Sat Jul 18 14:31:32 2015 UTC,While that may be... EVGA has a better warranty process. They also allow you to replace the cooler and such as long as there isn't physical damage to the card and is returned to them in Stock configuration. It's a win win.
nvidia,3dni7u,CaptSkunk,2,Sat Jul 18 15:10:07 2015 UTC,"Because the Classified is made from a binned chip and an extra robust power delivery system. The whole thing just screams quality and ocerclocking. Overclocking is this things main purpose in life. Well that and making fun of the other graphics cards...  ASUS has just been making inferior cards in the last few years. Heck, they couldn't even be bothered to put the right heatsinks on the right cards. Ask owners of the 290X about that one (supposedly, there is now an Nvidia card using an AMD heatsink).   Then, take all of the horror stories of practically non-existent customer service from ASUS and factor that in. If your Strix breaks, you are more than likely going to have to fight to get an RMA. I've heard plenty of stories where the customer was blamed for breaking things that are clearly caused by faulty parts.   EVGA on the other hand, does not care how the stuff broke. As long as you can make it look stock, then they will RMA your hardware. Heck, they don't care if you remove the heatsink, attach a Scythe Mugen to the chip and break it. Just slap the stock heatsink back on, wait a week and you'll have a new card.  ASUS would tell you to get lost.   The only thing I feel that ASUS does good are motherboards and even those are overpriced, have the same bad QC/CS and are not better than other boards from say MSI, Gigabyte or EVGA."
nvidia,3dni7u,BKachur,1 point,Sat Jul 18 05:23:06 2015 UTC,"I can't speak for Asus's warantee service because I've never used their products however with EVGA, if I have a question they have a 24 hour help desk stocked by people in California. Your on the phone with them in less than 5 min and they have the best RMA service in the industry. Asus is from Taipei, not that there is anything wrong with that, but there have been horror stories on this and other subs about people dealing with their crap."
nvidia,3dni7u,Sapass1,1 point,Sat Jul 18 06:53:27 2015 UTC,You can flash bios and change cooler and still have full warranty with the evga classified.
nvidia,3dp5oc,EnderGaming11,1 point,Sat Jul 18 02:51:20 2015 UTC,"I'm assuming you're checking for it under ""Global Settings""?"
nvidia,3dp5oc,SPapaJr,1 point,Sat Jul 18 05:50:10 2015 UTC,Yup
nvidia,3dp5oc,SPapaJr,1 point,Sat Jul 18 05:58:29 2015 UTC,Do you have G Force Experience installed?
nvidia,3dp5oc,SPapaJr,1 point,Sat Jul 18 07:16:35 2015 UTC,"Yes, the latest version"
nvidia,3doc4o,Tom_A_Haverford,3,Fri Jul 17 22:25:01 2015 UTC,"Running non reference cards in SLI is always a pain. If you put them next to each other, they'll heat up no matter what. I had this with non-ref 970s.   There are a few solutions that might help you:   if your mobo supports it, place 1st in the 1st PCI-E slot and 2nd one in 3rd slot, no all mobos do have this feature though. That'll create a space between the cards and make it a little bit better. side fan blowing some cool air at the cards helps as well, you might also try side fan as an exhaust.  i had an extra 120 mm fan attached to the front with a DIY frame blowing some air in between the cards, that helped a bit too. huge CPU air coolers like Noctua NH-15 or Dark Rock Pro 3 also take some space, I've switched to Swiftech H220 and the fans on rad helped to increase air flow as well if your cables aren't well organised - do it. Well laid-out cables can also help you increase the airflow."
nvidia,3doc4o,ariyapl,1 point,Fri Jul 17 22:30:37 2015 UTC,Something like that:  http://i59.tinypic.com/dfanip.jpg  http://i59.tinypic.com/30xdgk4.jpg
nvidia,3doc4o,ariyapl,1 point,Fri Jul 17 22:34:37 2015 UTC,I have the air 540 and sli gaming 6g 980tis. My top card hits 75c max but I modded a 140mm fan to the side panel.
nvidia,3dobst,QuenIgni,2,Fri Jul 17 22:22:10 2015 UTC,Any solution to this for those that want geforce experience and the latest versions?   Or just wait for nvidia to fix it.
nvidia,3dobst,Merkavar,1 point,Sat Jul 18 10:06:14 2015 UTC,shadowplay is not working on the witcher 3
nvidia,3dobst,ingmferrer,1 point,Mon Jul 20 01:22:58 2015 UTC,now it runs again :-) Update the games list (F5)
nvidia,3dobst,FarOut-de,0,Mon Jul 20 18:00:33 2015 UTC,"Yeeeaaaahhh, unistall GFE. Don't know why anybody would want it in the first place.  Seriously. Uninstall GFE and do a clean install of 353.06 using DDU, unless you have a Maxwell GPU, then use the latest hotfix drivers."
nvidia,3dobst,wanderjahr,3,Fri Jul 17 22:43:09 2015 UTC,Shadowplay?
nvidia,3dobst,Shyguythrowaway2,1 point,Fri Jul 17 22:55:56 2015 UTC,"I did everything you told me to and it worked like a charm! I uninstalled GFE, did a clean install of the 353.30 Game Ready Driver, and my game is back to the performance it used to be (in fact, I think it might have improved a bit).  Thank you so much!"
nvidia,3dobst,wanderjahr,1 point,Sat Jul 18 01:31:03 2015 UTC,"Awesome, glad to hear!!!  Now just to wait for 1.07......"
nvidia,3dobst,Fernichu,1 point,Sat Jul 18 01:35:00 2015 UTC,I did what you said and I lost 30 FPS... Ouch...
nvidia,3dntkf,Fusyion,3,Fri Jul 17 19:58:30 2015 UTC,"This will not work since the kit that comes with the 980ti cooler is configured for a reference board.  I have a titan x and bought the  980ti kit.  All parts fit perfectly.  I had an extra kit I bought for my little brother's 980 ti SC ACX 2.0, and it did not work.  We did not have the proper mounting holes to screw the mounting kit on."
nvidia,3dntkf,Dr_Paper,2,Fri Jul 17 20:21:09 2015 UTC,"I thought the card was a reference PCB, so I thought it would work. Shame, but thank you for letting me know about your experience!"
nvidia,3dntkf,Dr_Paper,4,Fri Jul 17 20:24:15 2015 UTC,"http://i.imgur.com/xRCNkph.jpg  Here is a pic of the screws points required.  If you look at the edges, those points are required to screw the plate and secure the housing.  The other big part is the ACX 2.0 has both fans connected to each other.  A reference only has one fan.  The hybrid kit operates with the gpu water cooler and a single fan for the vrms.  Unfortunately, you cannot separate the fans from each other in the ACX 2.0."
nvidia,3dntkf,Dr_Paper,1 point,Fri Jul 17 20:28:23 2015 UTC,"Thank you for the in-depth reply! It would seem that's not going to be an option then.   However, a custom water block (such as this one from EKWB should work, right, if I were to go for a full water cooling setup?  Edit: According to the compatibility list this should work."
nvidia,3dntkf,cyberpunk7t9,2,Fri Jul 17 20:46:26 2015 UTC,"No problem man.  I will be water cooling mine cards as well.  I wish I could answer your question, but if the compatibility says so the yes?  Good luck on the water cooling, and your gpu is a beast.  It seems the 980 Ti's achieve better OCs than Titans.  My brother's 980 Ti has an ASIC score of 68 but achieves a higher OC than both my Titan Xs (which have ASIC scores of 71 and 75)."
nvidia,3dntkf,jaymobe07,2,Fri Jul 17 20:55:30 2015 UTC,"Thank you again for your advice. I actually saw your Titan X identity crisis thread this morning, took me a moment to put one and one together when you linked an image from that album.  At the moment I'm successfully running both my cards at +240 MHz on the core clock and +440 MHz on the memory clock without increasing the voltage. Would love to clock them a little higher but with this heat wave I can barely stand playing games on stock temps.  Good luck to you too!"
nvidia,3dntkf,Cozmo85,2,Fri Jul 17 21:48:37 2015 UTC,No. The Hybrid kit comes with a shroud only (no heat sink). This shroud fits over the NVTTM heatsink+fan.
nvidia,3dntkf,bobutuber,2,Fri Jul 17 20:24:17 2015 UTC,Corsair is supposed to be releasing a adapter for the 980s to use their h series aio.
nvidia,3dntkf,joshruffdotcom,2,Fri Jul 17 21:22:30 2015 UTC,It requires the reference heatsink/fan assembly
nvidia,3dntkf,hdshatter,2,Fri Jul 17 21:45:18 2015 UTC,Just to piggyback on this thread.  I saw the thread where the guy put a 980 ti hybrid kit on his Titan X card. Would it work the other way of putting the Titan X Hybrid kit on a reference 980 Ti?
nvidia,3doedi,DonDalle,5,Fri Jul 17 22:43:46 2015 UTC,Go with GIGABYTE. I totally don't work for them :D  Joking aside... All the 980 Ti's are great. I'm sure you'll be happy with any of them.
nvidia,3doedi,GBT_Van,2,Sat Jul 18 00:26:03 2015 UTC,"Been eyeballing that Gigabyte for a couple of weeks now, and that Metal Gear deal makes it even more tempting. But Im waiting to see if AMD gets anything figured out with the Fury though..."
nvidia,3doedi,Colorfag,1 point,Sat Jul 18 02:13:16 2015 UTC,"I feel like there isn't a clear winner right now. I am in the same boat as you unsure of which one to buy, but my purchase date is about a month from now. The G1 seems to be the one that everyone wants, but I have heard a lot of horror stories about the coil whine on here and /r/buildapc for various reasons. I had my heart set on the Hybrid, but I don't know if I will ever see another one in stock. I think the Gigabyte guy is right. Buy whichever you like, and just be prepared to spend a bit on cables. Should only be another 30 bucks or so. Good luck with the upgrade."
nvidia,3doedi,Peer_o,1 point,Sat Jul 18 06:59:23 2015 UTC,"I went with reference because I planned to go sli. Got my 2nd card a couple weeks ago and I am very happy with the results. I'm able to push them to about 1450 core at solid temps while @ 4k. Reference design is typically better when going sli due to the blower style cooler. Heat is blown out the back as opposed to most aftermarket coolers, which cool the card itself more efficiently than the reference, but disperse the best INTO the case. Not out the back like ref. I haven't seen any videos or posts about people going sli with 2 non reference 989tis, but I believe the cards would soak up their own heat they dump.  75 degrees max for me with a custom fan profile and a 230+ core oc is perfect for me. Plus they look fucking bad ass lol  Edit-This was just my mindset from the beginning since I knew I was going to sli. If you plan on sticking with a single, go for aftermarket 100%"
nvidia,3dn32u,Omninnihilation,4,Fri Jul 17 16:42:30 2015 UTC,"acx 2.0+ means backplate, that's the only difference.  If it has a backplate then it's the +"
nvidia,3dn32u,pj530i,2,Fri Jul 17 16:49:33 2015 UTC,"That's false, the + refers to the new triple copper heatsink design.  Here's a GTX 970 2.0+ with no backplate:  http://www.newegg.com/Product/Product.aspx?Item=N82E16814487088  Look at the top-down picture of the fans and notice the copper heatpipes on the heatsink."
nvidia,3dn32u,TaintedSquirrel,1 point,Sat Jul 18 00:05:42 2015 UTC,Is it possible to tell if it has a backplate without opening the box? I want to see if I need to return it or not.
nvidia,3dn32u,pj530i,1 point,Fri Jul 17 16:51:58 2015 UTC,"I guess look for the word ""backplate"" on the box.  If the model number is correct it's probably the correct card.  My 980ti ACX 2.0+ box had ""backplate"" on it: http://i.imgur.com/wN70LR1.jpg  If you bought it from EVGA directly, just open it.  If it's wrong I'm sure they will send you the backplate or give you a replacement card that is correct."
nvidia,3dn32u,pj530i,1 point,Fri Jul 17 17:05:16 2015 UTC,"Scanned QR code on the back, got this:  http://www.evga.com/articles/00895/EVGA-GeForce-GTX-960/ It's weird how the box doesn't say ACX 2.0+ but the qr code does"
nvidia,3dn32u,quadraphonic,2,Fri Jul 17 17:21:44 2015 UTC,"Looks like I was mistaken about the + designation.  For the 960, only the FTW cards have a backplate, and the ACX 2.0+ just means the two fan cooler.  There is no 960 ACX 2.0.  You have the right card."
nvidia,3dn32u,Archibald_Seuss,1 point,Fri Jul 17 18:01:23 2015 UTC,Thanks mate!
nvidia,3dn32u,slykrysis,1 point,Fri Jul 17 22:31:55 2015 UTC,"Trust the product number, if that matches up with 2.0+ you're good.   2.0+ included a new cooling plate to help with VRM temp management, in addition to improved fan efficiency (read: lower power usage).   For the 970 (and presumably the 960), there is now a copper baseplate that contacts the GPU as opposed to just the heat pipes (which are now straight).  Back plates only come on/with FTW editions IIRC, though you may be able to buy one for your 960 model separately (I bought one for my SSC 970)."
nvidia,3dn32u,TaintedSquirrel,1 point,Fri Jul 17 18:30:21 2015 UTC,I have this same card.  Mine came with a backplate from evga
nvidia,3dp2eq,The_Everchanging,2,Sat Jul 18 02:19:36 2015 UTC,"There was an issue introduced on 352.86 that causes crashing in chrome/desktop, etc. It persisted across 352.86, 353.06 & 353.30. It was FINALLY fixed on 353.38 hotfix driver which you can find here: http://nvidia.custhelp.com/app/answers/detail/a_id/3694/~/geforce-hotfix-driver-353.38  This should fix the issue. If not roll back to 350.12 (last whql driver without the problem) & wait til the next WHQL driver rolls out.   Can read more about the issue here: https://forums.geforce.com/default/topic/833693/geforce-drivers/nvidia-driver-352-86-353-06-stops-responding-and-kernel-reloads/"
nvidia,3dp2eq,kittah,1 point,Sat Jul 18 05:57:52 2015 UTC,"Not necessarily. I just had two crashes with the 350.12 drivers while just on Chrome which was SUPER weird because I've had these drivers for a good 2,3 months and there's been no problem"
nvidia,3dp2eq,MrChangg,1 point,Sun Jul 19 20:33:27 2015 UTC,The timing issue that caused the widespread TDRs with Chrome wasn't introduced until 352.86 so if you have Chrome crashes on 350.12 you are experiencing a different problem.   This is the thread started by ManuelG from Nvidia about the 352.86 TDR issue and in the first post he mentions it not happening on 350.12: http://forums.guru3d.com/showthread.php?t=399780
nvidia,3dp2eq,kittah,1 point,Sun Jul 19 22:37:51 2015 UTC,Guess my card just bugged out for a bit. I restarted my PC and it's been fine
nvidia,3dp2eq,MrChangg,1 point,Sun Jul 19 22:42:28 2015 UTC,"Disable Hardware Acceleration in Chrome and it fixes it, or get the newer drivers."
nvidia,3dntzt,BusFullOfChildren,3,Fri Jul 17 20:01:59 2015 UTC,"Seeing artifacts suggests that the GPU is dying, unfortunately.  As far as I'm aware (and from my own experience), the known TDR issues with the recent drivers don't involve artifacts showing up."
nvidia,3dntzt,Aldarro,1 point,Fri Jul 17 20:33:40 2015 UTC,The weird thing is that re-installing windows prevents this from happening for around a week
nvidia,3dntzt,Aldarro,2,Fri Jul 17 20:38:18 2015 UTC,"Yeah, I'll give you that. That certainly is weird.  You could try contacting EVGA to see what they think?  Edit: just saw your edit! Sorry to hear it died. RIP"
nvidia,3dmiv0,bramabul5353,3,Fri Jul 17 14:02:23 2015 UTC,I got random stuttering at 60 locked until I unlocked it.  Unlock it and put vsync on.  That seems to be the smoothest.
nvidia,3dmiv0,TruckChuck,3,Fri Jul 17 15:52:48 2015 UTC,"What's the refresh rate of the monitor?  If it's 60Hz, keep Vsync on unless you like screen tearing  If it's 144hz, keep Vsync off."
nvidia,3dmiv0,ingo2020,2,Fri Jul 17 14:05:32 2015 UTC,"it is a plasma tv vt60 I have vsync on, but is a plasma actually capable of displaying over 60fps?"
nvidia,3dmiv0,ingo2020,3,Fri Jul 17 14:09:30 2015 UTC,"Unfortunately, no."
nvidia,3dmiv0,kittah,2,Fri Jul 17 14:15:02 2015 UTC,Possible but unlikely. Some TVs that support active 3d can do 120hz but its not terribly common. Can read more about it here: http://www.blurbusters.com/overclock/120hz-pc-to-tv/
nvidia,3dmiv0,schoff,2,Fri Jul 17 17:35:28 2015 UTC,shinanigans
nvidia,3dmiv0,shatteredhalo0,2,Fri Jul 17 14:51:59 2015 UTC,"Is it normal for this card in Witcher 3 to have GPU usage hover around 80ish% with everything maxed at 1080p?  I have a 4790K, so I wouldn't think that it's a CPU bottle neck.  I was under the impression that your GPU load being under 99-100% was often indicative of a bottleneck or inefficiency."
nvidia,3dmiv0,GeneralPickle,1 point,Fri Jul 17 16:09:22 2015 UTC,"Do you have the framrate locked? If so, that could be why. A 980ti could easily push beyond 60fps, so if it's locked to 60fps then I imagine it wouldn't be working that hard.  Personally I'm running SLI 780's OC'd and a 4790K, game settings maxed (minus hairworks) and I stay around 60fps give or take 10 at times. My roommie has a 980ti so I'll ask him what he gets for GPU usage when he gets home."
nvidia,3dmiv0,shatteredhalo0,1 point,Fri Jul 17 19:00:57 2015 UTC,"The GPU utilization actually seems to stick at 99% now after installing the hotfix nvidia drivers 353.38, so I'm not sure what that was about.    I'm getting weird behavior though, where my boost core clock drops from 1506 to 1499 when the card hits 68-70C or higher, even though I'm pretty sure the hardware limit nvidia set is 84C.  GPU Z doesn't show thermal throttling so I'm kind of at a loss as to why it's randomly deciding to downclock 7 mhz at a specific temperature, but a low temperature at that."
nvidia,3dmiv0,Soulshot96,2,Sat Jul 18 00:25:03 2015 UTC,"I use Vsync with unlimited. Idk if it changes shit, I just want to be able to toggle Vsync and check my max FPS in a area or whatever on the fly, with one setting."
nvidia,3dmiv0,motleyguts,1 point,Sun Jul 19 02:19:08 2015 UTC,I feel like having it locked will minimize physx anomalies in some cut-scenes.
nvidia,3dmiv0,GhostOfACyborg,1 point,Sat Jul 18 07:27:04 2015 UTC,What do you use to monitor the fps in game? I am running sli 980ti and Witcher 3 still doesn't seem as smooth as it should be for me.
nvidia,3dmiv0,go_balls_deep,3,Fri Jul 17 17:36:08 2015 UTC,"Not OP obviously, but I use Afterburner. I have a single 980ti on 1440p and get 60-80fps everything on ultra except hairworks. What are the rest of your system's specs?"
nvidia,3dmiv0,GhostOfACyborg,1 point,Fri Jul 17 17:58:29 2015 UTC,"EVGA GeForce GTX 980 Ti 6GB Superclocked+ ACX 2.0+ Video Card (2-Way SLI), Acer XB270HU 1440p 144hz, Intel Core i7-5820K 3.3GHz 6-Core Processor, Corsair H100i GTX 70.7 CFM Liquid CPU Cooler, MSI X99S Gaming 7 ATX LGA2011-3 Motherboard, Corsair Vengeance LPX 16GB (4 x 4GB) DDR4-2666 Memory, EVGA SuperNOVA 1000G2 1000W 80+ Gold.  It runs smooth but it just doesn't seem as smooth as other games, maybe it is the game camera? I have it set to ultra with unlimited frames."
nvidia,3dmiv0,go_balls_deep,1 point,Fri Jul 17 18:06:12 2015 UTC,"Yeah your system is much better than mine, are you talking about when you pan the game camera it looks a bit jittery? I think that is a game bug, it is better after the last few updates. It could have to do with sli possibly I suppose, I don't have much experience with it though. I came from a 970 Strix, so maybe that is biasing my perception too. I play on a qnix 1440p overclocked to 96hz, and it is smooth as butter in my eyes."
nvidia,3dmiv0,GhostOfACyborg,1 point,Fri Jul 17 18:14:32 2015 UTC,"Yeah the game camera seems a bit jittery, the character animations and movement all look really smooth but the game camera seems a little off. Its not a big deal but I do notice it when I pan the camera, I also have super high mouse sensitivity so it seems to be more noticeable."
nvidia,3dmiv0,go_balls_deep,1 point,Fri Jul 17 18:26:06 2015 UTC,"Ah yeah, I play it with a 360 controller so that might make a difference as well. Probably just their wonky camera setup though. You could use Afterburner to keep an eye on your frame times but I'd imagine you're getting at least 100fps so I doubt it's your system."
nvidia,3dmiv0,GhostOfACyborg,2,Fri Jul 17 18:30:15 2015 UTC,I was thinking about trying out a controller. It seems like witcher 3 is better for a controller since it was probably created with the consoles in mind. I'll try using a ps4 controller and see how it is. I think it may just be the weird camera .
nvidia,3dmiv0,billjanke,1 point,Fri Jul 17 18:35:05 2015 UTC,"Yeah, game is best play on controller.    Camera isn't always ideal.  Also there is a noticable delay in the character animation and speed.  I believe the new 1.07 patch will allow Geralt to be a bit more nimble in combat with a faster reaction option."
nvidia,3dmiv0,an_angry_Moose,1 point,Fri Jul 17 18:43:16 2015 UTC,Are you quite overclocked? Most reviewers on test rigs report the average FPS on Witcher 3 everything cranked except hairworks off 1440p to be around 60FPS on a 980Ti. That's a big difference from 60-80 that you claim.
nvidia,3dmiv0,go_balls_deep,1 point,Fri Jul 17 22:18:04 2015 UTC,"Yup, running at 1491mhz core. But also I didn't mean to imply that my average was anywhere near 80fps, just that it varies between 60 to 80, I think the highest I saw was 84fps. Most of the time it's in the 60's though."
nvidia,3dmiv0,sluflyer06,2,Fri Jul 17 19:34:17 2015 UTC,Afterburner +1.  I also have SLI 980ti and game runs like buttah.  I didn't bother to look at how to unlock it from 60fps but my load % is about 52% on ultra with hairworks at 1080p. (waitin for some ultrawide G-Sync monitors)
nvidia,3dmiv0,Love2Nurb,-2,Fri Jul 17 19:50:08 2015 UTC,"use the nividia application, its optimizes games on your drive."
nvidia,3dmiv0,ForTheBread,3,Fri Jul 17 18:08:55 2015 UTC,I find that if often uses settings that I wouldn't use. Such as motion blur and such.
nvidia,3dmiv0,Love2Nurb,1 point,Fri Jul 17 16:10:11 2015 UTC,i use it as a base then make changes after.
nvidia,3dn1c7,Midnaspet,2,Fri Jul 17 16:29:00 2015 UTC,"Only 1440p G-Sync monitors are the ROG Swift and the Acer XB270HU - both monitors that are over $700 even on a good day.  This Acer is much cheaper than the XB270HU both due to it's lack of a G-Sync module and because it isn't an IPS like the XB270HU. The Asus ROG Swift is more expensive mostly because it's an Asus - in general one could argue that Asus products are better made/receive better support than with Acer (at least in my experience)  There are no high-resolution/high-refresh rate G-Sync monitors in that price range. There is the 4k Acer G-Sync model, but thats a 60Hz display."
nvidia,3dn1c7,patriotsfan82,1 point,Fri Jul 17 16:49:17 2015 UTC,so if I want 1440p 144hz (I dont care about adaptive sync as much) this is pretty much my only option in this price range?
nvidia,3dn1c7,winter2,2,Fri Jul 17 17:22:01 2015 UTC,There is also Acer XG270HU  (freesync) for 500usd
nvidia,3dn1c7,winter2,2,Fri Jul 17 17:28:29 2015 UTC,I linked that in my post
nvidia,3dn1c7,patriotsfan82,1 point,Fri Jul 17 17:30:49 2015 UTC,oh sry didnt notice that. So it looks there isnt another.
nvidia,3dn1c7,p3dr0maz,1 point,Fri Jul 17 18:20:36 2015 UTC,"I've been a little out of the Korean-panel loop, but the answer is probably yes.  There is the Asus version of that display: http://www.newegg.com/Product/Product.aspx?Item=N82E16824236466, but it runs for more.  Beyond that, people use to purchase Korean 1440p IPS displays and overclock them to attempt to get up near 120Hz or so. If you search for ""Qniz Overclocking"" you'll find some examples. The obvious caveat here is that overclocking Monitors like anything else is a do at your own risk/no guarantees endeavor. Not sure what the foreign display market looks like for official 144Hz monitors."
nvidia,3dn1c7,NobleArchitect,1 point,Fri Jul 17 17:36:53 2015 UTC,"im basically paying the extra 150 for the stand. which if I had the money I would do, I love asus stands. but i will likely just go with the acer."
nvidia,3dn1c7,jerrolds,1 point,Fri Jul 17 18:28:14 2015 UTC,I got 1440p @ 96Hz on a koren samsung IPS for $300. just fyi in case you're looking for options.  Just beware the case it comes in is definitely not top quality but the panel was amazing.  That acer looks really nice though. Extra $200 but you get a nice jump in Hz.
nvidia,3dn1c7,jerrolds,1 point,Fri Jul 17 18:20:34 2015 UTC,Get a used rog swift off Amazon when back in stock. They were $525. Sold out at $450 for the amazon sale. Mine has no signs of ware at all. Great monitor.
nvidia,3dn1c7,jerrolds,0,Fri Jul 17 18:44:02 2015 UTC,"If you haven't heard the Korean panels like the QNIX QX2710 overclocks to 120hz, 1440p and IPS.  Got mine for $260a couple years ago and there are no monitors or now that is worth upgrading to for me.   Check it out if your a feeling a bit brave."
nvidia,3dn1c7,jerrolds,1 point,Fri Jul 17 19:29:12 2015 UTC,some people are reporting false positives and frame skipping above 60 hz on newer models. Any insight?
nvidia,3dn1c7,Sneerz,1 point,Fri Jul 17 20:51:46 2015 UTC,some people are reporting false positives and frame skipping above 60 hz on newer models. Any insight?
nvidia,3dn1c7,AexraelDex,1 point,Fri Jul 17 20:51:46 2015 UTC,"Not sure about the newer ones.. That's too bad if that's the case.. I'll have to visit overclock.net and catch up. I own an older QNIX panel and I can say with absolute certainty that it does not frame skip, it is a true 120hz (or whatever your can hit)   Best pc purchase I've made in the last 3 years.  Blurbusters don't mention anything new about the QNIX.."
nvidia,3dn1c7,bowgamer,1 point,Fri Jul 17 21:22:29 2015 UTC,"since they advertise 120 hz, could I return it if it doesnt work?"
nvidia,3dnbfm,NebulousWolf,1 point,Fri Jul 17 17:44:11 2015 UTC,That firestrike score is pretty much right on the nose for a 770 and a stock 4670k.
nvidia,3dnbfm,the_unusual_suspect,1 point,Fri Jul 17 17:47:49 2015 UTC,Really?   I looked some up and my CPU in particular seems to be underperforming? Thanks for the answer tho!
nvidia,3dnbfm,Fusyion,2,Fri Jul 17 17:49:32 2015 UTC,"You are probably comparing your CPU to other CPUs that do have Hyperthreading. 3DMark loves extra cores/threads, but your score is absolutely fine, and in games Hyperthreading doesn't benefit you a bit."
nvidia,3dnbfm,the_unusual_suspect,1 point,Fri Jul 17 19:24:04 2015 UTC,Thanks!
nvidia,3dnbfm,the_unusual_suspect,1 point,Fri Jul 17 19:51:54 2015 UTC,"Nah, even my 4670k only gets as high as 9k on physics with an overclock."
nvidia,3dn3n1,dentidcapacitor,1 point,Fri Jul 17 16:46:50 2015 UTC,"ASIC score is not a reliable indicator of performance.  My card has an ASIC score of 83% and it's not really any better than cards that score in the 60s or 70s.  The bios may help, or it may not.  If it clocks more aggressively, it may increase temps, which may in turn decrease clocks.  These cards start scaling back boost in the mid 60 degree range so to get any more performance out of your card you will probably need to upgrade the cooling.   If your card is boosting to 1424 (have you seen this in game with gpuz or afterburner?) then you aren't missing much vs a card that can boost into the 1500s.  Maybe a few additional frames per second, I wouldn't worry about it.  No idea on resale value.  If you're gonna sell, you'll definitely want to do it before the Pascal stuff comes out next year.  That may be 12+ months away though."
nvidia,3dn3n1,pj530i,1 point,Fri Jul 17 17:01:03 2015 UTC,I am aware that ASIC is very subjective but yeah 1424 max in EVGA Precision monitor logs. Thanks for your input.
nvidia,3dn3n1,huh009,1 point,Fri Jul 17 17:17:39 2015 UTC,"I have the EVGA Reference 980 ti SC and has a ASIC of 57.1%. I can't even break 1400mhz mark without almost pushing max over voltage in Precision X. So yea, your's better off than mine lol"
nvidia,3dn3n1,p3dr0maz,1 point,Fri Jul 17 20:03:37 2015 UTC,Your cards doing aight.  Mine's not reference but I had to go up to 1.224V to get stable around 1450/1480.  Also had to up my Power limits in the BIOS.  In order to stop throttling I put the kraken g10 /corsair h55 combo on it.  Max temps are 59c after long session of gaming now. Before it'd get up to 79c.
nvidia,3dn3n1,jbramont,1 point,Fri Jul 17 22:02:09 2015 UTC,"I've beenn thinking about doing this to my 980 ti, did you have to get vrm heatsinks?"
nvidia,3dn3n1,p3dr0maz,1 point,Sat Jul 18 02:14:20 2015 UTC,No because I did it to the MSI GTX 980 TI Gaming 6G and it comes with heatsinks already on the board.
nvidia,3dn3n1,kapella,1 point,Sat Jul 18 03:13:44 2015 UTC,"I have 2 ref 980tis. One card is at 68% the other  65%. Without OV I have hit a stable +230 core (roughly 1410mhz) on them. Like pj states, I don't believe quality score is always a major factor when it comes to stability and overclocking. But huh has issues with his even lower ASIC rating so it seems very random. That being said, I think you have a VERY solid reference card. Just keep in mind, if you end up getting the MSI, it could be unstable at the clocks you're doing now. But it would run with less noise and less heat. I'd say stick with it. But that's just my 2 cents"
nvidia,3dmf2n,mikochu,2,Fri Jul 17 13:28:28 2015 UTC,"Gsync helps, but it doesn't fix everything.  If you are around 55fps and vsync stutter and screen tearing annoys you, then Gsync will really help.  If you have 20fps in games right now well gsync won't fix that.  Gsync is just like really good vsync.  Think of it like that."
nvidia,3dmf2n,arikv2,2,Fri Jul 17 14:29:31 2015 UTC,Gsync is just like really good vsync. Think of it like that.   I like that way of describing it.
nvidia,3dmf2n,2FastHaste,1 point,Fri Jul 17 16:26:26 2015 UTC,"Ah, I see.  Thanks!"
nvidia,3dmf2n,arikv2,-2,Fri Jul 17 14:40:24 2015 UTC,"Consider it like an SSD.  Makes everything better, but also everything is technically still the same."
nvidia,3dmf2n,sluflyer06,1 point,Fri Jul 17 14:58:52 2015 UTC,"What clarification are you asking for? We dont' know what resolution your gaming at, also do you only like to play games at their highest graphics quality?  Are you wanting a higher resolution monitor?  If you move up to a higher resolution, G-sync alone may barely keep you playing at a smooth level with that card, however if you are say running 1080p monitor and you got a 980 you wouldn't need a G-sync monitor at all.  I wouldn't run the 780 with a 1440p or higher monitor even with G-Sync.  I myself just upgraded from a 780 last week and I think the 780 is great for 1080p but no more."
nvidia,3dmf2n,sluflyer06,1 point,Fri Jul 17 13:42:48 2015 UTC,"Ah, I was wondering if a G-Sync monitor would make framerate dips unnoticeable, making a game run smooth no matter what the framerate was.  Right now, I'm running at 1080P.  I was looking at the ROG Swift PG279Q, which is 2560x1440 and was curious if I'd notice the lower framerate with G-Sync.  Thanks for your insight."
nvidia,3dmf2n,2FastHaste,2,Fri Jul 17 14:09:51 2015 UTC,I mean...G-Sync would be essential with that monitor on a 780. Unless you are willing to play around with settings on each game your going to see some pretty low frame rates at 1440p with that card.  I'm also extremely sensitive to frame rate...I don't like dropping below 60.  That will be a great monitor whenever it comes out but the more intensive games are going to make that 780 struggle to keep up.
nvidia,3dmf2n,ErmWhatsMyNameAgain,1 point,Fri Jul 17 14:37:21 2015 UTC,"Ah, okay.  I'll hold off on G-Sync, then.  Thanks"
nvidia,3dmf2n,sluflyer06,1 point,Fri Jul 17 14:41:20 2015 UTC,Just to be clear we are not discouraging you from getting G-sync. Just because it can't make miracles doesn't mean it's not a must have.
nvidia,3dmf2n,2FastHaste,1 point,Fri Jul 17 16:27:41 2015 UTC,"Don't hold off mate, I had a 780 paired with a Rog Swift and it made a massive difference to my gaming. One of the best upgrades in a long time."
nvidia,3dmf2n,arikv2,1 point,Fri Jul 17 17:35:09 2015 UTC,"It's a tough call, ideally one would get the higher resolution monitor WITH a better card.  Everyone has different acceptable framerate opinions as well as what settings they want to play at. I guess I'm kind of a snob and I prefer to game at the best settings AND still have fluid framerates, many people do not have such steep expectations.  Can a 780 work on a 1440p G-sync monitor? Yes.  Will you be able to run games like Project cars, Witcher 3, etc at their best settings? No. but you should still be able to manage fairly good quality settings with a G-Sync monitor.  My buddy has a 290 with a 1440p monitor and it does reasonably well but not great and the 290 is pretty much on par with a 780."
nvidia,3dmf2n,baskinmygreatness,2,Fri Jul 17 17:51:47 2015 UTC,"Ah, I was wondering if a G-Sync monitor would make framerate dips unnoticeable, making a game run smooth no matter what the framerate was   No, G-sync can't break the law of physics and magically transform a low frame rate (which by definition is choppy) to smoothness."
nvidia,3dmf2n,arikv2,0,Fri Jul 17 16:24:57 2015 UTC,The Acer gsync is better and cheaper. I had both. Frame dips are less noticeable but 50fps still feels a little worse than 60+
nvidia,3dmf2n,oddients,2,Fri Jul 17 15:47:49 2015 UTC,The asus he mentioned is the ips version and has not been released. So unless  you work for asus you havent used it
nvidia,3dmf2n,oddients,1 point,Fri Jul 17 17:39:22 2015 UTC,My mistake I thought he mentioned the first Rog Swift.
nvidia,3dmf2n,pj530i,1 point,Fri Jul 17 17:53:53 2015 UTC,"Your 780 supports G-Sync, and G-Sync is awesome. I have the PG278Q. Of course you'll get better performance out of a 980 or 980ti but they all support G Sync."
nvidia,3djwly,-Josh,14,Thu Jul 16 21:33:33 2015 UTC,"And for those wondering, no I don't understand how 2,100 MB is 80% of my 16Gb of RAM. (I'm fairly sure I got the big and small Bs the right way around)"
nvidia,3djwly,MediocreMango,6,Thu Jul 16 21:34:56 2015 UTC,It's saying your total RAM usage is 80%
nvidia,3djwly,TheSW1FT,3,Thu Jul 16 23:05:08 2015 UTC,"Yeah, with you. But given that the next largest sector of RAM being used is only 100Mb and only a couple of programs even breaking the 80Mb barrier, no matter how I add it up, I still don't see how 80% of my RAM is being used. Even accounting for OS usage being 2Gb, I don't see how more than 30-40% of my RAM is being used. It makes no sense!"
nvidia,3djwly,Connorbrow,2,Thu Jul 16 23:36:29 2015 UTC,"I've experienced something similar with RzSynapse, it was using 3 GB of paged memory with conhost.exe and powercfg.exe (about 200 processes of each) even though Windows couldn't tell. I had to download RAMMap to see it and then uninstall that crappy Razer software."
nvidia,3djwly,PhyterJet,2,Fri Jul 17 00:16:36 2015 UTC,Razer's software is so bad. I detest it so much. Definitely not getting any more Razer products.
nvidia,3djwly,jscheema,1 point,Fri Jul 17 09:42:16 2015 UTC,Agreed. It's totally unnecessary and used tons of RAM
nvidia,3djwly,Mr_s3rius,2,Fri Jul 17 10:11:53 2015 UTC,"back in the old windows NT days, you could only address 4GB of memory (because of the 32-bit address space), 2GB was reserved for kernel-space and 2GB was given as user-space, so ram usage was a percentage of 2GB even if you had 4GB of physical memory. I don't know how win7/8 divide their memory, I assume what the OS/kernel use, is kept separate."
nvidia,3djwly,njofra,1 point,Fri Jul 17 03:33:59 2015 UTC,"Yeah, I mean I figure that a couple of gigs is kept separate for the basic OS and stuff, but even assuming stuyff like that I can't make it work out at 80% of 16Gb. I was careful to ensure I installed the 64 bit version of Windows 8.1"
nvidia,3djwly,djpokeboy,2,Fri Jul 17 09:40:30 2015 UTC,Windows is making a prediction as to how much RAM it will end up using at 1.2MB/sec in a little while.
nvidia,3djwly,kendirect,1 point,Thu Jul 16 23:44:39 2015 UTC,"1.2MB/sec   But this is disk usage, not memory."
nvidia,3djwly,revengeofbob,1 point,Fri Jul 17 22:07:42 2015 UTC,"How long is ""a little while""? Because I still don't 'get how that would make it to 80%."
nvidia,3djwly,steak4take,1 point,Fri Jul 17 09:43:07 2015 UTC,"It 16GB of RAM(if it's 16 gigabytes), not Gb. 16Gb would be 2GB."
nvidia,3djwly,djpokeboy,1 point,Fri Jul 17 12:09:02 2015 UTC,"Look at the bottom, how many MB is in your nonpaged pool? I've had a memory leak on my rig for months now (still not fixed), and it fills up the nonpaged pool gradually, the program isn't shown for nonpaged."
nvidia,3djwly,steak4take,4,Fri Jul 17 14:05:35 2015 UTC,"Wait until you see iTunes!  (I know, nothing to do with nVidia...)"
nvidia,3djwly,djpokeboy,1 point,Fri Jul 17 03:04:12 2015 UTC,"Had the same problem, can't have shadowplay on for more than 2 minutes without it going crazy with RAM."
nvidia,3djwly,tgl3,1 point,Fri Jul 17 00:25:29 2015 UTC,"Use Resource Monitor.  I bet that it's opening a file of that size while saving.  I also bet it's not the culprit causing 80% consumption,"
nvidia,3djwly,ChikNoods,1 point,Fri Jul 17 04:10:18 2015 UTC,"Unrelated to OP's problem, but is there a way to get resource manager to show what's in my Nonpaged pool? I've had a memory leak since day 1 with my build which causes my nonpaged pool to steadily tick up as data passes through my ethernet. No programs in task or resource manager appear to have any RAM usage above average."
nvidia,3djwly,Vas0ly,2,Fri Jul 17 14:11:15 2015 UTC,This is what you want.
nvidia,3djwly,TidusJames,1 point,Fri Jul 17 21:41:50 2015 UTC,Thank you! Hopefully this will finally fix my problem.
nvidia,3djwly,djpokeboy,1 point,Sat Jul 18 09:15:21 2015 UTC,I had this exact issue last night. Except it was using over 13gb of RAM. (took me a while to notice why everything was running slow)
nvidia,3djwly,Sethos88,1 point,Fri Jul 17 13:21:32 2015 UTC,I kill all shadow play stream services. Disable those things every install
nvidia,3djwly,djpokeboy,1 point,Fri Jul 17 15:09:31 2015 UTC,"Fix i wrote before finding this thread: https://www.reddit.com/r/nvidia/comments/3drhgj/fix_for_win10_shadowplay_memory_leak/ EDIT: Nevermind, still leaking but more slowly"
nvidia,3dny35,dntnofngshtboutaPC,2,Fri Jul 17 20:32:46 2015 UTC,"this happened to me.......... once, after i enabled my on board HD 4600 graphics and forgot to update the monitoring in AB/Precision. It had changed my primary GPU to the integrated one. I lol'd."
nvidia,3dny35,p3dr0maz,1 point,Fri Jul 17 20:38:07 2015 UTC,should of mention i don't have onboard graphics
nvidia,3dny35,p3dr0maz,1 point,Fri Jul 17 20:40:53 2015 UTC,does it do it with all games or just one? might just be the game.
nvidia,3dny35,p3dr0maz,1 point,Fri Jul 17 20:42:30 2015 UTC,damn i hate good questions be back in a minute
nvidia,3dny35,p3dr0maz,1 point,Fri Jul 17 20:52:38 2015 UTC,tried two other games still showing 0% usage but it shows correct temp and fan speed
nvidia,3dny35,p3dr0maz,1 point,Fri Jul 17 20:58:38 2015 UTC,Is it possible you just disabled the monitoring on accident somehow?  I would download GPU-Z and see if it displays properly there. At least that will show your card is ok.
nvidia,3dnxmf,ariyapl,2,Fri Jul 17 20:29:04 2015 UTC,"Yeah probably all speculation. you can google pascal and get their ""roadmap"". It says 2016 with no real month or quarter so its anybodies guess."
nvidia,3dnxmf,p3dr0maz,1 point,Fri Jul 17 20:41:42 2015 UTC,"Thanks man. Well until then I'll just stick to 980ti. If Pascal's gonna be as powerful as they promise, maybe a single card will be able to handle Witcher 3 in 4k at 60+ fps constant."
nvidia,3dnxmf,Ohmps_,1 point,Fri Jul 17 20:53:51 2015 UTC,We can only hope this would drop all prices in graphics cards
nvidia,3dnxmf,Disturbed_Wolf88,1 point,Sat Jul 18 03:19:36 2015 UTC,I'm going to make a new build in March... debating on dual 980tis or waiting for Pascal and get a single... but I don't want to wait until end of 2016
nvidia,3dnxmf,Disturbed_Wolf88,1 point,Sun Jul 19 19:15:28 2015 UTC,"If you want to do dual 980ti, then go for full blown watercooling man. These cards are hot as f...k. Everything in your case will be boiling with two of them running (I guess even the reference ones) in SLI. When Pascal's out, all us, 980ti owners will experience a very sharp drop in 980ti prices.   If Pascal's going to be as powerful as they promise, and my i4790k will be able to handle it properly, I will sell 980ti and buy a Pascal. We'll see. As long as something's able to run Witcher 3 60 fps constant in 1080p with HairWorks, I'm happy (and one 980ti is)"
nvidia,3dnxmf,Disturbed_Wolf88,1 point,Mon Jul 20 17:21:15 2015 UTC,Water cooling still terrifies me... 4k$ on a rig just for a hose to leak and fry it...  My goal is ark survival evolved cranked up to max with the rift...
nvidia,3dnxmf,Disturbed_Wolf88,1 point,Mon Jul 20 19:03:18 2015 UTC,"Well you can always find a company that builds custom PCs and maybe they'll give you some kind of leak warranty. Or just find someone who knows what he's doing. From what I've read, water cooling rigs, if assembled properly aren't that prone to leak."
nvidia,3dm9kc,ahh_real_monsters_pc,1 point,Fri Jul 17 12:30:38 2015 UTC,I've never heard of that before but yes you can use the bottom GPU for your displays.
nvidia,3dm9kc,hdshatter,1 point,Fri Jul 17 13:18:49 2015 UTC,Do you know if I have to set the GPU to primary or something like that somewhere?
nvidia,3dm9kc,Gohanburner,-3,Fri Jul 17 13:42:32 2015 UTC,"I'm using a PB278Q [https://www.asus.com/us/Monitors/PB278Q/] and I get over 60fps on pretty much any newer game with the highest settings. I used to have a 4K monitor, sold it and went to the VG248QE, sold that, and went to this current monitor because it is PLS and not TN. Looks beautiful and its perfect for gaming if you ask me."
nvidia,3dm9kc,jedimstr,1 point,Fri Jul 17 15:57:51 2015 UTC,So.... commenting on wrong post or just randomly comment bombing ?
nvidia,3dn955,The_Lord_Bane,1 point,Fri Jul 17 17:27:25 2015 UTC,"What's your psu?  You also have very aggressive cpu and gpu overclocks, maybe try dialing it back just to see if that helps."
nvidia,3dn955,aquaknox,1 point,Fri Jul 17 18:10:18 2015 UTC,"My psu is a superflower 1000w 80+ gold. I've tried with stock clocks, the same problems occur."
nvidia,3dn955,Syn246,1 point,Fri Jul 17 18:47:42 2015 UTC,"The 353.38 hotfix driver has provided the most stability for my setup so far (SLI G1 980 Tis). I can run 1506 core / 8000 memory stable in every game and benchmark I've tried so far.  You noted you've tried each driver; I'd say try 353.38 again and be sure to do a clean install of the driver, or better yet use the DDU tool to clear all traces of prior drivers from the system.  Also, what voltage offset are you using, if any? Sometimes adding a slight amount of voltage will grant stability to an otherwise unstable card. For example, one of my original G1s that I have since RMA'd (due to coil noise) would not successfully complete a Fire Strike Ultra bench without setting the voltage to +87 in Afterburner at stock clocks. it was fine on regular and Extreme, but Ultra pushed it hard enough to expose this weakness. This same card went on to OC at 1481mhz with said added voltage offset before I returned it."
nvidia,3dn955,Slayer418,1 point,Fri Jul 17 18:47:45 2015 UTC,"I'll give 353.38 another go then with DDU method. I'm using +87mV any the overclock is stable, tried heaven, valley, firestrike normal, extreme and ultra all passed fine."
nvidia,3dmfz3,DonDalle,2,Fri Jul 17 13:37:13 2015 UTC,It's been released in Poland already. I was going to buy that card but when I saw the temps (more than 80 degrees) I bought another one.
nvidia,3dmfz3,ariyapl,1 point,Fri Jul 17 14:02:17 2015 UTC,Which one did you buy? I also like the Gigabyte one.
nvidia,3dmfz3,flame03,1 point,Fri Jul 17 14:38:02 2015 UTC,The ASUS Strix 980 Ti is slowly becoming available in webshops here in the Netherlands.  I went with the MSI card myself.
nvidia,3dmfz3,ariyapl,1 point,Fri Jul 17 14:56:26 2015 UTC,"I bought the Gigabyte but I kinda regret it since mine had really bad coil whine. I've already RMA'ed it, getting new one next week. If I were to choose again, I'd probably go with MSI."
nvidia,3dmfz3,I_Will_be_Nice,1 point,Fri Jul 17 20:17:35 2015 UTC,"Thanks for the input. I prefer the additional DVI port on the Gigabyte card, I can make good use of it. But I will check out the MSI card"
nvidia,3dmfz3,ariyapl,2,Fri Jul 17 21:17:32 2015 UTC,Don't buy the Strix.   The cooler is not meant for the 980ti but they crammed it on anyway.  It barely cools better than reference.
nvidia,3dmfz3,I_Will_be_Nice,1 point,Fri Jul 17 19:36:09 2015 UTC,"Yeah, huge disappointment. I was waiting for the ASUS release since my 970 DirectCU II Black OC was aswesome. DC III is crap."
nvidia,3dmfz3,ariyapl,1 point,Fri Jul 17 20:18:14 2015 UTC,You'd think that they'd have learnt from the DCU II on the R9 2xx cards - apparently not.
nvidia,3dn5sr,Rate_My_Build,1 point,Fri Jul 17 17:02:54 2015 UTC,"Okay decided to get the MSI, but if I wait would this product come witht the new promotion?"
nvidia,3dn5sr,ktc64,1 point,Fri Jul 17 17:27:50 2015 UTC,"Post on here from yesterday suggested Metal Gear V would be the new free game based on a newegg listing (that appears to be fixed now) So if you can maybe wait a week or two and see if the promotion starts.   As far as card choice goes, I personally am gonna go with the Gigabyte G1 980ti. From all the reviews that I've read it seems to be the air cooled card that can reach the highest OCs."
nvidia,3dn5sr,sluflyer06,1 point,Fri Jul 17 17:44:20 2015 UTC,"Have 2 G1's, can confirm they rock.  Impatiently waiting on EK to release blocks."
nvidia,3dn5sr,sluflyer06,1 point,Fri Jul 17 18:01:55 2015 UTC,You didnt have any flaws with it?
nvidia,3dn5sr,ariyapl,1 point,Fri Jul 17 20:31:35 2015 UTC,No Sir. What flaws are you afraid of?
nvidia,3dn5sr,ariyapl,1 point,Sat Jul 18 14:47:54 2015 UTC,"I got the Gigabyte but had terrible coil whine. RMAed it, waiting for the next one. Hopefeully you'll get lucky."
nvidia,3dn5sr,ariyapl,1 point,Fri Jul 17 20:19:13 2015 UTC,Yeah i ditched the msi because it messed with my color scheme now its either g1 or evga  How loud is the whine? Like is it really that bad that you cant use it?
nvidia,3dmfv4,slole,1 point,Fri Jul 17 13:36:12 2015 UTC,"NV Surround requires you monitors to have the same resolution. If it is not, then, it runs both monitors at the resolution of the lowest resolution-wise monitor.   In your case, it runs the 1920x1080 monitor at 1680x1050 which results in a lower quality image. There are inconvenient ways to solve the problem.  1] Implement that feature yourself at the game engine level if you are the one writing code. Essentially, query the resolution of available monitors then set-up a render texture which takes the bigger height as y and the sum of widths as x. Create two windows (one per monitor) that share a rendering context and then copy the appropriate part of the render texture to each window.  2] If your game uses DirectX 9 the you could try using softth, http://softth.net/."
nvidia,3dmfv4,crschindler,1 point,Fri Jul 17 19:36:24 2015 UTC,Yeah but 3840 x 1080 resolution suggests that i'm running the 1080p monitor at it's native resolution and the second monitor at a higher resolution. Well ill try softth after it gets dx 10/11 support.
nvidia,3dlbom,arid_tare,1 point,Fri Jul 17 04:59:07 2015 UTC,I had similar issues with many games on Gtx 970 in SLI when I updated to that driver. Roll back to HoTS version.
nvidia,3dlbom,ariyapl,1 point,Fri Jul 17 07:31:12 2015 UTC,Had the same issue with Witcher 3 and GTA V. Reinstall seemed to fix it in my case.
nvidia,3dl7xj,Delerium89,2,Fri Jul 17 04:19:36 2015 UTC,Might be better off just getting new card or cards then spending $250-300 watercooling older cards.  780ti's are going for around 300 on ebay.  You could get a 980ti which in alot of cases is going to do better than this setup (but not all).  It's something to think about.
nvidia,3dl7xj,sluflyer06,2,Fri Jul 17 13:50:43 2015 UTC,"Sell the 780's and get a 980ti. Also, the brackets that allow you to use closed loop coolers on GPU's don't generally cool your VRM's on the PCB, which is a must when it comes to wanting a high overclock."
nvidia,3dl7xj,bkral93,1 point,Fri Jul 17 14:12:19 2015 UTC,"Go for full coverage waterblocks. More expensive, but with 2 top end cards, it's a must to get the over clock you want. This will help more than the pre filled loops because those only cool the GPU, and not the VRMs or RAM.   Edit: sorry buddy, I misread, and thought you had 2x 980 ti. Don't dump the money on waterblocks and a full loop. Just ditch the cards and get a 980 ti. Possibly save one card and dedicate it to physically through the nvidia control panel."
nvidia,3dl7xj,scloutkst2889,3,Fri Jul 17 11:40:57 2015 UTC,"Please don't recommend that he goes and buys FC blocks and does a custom loop just to overclock his 780ti's. He should sell them and get a 980ti, and he won't have to worry about needing to OC.  Building a custom loop is a large investment, and seeing as how the guy is still wanted to stretch his 780ti's out that long, he probably isn't willing to drop $100's of dollars on a good loop.  If this was somebody with a 9xx card or titan, wanting to know about watercooling, sure... recommend a custom loop and you're right to not recommend one of those shit closed loop brackets. But for his situation, he should just look to sell his 780's and upgrade."
nvidia,3dl7xj,bkral93,1 point,Fri Jul 17 14:10:17 2015 UTC,Sorry when I read it this morning I thought he had 2x 980 ti. I misread.
nvidia,3dl7w1,cylindricalsea,4,Fri Jul 17 04:19:13 2015 UTC,"When you push a certain number of pixels, and your pushing a lot since its the same a 4 1080p monitors, the cards will refuse to downclock to the lowest settings even when your doing fuck all. This happens when you use a 144hz 1440p panel as well."
nvidia,3dl7w1,BKachur,3,Fri Jul 17 08:17:32 2015 UTC,The exact same thing happens to me with the same set-up.  I use a 55 inch 4k TV and I have two of your gpus in sli. My card idles in the high 40's but doing something simple like web browsing and watching videos brings it up to the low 60's as you said.  I'm about 95% sure it's having everything running at the high resolution. I've been away from my house the past few days (with my computer) and am running on a regular 1080p display and right now my cards are at 42 and 30 Celsius after the same type of web browsing.   I was literally just talking to my brother about this observation I had a few hours ago. Hope this reassures you! Edit: Would like to add that I also started having the fans run at 15-20% so they stay cool. Can't hear them and it keeps the light load temps in check.
nvidia,3dl7w1,Notedeel,3,Fri Jul 17 04:36:19 2015 UTC,"It the resolution, when you push a certain number of pixels (this happens with me I have 1440p and 2 1080p), then the card won't downlock to 135 or 410 mhz but will stay clocked at 855. Nvidia claims this is intentional so there will be no lag on high rez systems."
nvidia,3dl7w1,BKachur,2,Fri Jul 17 08:22:20 2015 UTC,Im using 2 980 tis in sli. YOU SHOULD NOT BE AT 60 IDLE. RIght now as I type this I turned off my fan profile and one is idling at 34 the other 41. I am at 4k as well. With a 2nd monitor constantly on. You shouldnt be that high
nvidia,3dl7w1,kapella,1 point,Fri Jul 17 11:25:30 2015 UTC,How's the airflow in your case?  It could be you don't have enough.  I'm running a highly overclocked 980 and it's temps under load were hitting 85 until I added a second intake running at half speed (now it never goes over 79 room temps allowing) and a second top outtake running fan controlled by my second CPU header.   Two in and two out however the way set it up to run meant a positive airflow when the internal case temp (as this will affect the CPU temps) was lower and a negative when the internal case temp got too high.
nvidia,3dl7w1,DTfan82,1 point,Fri Jul 17 12:40:42 2015 UTC,I had the same issue with mine. From what I can understand the fan of the EVGA wont even turn on until the card hits 60. I installed EVGA precision X and changed the fan curve to Aggresive and it fixed the problem for me. I now Idle at around 32-35. Hope this helps! Only issue is you have to leave the program open and minimized for it to work.  Edit: Saw you mentioned that you messed with the fan curve. I would also like to add that I read that you can fix this issue by using Nvidia inspector and enabling the power saving features
nvidia,3die1e,himmatsj,11,Thu Jul 16 14:54:40 2015 UTC,Well fuck! I just bought my 2 gtx 980ti's. I wonder if I can cancel my G1 order and wait for this to happen. I don't need 2 broken ass games.
nvidia,3die1e,FunktasticLucky,6,Thu Jul 16 16:14:32 2015 UTC,Any indication on timing? Looking to pick up a 980ti in the near future and I already have the train wreck that is Arkham Knight.
nvidia,3die1e,NoobasaurusWrexx,3,Thu Jul 16 17:36:00 2015 UTC,I just looked at some past promos and it seems they launch those bundles roughly 1 or 2 months before the release of the bundled game.  So with a bit of luck we may see this bundle very soon (I really hope so).
nvidia,3die1e,BaNaNaKING42,6,Thu Jul 16 20:37:35 2015 UTC,GODDAMNIT.
nvidia,3die1e,-mr-tibbs-,4,Thu Jul 16 18:19:08 2015 UTC,Does anyone actually believe Nvidia didnt know exactly what a terrrible state Batman was in? And still let it go as a promotion. Looks bad on them and fucked us people over who ordered cards around that time. Especially those who paid a damn lot for the 980 tis.
nvidia,3die1e,XXLpeanuts,3,Fri Jul 17 12:52:31 2015 UTC,"I'm gonna contact nvidia to see if I can ttrade my batman code for this , cause fuck batman."
nvidia,3die1e,solidnitrogen,3,Thu Jul 16 18:58:01 2015 UTC,Report back please.  I don't want a game that's locked at 30 fps.
nvidia,3die1e,djisadud,2,Thu Jul 16 20:15:42 2015 UTC,"Will do, though you can unlock it in the ini file, but it's a bullshit game to be advertising their tech with, especially since it doesn't work."
nvidia,3die1e,solidnitrogen,2,Thu Jul 16 20:57:35 2015 UTC,I read that changing the ini messed with the physics and made completing the game impossible.
nvidia,3die1e,djisadud,2,Thu Jul 16 20:58:54 2015 UTC,"Huh didn't know that, didn't really car after the game started crashing after the opening with or without fps cap."
nvidia,3die1e,solidnitrogen,1 point,Thu Jul 16 21:08:24 2015 UTC,I finished the game with fps unlocked via ini and it worked but there are lots of issues that was there even with the 30fps cap.
nvidia,3die1e,thilinac,3,Fri Jul 17 11:16:44 2015 UTC,"Damnit, I just used my code for Batman last night thinking I may as well since it's not like I can do anything else with it. I wonder if they'd let you ""return"" the game considering how broken it is. I mean, hell, it's not even sold by stores anymore it's so bad (at least Gamestop and Steam, which granted are basically the entire PC market at this point). Even the publisher pulled it and admitted it was terrible. It's a puzzling choice to show off your cards with considering how bad it is performance and graphics wise.  It'd be nice if nVidia just gave everyone the MGSV code since B:AK is literally unplayable, but of course that's not going to happen since nVidia had to pay for those codes, and I'm sure they're just as pissed about it since you know they thought they were getting another great AAA Batman Arkham game and wound up with the blunder of the century.  ...sigh"
nvidia,3die1e,Ofactorial,1 point,Thu Jul 16 21:03:27 2015 UTC,It might be worth emailing and asking. Especially if you have a Titan X or 980 Ti.
nvidia,3die1e,Zinthros,1 point,Fri Jul 17 01:18:42 2015 UTC,Huh... I guess Ill just buy a 2nd one in September.
nvidia,3die1e,zkDredrick,1 point,Thu Jul 16 18:30:29 2015 UTC,wooohoo!
nvidia,3die1e,TILfuck,1 point,Thu Jul 16 18:38:34 2015 UTC,I'm not seeing anything? Also I have that 960 :D
nvidia,3die1e,jaredmw,1 point,Thu Jul 16 19:39:32 2015 UTC,Anyone have any experience with stuff like asking if they can get a code for a game in a new bundle when they recently purchased a card with a previously bundled game? I just bought my 980ti and redeemed by batman code. It's an Evga card and got it through B&H Photo. Who would I ask?
nvidia,3die1e,Icanhaswatur,1 point,Fri Jul 17 05:24:01 2015 UTC,"Should I hold off of buying a card right now until the promotion happens? Also, when does the promotion happen?"
nvidia,3die1e,Rate_My_Build,1 point,Fri Jul 17 09:07:49 2015 UTC,"Should begin anytime soon, latest by next week.   I mean, would you be buying the game at launch anyways? If so, it saves you a good $60 and I reckon it'd be worth waiting for."
nvidia,3die1e,Rate_My_Build,1 point,Fri Jul 17 09:22:25 2015 UTC,"No it doesnt interest me. However, I figure I can just sell the code and get more money in my pocket."
nvidia,3die1e,ScribbleJ,-2,Fri Jul 17 09:27:14 2015 UTC,"Considering how terribly ground zeroes works with gamestream, this is likely to go poorly."
nvidia,3diw9q,Dr_Paper,11,Thu Jul 16 17:09:48 2015 UTC,"Just put a waterblock on that damn thing, what´s the point on having better cooling the gpu and worse cooling on vrm ,vram .  Waterblock all the way"
nvidia,3diw9q,KoreanGirlLoL,5,Thu Jul 16 18:40:12 2015 UTC,"Having never done a waterblock or water cooling system before, I'd rather not fry 6k + of hardware.  Maybe on my next build.  I'm pretty satisfied with the results of the Hybrid AIO thus far."
nvidia,3diw9q,dangitalvin,5,Thu Jul 16 18:52:43 2015 UTC,"I'll look more into it, but I'm skeptical about messing up on the install and getting a leak.  I'm pretty sure EK has great stuff, I'm just not too sure about my ability to completely do it right.  With the Hybrid, every water related piece was fully sealed.  The end result is still a much cooler and quieter gpu setup than what I had previously."
nvidia,3diw9q,KoreanGirlLoL,4,Thu Jul 16 19:06:20 2015 UTC,You don't have to do anything you aren't sure of OP
nvidia,3diw9q,KoreanGirlLoL,1 point,Thu Jul 16 19:14:38 2015 UTC,"When I was 14 years old I made my first water cooling kit, it is truly not hard and you can test your loop before damaging any of your components. I encourage you to do it if you have the chance , follow youtube videos if you´re trully that insecure....but anyone should do this."
nvidia,3diw9q,qazme,2,Thu Jul 16 23:54:30 2015 UTC,"Oh that's cool.  I did not know you can test it prior.  That actually makes me a lot more comfortable with doing a full water build.  Thanks for the inspiration!  Funny thing about this whole build was that I initially just wanted to get a new laptop for around 2,000.  My brother then convinced me to build a desktop.  6,400 dollars in so far... haha.  I've been going in with the mindset of ""I'm going for the best parts only.""  So I suppose I should follow through with full on water cooling!"
nvidia,3diw9q,NCEMTP,3,Thu Jul 16 19:17:20 2015 UTC,"Haha yeah you become addicted, it´s such a healthy and rewarding hobby....best of lucks... if you need help you can PM me"
nvidia,3diw9q,qazme,3,Thu Jul 16 19:22:10 2015 UTC,I definitely will!  I've got some homework to do first.  Looks like the S-frame case with EK block on triple Titan project is a go!  Thanks for the inspiration again!
nvidia,3diw9q,NCEMTP,2,Thu Jul 16 19:39:55 2015 UTC,You won't regret it - grab a colored theme and get an EK backplate to match. I did blue on mine it's great!
nvidia,3diw9q,qazme,2,Thu Jul 16 19:42:38 2015 UTC,"Hey, I want to put together a custom loop for my dual 980Tis. No idea what to buy to do it, though. Could you advise on what I should get to get started? I tried posting to /r/buildapc but no joy there."
nvidia,3diw9q,NCEMTP,5,Fri Jul 17 02:00:06 2015 UTC,"You need a pump, res, and at least a 360mm radiator (3 x 120mm fans) or 2 x 240mm radiators (you case will determine generally what you will use unless you are handy with tools), the fans to put on the radiator and hose, plus blocks for both cards and some sort of fittings (x2 per part in the loop). The only special part you will need are SLi connectors to go between cards, however you can accomplish the same thing with 4x compression fittings, tubing and patience.  More radiators does not mean a cooler loop. Pick what you like the look of and put it together - if you don't like it you can always swap it for something else. Get block you like and know you like because they are the most expensive parts most of the time.  And last but not least don't mix metals copper/nickel are ok copper/aluminum or copper/brass causes corrosion. And make sure you get something for algae growth like a silver coil, PT nuke, or like I do aquarium stuff (way cheaper) - pick up some distilled water till you decide on a color (CHEAP) and have fun!"
nvidia,3diw9q,techno_babble_,2,Thu Jul 16 20:49:20 2015 UTC,"Sweet...thanks for the info! Are there vendors that I should look at over others for the parts? Also how much do you think I'd be spending on this sort of setup? Is it feasible to just cool the cards and keep my hyper 212 evo on my CPU? I'm working with a Coolermaster HAFX case, for what it's worth."
nvidia,3diw9q,joshruffdotcom,4,Fri Jul 17 01:55:04 2015 UTC,"I've never worked with a HAFX so I don't know for sure. However yes you could run a loop without including the cpu as long as you have the room to put everything in there.  Personally since DangerDen shut down I buy most/if not all my stuff from performance-pcs.com. Prices are about right and the shipping is decently fast/cheap. US based = not sure where you are located.  Price wise it varies, the EK titan block I got copper/acrylic + back plate was ~$150 combined(1 card), compression fitting vary from $3-8 a piece radiator can run as low as $50 and as high as ~$120. Pumps depending on options run about that same price range. RES aren't neccessary but make filling and bleeding the system a lot easier plus they look nice - $20-$100. Tubing would be about ~$25 for 10 foot of decent stuff like PrimoChill LRT. 10 foot should be enough to do the whole system and have some left if you don't waste it.  All in all it will tubing yadda yadda added up you will be between $650-$800 depending on options for a relatively simple loop, all the bells and wistles the skys the limit. Granted ~$450-500 will be on the blocks/backplates alone for 3 titans.  Personally however - with 3x titans I would just cool the cpu as well. It'll be another $50-$60 for the block and throw in two BIG radiators.  Something like this: https://www.youtube.com/watch?v=MFo0kqiU8AA  Depending on how picky you are, and if you don't want to mod the case I would put the thickest 360mm I could fit in the top and put a decent 120mm in the back and you will be fine temp wise - maybe not as low as some depending on the thermal dissipation of the radiators and the flow rate of your pump.  It's like building a race car - 400 ways to get to the same point but there's always something better. For instance I know guys who have as much money in their fittings as some people do their entire water cooling setup. There's corners that can be cut in multiple places, fittings are a good example, that can save substantially on. 3x gpu much less 3x gpu + 1 cpu is quite a feat for a first time water cooling project.   If I was putting that sort of money into a rig and wanted to go full water cooled - I would look into a case made for water cooling. Mountain Mods or Case Labs would be my choice BUT $$$. Like anything else, once you go down that rabbit hole - hold on to your wallet - it gets expensive to have nice things - as you should know by now. ;-)  And if you don't know - remember to keep the RES higher than the pump in the loop - order doesn't matter just make sure you are flowing in on the correct side of the block. Your system will hit thermal saturation at some point and the 'sweet spot' in my opinion, if you can hit it, is within 10C of room temp. Gonna be a rough ride for 3x titans inside a case with limited radiator space. Best of luck! (My titan with appropriate - actually overkill cooling - 28C idle 38C load with the cpu running 41C load)."
nvidia,3diw9q,KoreanGirlLoL,1 point,Fri Jul 17 02:58:37 2015 UTC,This is wonderful info!  Mind if I run a part list by you when I get a chance?  I'll be trying to cool 3 Titan Xs and 5960 cpu in most probably an S-frame case.
nvidia,3diw9q,joshruffdotcom,1 point,Fri Jul 17 05:14:01 2015 UTC,"Thanks again for the detailed info. I'm running only 2x 980Tis, so that should allow for more space to maneuver and let me keep a little bit more in my wallet. With over $2500 in this build already, I figure I might as well get as much bang for my buck as I can with these top-end cards, and the only way to do that now is liquid cooling.  I'll have to look into the specific more when I have the time to get into it! Thanks much."
nvidia,3diw9q,KoreanGirlLoL,2,Fri Jul 17 05:42:05 2015 UTC,Try /r/watercooling
nvidia,3diw9q,Dippyskoodlez,1 point,Fri Jul 17 06:28:20 2015 UTC,I'm going to be packing two of these Hybrid coolers in my Air 240 mATX case. A proper custom loop isn't always the best option depending on your setup.
nvidia,3diw9q,happyusername,1 point,Thu Jul 16 22:31:39 2015 UTC,An air 240 can fit a watercooling kit perfectly :)  http://www.ocaholic.it/modules/xcgal/albums/userpics/10642/normal_rod_240hrcblu1.jpg
nvidia,3diw9q,tamzta,1 point,Fri Jul 17 02:27:54 2015 UTC,"That's a single GPU + CPU loop. For a Titan X SLI setup there isn't sufficient radiator space to properly cool 2 GPU's and a CPU without modifying the case. It's also twice as expensive to buy waterblocks, pumps, fittings, tubing and radiators than it is to buy 2 Hybrid kits."
nvidia,3diw9q,sluflyer06,1 point,Fri Jul 17 02:32:25 2015 UTC,"Yeah, budget is obviously something that might limit you.   But without $ concern you can perfectly setup a watercooling unit, enough for cpu and 2 titans , no modifications 240 rad in front 120 or 240 rad on top.  You could even cool 3-4 titans....stilll no modification"
nvidia,3diw9q,sluflyer06,3,Fri Jul 17 09:23:06 2015 UTC,"Yeah, dem EK blocks are sexy as fuck."
nvidia,3diw9q,jscheema,3,Fri Jul 17 12:36:11 2015 UTC,"that's actually impressive, I might consider this cooler"
nvidia,3diw9q,jscheema,2,Thu Jul 16 18:57:04 2015 UTC,"Doing some research online, it seems a 980 or 980 ti hybrid cooler fits on Titan Xs.  My main issue was the sound the Titan X fan was making.  It is now very quiet."
nvidia,3diw9q,jscheema,3,Thu Jul 16 18:00:28 2015 UTC,"How's the noise from the pump? One of my reference cards makes a really loud irritating metallic noise as the fan spins, while the other is silent, considering options to remedy as otherwise its a near silent build (while idle)"
nvidia,3dkm72,louvillian,2,Fri Jul 17 01:01:54 2015 UTC,MSI Afterburner's built-in fps counter should work for you.
nvidia,3dkm72,Progaine,1 point,Fri Jul 17 07:58:34 2015 UTC,"Fraps can be used, and is nice because you can make a hot key for turning it on and off.   Also if you are launching games through steam, it had a nice built in FPS counter."
nvidia,3dkm72,sidious911,1 point,Fri Jul 17 13:21:43 2015 UTC,"I use RivaTuner Statistics Server (Through MSI Afterburner) and set a shortcut key to show/hide fps counter. Mine is currently Shift+F8, you can also use it to show gpu load, core clock, memory clock, etc. Very useful."
nvidia,3dkm72,jbramont,1 point,Fri Jul 17 16:33:47 2015 UTC,"You could use RivaTuner Statistics Server (RTSS) as Progaine and jbramont suggested. In RTSS, you have a per application settings for the overlay."
nvidia,3djaor,SirPatchington,7,Thu Jul 16 18:53:12 2015 UTC,"It may be a stupid question, but I have seen that before: did you connect your monitor to your GPU instead of your motherboard?"
nvidia,3djaor,njofra,3,Thu Jul 16 20:44:28 2015 UTC,"There was a csgo update recently. Frame drops are a thing. Hopefully it will be patched in an update.  If only csgo is affected, the game is the issue, not the card."
nvidia,3djaor,Henaree,2,Thu Jul 16 23:32:13 2015 UTC,You could always just go and rollback your drivers from nvidia's website.
nvidia,3djaor,Arko4576,2,Thu Jul 16 19:04:14 2015 UTC,"Since I re-installed them, it won't let me. It's greyed out."
nvidia,3djaor,SolidSean,2,Thu Jul 16 19:09:25 2015 UTC,Get a ddu then redownload the old driver.
nvidia,3djaor,SolidSean,2,Thu Jul 16 19:48:28 2015 UTC,"When I get home I'll try, if it doesn't work, do you think re-formatting my HDD will help at all?"
nvidia,3djaor,SolidSean,1 point,Thu Jul 16 19:54:11 2015 UTC,No clue. I'm pretty confident a ddu and the old driver should be fine.
nvidia,3djaor,SolidSean,1 point,Thu Jul 16 19:55:47 2015 UTC,"Just done this get about 150fps on CS:GO, still not amazing. Any advice?"
nvidia,3djaor,Arko4576,1 point,Thu Jul 16 22:41:07 2015 UTC,Check nvidia control panel to make sure no settings were forced on in there.
nvidia,3djaor,p3dr0maz,1 point,Thu Jul 16 22:54:50 2015 UTC,Like what settings?
nvidia,3djaor,RevoMarine,1 point,Thu Jul 16 22:56:32 2015 UTC,"Dsr or any antialiasing. Basically, super gpu heavy stuff."
nvidia,3djaor,ariyapl,1 point,Thu Jul 16 23:02:45 2015 UTC,Yeah nothing is forced.
nvidia,3djaor,Bickell,1 point,Thu Jul 16 23:07:34 2015 UTC,I highly doubt reformatting will change anything
nvidia,3djaor,BarnesDude,1 point,Thu Jul 16 20:06:07 2015 UTC,"Okay, I'll try the old drivers soon."
nvidia,3djaor,Bickell,2,Thu Jul 16 20:06:39 2015 UTC,Try changing the Power Management mode from Adaptive to Performance in the 3d settings just for this program. I was having issues with another DX9 title until I did this.  It wanted to save power more and it would make my frames drop from 300 some odd to 150 some odd too.
nvidia,3djaor,BarnesDude,2,Thu Jul 16 19:35:45 2015 UTC,"Tried it and still low FPS, no difference."
nvidia,3djaor,Bickell,2,Thu Jul 16 19:37:34 2015 UTC,Do you play in full-screen windowed in csgo?   Change it to full screen to fix your fps issues in the game
nvidia,3dn586,Dovakhan,2,Fri Jul 17 16:58:40 2015 UTC,"It's an interesting rumor, somewhat ruined by the OP's attitude."
nvidia,3dn586,cyberpunk7t9,-4,Fri Jul 17 20:53:35 2015 UTC,"It's not something to be interested, but something worrying and where some official statement would be welcome. And I dare to say it even on behalf of all future Pascal owners.  And if you wanna put up with AMD fanboys, well...it's not my problem really. Go enjoy them yourself."
nvidia,3dn586,Randomness6894,1 point,Fri Jul 17 20:57:22 2015 UTC,"As if to suggest that Nvidia should get first use at something AMD developed. Both AMD and SK Hynix made HBM and HBM2, they should rightly get to have it first. You must a fool to end up being disappointed by this, at least Nvidia will be able to use HBM in the future with their Pascal cards. You seem to think of this as a bad thing, which is insane. AMD is a brilliant company that has pushed innovation and technology forward, and has made PC gaming what it is today. Of course, they would be nowhere with Nvidia and vice versa.  Adding to this AMD also assisted in the development of GDDR5 too. Honestly, you must be a true cretin to even be a die hard fanboy these days, of either company, if it means hating on the other."
nvidia,3dn586,Randomness6894,-1,Sat Jul 18 21:18:08 2015 UTC,"And this idiot dares to call ME a fanboy....oh, the irony.  Anyway, go ahead and stay with your ""brilliant"" company. So brilliant even rumours of bankrupt have started to arise :D I can't wait to see your face when your little brain finally realizes you've bet for the wrong horse.  (sigh) If stupidity was food, no one on Earth would go hungry."
nvidia,3dn586,Randomness6894,1 point,Sat Jul 18 21:21:53 2015 UTC,"Are you actually serious? AMD won't go under at all. Yes, they have been losing money, but it comes as no surprise as they were releasing new products and working in other areas. And yes I call you a fanboy, in fact, you are one of the biggest fans I have come across. Honestly I don't fully understand why or what you see in it, but that is up to you. Most people on this subreddit and many others are well aware that competition is good for consumers and we need AMD and Nvidia.   Lets take a look at the Titan X. The best card in the world, for 1100$. Not an aftermarket cooler, oh no. But then the Fury X was coming out and Nvidia had to make a slightly gimped version of the Titan X as the 980Ti and sell it at a competitive price point. A month ago the 970 was the best card for the performance in its price range, that title now belongs to the 390. Now the 980 is challenged by the 390x and Fury. The Fury is better but costs a little more, and the 390x is much cheaper and competes well in a few games. The 960 is...really I don't know. The 960 compares to that of a 280 but is beaten by the 280x and 380 at its price point.   The point is, do you seriously think Nvidia won't bring up the price without AMD? The Titan X basically went from 1100$ to 650$ due to the Fury. Intel and AMD also depend on each others existence. If AMD goes under, Intel will have to be split due to US laws, same goes for AMD if Intel were to go under.   Now lets talk Mantle. Mantle was a massive success! Why? Because it started as a single goal: to bring a low level API to the developers. Microsoft reacted with DirectX 12 and Khronos Group were given the Mantle code, AMD has helped both Microsoft and Khronos with the development. Mantle is no longer used, but is now private for AMD to use to test ideas and other thing on it. Mantle itself, never got far, but the goal managed to bring 2 APIs to developer, or well it will.  PhysX by Nvidia would be great if they had done the same as AMD and helped Microsoft and Khronos perhaps implement it into the API, or perhaps taken a more practical route to allow PhysX to run on both Nvidia hardware and AMD hardware. This kind of support would be excellent for gamers. Why? Because with this added support with both hardware companies and the newer APIs, PhysX could become very light and use innovative way to add advanced features that require much less resources. HairWorks is the response to TressFX. Now TressFX runs well on both AMD and Nvidia cards and was released as open source to both. HairWorks on the other hand isn't and runs poorly on anything that isn't a GTX 900 series graphics card. Of course TressFX is not without its faults, and although it most certainly looks excellent, it also sometimes made the hair stretch and didn't have great shadow support. TressFX 2.0 plans to be a landslide better than the first and addresses many of these problems. HairWorks on the other hand does look a little better but comes at a cost, which is framerate and performance, we all saw Witcher 3 get a downgrade from Witcher 2's graphics.  Point being is that being a fanboy is madness, we need both companies. Nvidia is the greedies of the two, but their is no doubt they hold an important role."
nvidia,3dn586,THAT0NEASSHOLE,-1,Sat Jul 18 22:01:40 2015 UTC,"Didn't read it. Go fuck yourself, fanboy."
nvidia,3dn586,THAT0NEASSHOLE,1 point,Sat Jul 18 22:02:17 2015 UTC,"Hahahahaha you are so simple, it amuses me. :P"
nvidia,3dn586,THAT0NEASSHOLE,5,Sat Jul 18 22:03:47 2015 UTC,Well AMD did help develop HBM. So why would this be a surprise?
nvidia,3dn586,Lakus,5,Fri Jul 17 16:59:29 2015 UTC,"wow, rude man. I don't think it's AMD fanboys downvoting you. I think it's that you called the company that helped develop HBM Morons. A lot of manpower went into it and you're acting like a spoiled toddler. They made it, they get it. yes it is rumor, but don't act like someone just stole something from Nvidia. Nvidia has all the options to go with AMD and be apart of this development, but no, they just decided to let others do it for them.  I think it would be fair.  Let's examine Nvidia now for just one thing. G-Sync, yes it's not an open standard and yes it rocks. But they have held onto it as only a cash cow, they will never share it or give it away. Amd does this once and their toddlers? They could have fought for this to not be an open standard. Btw thank AMD and ATI for the GDDR5 we all use now too   ATI worked in solitude for a whole year before it sent initial specification to JEDEC in 2005. Then, Hynix, Qimonda and Samsung joined the effort to bring the new memory standard to life. When AMD acquired ATI in 2006, new management didn’t touch GDDR5 development and let the team to work in peace.   Also there are many people who support neither company and go to both /Nvidia and /AdvancedMicrodevices  I think you need to stop being an Nvidia fanboy when posting on reddit. Makes you look quite childish."
nvidia,3di1vn,ariyapl,14,Thu Jul 16 13:10:44 2015 UTC,"Your playing the lottery when it comes to coil whine, the G1 is one of the most if not the most popular 980ti so the amount of complaints is going to be higher."
nvidia,3di1vn,hdshatter,3,Thu Jul 16 14:37:44 2015 UTC,"I've had 3 Gigabyte G1s so far. 0 coil whine on any of them.   That said, the MSI doens't seem like a bad way to go. As far as temps\noise is concerned, most reviews do cover this. Here's Guru3D:  Guru3D Gigabyte G1 review   Temps Noise levels   Guru3D MSI 6G review   Temps Noise levels   The Gigabyte runs a little cooler and the MSI is a little bit quieter."
nvidia,3di1vn,jim2point0,2,Thu Jul 16 16:10:29 2015 UTC,"I've also had 3 980 TI G1s, 2 from the same shop and 1 from a different country, all had the same, loud coil whine."
nvidia,3di1vn,vldc,1 point,Thu Jul 16 21:06:06 2015 UTC,Did you RMA them ?
nvidia,3di1vn,billjanke,1 point,Thu Jul 16 22:31:15 2015 UTC,I don't know where the hell they got these idle temps. My G1 was running at around 45 Celsius or even more in idle with fans off.
nvidia,3di1vn,emperorpopeatine,2,Thu Jul 16 22:45:44 2015 UTC,"This!  Also note, that coil whine is a tricky issue to solve and can be attributed to many things other than the GPU itself.  For example, my buddy had a G1 980 that had extremely load coil whine in the rig.   Before we decided to RMA it, we check to see if the coil whine was caused from the GPU itself by putting it inside my PC.  Sure enough, it was deathly quiet so the GPU wasn't the issue in his case.  Flash forward, he decided to buy my exact PSU thinking this was the root issue on the whine.  Unfortunately, it didn't solve the coil whine and simply change the ""tone"" of the coil whine.  All in all, we determine that the coil whine must be attributed to a combination of the PSU and MOBO more than the GPU itself.  Since then he had decided to live with it and use a frame limiter especially in game menu windows to reduce the coil whine sound.  Anyhow, good luck on the coil whine issue and hope it gets solved.  But don't be surprised if happens again with a different G1 or a different model."
nvidia,3di1vn,jim2point0,1 point,Thu Jul 16 17:24:08 2015 UTC,Thanks man. Keep your fingers crossed for me please.
nvidia,3di1vn,emperorpopeatine,1 point,Thu Jul 16 22:46:21 2015 UTC,"Slightly unrelated as not 980 Ti: I previously had a 970 and a 980, both Gigabyte G1 models and both had awful coil whine at times, with the 970 being worse.  I now have a EVGA TItan X which has absolutely no coil whine whatsoever.  I initially considered downgrading to a 980 Ti once it was announced but figured I didn't want to play the lottery again with the whine."
nvidia,3di1vn,Aldarro,2,Thu Jul 16 16:50:19 2015 UTC,"I made the swap from TitanX to 980TI. No ragrets! Sold the TitanX for more money than the 980TI costs as well.  Seems like it would be a downgrade, but any non-reference 980TI will be faster than the TitanX. That 12GB of VRAM sure seems nice on paper, but I've yet to run into VRAM issues even at 5120x2880.  (then I bought another 980TI because... Witcher 3)"
nvidia,3di1vn,jim2point0,3,Thu Jul 16 17:23:44 2015 UTC,"I did also factor the cooling issue in, but in the end I decided to keep the Titan X and take the reference cooler off and install an ACX 2.0+ one instead.  u/Aldarro posted a gallery a few weeks ago of them performing this install with their own card and I decided to go ahead with it as Scan were just getting the cooler in stock here in the UK at that time.  Temperatures are MUCH better now, almost as low as my old 970 G1, and the noise level is also MUCH better.  Did I say MUCH better already?"
nvidia,3di1vn,joininggreenfromred,3,Thu Jul 16 18:49:49 2015 UTC,Thanks for the mention! I'm glad your new cooler is serving you well. I think you made the right decision. :)
nvidia,3di1vn,Gaptun,1 point,Thu Jul 16 20:35:13 2015 UTC,I wish I were confident in my GPU surgery skills to do that. I'd be too afraid to break something.
nvidia,3di1vn,FapFlop,6,Thu Jul 16 19:32:16 2015 UTC,My MSI 6G has slight coil whine (barely noticeable) at very high FPS (like 500+). I have set an aggressive fan profile. maintain temps around 70-72c on full load with the fans at 80% and can barely hear them. most 980Tis generally run hot but the MSI ones are slightly hotter. I had mine reach 80c+ until I put a exhaust side case fan right on top of the GPU.
nvidia,3di1vn,FapFlop,1 point,Thu Jul 16 14:45:26 2015 UTC,Mine's whining at 60 fps :(  What PSU do you have?
nvidia,3di1vn,FapFlop,1 point,Thu Jul 16 15:49:51 2015 UTC,Corsair rm850. 60 fps coil whine is bad. Better swap it out.
nvidia,3di1vn,ddeuced,1 point,Thu Jul 16 16:57:39 2015 UTC,"Are you certain that it's not the PSU making a racket? My Antec 900W makes a hellish noise at any kind of load and the card is silent.  I've read disabling the C-states on your motherboard silences this, but mine is missing some of the options."
nvidia,3di1vn,joininggreenfromred,1 point,Thu Jul 16 18:29:09 2015 UTC,"Running EVGA G2 SUPER NOVA 850 W, shouln't be the issue."
nvidia,3di1vn,XXLpeanuts,1 point,Thu Jul 16 20:16:42 2015 UTC,"Roll up a sheet of paper and listen for it, earphone style."
nvidia,3di1vn,XXLpeanuts,1 point,Thu Jul 16 20:18:25 2015 UTC,I had 2x 970 GTX running on this PSU before I decided to swap to 980 ti - no coil whine on any of them even though they were taking more power than the ti. Just rolled and listened - no coil whine from the PSU whatsoever.
nvidia,3di1vn,XXLpeanuts,1 point,Thu Jul 16 22:35:15 2015 UTC,RMA it.
nvidia,3di1vn,XXLpeanuts,1 point,Thu Jul 16 22:38:38 2015 UTC,Already did. DHL guy picked it from me today afternoon. Hopefully the seller won't make it any worse that it is (they're already saying they want to send it to Gigabyte for expertise - WTF?) and just give me another one or swap it to MSI.
nvidia,3di1vn,XXLpeanuts,1 point,Thu Jul 16 22:43:06 2015 UTC,Edit: there is a really slight coil whine from the PSU or something else but I can barely hear it (sitting 30 cm away from my PC). I can only hear it when I turn all the fans off. The whine that was coming out from the card was typical for GPUs - getting more intensive when you rotated the camera in game etc.
nvidia,3di1vn,Xintros,1 point,Thu Jul 16 23:10:08 2015 UTC,"this is my experience exactly, coil whine above 500 but only noticeable in almost complete silence. my card (84 asic) clocked to 1530 and 7800 hovers at 73deg in firestrike with 80% fan profile which is quiet enough that any practically any sound would drown it out. my noiseblocker case fans at around 80% as well are nearly the same sound level, and those fans are really quiet. so yeah, its a pretty damn quiet card. thing is, this is my third card- first two went back due to unacceptable temps at stock clocks. which is the issue plaguing the MSI cards.. moral of the story? keep rolling the dice til you get a winner?"
nvidia,3di1vn,Gaptun,1 point,Fri Jul 17 13:20:55 2015 UTC,"yup noise levels are impressive as in can barely hear them over case fans, that's I have the fan profile a bit aggressive.   84 Asic is impressive there and a nice overclock. Mines 74.8 asic quality. Can push 1504 stable and slight artifacts begin at 1520Mhz."
nvidia,3di1vn,thetonyk123,4,Fri Jul 17 14:32:58 2015 UTC,"Did you try a custom fan curve? My G1 is far quieter than my old XFX 280x card was with my fan curve. Its temps are really good I have found, the 3 fans really helps. Coil whine I only get at like 300+ fps so if you were getting it lower then fair enough, but I think most if not all 980 tis will have coil whine.   My g1 is OC at 1429/7500 and never reaches about 75C with a very aggressive fan curve that does not go over 50% until it reaches 80C which it never does.  Sorry I cant provide any of the info you wanted on the MSI card, I just thought it best you know the G1 can be good, though it may be I was used to a loud card and the G1 is quieter than it but still too loud for you."
nvidia,3di1vn,BearinG,2,Thu Jul 16 13:46:33 2015 UTC,I wish I was getting the whine and buzz only at 300+ fps. Mine was whining at 60 fps in Witcher 3. Which software did you use for the fan curve? OC Guru II ?
nvidia,3di1vn,BearinG,3,Thu Jul 16 14:12:25 2015 UTC,"Damn thats bad yea I would have RMA'd that too. But that was definitely a faulty card. Yes the OC Guru II, its good for OC and Fan curve but my god its annoying that it wants to inform me every time I start my computer up that I have overclocking enabled. You would think a program specifically designed to OC a GPU would not have a pop up every damn launch."
nvidia,3di1vn,UnrepentantN00b,2,Thu Jul 16 14:14:27 2015 UTC,Hopefully they'll change it to another one. I've noticed that I can make it run much quieter with a custom fan curve (what's yours) so these loud fans are not that much of a problem but the coil whine is. I hate playing on headphones and honestly high-end card shouldn't have this crap.
nvidia,3di1vn,djisadud,2,Thu Jul 16 14:24:32 2015 UTC,I'll post mine when I get home if you like. Agreed coil whine is not really acceptable.
nvidia,3di1vn,attomsk,2,Thu Jul 16 14:34:26 2015 UTC,"Thanks man, really appreciate."
nvidia,3di1vn,Ofactorial,2,Thu Jul 16 15:09:25 2015 UTC,"Hi dude, so the Fan curve I currently have is:  https://imgur.com/UHyDBXH  Its not incredibly aggressive its just gradual, enough that it does not make more noise than my case fans.   And the temps are really good in the 60's mainly, if you are ok with running hotter you could keep the fan at 50% through out I am sure.   This is with an OC at boost of 1429/7492"
nvidia,3di1vn,sluflyer06,1 point,Thu Jul 16 21:15:37 2015 UTC,Thanks a lot man! When I will get it back (hopefully a new one with coil whine) I'll definitely try your curve. You're not using the 0db feature at all? Or do you enable this curve only during gaming?
nvidia,3di1vn,Luckyduck1337,1 point,Thu Jul 16 22:36:28 2015 UTC,I think that feature is automatic isnt it? I like my card at 30C idle tbh :D.
nvidia,3di1vn,jim2point0,1 point,Fri Jul 17 07:31:27 2015 UTC,It is when you run auto curve or you just change your curve to 0 until some point.
nvidia,3di1vn,jim2point0,3,Fri Jul 17 07:40:17 2015 UTC,"Well since no one with the MSI has chimed I'm yet I will. Mine has no coil wine whatsoever, really high frame rates or otherwise. It's also a pretty quiet card in my opinion (very subjective thing of course.) I have not touched the fan profile or clock speeds at all. The temps top out at 79° and the fans are up to about 80% at that time. To me the card is quieter than the sapphire R9 290 tri x I just replaced it with."
nvidia,3di1vn,Tissue285,2,Thu Jul 16 14:50:38 2015 UTC,"Nice, I come from the 290 tri -X as well."
nvidia,3di1vn,Aaron8001,1 point,Thu Jul 16 15:15:32 2015 UTC,"Much louder then my old MSI HD 7870 OC Edition but hey, it also has alot more power :p. Plus its silent at idle thanks to the ZeroFrozr thing which is nice."
nvidia,3di1vn,Aaron8001,3,Sun Jul 19 15:06:50 2015 UTC,"I've only had my MSI 980Ti Gaming 6G for 3 days but have not noticed any coilwhine myself. My case (Corsair 600T) is closed with a ""mesh"" window.  I don't have a G1 to compare but I find the fans are surprisingly quiet even under load. My case fans make more noise than the GPU... CPU is watercooled (H100 set on low speed.. 2x120mm).  The GPU fan speed goes up to 63% when doing benchmarks back to back (Core@1504Mhz, Mem@~7600Mhz..ASIC 67.5%) and temps reach ~75-76°C. Ambient temp was fairly warm this week at ~25-26°C in my basement.  I was going to get the G1 (best performance it seems) but the local shop had an MSI in stock so I went for it. I really have no regrets since I do prefer quieter myself and the performance is there.  I guess I automatically assume the G1 might be louder since it has 3 fans but as someone else mentioned, maybe with a custom fan profile, you might get the same results."
nvidia,3di1vn,Aaron8001,1 point,Thu Jul 16 14:59:12 2015 UTC,"That's a pretty nice overclock you have on it. I'm not planning to overclock mine, if I get the MSI but I might try to boost it just a little bit."
nvidia,3dl0a4,Cobra0702,5,Fri Jul 17 03:07:44 2015 UTC,Its GTA itself. Its happening on AMD cards too.
nvidia,3dl0a4,Colorfag,1 point,Fri Jul 17 08:48:00 2015 UTC,"Downgrade to previous GTA V patch seems to be the only way. I struggled really hard with my 750 Ti. Awful, AWFUL stutters."
nvidia,3dl0a4,HoloKK,1 point,Fri Jul 17 03:35:56 2015 UTC,how can I downgrade?
nvidia,3dl0a4,Dr_Paper,1 point,Fri Jul 17 03:43:06 2015 UTC,So i figured out the process I just need the older exe and update files.
nvidia,3djl3h,SolidCake,3,Thu Jul 16 20:08:58 2015 UTC,Newer GTX 970's have been having lots of crash issues. I tried many different ways to fix the issues with no avail so I had MSI RMA it and they replaced it with a working GTX 970. Now I only have FPS drop issues which is better than the GPU crashing every hour.
nvidia,3djl3h,Harv_Spec,2,Thu Jul 16 20:54:42 2015 UTC,353.38 hotfix driver. Although if you are getting it on 347.88 the hotfix may not work since the crashing issue the hotfix addresses was introduced on 352.86 and persisted with 353.06 & 353.30. It wasn't present on 347.88 or 350.12.  If you still get the problem on 353.38 you may need to RMA that card.
nvidia,3dlg67,aquaticsnipes,3,Fri Jul 17 05:50:38 2015 UTC,Their marketing is a heck of a lot better than the competition. I don't think they have anything to worry about right now.
nvidia,3dlg67,r0tten_wAlrus,2,Fri Jul 17 17:36:51 2015 UTC,Nvidia does not need to spend any more on advertising. They already have the vast majority of the market.
nvidia,3dlg67,THAT0NEASSHOLE,1 point,Fri Jul 17 17:25:36 2015 UTC,"I don't think they need to spend more, I think they just need to adjust their focus.  It seems like most of their advertising is online where I think they could reach a new audience through TV ads.  If they want to break out of strictly graphics card market I think they need to focus on more mainstream methods.  Does that make sense?"
nvidia,3dlg67,cyberpunk7t9,2,Fri Jul 17 22:17:31 2015 UTC,"Marketing majors would tell you that Nvidia aren't so bad themselves at marketing. Actually, they rely on modern marketing strategies with a lot of CRM (customer relation management). Nvidia's CEO once said ""We are like the Nike of graphics cards"". It is something Nvidia did well in the last years. Treat PC gaming as a ""sport"". Enchant gamers and make them feel like ""belonging"" to some kind of a sport team."
nvidia,3dlg67,billyalt,1 point,Fri Jul 17 21:03:03 2015 UTC,I buy their product but I am not at all concerned about their marketing.
nvidia,3dlsz8,natanxd,3,Fri Jul 17 08:42:28 2015 UTC,"https://redeem.geforce.com/  The promotion ended, however you can still redeem it. At least when I enter my code the website displays my game and asks to login to steam to proceed (I didn't login because I don't want to activate it on my account)."
nvidia,3dlsz8,feyos,0,Fri Jul 17 12:37:40 2015 UTC,Thank you i didn't know this was the link Amazon told me to go to.
nvidia,3dlsz8,sluflyer06,5,Fri Jul 17 13:32:38 2015 UTC,"Bullshit. I have 2 codes from 980ti purchases that I had not redeemed, I just attempted to redeem one and it worked just fine."
nvidia,3dlsz8,r0b3r71,1 point,Fri Jul 17 13:07:44 2015 UTC,"Do you have CSGO? I would be willing to trade you a code for CSGO for the other code you have. I got it with the intel gaming bundle, but I already have it."
nvidia,3dlsz8,mrstejdm,2,Sun Jul 19 20:44:01 2015 UTC,"The deal ended no? Like the Witcher deal did nearly a month after the games launch. So we'll have new promo soon, what other big title is coming out."
nvidia,3dlsz8,KnifeFed,3,Fri Jul 17 09:22:38 2015 UTC,I just bought a 980 Ti but missed out on this deal. Will I be able to redeem the next one?
nvidia,3dlsz8,hdshatter,1 point,Fri Jul 17 11:01:28 2015 UTC,I just bought one too i don't know
nvidia,3dlsz8,2FastHaste,2,Fri Jul 17 11:47:46 2015 UTC,Its confirmed its MGSV: Phantom Pain.
nvidia,3dlsz8,Ofactorial,2,Fri Jul 17 09:30:58 2015 UTC,"After all those unoptimized games, now it's a game with a forced 60FPS cap.  Why does NVIDIA always choose the worse games possible to compliment their cards?"
nvidia,3dlsz8,djpokeboy,3,Fri Jul 17 13:25:24 2015 UTC,"They choose whatever the biggest name AAA title releasing around that time is. Welcome to modern gaming, where everything is poorly optimized and broken upon release."
nvidia,3dlsz8,Put_It_All_On_Blck,1 point,Fri Jul 17 14:18:57 2015 UTC,At least it's not 30. Some devs are making progress.
nvidia,3dlsz8,mrstejdm,1 point,Fri Jul 17 14:12:48 2015 UTC,"AMD and Nvidia almost always choose games they have had a role in. Witcher 3 and batman both use gameworks. AMD chose tomb raider and battlefield.  I havent paid much attention to phantom pain, but if it doesnt have gameworks components its a huge upgrade if you ask me."
nvidia,3dlsz8,almost_s0ber,1 point,Fri Jul 17 20:44:48 2015 UTC,Sounds about right.
nvidia,3dlsz8,clanky69,1 point,Fri Jul 17 09:31:34 2015 UTC,Hmm MGSV release date is 9/1/2015 so a 1 1/2 month stretch with no free games?
nvidia,3dlsz8,Leonick91,1 point,Fri Jul 17 17:12:27 2015 UTC,somone said metal gear is going to be the new promo. might just be the push i needed to replay my 760 and get a 970. Which the 760 would be going into another computer to replace a 560.
nvidia,3dlsz8,bbqwoa,1 point,Fri Jul 17 15:47:51 2015 UTC,"Well, that's not the way to do it...  You make retailers stop handing out the promo-codes, not make it impossible for people with a valid purchase unable to redeem it, that's just stupid.   I just bought a new card, arriving today, this promotion was part of the deal and now I won't be able to take advantage of it. Granted, I already have the game and completed it (without issues) so would have just handed it to a friend..."
nvidia,3dlsz8,XXLpeanuts,1 point,Fri Jul 17 11:45:38 2015 UTC,return it.
nvidia,3dlsz8,terencecah,1 point,Fri Jul 17 12:17:45 2015 UTC,"Contact Nvidia or your re-seller, tell them this and that there is a new deal that you should be able to get."
nvidia,3dlsz8,sluflyer06,1 point,Fri Jul 17 12:50:58 2015 UTC,"Well fuck, I had 2 unused codes from nvidia"
nvidia,3dlsz8,tuirn,1 point,Fri Jul 17 13:10:39 2015 UTC,"It's still good, don't worry."
nvidia,3dlsz8,keviig,1 point,Fri Jul 17 13:36:13 2015 UTC,I redeemed one yesterday just fine.
nvidia,3dk358,Aldarro,1 point,Thu Jul 16 22:22:52 2015 UTC,"Might as well just turn off the overclock when playing Guild Wars 2. If it's like other MMO's, CPU bottlenecks will come into place far before more GPU power is needed."
nvidia,3dk358,frenchpan,1 point,Fri Jul 17 01:03:16 2015 UTC,"Yeah, it's a bit of a pain, but the OC really isn't needed for this game. Even with supersampling and everything maxed, it's only using half my GPU power at peak times."
nvidia,3dk358,hdshatter,1 point,Fri Jul 17 01:18:17 2015 UTC,"When I used to play this game I had to turn off my overclock, I could run everything else fine and all the benchmarks but this one game made me get a TDR. That was on an AMD GPU though."
nvidia,3dk358,iSmackiNQ,1 point,Fri Jul 17 09:33:12 2015 UTC,"Thanks for your comment. From what I can gather, this particular game is very sensitive to overclocks, far more so than most others. I have no idea why, but it's apparently always been like that!  I guess I'll have to just turn off the overclock for GW2. It seems to be the same for every driver I've tried."
nvidia,3dk358,iSmackiNQ,1 point,Fri Jul 17 17:08:57 2015 UTC,"Are you also using EVGA Precision?  I've had a bunch of issues related to Precision so I switched to Afterburner (My GPUs are EVGA GTX 670 FTW 4GB in SLI - Single GPU mode though, cause 0 gain with SLi but increases stutters)."
nvidia,3dhh4k,Belzelga,7,Thu Jul 16 08:44:07 2015 UTC,353.38 by far
nvidia,3dhh4k,loqyaror,2,Thu Jul 16 13:21:10 2015 UTC,"Shouldn't the answer always be ""the newest ones"" or do things work differently on the green side?"
nvidia,3dhh4k,TaintedSquirrel,2,Thu Jul 16 13:49:32 2015 UTC,"Most recent drivers have been crashing a lot unexpectedly on ""older"" cards. I've heard it has something to do with poor support for Kepler over Maxwell, but idk. I had zero issues with mine, up until the last few drivers. Rolling back to older ones does seem to fix the problem, but it's pretty lame to have to do that IMO."
nvidia,3dhh4k,VLAD1M1R_PUT1N,2,Thu Jul 16 14:01:59 2015 UTC,"It really should be but from April onwards, it's been hit or miss with the newest drivers."
nvidia,3dhh4k,ITZSNAKE,1 point,Thu Jul 16 14:40:15 2015 UTC,"Typically, yes. Just recently like our fearless russian mentioned there have been some issues."
nvidia,3dhh4k,sluflyer06,1 point,Thu Jul 16 14:06:37 2015 UTC,I use 347.88 with my reference 780 as GSync doesn't work correctly (at least for me) with the newer drivers.
nvidia,3dhh4k,ITZSNAKE,1 point,Thu Jul 16 14:04:18 2015 UTC,353.38 gets my vote. It was best for my 780 and still rockin for my 980ti's
nvidia,3dhh4k,sluflyer06,1 point,Thu Jul 16 14:05:40 2015 UTC,I rolled back to the GTAV 350.12 drivers last week as Im sick of the random driver crashes in the last few releases. 780 Classifieds in SLI are now solid as a rock. Yet my 750 Ti machine STILL driver crashes with 353.38
nvidia,3dhh4k,sennan,1 point,Fri Jul 17 00:52:07 2015 UTC,"Kinda depends what game you want to play. Synthetic benchmarks are cool and all, but optimizing gpu performance in-game is much better."
nvidia,3dj6tq,Doctor_sandvich,1 point,Thu Jul 16 18:25:09 2015 UTC,Did you Uninstall old drivers with DDU?  What is your power supply?
nvidia,3dj6tq,hdshatter,1 point,Thu Jul 16 18:36:12 2015 UTC,"I did uninstall with DDU, and the PSU is an unknown 850 or 1000w PSU with an ""S GALAXY"" branding."
nvidia,3dj6tq,hdshatter,1 point,Thu Jul 16 18:41:27 2015 UTC,Enermax EGX850EWL?
nvidia,3dj6tq,hdshatter,1 point,Thu Jul 16 18:43:59 2015 UTC,Something like that.. I sadly dont have access to the recipts atm for it and the case.  EDIT: Don't think it's modular.
nvidia,3dj6tq,hdshatter,2,Thu Jul 16 18:45:44 2015 UTC,"I see that PSU is 8 years old, could be going bad."
nvidia,3dj6tq,p3dr0maz,1 point,Thu Jul 16 18:47:01 2015 UTC,Is there some way I could check it's output usage?
nvidia,3dj6tq,p3dr0maz,1 point,Thu Jul 16 18:49:48 2015 UTC,See if your card is boosting to its max clock in GPU-Z
nvidia,3dj6tq,p3dr0maz,1 point,Thu Jul 16 18:53:06 2015 UTC,Highest clock: 1215.0 core  1752.8 mem   Also said power consumption maxed at 101% TDP?
nvidia,3dj6tq,p3dr0maz,1 point,Thu Jul 16 19:12:01 2015 UTC,Is it a SC ACX 2.0? 1215 is a good boost clock. That's all normal.  Has it ever worked properly? Is this your first test?
nvidia,3dj6tq,schwat,1 point,Thu Jul 16 19:58:11 2015 UTC,"It's a purely reference card aside being made by EVGA (I need a blower card, hence not going for the ACX). I've run several tests today after many driver reinstalls and the latest one: http://www.3dmark.com/3dm/7782315 isn't much better, this is the best its performed (just got it today..)"
nvidia,3dj6tq,schwat,1 point,Thu Jul 16 20:37:13 2015 UTC,"Yeah somethings off, can't really say. This is weird but is there a way maybe you can test a different PCI-E slot? and/or maybe try a MB BIOS update if available or BIOS reset even.  Run GPU-Z and check what your voltage is running at see if something's up there and double check what your temps are."
nvidia,3dj6tq,schwat,1 point,Thu Jul 16 20:43:20 2015 UTC,On my to do list for troubleshooting:  Reinstall windows then drivers and try firestrike again after updates  BIOS Update  I can't realistically change PCIe slots atm thanks to a hard drive cage I cant remove in the way.. really lame 8 year old case with no branding.. may I ask why you think a PCIE slot may be the cause? My GTX660SC worked just fine.. and.. BIOS or win reinstall first?
nvidia,3dj6tq,schwat,1 point,Thu Jul 16 20:46:12 2015 UTC,Depending on your motherboard only certain slots are certain versions PCI-E 2 vs 3. Idk if it can make that big a difference but with these higher end cards it might idk. Just a guess. I would test the BIOS reset before a windows reinstall since it's a real quick test.
nvidia,3dj6tq,tuirn,1 point,Thu Jul 16 20:48:49 2015 UTC,It's on a PCIe 3.0/2.0 x16_1 slot according to the manual
nvidia,3djsah,bramabul5353,3,Thu Jul 16 21:01:21 2015 UTC,"I think the XB270HU is going to be the best spec wise.  I just bought one, and it should arrive today (WHERE ARE YOU UPS?!).  The downside is the QC issues, like backlight bleed, dust behind the screen, and dead pixels.  Though, it seems they have recently fixed a lot of these QC issues.  Build models after May don't seem to have as many issues.  It's 1440p, IPS, 144hz, Gsync.  Basically the holy quartet (quattuorvirate?, don't know what four's trinity equivalent is) right now for monitors.  No other monitor comes close spec wise.  Asus is releasing a similar monitor, but it is slated for a Q4 release.  EDIT: It just came.  Very limited BLB, no dead pixels.  Looks like I got pretty lucky!  I can never go back to 60hz now.  This is amazing."
nvidia,3djsah,djisadud,2,Thu Jul 16 22:54:06 2015 UTC,"I'll second the XB270HU.  I was a day-1 adopter and I haven't experienced any QC issues.  The backlight bleed on my panel (YMMV) is seriously the least intrusive that I've ever seen on an IPS monitor.  The colors are amazing and G-sync is a godsend for these new ""broken"" AAA games.  I had a Swift (two of them technically due to bad QC) and there is a world of difference."
nvidia,3djsah,LendlGlobal,1 point,Fri Jul 17 03:24:09 2015 UTC,Thank you for your suggestion! I am going to take a look into this monitor.
nvidia,3djsah,djisadud,1 point,Fri Jul 17 13:25:28 2015 UTC,Thanks for the response! That monitor looks very interesting.
nvidia,3dhsmm,ihate_registering,2,Thu Jul 16 11:30:40 2015 UTC,"Display driver crashed and recovered..., gta also returns the famous ""err_gfx_3d3_init"" error  Specs: Evga 750w psu i7 4790k gigabyte gtx 760 2gb 16 gigs ram asus z97-c mobo  The card is a few months old at most.  Screens go black for a second the moment when the 3D world of games initializes, then TDR crash.  This happens in GTA 5 and Witcher 3, other games I've tried that work fine are TF2 and Cities Skylines, haven't tried any more demanding games.  My system is freshly installed with nothing but the games, drivers, directx runtimes etc. on it (no logitech stuff etc.)  This started happening when the last GTA update came out, sometimes I'm lucky enough to get the game to render a frame or 2 before it crashes (gta), in these frames, shiny surfaces are usually green or pink. Witcher 3 crashes the drivers instantly when I skip the intro movie.  GPU is NOT overheating - the drivers crash instantly so there isnt even time to get the temps up  Things I've tried:  Underclocking/Overclocking GPU  Underclocking/Overclocking CPU  Display Driver Uninstaller, installing the last 5 or so nvidia drivers, including latest hotfix, selecting only the graphics driver and physx each time  Reinstalling OS: tried with fresh installs of win 7, win 8.1, win 10  Removing Nvidia GF experience  Unplugging my second monitor  All possible game graphics settings permutations  Power management settings in win to ""high performance""  10 passes of memtest86  5 hours of furmark  5 hours of prime95 (all temps are fine, no errors)  Changing the CPU and motherboard  Changing the PSU  I've probably forgotten something I've tried, but the point is that the fault is clearly on Nvidia's side and they aren't doing anything to fix it. More suggestions are welcome."
nvidia,3dhsmm,rapid66,1 point,Thu Jul 16 11:44:30 2015 UTC,I get very similar issues on my 970. I'm playing a game when suddenly the monitor goes black and says no signal. I end up having to force restart my whole computer. Anyone know a fix for this?
nvidia,3dhsmm,wanderjahr,1 point,Thu Jul 16 15:45:56 2015 UTC,Did you try the hotfix drivers?
nvidia,3dhsmm,iwillkicku67,1 point,Thu Jul 16 17:07:12 2015 UTC,Probably should just RMA it.
nvidia,3dgl4m,Nose-Nuggets,4,Thu Jul 16 02:43:03 2015 UTC,You can disable automatic driver updates in Windows 10.  http://windowsinstructed.com/disable-automatic-device-driver-download-in-windows-10/
nvidia,3dgl4m,SP3333D-O,3,Thu Jul 16 13:06:32 2015 UTC,"Same here, W10-10240 with GTX760."
nvidia,3dgl4m,mrcs2000,1 point,Thu Jul 16 03:18:31 2015 UTC,"im still on 10158 with a 980ti. i was hoping RTM would fix it, crap."
nvidia,3dgl4m,mrcs2000,2,Thu Jul 16 03:32:03 2015 UTC,pro-tip: disable windows update service in services.msc and then install 353.30 or 353.38. done!  at least until they update it.. u_u
nvidia,3dgl4m,HawkEye0,1 point,Thu Jul 16 04:39:36 2015 UTC,not a terrible idea... thanks!
nvidia,3dgl4m,HoloKK,1 point,Thu Jul 16 05:43:09 2015 UTC,"Quick Question, I'm on W10 353.30, is it worth updating to 353.38?"
nvidia,3dgl4m,Bartimaeuse,1 point,Thu Jul 16 06:23:43 2015 UTC,"Go 353.49, (wrked fine here)"
nvidia,3dgl4m,WorldwideTauren,3,Thu Jul 16 18:05:25 2015 UTC,Ok so its not just me good to know.
nvidia,3dgl4m,troublegoats,3,Thu Jul 16 03:51:23 2015 UTC,Same here 770. Has NVidia acknowledged this at all?
nvidia,3dgl4m,Jaskys,2,Thu Jul 16 16:58:07 2015 UTC,sadly i don't really know where they would... which is why i posted here.
nvidia,3dgl4m,MiradoOne,1 point,Thu Jul 16 17:13:52 2015 UTC,I think you can turn off the auto updates in ddu
nvidia,3dgl4m,hdshatter,1 point,Thu Jul 16 03:35:07 2015 UTC,"You can't, it's disabled for a long time on my PC and it still downloads drivers regardless."
nvidia,3dgl4m,mrcs2000,1 point,Thu Jul 16 07:03:30 2015 UTC,"I'm on 10162, I get the text select cursor bug (and when you hover over an image that can be made full size), but the start menu is fine. I only have a single monitor so I cannot check the rest.  980 Ti G1 on 353.50."
nvidia,3dgl4m,hdshatter,1 point,Thu Jul 16 03:35:10 2015 UTC,"No issues here, 960m + 10240."
nvidia,3dgl4m,jravn,1 point,Thu Jul 16 05:10:08 2015 UTC,already restarted after installing 353.50?
nvidia,3dgl4m,PiggeryJokery,1 point,Thu Jul 16 05:22:00 2015 UTC,very curious! do you have more than one display with different refresh rates by chance?
nvidia,3dgl4m,mrstejdm,1 point,Thu Jul 16 05:42:35 2015 UTC,I might not be getting the problems because Nvidia Optimus uses Intel Integrated for Windows. So pretty much the one time ever that Optimus will do something good I guess.
nvidia,3dgl4m,FunktasticLucky,1 point,Thu Jul 16 08:29:02 2015 UTC,"I have the same issue when SLI is enabled.  My main monitor is connected to one of my GTX 680s via HDMI. Its resolution and refresh rate are reported correctly by the NCP.   My second monitor is connected to the same GTX 680 via DVI. The NCP does not recognize its resolution nor its refresh rate at all.  What's more, when I go to the ""Adjust desktop size and position"" option, the second monitor is reported to be running at 1680x1050. I don't have the option to set it to its native 1920x1080, nor can I force it to resize on the GPU. I can't change the position of the second monitor either: whenever I move it to align it with the main one on the NCP, it resets to touch one of the corners of the other monitor (as if they were placed diagonally).  So, yeah. I'm pretty sure something's wrong somewhere here..."
nvidia,3dgl4m,GoldieEmu,1 point,Thu Jul 16 06:23:09 2015 UTC,I was actually planning to record this flicker/tearing issue with Shadowplay to demonstrate the issue when I noticed that if you enable Shadowplay the tearing goes away (shuddering on dragging windows remains).  It still seems to be 'fixed' when you disable Shadowplay again! 353.50 - 10240
nvidia,3dgl4m,GoldieEmu,1 point,Thu Jul 16 08:30:50 2015 UTC,"Welp, I'm not updating my drivers..again."
nvidia,3dgl4m,shnytzl,1 point,Thu Jul 16 08:54:25 2015 UTC,Shit... when did this update happen. Last night I was still on 353.38 when I checked around 10pm EST. I'm on 10162.   Edit: BTW. I forgot to link this. This is a powershell module I have been using to hide certain updates from the windows updater in Windows 10. Works great! I hope this works for everyone. I will most likely have to run this and hide this Nvidia update.
nvidia,3dgl4m,Bytex86,1 point,Thu Jul 16 09:14:56 2015 UTC,"Thanks for that, I have a Windows 10 update for my network printer than always fails, I've been wondering how to hide that update."
nvidia,3dgl4m,w00t692,1 point,Thu Jul 16 11:27:17 2015 UTC,"I'm using Windows TP (10240) with Nvidia Drivers 353.49, So far so good, no issues yet. I'm using 4 monitors.  I'll give the .50 version ago."
nvidia,3dgl4m,w00t692,1 point,Thu Jul 16 10:18:05 2015 UTC,same problem here with a GTX 770 and the Win 10 RTM TH1 Professional 10240
nvidia,3dgl4m,Lobrauski,1 point,Thu Jul 16 14:25:05 2015 UTC,"I´m still running .30 drivers and for some reason csgo is capped at 144fps since I updated to 10240 today. I had the same problem before but fixed it by disabling shadowplay, unfortunately that doesn´t work now.  Edit: CSGO is in fullscreen."
nvidia,3dgl4m,Lobrauski,1 point,Thu Jul 16 14:47:46 2015 UTC,i'm on 353.38 w10-build 10240 with a 980 gtx and have no issues.
nvidia,3dgl4m,Zadrym,1 point,Thu Jul 16 17:43:25 2015 UTC,yeah i was on .38 before with no issues. windows update auto-installed .50 and that's when the problems started.
nvidia,3dgl4m,Ranma_chan,1 point,Thu Jul 16 18:08:08 2015 UTC,dang.
nvidia,3dgaj9,t0shki,3,Thu Jul 16 01:14:25 2015 UTC,"Getting my TI on Friday and just dug up some cables for my modular PSU. Wondering which one to use once i plug it in. Should i switch to dual single 8 pin for better power delivery or is the single rail split cable okay? I am getting the MSI Gaming 6G which is overclocked, so i'm not sure."
nvidia,3dgaj9,BlayneTX,3,Thu Jul 16 01:17:02 2015 UTC,If you have two 8-pin cables then use those. This is especially true if you end up overclocking with a custom bios and a higher TDP and are sending 150w+ through each cable.
nvidia,3dgaj9,Dommy73,3,Thu Jul 16 04:25:04 2015 UTC,Thank You! I will go with 2 x single 8-pin then.
nvidia,3dgaj9,Shitty_Human_Being,3,Thu Jul 16 05:44:22 2015 UTC,Spreading the load is never a bad idea. Not just with PSUs.
nvidia,3dgaj9,Dommy73,1 point,Thu Jul 16 08:00:13 2015 UTC,He's gonna be fine with that PSU.
nvidia,3dgaj9,scloutkst2889,2,Thu Jul 16 09:43:49 2015 UTC,"Yup, SeaSonic with japs is great.  But spreading it doesn't hurt a thing."
nvidia,3dgaj9,ImaEvilDoctor,2,Thu Jul 16 10:10:59 2015 UTC,Always split up power as much as you can. Good choice. No need to starve a monster card.
nvidia,3dgaj9,ImaEvilDoctor,2,Fri Jul 17 03:32:13 2015 UTC,"You should be able to use the dual 6+2 pigtails on the right for your GPU. I use the same on my 970s.  I personally don't think it looks that messy, but I guess it depends on how you tuck the cables. Using your image as reference, I have the top 6+2 plugged into the GPU's right set of PCIE slots and the bottom set of 6+2s plugged into the left PCIE slots. The extra 2 pins is tucked in between the pigtail loop.  It does stick out a little bit, but I have a 760T which is rather wide so it doesn't get in the way of my side panel."
nvidia,3dgaj9,Juts,1 point,Thu Jul 16 02:22:52 2015 UTC,"I have a Define R4 which is also pretty wide. For my current card it is okay. I chose the loop as i wanted to use the least amount of cables necessary to get a very clean build. I removed most of my interior. I must say the loop always kinda annoyed me, but i wasn't sure if it's safe to use two cables for one card on my modular. The cables are kinda misleading. I thought i had to use the one that has all the connectors required on a single cable and only use the other if the card only has one connector."
nvidia,3dgaj9,hdshatter,1 point,Thu Jul 16 02:39:07 2015 UTC,I think the pigtail type cables were manufactured later due to consumers complaining about having to run two separate power cables from their PSU to their GPUs.  I did a little bit more Googling and found some posters on LinusTechTips who said they were able to run 400W through a braided pigtail without much issue. If it's true then a 980Ti should be perfectly fine using a pigtail since it's at 250W at max load.  If someone more technically inclined could confirm/dispute this that would be a nice help to me as well since I'm on EVGA's step-up waitlist for my own 980Tis. I'm fairly certain it's safe for 970s which are only around 150W.
nvidia,3dgaj9,hdshatter,1 point,Thu Jul 16 03:59:49 2015 UTC,"Yes, that's what i thought too on first sight. I currently run a AMD R9 280x on single pigtail cable and that is known to draw up to 300w under load. I am almost certain there wont be an issue in delivery, but i have respect for the more high end hardware and guessed things have to run a little different now. Especially when going into (factory) overclocks."
nvidia,3dgaj9,rodrosenberg,2,Thu Jul 16 05:41:22 2015 UTC,"If you have a single rail power supply I dont think it matters. If you have one with 2 rails, you probably want to use two cables to split the load"
nvidia,3dgaj9,djisadud,2,Thu Jul 16 02:55:15 2015 UTC,"I always use separate cables, when I had a 290x I was able to get higher overclocks by using seperate cables."
nvidia,3dgaj9,kmarsara,1 point,Thu Jul 16 08:35:41 2015 UTC,This would confirm my speculation that overclocked cards will benefit from using separate cables.
nvidia,3dgaj9,Neumayer23,0,Thu Jul 16 10:13:25 2015 UTC,I also used to have an MSI board with 2 CPU power in and if I unplugged one of them it made by overclock unstable.
nvidia,3dip9a,XanthosAcanthus,2,Thu Jul 16 16:18:07 2015 UTC,Use DDU to remove all drivers (including integrated) and clean install 353.38 http://www.guru3d.com/files-details/geforce-353-30-driver-download.html it fixed all problems with my MSI 980 Ti
nvidia,3dip9a,Solarityful,1 point,Thu Jul 16 16:57:11 2015 UTC,Did you experience any of the same issues though?
nvidia,3dip9a,Solarityful,1 point,Thu Jul 16 17:46:44 2015 UTC,I haven't tried OCing but FPS was really bad for Mass Effect 3 and Witcher 2 and I had many games crash. The low resolution happens when you remove all drivers and should be fixed when new drivers are fully installed
nvidia,3dip9a,p3dr0maz,1 point,Thu Jul 16 21:17:07 2015 UTC,"Well the thing is that I didn't remove the drivers when that happened. Just happened out of the blue. Experience, control panel and all that were still there acting like they worked. I have installed the driver you linked though, so hopefully that clears some things up for me :)"
nvidia,3dip9a,kittah,1 point,Thu Jul 16 21:45:38 2015 UTC,This helped me before and i mentioned it to someone else and it didnt help them but ill say it again in case it does. If you have power management mode set to adaptive some games which dont use a lot of CPU wont boost your clock. It happened to me with DX9 on Path of Exile. I changed Power Management mode just for that game to Performance and after that it would stay at the base clock permanently.
nvidia,3dhxu5,Scalion,2,Thu Jul 16 12:29:55 2015 UTC,"If I am understanding you correctly, you have both cards in your computer right now? If so, what's your reasoning? Also have you checked your settings within the BIOS to make sure nothing has been reset with the hardware change?"
nvidia,3dhxu5,Krooksy,1 point,Thu Jul 16 12:36:21 2015 UTC,I feel like theres over watt/volt from what my psu can deliver. Yes i checked the bios.
nvidia,3dhxu5,MRChuckNorris,2,Thu Jul 16 15:23:38 2015 UTC,Let us know if you sort this out.  I had the same issue after a 980ti install.  Was tired and fed up after a bios flash and messing with everything so I just gave up and bought a sound card.
nvidia,3dhxu5,empho,1 point,Thu Jul 16 21:58:08 2015 UTC,"Well i'm doing the same thing, i don't want to send my motherboard for repair or what ever, too complicated, i bought a sound card and waiting for it."
nvidia,3dhxu5,FrostyShakez,1 point,Thu Jul 16 22:34:58 2015 UTC,You sure you're actually using the 970 when gaming? I upgraded from a 760 and I noticed heck of a difference. On-topic; The possibility that the audio being fried by the gpu is highly unlikely. It might be the realtek audio drivers which comes with the graphics driver conflicts with earlier drivers you had installed with the 660. You might want to reinstall drivers thoroughly.
nvidia,3dhxu5,KaiForceOne,1 point,Thu Jul 16 12:55:15 2015 UTC,"Yeah the 970 work correctly, im just disapointed for the performance for 440$ (cdn). Im gonna try old driver see if it does anything, didnt try that, but the sound worked correctly even with the new driver version before it.. suddenly went out."
nvidia,3dhxu5,fuccboi9000,1 point,Thu Jul 16 15:25:51 2015 UTC,Did you run the installation disk that came with the GPU? If you did then try removing and reinstalling the drivers on the disk.
nvidia,3dhxu5,darlysama,2,Thu Jul 16 13:19:06 2015 UTC,"No i never use disk that come with hardware, i always go to the website and download the last version."
nvidia,3dk0r4,Dexinthecity,3,Thu Jul 16 22:04:21 2015 UTC,Not to be negative but if someone needs a nearly 2 minute video to install a backplate then they shouldnt be putting on a backplate. They come with a 1 page sheet showing which screws to remove. This is basically a video on how to use a screw driver...
nvidia,3dk0r4,letmeshowyou,-1,Thu Jul 16 22:58:12 2015 UTC,I thought the video was an odd attempt at humor at first.  Especially since this card ships with the backplate already attached (source:  I own it).
nvidia,3dk0r4,bogus83,2,Thu Jul 16 23:56:42 2015 UTC,I dont think they all do. I have 2 reference EVGA SC ti's and had to purchase the backplates seperately. I dont  think the non SC card from EVGA with the ACX cooler comes with one.
nvidia,3dk0r4,letmeshowyou,1 point,Fri Jul 17 00:05:47 2015 UTC,"Yeah, only the SC+ ACX+ model comes with it installed.  I just didn't think many people bought them separately since there's a model that includes it.  Makes sense though since that model has been out of stock pretty frequently."
nvidia,3dk0r4,bogus83,1 point,Fri Jul 17 00:12:38 2015 UTC,Do they not include longer screws to be used? my 780 Classified's backplate did...
nvidia,3djhmx,jedimstr,2,Thu Jul 16 19:43:58 2015 UTC,"The hybrids aren't full blocks anyway. Don't know why they are so popular, guess nobody realized what Hybrid actually means.  The Fury X is a full block, so is pretty much every other water cooled GPU."
nvidia,3djhmx,hdshatter,2,Thu Jul 16 20:16:22 2015 UTC,"Going with a full block is an extremely expensive 8 hour ordeal that has some (very limited) potential to destroy your system. Also, it is exceptionally difficult to resell cards with a custom waterblock.  Going with the EVGA Hybrid cooler is $100 extra. It is an 8 minute ordeal to install. Resell value will be substantial as it is an idiot proof closed loop."
nvidia,3djhmx,Megatag,1 point,Fri Jul 17 04:30:24 2015 UTC,"Full block or not...the blower keeps the VRM's cool enough apparently and the radiators can keep the GPU/mem to less than 30-40C at load, it's good enough for me."
nvidia,3djhmx,hdshatter,2,Thu Jul 16 20:19:59 2015 UTC,The block doesn't touch the memory chips.
nvidia,3djhmx,FunktasticLucky,1 point,Thu Jul 16 20:21:02 2015 UTC,"really? so the blower is keeping the VRM and memory cool and the AIO loop is just for the GPU? hmmm.. still doesnt change my planned system upgrade... I've already got the Acer G-Sync monitor, just want to get get a good and cool running SLI setup without having to go all out with a open loop system.  I've already got my CPU using a Corsair H110i, so call me lazy I guess, but it's what I want. :)"
nvidia,3djhmx,andrew5161,1 point,Thu Jul 16 20:24:59 2015 UTC,"Honestly, if you buy a reference card you can always remove the shroud and install one of the kits floating around. It looks like it would break even in price."
nvidia,3djhmx,hdshatter,1 point,Thu Jul 16 20:37:46 2015 UTC,Yeah...that was Plan B for me... it's not a total even trade because you have to throw in the backplate and overall it's a little more for the kit than getting the Hybrid outright... but the 980ti Hybrid kit is out of stock lately too...
nvidia,3djhmx,kaioshade,1 point,Thu Jul 16 20:40:39 2015 UTC,"Or you can go really crazy and just get a g1, non ref board with a huge full block. Temps rarely exceed 60 under full load with custom fan profile."
nvidia,3djhmx,Ohmps_,1 point,Thu Jul 16 20:44:59 2015 UTC,It's still good its just not as good as a full block.
nvidia,3djhmx,TaintedSquirrel,1 point,Thu Jul 16 20:52:27 2015 UTC,Wait for the corsair cooler to come out. EVGA is completely worthless and cant even stock their own product.   Hopefully corsair can actually make good on their promises and get their coolers released.
nvidia,3djhmx,TeslaFusion,1 point,Thu Jul 16 23:29:49 2015 UTC,you think you have problems  I´m sitting here planning to get one hybrid and suffering with one hd7750 but in germany there are some places were you can get one in at least 2 weeks
nvidia,3djhmx,ddeuced,1 point,Thu Jul 16 20:47:25 2015 UTC,"Just buy the cheapest 980 Ti you can find (ref or not) and slap a G10 + VRM heatsinks w/ a CLC on it.  Should be about the same price as the Hybrid...  Better cooling, too.  Both the MSI and ASUS models were around $620 in the past week."
nvidia,3dgpk9,TurbanSenpai,4,Thu Jul 16 03:22:17 2015 UTC,Yes it is kind of like full image AA instead of just the edges. It's a bigger performance hit though. To me 20-30 fps wouldn't be acceptable for a nicer image but it's up to you.
nvidia,3dgpk9,BlayneTX,1 point,Thu Jul 16 04:50:04 2015 UTC,"Yep. Thats what am dealing with. Everything, including the witcher, drops about 20-30 FPS. But I still get about 49 - 64FPS which is not bad at all."
nvidia,3dgpk9,XXLpeanuts,1 point,Thu Jul 16 04:56:04 2015 UTC,"Try it in older games, its so glorious playing older games at 4k (I have a 980 ti and a native 1440p monitor, so scale accordingly) or with any resolution higher than native. Games that run on unreal engine work great, mirrors edge for instance looks fantastic."
nvidia,3dgpk9,jim2point0,1 point,Thu Jul 16 13:55:04 2015 UTC,"Define old. Tomb Raider Def gives me the same frames as Witcher and GTAV at 1440 except I have hairworks turned on tomb raider.   Games like splinter cell blacklist look about the same at 1440, 2K and 1080."
nvidia,3dgpk9,Archmagnance,1 point,Thu Jul 16 16:12:46 2015 UTC,"1440, 2K and 1080.   Wut?  2K = 1920x1080  You won't really notice a huge difference going form 2560x1440 -> 1920x1080. The real difference comes from 3840x2160 -> 1920x1080"
nvidia,3dgpk9,jim2point0,1 point,Thu Jul 16 16:16:42 2015 UTC,"people refer to 1440p sometimes as 2k  edit: i misread that the comment had 1440, 2k and 1080 nvm"
nvidia,3dgpk9,Archmagnance,1 point,Thu Jul 16 16:21:27 2015 UTC,"That's... weird. Why? That's the equivalent of making up your own terms when it's already been defined.  2K resolution - wikipedia)   2K resolution is a generic term for display devices or content having horizontal resolution on the order of 2,000 pixels. DCI or the Digital Cinema Initiatives defines 2K resolution standard as 2048×1080, or 1998x1080 as Flat presentation.   When 16:9 became the standard for consumer HD displays, the 1080 vertical pixels was kept, and 2048 was narrowed to 1920."
nvidia,3dgpk9,XXLpeanuts,1 point,Thu Jul 16 16:34:43 2015 UTC,Cause people are weird or don't know what they're talking about
nvidia,3dgpk9,HawkEye0,1 point,Thu Jul 16 22:15:13 2015 UTC,"Well games that are not witcher and gta v. Ones made on older engines etc. Well the reason you can use the hair on tombraider is because its amds treesfx and no hairworks, hell i cant run hairworks on a 980 ti in any game other than fc4 and it dont look great in that.   All games look better though, i am going from 2560x1440-3850x2160 and the change is big, anything over 1080 looks a lot better though. Could not go back to that now personally."
nvidia,3dgpk9,HawkEye0,2,Thu Jul 16 17:09:31 2015 UTC,"I prefer DSR by a long shot but AA is much cheaper and with some tweaking you can get looking 99% as good in most games. Is it worth it? If you have the GPU power to spare it certainly is but it's preference. I don't mind playing TW3 at 45 fps, I find it fine for gameplay. Max Payne 3 is a good example without DSR you get 200fps+ why not run it at 4k. It's really just another performance/visuals trade off that is kind of up to each persons preference.   As for GTA V it will always have aliasing at 1080p. Even at 4k with AA on you will still notice small amounts but it does look a lot better, it's the engine I guess."
nvidia,3dgpk9,jim2point0,1 point,Thu Jul 16 06:19:16 2015 UTC,While playing all games I notice that my frames indeed to up at 1080 if I keep my settings but running a game at 50 to 60 at 1440p feels more sluggish than 55 to 75 at 1080. Thus I feel that anything less than 60 just feel slower.   Is that just me?
nvidia,3dgpk9,hdshatter,1 point,Thu Jul 16 06:49:15 2015 UTC,"I think I get what you mean but that feeling goes away after 5 minutes of playing, I can definitely notice the difference between 1080p60 and 1440p45fps but once you adjust 45 is perfectly playable but again it's preference. Plus some games you can DSR with 60fps+ easy.   I know some people that prefer fps however I lean towards visuals, it does depend on the game though. It also depends on your monitor refresh rate, if you have 120/144hz it's very different"
nvidia,3dgpk9,MilkManEX,1 point,Thu Jul 16 07:04:06 2015 UTC,Well anything below 60 and you'll start feeling the variable framerate differences which will always be a bit jarring. That's why Gysnc was invented.
nvidia,3dgpk9,hdshatter,2,Thu Jul 16 16:37:07 2015 UTC,I prefer injecting SMAA over DSR.
nvidia,3dgpk9,kovensky,1 point,Thu Jul 16 11:19:17 2015 UTC,"That's reasonable from a performance perspective, but as far as image quality goes (because post process AA doesn't touch subpixel or temporal aliasing), there's just no substitute for downsampling. SMAA does make a nice companion to DSR/GeDoSaTo, though. Now, if you're sporting a 4k monitor anyway, SMAA is probably all you'd ever need to make things look nice."
nvidia,3dgpk9,XXLpeanuts,2,Thu Jul 16 11:34:22 2015 UTC,Should of added I play at 2560x1440 so I don't need as much AA to make it look good.
nvidia,3dgpk9,sn1313,2,Thu Jul 16 11:40:39 2015 UTC,"You actually want to combine DSR with AA if you can, because DSR uses a rectangular sampling grid, while higher quality AA uses various sampling grids to better resolve the render.  What this means in practical terms is, DSR will handle everything well except for nearly-vertical or nearly-horizontal lines. Adding another AA to it (even FXAA) improves the quality by, at worst, hiding the parts DSR fails at."
nvidia,3dgpk9,cykbryk,1 point,Thu Jul 16 09:28:32 2015 UTC,"While games look amazing using DSR (I have native 1440p monitor and I use DSR to get 4k resolution in a lot of games) I still find myself needing fxaa or smaa in some games to get rid of ALL the jaggies. I use 25% smoothing though, you can use higher values to lessen the sharpness of edges but the image quality suffers.   Overall, if you can afford to performance wise, use dsr because its awesome."
nvidia,3dgpk9,crschindler,1 point,Thu Jul 16 13:52:13 2015 UTC,"I usually use DSR over AA at 1080p...the only thing I notice is that the game is sharper...but I prefer it over the blurred dull look of 1080p textures...and soft edges...  I bounce back and forth, especially since I record and have to set everything at 1080p"
nvidia,3dh7su,Yamiks,6,Thu Jul 16 06:33:01 2015 UTC,I didn't think SLI in a 4x lane was supposed to even work...
nvidia,3dh7su,Soulshot96,2,Thu Jul 16 06:58:30 2015 UTC,does work but with reduced performance and gliches
nvidia,3dh7su,hdshatter,2,Thu Jul 16 07:57:07 2015 UTC,"I am pretty sure you found a bug, Nvidia doesn't let their GPUs run below x8."
nvidia,3dh7su,Soulshot96,2,Thu Jul 16 08:37:22 2015 UTC,"Yep, thats what I thought. He is gonna need a new motherboard lol."
nvidia,3dh7su,sluflyer06,1 point,Thu Jul 16 15:24:08 2015 UTC,contacting support now! guessed so much
nvidia,3dh7su,sluflyer06,3,Thu Jul 16 08:47:44 2015 UTC,"I don't understand why you'd even want to run your cards in a way that might lead to serious performance issues, your motherboard has 2 x16 slots, if the slot spacing is too tight I'd find a cooling setup that makes it work.  SLI is supposed to be disabled unless your run at least x8 x8"
nvidia,3dh7su,sluflyer06,1 point,Thu Jul 16 13:18:44 2015 UTC,i was looking into cooling thing but couldnt find any good or accessable part..see i live in Latvia so maybe you have a suggestion (i can get ebay to deliver things but not sure about anything else!)
nvidia,3dh7su,andrew5161,1 point,Thu Jul 16 13:30:17 2015 UTC,Are your cards triple slot coolers?
nvidia,3dh7su,sluflyer06,1 point,Thu Jul 16 13:39:17 2015 UTC,its funny but they take only 1/2 of the next slot so its like 2.5 slot card
nvidia,3dh7su,andrew5161,1 point,Thu Jul 16 14:03:56 2015 UTC,"Maybe worst case you could get some reference coolers and put them on, I'd think you could find some on Ebay.  I know reference kinda is weak but its better than a fked up SLi setup with problems and performance degradation."
nvidia,3dh5ii,NickMotionless,1 point,Thu Jul 16 06:04:36 2015 UTC,AMG GE is a skin for Raptr. Just download Raptr or Plays.tv and the feature will be available to you on nvidia or amd.
nvidia,3dghbi,kmarsara,2,Thu Jul 16 02:10:36 2015 UTC,They closed their store in Santa Clara :(  It was practically down the street from Nvidia's HQ!
nvidia,3dghbi,ImaEvilDoctor,2,Thu Jul 16 04:08:08 2015 UTC,Is MSRP really $800 or is this $100 off an inflated price?
nvidia,3dghbi,BlayneTX,1 point,Thu Jul 16 04:53:49 2015 UTC,"thats just an advertising gimmick on all retailers, but they do price match all the major web places like newegg and amazon"
nvidia,3dghbi,sluflyer06,1 point,Thu Jul 16 05:24:34 2015 UTC,"Microcenter does not have a actual price match policy, they USED TO reliably price match years ago, these days it's quite rare. People have the most luck price matching if you spend a ton of money at once like if you are buying most/all the parts for a build at once.  I have a microcenter in town and go there fairly often as do my friends, we have been denied price matching most of the time even with the store manager.  They usually make up some bullshit that part A is cheaper than store X so it doesn't matter that part B is more."
nvidia,3dghbi,Bobbycats123,1 point,Thu Jul 16 13:21:43 2015 UTC,But isn't it more expensive to buy it through them because of tax?
nvidia,3dghbi,GBT_Van,2,Thu Jul 16 05:38:34 2015 UTC,"well to me personally its worth it, instant gratification and also i tend to buy their warranty plan. so anything goes ""wrong"" in the next 2 years i can take it back and get a full refund in store credit"
nvidia,3dhmom,infamous2117,1 point,Thu Jul 16 10:05:32 2015 UTC,It is a competitor so most likely done intentionally.
nvidia,3dfi9p,Izviral,7,Wed Jul 15 21:30:24 2015 UTC,I'll recommend the Gigabyte G1 as it will likely have the highest OC potential of any of the vanilla aftermarket cards.
nvidia,3dfi9p,BlayneTX,2,Thu Jul 16 05:22:49 2015 UTC,"Agreed currently have one and while I came from a 280x, its quieter, has fantastic OC room, I like the default OC program too guru II its decent enough.   The thing is great and the exact same size length wise as my 280x."
nvidia,3dfi9p,XXLpeanuts,8,Thu Jul 16 13:56:31 2015 UTC,"EVGA's coolers are pretty good, but they are smaller than a lot of their competitors' because they're the same size as reference.  Personally, I think the ACX coolers look the best (if that means anything to you), and I happily take a slight cooling performance hit for the excellent support EVGA provides."
nvidia,3dfi9p,Aldarro,1 point,Wed Jul 15 22:38:36 2015 UTC,It's the same size but is it a custom PCB or is it still a reference? I'm curious because I'm in EVGA's step up to their base 980Ti model and I might be interested in water cooling in the future.  EKWB looks like it has GPU coolers for reference 980Tis but when I was previously looking at coolers for my 970s the EVGA ones were not compatible with any of EK's full blocks.
nvidia,3dfi9p,ImaEvilDoctor,2,Thu Jul 16 02:27:49 2015 UTC,All of EVGA's 980 Tis are reference PCB except for the Classified.
nvidia,3dfi9p,Syn246,1 point,Thu Jul 16 03:02:09 2015 UTC,Really? Man I can't stand the ACX looks. Id take any other cooler over its looks.
nvidia,3dfi9p,zkDredrick,3,Thu Jul 16 08:26:21 2015 UTC,Feast your eyes on this beauty
nvidia,3dfi9p,jim2point0,1 point,Thu Jul 16 16:40:23 2015 UTC,"The new K|ngp|n model is amazing... So much nicer than the 780 Ti version. I love how there's a little window to show the heat-pipes in the middle of the shroud!  That said, those cards are a bit of a waste to run with the ACX cooler. Most will be water-cooling those bad boys."
nvidia,3dfi9p,Aldarro,2,Thu Jul 16 21:06:19 2015 UTC,"Well, that's the beauty of choice, I suppose! I personally think the ACX cooler looks simple and classy, but I do have an appreciation for the more extravagant designs as well (such as the new Strix or Windforce coolers)."
nvidia,3dfi9p,Aldarro,3,Thu Jul 16 20:34:13 2015 UTC,"The BP stands for backplate - it helps with rigidity I believe ( more info on SuperUser ).  I just purchased that same card two weeks ago - upgraded from an AMD 7970, very very happy with it thus far."
nvidia,3dfi9p,CompleteTruth,1 point,Wed Jul 15 21:55:55 2015 UTC,How much of a difference do you notice? I currently am using a AMD 6870 so I'm sure the difference will be huge
nvidia,3dfi9p,CompleteTruth,1 point,Wed Jul 15 22:04:14 2015 UTC,"It's been a big difference for me, I'm running at 4096x2160 and it has really helped with the newer games."
nvidia,3dfi9p,BKachur,1 point,Wed Jul 15 22:22:57 2015 UTC,"Difference will be massive from what you have now. If you intend to overlock your card (there is no reason not to really) and don't mind spending the extra 30 buck I would highly recommend going for the EVGA classified 980ti, its better than the one you have linked, alternatively also checkout the G1 from Gigabyte, it is apparently one of the best."
nvidia,3dfi9p,WolfyCat,4,Thu Jul 16 03:30:51 2015 UTC,"I know this opinion kind of goes against the grain, but purely from an aesthetics point of view, the Zotac 980 TI's do look pretty damn sexy."
nvidia,3dfi9p,BKachur,3,Thu Jul 16 01:54:01 2015 UTC,I've read a few threads on here about the Zotac having poor OC headroom from their binning but that could just be a vocal minority.
nvidia,3dfi9p,hdshatter,1 point,Thu Jul 16 03:28:29 2015 UTC,A lot of people don't see zotac as a serious brand so their amount of users is pretty small to begin with.
nvidia,3dfi9p,billjanke,2,Thu Jul 16 11:21:20 2015 UTC,"Can't go wrong with EVGA.  Personally, I went with the EVGA Reference SC model simply due to availability back on June 2nd.  Haven't regretted it sense.  Of course the benefit of your model that you picked is the added backplate and the aftermarket cooler design.  But in all honest, you can't go wrong with any 980Ti.  Also in terms of overclocking, it's actually pretty easy to do without modifying the voltage on these cards so I'd consider trying it and see what it's all about."
nvidia,3dfi9p,Syliss1,2,Wed Jul 15 22:12:58 2015 UTC,"I just have a stock Gigabyte one, but my old 770 was EVGA, and I was very happy with it."
nvidia,3dfi9p,DrShockalypse,2,Thu Jul 16 01:28:40 2015 UTC,"I was in the same boat, and ended up buying an EVGA 980ti reference from their official site. This gives me the card now and then the option of buying a water cooler later (likely the Hybrid). I only have a 1080p display so the 980ti is super overkill, but for my system I couldn't be happier ."
nvidia,3dfi9p,t0shki,2,Thu Jul 16 02:27:24 2015 UTC,"It's a tough one. Depends on what you are looking for in a card. Is it  size, silence, balance, speed, looks.. etc. All factors in on your decision.   Please take the following with a grain of salt. I do not own any of them and use my knowledge gathered from studying reviews. Mainly Guru3D and watching many YouTube reviews.  From my research i can tell you that the ""Zotac AMP! Extreme"" is the fastest yet, but also not very silent at that. The ""MSI Gaming 6G"" comes close by a few FPS but is a also less noisy. ""Gigabyte G1"" is in the middle in performance and noise.  The ""EVGA SC+"" is very silent, but also doesn't have that huge of a factory overclock to work with.   I don't have any RMA experience with said companies and i personally went with MSI. It really really depends what you want. If its raw performance, go with Zotac i'd say. They really pushed their model hard. But i can't tell first hand."
nvidia,3dfi9p,DexRogue,2,Thu Jul 16 02:29:27 2015 UTC,"My vote is for the Gigabyte G1. I love mine, it's a beast and it's super silent, I can't hear it over other fans in my case and the temp never goes above 71."
nvidia,3dfi9p,Icanhaswatur,1 point,Thu Jul 16 05:51:55 2015 UTC,"Take a look at reviews of each card and read the temperature part. I just ordered a SC w/ACX, and reviews don't show it ever going over 75c at load. That's is pretty good and Def good enough for me. My reference regular 980 hit 80c so fast, that I'd so so glad to have it at 75c full load.   It may not be the best among all of them but for 650 it seems like a steal to me."
nvidia,3dfi9p,Syn246,1 point,Wed Jul 15 23:02:21 2015 UTC,"If you are not overclocking at all, buy the card with the highest factory OC so that you get the best performance for your money. Cooling and power delivery will not be issues at these clock ranges.  If you do decide to OC, you will be better served with a card that has an aftermarket PCB due to the increased power target and improved delivery of stable, clean power to the GPU. Plus, non-reference PCB means it will have a non-reference cooler, allowing you to clock higher while maintaining lower temps vs. a reference cooler."
nvidia,3dgz3q,Delerium89,1 point,Thu Jul 16 04:53:12 2015 UTC,"update to latest drivers, and install msi afterburner or nvidia inspector.then slowly increase cpu clock and memory clock while doing a stress test .check the temperatures of both gpu and memory whiles doing it.try to aim at an desirable speed without straining your coolers or overheating your card."
nvidia,3dgz3q,emmausgamer,1 point,Thu Jul 16 07:49:34 2015 UTC,"Get afterburner, move power and temperature targets to maximum then start bumping clock speed a little at a time and run benchmarks and games and look for lack of artifacts and stability issues.  I would find your best core speed before even touching the RAM.  I'd also suggest finding your max core speed at the stock voltage and then moving up from there with more voltage.  Essentially you want to change only 1 thing at a time."
nvidia,3dgz3q,sluflyer06,1 point,Thu Jul 16 13:48:30 2015 UTC,"Check out the official 780 ti classified club on overclock forums. They have great resources such as modded BIOS, the classified voltage tweaker, etc. I used a waterblock and got my 780 ti classy overvolted to 1.45V and OC'd the hell out of  it. The temps remained below 65C, so it still had headroom for more. Someone posted their 980 STRIX benchmark in a thread I made and it was 2 fps slower in Heaven 4. the 780 ti classy is my favorite card i ever owned because of the OC potential"
nvidia,3dfwrg,Icanhaswatur,7,Wed Jul 15 23:20:44 2015 UTC,None.  Welcome to flagship GPUs.
nvidia,3dfwrg,Zinthros,1 point,Thu Jul 16 01:36:59 2015 UTC,Have a flagship gpu for the first time in my life. Feels good man.
nvidia,3dfwrg,TruckChuck,1 point,Fri Jul 17 06:51:43 2015 UTC,"Thanks, tho I had a regular 980. Decided to sell it over a month ago to get a 980ti upon release, but shit happened, never got money, lost the card in the process. Worked 100 hours over the past 2 weeks, and decided yesterday to say fuck it, and finally get it.   Was just a little weary with all the posts about people having errors. Didnt know if they were 980ti related or mjust in general as I didnt have errors with my 980."
nvidia,3dfwrg,andrew5161,18,Thu Jul 16 02:22:40 2015 UTC,Your fps being too high. G1 pride world wide.
nvidia,3dfwrg,mermaliens,1 point,Wed Jul 15 23:28:37 2015 UTC,this. The screen tearing is real
nvidia,3dfwrg,sluflyer06,1 point,Thu Jul 16 01:14:43 2015 UTC,G1 represent!
nvidia,3dfwrg,hdshatter,4,Thu Jul 16 13:22:39 2015 UTC,"Before installing the card Uninstall your current drivers, then run DDU. After that install the card in your system and  install the drivers.  Only issue you may have is coil whine."
nvidia,3dfwrg,Soulshot96,1 point,Wed Jul 15 23:55:21 2015 UTC,"Do what he just said, but ALSO select custom install and DO NOT install 3D Vision or HD Audio drivers unless you NEED them, this has helped me avoid the TDR issues since they became a problem."
nvidia,3dfwrg,baskura,1 point,Thu Jul 16 07:04:14 2015 UTC,How is coil whine still a thing on these top end cards :(  Got a GTX680 when they were current - coil whine. Then upgraded to a GTX780ti some time later - coil whine. It's annoying and hurts my whittle ears! :(
nvidia,3dfwrg,Lanessar,1 point,Thu Jul 16 09:02:56 2015 UTC,"In my experience, coil whine only occurs when your monitor's refresh rate is much lower than the frames generated by the card. There are a number of ways you can resolve this, just takes some google-fu."
nvidia,3dfwrg,XXLpeanuts,1 point,Thu Jul 16 13:14:22 2015 UTC,Adaptive vsync ftw.
nvidia,3dfwrg,sluflyer06,1 point,Thu Jul 16 14:00:14 2015 UTC,I have 2 980ti's..no coil whine.
nvidia,3dfwrg,sluflyer06,1 point,Thu Jul 16 13:23:23 2015 UTC,"I didn't even bother to do that, I'm running 353.48, uninstalled my 780 and then popped in both G1's and booted up. Desktop came up low res but after about a minute windows loaded the nvidia driver up with the new hardware and all I had to do was enable SLI."
nvidia,3dfwrg,XXLpeanuts,1 point,Thu Jul 16 13:24:38 2015 UTC,You mad man.
nvidia,3dfwrg,sluflyer06,2,Thu Jul 16 14:00:36 2015 UTC,Loco.
nvidia,3dfwrg,Delbunk,0,Thu Jul 16 14:04:30 2015 UTC,"With drivers nowdays all that isn't really necessary anymore, just uninstall old drivers in control panel. Went from HD7950 to 980ti g1 and that's what I did, everything's been stable."
nvidia,3dfwrg,XXLpeanuts,1 point,Thu Jul 16 00:35:03 2015 UTC,If you know anything about windows you know this is NOT a guarantee.
nvidia,3dfwrg,BKachur,1 point,Thu Jul 16 14:01:09 2015 UTC,"You literally never know when problems come up. When you don't use DDU there are still bits of the old memory driver left in the registry, maybe it could never pop up, but it takes literally 5 min and saves you potential future headaches if anything ever acts up (for instance getting BSOD driver crashes months down the line when a new update is pushed)."
nvidia,3dfwrg,Delbunk,1 point,Thu Jul 16 03:27:13 2015 UTC,"I've heard conflicting opinions about this on Reddit, and don't know who to believe. But yeah, better safe than sorry, you're right about that."
nvidia,3dfwrg,Syn246,3,Thu Jul 16 04:17:05 2015 UTC,"My advice is just to be mindful of the fact that Nvidia's latest driver releases have been garbage quality, especially relative to how stable they always used to be. They've had major issues with every release since the Far Cry 4 driver last November. The good news is that some of the newest hotfix drivers are finally addressing some common stability issues so hopefully this will all be behind us soon.  My overall point is: if you experience crashing with a certain driver, assume it's the driver itself and try a new version before dismissing it as a defect with the card or a result of an unstable OC. My first driver test with SLI G1s resulted in crashing in the low-1400 core clock range--I was horrified that I got one or two dud cards but as soon as I tried the 353.38 hotfix driver I was able to go over 1500 core clock fully stable."
nvidia,3dfwrg,sluflyer06,2,Thu Jul 16 02:59:27 2015 UTC,I also found 353.38 to be my version of choice right now.
nvidia,3dfwrg,Fukui_San,2,Thu Jul 16 13:35:09 2015 UTC,"I had absolutely no issues installing mine. Like others have said, run ddu, shut down compulator, remove old GPU. Install new GPU, download drivers, right as rain."
nvidia,3dfwrg,XXLpeanuts,2,Thu Jul 16 02:07:48 2015 UTC,"Adaptive V-sync is your friend, so are the 353.38 drivers currently."
nvidia,3dfwrg,NickMotionless,1 point,Thu Jul 16 13:59:08 2015 UTC,"DDU is bae. Use it, install the card and cream your panties at the glory that is the 980 Ti."
nvidia,3dfwrg,Des_Eagle,1 point,Thu Jul 16 06:06:03 2015 UTC,"You will either get crashes or you won't, there's almost nothing you can do about it in some cases other than wait for new drivers. If you are lucky enough to not be getting them, never speak of it again and enjoy your new GPU."
nvidia,3dibyl,Omninnihilation,2,Thu Jul 16 14:37:52 2015 UTC,Short answer: No.
nvidia,3dibyl,Xintros,1 point,Thu Jul 16 14:51:53 2015 UTC,"Nope.  Only way to get some sort or value is to resell the code on a place like Ebay before you redeemed it.  Since you redeemed it,  you've exhausted your options.  The good news is they'll be fixing the game and re-release probably sometime late in the Fall or even into November seeing the condition of the game.  Game will be good once it becomes playable.   ;("
nvidia,3dibyl,billjanke,1 point,Thu Jul 16 14:57:14 2015 UTC,"I thought the free games were just an incentive that didn't add or take any value for the GPU. When I bought my 970. It was still 329 from newegg plus I remember the 970 were selling for 329 (around late last year) before Nvidia  started the free games promoition a few months ago. So either way, Nvidia paid out of their pocket just for the free games promotion (probably out of their marketing budget for the games). I don't believe Nvidia ""Has"" to entitle their customers to another ""free"" game but it would make Nvidia the Good Guy Nvidia if they decide that route."
nvidia,3dibyl,spoondigg,0,Thu Jul 16 15:51:09 2015 UTC,Just hang on in there buddy. It's likely Nvidia will send the promo codes for their next game (The Phantom Pain) to those who got the AK promo code.
nvidia,3dibyl,himmatsj,2,Thu Jul 16 14:59:44 2015 UTC,Does that include everyone that received an AK code or only people who haven't activated their AK promo codes?
nvidia,3dibyl,Harv_Spec,1 point,Thu Jul 16 21:11:36 2015 UTC,I hope. Thanks anyways.
nvidia,3dibyl,Fweep321,1 point,Thu Jul 16 15:06:53 2015 UTC,How did you find that out? I've been looking for what and when their next promo game will be and can't find where to look.
nvidia,3dibyl,himmatsj,1 point,Thu Jul 16 15:26:42 2015 UTC,I just found out in fact. You'll see it hitting the news within the next day or two. Pretty good I'd say...better than any Ubisoft bullshit game.
nvidia,3dibyl,jaredmw,1 point,Thu Jul 16 15:28:39 2015 UTC,"I REALLY hope so, that game looks fantastic!"
nvidia,3dibyl,-mr-tibbs-,1 point,Thu Jul 16 18:05:26 2015 UTC,That would be awesome.
nvidia,3dibyl,Ofactorial,1 point,Thu Jul 16 18:22:11 2015 UTC,"It'd be nice, but I doubt it'll happen. nVidia has to pay money to get these promo codes (publishers aren't in the business of giving away free games because a company asked). So even with AK being the disaster that it was, that was still a not-trivial amount of money spent on the promo, money that they can't get back. Anyone who got the AK promo code already bought a card, so those sales are already done, meaning there's no incentive to give those people more games. Maybe you could argue customer loyalty, but come on, how likely is it that anyone is going to switch to AMD next time they buy a card because they remember getting AK as the advertised promo?"
nvidia,3dha37,bmwmk3,2,Thu Jul 16 07:02:50 2015 UTC,"Ha, I was considering side grading from a SC+ to a G1 - just can't get over how I can't get anywhere near the G1 overclocks. Total first world problem.  Question - if you're gaming with a headset on, do you hear the coil whine? I can't tell if I've somehow never experienced it by sheer luck, or never heard it due to how I game."
nvidia,3dha37,big_three,1 point,Thu Jul 16 13:50:16 2015 UTC,"I keep hearing about this coil whine, but both me and 3 of my friends all own g1's and i have heard all of theirs functioning in person and mine and never heard it. I think people may be referring to the coil while every gpu in the world has on opening screens when you hit 2-3 k fps. Otherwise it must be a rather small chance of it occurring if none of our 4 cards + several of my online friends haven't had it as a problem."
nvidia,3dha37,andrew5161,2,Thu Jul 16 15:14:22 2015 UTC,Even when I am playing at 60 fps I hear it. I guess I just got unlucky.
nvidia,3dha37,andrew5161,1 point,Thu Jul 16 15:20:45 2015 UTC,"Yea, that must be it, that or we some how got lucky with 5 cards all shipped within a day or 2 of each other."
nvidia,3dha37,keviig,1 point,Thu Jul 16 15:30:48 2015 UTC,"I have the same issue with the G1 i got yesterday. Coil whine at anything from 40 fps and upwards. I can hear it easily while sitting a meter away from my case. The whine is worse, and more high pitched at menus with lots of fps.   I'm RMA'ing it. Hopefully my next one will be better. However, no G1's in stock in Norway anymore, so i'm giving the MSI Gaming one a go. *This is the 7th GPU i've bought.   I've had AMD, Nvidia, low end and high end gpus before, but this is the first one with coil whine i can hear."
nvidia,3dha37,CaptSkunk,1 point,Thu Jul 16 16:42:28 2015 UTC,"Just to be on the safe side, I'd recommend you use DDU to uninstall the drivers and then install the latest drivers. If you want to OC, then Afterburner is the best program out there and works with all cards."
nvidia,3dha37,stealer0517,1 point,Thu Jul 16 11:56:49 2015 UTC,"Nothing   Hell upgrading from my 660 to a 970 didn't even require a driver upgrade, and when i tried to do it it would just say that I'm all up to date"
nvidia,3dha37,GBT_Van,1 point,Thu Jul 16 16:59:48 2015 UTC,Sorry to hear you returned the card D:  I'm a big fan of using DDU to uninstall old drivers when I'm swapping out cards. Will it be necessary for you? Probably not. It's good to be safe though.
nvidia,3dgca2,Illamerica,2,Thu Jul 16 01:28:58 2015 UTC,"Have you tried uninstalling your graphics drivers completely and reinstalling them? At first I was about to say it might be an issue with the VRAM if you're playing something like Modor with high res textures, but Maple Story shouldn't be doing that."
nvidia,3dgca2,iWoundPwn,1 point,Thu Jul 16 03:11:40 2015 UTC,Is the card over clocked? Which version of drivers are you running? I had small issues like this once and it was the graphics card getting hot and down clocking for like a second or two then recovering back to its boost clock. Try using precision X or afterburners osd to look at temps and boost clock.
nvidia,3dgca2,FunktasticLucky,0,Thu Jul 16 16:45:53 2015 UTC,"It's at stock clocks and current drivers, what did you do to fix that"
nvidia,3dgca2,FunktasticLucky,1 point,Thu Jul 16 21:56:36 2015 UTC,You need to verify that's the issue first. In my case I just needed to make a more aggressive fan profile.
nvidia,3deogw,FunktasticLucky,2,Wed Jul 15 18:05:01 2015 UTC,"2 Gigabyte G1s here. I'll mostly be talking about my experiences with SLI. But I'll post some single card benches.    Benchmarks  Card 1: 76.5% ASIC - max boost 1530mhz, +400 memory   Firestrike Normal Firestrike Ultra   Card 2: 69% ASIC - max boost 1520mhz, +600 memory   Firestrike Normal Firestrike Ultra   Both Cards in SLI   Firestrike Normal Firestrike Ultra   Note: All benches are run on Windows 7. Those graphics scores would actually be higher on Windows 8.1, so keep that in mind if you're using that operating system instead.    Temps, Noise  Here's my afterburner setup in SLI:   +130% power limit (no harm in maxing this) +87mV +110mhz core - boosting to a max synced core clock of 1510mhz +400mhz memory   When pushing both of these cards (for example, Witcher 3 in 4K) you're going to be generating a decent amount of heat. 2 non-reference 980TIs in SLI is a recipe for a space heater in your room. Especially if you overclock them further. I can feel the room temp raise a degree or two, and my entire case is probably feeling the burn as well. The top card will easily jump to 80 degrees unless you set an aggressive fan profile. In turn, that will cause a lot of noise.   The best solution is to have a big fan in the side panel of your case to blow air into the GPUS for the top card to suck in. That's been keeping my top card at a maximum of 75 degrees and 75% fan speed, which is not audible when I'm playing any games due to wearing headphones. My wife can't hear it from the next room over so that's my very specific and indepth review of the noise levels :P   Several reviews cover this sort of thing though. Here's Guru3D:   Temps Noise levels   Compare that to another popular card, the MSI:   Temps Noise levels   The Gigabyte runs cooler and a little bit louder."
nvidia,3deogw,jim2point0,1 point,Wed Jul 15 19:23:52 2015 UTC,Could you run an SLI test in regular Firestrike?
nvidia,3deogw,DrexelDragon93,1 point,Thu Jul 16 02:11:49 2015 UTC,"The scaling I get in firestrike normal is pretty bad. Not sure what's going on there.  http://www.3dmark.com/fs/5415784  I immediately ran ultra again and saw over 9000  http://www.3dmark.com/fs/5415828  I don't know if that graphics score for normal is good or not, but I've seen 2 980 TIss break 40,000. That is insane (but also Windows 8.1)"
nvidia,3deogw,jim2point0,1 point,Thu Jul 16 03:15:55 2015 UTC,something about that score looks fishy. and I don't think that's 8.1. I believe it's windows 10.  It's still 8.1 at heart and my 3dmark scores always show up as 8.1.  It would also explain the not approved drivers.
nvidia,3deogw,jim2point0,1 point,Thu Jul 16 03:48:42 2015 UTC,"All of the hotfix drivers show up as ""not approved."" You see that in my benches as well."
nvidia,3deogw,jim2point0,1 point,Thu Jul 16 04:26:56 2015 UTC,That makes sense. But still. It's reporting extremely low clocks but extremely high score.
nvidia,3deogw,the_unusual_suspect,1 point,Thu Jul 16 08:47:10 2015 UTC,"If he has the voltage on the cards cranked up real high, then the clocks are going to skyrocket compared to what's reported in the benchmark. But yeah, something seems fishy. Probably one of those people that uses every single ""trick"" to get higher bench scores. Because according to all the results I've seen thus far, regular firestrike has 160-170% scaling in SLI. That would mean his 2 cards are getting 24,500-25,000 graphics scores in single card mode.... O_O"
nvidia,3deogw,sluflyer06,2,Thu Jul 16 13:57:34 2015 UTC,"1520 boost, +305 on the memory, 130% power limit, 0 extra voltage, 77.8% ASIC. Runs at about 72c in heaven."
nvidia,3deogw,ggwp9999,1 point,Fri Jul 17 04:39:56 2015 UTC,Damn. Sounds like you really won the lottery there. That's awesome.
nvidia,3deogw,sluflyer06,1 point,Fri Jul 17 10:44:35 2015 UTC,"Well, I'm not that useful yet. I have 2 G1's in SLI and I knew from previous SLI experience that non-reference SLI setups are like easy-bake ovens, therefore I'm not overclocking yet because the cards are struggling to get good air (top card fan runs 15% higher rpm to maintain a higher temp than the bottom card arleady).  With that said left at default my cards are boosting to 1314mhz.  As son as EK releases blocks I'll be ordering them and then OCing."
nvidia,3deogw,ggwp9999,1 point,Wed Jul 15 18:18:59 2015 UTC,just curious what is the fan speed for your top and bottom card and the temps? Thanks
nvidia,3deogw,jim2point0,1 point,Wed Jul 15 18:24:57 2015 UTC,"I'll get some data for you later (I'm in the office). I'll have to qualify each fan speed and temp with a load % since they change drastically based on that variable.  In world of tanks the fans don't even turn on, but something like Project Cars is closing in on 90+% load and Witcher 3 is 52% load on average on Ultra w/ hairworks at 60fps cap. (I still have ""low res"" 1080p 120hz monitor, waiting on the new batch of Gsync IPS monitors to come out to pair with this setup)"
nvidia,3deogw,jim2point0,1 point,Wed Jul 15 18:29:17 2015 UTC,"Thanks a lot. I'm asking because I too have GTX 980TI in SLI but they are struggling to keep the temp at normal range (75-82c top card), i have to turn the fan speed all the way to 80% to have it stable at that."
nvidia,3deogw,Luckyduck1337,1 point,Wed Jul 15 18:34:55 2015 UTC,"Thing is, non-reference coolers are all intake while reference coolers use blowers. The blower style coolers are usually recommended for SLI. The non-reference cards needs to suck in air to keep the GPU cool, but the bottom card will restrict that airflow and will also cause the top card to suck in hot air as well.   The only solution is to have cooler air from outside the cause get blown into the case via a fan on your side panel. A lot of airflow from the front of the case will help too, of course. Or just have the door off and have a fan sat next to the tower :P"
nvidia,3deogw,jim2point0,1 point,Wed Jul 15 19:33:04 2015 UTC,I'm not so sure this is very accurate. I don't know how much air is being restricted. The issue is that it's dumping so much heat in the case and the bottom card is directly feeding that hot air into the top cards fans. I'm gonna buy a G1 and run it up top and then a reference down under and see how it goes.
nvidia,3deogw,Luckyduck1337,1 point,Wed Jul 15 20:07:42 2015 UTC,"and will also cause the top card to suck in hot air as well.   That should have been my primary point. You're right, I'm not sure that ""restricting airflow"" is the primary reason, but it makes sense in my head. It's mostly that the air between the cards is going to be hot, and the only way to solve that is to have cool air being blown into that space. Or.... ya know... water cooling. But that's too hardcore for me."
nvidia,3deogw,jim2point0,1 point,Wed Jul 15 20:12:20 2015 UTC,Well for now I'm keeping my HAF932. I have 4 gentle typhoon's ap-15 running wide open. It keeps my EVGA 980 acx's pretty cool. They are on a very aggressive profile but you still see the temps rise when running heaven benchmark. They will both get to like 54C and then slowly climb as they saturate the air with heat. After about 30 min the bottom will stabilize around 59-60 and the top 67-68.
nvidia,3deogw,Luckyduck1337,1 point,Wed Jul 15 22:09:52 2015 UTC,"My one G1 980 TI does push out a lot of hot air into the case, which is to be expected. With my current OC I'm able to get to a 1500mhz boost clock but I'm at 79-80C with the cards fans set to 100%. I think I could get a bit more performance out of it but I think the heat is becoming an issue. Also the memory can go higher without issue, I was just trying to bring down overall temps. It also took me a long time to get the below stable, different combinations of mvs and core clock increases would change the stability.    Asic quality: 74% Power: +87mv Power Limit: 122% Core Clock Speed: +120 Memory Clock Speed: +400"
nvidia,3deogw,ggwp9999,2,Wed Jul 15 18:34:50 2015 UTC,"My one G1 980 TI does push out a lot of hot air into the case   You should feel these suckers running in SLI. I have great case cooling, so the heat is exhausted into the room at a rapid pace. It's basically a space heater at that point. In the summer!"
nvidia,3deogw,Tylerdurden516,1 point,Wed Jul 15 19:28:00 2015 UTC,"Haha! I only wish I could have the same problem. I may pick up another when the prices drop a bit.    I will say though the single G1 is handling 4k like a champ. I really am extremely happy here, I just can't help but want to dial in the performance to my cards max."
nvidia,3deogw,bisjac,1 point,Wed Jul 15 20:02:07 2015 UTC,"I just can't help but want to dial in the performance to my cards max.   That's definitely fun. A custom bios will help. I was able to squeeze 60mhz more on the core clock witha custom bios when I had a single 980TI (The EVGA SC+). But due to availability issues, I couldn't get a 2nd one of those so I opted for 2 Gigabytes for SLI instead.  But having a single card makes a custom bios easier, and allows you to push the clocks to the max whereas in SLI you need to find common ground between the two cards."
nvidia,3deogw,Tylerdurden516,1 point,Wed Jul 15 20:14:37 2015 UTC,"Yeah, a custom bios is something I've considered but I'm still on the fence about. I'll look into it though for sure, thanks for the suggestion!"
nvidia,3deogw,ggwp9999,1 point,Wed Jul 15 21:02:39 2015 UTC,For benchmarking I have run them both at way different speeds with no issues. My one 980 can do +296. The other one only does 246 :( it lost the lottery there lol.   I have great cooling but I'm still worried about the 980 ti dumping way more heat than my 2 980's. Which is why I went reference on the bottom and will probably pick up a G1 for the top since everyone has been getting great results on here with them. Plus I can run both my monitors without an adapter.
nvidia,3deogw,Aerundel,1 point,Wed Jul 15 22:04:03 2015 UTC,Can you sli a reference and a non reference card? Would that limit the clock speed on the non reference model?
nvidia,3deogw,Syn246,1 point,Wed Jul 15 22:26:16 2015 UTC,They are still 980ti's so it shouldn't be an issue. And sli clocks don't have to be the same. But I do anyways.
nvidia,3deogw,terencecah,1 point,Wed Jul 15 23:33:21 2015 UTC,"With a single 980ti i was getting 72 degrees at full load, exactly what i had seen on other review sites. I ordered another 980ti, as well as an ekwb ff5 vardar 3000 rpm 120 mm fan ($20) which i put into the side vent on my case that blows directly on the gpu's. At full blast, my top card now peaks at 69 degrees and the bottom card at 66 degrees. That is without playing with the gpu fan speed, just the ekwb fan is at full speed. If you have the side vent, i highly recommend doing this."
nvidia,3deogw,MentalMike72,1 point,Wed Jul 15 20:37:26 2015 UTC,"where did you even get another g1 x.x i got lucky on newegg, and now cant even find them in stock anywhere."
nvidia,3dfr30,supercow_,3,Wed Jul 15 22:37:15 2015 UTC,If you're OK with spending the money it is absolutely worth it. I went from a 980 to a 980Ti through step up and I do not regret the ~$110
nvidia,3dfr30,ingo2020,1 point,Thu Jul 16 01:18:46 2015 UTC,"OK cool, that is encouraging. I'll be sad to lose my reference cooler design, but it will only be like $80 to step-up. Do you like the ACX 2.0 fans OK?"
nvidia,3dfr30,ingo2020,2,Thu Jul 16 01:30:31 2015 UTC,"They're alright. My AC unit blocks out their sound so I'm always hearing fans all the time anyway. When my AC unit is off, they're pretty quiet. Sometimes they aren't even on if the GPU is below 60c (which is normal)."
nvidia,3dfr30,kmarsara,0,Thu Jul 16 01:55:09 2015 UTC,"its a high pitch whine sound for the acx once it gets pass 50% fan speed, and to be honest i have 980ti sli, i like the sound profiles of the reference over the acx"
nvidia,3dfr30,ingo2020,3,Thu Jul 16 02:03:23 2015 UTC,You may have a faulty unit if it's whining..
nvidia,3dfr30,FunktasticLucky,1 point,Thu Jul 16 02:17:07 2015 UTC,Yeah. I have 2 980's with the ACX coolers and they can get loud AF. But the fans spool up to over 4300rpm. It sounds like a jet engine lol.
nvidia,3dfr30,Popingheads,1 point,Thu Jul 16 20:57:31 2015 UTC,"I don't personally own the computer but my brother is running EVGA SC 980 Ti's. Under a full gaming load (specifically Metro LL) the bottom card hits about 72-73 degrees and the top one hits about 83. That is with the default fan profile by the way.  Case cooling is one 140mm rear fan (on a radiator), two 120mm top fans and two 140mm intake fans at the bottom/front. It is set up to have a positive case pressure.  So the temperatures aren't the best however with the stock blower you will still be hitting the low 80's on an overclocked card anyway. I guess having the blowers would be nice for keeping the CPU cooler, but to be fair the CPU doesn't get over the mid 50's anyway."
nvidia,3dfr30,slykrysis,2,Thu Jul 16 08:05:48 2015 UTC,"Yes. I got a 970 for $350, found it wasn't good enough of a leap for me from a 670, and just went straight to the Ti. Not looking back...  Pros: faster card for price/performance increase is amazing, cooler, quieter, longevity, more VRAM  Cons: out of pocket expense for increase... that's it"
nvidia,3dfr30,cadgers,1 point,Wed Jul 15 22:51:18 2015 UTC,"Yeah the only major thing I'd be giving up is the reference design, which I prefer. But I guess if it is a 30% increase it is worth it."
nvidia,3dfr30,dng25,1 point,Thu Jul 16 00:58:19 2015 UTC,If you can afford it definitely take advantage of the step-up.
nvidia,3dfr30,schwat,1 point,Thu Jul 16 00:19:17 2015 UTC,"If its ~$100, I would step up.  I stepped up from 980 classified."
nvidia,3dfr30,andrew5161,1 point,Thu Jul 16 10:53:36 2015 UTC,"It's a hell of an upgrade. I stepped up from a GTX 980 FTW (bit faster than an SC) & the difference is impressive.   980 FTW @ 1279MHz core vs 980 Ti @ 1250 Mhz core: http://www.3dmark.com/compare/fs/4450166/fs/5413053  Also I had 770s with ACX coolers in SLI and the top card runs about 10-11C hotter than the bottom card. Reference coolers may do a bit better but I always hated how loud the blower fan is compared to the dual fan designs. This 980 Ti isn't terribly loud, fans around 70% at max load & never seen it get above 72C."
nvidia,3ddg6i,chopfab,6,Wed Jul 15 12:13:28 2015 UTC,"Only 3 of the heatpipes has a function, it seems."
nvidia,3ddg6i,scarystuff,3,Wed Jul 15 14:05:17 2015 UTC,"Yea..too bad, no copper plate :("
nvidia,3ddg6i,Zinthros,3,Wed Jul 15 14:09:40 2015 UTC,"Yeah ASUS claims direct contact is better ... but all the cards that beat this one on cooling have a plate, so I feel like they're full of it and wanted to cut costs."
nvidia,3ddg6i,SirDPP,1 point,Wed Jul 15 17:28:14 2015 UTC,"Same idea..marketing.. I think MSI is a good solution despite the fact that the card is extra big and could not fit in all cases. Gigabyte is longer but with ""standard"" pcb."
nvidia,3ddg6i,mortiis69,1 point,Wed Jul 15 18:29:57 2015 UTC,I'd imagine while not directly touching the 5th and 1st still transfer heat given they're connected to the others and are at most a few mm from direct contact if that paste is anything to go by.
nvidia,3ddg6i,Zinthros,1 point,Wed Jul 15 14:13:13 2015 UTC,The build quality should be higher than an Asus reference card right? I'm in debate between sli these or crossfire Fury X.  I think it will come down to which is in stock first.
nvidia,3ddg6i,luftwaffles_,1 point,Wed Jul 15 16:01:54 2015 UTC,"I don't know, I think that reference quality is on a very good level. If you check the Matrix ok, maybe is better but Strix Gaming..I'm not sure. If you check cooler for example, there are few heatpipes not used in a proper way since is missing a copper plate. What about MSI?"
nvidia,3ddnh1,killerkid745,5,Wed Jul 15 13:25:50 2015 UTC,Sounds like you are CPU limited.  Increasing resolution without a drop in performance is a pretty good indicator that your CPU is holding you back.
nvidia,3ddnh1,pj530i,1 point,Wed Jul 15 13:53:37 2015 UTC,"Stemming from this possible solution, what are the specs. Could be a bottleneck somewhere? Might help to better understand your set-up."
nvidia,3ddnh1,H_E_X,1 point,Wed Jul 15 18:05:52 2015 UTC,His flair says i7 3770
nvidia,3ddnh1,gweedo767,1 point,Wed Jul 15 18:26:50 2015 UTC,I just noticed his flair says i7 3770 which definitely should not be a bottle neck.  I'd still be interested to see what the CPU usage is in these games
nvidia,3ddnh1,pj530i,2,Wed Jul 15 18:28:14 2015 UTC,What's your CPU usage in Borderlands when it's only giving you 20-30% on your GPU?
nvidia,3ddnh1,kraM1t,2,Wed Jul 15 16:48:20 2015 UTC,"Re-ran fire strike normal: http://www.3dmark.com/3dm/7773520  Got around 50-60% gpu usage and 30-40% cpu usage on the first graphical scene, second got 70-80% gpu usage and 30-40% cpu usage.  Physics test made my cpu sit on 85%+ usage.  Final combined test resulted in a measely 18 average fps because my gpu usage was sitting on 50%, cpu sitting on 35-50%.  Temperatures are fine, GPU maxed out at 72c, CPU maxed out at 71c.  Another note that GPU-Z says its limited by GPU Utilization."
nvidia,3ddnh1,Renalan,2,Thu Jul 16 04:41:46 2015 UTC,There's a thread on the nvidia forums that is 100s of pages long on this.  Everyone jumps at suggesting a CPU bottleneck first and it's annoying as shit.
nvidia,3ddnh1,takatori,1 point,Wed Jul 15 22:12:37 2015 UTC,Link to thread?
nvidia,3ddnh1,FlyingChainsaw,1 point,Wed Jul 15 22:19:59 2015 UTC,"Yeah I just came here after noticing PAYDAY 2 of all games dropping down to 50FPS with GPU usage at 20% or so - I normally run it at a capped 130, and did so even before I got the card. Definitely not a CPU bottleneck."
nvidia,3ddnh1,Syliss1,1 point,Thu Jul 16 23:48:56 2015 UTC,Sounds like you could have a CPU bottleneck.
nvidia,3ddnh1,Machinebummer,1 point,Wed Jul 15 19:43:33 2015 UTC,"I have the same card and was wondering the same as I've been having a few problems. Although I'm sure I am being bottle necked by my i7 2600 and 16gb ddr3 RAM  I just tried out 4 games in this order:   The Witcher 3 GTA 5 Batman Arkham Knight Hitman Absolution  http://i.imgur.com/8w1wYmn.jpg?1  I think you can see the differences in the graph above, not sure what I should be expecting though to be honest."
nvidia,3dgdxh,mcdiddles69,2,Thu Jul 16 01:42:36 2015 UTC,"Definitely go with 4GB version, it's just +20$. 2GB probably won't be enough for smooth gameplay (well, depends what texture setting you like to use, but we all know the higher the better because it has high impact on graphics quality). With 4GB version you will be fine. You can also overclock the card."
nvidia,3dgdxh,stalkiii,1 point,Thu Jul 16 01:50:05 2015 UTC,"Go for the higher VRAM. You can't add that at a later date. As for clock speeds, you can always overclock. I think it'd be a poor decision not to pick the 4GB card."
nvidia,3dgdxh,Krooksy,1 point,Thu Jul 16 12:41:37 2015 UTC,"Fallout 4 isn't coming out until October, and Doom not until next year.  Save up between now and then and buy a better card.  (BTW that's exactly why I just upgraded, well that and to play witcher 3)"
nvidia,3df2ai,xTommy2016x,3,Wed Jul 15 19:39:17 2015 UTC,"I dont even have this installed...or Geforce Experience. I did acquire the latest hotfix driver posted recently...hasnt been through WHQL processing, but still...  I still get TDR's...not as frequently, and I can still use a full stable OC."
nvidia,3df2ai,sn1313,1 point,Wed Jul 15 19:42:22 2015 UTC,"im using 350.38, and after this i have had no issues for an entire day so far"
nvidia,3df2ai,1stMora,1 point,Wed Jul 15 19:52:00 2015 UTC,"I don't even have this installed. I only ever install the main driver, PhysX and geforce experience because of shadowplay."
nvidia,3df2ai,facegas,1 point,Wed Jul 15 20:56:06 2015 UTC,I heard shadow play causes issues too
nvidia,3df2ai,facegas,1 point,Wed Jul 15 22:08:47 2015 UTC,Thanks I'll give it a try.
nvidia,3df2ai,barber107,1 point,Wed Jul 15 22:40:28 2015 UTC,Did it help?
nvidia,3df2ai,XXLpeanuts,1 point,Thu Jul 16 03:39:51 2015 UTC,Can't say it did. Worth the try though.
nvidia,3dg5cv,HeyOmni,1 point,Thu Jul 16 00:30:45 2015 UTC,i just upgraded from mine and bought a used 780. i also upgraded my monitor to a 1440p one. Its nice but even that big a jump was not as big in performance as I was expecting. If your at 1080p get a 770 for cheap and ride out til next gen.
nvidia,3dfayz,frostfire2k,4,Wed Jul 15 20:39:04 2015 UTC,"In witcher 3, what is the resolution set at? DSR isn't like AA, it enables higher resolutions for games to use and then intelligently filters and samples it down to your native resolution. Having DSR on doesn't change anything unless you're set to those higher resolutions."
nvidia,3dfayz,SirCrest_YT,3,Wed Jul 15 20:45:07 2015 UTC,"So if i enable the newly allowed resolution within the game after turning up DSR, then ill see the new resolution? I knew it always sortof felt like it wasn't changing much haha"
nvidia,3dfayz,SirCrest_YT,7,Wed Jul 15 20:49:22 2015 UTC,"In the NCP, Enabling DSR adds extra resolutions over the native resolution. So for example, you're on a 1080p monitor, you enable 4.00x DSR, you then go into Witcher 3 and select 3840x2160, you are now using DSR. DSR when enabled is simply adding extra resolutions, nothing changes if they aren't selected in the game.  If you have your game still at 1080p, DSR isn't being used, hence probably getting some ridiculous FPS, and also once you see 4x DSR you will instantly know the difference. besides performance Witcher 3 looks so damn good with downsampling."
nvidia,3dfayz,SirCrest_YT,3,Wed Jul 15 20:52:17 2015 UTC,"I turned it up to 1440p and selected it in-game. MY frames went down to the 60-70 range. This is good buuuut.  Strangely, after playing it at over 100 frames this whole time, 60 frames doesnt even look that smooth anymore. WTF! 60 frames used to look super clean to me and now that i've been playing everything at over 100 frames it looks only sub par. Yikes, maybe i will stay at 1080p after all"
nvidia,3dfayz,SirCrest_YT,2,Wed Jul 15 21:06:39 2015 UTC,"You got hairworks off, right?"
nvidia,3dfayz,SirCrest_YT,2,Wed Jul 15 21:09:28 2015 UTC,"Ya its off. Another thing i'd like to point out is how DSR seems to cause a very difficult to describe ""wave"" on textures, i'm mostly seeing this while moving around and looking at large textures i.e. a big roof (all one color). It's very distracting and imo this renders DSR totally useless. But it does give me a good sense of how it would look at 1440p"
nvidia,3dfayz,billyalt,3,Wed Jul 15 21:15:59 2015 UTC,"Adjust the gaussian filtering in NCP, perhaps lower the smoothness factor which might contribute to that."
nvidia,3dfayz,kittah,2,Wed Jul 15 21:18:15 2015 UTC,"As it turns out, turning the smoothness factor up got rid of the waves. Thanks for the help man, would have never found out how to use DSR on my own haha"
nvidia,3dfayz,Lobrauski,2,Wed Jul 15 21:23:28 2015 UTC,"Increasing the smoothness basically spreads out the filtering width on the downscaling filter. Probably helps those wave artifacts but makes the image softer.  Should still feel sharp due to the higher resolution in any case, glad it worked out."
nvidia,3dfayz,billjanke,1 point,Wed Jul 15 21:36:10 2015 UTC,"Those waves are known as moire patterns, a side-effect of DSR's downsampling implementation. Smoothness factor is a Gaussian blur shader that is supposed to help reduce moire patterns, but can also blur the image."
nvidia,3dfayz,Syn246,1 point,Thu Jul 16 08:45:33 2015 UTC,By far the best looking DSR settings for 1080p is 4k downsampled since its actually a perfect multiple (2x in each direction) so you don't get that weird crawling on pixels. At 4K->1080p you can set the smoothing factor to 0. Unfortunately 4k kills framerate.  At any factor below 4x you really need to use that smoothing filter to eliminate the crawl. 15-17% seems to work well enough for 1440p->1080p
nvidia,3dfayz,HawkEye0,1 point,Thu Jul 16 23:46:50 2015 UTC,This was my experience too with the FPS it just didn't seem as smooth anymore. Occasionally it'll drop into the 40's at 1440p and that is just unpleasant.
nvidia,3dg1gw,Ohnae,2,Wed Jul 15 23:59:31 2015 UTC,490fps? Who has time for that shit?
nvidia,3dg1gw,wanderjahr,2,Thu Jul 16 00:55:22 2015 UTC,"Try a real test before jumping to conclusions. 3dmark, unigine, in-game benchmarks."
nvidia,3dg1gw,Poopcoveredmonkey,1 point,Thu Jul 16 01:37:57 2015 UTC,"Haha!  But wait!  We need to get benchmarks performances in to 400-500fps where no current monitor is able to even utilize it since its capped at 144hz not to mention the crazy screen tear that I could imagine while playing the game that I'm guess this is for..CS:Go?  But hey, I guess we can all hope to shoot for 1000 fps for our games one day!"
nvidia,3dg1gw,billjanke,2,Fri Jul 17 05:08:42 2015 UTC,shitpost
nvidia,3dg1gw,hkgrx8,1 point,Thu Jul 16 03:13:35 2015 UTC,Did you uninstall the driver for the 970 and then run DDU?
nvidia,3dg1gw,hdshatter,1 point,Thu Jul 16 00:04:55 2015 UTC,"Yes, I was having problems with my 980 Ti earlier so I uninstalled Geforce Experience, the drivers, and everything related to my graphics card and reinstalled it with a 353.38 driver. And no, I didn't run DDU."
nvidia,3dg1gw,hdshatter,1 point,Thu Jul 16 00:11:42 2015 UTC,Do that again but this time use DDU.  Also what is your PSU?
nvidia,3dg1gw,hdshatter,1 point,Thu Jul 16 00:15:17 2015 UTC,Which DDU would you recommend? And I am currently using 750 watt PSU.
nvidia,3dg1gw,hdshatter,1 point,Thu Jul 16 00:17:51 2015 UTC,"The latest version of this. http://www.guru3d.com/files-details/display-driver-uninstaller-download.html  Run it in safe mode after you remove the drivers with Windows unistaller, personally I use Revo Unistaller/Advanced Uninstaller but DDU should also delete anything those 2 programs would find."
nvidia,3dg1gw,andrew5161,1 point,Thu Jul 16 00:21:08 2015 UTC,"Gotcha. Also when I install my drivers, do I choose the option to install a clean version of my graphics card after I choose custom installation?"
nvidia,3dg1gw,andrew5161,1 point,Thu Jul 16 00:26:53 2015 UTC,"I don't think that does anything if you do a clean install of the driver, I always select it anyway."
nvidia,3dg1gw,andrew5161,1 point,Thu Jul 16 00:29:22 2015 UTC,"I just used DDU in safe mode, reinstalled the graphics card and get the same performance as before."
nvidia,3dg1gw,stalkiii,1 point,Thu Jul 16 00:57:18 2015 UTC,"It probably has something to do with the game you are playing, either of those fps are far higher than you need. If you put in a game that's even a tiny bit demanding you will see the difference in performance."
nvidia,3dg1gw,BKachur,1 point,Thu Jul 16 00:33:28 2015 UTC,"I agree but the issue is that my 970 shouldn't be performing better than my 980 Ti at all. The 980 Ti should be performing substantially better than the 970, but in this case, it's the total opposite."
nvidia,3dg1gw,BKachur,2,Thu Jul 16 00:59:22 2015 UTC,"There is a point at which we can only theorize fps is capped in firmware to limit coil wine. My r9 270x 980 ti both get 400-500 fps in league. I assume there is a hard limit to keep it from going into the thousands and creating coil whine. Like i said before, get a real game that requires the 980 to even begin to stretch its legs and you will see the diff."
nvidia,3dgo3i,sn1313,3,Thu Jul 16 03:09:33 2015 UTC,The 980ti has more cuda cores and is a different chip than a 980 classified. A 980ti will beat the 980 classified in every single category relating to performance even if the 980 is clocked higher.
nvidia,3dgo3i,BKachur,0,Thu Jul 16 03:32:16 2015 UTC,Good to know.
nvidia,3dgo3i,hdshatter,3,Thu Jul 16 03:33:23 2015 UTC,If you bought it in the last 90 days you can step up to a 980ti.
nvidia,3dgo3i,Lanessar,1 point,Thu Jul 16 08:42:16 2015 UTC,"Why was this downvoted? I just moved to a 980 TI using EVGA's step-up program, saved loads of cash."
nvidia,3dgo3i,Vinnyk84,1 point,Thu Jul 16 13:22:40 2015 UTC,"I have the asus swift 1440p 144hz gsync screen running SLI 780s 3gb so if you do get a screen like mine, then ya, 980 ti is your best bet to get your money worth.  My 780s do the trick to get games over 70fps at 1440p that are heavy in graphics or newer. Older games  are alot more easier to get the perks out of it.   Anyways long story short. Get a 980 ti if you want those type of screens. If you ever go 4k, sli it. Simple  as that"
nvidia,3dgo3i,Vinnyk84,0,Thu Jul 16 03:38:08 2015 UTC,"Yeah once I can get a decent 28-32"" monitor that runs the higher fps/hz and 1440p is nice...I just wish they could make a perfect monitor. 1ms 140hz and 4k...but then what card could not only give you those kind of frames and resolution...maybe we are a year or two from it...who knows. Or I'll need SLI regardless."
nvidia,3dgo3i,CaptSkunk,1 point,Thu Jul 16 04:33:16 2015 UTC,Why not if you can spare the cash. But if you plan on just 1080p a normal 980 or even a 970 will do just fine. 980 ti is pretty much for higher resolutions
nvidia,3df1ib,Broccolisha,1 point,Wed Jul 15 19:34:25 2015 UTC,"from what i know,optimus is supposed to function that way.the main gpu is the intergrated intel gfx,with the discrete nvidia gpu being the secondary one.you dont really need to run windows at 4k, with full buffering and filters,as this will increase power usage.so programs that need 3d rendering always start with the geforce gpu as default, and all other software on the intel gpu.trust me, if your laptop uses a gtx970, then its accompany intel gpu would be powerful enough to handle all other application at the same level.a normal 16:9 laptop screen shouldnt run at 4k on default.windows 10 wont change it ."
nvidia,3ddc2e,Stargenx,5,Wed Jul 15 11:23:04 2015 UTC,"No card is going to be a sure bet for 1500 at any voltage.  I have 2 G1's but I'm not going to bother to OC them until EK comes out with blocks.  Left totally at default my cards are boosting themselves to 1314.  I think either one is a good option, if you are going to remove the HSF the copper setup in the Classy is probably a good chunk of what you are paying for."
nvidia,3ddc2e,sluflyer06,1 point,Wed Jul 15 12:56:43 2015 UTC,What does HSF stand for?
nvidia,3ddc2e,ConsolePeasantNoMore,5,Wed Jul 15 14:18:27 2015 UTC,HeatSink & Fan
nvidia,3ddc2e,SirCrest_YT,3,Wed Jul 15 14:32:21 2015 UTC,Heatsink Fan
nvidia,3ddc2e,sluflyer06,1 point,Wed Jul 15 18:24:28 2015 UTC,Do you have any coil whine on your cards if I may ask?
nvidia,3ddc2e,BaNaNaKING42,1 point,Wed Jul 15 17:13:34 2015 UTC,"Not that I've noticed running up to 120fps, I haven't had a ton of time of them yet though. I've played Witcher 3, Project Cars, World of Tanks, no whine on those games. I haven't run through benches yet but I'd assume there will be whine on high uncapped framerates but that is normal."
nvidia,3ddc2e,sluflyer06,1 point,Wed Jul 15 18:24:15 2015 UTC,ok thanks. sounds reasonable
nvidia,3ddc2e,BaNaNaKING42,1 point,Wed Jul 15 18:28:26 2015 UTC,"On my G1, I only got coil whine when I was working on my cards OC. The fans were running at 100% and as I was dialing in the OC it would whine and make a bit of a grinding noise, also classified as coil whine I believe, right before the drivers would crash and restart.  Other than that, no coil whine under normal use.  For reference I got my card stable at 1520 but I dropped it to an even 1500 to bring down the thermals. Still working on the OC though so hoping I can get a bit higher. Still very happy with the results!"
nvidia,3ddc2e,Luckyduck1337,1 point,Wed Jul 15 20:05:34 2015 UTC,1500 is pretty great. What temps are you getting at which fan setting and how loud is the card under load (gaming).
nvidia,3ddc2e,BaNaNaKING42,1 point,Wed Jul 15 20:10:58 2015 UTC,"My temps are currently higher than I'd like, 78-79C, and my fans are at 98%-100%. I think part of the issue is airflow in my case though, I made a seperate r/nvidia thread about this. I removed the side of my case and that dropped my temps by 4-5C, when overclocked. Also my apartment was pretty warm, ~26-27C, that day as I had just gotten home from work and the AC wasn't on.  Once I get my new fans installed and all I'll see where my thermals land."
nvidia,3ddc2e,Luckyduck1337,4,Wed Jul 15 21:01:22 2015 UTC,"On paper the classified is better, but the G1 cooler is better than the ACX 2.0+. There are no reviews for the classified, I don't think it even has a release date yet."
nvidia,3ddc2e,hdshatter,3,Wed Jul 15 11:34:52 2015 UTC,It was in stock for 2 hours at Newegg yesterday.
nvidia,3ddc2e,DrexelDragon93,2,Wed Jul 15 12:16:35 2015 UTC,"That's actually a long period of time for Classifieds. The first time they went on sale, they were up for about 45 minutes."
nvidia,3ddc2e,jim2point0,5,Wed Jul 15 14:38:48 2015 UTC,"My thoughts are if you are running a custom loop the classified is probably the safest bet for high clocks.  If you want the highest clocks with on air - the G1 is probably going to do that better.  However, EK is supposedly coming out with a block for the G1, and at voltages most everyone uses it will probably come down to the silicon lottery.  If you are going to go crazy and go over 1.3v with the card - then the classified is the only one that can probably do that safely, however KINGPIN himself has apparently said that there isn't much to be had with these cards past 1.28v"
nvidia,3ddc2e,attomsk,3,Wed Jul 15 16:48:51 2015 UTC,"Silicon lottery, no card can guarantee 1500mhz, Im surprised my reference can hit 1507mhz to be exact."
nvidia,3dcuqn,MRChuckNorris,6,Wed Jul 15 07:22:56 2015 UTC,"Hmm I have logitech gaming software and if this is true, thats a huge issue for me :/ How else can I get the surround sound feature of the G930s?  EDIT: So I tested this using 3D Mark and the score variance between, using Logitech gaming software, simply not have it running and fully uninstalling it is minor for me( small enough to count as percentage of error). This was done at stock speeds and not my OC speeds."
nvidia,3dcuqn,covertskippy55,1 point,Wed Jul 15 09:50:25 2015 UTC,"The Logitech software actually causes my 930s to reconnect and disconnect continuously. I have to disable it in order for them to work at all.   I've updated and uninstalled it multiple times. None of it helps. (Win 8.1, 560ti)"
nvidia,3dcuqn,YouShouldKnowThis1,3,Wed Jul 15 12:13:44 2015 UTC,"I posted this a while back for some people when a sale in /r/buildapcsales was going on for the 930.  PEOPLE WITH DISCONNECTING ISSUES: On the last sale of these myself and 3 other friends bought them. We all had disconnecting issues and now we don't.  There are a few major things that will cause these to disconnect randomly.   Cell phone near (~4ft) next to the receiver. Routers that use channel 6 that are within ~30 ft of the headset. Change your routers channel and this will probably go away. (worked for 2/2 of us who were still having problems) Multiple G930s in close proximity to each other. I play games with my girlfriend across the room ~10 ft away all the time with no problems, but when one of my other friends comes over and sits on the desk between us with his set, we all get disconnects (not often though).   I hope this helps. This is a fantastic headset and for this price you really can't beat it. If you live in a close quarters apartment complex though you may have issues with other people's routers. Otherwise, fantastic for the price. Sounds amazing and feels amazing, and has voice modulation."
nvidia,3dcuqn,Cryptophagist,1 point,Wed Jul 15 14:43:09 2015 UTC,I only fit the first criteria but the problem only started recently. I think it's a software issue.
nvidia,3dcuqn,YouShouldKnowThis1,1 point,Wed Jul 15 17:52:12 2015 UTC,"It's not, because I've gotten it to stop 3 times on 3 different setups using these methods. Not changing anything software related. It's something to do with the receiver getting signal jammed somehow. There's mostly likely some type of wireless in your house interfering with it. Which is pretty shitty regardless if you ask me. Maybe they should have used a different wireless method."
nvidia,3dcuqn,Cryptophagist,-1,Wed Jul 15 18:24:34 2015 UTC,"The solution to surround sound is to actually use a surround sound system, not stereo headphones. :P  I have the same headset btw, love it, don't run the software though."
nvidia,3dcuqn,sluflyer06,9,Wed Jul 15 12:47:41 2015 UTC,"My guess is Razer, as i am using 980ti and logitech gaming software and have no problems."
nvidia,3dcuqn,nbiscuitz,4,Wed Jul 15 12:44:34 2015 UTC,"I killed Razer Synapse last night, if it's running GPU usage on 1 of my cards never drops below 35-40% on the desktop."
nvidia,3dcuqn,sluflyer06,3,Wed Jul 15 12:50:24 2015 UTC,"How would the razer software require so much GPU horsepower? If anything was wrong with the program, would it not use CPU horsepower instead?"
nvidia,3dcuqn,Bigkefjee,0,Wed Jul 15 14:08:38 2015 UTC,I don't think it requires it so much as causes a conflict.
nvidia,3dcuqn,sluflyer06,0,Wed Jul 15 15:27:58 2015 UTC,"It wouldn't require any measurable # of cycles, its just bad coding/some kind of bug."
nvidia,3dcuqn,attomsk,1 point,Wed Jul 15 18:25:29 2015 UTC,Yeah I have logitech and corsair stuff on my machine and they don't interfere with anything so far.
nvidia,3dcuqn,hdshatter,4,Wed Jul 15 16:58:08 2015 UTC,"I hate Razer Synapse and all their other software. It's junk, when I had a Razer mouse it would randomly start using like 20% of my CPU for no reason and I had to kill the process.  Another program to unistall is Samsung Magician if you have a Samsung SSD. Download it, check for firmware updates enable whatever you want to use and then Uninstall it forever."
nvidia,3dcuqn,Supernormalguy,1 point,Wed Jul 15 09:17:33 2015 UTC,Haven't heard anything bad about Samsung Magician... care to shed some details?
nvidia,3dcuqn,MegafunkSA,1 point,Wed Jul 15 15:46:01 2015 UTC,Hopefully this helps my subpar 970 performance...
nvidia,3dcuqn,killerkid745,1 point,Wed Jul 15 09:41:55 2015 UTC,"Welp, signed up finally since i've been scouring this subreddit every so often.  Didn't really do much for me removing Logitech Gaming Software, problem with my card is low gpu utilization. (Power set to Pref Max)  On Witcher 3 the game runs flawlessly, as it should with any 980ti.  However, on pretty much every other game, it likes to push out like no gpu usage, ending up with less fps.  An example would be Borderlands 1, Usage sits around 20-30% with 60-90FPS, this is annoying since I play on 144hz. Another example is Far Cry 4. Game pretty much sits on 70-75% usage and results in like 70-90FPS constant. The above happens with every game the card is meant to absolutely destroy.  Specs are i7 3770 & MSI 980 TI Gaming.  Forgot to mention that even on Fire Strike (normal) the usage stays low, so i'm getting a shit score from it. When I up it to the 1440p variant, usage goes up but it stays around the same fps as the other test because its like trying to perform consistently or something.  Update:  Did another test on normal Fire Strike. In the first graphical test my usage was at 45%, resulting at 48fps during the test... why?"
nvidia,3dcuqn,Davoelkanator,1 point,Wed Jul 15 12:07:16 2015 UTC,I will run Firestrike and let you know my results.
nvidia,3dcuqn,sluflyer06,1 point,Wed Jul 15 15:33:33 2015 UTC,What kind of motherboard do you have?
nvidia,3dcuqn,hofern,-8,Wed Jul 15 18:00:45 2015 UTC,"Your cpu could be a bottleneck, get an i5 Edit : whoops I thought he said he had an i3"
nvidia,3dcuqn,goodpricefriedrice,7,Wed Jul 15 12:41:45 2015 UTC,your solution is that he should downgrade?
nvidia,3dcuqn,Bierdusche,2,Wed Jul 15 12:49:12 2015 UTC,Rofl
nvidia,3dcuqn,JoeLithium,1 point,Wed Jul 15 13:17:29 2015 UTC,"Hmmm, I have Logitech, razer and corsair mouse/keyboard software installed. Don't notice any performance hits"
nvidia,3dcuqn,JoeLithium,1 point,Wed Jul 15 23:49:32 2015 UTC,I haven't been brave enough to reinstall the software yet but i now see that flashing my bios has made it work.  I lost my audio in the process somehow but....
nvidia,3dcuqn,TheHolyCabbage,3,Thu Jul 16 00:56:53 2015 UTC,"He probably tried different stuff, Not just that. Your comment is a joke. Looking at your profile: you seem to have quite a salty attitude, u should check your blood pressure"
nvidia,3dcuqn,Valhalla_I_Am_Coming,2,Wed Jul 15 11:43:44 2015 UTC,Sometimes troubleshooting takes a long time. The answer isn't always right there in front of you.
nvidia,3dcuqn,foxtrot1_1,2,Wed Jul 15 14:18:09 2015 UTC,You obviously don't do IT as a profession.
nvidia,3dcuqn,MediocreMango,1 point,Wed Jul 15 14:22:12 2015 UTC,Welcome to the salty splatoon. How tough are ya?
nvidia,3dcuqn,foxtrot1_1,1 point,Wed Jul 15 14:39:05 2015 UTC,I had a bowl of nails this morning..... without any milk.
nvidia,3debwi,Terra_Nautilus,2,Wed Jul 15 16:36:53 2015 UTC,"depends on what u play, if you play bf4 you can have the setting almost on ultra with playable 40 to 60"
nvidia,3debwi,kmarsara,1 point,Wed Jul 15 17:24:22 2015 UTC,You'll be fine at medium-high settings and no aa with most games.
nvidia,3debwi,floydian32,1 point,Wed Jul 15 17:51:06 2015 UTC,"It should work but expect to play with settings fairly extensively for each game to get decent FPS, obviously G-Sync will help alot.  You may also run out of texture memory in some games, I've got some games that burned up almost all 3GB on my 780 at 1080p at times.  Don't forget ASUS is coming out with a competing model for the Acer XB270HU although it will likely be the same panel."
nvidia,3debwi,sluflyer06,1 point,Wed Jul 15 19:01:37 2015 UTC,"First, 1440p IPS G-Sync is the absolute best choice assuming it's within your price range. You didn't specifically mention it, but toss in 144hz as well and that's the Acer XB270HU which you've likely already seen and/or considered. You'll thank yourself later for not settling on 1080p.  Regarding your GPU, have you thought about selling it and putting the money towards a 980 Ti? You can always sell the 980 Ti when Pascal comes out, and this might not be for another 12-18 months.  Another option would be to add a second 780 Ti for SLI--I imagine they're well into the mid-low $300s second-hand, if not lower. You'll be stuck with 3GB VRAM going this route though."
nvidia,3debwi,Syn246,1 point,Thu Jul 16 02:49:11 2015 UTC,"Definitely want 144hz as well.  I live in Australia, so prices are higher here. I bought the 780 Ti in July last year for $840 AUD. I'm guessing I could sell it second hand for around 3/8 of the original price?"
nvidia,3deqle,ggwp9999,2,Wed Jul 15 18:19:21 2015 UTC,The temperatures are normal. If you were seeing for example 70c and 75c with gtx 970 in sli with acx+ it's only normal if you see a lot higher with 2 980 ti with the same cooler. Because 980 ti's are a lot warmer.
nvidia,3deqle,mull54,2,Wed Jul 15 18:25:09 2015 UTC,"Thanks, actually it's not so much the temps but the fan speed, I have to use a user defined fan curve to keep them in those temps which is really loud. There is currently no reviews of classified so I'm not sure if the fans are required to run that fast."
nvidia,3deqle,jim2point0,2,Wed Jul 15 18:29:36 2015 UTC,"With 2 cards in your case, you're restricting airflow to the top card. The non-reference cards use intake fans (not blower fans like reference). Also, because you have a GPU already running hot on the bottom, the air the top card will be sucking in will likely be hot as well. So it's no surprise that the top card heats up WAY more than the bottom card.  The only solution other than a custom fan profile (which will ramp up crazy high) is to have plenty of intake fans in your case, especially a big on the side panel that's situated in front of the GPUS (so the top card has cool air to suck in)."
nvidia,3deqle,mull54,1 point,Wed Jul 15 20:06:27 2015 UTC,I had 2 gtx 970 g1 gaming and the top card had a temp of 60-62c and the bottom card had a temp of 75c(my target temp. So it would throttle after that). So if you have 70-72 and 84c on the second card I think it's seems pretty logical. It is often recommended that you don't use two custom cards if you don't have extremely good case cooling because the top card will dump all of its heat on the bottom one.   Don't worry. 84 is well within safe limits(And Yes With The Acx Cooler You Most Likely Will Have To Run It As Fast ). But lower temps never hurts so you could check out a waterblock for the card/cards or buy a new chassi/fix your case flow.
nvidia,3deqle,sluflyer06,1 point,Wed Jul 15 20:07:06 2015 UTC,"I'm ok with 80ish temps but running 80-90% fan speed worries me more, i guess i will just have to get used to that."
nvidia,3deqle,sluflyer06,2,Wed Jul 15 22:23:07 2015 UTC,"Is this in a game or stresstest/benchmark? Also are you running stock fans or fans of your own choosing?  non-reference SLI setups are extremely problematic for temperature, they just dont' work well as there is too much super hot air blowing into the case.  I'm in the same boat but only until EK releases block."
nvidia,3deqle,ConsolePeasantNoMore,2,Wed Jul 15 18:34:31 2015 UTC,"Yes this is in game, If i run furmark it will shoot up to 85+. I will be getting a new case soon (enthoo luxe) with added fans so hopefully that will help a bit."
nvidia,3deqle,djisadud,1 point,Wed Jul 15 18:38:02 2015 UTC,"try turning your sidefan so that it exhausts, I flipped my 2 120mm side fans and dropped 4C off the cards.  I think I'm gonna pull 2 of my spare 120's out and temporarily run 4 exhaust side fans until water blocks are out."
nvidia,3deqle,potatocucumber,1 point,Wed Jul 15 18:44:52 2015 UTC,"FYI, I have the Luxe. It's a great case, but doesn't have any side fans which seem to be recommended for SLI."
nvidia,3dfkmh,nlnl,2,Wed Jul 15 21:48:13 2015 UTC,"For most games, your i5 should should perform well. OC it a bit and you should be good to go for a long time."
nvidia,3dfkmh,GBT_Van,2,Wed Jul 15 21:52:29 2015 UTC,Great. Thanks for the reply.
nvidia,3dfkmh,BeanBandit420,2,Wed Jul 15 22:00:37 2015 UTC,"i5 3570k is a great quad core CPU, it should be more then enough for the vast majority of games. But here are some charts that can give you more info.  http://www.pcper.com/reviews/Systems/Quad-Core-Gaming-Hardware-Roundup/BioShock-Infinite-and-Civilization-Beyond-Earth  Honestly the only real bottleneck with a modern intel quad core CPU is at frame rates EXCEEDING 144hz, and if you turn up the graphics settings or play at a resolution over 1080p those disappear. You really should not have a CPU bottleneck unless you play games with horrible CPU scaling and optimization. Also you could overclock if you aren't getting 99% GPU usage but I would really be surprised to find out you had a CPU bottleneck at actually hurts your gaming experience. (OH NO I'M ONLY GETTING 165FPS INSTEAD OF 181FPS on my 144hz monitor!)"
nvidia,3dfkmh,penguin_alex,1 point,Wed Jul 15 23:40:01 2015 UTC,i was running a i5 2500k up until yesterday and it ran with the 980 ti completely fine so yours should run perfectly.
nvidia,3dfkmh,SzyjeCzapki,1 point,Thu Jul 16 00:02:32 2015 UTC,"Funny enough, my 2500k at 4.4Ghz bottlenecks my 980ti quite hard in Novigrad in witcher 3."
nvidia,3dfkmh,NoAirBanding,1 point,Thu Jul 16 02:23:46 2015 UTC,Then the 3570K and 4690K would probably struggle as well.
nvidia,3dfkmh,penguin_alex,1 point,Thu Jul 16 07:27:39 2015 UTC,i was playing gta v with it and was managing to get a decent 60fps with pretty much all the settings on max
nvidia,3dfkmh,SzyjeCzapki,1 point,Thu Jul 16 17:56:56 2015 UTC,I get as low as 58% GPU usage in the busiest areas of Novigrad while CPU skyrockets to 99%.
nvidia,3dfkmh,hdshatter,0,Thu Jul 16 18:31:53 2015 UTC,Only if you go SLI.
nvidia,3de2fe,himmatsj,1 point,Wed Jul 15 15:27:10 2015 UTC,"If you enable just the frame rate limiter and nothing else, then only your frame rate will be capped."
nvidia,3de2fe,Wifi-rape,1 point,Thu Jul 16 11:13:14 2015 UTC,"So even though the default settings seem to show that AO is off, amoung other things, it won't take effect?"
nvidia,3de2fe,Wifi-rape,1 point,Thu Jul 16 11:22:18 2015 UTC,"As long as you haven't touched anything besides the frame rate limiter, nothing else will have an effect on your game."
nvidia,3dfe7p,envy0,1 point,Wed Jul 15 21:01:43 2015 UTC,"Of course the specs matter. One of the points of the Shield is to stream games from your more capable GeForce GTX-enabled PC to your less powerful Shield. You can also use Grid streaming to stream from NVIDIA's servers, but I think there's a subscription fee."
nvidia,3dfe7p,TheSLSAMG,1 point,Wed Jul 15 21:04:38 2015 UTC,"So the Shield is not like a powerful console but more of an extra and portable screen... ok, I was just confirming... thanks! And sorry to Nvidia that I haven't ever bought any Nvidia product yet, they're just so expensive :("
nvidia,3dfe7p,TheSLSAMG,1 point,Wed Jul 15 21:10:29 2015 UTC,"The Shield is a pretty powerful little thing. It will play games from it's app store, but if you want to play full PC titles on it you need either a desktop/laptop with a GTX 650 series or better card and the Shield itself or you can do it by using NVIDIA's Grid service. However, I believe Grid is paid."
nvidia,3dfe7p,TheSLSAMG,1 point,Wed Jul 15 21:14:16 2015 UTC,Why don't they just make a Shield-like app for already-built-a-long-time-ago mobile devices like Android and iPhone so they won't have to make a nearly $500 device just for that... just my idea
nvidia,3dfe7p,TheSLSAMG,1 point,Wed Jul 15 21:12:19 2015 UTC,Or you can just buy a laptop for phone-level portable yet PC-level powerful gaming
nvidia,3dfe7p,x_Sligh_x,-2,Wed Jul 15 21:13:44 2015 UTC,"Nvidia Shield is one of the worst products of Nvidia, it's so useless, it's like a mere extra screen and controller and nothing else, it's like God created a $500 literal piece of poop, I can't believe Nvidia did such a thing, I still like that company though, especially their GeForce GTX GPU's, etc. etc. etc. 69 420 hahahalololo"
nvidia,3dfe7p,redlobster222,1 point,Wed Jul 15 21:16:51 2015 UTC,"There are existing apps that do it. iOS can do it jailbroken and Android can do it either rooted or non-rooted with Moonlight, which uses the Shield streaming protocol (however it's unofficial.)"
nvidia,3ddwtv,masterblaster278,6,Wed Jul 15 14:45:23 2015 UTC,"My vote goes for the 980 Ti. It's better to have one powerful card than SLI. Plus, the 6GB of memory will help out with running games at that kind of resolution."
nvidia,3ddwtv,GBT_Van,3,Wed Jul 15 16:03:03 2015 UTC,"plus itll run alot cooler, uses less power, and dont have to deal with sli profiles and issues."
nvidia,3ddwtv,kmarsara,1 point,Wed Jul 15 17:27:34 2015 UTC,"Alright, thank you for your help. :)"
nvidia,3ddwtv,WobbleTheHutt,2,Wed Jul 15 21:43:11 2015 UTC,Just came from dual 780ti cards in SLI to a single reference 980ti. I run a Samsung 4k monitor and game at native resolution. So far in real world seems like the 980ti when oced a bit keeps up with or out performs my dual 780ti SLI setup. I'm sure there are a few cases where the SLI pair would eek out being a bit faster but you would be crippled on vram vs a 980ti.  Get the single card!
nvidia,3ddwtv,grnrrtrr4444,1 point,Thu Jul 16 04:26:22 2015 UTC,Sell 780 ti and go to 980 ti
nvidia,3ddw06,bilc0,2,Wed Jul 15 14:38:52 2015 UTC,I'd go for 980 Ti's to really get a performance increase.
nvidia,3ddw06,DrexelDragon93,1 point,Wed Jul 15 15:29:13 2015 UTC,"The 980s will use considerably less power, so if you want to spend the money go for it."
nvidia,3ddw06,Alarchy,1 point,Wed Jul 15 14:45:26 2015 UTC,Well deal is over.  I think I'll wait to see if the 980 ti drops in price at all.
nvidia,3ddw06,BKachur,1 point,Wed Jul 15 16:03:32 2015 UTC,"I think it was a good move on your part. The upgrade would have been marginal at best. The Titan is by no means a bad card, and the gain in performance would not have been worth the extra cost + hassle of selling your old cards."
nvidia,3ddw06,Prefix-NA,1 point,Thu Jul 16 03:58:30 2015 UTC,"The 980's won't be that much stronger. Also the performance nerfs on Kepler is slightly exagerated. Most of it comes from the driver ""enhancements"" on Maxwel where they lower the game quality on Maxwell to outperform in benchmarks.    https://www.reddit.com/r/pcmasterrace/comments/3c2kiw/bug_or_nv_cheating_with_optimized_image_quality/  So going to Maxwell is also turning your games down (you can fix in Nvidia CP but by default its lower) to look better."
nvidia,3ddqtz,chocwaf,2,Wed Jul 15 13:56:30 2015 UTC,Everyone hates Geforce experience so much and I don't really get it. It's an easy way to keep my drivers current and a nice fps counter.
nvidia,3ddqtz,aquaknox,1 point,Wed Jul 15 20:05:23 2015 UTC,"I'm having 0 issues with it for recording, the video isn't as good as if I use DxTory but it also uses about 95% less space for the video. I beleive you also don't get a choice with the codec it uses (I haven't found an option for changing it yet and I've been using it since it came out).   If you're recording for professional play or want the crispest clips use another (albeit heavier) capture software.   If you're using it for 1080p youtube and need to save disk space use Shadowplay, the ability to buffer up to 20 minutes of gameplay is a godsend."
nvidia,3dei8b,Ohnae,2,Wed Jul 15 17:22:33 2015 UTC,"Alright, so 2 things.   Fans:  Most 980 Tis have a silent mode feature where the fans won't turn on until the board reaches 60C by default. So just because your fans aren't spinning doesn't mean something is wrong. As long as they turn on when the card gets hot your card is fine.  If you aren't happy with idle temps you can use something like MSI Afterburner to set a custom fan profile so the fans never turn off. But the cards are designed to be OK below 92C & throttle around 80C so never will get that hot.  Crashes:   The latest WHQL (official release) driver 353.30 has the same issue that has persisted since 352.86 where you get driver crashes with google chrome & in some games. To fix this you need to install the 353.38 hotfix driver that was released to specifically address the chrome crashes. You can find that driver here:  http://nvidia.custhelp.com/app/answers/detail/a_id/3694/~/geforce-hotfix-driver-353.38"
nvidia,3dei8b,kittah,1 point,Wed Jul 15 17:34:37 2015 UTC,"Thanks for the speedy response.   Fans :  I just measured my GPU's temp and it's currently 46C and the fans are still not running.  As for the lights, only the ""Silent"" and ""Stop"" buttons have LED lights but not the ""Windforce"".   Crashes :  While installing the driver 353.30, my Google Chrome crashed after it finished installing."
nvidia,3dei8b,pj530i,3,Wed Jul 15 17:52:09 2015 UTC,The fans probably won't turn on until the temps reach 50C.  The Silent and Stop lights are lit up because the fans are stopped and the card is in silent mode.
nvidia,3dei8b,kittah,1 point,Wed Jul 15 18:00:45 2015 UTC,"You are actually right @pj530i! Thanks for @kittah for introducing me to the Afterburner program, I turned my fan speed % on and the lights turned for ""Windforce"" turned on and my fans were running. However, what would you recommend that I should turn my fan speed % to? Auto or a certain %?"
nvidia,3ddj8e,insarius,3,Wed Jul 15 12:44:40 2015 UTC,Is it a big upgrade for me to go from a 7950 to a 970 Strix?   http://anandtech.com/bench/product/1033?vs=1355
nvidia,3ddj8e,Leonick91,2,Wed Jul 15 14:04:55 2015 UTC,"If I were you I would wait to see how your current card handles at 1440p and to see how it performs with the newer APIs (Windows 10 launches on July 29th).  The gap between the 970 and the 7950 isn't massive but could be filled by another cards between in the future.  If the performance increase is worth the cost to you then go for it, after you are sure. Bare in mind that multi-GPU support should be far better in DX12 so XFire may be a good idea (as far as I know DX12 allows any GPU combo). Also take a look at the 390, most would argue it is better than the 970 for the same price, but go for whatever, if you choose to buy one. Hope it helps!"
nvidia,3ddj8e,Randomness6894,2,Wed Jul 15 18:37:11 2015 UTC,"Not going 1440p, only 144hz  EDIT: Already running Win10, but yeah, no games run DX12 yet haha."
nvidia,3ddj8e,Anarchyz11,2,Wed Jul 15 19:19:39 2015 UTC,"If you have a 7950, it would probably be more economical to just crossfire another 7950 (or R9 280).  The 970 is a decent jump.  But not huge."
nvidia,3ddj8e,Anarchyz11,1 point,Wed Jul 15 14:19:34 2015 UTC,Thanks!  I'll wait and see what the next cards will do or if I can find a 2nd hand for a good price. :)
nvidia,3ddj8e,Anarchyz11,1 point,Wed Jul 15 17:32:01 2015 UTC,"/r/Hardwareswap has them everyday, usually around $90"
nvidia,3ddj8e,GreyJersey,1 point,Wed Jul 15 18:10:27 2015 UTC,"I'm not american, that limits that sub heavily."
nvidia,3ddj8e,floydian32,2,Wed Jul 15 18:12:04 2015 UTC,What country?  There are country specific subs.  You can also crossfire a 7950 with an R9 280.
nvidia,3ddj8e,Anarchyz11,1 point,Wed Jul 15 18:13:53 2015 UTC,The Netherlands!
nvidia,3de896,Cereaza,6,Wed Jul 15 16:10:37 2015 UTC,If i was getting a reference card I would get an evga because of their great track record with their customer service and warranty.
nvidia,3de896,attomsk,3,Wed Jul 15 17:00:02 2015 UTC,EVGA is typically considered top-tier with Asus and Gigabyte coming close behind. I've had Asus and Gigabyte in the past and both cards have lasted years and I was able to mess with the oc quite a bit.
nvidia,3de896,wanderjahr,2,Wed Jul 15 17:54:39 2015 UTC,I have heard horror stories with Asus graphics cards.  I won't touch them for this reason.  I stick with EVGA and actually just purchased an evga reference 980ti from newegg this morning.  Now i'm debating on MSI or the gigabyte for the top card to make SLI.  edit: Disappointed in newegg for limiting the 980ti to single cards.  I really wanted dual 980ti's.
nvidia,3de896,FunktasticLucky,2,Wed Jul 15 17:57:06 2015 UTC,What horror stories of asus cards?
nvidia,3de896,Wolfie3101,3,Wed Jul 15 20:29:19 2015 UTC,Their tech support not responding and then finally getting the card and saying it wasn't a failure it was user error and not fixing it stuff like that.   I also have a bad taste in my mouth from the Radeon 9800 days I believe when Asus used these stupid electronic circuit breaker type things. They would trip and drop the card into safe mode. And you couldn't get the card to return to stock speeds without shutting down and powering back up. Horrible design.
nvidia,3de896,FunktasticLucky,1 point,Wed Jul 15 22:12:19 2015 UTC,"MSI, Asus, Powercolor, XFX and Gigabyte are all Chinese companies in Taipei and Chinese companies and they just do not value HR. From what I've heard it is a mixed bag dealing with these guys. Sometimes they will handle a warrantee normally, other times they will just dick you over and rake you over the coals and make it an adversarial process.  There was a thread...somewhere on reddit where a guy bought a 295 x 2 from one of these companies which had a cracked fan and coil whine. He was forced to literally make a video and send it over. They then tried to undercut him because his card ""depreciated"" then send him back the same card he sent them without fixing it. The entire process to RMA the card took 5 months and he was forced to go though Newegg to force a warantee. I've also heard some wild shit come out of asus as well, where they would refuse to honor guarantees for arbitrary reasons, often blaming consumers. I have to stress its not all bad stories and there are plenty of people who don't have any issues but it is more of a gamble dealing with the Chinese manufactures when things go south.  EVGA however is a California company. You pick up the phone now, call them and you will talking to some dude sitting in an office chair in CA in the next 10 min. They are generally super helpful and they have the best warantee in the business. In fact, they even allow you remove the custom cooler from their cards and just put it back on if you need to RMA it for whatever reason, this would void a lot of the other manufac. warantes. From personal experience I buy EVGA, their cards have worked flawlessly for me for years now and I've heard more than a few people who do comptuer service and repair that generally they have less problems with EVGA stuff."
nvidia,3de896,BKachur,1 point,Thu Jul 16 04:12:17 2015 UTC,"Hey, Buy American is a good enough reason for me. lol. But this is my primary concern. I know cards sometimes don't work, and I'd rather not have to run a marathon to get another one."
nvidia,3de896,drummerdude41,2,Thu Jul 16 22:15:09 2015 UTC,"MSI, hynix memory modules used and overclocks almost as high as non-reference with stock cooler. Moving to a water cooled option would be outragious on this card."
nvidia,3de896,DrDaxxy,1 point,Wed Jul 15 20:47:25 2015 UTC,"Can't give specific suggestions, but compare warranty periods and conditions."
nvidia,3db0lb,TaintedSquirrel,7,Tue Jul 14 21:48:10 2015 UTC,Man I just love being a PC user. So much moddability. Makes me want to go exotic on my cards cooling system.
nvidia,3db0lb,keedxx,0,Tue Jul 14 22:02:51 2015 UTC,"Same here. I've been wanting to get the NZXT G10 bracket and the X41 cooler... not terribly exotic, but it would be awesome to water cool a card :D"
nvidia,3db0lb,UFO_cop,3,Tue Jul 14 22:46:31 2015 UTC,Excuse my French; but that is fucking sexy.
nvidia,3db0lb,LarryFromAccounting,1 point,Wed Jul 15 13:03:07 2015 UTC,what kind of temps/noise you getting?
nvidia,3db0lb,Zarknox,1 point,Wed Jul 15 01:44:11 2015 UTC,This is beautiful
nvidia,3db0lb,mermaliens,1 point,Wed Jul 15 02:13:25 2015 UTC,wher'es hid hard drive?
nvidia,3db0lb,brand0n,1 point,Wed Jul 15 04:16:32 2015 UTC,"i think its above the power supply, to the left"
nvidia,3db0lb,xk4l1br3,1 point,Wed Jul 15 04:30:05 2015 UTC,"ah good call, the last pic orientation vs the others had me thrown off.  I'm glad it's doable to swap those fans and what not.  I just repainted my 970 GTX and it's doign okay thus far but if things go downhill I will def look into doing something similiar"
nvidia,3db0lb,brand0n,1 point,Wed Jul 15 04:32:35 2015 UTC,I want my build skills to hit this level. That's tight man
nvidia,3db0lb,EHGepic,1 point,Wed Jul 15 10:30:21 2015 UTC,How is your OC/temps compared to before the mod?
nvidia,3db0lb,wanderjahr,1 point,Wed Jul 15 18:00:45 2015 UTC,I've got a MK-26 Black edition waiting to be installed on my 980. Sadly I'll need a new case to house that behemoth :'(
nvidia,3dbghe,douchiesnacks,5,Tue Jul 14 23:48:00 2015 UTC,"Wait, are 980s really that cheap now? What did I miss?"
nvidia,3dbghe,Stubrochill17,10,Wed Jul 15 00:03:50 2015 UTC,i price matched a microcenter 980 with office depot.
nvidia,3dbghe,DjMcfilthy,5,Wed Jul 15 00:44:41 2015 UTC,Man I missed the boat on that one.
nvidia,3dbghe,TaintedSquirrel,7,Wed Jul 15 01:38:03 2015 UTC,It was all over Reddit this weekend.
nvidia,3dbghe,DjMcfilthy,4,Wed Jul 15 01:44:54 2015 UTC,I guess I'll go cry now
nvidia,3dbghe,Soprelos,3,Wed Jul 15 01:55:59 2015 UTC,"They cancelled nearly every single order anyways, so you didn't really miss anything."
nvidia,3dbghe,sn1313,1 point,Wed Jul 15 05:08:07 2015 UTC,i think that only applies to staple. best buy and office depot/max honored the price match.
nvidia,3dbghe,erikv55,2,Wed Jul 15 05:15:05 2015 UTC,Imagine buying a 980 classified back in feb...wish i waited for a 6gb TI...fml
nvidia,3dbghe,iamlegend235,1 point,Wed Jul 15 04:36:00 2015 UTC,you might be able to price match a 970 for around 130$! the deal is in the buildapcsales subreddit! EDIT: 144$ https://www.reddit.com/r/buildapcsales/comments/3d9wjt/video_card_microcenter_evga_gtx_970/
nvidia,3dbghe,iamlegend235,1 point,Wed Jul 15 02:01:58 2015 UTC,My buddy was able to at Best Buy
nvidia,3dbghe,iamlegend235,1 point,Wed Jul 15 19:39:17 2015 UTC,hell yeah!
nvidia,3dbghe,PappyPete,1 point,Wed Jul 15 21:32:39 2015 UTC,"I just did that with a gtx 970. Got it for $155 with Office Depot. It says it will arrive in 7 days though, will it come earlier?"
nvidia,3dbghe,TaintedSquirrel,0,Wed Jul 15 03:15:19 2015 UTC,depends on where its shipping from. mine shipped from ontario which is less than 150 miles away from where i live.
nvidia,3dbghe,Soprelos,1 point,Wed Jul 15 04:09:46 2015 UTC,How do you see where it ships from?
nvidia,3dbghe,Soprelos,0,Wed Jul 15 04:13:35 2015 UTC,"it should say on your UPS tracking page, under shipment progress."
nvidia,3dbghe,HawkEye0,1 point,Wed Jul 15 04:27:58 2015 UTC,"Ah, its still in processing by OfficeMax. I should get a tracking number when it does ship"
nvidia,3dbghe,everyZig,1 point,Wed Jul 15 04:42:20 2015 UTC,"yeah you should, it does take a little while for UPS to activate your tracking number."
nvidia,3dbghe,everyZig,3,Wed Jul 15 04:48:57 2015 UTC,There was a crazy price match sale from a few days ago.
nvidia,3dbghe,BerserkD91,1 point,Wed Jul 15 00:33:55 2015 UTC,Why only one?
nvidia,3dbghe,gweedo767,1 point,Wed Jul 15 01:45:30 2015 UTC,i tried getting 2 but the assistant store manager wouldn't let me get more than 1. and tbh i don't really need SLI 980s because i only play @ 1080p and don't plan on upgrading for a while!
nvidia,3dbghe,Kautkto,1 point,Wed Jul 15 01:57:17 2015 UTC,"I thought staples, office max, and office depot were all owned by the same company?"
nvidia,3dbghe,maxiimus1,1 point,Wed Jul 15 05:28:36 2015 UTC,"i don't think they are, i know that office max and depot are owned by the same company but not staples. i think they were talking about a future merger between those 3 companies."
nvidia,3dbghe,TheAcePool,1 point,Wed Jul 15 05:46:21 2015 UTC,"Ah well that's just my luck. Picked the one store that decided to cancel :( was really excited about finally getting an affordable upgrade then bam, stapled."
nvidia,3dc8pm,Mobile_Lawnmower,2,Wed Jul 15 03:31:38 2015 UTC,"In the Nvidia Control Panel under Manage 3D Settings, there is an option for Monitor Technology that was recently added. Enable Gsync under that option. Lower down in the settings is Vertical Sync, if it is enabled it will only enable v-sync if your framerate is at the max refresh rate, if you turn that off then your framerates can go over the max refresh rate. Also make sure v-sync is off in-game so gsync will function"
nvidia,3dc8pm,RoastMostCookToast,0,Wed Jul 15 04:53:44 2015 UTC,Why doesn't Nvidia just make a framelimiter like AMD did?
nvidia,3dc8pm,hdshatter,3,Wed Jul 15 11:36:06 2015 UTC,"They did, that's what he just said .."
nvidia,3dc8pm,Integrals,1 point,Wed Jul 15 15:01:35 2015 UTC,"What display do you have? Most Gsync Displays I've seen have some sort of feedback that it's enabled.  The ROG swift had the red LED, the Predator has a refresh bar you can enable."
nvidia,3dc8pm,Integrals,0,Wed Jul 15 15:02:49 2015 UTC,No.
nvidia,3ddmg2,Luckyduck1337,2,Wed Jul 15 13:16:19 2015 UTC,Is the 41c you say your CPU runs at idle or load?  According to open hardware monitor my g1 fans kick in at 57C. in games it typically maxes out at 72C but I have seen a temp of 73C once. I have an r5 with a d15 and 4 noctua a14 ULN case fans though.
nvidia,3ddmg2,Zarknox,2,Wed Jul 15 15:56:15 2015 UTC,"My CPU hits around 41C at load and then around 34C idle. It's OCd but I feel like the d15 is disipating a good deal of heat into the case likely, it's large, in order to keep the CPU cool.  I went ahead and ordered two NF-A14 iPPC-3000 fans for the top of the case. I feel like it couldn't hurt to push some of that hot air out the top.  In retrospect I should have just gone for a full size case. Maybe I'll go ahead and make the move one day."
nvidia,3ddmg2,Zarknox,1 point,Wed Jul 15 16:24:55 2015 UTC,"hm wow. My cpu is anywhere from 31-35C idle but at load it gets around 57C on games. OH I think that would be due to having the low speed adapters on, ya that makes sense.  PS 74C is fine I think"
nvidia,3ddmg2,KaiForceOne,2,Wed Jul 15 16:45:54 2015 UTC,74C is well within safe temps and a great temp for this chip. I wouldn't bother with all those case changes.
nvidia,3ddmg2,p3dr0maz,2,Wed Jul 15 16:15:05 2015 UTC,"Agreed my MSI 980TI Gaming with good cooling would get 72c under Witcher 3 1440p 99% GPU load no vsync and fans @ 100%.  ASIC 71%  It actually scared me away from OC'ing and actually made the card throttle.  KrakenG10 bracket and Corsair H55 on there now and that's probably the best thing I ever did. Does'nt go over 57c now.  Added volts and pushed some 100Mhz more out of the core over stock, still tinketing."
nvidia,3ddmg2,Johnnyrodger,1 point,Wed Jul 15 21:02:53 2015 UTC,"The default fan curve for the G1 isn't good if your wanting to keep your card cool, only beneficial if you want it to run as silent as possible. http://i.imgur.com/nyIk0l4.png That is my custom fan curve, sits at around 60C @ 100% load."
nvidia,3ddmg2,Johnnyrodger,1 point,Wed Jul 15 14:16:38 2015 UTC,"Okay, thanks for the feedback. Seems like I'm not alone in feeling like the fan curve could use some adjustment.    Still seems like your fans are running at 50% at 100% load and keeping it at 60C, cooler than what I'm seeing."
nvidia,3ddmg2,attomsk,1 point,Wed Jul 15 14:33:51 2015 UTC,"Nah, they hover around 70-75% at full load."
nvidia,3ddmg2,attomsk,1 point,Wed Jul 15 15:25:27 2015 UTC,"That fan curve is a bit more aggressive than I run, my curve hits 70% fan at 72c.  I take it you are trying to keep the card running at 1.243v?"
nvidia,3ddmg2,attomsk,1 point,Wed Jul 15 17:07:10 2015 UTC,What speed are your case fans running at?  Do you have good ambient temps inside the case?  What is the pressure gradient like inside your case?  I can get DRASTICALLY different temps on my video card just by flipping two fans in my case and changing airflow directions.
nvidia,3dco9o,reyllo,3,Wed Jul 15 06:02:15 2015 UTC,"I think performance-wise there wouldn't be a huge difference. But you have to consider two things:   You already have one 980 so it would be cheaper to buy another one, however You have to know that SLI is not always beneficial compared to a single-card solution. I'm running two Gigabyte GTX 780 GHz and am going to transition to a 980 Ti soon because I want to move away from SLI for now. Mainly because of noise, temps and some incompatibilities with certain games (see HDR flicker for example).   So yeah, from a financial standpoint go SLI. But from a compatibility and all-round standpoint take the single card. But that's just my humble opinion"
nvidia,3dco9o,BaNaNaKING42,2,Wed Jul 15 06:44:54 2015 UTC,The general consensus I'm seeing is that it's better to have one amazing card rather than two lesser cards. Plus some games don't support SLI.
nvidia,3dco9o,therusskiy,0,Wed Jul 15 06:11:18 2015 UTC,"The going rate for 980s is 450 on ebay; a 980 ti is 650, so you'd safe 250 bucks by going the 980 ti route, with nearly identical performance."
nvidia,3dco9o,JJSCHMITTY,2,Wed Jul 15 06:12:29 2015 UTC,Yeah but he already has one 980 so it would be cheaper for him to buy another one
nvidia,3dco9o,BaNaNaKING42,5,Wed Jul 15 06:40:49 2015 UTC,"On what planet does SLI 980 has ""nearly identical"" performance?   It curbstomps a single 980 ti. The question OP should ask is if the increase in performance is worth losing 2GB vram and going from single to multi-card.  I have been using SLI for years and in my library there are only a few games that don't run well/support SLI and all those games can be maxed easily with a single 980.  Expect SLI to only improve in Windows 10 / DX 12 with better API support and lower overhead.  But if you are aiming 4k i'd say go 980 ti. But Reddit basicly seems to jerk-off to the 980ti forgetting the merrits of other cards."
nvidia,3dco9o,Gutterblade,2,Wed Jul 15 06:46:37 2015 UTC,"I ran 2 670s in SLI for a year, I always ran DDU before updating drivers and I never had issues with it. My only real problem with it was 2GB of vram wasn't enough for 1440p so I got rid of them and bought a 290x which had about the same performance but with double the vram.  Were there games that didn't support SLI? Yes, plenty of them. But if you actually know how to use Google you could easily find out most of these games you can force SLI with NVInspector and it will work fine most of the time.  I am convinced the majority of people saying don't use SLI never actually used it themselves and have no experience with it."
nvidia,3dco9o,hdshatter,-3,Wed Jul 15 09:13:42 2015 UTC,"It actually doesn't curbstomp a 980 Ti surprisingly. A decently overclocked 980 Ti was getting me around 22,000 on Firestrike and my highly overclocked watercooled 980 SLI was getting me 28,000.  Edit: Okay thanks for the downvotes."
nvidia,3dco9o,DrexelDragon93,0,Wed Jul 15 12:20:37 2015 UTC,Get the 980 Ti
nvidia,3dco9o,pogo_is_god,0,Wed Jul 15 06:40:10 2015 UTC,Single 980ti however its hard to recommend SLI until we get NVlink on pascal we don't have good GPU scaling. Fury X CF may be better than 980ti in dual configs.
nvidia,3dbae8,rebirth1078,2,Tue Jul 14 23:00:22 2015 UTC,More projector related patents: http://www.freshpatents.com/-dt20150709ptan20150194128.php Any speculations?
nvidia,3dbae8,andrew5161,2,Tue Jul 14 23:03:01 2015 UTC,"As soon as a tech looks like it might have any chance of being a real thing, companies swarm in and patent everything reomotely related to it regardless of if they have any real working product. They seem to get around the normal ways of fileing patents that the rest of us have to abide by."
nvidia,3dbae8,ChikNoods,2,Wed Jul 15 02:53:23 2015 UTC,shield projector update?
nvidia,3dbae8,Put_It_All_On_Blck,2,Wed Jul 15 05:27:35 2015 UTC,"Seems like they are just after the patent to troll manufacturers in 50 years.   There are already pico projectors and projectors that are built into phones or in cases for them. We are far far away from having a worthwhile mobile projector setup. The things go through batteries extremely fast, the bulbs are delicate and need to be covered as oils and debris is terrible for them, they heat up which is terrible for batteries, the lumen and resolution will be low, etc."
nvidia,3dbae8,Rozenrot,1 point,Wed Jul 15 06:47:18 2015 UTC,"I work with projectors for a living and this isn't something that will be viable for a very, very long time.  Even the smallest pico-projectors are low resolution, hot, power hungry things. You'll not see a projector in your phone for years if not a decade or more.  Personally I wouldn't hold my breath for it ever happening, because the applications are almost none, and most people will see this is gimmicky as hell."
