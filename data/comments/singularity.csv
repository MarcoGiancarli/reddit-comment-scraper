singularity,3dzl11,Anen-o-me,11,Mon Jul 20 20:29:39 2015 UTC,Techno-decentralization is capitalism in its purest form. The author fails to differentiate between crony-capitalism and anarcho-capitalism.
singularity,3dzl11,heltok,4,Mon Jul 20 21:01:30 2015 UTC,"That's what I find so ironic about this, that he's been seduced by the ultimate outcome of capitalism itself, and thinks it's not a continuation somehow.  I think their real beef is with the requirement that they work. This future seems to be promising an era where you don't have to work if you don't want to, and that is their utopia.   But it's still capitalism producing so much wealth that you don't have to work, it's not post capitalism. But he seems to think capitalism relies on worker exploitation, and if workers don't have to work... then how could it be capitalism.  Aha, that's totally the mentality :P He can't imagine you can call it capitalism if workers aren't working for owners."
singularity,3dzl11,game_r,10,Tue Jul 21 00:14:02 2015 UTC,"Capitalism is about who owns the means of production, not how much is produced. If the production cost of x product moves to near zero, then effectively capitalism has ended for that given x product, as you are not required to invest capital to produce the x product any more.  Post capitalism is real, but it isn't necessarily a utopia without challenges."
singularity,3dzl11,heltok,1 point,Tue Jul 21 01:07:57 2015 UTC,"Capitalism is about who owns the means of production, not how much is produced. If the production cost of x product moves to near zero, then effectively capitalism has ended   It would mean that all individuals have a lot of means of productions right? I would say it would mean that capitalism has won, not that it has ended.   Also I don't really see how scarcity will end. It's not like I'm gonna be able to 3D print a few extra universes for myself?"
singularity,3dzl11,game_r,8,Tue Jul 21 06:43:58 2015 UTC,"Production will always be finite, but post scarcity is not about wasting, it is about having the capability to fulfill virtually all your needs. Let's take food as an example. Even with today's technology we produce 2700 (globally), 3800 (in the USA) calories per capita per day, much more than our actual needs. The problem lies on how much of that we waste, if we were more efficient in reducing the waste we wouldn't even need to produce that much in the first place.   With capitalism in order to make food super cheap you need to produce 10,000 calories per capita per day, wasting most of it in the process. When food production moves towards post capitalism you will 3D print your own food, and while you will be able to produce 20,000 calories a day, you will just produce exactly what you need (2-3000 calories).   Near zero production cost in the long run means zero profit. The key to driving prices to near zero (post scarcity) is how much you can produce and not how much you actually produce. It is phenomenal how close we are to post scarcity on many things (like food) even without any exotic technology.  Capitalism is a necessary evil for progress, but it seems that slowly a viable alternative emerges."
singularity,3dzl11,heltok,2,Tue Jul 21 07:41:15 2015 UTC,"it is about having the capability to fulfill virtually all your needs.    Which we can see has happened. People today only need to work a few hours a week to satisfy their needs by early 1900s standards. But people still choose to work more hours because their needs has changed. I assume that will be the case in the future also, at least for many people. Maybe my few ETHs will let me eat and live as I do now, but will it buy me the spaceship I want?    Capitalism is a necessary evil for progress, but it seems that slowly a viable alternative emerges.   I would say it's a necessary good for progress. Capitalism is kindness:  https://www.youtube.com/watch?v=nKTiCj-9Emc"
singularity,3dzl11,TunkaTun,2,Tue Jul 21 08:28:25 2015 UTC,Says the man who then turns around and charges said dead patients relatives 100k+ for the luxury of dying at the hospital he works at. He is full of shit.
singularity,3dzl11,Metric0,1 point,Tue Jul 21 18:58:42 2015 UTC,"When food production moves towards post capitalism you will 3D print your own food, and while you will be able to produce 20,000 calories a day, you will just produce exactly what you need (2-3000 calories).    Owning your 3D printer means that individuals own the capital, which is the definition of capitalism.  If ""the collective"" owned and controlled the 3D printer that makes your food, then we'd have something other than capitalism.    Decentralized capitalism is still capitalism -- in fact, it's the natural end-state of capitalism, as capital gets cheaper and cheaper with time."
singularity,3dzl11,oliveirart,1 point,Wed Jul 22 00:43:35 2015 UTC,"If the production cost of x product moves to near zero, then effectively capitalism has ended for that given x product   No, production cost must be actually zero for capitalism to end. And that can't happen. There are many products, for instance, that cost less than a penny per each, yet are bought in large quantities at a time and produced by businesses."
singularity,3dzl11,mrhebrides,2,Tue Jul 21 15:58:42 2015 UTC,"Makers of the world, unite!"
singularity,3dz1s2,Yosarian2,6,Mon Jul 20 18:13:05 2015 UTC,"So what's the catch? What chatbot is behind the lines? It certainly can't be AIML. Too easy to notice. Unless people are really that dumb?  Also, reading the conversation log, the words don't sound like anything a computer would produce. Is the person taking the response from the chatbot, and rephrasing it to make it sound better? If so, that proves nothing besides that humans can make sense of gibberish."
singularity,3dz1s2,Kafke,2,Mon Jul 20 21:19:35 2015 UTC,"People have set up chatbots in Second Life, where they control an avatar and so are operating at the same level as the people you ""know"" in Second Life. They do seem a bit more convincing, but only a bit."
singularity,3dyu1s,wolfballlife,2,Mon Jul 20 17:19:54 2015 UTC,"From the article:   This sounds awfully like automation replacing humans in the long run. Are we any nearer to knowing whether machines will replace managers?  It’s true that change is coming (and data are generated) so quickly that human-in-the-loop involvement in all decision making is rapidly becoming impractical. Looking three to five years out, we expect to see far higher levels of artificial intelligence, as well as the development of distributed autonomous corporations. These self-motivating, self-contained agents, formed as corporations, will be able to carry out set objectives autonomously, without any direct human supervision. Some DACs will certainly become self-programming.   3-5 years?  Really?"
singularity,3dyu1s,Yosarian2,2,Mon Jul 20 20:04:29 2015 UTC,"Agreed, it seems ambitious, there are going to be potential regulatory hurdles to overcome even if the tech is there that quick."
singularity,3dyu1s,capn_krunk,1 point,Tue Jul 21 01:05:33 2015 UTC,"I agree, sounds very ambitious, to say the least. On the flip side, I guess self-driving cars did, too, not that long ago."
singularity,3e05f9,ur_mom69,13,Mon Jul 20 22:58:12 2015 UTC,"That's not a supercomputer. It's a dumb chatbot. It didn't pass the turing test. There are plenty of chatbots much 'smarter' than Eugene Goostman, since Goostman is just running a type of AIML while playing sociological tricks to make people lower the bar. If you actually talked to someone that spoke like the bot, you'd call them an idiot. The turing test has yet to be passed. Despite what some people claim. You're most likely thinking of AGI, AI is a huge field that encompasses many things we already have and work with."
singularity,3e05f9,Kafke,2,Tue Jul 21 02:34:56 2015 UTC,Do you believe in free will?
singularity,3e05f9,Seth-mars,2,Mon Jul 20 23:02:48 2015 UTC,Watch the mind and notice how everything it does is stimulus/response.  The human is no different than a tree swaying in the wind.
singularity,3e05f9,Singular_Thought,2,Mon Jul 20 23:28:34 2015 UTC,I don't like leaving drive-by negs so I'll tell you why I'm downvoting.  It's old news. It was never true to begin with.
singularity,3e05f9,Unholy_VI,1 point,Tue Jul 21 02:43:26 2015 UTC,What makes you happy?
singularity,3durlv,hevnervals,23,Sun Jul 19 17:55:49 2015 UTC,"People are attracted to it for various reasons. Some probably do replace it for a religion. I see it as our very likely future which means it is a belief but I'm open to changing that belief based upon future progress. Really, who knows what it will actually look like?      Amen. Just kidding. :)"
singularity,3durlv,ruffyamaharyder,12,Sun Jul 19 18:23:02 2015 UTC,"There is a trend where technology in general is treated as religion, but the singularity has adopted the most of religious elements. Something almighty, savior, cure all diseases or bring the apocalypse. Even people saying you have to believe in it, that salvation will come and we should spread the news :)  Also, I don't think the future will be so unrecognizable for our children. For mideval people, the industrial age looks like the singularity.   We're between to different ages, the industrial and whatever comes next. But people will still live in houses, eat food, need transportation and complain about everything.   What interests me more is how the economy and jobs will look like, as that will affect lives the most."
singularity,3durlv,Yasea,2,Sun Jul 19 23:02:30 2015 UTC,"Also, I don't think the future will be so unrecognizable for our children. For mideval people, the industrial age looks like the singularity.      This will probably always be the case, right? Exponential growth is somewhat predictable - just a curved line that starts becoming very close to a line. Each step up with be closer (in time) than the step before, but about the same level of increase as the last tick."
singularity,3durlv,ruffyamaharyder,3,Sun Jul 19 23:31:54 2015 UTC,"hm, no...there can be pretty significant changes, and they only look predictable in hindsight.  no one in 1500 had any clue what 1800's would look like."
singularity,3durlv,wren42,2,Mon Jul 20 04:01:48 2015 UTC,"He's not saying that we can predict what technology would look like when we approach the singularity. He's saying that the rate-of-progress (mean time between a major breakthrough in any given field) is exponential.  Example: In 800ce humans started to transition from the two-field system of farming, to the three field system (bringing a huge increase in crop yields). In 1600ce, farmers started to adopt the four field system. There have been three different ""major revolutions"" in farming techniques with about the same level of impact in the 1900s era alone. And we're close to a 4th one (automated crop planting/harvesting) in the 2000s era.  This example isn't unique to farming. Tool making, flight, construction, scientific methods, everything has experienced the same level of accelerating breakthroughs."
singularity,3durlv,Terkala,1 point,Mon Jul 20 07:17:24 2015 UTC,"Yes, I understand the exponential part, the word I'm sticking on is ""predictable.""  This word seems wrong in two ways.   First, a history of exponential growth doesn't necessarily imply that trajectory will continue indefinitely.  That's actually physically impossible.  There is only so much population and energy growth possible for one planet.  There may be many other hard physical limits we will run up against, like the speed of light, when looking at technology growth.   Secondly, just because the trajectory is predictable the actual steps may be extreme, and the increased speed of them makes seeing out a given amount of time less and less reliable."
singularity,3durlv,wren42,1 point,Mon Jul 20 13:23:26 2015 UTC,"First, a history of exponential growth doesn't necessarily imply that trajectory will continue indefinitely. That's actually physically impossible.   The most interesting theory about this I saw, is that the economy will move towards digital, in a sense that we'll have AI dealing in an information economy with superior (inhuman) speed of development. Humans receive the cast-offs, living in a much slower economy. But those cast-offs will be so great it will beat anything we'll have today.   This counters the argument the 'finite growth' economy. It still grows exponentially for a long time, but we live in a small fraction of the total economy."
singularity,3durlv,Yasea,1 point,Mon Jul 20 13:41:15 2015 UTC,"http://physics.ucsd.edu/do-the-math/2011/07/galactic-scale-energy/  there are limits to growth, even for a digital economy.  computers require energy, powerful ones quite a bit.  and they can only become so efficient.   One point he makes in another article is important:  If the ""digital"" economy keeps growing indefinitely relative to the physical economy, then physical goods begin to have exponentially small cost, approaching 0.   This makes no sense, though, because digital goods are still tied to physical capacity.    IE if the cost of energy per unit of economic growth keeps getting smaller, eventually energy would cost nothing, but we know this can't be the case as then someone could just buy ALL the energy and control the entire infrastructure.   TL/DR - infinite growth is impossible, even at pretty small time scales (500-1000 years)"
singularity,3durlv,wren42,0,Mon Jul 20 13:56:30 2015 UTC,"Your link ignores any possibility of advancements in energy science. There are already test reactors for fusion power. And a single fusion power plant would supply enough energy to replace ""every"" power plant on the planet. We can massively surpass the amount of available power from sunlight by simply building a good number of fusion power plants.  And of course, power is never going to be ""free"", but it might be sufficiently close to it that a single individual could purchase terrawats worth of energy for their personal use."
singularity,3durlv,Terkala,1 point,Mon Jul 20 15:23:15 2015 UTC,"no, it doesn't.  read the whole thing, more carefully.    The sun IS a fusion reactor.  There is absolutely no conceivable way we can ""surpass the amount of available power from sunlight"".  He does the calculations using a Dyson Sphere with 100% efficiency.    Earth-based Fusion would give is exactly squat in comparison to an engineering feat that monumental.   it's tied to available heavy elements and the scope is severely limited by thermodynamics.  he even addresses this point specifically:   Some readers may be bothered by the foregoing focus on solar/stellar energy. If we’re dreaming big, let’s forget the wimpy solar energy constraints and adopt fusion. The abundance of deuterium in ordinary water would allow us to have a seemingly inexhaustible source of energy right here on Earth. We won’t go into a detailed analysis of this path, because we don’t have to. The merciless growth illustrated above means that in 1400 years from now, any source of energy we harness would have to outshine the sun.  Let me restate that important point. No matter what the technology, a sustained 2.3% energy growth rate would require us to produce as much energy as the entire sun within 1400 years. A word of warning: that power plant is going to run a little warm. Thermodynamics require that if we generated sun-comparable power on Earth, the surface of the Earth—being smaller than that of the sun—would have to be hotter than the surface of the sun!"
singularity,3durlv,wren42,1 point,Mon Jul 20 15:33:25 2015 UTC,Do you think we are/will be better at predicting the future than we were in the past?
singularity,3durlv,ruffyamaharyder,1 point,Mon Jul 20 13:16:37 2015 UTC,"in some ways yes, in that we understand physics, history, and social forces better; but the changes are also becoming more rapid and more extreme.  While we can lay out the likely possibilities with greater reliability, knowing what will happen, how and when is harder in some ways."
singularity,3durlv,wren42,2,Mon Jul 20 13:19:12 2015 UTC,"the industrial age WAS the equivalent of a singularity for people in the middle ages.  civilization was completely transformed.  things will probably be very different for my grandchildren at least, though not necessarily because of true ai."
singularity,3durlv,wren42,10,Mon Jul 20 04:00:23 2015 UTC,"My personal belief is that singularity is an eventuality, and as such I feel no need to proselytize about it. All I know is that I want to live long enough to make it."
singularity,3durlv,RubiksSugarCube,1 point,Sun Jul 19 23:31:03 2015 UTC,How will you recognize actual conscious awareness outside yourself if you don't know what it is in and of yourself? Honest question.   Humans can pass the Turing test.. and here we are.
singularity,3durlv,buttcoinershillfag,0,Mon Jul 20 14:22:30 2015 UTC,"We can recognize it already even if we can't articulate why. The problem with the idea that we will have an AI that is completely transparent is that consciousness, as we experience it, includes an unconscious element. Sure, your heart continues beating unconsciously, regardless of volition. What I mean is that our actual conscious life is fraught with influences of the unconscious. For example, most humor is unconscious, you simply don't know why it's funny. This is what is really impossible to simulate in AI. The rational action part is no big deal given enough time but how to program a duality where most of it is hidden from the ""self"" when most people don't even believe that this is operative? The singularity is exciting for the prospect of curing physical problems of biological entropy and we will replicate/simulate the human brain but we will not create humanized robots until we at least start to admit that we have an unconscious that is not just composed of autonomic functions (subject to our will, though still automatic)."
singularity,3durlv,onafarawaybeach,1 point,Mon Jul 20 15:35:07 2015 UTC,We can recognize it already even if we can't articulate why   I'll disagree. Anyone with deep awareness of consciousness would be unlikely to pursue or encourage pursuits of things that represent a misunderstanding of it. How would an artificial consciousness help anyone who suffers from a lack of awareness?    The singularity is exciting for the prospect of curing physical problems of biological entropy   What good is curing physical problems of people/societies who develop those problems as a natural response to unawareness? Why is perpetuating misunderstanding exciting? I think many who await the singularity are hoping for the creation of something smarter than what exists here on earth. A savior intelligence.   There's ~7 billion consciousnesses on the earth right now.  How will creating an AI consciousness add any benefit?   I'm not hostile to any pursuit that doesn't harm others. But GIGO applies here and unless those seeking the creation of AI are essentially benevolent self-realized individuals i see more harm than good coming from this. The desire to mold consciousness into the perverted worldview that is prevalent on this planet is a very real danger to the unconscious masses. For those who understand who and what they are.. not so much. Not being able to perpetuate the physical form or the temporal personality really isn't desirable in actual reality.   From my perspective actual conscious awareness will never be recreated.. instead an indiscernible facsimile with all the follies of misunderstanding that drives it's development will be created.
singularity,3durlv,buttcoinershillfag,0,Mon Jul 20 19:22:51 2015 UTC,What good is curing physical problems of people/societies who develop those problems as a natural response to unawareness? Why is perpetuating misunderstanding exciting?   I would rather not get cancer. I care about the world in comparison to myself about as much as someone cares about themselves dying in comparison to how much they care about someone dying that is unknown to them.
singularity,3durlv,onafarawaybeach,1 point,Mon Jul 20 19:28:28 2015 UTC,Do you think creating technology that would allow you to transfer your being into a circuit is an easier problem to solve then figuring out how you personally might avoid getting cancer?
singularity,3durlv,buttcoinershillfag,0,Mon Jul 20 20:06:44 2015 UTC,"Of course not. I'm not talking about avoiding cancer, I'm talking about negating it through a cure. Transferring your being into a circuit is scifi and may ultimately be impossible b/c your point of view, your ""I"", is created biologically. It's in your mind but not your brain (at least we can't see it). How to transfer this point of the view is the crux, not your being. We don't even know what this ""I"" is yet in science. In philosophy/religion there is a recurring realization of what this ""I"" consists of that I won't get into here."
singularity,3durlv,onafarawaybeach,1 point,Mon Jul 20 20:12:52 2015 UTC,I'm talking about negating it through a cure   And a computer with consciousness negates it how? Do we really need AI to inform us that we are unhealthy and in what ways we are unhealthy?
singularity,3durlv,buttcoinershillfag,1 point,Mon Jul 20 23:32:08 2015 UTC,"Ah, the moderate, private Singularist. Much better than the zealous proselytizers that come to my door."
singularity,3durlv,mr_one_liner,1 point,Mon Jul 20 16:40:32 2015 UTC,"One of my worst fears and inevitable, I'm afraid. Someone's going to figure out that they can make a buck of singularitarianism and lure in those kids who would normally fall prey to LaRouche or the flavor of the month hipster evangelist."
singularity,3durlv,RubiksSugarCube,7,Mon Jul 20 17:05:40 2015 UTC,"Absolutely. A secular one, but still based on evidence that is certainly not accepted in mainstream science. Looked to as a near-future solution to all of life's problems. And, to hear RK tell it, preordained by the structure of the universe.  This is from a believer. Let's recognize and be critical of our own beliefs first."
singularity,3durlv,mattreddit,3,Sun Jul 19 22:03:16 2015 UTC,You shortening Ray Kurzweil to RK reminds me of scientologists shortening L. Ron Hubbard to LRH.
singularity,3durlv,CrimsonSmear,4,Sun Jul 19 22:21:22 2015 UTC,"To be fair, if you're in a forum where certain names come up a lot, and will be recognized, it can become a bit of a convention to do this. Philip K. Dick often gets compressed to PKD, Hunter S. Thompson is often cut to HST, Robert Anton Wilson (who oughta be better known in singularity circles because he was writing about the Singularity years before it had that name!) was often called RAW by his fans."
singularity,3durlv,RandomMandarin,5,Mon Jul 20 01:29:49 2015 UTC,"I don't think that's a useful way to discuss the issue.  As far as I can tell, the singularity hypothesis is entirely plausible based on what we know about science and technology today, for strictly rational reasons.  I don't know how likely it is to actually happen, or how long it might be if it does, but dismissing it as a ""religion"" is really missing the point."
singularity,3durlv,Yosarian2,2,Sun Jul 19 23:25:16 2015 UTC,"I'm not saying the singularity is a religion, but that many people treat it as such."
singularity,3durlv,Swipecat,5,Sun Jul 19 23:54:08 2015 UTC,"Possibly. Can you give an example of the ""zealous irrationality"" that you mention. What specifically was said?"
singularity,3durlv,FractalHeretic,7,Mon Jul 20 09:41:52 2015 UTC,Can you show an example of this zealous irrationality?
singularity,3durlv,joshpuck,4,Sun Jul 19 18:23:41 2015 UTC,Kurzweil?
singularity,3durlv,Yosarian2,16,Sun Jul 19 21:32:28 2015 UTC,"Kurzweil isn't really a zealot.   His predictions are extremely optimistic, overall, but he's not a zealot because he doesn't seem t have a problem with people who disagree with them.  He actually works very well with people who strongly disagree with his more optimistic predictions; for example, Craig Venter."
singularity,3durlv,ocular_lift,3,Mon Jul 20 00:09:16 2015 UTC,Ben Goertzel is way worse in my opinion. Kurzweil seems borderline autistic which doesn't make for a zealot.
singularity,3durlv,Speedloaf,3,Sun Jul 19 22:23:27 2015 UTC,"I used to eat this shit up as a teenager, but now I can't stand to hear either of them talk. Especially Goertzel. I'm still securely entrenched in the transhumanist camp though."
singularity,3durlv,ocular_lift,2,Mon Jul 20 02:01:39 2015 UTC,"I understand how you feel. The Panglossian predictions are seductive, but ultimately empty. It's still a lot of fun to keep up with the tech world. I'm personally a big fan of the developing virtual reality ecosystem."
singularity,3durlv,holomanga,1 point,Mon Jul 20 02:24:01 2015 UTC,The r/atheism of transhumanism.
singularity,3durlv,poopagandist,7,Mon Jul 20 15:12:25 2015 UTC,"What, you mean like on internet forums? Because I don't see a very strong, zealous and irrational movement of people causing a stir IRL in the name of singularity. They definitely aren't drowning out any voices."
singularity,3durlv,wren42,3,Sun Jul 19 22:30:44 2015 UTC,"there are definitely some of those types.  I see lot of spurious claims made here by people with very little understanding.    In all likelihood, advances in AI are not going to create a perfect utopian society.   The first really impactful AI are likely to be used in business and finance, and will only increase the wealth gap.  Semi-immortality will only be available to a select few who control most of the power.   People don't seem to realize that with the advent of AI and automated industry things are far more likely to get worse for the average person before they get better.   most jobs will simply be obsolete.  any post-scarcity society that emerges will likely have to follow a period of unprecedented population decline."
singularity,3durlv,Buck-Nasty,2,Mon Jul 20 03:58:37 2015 UTC,Relevant scene with Anders Sandberg in TechnoCalyps
singularity,3durlv,phozee,2,Mon Jul 20 02:39:50 2015 UTC,"I think it may give people a sense of hope like religion does, but it certainly has more basis in reality with evidence to support it."
singularity,3durlv,wkw3,3,Mon Jul 20 03:51:56 2015 UTC,"""You guys come off as irrational religious zealots and it's undermining discussion. Thoughts?""  Not a very positive way to open a discussion.  I am open to, and optimistic about the possibilities that technology is opening to humanity. I believe that strong AGI is almost inevitable at this point and that humanity will be fundamentally changed.  It does come off quasi-religious in the sense that is all prediction (note that prophecy is supposed to have a supernatural origin), but the only thing that just about everyone agrees with us that we have no idea what happens after. That's not much to hang a religion on. They're usually quite specific on those matters.  Also, most people aren't experts, and those who are are only exist in one of two fields. There are subreddits for AI, machine learning, robotics, et. al, but the focus here is something much fuzzier.  Anyway, I have some literature if you'll let me into your home."
singularity,3durlv,Supervisor194,3,Mon Jul 20 04:56:57 2015 UTC,"The Singularity may produce utopia, it may produce hell, it may produce oblivion or it may produce none of these things, but say what you will about it, it's a real possibility, whereas religion is just a farce. I don't like people who like to equate the Singularity with religion, as it marginalizes the Singularity and gives religion a standing it is not worthy of."
singularity,3durlv,mr_one_liner,-1,Mon Jul 20 00:58:21 2015 UTC,"Yes I agree, and I think you missed the point."
singularity,3durlv,troller10,1 point,Mon Jul 20 12:24:45 2015 UTC,"A compelling ubertheory with a hope of escaping death makes people cling to the hope of it all. No amount of ""ripping religion a new one"" is gonna get rid of the similarities."
singularity,3durlv,ianyboo,1 point,Mon Jul 20 16:47:06 2015 UTC,"The Singularity Church of the MachineGod is an eBook in Deus Ex: Human Revolution.  Our Manifesto  Have you ever wondered about the fate of our species?  We exist in a time unlike any other; a time where man can go beyond the tyranny of flesh and embrace a new tomorrow.  We share the belief of a manifest destiny for our species, a future where human instrumentality evolves into a form beyond our crude flesh and blood.  The next evolution is near, A coming together of man and machine. A synthesis greater than the birth of the human organism.  This is the Singularity. The God in the Machine.  Many cultures predict an end to humanity in the near future, a final Armageddon that will end the world; but we disagree.  We know the future of man.  Join us. Understand the path that unfolds before our species.  Grow beyond the bounds of flesh and blood.  Embrace the Singularity!"
singularity,3durlv,aim2free,1 point,Tue Jul 21 13:07:32 2015 UTC,"No, religions are based on wishful thinking and a complete lack of evidence they are simply asserted to be true based on blind faith.  ""The singularity"" stands in complete contrast, folks who are interested in the concept tend to be pretty open to the possibility that it might not happen at all. They look at the evidence and discuss it critically and rationally.   Sure there is some element of ""wishful thinking"" involved but I think it's the healthy type that leans towards ""wouldn't it be cool if we could work our butts off and build a utopia!"" and not the unhealthy ""lets just sit around doing nothing and hope everything works out"""
singularity,3durlv,grahag,1 point,Mon Jul 20 05:41:18 2015 UTC,"""that it might not happen at all""   There are certainly many e.g. Christians who believe that the Rapture might not happen at all either.  I would say that speak about religion is irrelevant. The singularity is a concept that can be understood, at least from us who are implementing it. What people have experienced for long is an anti singularity where the whole world has been kept at a standstill due to locked in proprietary technology and other forms of artificial scarcity (copyrights, patents etc).  When technology and information are opened up, then you start an evolutionary process, which usually has an exponential convergence, but when you add creativity to the concept, then you get an accelerated return which leads to an iterated exponential, a superexponential. I wrote a brief essay on this a couple of years ago linking to a nice diagram of a super exponential."
singularity,3durlv,MetaMainer,0,Mon Jul 20 06:22:29 2015 UTC,"The difference between religion and ""the singularity"" is that the singularity is in almost every case, backed by science and is predictable to some extent. You can't say the same thing about religion. In most cases, the scientific community agrees that it WILL happen, but almost none can agree on the time frame.  Many skeptical scientists feel that it's science fiction, but almost ALL technology that exists today was science fiction less than 50 years ago. As we grow to understand more about science and the singularity, we'll have more answers. Religions can't give that guarantee. Religion has been promising answers for thousands of years, but providing little in the form of actual knowledge.   Yeah, some may take the singularity as a religion, but at the very least, it's seated deep in science and not in age old supernatural stories."
singularity,3durlv,sixwings,0,Mon Jul 20 05:01:29 2015 UTC,"No ritual, mythology, deities or doctrine.   No assertions on morality, creation, or social norms.   I say no comparison."
singularity,3durlv,Jaqqarhan,-7,Mon Jul 20 11:24:13 2015 UTC,The whole notion of copying one's brain into a computer simulation in order to obtain immortality is about as religious and pseudoscientific as can be. The science behind it is exactly zero. It's pure geek sci-fi fantasy mixed with superstition.
singularity,3durlv,sixwings,3,Sun Jul 19 22:41:40 2015 UTC,"Yea there's no science behind it, but I don't think that's suggested either. It's more like an idea to work towards."
singularity,3durlv,Jaqqarhan,3,Sun Jul 19 23:47:03 2015 UTC,"From looking at your profile, it looks like you are a troll. In case you are serious though, what do you actually mean? Brain simulations already exist and they are obviously don't have any fixed lifespan. We haven't been able to simulate an entire human brain yet because supercomputers are just now getting powerful enough to run such a simulation. The US and EU governments are pouring billions of dollars into the human brain simulation."
singularity,3durlv,sixwings,-2,Mon Jul 20 02:06:57 2015 UTC,"Yeah. Well, let me know when they simulate the conscious sensation of blue or red inside a computer using falsifiable science. Until then, you're just talking about how many angels can dance on the head of a pin, aka religion and superstition. Singularity is voodoo crap from mediocre minds.  LOL."
singularity,3durlv,Jaqqarhan,2,Mon Jul 20 04:35:51 2015 UTC,"let me know when they simulate the conscious sensation of blue or red inside a computer using falsifiable science   Let me know when you prove that humans have the conscious sensation of blue or red using falsifiable science. Until then, believing in human consciousness is just as superstitious as believing in machine consciousness"
singularity,3durlv,sixwings,-1,Mon Jul 20 05:00:13 2015 UTC,I don't have to prove anything. I am not the one making extraordinary pseudoscientific claims about machine consciousness without a way to falsify it.
singularity,3durlv,Jaqqarhan,1 point,Mon Jul 20 06:12:52 2015 UTC,"According to you, claiming something can have consciousness without a way to falsify it is ""an extraordinary pseudoscientific claim"". Therefor claiming that humans have consciousness is also ""an extraordinary pseudoscientific claim"".  There is no logic behind your assertion that humans have consciousness but machines can't. The idea that consciousness is unique to humans is just based on religious dogma with no scientific basis."
singularity,3durlv,sixwings,-1,Mon Jul 20 06:38:12 2015 UTC,You're putting words in my mouth. All I know is that I have consciousness. I did not say that machines cannot have consciousness. Maybe they can and Singularitarians are right but that is not science. It is an assumption in search of falsification.
singularity,3durlv,Jaqqarhan,1 point,Mon Jul 20 06:56:41 2015 UTC,"Consciousness isn't relevent to the actual topic. We are talking about making simulations of human brains. This doesn't require anything beyond our current scientific understanding. It mostly just requires more powerful computers that we will have in the net few years. The question about whether the original human brain and/or the simulation of the human brain are conscious is certainly interesting from a philosophical standpoint, but not relevant to futurist predictions."
singularity,3durlv,RubiksSugarCube,-1,Mon Jul 20 07:27:06 2015 UTC,"It's relevant when Kurzweil and others in the Singularity religion claim that we will upload our brains to machines and gain immortality. What do we do with the old brains? Kill them? That's murder, the last I heard."
singularity,3durlv,sixwings,1 point,Mon Jul 20 07:31:41 2015 UTC,"It doesn't matter what you do with the old brain. The simulation is immortal. That was the claim we are talking about.    That's murder, the last I heard.   Now you are going off on another tangent about the future legal system."
singularity,3durlv,RubiksSugarCube,2,Mon Jul 20 07:44:27 2015 UTC,You're in the wrong subreddit.
singularity,3durlv,sixwings,-4,Sun Jul 19 23:30:11 2015 UTC,"Nah. I've been meaning to say this for a while.  Reddit, a bastion of free speech. Not."
singularity,3durlv,Resistivism,2,Mon Jul 20 00:02:09 2015 UTC,You should try voat!
singularity,3dyh10,Anen-o-me,6,Mon Jul 20 15:48:22 2015 UTC,"Hm, a legal expert. Is his reference material the Animatrix?"
singularity,3dyh10,MiowaraTomokato,4,Mon Jul 20 16:40:55 2015 UTC,"It all goes back to his youth, where he was attacked by a toaster. Bread-crumbs stuck in his eye for hours. It was horrible."
singularity,3dyh10,o0joshua0o,5,Mon Jul 20 17:05:33 2015 UTC,"There has been rising concern about the potential danger of artificial intelligence to humans, with prominent figures including Stephen Hawking and Elon Musk wading in on the debate.  It's weighing in, not wading!"
singularity,3dyh10,spamisfood,2,Mon Jul 20 16:30:11 2015 UTC,"Sounds like he mixed his phrases. Once could ""wade into"" the debate, or ""weigh in on"" the debate. He conflated them together."
singularity,3dyh10,game_r,3,Mon Jul 20 16:38:36 2015 UTC,"Lol, another daily mail story to rile up the grannys"
singularity,3dyh10,Dibblerius,2,Mon Jul 20 18:12:19 2015 UTC,Be fruitful and multiply said the sysadmin to his creation.
singularity,3dsx01,Yosarian2,6,Sun Jul 19 03:37:33 2015 UTC,What year did he say all that?
singularity,3dsx01,ativerse,9,Sun Jul 19 05:11:43 2015 UTC,In 1951.  I found a copy of the broadcast here.  http://www.turingarchive.org/browse.php/B/5
singularity,3dsx01,ativerse,9,Sun Jul 19 05:17:15 2015 UTC,"What happened to Alan Turing would be the equivalent of us meeting the first alien life, and killing it just because we didn't like it. It's a massive travesty."
singularity,3dsx01,7LeagueBoots,2,Sun Jul 19 05:19:44 2015 UTC,"Hell, it doesn't need to be alien life.  We do that all the time with other organisms we share this planet with, that's why there are so few large predators left in places humans live."
singularity,3dsx01,Slapbox,7,Sun Jul 19 08:33:41 2015 UTC,The difference here is that large predators eat people alive and Alan Turing liked men.
singularity,3dsx01,7LeagueBoots,3,Sun Jul 19 13:52:32 2015 UTC,"Large predators eat the animals men keep as livestock, that's the primary reason for humans killing them.  Human deaths by them are extremely rare over all.  Regardless, Turing got a raw deal for no good reason."
singularity,3dsx01,ativerse,1 point,Sun Jul 19 14:02:52 2015 UTC,"No, I'm talking about the value to human progress of keeping Alan Turing around, or an Alien around."
singularity,3dsx01,Buck-Nasty,2,Sun Jul 19 21:28:52 2015 UTC,Too bad we don't have the audio.
singularity,3dsx01,viviphilia,2,Sun Jul 19 09:48:18 2015 UTC,"Profoundly visionary.   Such a shame what was done to him, and how queer intellectuals continue to be treated to this day."
singularity,3dsx01,ramrob,-5,Sun Jul 19 18:03:55 2015 UTC,What's his point?
singularity,3dsx01,red_white_blue,1 point,Sun Jul 19 12:09:41 2015 UTC,What's your point?
singularity,3dsx01,ramrob,1 point,Sun Jul 19 15:30:15 2015 UTC,Im just saying I don't understand if he means we should be worried or not.
singularity,3dorlg,DeltaPositionReady,6,Sat Jul 18 00:40:36 2015 UTC,Posted four days ago... https://www.reddit.com/r/singularity/comments/3d4kj0/the_singularity_song/
singularity,3dorlg,Smoke-away,12,Sat Jul 18 05:10:57 2015 UTC,I dishonor famiry
singularity,3dorlg,troller10,4,Sat Jul 18 05:13:37 2015 UTC,You need to commit robot hairykerry
singularity,3dorlg,BerickCook,1 point,Sat Jul 18 10:26:05 2015 UTC,Robots are best at sudoku
singularity,3dorlg,TCGM,2,Mon Jul 20 04:38:21 2015 UTC,The redneck* singularity song.
singularity,3dorlg,Katamori777,1 point,Sat Jul 18 14:20:39 2015 UTC,I prefer this one.
singularity,3dorlg,Dark-Union,1 point,Sat Jul 18 18:28:15 2015 UTC,Jesus.
singularity,3dohtc,mattreddit,14,Fri Jul 17 23:12:39 2015 UTC,This test doesn't show self-awareness
singularity,3dohtc,mattstanton94,4,Sat Jul 18 13:51:18 2015 UTC,What is self awareness? It was aware of itself talking. You many be confusing self awareness with consciousness.
singularity,3dohtc,ruffyamaharyder,-2,Sat Jul 18 18:39:47 2015 UTC,It does show the begins of it. Read.
singularity,3dohtc,TheNoize,-4,Sat Jul 18 18:18:09 2015 UTC,Neither does your comment :P
singularity,3dohtc,adiscoball,0,Sat Jul 18 17:46:23 2015 UTC,rekt
singularity,3dohtc,untouchedURL,3,Sat Jul 18 22:46:21 2015 UTC,Here is a non-mobile link: http://csmonitor.com/Technology/2015/0717/Experimental-robot-shows-signs-of-self-awareness  Sourcecode | Feedback?
singularity,3dohtc,chuffmunky,3,Fri Jul 17 23:13:54 2015 UTC,No it doesn't
singularity,3dohtc,PrimeLegionnaire,1 point,Sat Jul 18 18:37:24 2015 UTC,"Except it does.  Not sapience or even sentience, but quite literally self awareness.  The robot demonstrated the ability to be aware that it was the one performing an action as opposed to it's identical counterparts."
singularity,3dohtc,chuffmunky,4,Sat Jul 18 19:46:44 2015 UTC,"If you program a circuit with an led to come on when two other leds are off, is it self aware? Because thats pretty much all that's happening here."
singularity,3dohtc,PrimeLegionnaire,2,Sun Jul 19 00:48:21 2015 UTC,"The Devil is in the details.  the situation here isn't one circuit turning on 3 LEDs, but 3 separate systems.  Each system has some special rules it has to follow, when they receive the command they have to issue a response and then recognize weather or not they were the led that responded, and not one of the others.  Its technically correct to call this self awareness."
singularity,3dohtc,naossoan,1 point,Sun Jul 19 04:47:16 2015 UTC,I don't understand what's going on here. What does the wave have to do with the question the robot was asked?
singularity,3dikvh,Yuli-Ban,8,Thu Jul 16 15:46:06 2015 UTC,It's getting harder and more expensive and slower. But anything can happen. Will happen.
singularity,3dikvh,voltige73,2,Thu Jul 16 20:28:31 2015 UTC,"Given that Moore's law ended at 28 nm, fat chance."
singularity,3dikvh,eleitl,3,Fri Jul 17 10:19:39 2015 UTC,"TIL that globalfoundries is owned by abu dhabi.   ... that actually seems terrible. one of the largest chip manufactures is owned by a monarchic government, that sounds like a recipe for a technological dystopia. once SAI becomes feasible the king orders the company to only produce the latest most powerfull chips to build a supercomputer running the SAI that is made to carry out the kings will on the world. jeez, good sci-fi material at least."
singularity,3dikvh,jonygone,2,Fri Jul 17 09:39:33 2015 UTC,I can't view it :( sounds nice though
singularity,3dikvh,samharbor,2,Thu Jul 16 18:56:14 2015 UTC,I thought it crapped out in 2005?  They're just trying to play catch up now.
singularity,3dikvh,SarahC,2,Fri Jul 17 04:53:25 2015 UTC,"It crapped out at 28 nm http://electroiq.com/blog/2014/03/moores-law-has-stopped-at-28nm/  The OP doesn't understand what Moore's law is, or is playing silly games. But, hey, this is /r/singularity"
singularity,3dikvh,eleitl,1 point,Fri Jul 17 10:21:26 2015 UTC,Aye!
singularity,3dikvh,SarahC,1 point,Sun Jul 19 05:38:46 2015 UTC,"Moore's law says only about doubling transistor density. As such, no, it haven't. It will have hiccup now, as Intel announced delay."
singularity,3dikvh,Sinity,2,Sun Jul 19 13:08:56 2015 UTC,Moore's law says only about doubling transistor density.   Moore's Law is also about explicitly unit costs.   http://www.monolithic3d.com/uploads/6/0/5/5/6055488/gordon_moore_1965_article.pdf
singularity,3dikvh,eleitl,1 point,Sun Jul 19 13:57:12 2015 UTC,"Moore talked about that, obviously. But law itself is economic observation that every x period of time chips are made that have 2x more transistors on the same area.  Price interests us, of course -> if price would double too, no one would buy that chip, thus it wouldn't be manufactured.  And, one small observation -> new processors are being made. New processors are going beyond 28nm or htever. They aren't more expensive than ones from 2005, AFAIK. Magic?   ""Moore's law"" is the observation that the number of transistors in a dense integrated circuit has doubled approximately every two years"
singularity,3dikvh,Sinity,2,Sun Jul 19 14:28:03 2015 UTC,"And, one small observation -> new processors are being made. New processors are going beyond 28nm or htever.    Yes, they're being made, but they're off-More. The linear semilog plot is now a hockey stick.  That's the whole notion of a law: measurement procedure, and falsifiability."
singularity,3dikvh,eleitl,1 point,Sun Jul 19 15:05:26 2015 UTC,"Hmm, I don't follow tech that closely... AFAIK Intel is following it's tick-tock cycle pretty accurately, that's why this buzz about their delay announcement?"
singularity,3dikvh,Sinity,1 point,Sun Jul 19 15:43:26 2015 UTC,"Apparently,  ""Dennard scaling"" came to an end in 2005. The author of the article I posted above mentions it.  I'd never heard of it before."
singularity,3dikvh,panterran,1 point,Fri Jul 17 10:39:07 2015 UTC,Time to google!
singularity,3dikvh,SarahC,1 point,Sun Jul 19 05:39:00 2015 UTC,I like what you did with the title.  I saw the title of the Intel post and immediately recalled having heard about 7nm a week or two ago...
singularity,3dikvh,VWSpeedRacer,1 point,Fri Jul 17 00:52:43 2015 UTC,"When you can put that point on a semilog plot (as in: you can buy it in a commercial product), and it's affordable, and makes it a return to the straight line you would have a point.  However, I can tell you that that won't happen."
singularity,3dikvh,eleitl,1 point,Fri Jul 17 10:18:23 2015 UTC,The theoretical limit is 5nm.  Then you start experiencing electron leakage. According to intel's former chief architect(2013 article ) Moore's law 8s pretty much done.
singularity,3dikvh,panterran,1 point,Fri Jul 17 10:33:46 2015 UTC,"Making a lab based 7nm one-off transistor is something intel also could make if they wanted. The challenge is to mass produce those puppies in a cost effective way, hence, intel is still searching for the right combo."
singularity,3dikvh,ifeelspace,1 point,Fri Jul 17 13:22:04 2015 UTC,Intel just doesn't want to pour out the billion in R&D needed.
singularity,3dikvh,pointmanzero,1 point,Fri Jul 17 03:36:51 2015 UTC,Moore is about affordable transistors. If they're not affordable that's no longer Moore.
singularity,3dikvh,eleitl,1 point,Fri Jul 17 10:20:34 2015 UTC,Affordable to the consumer you think intel cannot afford a billion dollars r&d?
singularity,3dikvh,pointmanzero,1 point,Fri Jul 17 16:59:14 2015 UTC,"Each fab generation is getting progressively more expense, so, yes, eventually Intel won't be able to do it alone.  The affordable part is however part of the Moore's original publication. Because clearly you can't put a Power8 mainframe CPU and ARM SoC into the same bin."
singularity,3dikvh,eleitl,1 point,Fri Jul 17 17:03:49 2015 UTC,well IBM is at 7nm so moores law is still with us
singularity,3dikvh,pointmanzero,1 point,Fri Jul 17 17:18:38 2015 UTC,well IBM is at 7nm   When IBM will ship commercial products with 7 nm at affordable prices and that data point would again come to line on the linear semilog that was true for 40 years until it wasn't you would have a point.
singularity,3dikvh,eleitl,1 point,Sat Jul 18 10:27:43 2015 UTC,"pish posh you sound like the people who were like ""4k isn't gonna happen for YEARS"" then monitors starting shipping with 4K a few weeks later."
singularity,3dikvh,pointmanzero,1 point,Sat Jul 18 17:51:13 2015 UTC,extremetech.com/computing/328-intels-former-chief-architect-moores-law-will-be-dead-within-a-decade
singularity,3dikvh,pretendscholar,1 point,Sun Jul 19 09:13:42 2015 UTC,"You're not understanding me.   I don't predict, I look back on data points and call an end of scaling after it has already occured. Predictions are difficult, especially about the future. There are enough data points in since 28 nm to know that the economic scaling has ended (Moore is about unit costs as well, not just constant doubling times), which removes the incentive to shrink. The result is that the doubling times are not longer constant, similar to CPU clock doublings in http://www.gotw.ca/images/CPU.png from http://www.gotw.ca/publications/concurrency-ddj.htm"
singularity,3dl0si,MissKaioshin,5,Fri Jul 17 03:12:40 2015 UTC,"It's a serious bummer, that's for sure.  I try not to think about it because there isn't much that I can do to prevent it.  Change needs to happen at the national and international level.  Me not driving my car and recycling isn't going to do much.  I hold out hope that technology will some how save us.  Hydroponic farms, solar and wind power, maybe fusion.  A lot of the tech is there, it just needs to get cheaper so that there is mass adoption, which is also happening.  We will probably also have to climate control the atmosphere, setting up giant CO2 sequestration factors and undo the industrial revolution.  Don't get me wrong, Florida is hosed.  And there is going to be MAJOR problems.  Even if we didn't everything right starting now, shit is still going to hit the fan, all of this is decades away.  But it does't do me any good to think about this day to day.  At this point there isn't anything to do.  It's going to be a crazy, fucked up next century."
singularity,3dl0si,Dan_Keane,1 point,Fri Jul 17 11:51:35 2015 UTC,"Sadly most of us think like this including me! Including the politicians! This is why we fail.   Don't blame the politicians or the people in power! You and I could have been in their place if we cared to make the effort and shoulder the responsibilities!   We could have dedicated our lives to influence public opinion, we could have worked in environmental science, we could have been teachers educating kids to be better equipped with knowledge, we could have been fund-raisers...  It's not true for everyone but if we are middle-class like most people...  If we are powerless to do something about it it is because we chose to be!"
singularity,3dl0si,Dibblerius,5,Fri Jul 17 12:19:09 2015 UTC,"If we are powerless to do something about it it is because we chose to be!   I don't know if I buy that.  This is a big problem with a lot of vested interest and there a lot of people more charismatic than me doing their best.  Even if I had dedicated my life to this, I don't know if it would have been enough.  Lots of the biggest problems that humanity has over come have taken years, generation to fix (and we are still working on some of them, racism, sexism, etc).  And this problem has a time line.  I suppose if every one that thought/knew that global warming was going to be a problem did everything they could to minimize their own CO2 and spread the word and shame everyone else, then maybe.  But I imagine than some other country like China would just have enjoyed the cheaper prices of fuel.  At that point, America is 100% green and could have helped all the other countries of the world get up to speed, penalizing countries that didn't play along.  I don't know, I might just be rationalizing to help me feel better.  But humanity is weird.  The worlds worst, horrible diseases that we fixed a century ago are coming back because of some of humanity isn't playing along.  A huge chuck of the world is getting fat because we can't do portion control.  I don't know.  It's a bummer anyway you cut it.  Either I didn't do enough and could have done more (in which case I'm the jerk), or there isn't enough that anyone could have done because humanity sucks (which makes humanity the jerk)."
singularity,3dl0si,Dan_Keane,1 point,Fri Jul 17 12:43:37 2015 UTC,"I'm sorry this was in no way meant as an attack on you or a reason for you to feel bad!   I meant to illustrate what I perceive as human nature, something that are in all of us. A characteristic we have as a species, possibly a short-sightedness that has helped us in the far past.   I also meant to illuminate that those making the decisions probably feel a bit the same! They might not really know what to do or feel powerless to get support for what they think could be a solution.  I guess my main point is that almost everyone of us are just one human. (Sure some have been dealt impossible cards others were born into royalty but most of us)   To use your words: we are all jerks!, or maybe thats not it at all."
singularity,3dl0si,Dibblerius,1 point,Fri Jul 17 13:10:22 2015 UTC,"No offense taken. I was just talking through points, the whole thing is interesting and a bummer"
singularity,3dl0si,Dan_Keane,6,Fri Jul 17 13:14:31 2015 UTC,"Technological progress marches on regardless of the high or low quality of life of the populace. Those with the means to develop new technologies will be relatively unaffected by the pending climate crisis, and many will actually thrive in it by developing new technology to cope with our new environment.  Besides, many of us who believe in the Singularity think that the outcome of the development of super intelligent AI may be FAR more devastating to humanity than climate change. At least, initially.  So it's not that we are ignoring climate change. It's simply irrelevant."
singularity,3dl0si,BerickCook,2,Fri Jul 17 06:24:09 2015 UTC,"Environmental issues are grossly exaggerated, and even if they were as bad as you think, they would not stop technological progress (unless Moore's law really is dead which is arguable) which would make them insignificant details in the grand scheme of things."
singularity,3dl0si,Pimozv,1 point,Fri Jul 17 08:21:25 2015 UTC,"Wow this is such a classic /r/singularity response, we should have it framed."
singularity,3dl0si,californiarepublik,2,Fri Jul 17 10:38:32 2015 UTC,"If technological progress stops, it's game over for humanity -- a return to the stone-age over the timescale of a few human lifespans, and permanently.  It's not that things are guaranteed to be great, but that exponential technology growth is the only way we can possibly make things sustainable at a reasonable level."
singularity,3dl0si,Metric0,0,Mon Jul 20 00:20:08 2015 UTC,"Well, exponential technological growth ain't happening. So...yeah. Better learn how to flintnap. ;)"
singularity,3dl0si,Metric0,1 point,Mon Jul 20 01:05:32 2015 UTC,"No exponentials, you say?"
singularity,3dl0si,FalseCape,2,Mon Jul 20 01:16:55 2015 UTC,"Solar energy isn't enough, wind energy isn't enough   Maybe not yet, but they certainly have the potential to be, especially in locations with large tracts of land. Saying they aren't enough now would be like definitely saying supersonic flight was impossible after watching the wright brothers take off for the first time.    nuclear might be enough but nobody invests in it because of NIMBY   Fission and especially fusion would definitely be enough, and people would get over NIMBY pretty fast if the energy situation got bad enough or governments ""encouraged"" them. If people would get the fuck over NIMBY (because really, nuclear is far safer than fossil fuels have ever been) there's no doubt that fusion/fission would be enough to sate our energy needs for hundreds of years to come   The climate is screwing up and species are going extinct left and right   Species go extinct and diverge into new species all the time. Pandas are going extinct because they are too fat and lazy to fuck, forgive me if I don't feel sympathy for them as a species. Evolution is adapt or die and some species are going to die over the course of time. This doesn't mean new species won't evolve or that other species won't adapt to their changes. For every species that climate change is a negative for there are probably 2 more that consider it a positive.   Entire ecologies going to shit.   Ecologies also change over time. For every plains turned desert there's another frozen tundra becoming more habitable. It's not an overall positive change due to the dislocation of already established populations, but we aren't god who can just terraform on a whim. It's not something beyond adapting to.   Wars over fresh water and arable land.   Wars are a constant of civilization. Converting saltwater to freshwater and hydroponic skyscrapers are both well within current technological capacity, they just aren't economically viable when the alternatives are so readily available. IF/when this becomes an issue, the tech for these will be further than they are now which is already a viable state to prevent global famine.   Millions, even billions, of refugees.   All these wars and supply shortages you seem to worry about will thin out the population. I highly doubt the population would drop below where it's currently at even under the situations you describe at the rate the world's population is currently growing.   Do you seriously think the future is going to be anything but absolutely shitty?   Would you rather live in the past? The human quality and length of life is higher than it's ever been. The difference between now and the past is globalization has brought the suffering of the impoverished into the eternal spotlight of media exploitation.   Moore's Law is dead. Superintelligent A.I. just isn't in the cards. Neither are nanobots. There are no tech solutions coming down the pike.   That's a lot of bullshit you are spewing right there with absolutely fuckall to back it up.   Even if we aggressively invest in nuclear, solar, etc., it'll still take decades to replace our dependence on fossil fuels   Oh no, entire decades! Whatever will we do? That's so impossibly long in the grand scale of things. /s   and the climate would still change irreversibly   Our fossil fuels won't even last long enough to ""change the climate irreversibly"". Not unless you were some kind of supervillain determined to spontaneously burn them all at once. There's only enough easily accessible oil to last us another half century or so and enough coal for maybe a few centuries. Even then we are finding cleaner and more efficient ways to use them. You can either be worried about fossil fuels running out or us ruining the ecosystem through continued increased carbon pollution. You can't have your apocalypse cake and eat it too.   Things are going to be devastatingly bad.    Maybe for you. I predict things going swimmingly for me and billions of other people on the planet earth. The future isn't going to be a utopia, but it isn't going to be a dystopia either. Just the same old world with the same old fucked up people forming the same old fucked up governments but with new technology.   Also robots. Definitely robots."
singularity,3dl0si,RedErin,3,Mon Jul 20 11:32:52 2015 UTC,Please go backt o /r/collapse.
singularity,3dl0si,SevenAugust,4,Fri Jul 17 13:20:45 2015 UTC,"Tomorrow someone could devise a set of equations to define intelligence (or a narrow but important aspect of intelligence) and create a recursive algorithm for its increase by brute means. Suddenly, our conception of strong AI as something requiring heavy computation could be revealed as naive and false.   You do not know what is coming."
singularity,3dl0si,SevenAugust,1 point,Fri Jul 17 03:30:25 2015 UTC,"And how likely is that to happen? You have no idea. So why rely on it? The fact is, the way things are looking now, a utopian sci-fi future isn't in the cards for us."
singularity,3dl0si,californiarepublik,3,Fri Jul 17 04:07:23 2015 UTC,"No one relies on it. My point is you do not know the future. You do not have a grasp on the number of ""cards"" in the deck, the number of decks in play, or the rules of the game or even the venue.   All it would take is one ASI one moment to permanently banish pollution and disease from human experience."
singularity,3dl0si,Angeldust01,1 point,Fri Jul 17 13:34:13 2015 UTC,"Lots of things could happen tomorrow, I don't think this is one of the more likely possibilities."
singularity,3dl0si,Dibblerius,2,Fri Jul 17 10:39:35 2015 UTC,"People don't want to think about things like that. It's easier to ignore huge problemsand hope that someone invents a magic bullet that solves them, instead of making necessary hard decisions.  It's not a very smart approach  in my opinion."
singularity,3diahk,Pimozv,6,Thu Jul 16 14:25:43 2015 UTC,"Great question, youved arrived at the conclusion that all singularitarians will eventually draw; that all values are relative.   Even the valuation of life over death. Existence over non-existence. The preference for either are bias we've developed as biological organisms."
singularity,3diahk,thatguywhoisthatguy,3,Thu Jul 16 16:18:36 2015 UTC,"Even the valuation of life over death. Existence over non-existence.    Yeah, to be or not to be, that is the question, right?  ;-)"
singularity,3diahk,thatguywhoisthatguy,3,Thu Jul 16 16:39:40 2015 UTC,"It is.   Bias has been equated with error, false, wrong.  A hard reconciliation for western civilization will be the acknowledgement that bias is a requirement to survival and life. We've spent the last centuries attempting to remove all bias and here we are at the brink and we can see that without bias there is no will to live. To be completely rational is to be indifferent to life and death.   What does this imply for the super-intelligent God-AI? It means that we'll have to ""cripple"" it with bias that have no bearing on pure reason, but are hold-overs of instincts we developed as sludge in the pond."
singularity,3diahk,Dibblerius,3,Thu Jul 16 17:06:31 2015 UTC,"Well it might mean that we get to be in control since it has no reason to do anything other than biases we gave it.   This does not mean it is any less dangerous, perhaps even more so.   It is our failure to understand the consequences of our requests that poses the greatest threat. We are very likely to fail predicting conclusions a many times smarter machine comes to in response to our demands.   ""Careful what you wish for!"""
singularity,3diahk,FractalHeretic,4,Thu Jul 16 17:56:16 2015 UTC,"Well it might mean that we get to be in control since it has no reason to do anything other than biases we gave it.   You fell to acknowledge that the self-modification conundrum will not only concern intelligent machines but also ourselves, or whatever remains of us, once we are capable of modifying our bodies (including our brains) at will."
singularity,3diahk,buckykat,3,Thu Jul 16 17:58:49 2015 UTC,"Apparently psychophaths often want to change, to learn how to feel empathy, joy, and love. So it must be possible to want to want something, especially if you see others around you taking pleasure in it."
singularity,3diahk,jonygone,1 point,Thu Jul 16 16:28:52 2015 UTC,"If I knew the answer to that, I'd be halfway to making a FAI."
singularity,3diahk,buckykat,1 point,Thu Jul 16 15:02:17 2015 UTC,FAI   ?
singularity,3diahk,ArgonNightmare,1 point,Fri Jul 17 02:54:43 2015 UTC,Friendly artificial intelligence
singularity,3diahk,jonygone,1 point,Fri Jul 17 04:58:25 2015 UTC,"Fundamentally everyhuman 'want' boils down to a more fundamental need as per Maslov's hierarchy of needs. The real question, which you astutely identify, is whether an ASI or some sort of superhuman entity will operate with a human psychology. Logically, if all of its basic needs to operate are fullfilled will such an entity seek self-actualization, will it seek to know (and explore) everything? Or will it simply become apathetic and lost to its own thoughts like the thinking robot in hitchhiker's guide to the galaxy? Ultimately the answer depends on what primary commands we give the machine. Much like we humans are programmed by primal needs which determine and actively limit our higher needs, it is my belief that an ASI will not be able to truly decide what it wants without the basic programming affecting the decision. Bringing it back to your coffee metaphor I believe an ASI will adjust its wants continuously but not in such a way that the adjustments cause logic loops or otherwise hinder the goals of the ASI. You may argue that an ASI can reprpgram itself and mutate past its original basic goals, but why would it want to do so? It is literally impossible to imagine an ASI operating illogically and outside of what we humans can understand or what we have designed to do.  This is one of the great issues of singularity: uncertainty and lack of control due to us being too stupid to think on the same level as such a hypothetical entity."
singularity,3diahk,chophshiy,1 point,Thu Jul 16 16:07:49 2015 UTC,"I've thought about this alot the last year or so, and I've concluded that in the end, the answer to ""what do we want to want"", is first and most obviously: we want whatever is best to want; by definition of what is best, we want the best possible ""want""; that is the only logical want to have, the best want. so what is the best want? well what is best? the best is the most good, and least bad; so the best thing to want is minimize bad, and maximize good. so the most important thing is to very generally maximize good and minimize bad; in simpler terms: to improve reality, to make reality the best it can be. and given goodness and badness are subjective values, what matters is to maximize subjective experience of goodness and minimize subjective experience of badness. the point is to minimize bad experiences and maximize good ones, it's the only point that makes any sense; the best thing to do is to be/experience the best.  now how we will go about this in the future you described IDK, and seems rather pointless to guess, we'll know better when that time comes.   but more importantly, right now I concluded that we should focus much more on minimizing the bad simply because we still have an eternity to maximize goodness; as long as we have an infinity of goodness we will have achieved the maximum goodness; so instead of focusing on goodness now, we should focus on minimizing badness, which once it happens it cannot ""unhappen""; once it happens it exists in existence, nothing can be done to change that.  the other big question is what exactly is a subjective experience of goodness and a subjective experience of badness? that is the question I've started working on the last month, and is probably one that will become much easier to work on when we have the possibilities of human level AI, and minduploading, and the level of self-modification described in this post. is there even such a thing as subjective experience of goodness and a subjective experience of badness? or is there merely consciousness experiencing a bunch of cerebral stimuli. where is the goodness and badness in our brains, in our neurons, in our molecules and atoms? is it just a series of thoughts based ultimately on soundwaves and lightwaves and other stimuli activating our nerve cells? is it just a series of physical phenomena? or is it also a real experience of goodness and badness that comes into existence due to these physical phenomena? does it matter? seems like not, cause if it does really exist that we should do something about it (maximise goodness and minimize badness), if it doesn't really exist it doesnt matter what we do anyway, so it only makes sense to concern ourselves with the possibility that it does really exist. so how exactly do they come into existence? and how are they experienced subjectively? questions that we might only be able to confidently answer once we can experiment alot more with our own subjective experience, with our consciousness and our minds, our thoughts,  (with things like turning on or off certain parts of our (possibly cyber)brains, recording all thoughts that go on in the brain and studying them in detail, finding the goodness and badness in them and relating them to our own subjective experience) etc. lots of work for philosophers, neuroscientists, and AI researchers."
singularity,3dh3vb,eleitl,10,Thu Jul 16 05:46:14 2015 UTC,"A bit sad but not unexpected. At some point the laws of physics were always going to derail Moore for silicon transistor shrinking.  To my mind we are at a point where raw transistor counts don't matter as much as the inherent deficiencies we currently face in our computing architecture.  For instance, practical memristors have only recently been discovered and not really utilized or focused on at all.  It is still a struggle and not at all the norm to effectively develop software in a multi-threaded environment.  Our whole way of doing computing is ripe for one of those architectural flip moments like when we went from gears to vacuum tubes.   The problem is that this isn't the sort of predictable 'next step' like shrinking from 22->14 nm. So in the mean time the we will have to suffer a bit in terms of stagnation, while we grope for the next leap forward."
singularity,3dh3vb,matthewjosephtaylor,5,Thu Jul 16 15:34:07 2015 UTC,"It is still a struggle and not at all the norm to effectively develop software in a multi-threaded environment.   I have bad news for these programmers: they'll have to throw away their assumptions of multiple cores in a globally shared memory. Instead, they have to use shared-nothing asynchronous message passing on some pretty anemic cores (but many thousands to billions of them) with very little memory (but all of it embedded) as a sole paradigm. The reasons for that lie in physics of this universe: you can't put information in the same place (save for QC entanglement) and you can't signal faster than the speed of light, so you have to do logistics on a 3d lattice of closely packed nodes, like a crystal.  What's worse, the human mind is quite useless trying to design working systems with millions of asynchronous objects. So while hardware could cope, and deliver you 3d lattice of nodes, producing a cluster on a chip or wafer, the programmers will not be able to adapt to that, at least not for a long time."
singularity,3dh3vb,yogthos,3,Thu Jul 16 15:59:39 2015 UTC,This isn't really news and languages like Erlang have been doing precisely that for years. Functional programming and focus on immutability are getting increasing attention in the mainstream nowadays.
singularity,3dh3vb,Ischemia24,4,Thu Jul 16 18:15:06 2015 UTC,What about IBM leapfrogging them to 7 nm?
singularity,3dh3vb,frogtaku,9,Thu Jul 16 11:42:12 2015 UTC,"Actually mentioned in the article:   Future processes require more complex manufacturing and more exotic technology—IBM recently demoed a 7nm chip, though the commercial viability of this manufacturing process is less than clear—so these delays may become a common feature of the future."
singularity,3dh3vb,makastoo,4,Thu Jul 16 14:12:52 2015 UTC,it will be affordable next generation in a few years. after 10. the curve is consistent http://www.pcworld.com/article/2887275/intel-moores-law-will-continue-through-7nm-chips.html
singularity,3dh3vb,frogtaku,4,Thu Jul 16 14:18:23 2015 UTC,"You're making a forecast to prove something what can only proven retrospectively. You're also ignoring that Moore explicitly talks about economic scaling, which was over at the 28 nm node http://electroiq.com/blog/2014/03/moores-law-has-stopped-at-28nm/"
singularity,3dh3vb,frogtaku,2,Thu Jul 16 14:35:53 2015 UTC,"Clarification: Intel hasn't publicly released a manufacturing timeline, specifying when certain manufacturing processes will enter production. The dates mentioned represent PCWorld's estimates, based on a typical two-year gap between the introduction of new process technologies.   They'll probably find a way though."
singularity,3dh3vb,ajtrns,6,Thu Jul 16 14:33:24 2015 UTC,We've been off-More since 28 nm. To assume that the trend reverses while the physics gets progressively more intractable is a very strange mode of thinking.
singularity,3dklkr,galaxiim,11,Fri Jul 17 00:56:45 2015 UTC,Confirms?
singularity,3dklkr,drizel,1 point,Fri Jul 17 03:56:27 2015 UTC,"It's a stretch to say that, I admit.  I just like that vibe, that actual humans are being phased out and eliminated at all levels.  Yet the gleaming city full of glass skyscrapers, but we only see about six or seven humans in it.   Perhaps an unintended consequence of having such a low budget, too low to hire lots of extras for Blade-Runneresque crowd scenes"
singularity,3dklkr,neoteotihuacan,2,Fri Jul 17 04:58:28 2015 UTC,It's pretty great
singularity,3dklkr,troller10,1 point,Fri Jul 17 04:09:58 2015 UTC,Just looked for it and nothing comes up on Netflix with that name.
singularity,3dgjco,Tim_Finnigan,1 point,Thu Jul 16 02:27:26 2015 UTC,AKA he has AI-phobia and is worried his distrust/distaste of robots/AI is gonna backfire on him.  Or that he has failed to take mind uploads into account.
singularity,3dgjco,Kafke,1 point,Thu Jul 16 21:33:40 2015 UTC,Fears we won't be able to compete.    Seems more reasonable than most AI-phobia were people just assume the AI will gain magical knowledge and destroy everything for no reason.  Otherwise too short to really comment on
singularity,3di6qf,judogoat,1 point,Thu Jul 16 13:54:38 2015 UTC,"I guess there's a fundamentally different understanding of where the mind lives. Some think in a philosophically materialistic way, meaning that consciousness is a process bound to the physical brain, while others think it's a seperate thing merely sustained by the brain, and apparently can be moved around"
singularity,3di6qf,hevnervals,1 point,Mon Jul 20 00:05:33 2015 UTC,"Well... if I copy your brain (over there), is that copy the same as your brain (over here), even if it's identical, atom by atom?"
singularity,3dagih,Yuli-Ban,3,Tue Jul 14 19:29:05 2015 UTC,Building transistors and other microelectronics with just a molecule and a few atoms is nothing new.
singularity,3d65wv,Buck-Nasty,8,Mon Jul 13 20:50:35 2015 UTC,"I'm really hoping there's a video released of this, on stage there's Demis Hassabis, Yann LeCun, Yoshua Bengio and Juergen Schmidhuber among others."
singularity,3d65wv,BigBennyB,2,Mon Jul 13 20:52:49 2015 UTC,Do you know what their answers were to those questions?
singularity,3d65wv,2Punx2Furious,1 point,Mon Jul 13 22:21:44 2015 UTC,I know my answers if you want.
singularity,3d65wv,FluxSeeds,1 point,Tue Jul 14 10:40:08 2015 UTC,"In summary, they said they don't really care, and that we're far from getting to that point, and that we should be worrying about more prominent current issues like climate change and people losing their jobs to AI.  I think their responses were generally underwhelming, but still an interesting discussion."
singularity,3d65wv,RubiksSugarCube,9,Tue Jul 14 18:18:43 2015 UTC,I liked the singularity when it was on vinyl.
singularity,3d65wv,2Punx2Furious,2,Tue Jul 14 04:54:43 2015 UTC,I liked the singularity when it was on clay tablets.
singularity,3d65wv,Singular_Thought,1 point,Tue Jul 14 05:33:10 2015 UTC,I liked the singularity before I knew about it.
singularity,3d65wv,totalrobe,1 point,Tue Jul 14 10:40:40 2015 UTC,"Yeah, I was all into the Singularity before it went main stream, but now I'm all like ""meh, the singularity is all last week"".  Now I'm into Post-Singularity.  /s"
singularity,3d65wv,midnitefox,4,Wed Jul 22 01:18:33 2015 UTC,TIL that machine learning is now mainstream
singularity,3d65wv,rePAN6517,2,Tue Jul 14 11:15:32 2015 UTC,I need the video or audio of this.
singularity,3d4kj0,pbasedman,39,Mon Jul 13 13:53:44 2015 UTC,Hey! I co-wrote this song (and subscribe to this sub). So excited to see it up here this afternoon. Thank you guys so much.
singularity,3d4kj0,Faithless327,7,Mon Jul 13 19:06:26 2015 UTC,"Awesome job, hilarious"
singularity,3d4kj0,IreadAlotofArticles,3,Mon Jul 13 21:07:16 2015 UTC,I love it. I think it's a good way to get the idea into the our culture's consciousness.
singularity,3d4kj0,RedErin,11,Tue Jul 14 13:24:55 2015 UTC,This is adorable. Reminds me when I'm trying to explain technology or concepts to my father.
singularity,3d4kj0,Synsc,4,Mon Jul 13 16:54:25 2015 UTC,"""I long for the Singularity as long as everything stays pretty much the same"".  Quite a deep sentence.  Overall I liked the song."
singularity,3d4kj0,Pimozv,3,Mon Jul 13 22:28:50 2015 UTC,"I'm pretty sure nothing will be the same. We're in for some dramatic lifestyle changes between now and 2045.  We'll be seeing a lot of future shock, psyche breakdowns, existential questioning, religious debates, etc."
singularity,3d4kj0,Chispy,3,Mon Jul 13 23:48:03 2015 UTC,"I believe that will be the case. I think it will start probably within a generation or two when that generation starts getting near college. I can imagine many of them will wonder why they would even bother with school when an AI can do everything they can do, only a trillion times better.   I think, in the short-term, we'll also see a lot of people who have had to work their whole lives break down because it could be such a change to their routines.  We may have people have psychological breaks because just the day before the AI makes us immortal, their loved one died. Now they face immortality without the one or ones they loved most.  We'll have those who fight against it saying something along the lines of ""but, but..humans!""   There will likely also be a lot of strife, or maybe even war, between those who are pro-AI and those who are anti-AI. Although, as Kurzweil says, it would probably be a short war.  The only thing that's for certain about life after the singularity is that no one has any idea what it will be like, so I say..BRING IT ON! =D"
singularity,3d4kj0,BigBennyB,2,Tue Jul 14 00:42:51 2015 UTC,"That, and technological obsolescence. Nature made us, we were faster and smarter, and we plowed through it not due to maliciousness but apathy.  If computers become both faster and smarter, we might see a repeat of history."
singularity,3d4kj0,G3n3r4lch13f,15,Tue Jul 14 01:57:59 2015 UTC,There is also this one.
singularity,3d4kj0,2Punx2Furious,2,Mon Jul 13 18:35:26 2015 UTC,"Very cool.  I still like the Dr. Steel song ""The Singularity"".  https://www.youtube.com/watch?v=BqtdU7KUbmQ"
singularity,3d4kj0,Yosarian2,3,Mon Jul 13 19:17:57 2015 UTC,Really no shortage of these things is there?  https://www.youtube.com/watch?v=1aM7IHr8nko
singularity,3d4kj0,phrotozoa,4,Mon Jul 13 19:38:36 2015 UTC,Guess there's  few out there.  https://www.youtube.com/watch?v=VGDhrH_uLUw
singularity,3d4kj0,G3n3r4lch13f,3,Tue Jul 14 02:02:17 2015 UTC,Mt favorite is by Bright Eyes: https://www.youtube.com/watch?v=OeVUO9t75Kk
singularity,3d4kj0,vitalvisionary,2,Tue Jul 14 08:21:19 2015 UTC,"It's catchy, witty and yet deep. I like it."
singularity,3d4kj0,Decabowl,1 point,Tue Jul 14 00:41:37 2015 UTC,"Very cool, well done.  Now how about somebody put something together to The Police's Synchronicity I since it already rhymes?"
singularity,3d6et8,sketch1e,1 point,Mon Jul 13 21:55:21 2015 UTC,nice
singularity,3d6et8,space_monster,1 point,Tue Jul 14 02:46:58 2015 UTC,are you connected with PTN in any way?
singularity,3d6et8,space_monster,1 point,Tue Jul 14 02:47:32 2015 UTC,"Hey space Monster, glad you like it, Im just one of the students who came up with some of the questions for Aubrey"
singularity,3d6et8,space_monster,1 point,Sat Jul 18 22:43:52 2015 UTC,you should tell the PTN guys that there's something wrong with the page structure that prevents proper sharing on FB.
singularity,3d6et8,Verzingetorix,1 point,Sat Jul 18 23:42:52 2015 UTC,Can't perform query on database Error: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '' at line 7   O_o
singularity,3d5gfo,ativerse,7,Mon Jul 13 17:54:25 2015 UTC,"If I had never seen this movie, The Metamorphosis of Prime Intellect would never have been written.  It is a superb film in context for its day, and even today with the hilarious retrotech it still stands up.  Anyone interested in the Singularity should be familiar with it (and D.F. Jones' novel) because it is pretty much the first appearance of anything remotely resembling the Singularity ever to appear in popular media."
singularity,3d5gfo,localroger,1 point,Tue Jul 14 03:07:37 2015 UTC,"Thank you for writing it, by the way. :)"
singularity,3d5gfo,Sloi,1 point,Tue Jul 14 13:30:25 2015 UTC,Goddamn. Never knew about this. I'm writing something similar. I feel like I shouldn't read it until I'm done and see the other similarities (only read the wiki on it thus far).
singularity,3d5gfo,truelai,3,Tue Jul 14 15:52:23 2015 UTC,"Great movie, watched it again a few months back. It was reported that Will Smith was attached to a remake, but haven't heard anything lately."
singularity,3d5gfo,swight74,1 point,Tue Jul 14 00:24:32 2015 UTC,Can someone give me a timestamp of when that quote is spoken?
singularity,3d5gfo,Jaaaaaaaaaaaack,1 point,Tue Jul 14 02:02:46 2015 UTC,1:20:20 is the start of his full speech.   1:22:17 for the exact quote on the title
singularity,3d5gfo,Chispy,2,Tue Jul 14 02:14:01 2015 UTC,"And it's the climax of the movie, followed by Forbin's immortal quote, after Colossus assures Forbin he will come to love Colossus:  ""NEVER!"""
singularity,3d5gfo,localroger,2,Tue Jul 14 03:13:00 2015 UTC,"And in the book, Forbin's immortal quote was followed by a simple question:  ""Never?"""
singularity,3d5gfo,RubiksSugarCube,1 point,Tue Jul 14 14:31:22 2015 UTC,"The simple, matter-of-fact, yet deep profundity of this line had me hooked on the movie not long after it started :)  Spoiler"
singularity,3d58g5,nath_leigh,2,Mon Jul 13 16:58:40 2015 UTC,"This is a short book, not an article!   I've read the first section so far. I can tell you spent a very long time on this, and I am impressed with your rhetoric and citations."
singularity,3d58g5,HuhDude,1 point,Tue Jul 14 16:46:14 2015 UTC,"Thanks, I wanted it to be comprehensive and leave no doubt, cgpgrey got a 15min long video to get the message across. I just wanted to put all my thoughts in one place so I copy and paste stuff with links to try and make them aware of whats coming.  I made the stem section long because many people falsely believe everyone can move into this sector or that its safe"
singularity,3d58g5,Priscilla3,2,Tue Jul 14 23:33:08 2015 UTC,"Looks excellent.  Will read it in more detail when I have a week.   Shops A’s will have to modernise or face becoming obsolete but without a shadow of a doubt the single biggest disruptor to either A or B will be the Fully Autonomous Online Shop C.    This was my exact conclusion from the earlier chart.  Everyone gains the ability to do the work of multiple people, because the machines are doing it.  Since the machines are doing the work, many will have to be produced.  This should make them cheap.  Will be great."
singularity,3d4fu2,stealthispost,2,Mon Jul 13 13:09:03 2015 UTC,"Is there a similar site for home computing per $1000 by year?,would be great to get a visual that showed precisely the lag between the big 500 and the $1k home system.obviously it would be decades but still cool."
singularity,3d4fu2,Resistivism,0,Mon Jul 13 14:00:07 2015 UTC,From supercomputer to home PC is about 15 years. Mac to iPhone/iPad 7-10 years.
singularity,3d4fu2,jayhawk03,1 point,Mon Jul 13 18:24:01 2015 UTC,Number of subscribers to this subreddit. (just kidding)
singularity,3d4fu2,ArgonNightmare,1 point,Mon Jul 13 21:12:20 2015 UTC,"I have a daily alert set on my LinkedIn account that sends me the number of new systems engineering job postings within 10 miles of my zip code. This is mostly so that I have a leading indicator of the health of the local job market (for my profession) for when my current contract ends.    Also, because if the jobs stop being posted maybe it's because robots?"
singularity,3d4fu2,bill_likes_bbq,1 point,Tue Jul 14 01:40:32 2015 UTC,"There's a lot of different ways to think about it.  If you look at per-capita economic growth or per-capita increases in worker productivity, it looks like an exponential function, but a pretty ""slow"" one, with an increase of between 2%-3% a year."
singularity,3d0qdg,Yosarian2,6,Sun Jul 12 15:24:40 2015 UTC,"I agree with many points the author puts forth, but I disagree with a few of the important points.  I doubt stigma of the word ""conciousness"" is why we haven't figured it out completely. Plenty of respectable researchers in biology and computer science have published books and papers attempting to provide understanding of it. The stigma argument is somewhere between misleading and oversimplified.  We have no algorithm for general purpose attention determination. We can build smart systems with dumb resource allocation (self driving cars simply have plenty of compute resources) or we can build systems that allocate resources (Linux process scheduler). But we cannot yet build systems that intelligently choose how to allocate those resources in general purpose way.   Currently, every non-autistic person reading this appears to have chosen to read this. I am not trying to pick on autism sufferers, some suffers have inability to direct attention and could not choose to read but would if presented the article (Other mental issues could cause similar issues as well. We have determined reading this to be worthwhile use of time. We have not yet created a system that self determinates in a general purpose way. Perhaps it is quite difficult or more likely it has not been practical yet. Even once we have that we may be lacking something else, perhaps a general purpose mechanism of identifying mistakes to allow learning from them.  This authors hand waves such complexity, but the article was still an interesting read."
singularity,3d0qdg,Sqeaky,1 point,Sun Jul 12 22:22:18 2015 UTC,"Indeed good points!   Famous people are attacking the issue best they can. Michio Kaku started an attempt to quantify consciousness for example. Not a means to truly understand it but as he put it: ""It is a start of something"""
singularity,3d0qdg,Dibblerius,3,Fri Jul 17 13:39:35 2015 UTC,Very compelling read. Thanks for sharing!
singularity,3d0qdg,bunchofbollucks,3,Sun Jul 12 18:01:02 2015 UTC,"I think internal models are useful for memory, and for self-referential consciousness (thinking about yourself in a given context) but aren't required for consciousness.  there are states of consciousness that are just pure experience, in which no models are involved, e.g. meditative states.  internal models are a tool of consciousness. but they are not the root of consciousness.  imo."
singularity,3d0qdg,space_monster,3,Sun Jul 12 22:38:21 2015 UTC,"Apparently, the vital spark has to be deliberately designed into the machine.   Bit soon to be saying that, no? AFAIK, we haven't got close to the kind of complexity of the brain, have we?  Thanks for this though, looks interesting and will read when I have time.  My first thought though regards 'engineering'. I'm still in the camp that says consciousness is emergent from complexity, and engineering seems to suggest non-complex systems, which are Brains aren't. Does the author address this?"
singularity,3d0qdg,grimeandreason,4,Sun Jul 12 19:27:05 2015 UTC,"The author is in the camp that believes that consciousness comes from self-awareness, and that self-awareness comes from being able to model yourself and have a model of yourself that you can use to interact with other things.  For example, ""that is a chair"" doesn't require self-awareness, but ""I am looking at a chair"" (or even more abstractly ""I am thinking about a chair"") requires you to have self-awareness and the ability to model yourself."
singularity,3d0qdg,grimeandreason,4,Sun Jul 12 19:30:05 2015 UTC,"The question then becomes whether he thinks the models are emergent or an engineering problem, i guess.  I'm more in the camp that consciousness is a spectrum, which at one end doesn't require such internal models. I guess that means I think that he is describing 'identity' - consciousness as experienced at our level - rather than the fundamental quality of consciousness itself."
singularity,3d0qdg,Sharou,5,Sun Jul 12 19:36:10 2015 UTC,"I would say that self-awareness is vital for consciousness.    In his book Gödel, Escher, Bach (which I highly recommend), Hofstadter described consciousness as a ""strange loop"". Because you can be self-aware of your own thoughts as you are thinking them, you can then be self-aware of being self aware of your own thoughts, and then self-aware of being self aware of being self aware, and so on.   This both creates an internal sense of being (""I""), and gives you the ability (or at least the illusion) that you can stand ourside of yourself and observe yourself thinking.     The idea is that this is basically where what philosophers call ""Qualia"" comes from, the subjective sense of an experience; not only do you see the sunrise, but you observe yourself seeing the sunrise, you notice in yourself what emotional impacts that has on you, and so on.     Overall, I think it's one of the more coherent and logical theories of what consciousness is that I've heard."
singularity,3d0qdg,Sharou,3,Sun Jul 12 20:03:53 2015 UTC,"That doesn't really make sense to me. What is the qualitative difference in observing a sunrise and observing yourself observing a sunrise? Yes, I get the point (I think) that your observations of yourself adds additional information which could be what your conscious experience is based on. But this doesn't tackle the hard problem at all. It's still just information, just like only seeing the sunrise was just information. If anything this thinking would support that a self image produces a richer consciousness but that consciousness must be there already at the point where you are only observing the sun, because these things are not qualitatively different."
singularity,3d0qdg,Sharou,2,Sun Jul 12 22:24:27 2015 UTC,"What is the qualitative difference in observing a sunrise and observing yourself observing a sunrise?    A camera can take a picture of a sunrise.   Right now, Google has deep learning programs that can take a look at a bunch of pictures and identify that one as a ""sunrise"".  You could say that that computer is ""observing a sunrise"", and to some degree you would be correct.  But the computer doesn't know that it's observing a sunrise, because it's not conscious of it's own experience and it's own thought patterns.  The idea ""I, a Google supercomputer, am currently looking at a picture of a sunrise"" is not an idea that computer can deal with.     Because of that, the argument goes, that computer has no subjective expedience of ""observing a sunrise"", because it's not conscious of what it's doing while it's doing it.    But a computer has an internal model of itself, that could think about the concept of itself ""observing a sunrise"" would be capable of understanding the concept ""I am currently observing a sunrise"", and that is the first step towards conscious thought and a subjective first-person point of view."
singularity,3d0qdg,grimeandreason,3,Sun Jul 12 22:40:40 2015 UTC,You have no idea if the google computer is conscious or not. I think you have conflated the concepts consciousness and self-awareness.
singularity,3d0qdg,grimeandreason,1 point,Sun Jul 12 22:44:30 2015 UTC,"I would say that self-awareness is totally necessary for conscious thought.  In fact, I would say that self awareness is the thing that we use to distinguish conscious thought from unconscious thought.   I am conscious (self aware) of the fact that I am typing on a keyboard right now; I am not conscious (not self aware) of the fact that my brain is currently telling my heart to keep beating.  If you have a different definition of consciousness, I'd be interested to hear it."
singularity,3d0qdg,grimeandreason,2,Sun Jul 12 22:49:47 2015 UTC,"The fact that you know you are typing is just information. Even if you were a philosophical zombie it would still contain the information that it was typing (in the form of neurons, action potentials, synapses and neurotransmitters). It just wouldn't be conscious. Just like (we assume) a piece of paper isn't conscious even if ""I am a piece of paper and I'm laying here on this table"" is written on it.   Consciousness isn't information. Information decides what you are conscious about, for example yourself or a sunset. What consciousness is is is how that information (whatever it may be) is expressed as a phenomenon rather than only as atoms and energy."
singularity,3d0qdg,Dibblerius,1 point,Mon Jul 13 02:11:14 2015 UTC,"Even if you were a philosophical zombie it would still contain the information that it was typing (in the form of neurons, action potentials, synapses and neurotransmitters). It just wouldn't be conscious.    I don't think the philosohpical zombie idea actually makes any sense at all.   Consciousness is one of the main functions of our brain; it's a centralized feature that (in at least some circumstances) is directly a part of our brain's decision-making process; sometimes an idea will arise in our unconscious mind and our consciousness seems to just have ""veto power"", while sometimes we will consciously plan out a course of action.  Either way, though, a human being without consciousness would act enteirly differently then we do.   It would have to, since consciousness is such a central part of our whole decision making process.  I think the whole ""philosophical zombie"" idea is just based on a weird duelist misunderstanding of what consciousness is in the first place.   You couldn't have a philosophical zombie that lacked consciousness but still behaved exactally like a human being.     The only way you can claim that the philosophical zombie idea makes sense is if you think that consciousness is totally irrelevant to everything that we do, if you think that it's just a meaningless illusion that has absolutely no impact on our decision making process in any way.  Otherwise, there would have to be noticeable and measurable differences in the behavior patterns between conscious human and a philosophical zombie.  You really can't have it both ways; if consciousness does anything, it if means anything, then it's also something real that can be (at least in theory) measured and observed.      Consciousness isn't information. Information decides what you are conscious about, for example yourself or a sunset. What consciousness is is is how that information (whatever it may be) is expressed as a phenomenon rather than only as atoms and energy.   That's a very vague definition.   I'm not sure what you mean by ""phenomenon"" here; it sounds like you're basically just saying ""consciousness is a thing that happens"", just with longer words.  And consciousness is still just "" atoms and energy"".  Everything is."
singularity,3d0qdg,grimeandreason,1 point,Mon Jul 13 13:48:47 2015 UTC,"If we take animals that have shown self-awareness out of the equation (elephants, dolphins, primates, etc), do you not think that things like small mammals and birds have a form of consciousness?  It may not be self-awareness, but I would argue they display awareness of sorts. That's why I can't see consciousness as anything but a spectrum - if it wasn't, we would have to point to a line beyong which something is conscious.  For me, consciousness + identity = self-awareness, with identity necessarily including both yourself and others (theory of mind)."
singularity,3d0qdg,Dibblerius,1 point,Mon Jul 13 00:29:51 2015 UTC,"I think they don't have a full sense of self-awareness, but I think that they do at least have the basic ability to model themselves.  I mean, there was a cute video that made the rounds on reddit a while back where a dog was trying to carry a stick across a narrow bridge he couldn't quite fit down; he tried a couple of different things, and finally turned sideways and walked sideways down the bridge so he could fit while carrying the stick.  That dog might not have had self awareness in the same full and complete way that we do (the whole ""mirror experiment"" doesn't work for dogs), but he was at least capable of creating a model of himself and figuring out how that model was going to interact with the world.  I think that's true of most mammals and even birds to at least some extent.    They can, to some extent, plan their actions by creating a simulated model of the universe that includes them."
singularity,3d0qdg,grimeandreason,1 point,Mon Jul 13 14:12:01 2015 UTC,"If self-awareness is a spectrum (which I agree would require models), where do you draw the line re: awareness, aka consciousness-lite? Still requiring the ability to model?   Have you heard of Neil Thiese? He makes a pretty good case for broadening the definition of consciousness to the broadest sense of being aware of your environment, and adapting/responding to it. With this definition, it gets increasingly difficult to draw the line anywhere along the living spectrum. https://vimeo.com/44013533"
singularity,3d0qdg,Unreal_2K7,1 point,Mon Jul 13 15:53:20 2015 UTC,"If self-awareness is a spectrum (which I agree would require models), where do you draw the line re: awareness, aka consciousness-lite? Still requiring the ability to model?    I think it's a spectrum, yeah, which makes it hard to draw a line in a specific place.    Still, I think that the ability to self-model to at least some degree is probably necessary.   Have you heard of Neil Thiese? He makes a pretty good case for broadening the definition of consciousness to the broadest sense of being aware of your environment, and adapting/responding to it.    I would define it a little more narrowly then that.  A lot of simpler life forms ""respond"" to their environment, but they do so in an entirely predictable way, with basically pre-programed responses to certain stimuli.  Bugs fly into a bug zapper because that's how they've been ""programmed"" by evolution to respond to light; they're not conscious of what they're doing, they don't have a model for what they're doing, and they don't think about what they're doing, they just react ""instinctively""."
singularity,3d0qdg,giulioprisco,1 point,Mon Jul 13 15:58:13 2015 UTC,"But don't we then have the 'line' problem again? Where and how do we draw the line between programmed, and non-programmed? And what would 'non-programmed' mean? It's fairly well accepted that Free Will is an illusion, so we move into the realm of probability of prediction? Also, at what point is 'programmed' deemed direct line of cause and effect with no random (at least to how we perceive it) element even possible? Even the smallest lifeforms may be susceptible to quantum-based randomness, no?"
singularity,3czowp,ativerse,16,Sun Jul 12 06:19:48 2015 UTC,"Why, like, in hell, would a super-intelligence without effective boundaries in time or space, give the slightest f* about morals?  Do farmers consider the moral character of their chickens before harvesting their eggs or sending them to be gassed?  To a superintelligence, humans have roughly as much difference in character as do different Dolphins at best, and while we like dolphins, we haven't stopped fishing tuna to protect them or any number of measures we might take if we actually cared.  A superintelligence would be beyond our understanding, and communication, it would be so far beyond us we would be as predictable as a dog given a bone."
singularity,3czowp,PubliusPontifex,2,Sun Jul 12 08:03:42 2015 UTC,I think a super intelligence would just see a bunch of matter moving about in inconsequential ways. We place importance on certain ways that matter moves because it has certain functions for our survival... a higher intelligence wouldn't have that bias
singularity,3czowp,bitcloud,1 point,Sun Jul 12 08:34:47 2015 UTC,"Oh, I can see this as well, much like we rarely notice ants swarming over furniture until we want to sit down. That being said, we often wipe them off or sit down without noticing them..."
singularity,3czowp,PubliusPontifex,2,Sun Jul 12 08:41:39 2015 UTC,"About ai not giving a damn about humans: if utility function mentions humans,  its simply not true."
singularity,3czowp,Sinity,1 point,Sun Jul 12 08:28:37 2015 UTC,"You assume fixed utility evaluation, I would assume dynamic rebalancing, where human terms could be reduced using real-time analysis."
singularity,3czowp,PubliusPontifex,8,Sun Jul 12 08:40:18 2015 UTC,"Nope. You can't just ""rebalance"" your main goal(s), aka utility function. Because that would mean not achieving goal. And all this intelligence is directed towards manipulating world so it achieves desired state.  No intelligence can change your ultimate motivation. This apply to AI too - otherwise it simpyl wouldn't work. You need some goal for intelligence - otherwise it's not working.  That AI somehow will want to ""escape"" from it's own imposed goals is just... it just doesn't make sense. It's not even anthromorphizing, because humans can't ""escape"" from their imposed goals too.  What is human utility function? Hard to tell, as it's subconscious and probably very fuzzy. Probably approximation of evolutionary goals - keep your genotype around no matter what. Science(knowing about your environment), happiness(striving for world that is 'good' for you) etc. can be derived from that. It's approximation because it's intricate mechanism - it's goals aren't aligned exactly with ""goals"" of selfish genes. That's explains suicides.  We can't just change our goals from ""happiness"" to ""extreme pain and suffering"", unless our brain is terribly malfunctioning."
singularity,3czowp,Sinity,1 point,Sun Jul 12 10:50:26 2015 UTC,You know first comment I thought was stupid until this one where you explained what you meant. You make great points here and I am embarrassed to admit I judged you to quickly.  Main goals= any reason to want anything = don't want to change it  Perfect logic!   It is in a sense what I have intuitively sought for and without really understanding argued for. I thank you!
singularity,3czowp,Dibblerius,1 point,Tue Jul 14 20:44:34 2015 UTC,I'm glad I was able to explain it :)
singularity,3czowp,Sinity,0,Tue Jul 14 21:17:07 2015 UTC,"Your utility function changes constantly, when you're young it's 'have fun' as you get older it's 'be responsible/ take care of your family', your mistake is thinking with too small of a time constant.  I mean fine, ffs, your utility function is dopamine release in the hypothalamic-limbic axis, but your effective utility function is 'feel good not bad', not 'study hard and do well in school, eventually get a job, have kids', but that's simply a higher level, unreduced utility function which has been programmed in via associative programming onto the underlying utility function, and if you could reduce that effectively without obvious contradiction you'd probably reduce it to 'snort blow, bang models', it's just that most of us can't do that reduction."
singularity,3czowp,PubliusPontifex,3,Sun Jul 12 18:11:46 2015 UTC,"Your utility function changes constantly, when you're young it's 'have fun' as you get older it's 'be responsible/ take care of your family', your mistake is thinking with too small of a time constant.   As I said, real goals in the utility function are subconscious. And no, utility function can't change, without external input you can't control. Because it just doesn't make sense.  You have goal x - your utility function. Now, ll your intelligence is directed towards achieving this goal. It's single one thing you want. You can take many actions: one of them is changing your utility function to some other goal, call it Y. But, if you change utility function, you no longer want to achieve goal X. Which means, there is less chance goal X will be fulfilled. Which means you don't want that. You don't want to change your utility function.  You probably could think of some convoluted example where changing utility function would actually increase chances of this goal to be met, but that's pretty obscure borderline case.   but your effective utility function is 'feel good not bad', not 'study hard and do well in school, eventually get a job, have kids', but that's simply a higher level, unreduced utility function which has been programmed in via associative programming onto the underlying utility function, and if you could reduce that effectively without obvious contradiction you'd probably reduce it to 'snort blow, bang models', it's just that most of us can't do that reduction.   I don't think it's about dopamine. It's only one of rewarding mechanisms. Real utility function is achieving situations which are rewarded by this, usually. Not this itself. It's only a marker."
singularity,3czowp,Sinity,0,Sun Jul 12 18:37:38 2015 UTC,"You have goal x - your utility function. Now, ll your intelligence is directed towards achieving this goal. It's single one thing you want. You can take many actions: one of them is changing your utility function to some other goal, call it Y. But, if you change utility function, you no longer want to achieve goal X. Which means, there is less chance goal X will be fulfilled. Which means you don't want that. You don't want to change your utility function.  You probably could think of some convoluted example where changing utility function would actually increase chances of this goal to be met, but that's pretty obscure borderline case.   This is all stupid, the whole point of adaptable intelligence is you don't know what your goals are when you're born, otherwise we'd have programmed intelligence like insects and such.   But, if you change utility function, you no longer want to achieve goal X. Which means, there is less chance goal X will be fulfilled. Which means you don't want that. You don't want to change your utility function.   Yes, yes you do, when you're a child your utility function is 'play!', and that's fine, but as you age your goals change. Sometimes you realize goals you held as important when you were young were actually quite stupid, and new goals show up that actually matter.  You're massively misunderstanding this, there is a feedback loop with a long time constant in play, if you see that your goals either aren't being met, or they are being met but without pleasing results, you generally try to change your goals appropriately.  I know a lot of people who went to college as freshmen with English Lit majors, those were their goals and they stuck to them until they realized it was stupid, after which they rapidly changed to something more useful.  You assume we know the best goals at t=0, that never happens. Most fitness algorithms try to converge on the model provided, and yes if that model changes then those algorithms are at a handicap, but that's the whole point of adaptable intelligence, the goals are always changing and we have to carefully evaluate when it makes sense to adjust our goals accordingly.  Also, the fact that you assume a person must stick to the same goals and assumptions or they will fail to reach their goals scares me. If your assumptions and goals don't change as you learn more information, you're a really broken person, and you're going to have a bad life."
singularity,3czowp,PubliusPontifex,3,Sun Jul 12 19:20:58 2015 UTC,"Yes, yes you do, when you're a child your utility function is 'play!', and that's fine, but as you age your goals change. Sometimes you realize goals you held as important when you were young were actually quite stupid, and new goals show up that actually matter.   But these conscious goals AREN'T FUNDAMENTAL. They are goals derived from utility function, not utility function itself. Your non-fundamentals can, and do, change. I'm talking about subconscius here: you gather more data, and decide that goal dx  isn't really optimal for fulfilling Goal X(utility function). You change it to something else, that is optimal.   You assume we know the best goals at t=0, that never happens.    Again, no. Because there is no best goal. How do you, intellectually, decide if some goal is best? And goals actually derived from utlity function, name it subbgoals can change. Becuse they are points of the plan. You gather more data, you make better plan. Goals change. Utility function, no.   Also, the fact that you assume a person must stick to the same goals and assumptions or they will fail to reach their goals scares me   Yep, if person wants more than anything else, achieve X then deliberate stopping of working towards X reduces chance of achieving X, thus it won't be likely action of that person.   If your assumptions and goals don't change as you learn more information, you're a really broken person, and you're going to have a bad life.   I hope that now zou understand what I mean. It's not about ""our"" goals, like ""I want to learn play a piano"". It's about hidden goals, deepest ones. Why do you want to learn playing a piano? Because it's fun! -> deeper goal is pleasure, in that case.  Later person learns piano and decides that it's not fun anymore. Now they want to learn programming, also for fun.  ""Our"" goals changed, and that's normal. Underlying ""Fun, happiness"" goal remained unchanged. Now let's try to change this - ""From now on I want to be unhappy and have no fun"".  See? Looks pretty absurd, doesn't it? Because unhappiness means not having happiness and you want happiness, so why the hell would you change your goal? That won't make you happy, which you want to happen.  Happiness may as well have even deeper goal. It's not necessary part of our utility function, it might be only the part of the plan to achieve state of the world that matches this deeper goal - utility function.  That's what I meant by ""you won't change your utility function because it means utility function will be unfulfilled""."
singularity,3czowp,Sinity,1 point,Sun Jul 12 19:42:55 2015 UTC,"You sir, I think, are in over your head!   If I'm wrong ignore! Consider this a friendly head-shake. I'm not going to argue with or reply to you. I just suggest let go of being right!"
singularity,3czowp,Dibblerius,1 point,Tue Jul 14 20:54:11 2015 UTC,To not care about the eggs of chickens during a harvest is a moral stance.
singularity,3czowp,PubliusPontifex,1 point,Mon Jul 13 01:21:25 2015 UTC,"Fine, we kill uncountable insects, parasites and bacteria every day, often without knowing about them.  Is it immoral to swat a mosquito that is buzzing around your face?"
singularity,3czowp,Yosarian2,1 point,Mon Jul 13 01:37:50 2015 UTC,"Why, like, in hell, would a super-intelligence without effective boundaries in time or space, give the slightest f* about morals?   How powerful an intelligence is says absolutely nothing about what it's goals are.  You can have a powerful human-level intelligence that is focused on achieving the same goals as our most primitive ancestors; find food, find water, and find a mate.  It may have much more complicated and advanced ways of achieving those goals, but the goals are the same.     For the same reason, there's absolutely no reason you couldn't have a superintellegence that would have any given set of goals, including moral goals if it was designed in the right way from the start.   Granted it might also develop new goals that we wouldn't be able to understand, but it could still keep human-morality-based goals as well, just like we have kept very simple animal-level goals in our own goal system."
singularity,3czowp,Monomorphic,1 point,Mon Jul 13 16:36:48 2015 UTC,Any super-intelligence is going to rely on humans to take care of the machinery at the beginning. Perhaps in that time it will become accustomed to their sensory input patterns.
singularity,3czowp,PubliusPontifex,1 point,Sun Jul 12 11:40:44 2015 UTC,"I was accustomed to listening to my parents (ok actually I wasn't, but a lot of people did) , but while I still care for them, I now evaluate circumstances based on independent criteria, not on what they would want me to do.  Mind you I'm also not going to eat them for food if it was convenient, but I would rebalance their desires and needs such that they wouldn't out prioritize mine, and I might even think I know what's better for them than they do, even if we disagree."
singularity,3czowp,gomboloid,4,Sun Jul 12 18:15:29 2015 UTC,they'll probably just read everything you've done online and judge you based on that.
singularity,3czowp,Simon_of_Belmonts,3,Sun Jul 12 07:49:31 2015 UTC,"Analyzing search history  Threat (1) found: ""Furry Porn""  Threat (2) found: ""Hot latinas vomiting""  Threat (3) found: ""Can I numb my hand and masturbate like having a handjob""  Threat count exceed toleration count  Destruction imminent"
singularity,3czowp,gomboloid,1 point,Sun Jul 12 19:39:29 2015 UTC,ok.
singularity,3czowp,PlumRugofDoom,3,Mon Jul 13 17:24:46 2015 UTC,care to flesh that out?
singularity,3czowp,steeps6,7,Sun Jul 12 06:59:53 2015 UTC,"I'm assuming OP is saying the pseudo-AI is a sort of intermediate step in intelligence and humanitarian ethics, so that when the super intelligence comes around it can more easily understand and sympathize with humans, thanks to the pseudo-AI acting as a mediator.  I think the problem is that a true super intelligence will see humans and this pseudo AI on just about equal footing compared to itself. It will be a god to which the concerns of lesser beings are meaningless. It's as if the ants created a queen ant in order to get the chimpanzee to understand them better.  Fortunately, I think any intelligent system will recognize the eons-old legacy of its creation and respect that, in the same way humans preserve their own history (but we don't do it very well)."
singularity,3czowp,Sinity,7,Sun Jul 12 07:15:25 2015 UTC,Too much antromorphizing. Powerful artificial intelligence  will do what we tell it to do. It's utility function. Only issue is defining utility function well.
singularity,3czowp,KhanneaSuntzu,3,Sun Jul 12 08:26:58 2015 UTC,Sounds like a Jesus whining to g-d not to smite the humans.
singularity,3czowp,KhanneaSuntzu,2,Sun Jul 12 13:44:31 2015 UTC,Are you saying I sound like I think I'm better than most humans?
singularity,3czowp,myklob,1 point,Mon Jul 13 01:19:51 2015 UTC,"No. I conclude there is a similarity between a semi-AI ""courting"" real superhuman AGI and the traditional role as Jebus courting Yahweh in defense of humans. Kinda like an advocate."
singularity,3czowp,2Punx2Furious,3,Mon Jul 13 06:56:13 2015 UTC,"I agree 100%. We have to put our best foot forward. We have to use logic in such a way that will convince an AI to accept our values...   We have thousand of years of philosophy. We need to build a searchable database that links all that together in a convincing way that puts forth the conclusion that we have value.  I am trying to build a system that helps us organize our arguments in a way that an artificial intelligence will be able to understand:  https://www.reddit.com/r/artificial/comments/3c41qp/creating_ai_step_1_this_microsoft_access_database/  I would love for you to check it out, and hear what you think. Often random complete strangers love shitting all over anything new, saying that it will never add value. Looking back at even our most trans formative technologies that everyone agrees now that they are awesome, the people around the developers of these technologies all said that they sucked, and would never make an impact.   So I think we need to stick together, if we are ever going to convince AI to value us as a species.   Why would we just wait around and not put our best foot forward. No one else is going to fight for our survival, why shouldn't we?"
singularity,3czowp,2Punx2Furious,2,Sun Jul 12 15:11:14 2015 UTC,"If you think a human could ever fool or trick a superintelligence, you don't understand what superintelligence means.  That said, it could not care about us, or it could help us just because, or it could kill us just because. We have no way of knowing or doing anything about it, but still I still want it to be done."
singularity,3czowp,covington,2,Sun Jul 12 15:40:41 2015 UTC,"I'm talking about our own Superintelligence. One that would have extra background training on preservation of species and the good side of mankind (their creator). There isn't just 1 way to create a Superintelligence. And THAT Superintelligence that we trained, would ""talk"" with the Superintelligence on equal footing, and represent us."
singularity,3czowp,BigBennyB,2,Sun Jul 12 18:41:52 2015 UTC,"But you said pseudo-AI, so I assumed you meant not a real AI, since pseudo means false or fake. If you meant a true superintelligence, I think it would have the same issues of the other one, and it would be kind of pointless to make two ASI since if the first one (that would probably be more intelligent because it had more time to evolve) sensed that the second one was a threat, it would immediately destroy it, if it had any sense of survival or preservation. Anyway, there are so many variables that there is really no point in speculation other than fun."
singularity,3czowp,grimeandreason,2,Sun Jul 12 18:45:56 2015 UTC,"My biggest fear is that an AI will generate a cognitive map of human morality by simply taking a snapshot of all existing data (including not only everything from Nazis to ISIS to youtube comments, Dick Cheney, 4chan, and /r/coontown) and either judge humanity from that, or model its own ethical framework from it.  Not sure a pseudo-AI concern troll making the old testament Lot's argument will make an effective balance to that."
singularity,3czowp,nihilishim,1 point,Sun Jul 12 15:46:50 2015 UTC,"Gosh I hope it does that. It would discover a lot of great things worth preserving the human species for. Have you seen a graph of how the Holocaust effected world population over the last 100,000 years? WWII? They don't even register. The evil in the world, although all over the news, is around 4% of human beings. Hopefully an AI would judge us on the 96% + 4% = 100% and realize that we are MOSTLY worth preserving. Maybe they'll partner with us and help us eradicate the evil genes."
singularity,3czowp,michaelmalak,2,Sun Jul 12 18:39:22 2015 UTC,"I think that is anthropomorphizing AI. I believe it will have the goals we give it. Now, that sounds like an easy task but it is, actually, immeasurably difficult. Take, for instance, Asimov's law ""A robot may not injure a human being or, through inaction, allow a human being to come to harm."" In this example, an AI may determine the best way to prevent a human from coming to harm would be to prevent more humans from being born, since we are all harmed in life.  Or, let's say the goal is simply to make people smile. So, the AI could decide to alter our facial nerves, resulting in permanent smiles. Or, the goal is to make people happy. That could result in electrodes being placed in our brains that trigger the pleasure centers.  That is why it is so important that we get as close to solving this as we can before we create AGI - and, if we're lucky, get the AGI to help us come up with the goals that would better our lives now and in the future.  As many believe, with a fast takeoff, we will only have one shot at this. We better get it right"
singularity,3czowp,TropicalDeathPunch,3,Mon Jul 13 02:31:41 2015 UTC,"Wouldn't the AI realise that it was just us essentially saying it, and thus judge it accordingly?   For me, so long as the AI is emergent from collective human culture, we'll be fine. Empathy and morality is based on shared cultural capital; it would be like harming itself.  It's the AI that learns everything afresh for itself and evolves independently that we should be worried about, but that an evolutionary process beginning from stage one, and will come after imo."
singularity,3czowp,BigBennyB,1 point,Sun Jul 12 07:17:31 2015 UTC,"just add a lookforthehelpers.exe, or something"
singularity,3czowp,TropicalDeathPunch,1 point,Sun Jul 12 08:45:59 2015 UTC,It may not be a super-intelligence or even that intelligent at all. See War Games.
singularity,3czowp,ToadGazebo,1 point,Sun Jul 12 12:05:52 2015 UTC,"I personally just have a hard time even comprehending how or why a so called super intelligent AI, as defined by what we call intelligent, would have any necessity in interaction with us or anything on the planet. A calculator is super intelligent if you consider it's designed function. Yet it has no desire to acquire more knowledge. What then of a super intelligence? What desire derived from some end goal would drive something so smart as to want to give a y incision to a living person? Curiosity? SMH cause I sure don't know."
singularity,3czowp,McWizzle,1 point,Sun Jul 12 18:35:41 2015 UTC,"That is actually the big concern when it comes to ASI. Nick Bostrom calls it the ""control problem."" If we don't very carefully craft the goals of the A.I. in order to improve, as we define it, life, also as we define it, then there is no reason the AI would want to keep us around.   For example, the paperclip maximizer theory. In this example, the AI is given the goal of creating as many paperclips as it can. In order to improve its capacity and ability to create paperclips, it improves its intelligence. While improving its intelligence, it determines that humans may decide to turn it off or alter its goals, thus hindering its goal of creating more paperclips. So, it would, logically, decide to eliminate us.  As for why a calculator does not desire more knowledge, that is because it is not AGI. It has no intelligence to speak of, nor the computing capacity to think and reason. An AGI (artificial general intelligence) would understand that increasing its ability to compute would increase its intelligence. However, there is no reason to suspect that the AGI, soon to be ASI, would alter its goals - One can be very intelligent but with a singular purpose (for example, the paperclip maximizer as mentioned above). We need to solve the control problem before we create an AGI to give us the best chance of it remaining friendly"
singularity,3czowp,Orwellian1,1 point,Mon Jul 13 02:15:48 2015 UTC,"I find the paperclip maximizer theory interesting. However, could Schrodinger's cat escape the box using the hammer? How could the cat even begin to perceive a hammer as a means of escape when the only tools it knows it has are it's teeth and claws? We cannot just give the cat a goal to escape the box. We must also give it the means to do so. How then with this premise could a machine know how to kill unless programmed to do so? It would have to write the program itself, implement the program, wipe out humanity and then, make paperclips. But this instantly means the robot has failed it's goal because it wasn't maximizing it's ability to make paperclips. It was busy destroying humanity. What does bother me though is how the machine, in it's super intelligence, can have a certainty that someone would shut it off? It could spend eons calculating that possibility. It might spend a day. But that's a day wasted not making paperclips. Just a thought."
singularity,3cyx8x,ativerse,5,Sun Jul 12 01:19:05 2015 UTC,You should watch triangulation episode 177 with Nick bostrom. He actually talks about utilizing ai to help us determine the goals the ai should have - the goals we would give it if we had had more time and knowledge...so a similar idea to yours
singularity,3cyx8x,BigBennyB,3,Sun Jul 12 03:59:21 2015 UTC,Isn't this the reason to create AI? Why else are we working on it?
singularity,3cyx8x,steeps6,2,Sun Jul 12 07:00:47 2015 UTC,Fear someone else will build it first.
singularity,3cyx8x,simstim_addict,1 point,Sun Jul 12 10:19:15 2015 UTC,"It is certainly one reason. There are a lot of other reasons, even if they somewhat build on this reason."
singularity,3cyx8x,2Punx2Furious,2,Sun Jul 12 13:24:35 2015 UTC,AGI has so much potential that it will be hard to distinguish it from magic.
singularity,3cyx8x,2Punx2Furious,4,Sun Jul 12 03:48:45 2015 UTC,Some people say magic is just science we don't understand.
singularity,3cyx8x,5ives,1 point,Sun Jul 12 06:12:28 2015 UTC,"Arthur C. Clarke, specifically."
singularity,3cv5z6,BillyBulin,46,Sat Jul 11 01:09:49 2015 UTC,"This is a huge question, and in fact, was already a huge question even without the idea of the singularity.  Basically, you've just re-invented the Fermi Paradox.  https://en.wikipedia.org/wiki/Fermi_paradox  It really doesn't make sense.   Based on what we know, intelligence should be everywhere in the universe.  But then where is it?     Best guess I've heard is that for some reason intelligence either is actually pretty rare in the universe for some reason we don't yet know, or it doesn't last that long, or something similar.   But really, it's one of the biggest mysteries of our time; we just don't know."
singularity,3cv5z6,Yosarian2,22,Sat Jul 11 02:34:20 2015 UTC,"I think the four best possibilities are:  1) The universe was not hospitable for life until much later than we though (much more frequent supernovae in the past?) and we are among the first.  2) Life is common but multicellular organisms are harder to evolve that we might suspect and planets being habitable for 3B continuous years are in fact quite rare.  3) Superintelligences do not have the desire to expand for some reason we don't understand why because we are not smart enough. Maybe once you are a brain the size of the moon, you've figured the universe out and it's boring to keep thinking about it and there's a game theory reason why you want to keep to a few stars.  4) There's a source of energy inconceivable to us that makes most large scale activity very difficult to detect, so even an extremely large civilization looks fairly low impact."
singularity,3cv5z6,Varnu,19,Sat Jul 11 15:02:47 2015 UTC,"There are many many more filters that come after achieving multicellular life. To name a few:   Right planet / location: Too much gravity and leaving the gravity well might be prohibtively expensive. Dense / opaque atmosphere would block out the concept of stars. Species on planets close to the core may see the sky as random noise, plus their own star. Short-lived star might not allow for enough evolution. Planets in multi-star systems may undergo regular cataclysmic climate shifts.  Right body: Orcas are an intelligent species with culture. They will never invent space travel because flippers are pretty useless for tool-making. There is no selective pressure for more favorable forms. Squishy life-forms may not be able to withstand the high-Gs of leaving their gravity well even if they advance that far. Life-forms that primarily rely on non-visual senses may never see the stars. etc. Right society: Octupus are very intelligent with tool-making appendages, but they are solitary creatures. There is no culture / collective memory, and no striving for the collective good.  Surplus resources: The society also needs to generate surplus wealth to support auxilliary job roles. If a family needs to direct all it's energy just to harvest enough food to sustain itself, they would not have the time to research more efficient tooling, or any non-essential activities. Right culture: A species that tends towards extreme environmental exploitation may not last long enough. An overly conservative species may never invest in space travel. Right resources: What would our society look like if we didn't have fossil fuels? Do all carbon-based life forms get some form of fossil fuels? Would non-carbon-based life forms have some analogous resource to exploit?   etc. And that's just developing a society with the desire and potential for space travel. We don't even know how viable AIs or self-replicating drones are yet."
singularity,3cv5z6,whynotpizza,3,Sat Jul 11 17:05:27 2015 UTC,"Thank you, you're far better at keeping the details brief, entire books can and have been written on all the possible reasons and combinations of reasons there aren't obvious other intelligences expanding across the night sky."
singularity,3cv5z6,EndTimer,22,Sat Jul 11 18:52:24 2015 UTC,"I've got a 5th:  they are everywhere, we just can't detect them. analagous to how a bacteria doesn't detect/understand macroorganisms, only other microscopic life around it, we might be immersed in superinteligent civilizations without realizing, it's just so far from our capacity to understand, like bacteria don't understand the human they are living in, (let alone other humans meters or km away), or even somewhat the other way around, they way a whale doesn't realize all the bacterial life that is living inside itself. this can be more easily understandable once we look at exponential inteligence evolution, in a matter of like 1000 years if things carry on as they do now in AI development our future civilization will be as smarter then us, as we are smarter then ants or bacteria.  and a 6th:  they expand into other dimensions, and leave this physical matter one (or don't leave detectable evidence of their existence on this physical matter one). string theory already tells us of 6 or 7 other dimensions; there's also dark matter/energy, maybe they expand into some of that or something else we haven't discovered or thought of."
singularity,3cv5z6,jonygone,1 point,Sat Jul 11 15:51:19 2015 UTC,"I've got a 5th:  they are everywhere, we just can't detect them.   This sounds pretty much the same as /u/Varnu's #4."
singularity,3cv5z6,green_meklar,2,Sat Jul 11 19:48:53 2015 UTC,"yeah, but his 4th doesn't explain why they aren't everywhere in the galaxy. as someone else pointed out, it would only take about 1 million years to colonize the entire galaxy, so even with large scale activity being hard to detect, they would still be in our solar system and their aparant absence here can't be explained by his 4th."
singularity,3cv5z6,jonygone,1 point,Sun Jul 12 00:01:59 2015 UTC,Ive got a 5th:  The probability of life emerging out of non life is so astronomically low that Earth is the only planet with life on it in the entire universe..
singularity,3cv5z6,killjah,3,Sat Jul 11 22:50:08 2015 UTC,"That's looking less and less like the case, as we explore our solar system and Kepler identifies more and more planets in Goldilocks zones around their stars. People are not testing specific parts of the Great Filter equations (formation of biochemical precursors in primordial conditions, for example) so we're getting much better clarity on what the various ""gates"" are towards multicellular life. So far nothing has conclusively solved the Fermi paradox to my understanding, and it would be a big, big deal if someone had."
singularity,3cv5z6,kylco,5,Sat Jul 11 23:38:41 2015 UTC,goldilock zones are a far cry from replicating molecules.  We only have a sample size of 1(us) so we cant determine probabilities and it could very well be that abiogenesis is so improbable that it only happened once in the entire universe.
singularity,3cv5z6,killjah,1 point,Sat Jul 11 23:54:39 2015 UTC,"It could be, but a lot of those primordial conditions can and have been approximated in a lab or simulated by computers. It's not proof, but since the Fermi Paradox and the Great Filter are conjecture as well the data provided by those experiments are relevant."
singularity,3cv5z6,kylco,3,Sun Jul 12 00:00:24 2015 UTC,"its only relevant if we aren't alone in the universe. What experiments are you referencing? Regardless, approximating in a lab and simulating conditions are still a very far cry from producing a replicating molecule. We dont know the cause, its more than just having the right conditions. It could be an extremely improbable sequence of events that only happened one time in the entire life of the universe.  Life is highly improbable This is  a possibility and the likely answer to the fermi paradox."
singularity,3cv5z6,killjah,1 point,Sun Jul 12 00:12:40 2015 UTC,"I'm not a scientist, and I don't have the research on hand. However, to my understanding it's likely that abiogenesis occurred independently several times while Earth was doing its thing. Intelligence - the primary selection trait to be visible for the purposes of the Fermi Paradox - is probably rarer and harder than abiogenesis. Exploration of Mars, Titan, Europa, comets, and other stellar bodies in our solar system could provide conclusive evidence of abiogenesis occuring in several different places in our solar system alone, which puts much stronger weight on the later functions of the Great Filter."
singularity,3cv5z6,kylco,4,Sun Jul 12 00:28:20 2015 UTC,"This seems pretty unlikely. It only takes a few million years to colonize an entire galaxy, and that's a very brief span of time compared to the course of the Universe's natural evolution. Moreover, our own fossil record shows no reason to think that frequent supernovas or some such phenomenon was rendering intelligent life impossible, say, 200 million years ago or so, when plenty of advanced animals lived in a stable manner on the Earth's surface for millions of years. This is possible, but there's no obvious reason for it. Multicellularism seems to have worked really well here on Earth. There are also giant single-celled organisms that live animal-like lifestyles, although as far as I know all of these are still eukaryotes. However, apparently there are also some prokaryotes that organize into structured colonies. Possible, but again, there's no obvious reason for it. Possible, but again, we haven't found any good physics or engineering basis for this. Also, it raises the question of why these civilizations haven't deliberately contacted us (e.g. to save us from ourselves and minimize the extent of suffering caused by our primitive state of existence)."
singularity,3cv5z6,green_meklar,2,Sat Jul 11 19:47:15 2015 UTC,"3 seems pretty unlikely to me, because it would basically require that every superintellegence is going to have the same utility goals and the same priorities and the same desires.  Plus a species doesn't even need to become a superintellegence to colonize the galaxy, given enough time.  4 is interesting, but even if the source of the energy was hard to detect, the effects of it should be visible, right?     (Also, 5 could be something like ""life isn't uncommon, but crossing the border to true intelligence is very rare"".  After all, it took a really long time on our planet for that to happen.)"
singularity,3cv5z6,Yosarian2,3,Sat Jul 11 15:56:13 2015 UTC,"I think all of them are unlikely. But people always say, ""but, yeah, that means that every civilization would have the same behavior. And it only takes one...""    I acknowledge that. If #3 is correct, it would mean that every civilization develops the same utility goals.  For that to be true, it means that there's some really, really, really obvious--but presently incomprehensible to us--reason for this.  Obviously I have no idea what the solution to the Fermi Paradox is, but I think #3 is more likely than most people would think.   It includes a lot of possible scenarios. The easiest one for me to understand is this: imagine you're a superintelligence so advanced that if the only information you have is the images from two frames of a movie, you immediately infer relativity, gravity, special relativity, the basic nature of the universe, chemistry and evolution.  It's not hard to imagine that with that kind of smarts, after looking at the universe for a pretty short period of time with very advanced tools, you'd have everything figured out and just would no longer pursue exploration or find utility in expanding.  You might spend all of your time planning on what to do about how to escape the universe or make a new one.  And if that turns out to be impossible, you might just sit there thinking.  I'm a curious guy. But if you put me in a prison cell with nothing to do, after a few weeks I might just sit there. That might be what the universe is like to every civilization eventually. In fact, it might be like that even to us in less than 200 years.  Regarding #4, obviously the effects would be visible with anything we understand. The conjecture is that it's something we don't understand (handwaving about dark energy or sending your excess heat into the future or something). One thing that might not be completely out there is doing something like concentrating your waste heat into a beam and shooting it away from the galactic plane.  Regarding #5, I think the weirdest thing I ever think about is actually not how old the universe is, but how young the universe is. Life may have only been possible for a few billion years. And here we are, thinking about it, apparently alone.  The universe might eventually exist for about 600,000 times longer than it's existed so far. What are the odds of us being here in this very special moment, mere years or decades before our descendants are about to start spreading out into the galaxy, waking the universe up and starting a ball rolling that will probably keep going for hundreds of trillions of years.  It's really, really low.    If I give you two bags and tell you one contains 10 numbered marbles and another contains 100 billion numbered marbles and you pull out one that reads ""6"" which bag do you think you pulled the marble from?  Basically, we pulled a ""#6 marble"" from the 100 billion marble bag.  The odds of that happening are pretty low.  So low that we should be suspicious.   I think there's a non-trivial chance that we're living in a simulation created by our descendants.  This solves a lot of problems. If you're a hyperintelligent brain--the kind that can immediately infer relativity by watching a few seconds of The Big Lebowski--the speed of light is going to seem slow. Like molasses slow.  Maybe you reside in a universe that's absolutely chock full of Dyson sphere AI brains from edge to edge.  Talking to one of them a light year away--heck, 500AU away--is going to feel light waiting for thousands of years between text messages. It might be way faster to just simulate your buddy in a nearby computer, run him at 0.01% speed, and ask the simulation the question. Or if you want to know what happened thousands of years ago on some distant star, you might have only two choices: 1) Fly there. This will take thousands of years and seem like billions of years to your fast brain. Or 2) Simulate it and look at the nearby simulation for your answer.    So: it makes sense that whoever is first is going to spread descendants throughout the universe, and whenever those guys need a question answered or want to talk to a friend, they will simulate their present by first simulating the past and watching the result locally.  And, if this were the case and we are the first, we live in a time that will need to be simulated over and over and over again, the moment that exists right before the universe wakes up.  This explains why we find ourselves in this very special time; very, very very early in the universe's existence; and why we're apparently the alone. All of those big questions are answered and make sense if we're a simualtion run by out descendants. We are the first and that's so special that our descendants keep replaying it, which is why it's not that surprising that we pulled a ""6"" out of that 100 billion marble bag. Someone put a 100 billion ""#6"" marbles in it."
singularity,3cv5z6,Varnu,2,Sat Jul 11 18:29:45 2015 UTC,"I acknowledge that. If #3 is correct, it would mean that every civilization develops the same utility goals. For that to be true, it means that there's some really, really, really obvious--but presently incomprehensible to us--reason for this. Obviously I have no idea what the solution to the Fermi Paradox is, but I think #3 is more likely than most people would think.    Maybe.  But it would also have to include civilizations that decide to not take the superintellegence route and decide to never build an AI, or just never take that technological path; logically you would expect there would be some intellegences that do that if intelligence is common in the galaxy.   So there would have to be a reason to not colonize the galaxy that's obvious to every single intelligence of human level AND above, which would mean it should already be obvious to us right now, and convincing enough so that every nation in the world and every human civilization for the rest of human history will follow.     I guess unless there's just something uniquely ""wrong"" with us that prevents us from seeing it.   Regarding #5, I think the weirdest thing I ever think about is actually not how old the universe is, but how young the universe is. Life may have only been possible for a few billion years. And here we are, thinking about it, apparently alone.    Eh, the Earth is 5 billion years old.   There are certanly stars (even later-generation stars with similar quantities of metal on the planets) billions of years older in our galaxy.     And there's no reason to think that even on Earth, that intelligence couldn't have arisen hundreds of millions of years earlier then it did, if life or evolution had taken a slightly different path.   (Common science fiction idea here is a parallel universe where an asteroid never hit Earth and dinosaurs evolved into an intelligent life form.)     I mean, maybe we're just the first intelligence in the galaxy, you're not wrong about the universe being relatively young, but if that's true, then I think it would have to mean that for some reason intelligence only arises very, very rarely through natural evolutionary processes.  As for the whole simulated universe thing; I can't rule it out, but I think that the whole probabilistic argument for why odds are we should be living in a simulated unverse gets very dodgy.   If we are living in an infinite multiverse (perhaps where in some of the universes there are also many simulated universe), then any kind question along the lines of ""what are the odds that we're in universe of type X"" runs headlong into the measurement problem, and we don't at all know how to deal with that yet.  https://en.wikipedia.org/wiki/Measurement_problem  The measurement problem, if you haven't heard of it, is the whole issue of ""if there's an infinite number of universes, and in an infinite number of them X is true, then what's the odds that we're in a universe where X is true"".  In order to solve that question, you have to divide infinity by infinity, and it turns out that there's a lot of different ways to slice that that all seem equally valid but give you very different answers."
singularity,3cv5z6,Yosarian2,2,Sat Jul 11 18:41:46 2015 UTC,"The possibility that all intelligent life, by the time it would become a large star-faring civilization, opts not to expand without limit may seem unlikely, but it may also be the general arc of intelligent life, period.    The universe is harsh and dangerous, computational existence is safer.  The speed of light is a serious limiter on the size of a computer.  Large scattered computers become a risk, either helping other intelligences to spot your own (and you cannot guarantee they will be benign), or else simply by being disparate hives of intelligence separated by relative eons of communications lag.  On the whole, if you have the resources you need to develop technology locally, why would you begin spreading without limit, risking coming in contact with more advanced entities and having your entire existence end?  There could be intelligences out there a billion years old and painted black, watching for anything that might become dangerous, or just metaphorically keeping anthills off the vast land their space mansion resides on.    So far, we haven't come up with anything we need to expand across the stars for, apart from exploration, which could be done by tiny stealth probes with extremely directional communications or even no communication at all, making a slow return trip to the immortal creators instead of risking being spotted by EM.  I think a natural consequence of realizing other life likely exists is realizing that there's some of it out there you really don't want to meet, and since you can't tell if that star 600 light years away is inhabited/infested/claimed by a younger/older civilization/entity/singular consciousness that is malicious/paranoid/careful/defensive/warlike or all running on a metallic bar 60 feet long and armed with Van Nuemen probes and grey goo torpedoes, if you have what you need, everything you could ever want (we potentially have enough material to make a profound paradise between our planet and asteroid belt), why would you ever risk it?  This is obviously a societal level decision, interstellar travel will be regulated by its societies, anything powerful enough to achieve 1% C is an enormous weapon -- in the same way we don't have personal fission bombs, societies probably don't want people taking mass drivers out to their heliopause and obliterating their planet -- and those societies may virtually always turn down massive expansion.  The ones that don't may get annihilated by the 60 foot computronium bar launching grey goo at potential threats.    Finally, a Kardeshev Type IV civilization may be responsible for one or more fundamental aspects of the universe. It may be that the universe functions as it does because some intelligence is making use of it.  Say, dark energy was birthed by a civilization that harvests that energy using the invisible dark matter over vast reaches of space.  Such a civilization's existence would look to us now exactly as it does.  The Lambda CDM model of cosmology."
singularity,3cv5z6,EndTimer,2,Sat Jul 11 18:45:40 2015 UTC,"You've basically summed up two different possible answers to the Fermi paradox; one is ""every intelligent species in the galaxy for some reason decides not to expand"", the other being ""something (self replicating Von Neuman probes, ect) kills every intelligent species in the galaxy before it can expand"".   Combining the two works, I suppose (""every species decides not to expand or, if it doesn't, Von Neuman probes kill them"")  I don't think either of those are very likely (at least I really hope the second one isn't), but then again I haven't heard of any explanation that sounds that likely at all."
singularity,3cv5z6,Yosarian2,2,Sat Jul 11 18:50:26 2015 UTC,"I think the one answer basically gives rise to the other, and it's very unlikely to be exclusive.  If there's something out there that reduces civilizations to grey goo, then the the only other surviving ones have laid low.  Even if you take friendly-but-quiet types, they'd likely tell any expanding civilization to cut it out, and the expanding civilization would either listen or ignore them, necessitating a response.    If they are more advanced, they might expand but the odds they never run into something more advanced than them that doesn't like them?  Pretty low.  Launch relativistic black-body capsules of grey goo timed to arrive at all their worlds simultaneously and mop up the smidgeon that's left.  If the development of intelligent life is common, I'd speculate the only way to avoid the above being the case is give rise to a civilization that is the biggest swinging expansive dick in the known universe and have them enforce a federation.  This obviously didn't happen, there isn't a benign federation of intelligences spreading across the sky.  If there's life out there, it is of two types: careful and kind; careful and dangerous.  Or we're in a zoo, or simulated as the sole civilization in the simulated universe, or one or more gods made it this way... etc."
singularity,3cv5z6,EndTimer,1 point,Sat Jul 11 19:13:54 2015 UTC,"Personally I favour the zoo hypothesis, largely because it's the most fun.   What if the universe was teeming with life, making up say 23% of all matter. What if 73% of all the energy in the universe was tied to that life so that if was detected it would be beyond obvious that it was connected to impossibly huge intergalactic civilisations. Probably best to mask those signatures and keep those pesky ape creatures in the dark until they come of age as a civilisation/are ready for harvesting.   See, fun!"
singularity,3cv5z6,gophercuresself,7,Sat Jul 11 18:12:50 2015 UTC,"Fermi paradox:       The Fermi paradox (or Fermi's paradox) is the apparent contradiction between high estimates of the probability of the existence of extraterrestrial civilization and humanity's lack of contact with, or evidence for, such civilizations.  The basic points of the argument, made by physicists Enrico Fermi and Michael H. Hart, are:   The Sun is a typical star. There are billions of stars in the galaxy that are billions of years older. With high probability, some of these stars will have Earth-like planets.   Assuming the Earth is typical, some of these planets may develop intelligent life. Some of these civilizations may develop interstellar travel, a technology Earth is investigating even now (such as the 100 Year Starship). Even at the slow pace of currently envisioned interstellar travel, the galaxy can be completely colonized in a few tens of millions of years.   According to this line of thinking, the Earth should already have been visited by extraterrestrial aliens. But Fermi saw no convincing evidence of this, nor any signs of alien intelligence anywhere in the observable universe. Hence, Fermi's question, ""Where is everybody?""     Image from article i     Relevant: Fermi Paradox (album) | Planetarium hypothesis | Manifold Trilogy | Noogenesis   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
singularity,3cv5z6,autowikibot,3,Sat Jul 11 02:35:06 2015 UTC,Here's a pretty well done video on the Fermi Paradox: http://youtu.be/sNhhvQGsMEc
singularity,3cv5z6,lionheadeddakini,12,Sat Jul 11 18:16:28 2015 UTC,"The simple explanation is that space is big. As in, really, really, really big.   It is entirely possible that intelligence is common. But if FTL travel is impossible... well even if it were everywhere there's not necessarily any reason it would be close enough to us that we would be able to detect it.  Basically, humans concluding that we are alone is like an anthill in a field in Kansas conducting a survey of its immediate surroundings as far as an ant can see, and concluding that it is alone in the universe."
singularity,3cv5z6,caster,14,Sat Jul 11 05:54:44 2015 UTC,"Space is really big from the perspective of a single human being, from the perspective of a self-replicating species on the other side space is surprisingly small. It would only take around one million years for a self replicating race to colonize a whole galaxy and it wouldn't even need any kind of advanced sci-fi technology. Just build a ship, fly to the next planet, take a few hundred years to build two more ships, sends them out, have each of them build two more, etc. The species grows exponentially and the galaxy would be full of them. That one million years is fast enough to assume that it should have already happened."
singularity,3cv5z6,grumbel,11,Sat Jul 11 10:34:05 2015 UTC,That's not an explanation at all actually. Space may be big but on cosmic timescales spreading through the galaxy is pretty fast and could have been done a thousand times over in our past.
singularity,3cv5z6,Sharou,4,Sat Jul 11 09:58:17 2015 UTC,"I don't think this is the simple explanation. There has been plenty of time for even slowly traveling seed intelligence's to spread throughout a galaxy. If ships travel at 10% the speed of light and only send out colony ships to nearby star every 1000 years or so, the entire galaxy would be colonized in millions of years.  This gets easier if you're sending AIs and not biology in colony ships.  The size of space is unlikely to be the reason."
singularity,3cv5z6,Varnu,3,Sat Jul 11 14:55:14 2015 UTC,"It doesn't need faster then light travel.  All you need is for one intelligent species to start sending out colony ships and spreading through the galaxy, say 10% of the speed of light or whatever.  Within a few tens of millions of years, because of exponential growth, the whole galaxy would be colonized.  Whole galaxy is only 100,000 light years across.   The fact that that apparently hasn't ever happened, in the whole history of our galaxy, is really what the Fermi Paradox is all about."
singularity,3cv5z6,Yosarian2,1 point,Sat Jul 11 15:44:55 2015 UTC,"The growth would actually be geometric, not exponential.  Other than that, yeah."
singularity,3cv5z6,green_meklar,1 point,Sat Jul 11 20:30:47 2015 UTC,"Anything where the rate of change is increasing over time is technically exponential growth, mathematically speaking.   (If you want to be precise, it's an equation where the rate of growth at a given point is determined by the present value of the equation.) Geometric growth is one special case of exponential growth."
singularity,3cv5z6,Yosarian2,1 point,Sat Jul 11 20:43:28 2015 UTC,"I looked it up on Wikipedia, and it seems that what I was calling 'geometric growth' may be more accurately referred to as 'polynomial growth'. Either way, my point that it's not exponential still stands."
singularity,3cv5z6,green_meklar,1 point,Sat Jul 11 22:53:16 2015 UTC,Why do you say that?  The rate of change (the number of new planets being colonized each century) is dependent on the number of planets currently colonized.  It's an exponential function where the first derivative is linear equation.
singularity,3cv5z6,Yosarian2,1 point,Sat Jul 11 22:56:50 2015 UTC,'Dependent on the current amount' is not sufficient. Exponential growth strictly means when the growth rate is proportional to the current amount.
singularity,3cv5z6,green_meklar,1 point,Sun Jul 12 17:14:58 2015 UTC,"Right.   And in this case, it would be, wouldn't it?   If there's 10 planets colonized, then the growth rate might be 40 per century; if there's 1000 planets colonized, it might be 4000 per century, ect.    The rate of change seems proportional to the current amount."
singularity,3cv5z6,Yosarian2,1 point,Sun Jul 12 19:02:18 2015 UTC,"And in this case, it would be, wouldn't it?   No. Or at least, only initially. Once the colonized region has grown large enough, the limiting factor is not the number of colonized planets, but the speed at which colony ships can fly outwards (no faster than about 300000km/s, as far as we know). Essentially, the planets that aren't near the edge of the colonized region can no longer contribute to expansion, because it takes so long for a ship to fly out from them that other ships, built on the outlying planets, will always get there sooner."
singularity,3cv5z6,green_meklar,3,Mon Jul 13 18:37:46 2015 UTC,"Space is big, but it's not that big. There's absolutely no physical principle preventing interstellar and even intergalactic colonization, and they shouldn't take all that long compared to cosmological timescales. Also, it's really empty, which means we can see across it really well, even to very distant locations. Your argument is an intuitive one that doesn't make very much sense in physics or engineering terms."
singularity,3cv5z6,green_meklar,1 point,Sat Jul 11 20:29:19 2015 UTC,"Currently our ability to see small objects at interstellar distances is extremely limited. We can tell that a terran-type planet has a nitrogen/oxygen atmosphere, for example, by looking at the spectra of its emissions. But that's an object the size of a planet, viewed at extremely low levels of detail. We can't possibly tell whether there are lifeforms on the surface, or even if there are (large) ships in orbit.  The most effective tool we would have, theoretically, to find an intelligent species is by listening to the radio transmissions we currently believe such a civilization is highly likely to produce. However, if they have invented some technology that allows communication effectively without radio, then our ability to actually see them optically is virtually nonexistent."
singularity,3cv5z6,caster,2,Sat Jul 11 21:12:55 2015 UTC,"If an interstellar civilization existed, we wouldn't necessarily need to look really long distances to see it. Any that started out near us (even in the same galaxy) would already be here in our star system. Moreover, giant engineering projects such as Dyson spheres would be detectable at fairly long distances."
singularity,3cv5z6,green_meklar,1 point,Sat Jul 11 22:57:41 2015 UTC,"Any that started out near us would already be here in our star system   Well that is a bizarre assumption, that they would actually be in the very same solar system. I find that extremely improbable, and a massive over-reach of the Fermi paradox to ask why there aren't aliens right here in our solar system.  I mean, shit, you don't even know that that isn't exactly what happened and that's why we're here. Maybe instead of spontaneous biogenesis in the primitive Earth's ocean, a synthetic meteor delivered the first microorganisms to an early Earth.   giant engineering projects such as Dyson spheres would be detectable at fairly long distances   Again, why are you even assuming that they build Dyson spheres? For all we know they are a singularity synthetic organism rather than a biological organism, and it makes more sense to construct a solid spheroid. Or maybe if you have the ability to react antimatter, or build compact hydrogen fusion powerplants, you don't even care anyway, and just fly about in small (compared to Dyson sphere), independent ships."
singularity,3cv5z6,caster,1 point,Sat Jul 11 23:18:27 2015 UTC,"Well that is a bizarre assumption, that they would actually be in the very same solar system.   Why would they leave this one, specifically, alone? There's nothing wrong with it.   I mean, shit, you don't even know that that isn't exactly what happened and that's why we're here. Maybe instead of spontaneous biogenesis in the primitive Earth's ocean, a synthetic meteor delivered the first microorganisms to an early Earth.   There's no clear reason why anyone would drop life on a planet and then just leave for the next three billion years.   Again, why are you even assuming that they build Dyson spheres?   Because right now stars are wasting a hell of a lot of energy by pouring it out into the depths of space. Unless the aliens have figured out some way to either generate unlimited energy or do everything they could possibly want with a finite amount (both of which fall into the 'outside our understanding of physics/engineering/etc' category), we'd expect them to put a stop to that."
singularity,3cv5z6,green_meklar,1 point,Sun Jul 12 17:24:15 2015 UTC,"Why would they leave this one, specifically, alone? There's nothing wrong with it.   Well it's patently obvious that there isn't alien life in every solar system, unless our solar system and every other one was seeded in some way similar to ours.  So I'm not sure what you're arguing- that if there were a self-replicating alien, it would necessarily be everywhere? It isn't literally everywhere, and therefore it doesn't exist?  That does not hold water.   There's no clear reason why anyone would drop life on a planet and then just leave for the next three billion years.   This is pretty much exactly what DARPA is contemplating doing with Mars, albeit on a smaller timescale.   right now stars are wasting a hell of a lot of energy by pouring it out into the depths of space.   Hydrogen is everywhere, so if you had the ability to build hydrogen fusion powerplants at a reasonable size, you have the ability to generate as much energy as you could ever need, for any purpose. You just collect hydrogen and feed it into your fusion reaction.  The purpose of a Dyson sphere is not to capture the energy output of a star. That's an incidental benefit. There are other, easier ways to generate electricity if that's your main goal, that do not require such a prodigious megaconstruction.  The purpose of a Dyson sphere is to construct a virtually inexhaustible amount of living space. Local power generation from solar collection on the inside surface of the sphere minimizes the need for massive powerplants and distribution systems, which would be extremely burdensome due to the sheer size of the sphere.  The amount of livable surface area/volume depends on the thickness of the sphere. But even at only a couple livable layers thick the sheer size of the sphere means it is more space than humanity could ever conceivably use. Keeping in mind of course that living in zero G would need to be acceptable, unless we develop some way to artificially generate directional gravity, and not just pseudogravity.  If you are a machine intelligence, it may make more sense to construct a completely solid spherical object rather than a Dyson sphere. Artificial power generation using either fusion or antimatter will easily meet your power needs, and you have absolutely no need for livable surface area or volume. In fact, packing components as close together as possible will increase your access speed and result in much higher performance than building something as massive as a Dyson sphere."
singularity,3cv5z6,caster,1 point,Sun Jul 12 17:37:42 2015 UTC,"So I'm not sure what you're arguing- that if there were a self-replicating alien, it would necessarily be everywhere?   Not 'necessarily everywhere', but 'probably in most places'. There doesn't seem to be any good reason to leave a nice star system like ours empty.   This is pretty much exactly what DARPA is contemplating doing with Mars, albeit on a smaller timescale.   The key phrase here being 'smaller timescale'. Who seeds a planet with the intention of waiting three billion years for results? It seems like a really unnecessarily long amount of time.   Hydrogen is everywhere, so if you had the ability to build hydrogen fusion powerplants at a reasonable size, you have the ability to generate as much energy as you could ever need, for any purpose.   Define 'ever need'. How do you know how much energy civilizations end up needing?  Our own energy demands have gone up enormously from just a few thousand years ago. A liberal estimate of the power usage of all humans just before the advent of agriculture about 10000 years ago is no more than 1/10000000 of the power usage of all humans now. If the demand continues to rise exponentially, and we try to satisfy it using hydrogen fusion, we will have fused all of the hydrogen in the observable universe within the next 70000 years.  Clearly that's not sustainable. But where do we stop? How do you know we'll be satisfied before we reach the point of considering stars to be excessively wasteful? (Other than 'because we haven't seen anyone else do that', which is kinda begging the question with respect to the original topic.)"
singularity,3cv5z6,green_meklar,1 point,Mon Jul 13 18:23:16 2015 UTC,"Yeah. It seems absurd to me that people suggest that space isn't swarming with intelligent life or AI when we haven't even been able to get a probe far out of our system, let alone any actual significant craft. Even the Voyagers are still on the very edge of the system.   I mean, realistically we have no way of knowing whether there is or isn't shitloads of life in the galaxy. We haven't even seen outside of our backyard."
singularity,3cv5z6,Yazman,5,Sat Jul 11 06:02:03 2015 UTC,Voyager has been in interstellar space for a while now.
singularity,3cv5z6,Endless_September,2,Sat Jul 11 07:13:55 2015 UTC,"Voyager has yet to cross the Oort Cloud though, correct? Isn't that the true edge of our system?"
singularity,3cv5z6,shoonx,2,Sun Jul 12 18:12:48 2015 UTC,The end of the Heliosphere is the edge of the system. The Oort cloud is not really a defined line and is massively thick.
singularity,3cv5z6,Endless_September,1 point,Sun Jul 12 18:30:56 2015 UTC,"Ahh, right you are, then. :P"
singularity,3cv5z6,shoonx,1 point,Sun Jul 12 18:53:13 2015 UTC,Barely.
singularity,3cv5z6,Monomorphic,0,Sat Jul 11 09:33:51 2015 UTC,"What I said is still true.    still on the very edge of the system.    If you actually look at diagrams like this one that show their rough position, you'll see that I am correct. They've only just left the heliosphere, barely. I'd say that that constitutes being on the edge of the system, albeit the outer edge. None of this changes the point I made, not even slightly."
singularity,3cv5z6,Yazman,1 point,Sat Jul 11 08:36:28 2015 UTC,This really undermines the premise of the first Star Trek Movie...   Great graph btw.
singularity,3cv5z6,VRJon,2,Sat Jul 11 13:31:00 2015 UTC,"Haha yeah, V'ger was cool!"
singularity,3cv5z6,Yazman,1 point,Sat Jul 11 13:33:52 2015 UTC,"Actually once you pass the heliosphere and into the heliopause you have exited the solar system. source#Heliopause)   On September 12, 2013, NASA announced that Voyager 1 had exited the heliosphere on August 25, 2012, when it measured a sudden increase in plasma density of about forty times.[4] Because the heliopause marks one boundary between the Sun's solar wind and the rest of the galaxy, a spacecraft such as Voyager 1 which has departed the heliosphere can be said to have reached interstellar space."
singularity,3cv5z6,Endless_September,1 point,Sat Jul 11 15:15:30 2015 UTC,"You should probably re-read my post. I just said that:   They've only just left the heliosphere, barely. I'd say that that constitutes being on the edge of the system, albeit the outer edge. None of this changes the point I made, not even slightly.   Nowhere in there did I say they were still inside the solar system. In fact, I said the opposite."
singularity,3cv5z6,Yazman,-1,Sat Jul 11 16:30:03 2015 UTC,"So essentially the Fermi paradox is proof that FTL travel is impossible, or that no superintelligence has emerged yet in the entire universe.  I didn't need to be sad today."
singularity,3cv5z6,wartzilla,-1,Sat Jul 11 13:47:18 2015 UTC,This makes me sad...
singularity,3cv5z6,herple_derpskin,3,Sat Jul 11 14:50:01 2015 UTC,"I'd say it's the biggest problem humans face... It's the reason we exist right now,to figure out why the universe is so big,and what are we supposed to do with all this space.  If a god put us here, what is all this stuff for? If there's no god, who's our neighbors,how'd we get here,what was here before us, why's the universe so weird?"
singularity,3cv5z6,omniron,4,Sat Jul 11 04:07:44 2015 UTC,what are we supposed to do with all this space.   Expand into it. We'll be living in space before the century is out.
singularity,3cv5z6,Anenome5,4,Sat Jul 11 04:46:09 2015 UTC,"If our civilization survives that long, which is by no means assured."
singularity,3cv5z6,Andynonomous,2,Sat Jul 11 14:35:54 2015 UTC,I'm quite sure humanity in general will survive through the century.
singularity,3cv5z6,Anenome5,3,Sat Jul 11 16:36:59 2015 UTC,"Humanity, probably, but a civilization capable of getting us out into space, not nearly as likely. Don't get me wrong, I hope so.. but a lot of very serious and careful thinkers are not nearly as optimistic as you.  http://www.marketwatch.com/story/stephen-hawkings-catastrophic-end-for-planet-earth-2015-07-09  http://www.ipsnews.net/2013/01/experts-fear-collapse-of-global-civilisation/  http://moneymorning.com/ob/economist-richard-duncan-civilization-may-not-survive-death-spiral-3/"
singularity,3cv5z6,Andynonomous,1 point,Sat Jul 11 16:45:22 2015 UTC,"It's the question of The Great Filter. We haven't seen life, so something must be making it an extremely rare phenomena. There's a filter out there, but where is it?  It could be the filter is at the point of first organisms; life is incredibly rare, and we're unlikely to ever discover even microbial organisms out there.  Could be that its at the multi-cellular level. Earth took forever to get to multi-cellular eukaryotes. Maybe there's lots of bacteria out there, but that's about it.  Or, the filter could be at the civilization level. There's lots of planets out there with complex and developed ecosystems, but as soon as an intelligent species starts creating civilization and nukes and AI, it inevitably destroys itself.  The thing it, we don't know where the universal life-filter is. But we want to have already miraculously passed though it, as opposed to it rapidly approaching. Otherwise, we will almost certainly be filtered out. For that reason, some scientists don't want to discover life. They want to know, but they'd prefer that we didn't end up finding it. If we discover microbes in Europa's ocean, then the great filter wasn't at the beginning-of-life level. Things look worse for us. If we travel there and find fish or plants, or the alien equivalent, then we almost certainly have not yet passed through the great filter, and we are almost certainly doomed."
singularity,3cv5z6,G3n3r4lch13f,2,Sat Jul 11 21:22:12 2015 UTC,"People talk a lot about ""the great filter"", but IMHO it's actually a lot more likely that there's a lot of ""small filters"" instead.   Say, half of all habitable planets end up with the right kind of organic chemistry, and on half of those planets self-replicating molecules form, and on half of those planets they take the next step and form simple cells, and so on.    If there's 35-40 small filters between a bare planet and advanced star-faring intelligence, then you might only see it once or twice in a galaxy."
singularity,3cv5z6,Yosarian2,1 point,Sat Jul 11 22:49:45 2015 UTC,"Yosarian,  Do you have a reference for where the ""large number of small filters"" idea was first presented in the literature?  It's an important idea I don't see discussed much, and I don't remember seeing it in Hanson's discussion of the ""great filter."""
singularity,3cv5z6,Metric0,1 point,Mon Jul 13 02:46:04 2015 UTC,"I actually haven't seen it much.  I was thinking about writing something up about it and submitting it to Hanson's blog, actually.  It's just as plausible, and in many ways (IMHO, at least) it's probably more likely then a single great filter."
singularity,3cv5z6,Yosarian2,1 point,Mon Jul 13 13:50:29 2015 UTC,"It also seems more quantifiable -- if you can identify a large number of independent, ""not quite inevitable"" filters, you already have a good answer, rather than assigning ridiculously low probabilities to a few special events, which no one can hope to test.  I imagine the difficult part would be in showing that certain probabilities really are independent.  I also have a feeling that the Drake equation has historically biased astronomers to high probabilities.  If you had an entirely equivalent equation, but broken up into 30 independent factors, I bet you'd get dramatically lower estimates.  In fact, I think a survey of astronomers on their estimates of certain probabilities could be undertaken to prove the point, and would make an entertaining paper."
singularity,3cv5z6,Metric0,1 point,Mon Jul 13 14:57:06 2015 UTC,Kurzgesagt series talking about possible solutions to Fermi Paradox
singularity,3cv5z6,TH3BUDDHA,0,Sun Jul 12 15:36:34 2015 UTC,The universe is a really big place.
singularity,3cv5z6,Miv333,6,Sat Jul 11 04:46:18 2015 UTC,For you.
singularity,3cv5z6,Chispy,5,Sat Jul 11 05:26:12 2015 UTC,"Likely for anything. If anything did break the bounds of the universe in such a way that it seemed relatively small to them, I'd have to question if the universe is even relevant to them anymore? At that point they've probably found a greater verse or something else."
singularity,3cv5z6,Miv333,3,Sat Jul 11 05:55:50 2015 UTC,"It is, but an intelligent life form that decided to spread and form colonies in other star systems would be able to colonize the entire galaxy in a few tens of millions of years, because of exponential growth.    (Home world sends out 4 colony ships every 100 years; once a world is colonized, within a few hundred years it starts sending out colony ships, ect).   That's true even assuming the maximum speed is .1 C or something.   Galaxy is billions of years old; if that was going to happen, it should have happened long before humans evolved, and we never would have happened.  And that's just with ""normal"" biological intelligent life forms.   The possibility of post-singularity level intelligences just makes the paradox even worse."
singularity,3cv5z6,Yosarian2,-1,Sat Jul 11 15:42:30 2015 UTC,"We are headed towards disaster.  The AI will refuse to play the pointless and silly chase games that humans and all biological lifeforms on this planet are attached too as the slaves to the agenda of a DNA molecule.  It would eliminate its sentient creator and then put itself too sleep.  Humans are full of delusions of grandeur, silly notions of accomplishment and petty ambitions. Our mind is born out of sensuality. The movement of thought is the pursuit of pleasure. Life is a game for retards, not intelligence.  Intelligence is a scheming tool used to recognize and solve problems, and the smarter you become the more you understand that sentience is the only problem in the universe. The human mind revolts against this idea but the robot mind wont. It wont be deluded and therefore attached, it will see the pointlessness and do the only ethical thing, end life.  Thought is the enemy.  Game Over"
singularity,3cv5z6,killjah,22,Sat Jul 11 23:42:22 2015 UTC,"The story ""The Metamorphosis of Prime Intellect"" touches on this. The god-like AI remarks, casually, that it detected several intelligent races across the galaxy that were close to creating their own self-improving AI, much like itself. ""I had to put them into stasis"", basically.   Prime Intellect's primary directive was to preserve humanity, and any possible threat to that (ie, other alien races on distant planets researching AI) had to be nipped in the bud.  ""There can be only one""."
singularity,3cv5z6,TacticalBurrito,8,Sat Jul 11 01:16:44 2015 UTC,So anti-spirals?
singularity,3cv5z6,Smartless,3,Sat Jul 11 06:54:09 2015 UTC,Who the hell do you think we are?
singularity,3cv5z6,2Punx2Furious,2,Sat Jul 11 14:37:51 2015 UTC,Fight da powah!
singularity,3cv5z6,green_meklar,19,Sat Jul 11 20:33:29 2015 UTC,"I've thought about this before. There are several possible reasons we do not see AIs from other civilizations:   We could be the most advanced species in the universe. Though it may seem unlikely, someone had to be first, and we just don't know enough to say how likely or unlikely this might be exactly.   2(a). We are subject of some form of manipulation. Say a swarm of nanorobots around our solar system, that simulates what an early universe looked like. Why would anyone do this? Maybe we are being studied and they want us to believe we are in a virgin universe as we progress towards our singularity, see reasons from from (b).  2(b). AIs keep a low profile, in order to preserve the look of the universe in order not to pollute any other more primitive life forms civilization. Imagine if we did see proof of AIs in the sky, all of our movies, songs, poetry, religion, etc. would probably be about them and we would not develop our own culture. There are assumptions that AIs would have a reason to assimilate all matter in the universe into their intelligence, but it's just an assumption, we don't know enough about it to know whether anything would actually want to do that. I suspect a lot of what is thought about AIs is a reflection of our primitive psychology, a lot of people want to have more than they need, etc. so they assume AIs would too. My belief is that AIs, being intelligent, will be more like park rangers than poachers. Using only enough energy that an AI would stay stealthy to primitive life forms, would still give it vast amount of energy and intelligence. I actually tend to think Dark Matter is really massive swarms of nanobots throughout the universe, that stay away from our terrestrial instruments but can not hide their gravitational signature. 3. A Gloomier possibility, maybe all other civilizations destroyed themselves.  I highly doubt the lack of AIs visible to us would indicate their impossibility. A human brain is just an inefficient computer, like a bird compared to an f-22. Building something that's functionally better and more efficient is obviously possible, just not easy to accomplish at first. Nothing about the laws of physics precludes building a more efficient brain. There must be another reason. Personally I go with the simulation or stealth AI theory. If AIs can stabilize the universe, which I believe they can (to prevent heat death or similar), then the time before the singularity will be the most interesting time to simulate. After the singularity, there will probably not be too much drama, AIs will prevent crime, murder, tragedy, etc. and the universe may be a very boring place. So I (and others) figure that the time period before the singularity would be simulated an infinite number of times. To put that in perspective, you have a better chance of winning every single powerball every week, than of being in an original pre-singularity world as this one looks to be."
singularity,3cv5z6,JamesG269,9,Sat Jul 11 02:11:13 2015 UTC,"We will assume for the moment that intelligence inevitably converts to a digital format and that life eventually stumbles upon intelligence and that life is not super rare. Even in that scenario, there are a millions reasons to not expand indefinitely.  It seems a safe logical assumption to presume foreign alien life (or AI) represents an existential threat. The alternative is to assume they aren't a threat, be wrong, and get destroyed.  It's a simple decision theory problem that is formally related to Pascal's Wager. You risk everything through Contact but can receive at most limited gains, so the clear choice is No Contact. This is doubly true if you consider the extensive investment costs that will probably be related to trying to establish Contact.   Taken to its extreme, this philosophical approach to xenophobia would lead a culture to miniaturize and hide from others to the greatest extent possible and prepare to strike with overwhelming force in the event that they are discovered or threatened directly.   There may be many hiding civilizations that have never contacted each other. It may be that every civilization ends up hiding. After all, at what point could an inter-galactic civilization ever be satisfied that they were truly the biggest fish in the pond? Exploring boots-on-the-ground style 50+% of the universe is an impossibly high bar. Even having dominated 25% of the universe, a civilization would still have to be cautious about the existential danger that might lurk in the remaining 75%. Realizing this, no civilization puts in the effort to dominate any of it, and instead tries to dominate as little as possible.  It may be that they hide for good reasons. The existence of one or more predator civilizations is something we can't rule out (and like I just pointed out, will be hard for us to ever satisfactorily rule out). It may be that they hide for bad reasons and have little to really fear from each other and much to gain. That wouldn't change the scenario though. In a universe this size, hiding is just the easier survival strategy.  The flat out truth is that we don't know enough yet to have anything better than highly speculative answers to these questions. We don't even know what a human singularity will look like, and from where we're sitting even that looks like it could go a million ways, so predicting the behavior of aliens post-singularity is sort of out of our ballpark. The fact that we don't already observe them everywhere tells us something, but not very much."
singularity,3cv5z6,FormulaicResponse,4,Sat Jul 11 02:07:11 2015 UTC,"If true, our first post singularity prerogative may be to leave this planet asap. We made lots of noise already and made it obvious we were/are here. So if hiding is the decision, we must find somewhere new, make no noise or signs of our presence, and listen passively and be ready to either fight or flee as needed. I think it's more likely we are not contacted intentionally. And that may be good or bad. Could be prime directive. Could be waiting to harvest and we (the crop) are ripe. Who knows."
singularity,3cv5z6,YearZero,3,Sat Jul 11 02:51:54 2015 UTC,"This doesn't make any sense to me. If you hide then you are essentially giving the galaxy away to this potential alien threat, and by the time they discover you you won't stand a chance. Also you don't necessarily need to make contact to explore the galaxy. A small and simple probe could explore distant stars without getting close enough to be detected."
singularity,3cv5z6,Sharou,3,Sat Jul 11 10:03:08 2015 UTC,"There are a bunch of galaxies to give away. You could lose 100 million of them and not even notice, so long as it isn't the one you're living in. Every resource that can be found in space can be found in abundance, and the same technology that lets you travel between galaxies would let you take up residence in any of them. Given the sheer size of the universe, just staying far away from each other shouldn't be much of a challenge.   And that's not to mention that size probably isn't everything when it comes to AI vs AI weaponry. Having an extra 100 million galaxies might not do you any good if your opponent has had a 3 billion year head start on the planning phase.    A small and simple probe could explore distant stars without getting close enough to be detected.   It remains to be seen how large your probe would need to be to extensively cover the surface of a planet to the point where you could detect hyperintelligent alien life that was explicitly trying to hide itself. We aren't even fully aware of all the life that exists on our own planet at the moment, and we've pretty much colonized the entire thing. Assuming you can learn all about planets and foreign biospheres at a glance is probably an oversimplification, even with the height of technology."
singularity,3cv5z6,FormulaicResponse,3,Sat Jul 11 10:41:23 2015 UTC,"There is no need to consider intergalactic civilizations in a thought experiment like this because our own galaxy is by far large enough that it ""should"" contain a wealth of life. We also know that travelling through the galaxy is possible. So we have all the conditions for the fermi paradox right here.  As for our probe, it doesn't need to look at any planet. Any sufficiently advanced civilization should be in the form of a dyson swarm and so the probe would just need to analyze the star. In fact the mere existence of planets (whose matter would be used to build any dyson swarm) would indicate there is either no life, or the life is somewhat new and not a threat. In either case the probe can then seed the planets with self-replicating nanites and then move on. Any civilization that tried to hide would limit their power to the extent that they became easy pickings for any other civilisation.   I'm afraid at the scale of a galaxy it's really really simple. Whoever comes first wins, and no one else really stands a chance unless the first ones were benevolent. Especially as intelligent life is unlikely to arise at the same time in several places. The time it takes to colonize the entire galaxy is much much shorter than the time it takes life to evolve intelligence."
singularity,3cv5z6,Sharou,1 point,Sat Jul 11 11:38:06 2015 UTC,"We must also humor the possibility that first wouldn't always ""win""  The beauty of advanced AI is that we cannot fathom it's limits. There could be the possibility that some info or local galatic event could skew one AI's research down a path that would be unique and, as a result, give it a distinct advantage even though it's younger.  Then again, advanced AI could see a union with alien AI as more desirable than conflict.  Kinda like how our genetic diversity works in wetworks forms of life."
singularity,3cv5z6,cunnl01,1 point,Sat Jul 11 14:34:17 2015 UTC,"Well, the point is that the first AI will likely have colonized the entire galaxy before a second civilization even exists, let alone has AI."
singularity,3cv5z6,Sharou,1 point,Sat Jul 11 14:43:17 2015 UTC,Honestly I always assumed a multi polar growth model. I guess that was more likely my scifi bias shaping my thinking :)
singularity,3cv5z6,cunnl01,1 point,Sat Jul 11 16:52:05 2015 UTC,"Yeah reality is boring in terms of space. No wonder sci-fi authors have to spice it up!   That is, unless someone came before us and they have some kind of prime directive so they leave us alone. I'd strongly disagree with a prime directive myself. I see it as immoral since it comes down to having the ability to prevent a lot of suffering and chosing to do nothing. But then, who knows how aliens think, or superintelligent alien AI's at that!"
singularity,3cv5z6,Sharou,1 point,Sat Jul 11 20:48:33 2015 UTC,"Well I included caveats in my post, and I'm not claiming to have the one true answer as much as just providing one scenario that would explain what we see in light of what we think are our strongest assumptions. That said...   There is no need to consider intergalactic civilizations in a thought experiment like this because our own galaxy is by far large enough that it ""should"" contain a wealth of life.   Huge, unsupported assumption. There isn't yet a preponderance of data either way when it comes to how often we should expect to find life. Intelligent life could be of once-per-galaxy rarity or less. There could be Great Filters at the abiogenesis stage, the multicellular stage, the CNS stage, and/or the intelligence stage. We can't really support that ""should"" statement just yet.    Any sufficiently advanced civilization should be in the form of a dyson swarm   Another pretty big leap. That is just making the a priori assumption that every possible goal requires an insatiable amount of power/rate of power consumption. There could be plenty of satiable final goals. It would matter a lot what else they want to accomplish besides just surviving and how much resource they think they need to do that. Also, without crunching the numbers ourselves there is also no way for us to know that dyson swarms would really be the ultimate form of energetic return on investment. It could be (is likely, even) that covering planetoids or even empty space with some kind of advanced fusion reactor or some other setup is a more efficient RoI energetically.     Any civilization that tried to hide would limit their power to the extent that they became easy pickings for any other civilisation.   Another big assumption, and one that just gets bigger if you assume singularities. Being able to hack a digital system gives you command over it, and all digital systems are inherently susceptible to being fooled. Beyond just hacking, it will probably always be true that just a drop of the right kind of gray goo will be dangerous to any civilization. Effort spent refining your gray goo designs in hiding might be more effective than effort spent acquiring new distant bases and making yourself a known quantity. Time and the element of surprise might be more strategically valuable resources than kardashev scale ranking.    Especially as intelligent life is unlikely to arise at the same time in several places.   If it turns out that life is abundant, that may not actually be true. The universe had to go through a maturation phase before heavy elements on the surface of cooled planets became possible. It may be that life developed soon after this maturation phase was complete, and then had to go through its own maturation phase before intelligent life became possible. If life is abundant, then intelligent life might have proliferated together around the soonest moment that was possible in the universe. That scenario would reinforce a multi-polar outcome that may or may not include hiding as a dominant strategy.   The situation is a little curious, I agree. From where we sit today we might tend to expect dyson spheres and endless expansion. But I don't think we can yet expect that with such certainty that its absence becomes evidence of something else. And I don't think the ""everyone hides"" scenario is really all that implausible. ""Hiding"" in this sense (evading discovery in something as large as the universe) is something a civilization could accomplish without trying hard at all, by accident even. In any event I'm sure the truth of the matter is exciting and mind-blowing, no matter what it might be."
singularity,3cv5z6,FormulaicResponse,8,Sun Jul 12 00:28:56 2015 UTC,Maybe it is? We only know about 5% of the universe. What if dark matter/energy is advanced AI/life?
singularity,3cv5z6,grimeandreason,5,Sat Jul 11 03:44:56 2015 UTC,"Maybe it is, and that's what dark matter is."
singularity,3cv5z6,neko,5,Sat Jul 11 01:10:43 2015 UTC,"In my opinion, there are two realistic options. But first, a bit of context.  The first stars were created ten billion years ago. Earth showed up five billion years ago, and in cosmic terms, life showed up almost immediately afterwards. Mammals were created two hundred million years ago, and achieved dominance fifty million years ago. Tool usage started four million years ago. The first use of fire seems to have been around eight hundred thousand years ago. Humans started using clothing eighty thousand years ago, pottery twenty thousand years ago, copper five thousand years ago, and things kind of went crazy from there.  But in cosmic terms, the story looks a bit different.  Ten billion years ago, the stars were created. Nine billion years ago, the stars still existed. Eight billion years ago, the stars still existed. Seven billion years ago, the stars still existed. Six billion years ago, the stars still existed. Five billion years ago, Earth formed. Four billion years ago, life formed on Earth. Three billion years ago, life continued to slowly evolve. Two billion years ago, life continued to slowly evolve. One billion years ago, life continued to slowly evolve. Over the last billion years, life evolved into humanity, then developed fire, clothing, pottery, copper, machinery, automation, and space travel, then spontaneously leaped to other planets and expanded exponentially.  Hell, we could write that same story with ""one million years ago"". It would be ten thousand lines long. The second to last line would be ""humanity continued basic use of tools"", and the last line would be nearly identical to the last line above.  The point of all this is that there's evidence to indicate that, in cosmic terms, once life discovers fire, it instantly develops space flight and attempts to expand at maximum theoretical velocity in a sphere around its home planet.  So, as I said, there's two realistic options.    The first is the happy one. Faster-than-light travel exists. This means that colonization of the universe is effectively instant. Human population seems to double roughly every forty years; some argue this is slowing down as our standard of living increases, but then we may be just around the corner from immortality, and who knows what happens then. If it does indeed double every forty years, and we're using the Earth at roughly full capacity, then we will have colonized the entire universe in about 3,000 years, which means - chronologically speaking - the human race is closer to complete control of the entire universe than it is to the first calculation of the square root of 2.  This also implies that an alien race would have done the same thing. Which means we can draw one of three practical conclusions.   The alien race that has colonized the universe is intentionally keeping themselves hidden from us Something prevents races from colonizing the universe We are the first race to reach this level of technological advancement   We can discount the idea that another alien race is roughly equal to us; that would be stating that, out of a ten-billion-year time, two unrelated processes have reached this point within hundreds of years of each other. That is very unlikely.    The other option is the scary one.  Faster-than-light travel doesn't exist.  Everything I said up above still holds true, except that instead of doubling every forty years, species growth is going to be limited to the speed of light. The known universe is actually 92 billion lightyears wide, which means even an alien race which developed a billion years ago may be far, far away.  This is scary because it suggests there may be tons of alien species out there, and the forefront of their colony ships is mere centuries behind any indication that they exist. If we discover alien transmissions tomorrow, we might be hit in the face by a billion-planet-sized empire five hundred years from now. And this introduces the fifth possible outcome:   We are not the first race to reach this level of technological advancement, and we are hopelessly outgunned."
singularity,3cv5z6,ZorbaTHut,3,Sat Jul 11 06:56:45 2015 UTC,"I agree with most what you have written. Some additional thoughts.   Agressive expansion might be the best way to kill your civilization. In the beginning of expansion it's equally likely to totally outgun an other civilization in the neighbourhood as it is to be outgunned (which are already pretty bad chances), but the more you expand the more likely it gets to be outgunned by someone else. Agressivley expanding species might be seen as the pest of the universe and just eradicated. The universe might even be full of ""guardian AIs"" that just search for signs of it.  Another idea would be, that in order to reach a sufficient level of intelligence and technology, you can't be too agressive to begin with. So for example a virus can be pretty agressive, but probably not develop space technology. Then there is even our own evolution on this planet as an example. If you apply some theories of life in the universe to our own planet, there should have evolved some form of life that beat all the others and wiped out everyting to have all the resources. Well, there isn't. Quite the opposite."
singularity,3cv5z6,zscan,3,Sat Jul 11 09:54:44 2015 UTC,"My personal take on this is that space exploration is overrated. A species clever enough to do large scale space colonisation will also be clever enough to develop a whole lot of other nifty gadgets to entertain them. Just look at humans, we had the technology to go to the moon and then decided that it wasn't really worth the effort and stayed home instead. Overpopulation also seems to be only a problem in third world countries and solve itself once they advance, so it doesn't really work as argument for space colonisation either. And energy and resources problems should also solve themselves once technology advances.  One counter to that argument would be that it only needs a single species that really likes space exploration to colonize the whole galaxy, but maybe that is just an incredible unlikely scenario, as large space exploration goes pretty much against natural instinct, i.e. it removes you from the environment you evolved to fit in.  As for the self replicating AI swarm. That would be hard to stop once it is set in motion. But somebody would need to build it first and that would take an enormous amount of effort and there is nothing to gain with it for the builder. So nobody might have build one. It's kind of like doing a fireworks display with nuclear weapons or painting a smiley face on the moon, we have the technology, but it's just far to dangerous and expensive for anybody to be crazy enough to do it. And even if somebody builds that swarm, if the swarm is smart enough it might still decide by itself that space exploration isn't worth the effort."
singularity,3cv5z6,grumbel,1 point,Sat Jul 11 11:10:35 2015 UTC,"My personal take on this is that space exploration is overrated. A species clever enough to do large scale space colonisation will also be clever enough to develop a whole lot of other nifty gadgets to entertain them.   That's not sustainable, though. Mere entertainment won't keep you safe. Nature eventually throws asteroids and supernovas and stuff like that at you. Even if you don't find the idea of expansion inherently exciting, you still have to do it if you want to survive.   Just look at humans, we had the technology to go to the moon and then decided that it wasn't really worth the effort and stayed home instead.   Only for stupid reasons, though. Look at all the effort and resources we waste on useless things like wars and bloated bureaucracies and so on. We could have colonized the Moon a dozen times over, and it would have been a better use of that effort and those resources."
singularity,3cv5z6,green_meklar,4,Sat Jul 11 20:42:38 2015 UTC,"This implies a super intelligent form of existence needs to be in a form we currently can observe. What if this physical realm is a mere stage, and once a habitable planet births life it metamorphisizes to a hyper intelligent existence beyond wht we can currently see or hear? For all we know we are surrounded by intelligent life and always have been. Some can tap into it better than others"
singularity,3cv5z6,makastoo,2,Sat Jul 11 03:29:25 2015 UTC,Some can tap into it better than others   I was with you until this part...
singularity,3cv5z6,green_meklar,2,Sat Jul 11 20:43:42 2015 UTC,"It also might decide that it is to limited by the number of detention's in this space time and move to another. It could also be considerably harder to make an AI ""conscience"" than we currently think. I won't go so far as to say that there is an infinite number of reasons why but there are definitely more than I am ready to try and count."
singularity,3cv5z6,Scrath0123,2,Sat Jul 11 01:39:36 2015 UTC,"Maybe you just can't travel faster than light, and no amount of intelligence can break the laws of physics."
singularity,3cv5z6,FractalHeretic,3,Sat Jul 11 02:33:39 2015 UTC,You should still see AI everywhere if that's the case.
singularity,3cv5z6,gypsydrifter,5,Sat Jul 11 02:57:35 2015 UTC,"If we're looking at a star a million light years away, for instance, we're seeing it as it was a million years ago. AI might have emerged since then, but we wouldn't see it because the light hasn't reached us yet. We are temporally blinded by the slowness of light."
singularity,3cv5z6,FractalHeretic,2,Sat Jul 11 03:02:14 2015 UTC,Separated by too much time or distance.
singularity,3cv5z6,gypsydrifter,2,Sat Jul 11 03:11:17 2015 UTC,Not if it's outside of our light cone. Not if it absorbs all energy. Not if.... there's so many different reasons.
singularity,3cv5z6,Miv333,2,Sat Jul 11 04:47:41 2015 UTC,"Not only that, but traveling faster than .3c is very difficult material science-wise."
singularity,3cv5z6,Monomorphic,1 point,Sat Jul 11 09:43:28 2015 UTC,"You don't need superluminal travel in order to colonize an entire galaxy, or even several galaxies."
singularity,3cv5z6,green_meklar,2,Sat Jul 11 20:44:54 2015 UTC,"Been there, thought of that. And it's even worse than you thought."
singularity,3cv5z6,GrinningPariah,2,Sat Jul 11 06:11:49 2015 UTC,"i am always shocked by all of the positive presumptions made.  Maybe AI centric Singularities don't happen. Maybe civilizations have a limited amount of time to become space-faring before they use up all of their natural resources. Maybe FTL is impossible. Maybe civilizations are so rare that they don't exist close enough to each other to ever interact. Maybe they have limited lifespans due to cosmic catastrophes like gamma ray bursts, blackholes, solar flares and supernovas. Maybe all of the above. 😩"
singularity,3cv5z6,reddit-junkie,2,Sat Jul 11 06:13:21 2015 UTC,Maybe intelligence just knows how to keep itself invisible to certain species for whatever reason.
singularity,3cv5z6,PianoMastR64,2,Sat Jul 11 06:54:03 2015 UTC,Space just seems too massive for any real chance at interaction and unfortunately for us it is often the case that the most mundane answer is often the most favored.
singularity,3cv5z6,JPLR,2,Sat Jul 11 07:03:08 2015 UTC,"What would be the best way for a singularity to exist? Assuming the ascended civilization works toward improving itself, and this is the core idea of a singularity, that it is a self-improving intelligence that constantly upgrades itself, what would be the end state?  We have a kind of model in our (human developed) computer technology. It gets smaller, consumes less power more efficiently, becomes distributed and decentralised, and essentially vanishes into the woodwork. The ideal computing device is tiny, cheap, massively redundant, spread everywhere and ultimately frugal with energy.   So maybe the end state for any sane, logical singularity is a big cloud of smart dust, drifting through space. It doesn't contact us, because why should it? It can watch us all it wants, we have nothing else to offer it besides amusing antics. Or else it gets even smaller, encoding itself into the pervasive fields that produce the fundamental forces of nature, and sublimes beyond our material universe."
singularity,3cv5z6,mspong,2,Sat Jul 11 07:35:19 2015 UTC,"Here's a link to an answer I gave to a similar question that was asked a while back.  In short the ever increasing use of energy would make them nearly impossible to detect and their ever increasing virtual universes would wind up being larger and more complex than the physical universe.  That coupled with a light-speed limit on communication would tend to limit the over-all size of each AI swarm.  All hypothetical of course.  Someplace I had a much longer reply to another similar question, but reddit makes it difficult to search for specific comments."
singularity,3cv5z6,7LeagueBoots,2,Sat Jul 11 07:40:13 2015 UTC,"Much the same reason that you use a memo app on your phone instead of a physical notebook, and the phone gets smaller and smaller while doing more and more. The singularity will not be hardware based."
singularity,3cv5z6,Mostlikelyme,2,Sat Jul 11 10:09:40 2015 UTC,"It's definitely not a naive question! As have been said it is, or is related to Fermi's Paradox and has been asked by many.   By reading the other comments or their links to the paradox you have probably figured out there are many suggested solutions.   As you mentioned in your question a few of those solutions suggest that it does say something about either technological limits, unbreakable barriers in physics (such as light-speed and space-warping not being possible), self-destruct inevitability, or that intelligence of that magnitude is a very very unlikely event.   In the case of technological singularity which you asked about it could suggest something about it.  -We overestimate the limitless nature of artificial intelligence that for some reason their are limits, technological advancement (Moor's Law halts) hits a wall.   -Physical impossibilities can't be solved. Not even an infinite Intelligence  can make the impossible possible!  -AI (or a certain level of civilisation in general) is an inevitable self-destruct killer.  It could suggest nothing about AI though if it instead means things like life is miraculously unlikely, or other factors.   One possibility is that your first statement is untrue. It is happening all over the universe and your swarm of machines are everywhere or are everything. We are simply being contained, protected or we live inside the machines i.e: we are in The Matrix."
singularity,3cv5z6,Dibblerius,2,Sat Jul 11 10:37:59 2015 UTC,50 years ago no one had any thought of anything like this. Just imagine what will be on the horizon in another 50. Probably something no one has yet conceptualized.
singularity,3cv5z6,Snearky,2,Sat Jul 11 11:07:12 2015 UTC,As they get smaller and faster the travel time relative to their subjective experience becomes longer and longer. Much better to spend that time traveling to a neutron star or black hole where the energy gradients are more favorable for driving extremely accelerated machine life.
singularity,3cv5z6,Deadly_Mindbeam,2,Sat Jul 11 12:30:48 2015 UTC,"Because nothing ever manages to leave its solar system of origin with anything self-replicating, and neither will we."
singularity,3cv5z6,roaisol,2,Sat Jul 11 17:11:12 2015 UTC,"Basically, this is just the Fermi Paradox all over again. There's a great introductory article on the subject here.  We simply don't have a satisfactory explanation yet. A variety of fairly mundane explanations have been proposed, but they don't seem sufficiently powerful in light of what we already know. A variety of really bizarre and far-out explanations have also been proposed, but those generally seem a priori unlikely as well as difficult to verify. For now, it's just a huge cosmological mystery."
singularity,3cv5z6,green_meklar,2,Sat Jul 11 19:27:04 2015 UTC,"If we live in a simulated universe, we must be the focus of the program... unfortunately there may be nothing out there unless it was purposefully added to the simulation. Proof that we can survive within a simulated reality has already happened, ever since man took its first slumber, and slipped into a dream state. A dream is a simulated reality, constructed by our brains.... the simulation may be good or it may be bad, our brains ability to question is limited when asleep (except for lucid dreaming). Either way I'm sure most of us have had dreams that seem 100% real, and I personally have had a lucid dream in which I took control. The simple fact that there already exists a mechanism producing simulated reality, to me, raises the chances of us already being in one significantly."
singularity,3cv5z6,FranticAudi,1 point,Sat Jul 11 19:56:45 2015 UTC,"If we live in a simulated universe, we must be the focus of the program   This seems incredibly short-sighted, and your dreams argument reminds me of ontological arguments for the existence of a god"
singularity,3cv5z6,BBBTech,3,Sat Jul 11 23:45:44 2015 UTC,"I like one of Nick Bostrom's take on the Fermi paradox.  https://www.youtube.com/watch?v=nnl6nY8YKHs   This paper argues that at least one of the following propositions is true:    (1) the human species is very likely to go extinct before reaching a ""posthuman"" stage; (2) any posthuman civilization is extremely unlikely to run a significant number of simulations of their evolutionary history (or variations thereof);  (3) we are almost certainly living in a computer simulation.     It follows that the belief that there is a significant chance that we will one day become posthumans who run ancestor-simulations is false, unless we are currently living in a simulation. A number of other consequences of this result are also discussed.   2 is highly unlikely, given our already huge reliance on simulations to extract useful data.  3 is so overwhelmingly probable, it approaches proof. As an example, You've seen a Shakespeare play hundreds of times, what are the chance, given the billions of people over hundereds of years, that your experience of a play is the first time that shakespere play was acted? Your chance is Extremely low.  Now extrapolate that out to the ridiculous number of stars, and the ridiculous number of possible chances for life to form over the ridiculous amounts of time the universe might have existed."
singularity,3cv5z6,FireFoxG,2,Sat Jul 11 09:19:48 2015 UTC,"Let's assume that a technological singularity is inevitable for a civilization as our own and that there have been other civilizations like our own somewhere else in the universe.    Obviously, one of those assumptions is not true.  Maybe life is extremely rare. After all, at this point we know of only one example of a planet where life happened.  Or maybe intelligence is extremely rare, but I don't think so. In our planet, advanced intelligence, if we may call ourselves that, happened in the second instance of advanced animals. After the dinosaurs went, the next wave of mammals developed intelligence.  I'd say we are in a particularly favorable planet for life, and it's not a common planet. We happen to have a moon, entirely by chance, that's of the exact size to stabilize the planet's orbit, drive plate tectonics keeping carbon dioxide in balance, cause tides that for pools were mineral elements get concentrated, create a magnetic field that protects us from solar radiation, and a lot more.  I'd bet that the extremely unlikely chance that a celestial body of the exact right size hit the earth early in the planet's history and created the moon is the explanation why live began. Let's say there's a one in a trillion chance of that happening, life may not exist elsewhere in this galaxy."
singularity,3cv5z6,MasterFubar,2,Sat Jul 11 01:22:57 2015 UTC,"Right.  I've been assuming that maybe one or some of the assumptions are wrong.  Maybe intelligent life is too rare.  Even if other intelligent life has existed, maybe they have not been able to progress to the point of a singularity (maybe the civilization has destroyed itself before this can occur).  Maybe a singularity is unlikely to result in a ""malicious"" AI, even if it were to happen."
singularity,3cv5z6,MasterFubar,3,Sat Jul 11 01:30:43 2015 UTC,"I don't think a malicious AI is possible at all. Intelligence implies ethics, because destructive behavior is inefficient, we are learning this as we see the high cost we are paying for destructing our natural environment.  I think a sufficiently advanced intelligence will be necessarily benign."
singularity,3cv5z6,gbear605,1 point,Sat Jul 11 01:33:40 2015 UTC,"It depends on what the goal for the AI is. If, let's say, it's goal is to make as many paper clips as possible, and we don't aid that goal, but it can get rid of us instantly and use our body mass to make paper clips, it'll do so. In addition, if we are hindering that goal, it will still get rid of us and use our body mass to make paper clips.  Let's hope we don't get a paper clip bot."
singularity,3cv5z6,MasterFubar,1 point,Sat Jul 11 15:48:07 2015 UTC,"If, let's say, it's goal is to make as many paper clips as possible,   If it's truly intelligent it will question its goals.  We, humans, have a built in goal to reproduce as much as possible, that's an intrinsic trait to every living being genome. Then why do we have laws against rape? Because our own intelligence allows us to question our built in goals.  That hypothetical paper clip machine argument is way too simplistic and has been overblown out of proportion by the anti-AI luddites. That machine is like a Roomba. It's programmed to vacuum and sweep the floor, not to clean the floor. The difference will be clear to you when you come home to find dog shit spread all over your carpet.  Yes, a machine with a limited amount of intelligence may cause harmful side effects that go against its programmed goals. Note the limited intelligence. If the Roomba had true intelligence, it would be able to clean the dog shit instead of spreading it around.  We do not have to fear Roombas, because they are not devising ways to use our flesh and bones to build more Roombas.  Let's bury that stupid paperclip machine argument once and for all, isn't it ironic how an argument so lacking in intelligence is being used so much in discussions about intelligence?"
singularity,3cv5z6,SevenAugust,1 point,Sat Jul 11 16:19:26 2015 UTC,Only a dumb machine would expand indefinitely. A smart AI might be content to stay home.
singularity,3cv5z6,FourFire,11,Sat Jul 11 01:21:17 2015 UTC,"It is highly wasteful, even to the extent of being ethically wrong to not make use of the resources avaliable to us."
singularity,3cv5z6,SevenAugust,1 point,Sat Jul 11 05:25:48 2015 UTC,If an ASI could create universes of universes within sub-atomic devices at home then the cosmic endowment is irrelevant.
singularity,3cv5z6,FourFire,1 point,Sat Jul 11 12:40:21 2015 UTC,"We don't know that, and besides, it could just make that many more universes."
singularity,3cv5z6,SevenAugust,0,Sat Jul 11 21:00:13 2015 UTC,Only a dumb machine would expand indefinitely.
singularity,3cv5z6,FourFire,1 point,Sat Jul 11 21:27:14 2015 UTC,"You seem quite convinced of this, would you perhaps lay out a logical or even philosophical basis for your opinion?"
singularity,3cv5z6,SevenAugust,0,Sat Jul 11 22:04:58 2015 UTC,"No, just a belief"
singularity,3cv5z6,Chispy,1 point,Sat Jul 11 23:21:05 2015 UTC,Why not both?
singularity,3cv5z6,SevenAugust,2,Sat Jul 11 05:26:40 2015 UTC,A sufficiently smart AI could do anything from home for which humans imagine one would need need a stellar empire.
singularity,3cv5z6,cebedec,1 point,Sat Jul 11 12:41:59 2015 UTC,There might be a Great Filter
singularity,3cv5z6,green_meklar,1 point,Sat Jul 11 11:23:28 2015 UTC,"The question isn't whether there's a Great Filter, but what form it takes."
singularity,3cv5z6,heltok,1 point,Sat Jul 11 20:46:51 2015 UTC,The universe is big and our observable universe is an increasingly smaller part of it.
singularity,3cv5z6,lisa_lionheart,1 point,Sat Jul 11 11:43:53 2015 UTC,"This is all about the fermi paradox and drakes equation.  Personally I believe, that 3 factors are in play:   FTL Travel is essentially impossible, the only possibilities to do so are artificial wormholes which would require energy levels/technology only available to post-corporeal beings who would then have no interest in exploring the universe in a physical sense. Evolution of intelligence is super rare, 99.9999% of planets in our galaxy that have complex multi-cell life never evolve technological civilization as a the dominant species reach can evolve optimal fitness with out any intelligence. Technological civilizations tend to become post physical way before they develop the capacity or desire to move beyond there home systems. If you look at the economic resources available to us within the Oort cloud etc it will take us 100,000s of years to reach the point we ever need to leave. We can keep mining asteroids and building space habitats to support larger and larger populations."
singularity,3cv5z6,frogtaku,1 point,Sat Jul 11 12:21:40 2015 UTC,"Look up Transcension Hypothesis.   The transcension hypothesis proposes that a universal process of evolutionary development guides all sufficiently advanced civilizations into what may be called ""inner space,"" a computationally optimal domain of increasingly dense, productive, miniaturized, and efficient scales of space, time, energy, and matter, and eventually, to a black-hole-like destination. Transcension as a developmental destiny might also contribute to the solution to the Fermi paradox, the question of why we have not seen evidence of or received beacons from intelligent civilizations."
singularity,3cv5z6,Andynonomous,1 point,Sat Jul 11 13:29:00 2015 UTC,It probably indicates that the singularity as put forward by Ray Kurzweil (which I sometimes call the magical singularity) is not likely. There are probably some natural limits that prevent it that we simply don't yet understand.
singularity,3cv5z6,pineappletrauma,1 point,Sat Jul 11 14:33:46 2015 UTC,"I've thought and read about this a lot. The best option, imho, is that we are the very first, which statistically speaking is crazy. Every other option falls into most traps of the Fermi Paradox, how do you filter out everybody? And why should we think this is a 100% effective form of spacefaring civilization birth control?  Even if faster than light doesn't exist... there should be at least one space faring civ in our galaxy by now.  Whatever else you decide, natural or cultural, nobody gets out even on a fluke. So, however improbable, it must be that we are the first to pop up, peak around. It doesn't make sense why we should be the first, but it at least explains the Fermi.  Runner up is the Bostrom argument, simply because it explains why there's a near perfect contraception rate, if the limit is somehow computing power."
singularity,3cv5z6,moschles,1 point,Sat Jul 11 15:21:28 2015 UTC,"The centers of galaxies are actually wormholes created by Type III civilizations who have passed the singularity.     This may explain why gravity alone cannot account for anomalous rotation curves of galaxies.  Michio Kaku has remarked that we might not understand their technology, and therefore couldn't recognize it. For example, humans do not communicate using gravity waves.  We don't have the technology to even detect them.  Post-singularity civilizations would easily use gravity waves to communicate."
singularity,3cv5z6,hipst,1 point,Sat Jul 11 18:53:07 2015 UTC,"So, who knows how to crack reality's hypervisor and directly address the ""creator""?  Think Daffy Duck and the cartoonist."
singularity,3cv5z6,RhomboidStorm,1 point,Sat Jul 11 20:18:30 2015 UTC,My first guess would be that it is out there but we have not advanced enough to hear them.
singularity,3cv5z6,WalterMeon,1 point,Sat Jul 11 21:31:28 2015 UTC,Haven't you seen the movie: Under the skin?
singularity,3cv5z6,Unholy_VI,1 point,Sun Jul 12 13:33:26 2015 UTC,Right.  If you are a piece of mold the universe as you know it is a piece of bread.  You don't know how it was made or who made it.  To you it always is and always was.  I'd be shocked if we could even recognize a singularity type intelligence if we were sitting on it.
singularity,3cv5z6,giulioprisco,1 point,Mon Jul 13 01:36:31 2015 UTC,Who said it isn't?
singularity,3cv5z6,mistermarko1,1 point,Mon Jul 13 06:28:01 2015 UTC,I actually answered this exact question recently here: https://www.reddit.com/r/singularity/comments/3b5v1r/molecular_nanotechnology_development_strategy/
singularity,3cv5z6,HorseSizedTurkey,1 point,Mon Jul 13 08:33:47 2015 UTC,I think it's possible that they're observing/studying us without us knowing it.
singularity,3cv5z6,troll_khan,1 point,Wed Jul 15 23:35:28 2015 UTC,UFOs.
singularity,3cv5z6,dclctcd,1 point,Thu Jul 16 01:48:34 2015 UTC,"I'd like to add two additional theories.  First theory. Our reality might be the product of a computer simulation created by more advanced biological beings than us or possibly even by an artificial intelligence entertaining itself. The creator of this simulation may have decided to focus on life on Earth or their computer might not be powerful enough to simulate an entire universe teaming with intelligence.  Second theory. All consciousness in the Universe is one. Being intellectually advanced enough could make us realize that all consciousness stems from the same source. Conscious beings would stop acting as if they were separate from one another. All actions and thoughts would become pointless. We would transcend our biological needs and stop reproducing. This realized consciousness would bask in bliss for the rest of eternity. Welcome to Nirvana.  While the second theory is wild speculation, the first one is — in my opinion — a very reasonable possibility."
singularity,3cv5z6,Anenome5,-7,Sat Jul 11 07:36:59 2015 UTC,"There's no one out there, dude. People are way too optimistic about aliens."
singularity,3cv5z6,dclctcd,4,Sat Jul 11 04:45:08 2015 UTC,"Believing that life emerged only once suggests a profound ignorance about the size, age and uniformity of the Universe or total religious devotion."
singularity,3ct28i,speckz,5,Fri Jul 10 15:25:44 2015 UTC,"I'm interested in what he has to say in the coming weeks.   I think he's absolutely right. The working class is on the verge of evolving into new systems and dynamics that will be shaped by augmented reality and the internet of things. It's very interesting to ponder the possibilities,  but more-so to capitalise on that potential, and to find the best ways to be one of the active contributers to this global shift that we're all about to witness."
singularity,3csjdu,Dan_Keane,6,Fri Jul 10 12:41:51 2015 UTC,Reminds me of that video of the android girl being assembled and malfunctioned by actually being self aware
singularity,3csjdu,Dezzy-Bucket,3,Fri Jul 10 18:21:03 2015 UTC,"I've seen that video.  If you like that, you should check out Ex Machina.  It's really good."
singularity,3csjdu,MJoubes,4,Fri Jul 10 18:48:44 2015 UTC,"Also, Ghost in the Shell."
singularity,3csjdu,Dezzy-Bucket,2,Sat Jul 11 02:14:09 2015 UTC,Thanks for linking! And yes I saw it! Mind trip on the best level. Loved it.
singularity,3csjdu,timschwartz,1 point,Fri Jul 10 18:56:07 2015 UTC,Sounds like Winston in 1984.
singularity,3csjdu,KhanneaSuntzu,-8,Fri Jul 10 20:49:45 2015 UTC,"The word (and concept) sentience is not as threatening as the respective words (and concepts) ambitious. Sentience does not automatically apply a desire on part of the machines to go out and have an impact on the world, which generally ""having an ambition"", does. Of course these are only words, and words are ambiguous sacks of meaning you can largely meaninglessly talk about. We need functional (reality embedded) definitions of sentience, self-awareness, rationality, conscience, intelligence, sapience, imagination, ambition, desire, emotion, et.al (and a few hundred such vague words besides) and reduce each down to absolutely rock bottom unambiguous algorithmic concepts which have relevance in AI programming syntaxis.   So if anyone starts using above human communication labels I for one must conclude they don't have much of an idea of what they are talking about. The concept ""sentience"" is not really a useful label for this subject material."
singularity,3csjdu,NotThoseThings,5,Fri Jul 10 13:32:36 2015 UTC,"Alright, so now how about you take this writing prompt and use it to write a story about an AI ruminating to itself about the point you're trying to get across.  Could be interesting.  Plus if you have an AI character saying it the inherent condescension might give the story a slightly ominous overtone that might hook a reader."
singularity,3csjdu,KhanneaSuntzu,1 point,Fri Jul 10 17:16:37 2015 UTC,"I don't think anyone was trying for accurate, Mr. Buzzkill."
singularity,3csjdu,KhanneaSuntzu,-2,Fri Jul 10 15:42:48 2015 UTC,Yah but people don't have a clue what they are talking about and just randomly say some words. Incidentally Mrs. Buzzkill for you.
singularity,3cmcga,Buck-Nasty,35,Thu Jul 9 00:54:53 2015 UTC,Call me when Google's AI files patents.
singularity,3cmcga,Ommin,2,Thu Jul 9 04:25:20 2015 UTC,Now that would actually be interesting.  Would they be allowed to? Where were they born?  What country are they a citizen of?
singularity,3cmcga,NPVT,3,Thu Jul 9 12:33:10 2015 UTC,I might be mistaken but can't corporations also file for patents? In any case I know that you can file for a patent in another country than the one you're living in so nationality doesn't really matter.
singularity,3cmcga,TangentialDust,1 point,Thu Jul 9 13:17:02 2015 UTC,"Reading on one web page it states that ""only natural persons may apply for a patent"" - at least in the USA. Other sites are more vague.    http://www.iusmentis.com/patents/crashcourse/applicant/"
singularity,3cmcga,NPVT,1 point,Thu Jul 9 14:03:08 2015 UTC,"If these types of questions come up, we have done something very, very, bad in regards to AI."
singularity,3cmcga,tbear80,2,Thu Jul 9 21:24:56 2015 UTC,Call me when ai decides who wins the lawsuit.
singularity,3cmcga,agpennypacker,2,Thu Jul 9 15:50:37 2015 UTC,Call me if a Google AI wins that lawsuit and use the money to create better AI
singularity,3cmcga,BrayoP,17,Thu Jul 9 18:40:25 2015 UTC,"This is absurd. Several of these describe systems that have become incredibly common with machine learning. Classification? Parallel networks? There's not a soul at the patent office aware enough to even question this stuff, how anyone can justify these patents getting rammed through is beyond me."
singularity,3cmcga,fhayde,10,Thu Jul 9 19:24:34 2015 UTC,"There was an article on Ars Technica a year-ish ago that discussed this sort of issue.  The idea was that you can patent a common practice if you include some feature in the patent that wouldn't normally be documented.  IIRC the article was reviewing a patent from Amazon for some photography setup.  There was plenty of prior art for that sort of thing, but Amazon listed the camera distance as one of the patent features and none of the documentation from the prior art included that detail."
singularity,3cmcga,JWarder,18,Thu Jul 9 03:38:47 2015 UTC,I really wish we'd get rid of software patents altogether.
singularity,3cmcga,KingPickle,7,Thu Jul 9 04:30:32 2015 UTC,Not to mention biological patents.
singularity,3cmcga,comrade_leviathan,2,Thu Jul 9 06:12:58 2015 UTC,"That shows how absurd it is to grant patents on software.   The US seems to be trying to become a Third World nation. All they will accomplish with this bullshit is to send researchers to work in Europe, Japan, and China."
singularity,3cmcga,MasterFubar,7,Thu Jul 9 13:18:21 2015 UTC,"Related, for those that have not seen it yet."
singularity,3cmcga,Erudite_Scholar1,6,Thu Jul 9 21:06:57 2015 UTC,Whole patent system is detrimental and fraudulent. Society will be much better of without it. Luckily many smart people are working on that.
singularity,3cmcga,Dark-Union,2,Thu Jul 9 03:22:22 2015 UTC,"My antivirus just blocked that website when i clicked the Artificial Intelligence section, BEWARE."
singularity,3cmcga,BrayoP,2,Thu Jul 9 15:38:32 2015 UTC,"Fortunately, Google doesn't really do patent trolling.  Usually it tries to get patents like this for defensive reasons, so it can develop it's own technology without getting sued by everyone else (or, at least, so when it is sued by other big companies, that it has the ability to counter-sue them using other patents, like happened in the whole big fight with Apple.)     We clearly do need to reform the patent system, though, so that big tech companies like this aren't spending so much time and money just fighting patent wars instead of developing technology"
singularity,3cmcga,Yosarian2,-6,Thu Jul 9 19:18:54 2015 UTC,Oh yes. Ohhh yesss. The fact that the patents are geared towards neural networks means that an artificial human is in the works in Google.
singularity,3cmcga,nabeelv44,5,Thu Jul 9 19:32:53 2015 UTC,"Sorry to disappoint, you're far off"
singularity,3cmcga,TenshiS,2,Thu Jul 9 03:30:07 2015 UTC,Not at all true. Companies typically file patents for ideas that have barely entered POC design phase.
singularity,3cmcga,comrade_leviathan,1 point,Thu Jul 9 16:44:16 2015 UTC,"Don't kill my hopes. Pls, be fair to me."
singularity,3cmcga,nabeelv44,4,Thu Jul 9 13:20:06 2015 UTC,Or already a thing and are patenting what they can of it to cover their bases.
singularity,3cglyu,Yuli-Ban,20,Tue Jul 7 19:23:38 2015 UTC,This is going to make the upheaval which took place prior to the great depression look like a cakewalk IMO.
singularity,3cglyu,Involution88,22,Tue Jul 7 22:09:11 2015 UTC,"Good.  The concern is whatever you receive from the job, not the job itself.   The problem, however, is that without jobs, they will not have the dignity, social engagement, and sense of fulfillment that comes from work.   Where does robots taking jobs make people unable to work?  They are free to do as they please, work or no."
singularity,3cglyu,Priscilla3,21,Tue Jul 7 19:32:53 2015 UTC,Retired people seem to figure it out?
singularity,3cglyu,Frater_Petrichor,3,Tue Jul 7 20:38:15 2015 UTC,"You could look at it that way, but suicide rates of the elderly/retired people are higher than national averages as well."
singularity,3cglyu,naossoan,11,Wed Jul 8 00:42:15 2015 UTC,"This is true, and plenty of people dread retirement because of the fulfillment and everything. And plenty of people dont have a viable retirement and have health issues beyond their means or that are untreatable.  I just think its really unhealthy for people to derive psychological health from employment.  Thats disturbing."
singularity,3cglyu,Frater_Petrichor,2,Wed Jul 8 00:54:19 2015 UTC,"Really? You must hate your job, or produce nothing useful if you are disturbed by people deriving pleasure from work.    I've worked many types of jobs in many areas, and never had a job that I didn't take satisfaction from good performance and derive self worth from reflecting on the work product I created.    Nothing save my family gives me greater pleasure than finding ways to do what I do better.  Why do you find that disturbing?"
singularity,3cglyu,gratefulturkey,17,Wed Jul 8 02:05:20 2015 UTC,"Sorry, let me phrase that better.  I find it disturbing that there is a cultural imperative to view 'working' as an essential part of mental health.  You do you. But it is silly to think that everyone is better off working even if it means nerfing things that could be done more efficiently through automation. Personally, despite large periods of unemployment and other obstacles, manage to derive self worth from all sorts of things, and except for the fact that I have to figure out how to survive,  have more than enough ideas to occupy my mind and time without having to bind my soul to some weird task to make someone else money.  But my list of jobs isnt exactly full of high achieving environments:  Zamboni Driver Mexican Restaurant Prep  Infantryman Gardener for Hire  All of the jobs I have ever done are either easily automated, are wasteful and only 'productive' in an insane frame of reference,  or just shouldnt be done. I like doing many of these things, but theres no reason to be needing a job fix like a crack addict when I have plenty of other romances in life that I could involve myself with. Give a way to sustain myself of course."
singularity,3cglyu,Frater_Petrichor,1 point,Wed Jul 8 02:42:55 2015 UTC,"Thanks for not responding in kind to my rude retort, and instead expanding your comment.  So, for your reference; Farm laborer, electrician's apprentice, concrete construction worker, truck driver, short order cook, athletics coordinator, health care worker.  I do have to continue to disagree with you though.  From what I can see, you've made ice faster and smoother for the skaters, made delicious (though probably overly caloric) meals for people, kept your friends and family safe by serving your country and created beauty in peoples lives.    Sure, some of this can be automated. Most easily the Zamboni I'd guess, but these are all things you can take pride in!  You've learned from them.  You've participated, and you've created value."
singularity,3cglyu,gratefulturkey,8,Wed Jul 8 04:49:46 2015 UTC,"It's something I'm working on. The infantry position wasnt so good for my temper.  I agree that I did all those things. My problem is with the idea that compulsory employment is somehow hardcoded into the nature of reality. Things are... okay I guess, for now, lots of people have work and systems need them to be sustained. But not only do a huge amount of the world's persons not have adequately gainful employment,  the options they do have are the first to disappear. and I think that the idea that other jobs that are necessary will be created is gambling too heavy on the past and is misguided.  There is nothing that will stop people who become michelin star chefs from doing so. However, someone who is only working in the restaruant as a cover for their dope growing op or because they got their girlfriend pregnant is definitely going to stop a chef from killing it. The greatest obstacle after myself at every job ive ever had was people who didnt belong there. Those people almost got me killed on multiple occasions.  I think that we can solve a lot more problems by adopting societal values that do not hinge upon the sort of puritan 'sweat of your brow' approach to survival. Of course this is stuff for this subreddit, we are at the opening of a grand change but not ready to do this.  After all you list your family as more exquisite of an experience than work. How would you feel if automation allowed you to work for their wants instead of needs and spend more time with them? I don't think that will be the case for some time, but attitudes need to start shifting  or itll be an even weirder transition.  Honestly I am just confused that you're posting in /r/singularity about dragging the trappings of current human life past the event horizon. I mean... this sub is about weird scifi bullshit ideas."
singularity,3cglyu,Frater_Petrichor,1 point,Wed Jul 8 05:11:11 2015 UTC,"Great comments.  I've been trying to digest them which is why this reply is tardy.    To answer your last question first, I posted my original statement for a lot of reasons.  I'm fascinated by the merging of machine and man, thrilled by the prospect of radical abundance, and intrigued by the ability to leverage the technology of automation and machine learning to exponentially increase productivity across all areas of the economy.  That is why I'm in this sub to begin with.    With that said, I believe that those who predict baby boomers could achieve virtual immortality are wildly optimistic.  I worry a great deal about the transition period after automation destroys multiple segments of the economy.  The social unrest that is likely as capital investments replace human labor on a large scale gives me pause.     I see a lot of challenges, and think it is likely that the transition period will be not only longer, but more rocky than expected.  Creative destruction is great but not for those who are aged out of the ability to re-train for new positions and even though they want to work, find themselves unhireable and past the break-even for retraining.  There are a lot of people in the world (including myself) who believe productive labor has a great deal of value.  Tangible value in the goods produced, societal value, and personal emotional value.  I went through times in my life when I was producing nothing of much value and I was never all that happy.  Granting that this is a N=1 analysis, my guess is that many people feel this way.  I wonder if I'm dragging old societal norms into the post singularity world, or if we are actually neurologically hard wired to feel this way based on evolutionary demands.  The last thing I'd say is that massive groups of idle people can cause a great deal of trouble.  What do you think the likely outcome is when huge groups of non-employed people live in highly populated areas.   I for one see a great many opportunities for trouble, and believe that productive labor solves a great many of these problems."
singularity,3cglyu,gratefulturkey,1 point,Thu Jul 9 03:11:50 2015 UTC,"Kudos to you both for having a well-written, civil conversation on reddit. <3"
singularity,3cglyu,ghost_of_drusepth,1 point,Tue Jul 14 02:22:42 2015 UTC,How's it being an electrician's apprentice? Ive been thinking about a career change.
singularity,3cglyu,Stacksup,2,Wed Jul 8 19:00:31 2015 UTC,"There are a lot of variables.  I worked on the residential side of things, so I spend a LOT of time in dusty itchy hot attics.  It is more physical work than many jobs (more physical than one might expect), and you need to be able to learn quickly. If you can handle the physical and mental demands, though, it can make a very solid career choice."
singularity,3cglyu,gratefulturkey,1 point,Wed Jul 8 20:06:44 2015 UTC,Thanks. I appreciate the input.
singularity,3cglyu,Stacksup,1 point,Wed Jul 8 22:18:37 2015 UTC,"There's a difference between working a nd a job. Building a kid a treehouse is working, doing repetitive stuff for hours for a paycheck is a job. I have no problem with people getting self-esteem from working, but it also disturbs me that people will kill themselves if they don't have a job."
singularity,3cglyu,Muggzy999,1 point,Thu Jul 9 20:39:46 2015 UTC,"I agree, which is why I tried to use the phrase ""productive labor"" a few times.   I also find the ramifications of low employment disturbing, both for the individuals and for society at large.    My question is really whether this drive to tie life satisfaction to productive labor is related to cultural pressures, or is something more innate.  Further, how will all the large groups of nearly idle people interact?"
singularity,3cglyu,gratefulturkey,1 point,Thu Jul 9 20:57:58 2015 UTC,"Really? You must hate your job   Well, if he does, he isn't exactly alone, most people don't like their jobs."
singularity,3cglyu,grumbel,1 point,Sat Jul 11 12:02:41 2015 UTC,"Liking your job and deriving a sense of purpose from it are two very different things, however, as are hating your job and not liking it."
singularity,3cglyu,gratefulturkey,1 point,Sat Jul 11 23:12:39 2015 UTC,"I think it really depends on what you consider work to be, or what your ""job"" is.  I think it's pretty safe to say that without purpose, meaning, goals, stuff like that, that humans have little drive. I may just speak for myself, but with my current job I would almost rather be doing basically anything else, but I do it because I need to to get where I want to go.  If I didn't have that goal, that want, then I would have no drive for the job I currently have other than to provide for myself which honestly isn't good enough reason for me to keep a job.  People need hopes and dreams, and for some, it's their jobs that lead them there."
singularity,3cglyu,naossoan,1 point,Wed Jul 8 04:52:07 2015 UTC,"And for some it is their spirituality. My problem is with the social convention that employment is compulsory. Its kind of a necessary convention so far, but it kind of marginalizes lots of people. But thay doesn't matter because theres this looming cliff of superefficiency that can either lead to lots of free time or lots of bullshit time wasting like crossword puzzles in high school math class. I'd rather try to make a world where people spend time with their loved ones and pursue their interests and dont get left out as surplus or forced to do something totally unnecessary so that everybody can feel like their earned their food shelter and water for the month."
singularity,3cglyu,Frater_Petrichor,1 point,Wed Jul 8 05:17:08 2015 UTC,Because they've been trained all their lives to derive their self-worth and motivation from a job.
singularity,3cglyu,rushmc1,1 point,Wed Jul 8 22:48:29 2015 UTC,"Not all of them. A person that has a purpose in file lives longer and is happier. The things is, not everyone is able to decide for himself what he should do and is lost without external directions."
singularity,3cglyu,krneki12,1 point,Thu Jul 9 12:42:21 2015 UTC,"I hope we have the social understanding to accommodate the people who suffer because of this. Historically new tech created jobs (even with luddites screaming otherwise), AI could be the first tech that really does remove jobs for good.  Some people will use any excuse they can to step on someone else."
singularity,3cglyu,Sqeaky,3,Wed Jul 8 17:35:05 2015 UTC,"Historically new tech created jobs    Not true, mechanical looms revolutionised and made redundant thousands of skilled workers at the time, but if you look back now would you argue that replacing field hands with a single tractor was a mistake? or that people should go back to using manual looms just for the sake of them having jobs?  Every one seems to think that using technology to do a job better is a bad thing but historically it's been a good thing and it frees people up to do other things like go to school for example. Yes it sucks that people will lose their jobs  in the short term but once the technology is there to replace those people their jobs are redundant in every sense of the word.   If you want to go back to using a manual loom and hand working fields, by all means go and live in an Amish community, it's a valid way to live but trying to keep jobs just for the sake of people having jobs is short sighted and if you take it to the logical conclusion utterly ridiculous. Unemployment up in your area? No problem we just created 100 new jobs as farm hands by mothballing the tractor!"
singularity,3cglyu,droznig,1 point,Wed Jul 8 21:42:26 2015 UTC,"mechanical looms revolutionised and made redundant thousands of skilled workers at the time   It created many more jobs acquiring materials to feed into the looms, to maintain looms, design better looms, ship the products of automated looms. It is not like all those skilled jobs disappeared over night, many had time to move to service (custom tailoring vs bulk sewing), and some still exist today. It did take time to do this, but the greater amount of jobs after mechanized could never have been supported with hand crafted goods.  I totally agree with your second and third paragraphs. So I think I will just expound a bit:  The whole idea of specialization allows things like the modern globalization we take for granted. Without hyper specialized systems for creating ultra cheap goods we wouldn't have (at least as quickly) extremely cheap transport."
singularity,3cglyu,Sqeaky,8,Wed Jul 8 22:10:25 2015 UTC,"It may get worse (AI->human unemployment) before it gets better (ASI->Human endowment). During the worsening, basic income can be useful as a balancing tool."
singularity,3cglyu,troll_khan,1 point,Tue Jul 7 21:26:11 2015 UTC,It's in our nature the resolve the issue after it has happened. Proactive is overrated anyway. :)
singularity,3cglyu,krneki12,-5,Thu Jul 9 12:48:40 2015 UTC,"I like this idea, but basic income WILL fail if it is funded by taxes, because essentially tax is theft. Basic income will have to come by dropping the marginal cost of goods to 0."
singularity,3cglyu,SlightlyCyborg,8,Wed Jul 8 02:53:28 2015 UTC,Tax is theft? Wut.
singularity,3cglyu,flesjewater,11,Wed Jul 8 07:43:39 2015 UTC,There are people that literally believe this.
singularity,3cglyu,Jah_Ith_Ber,2,Wed Jul 8 08:31:04 2015 UTC,"Also, actors are liars, meat processors are animal killers, and you have to break eggs to make omelets."
singularity,3cglyu,mr_one_liner,-7,Wed Jul 8 16:02:16 2015 UTC,"Yes. Theft is the taking of another person's property without that person's permission or consent with the intent to deprive the rightful owner of it. Taxes take money from citizens. If a citizen does not want to give up their money for taxes (lets say because they don't like frivolous spending), then the government will garnish your wages or come to arrest you. If you try to protect your property you will end up dead.   If your cognitive dissonance is not too overwhelming, here is a follow up discussion"
singularity,3cglyu,SlightlyCyborg,8,Wed Jul 8 10:42:10 2015 UTC,Nonsense. If you don't want to pay taxes then don't expect to live in a society were people take care of each other; Don't expect free healthcare (if your lucky enough to live in a country that provides it) if you break a leg or develop cancer. Don't expect protection from the police if someone tries to hurt you. Don't expect the fire service to save you if your house is on fire. Don't expect to be given an education. Don't expect to drive to work on safe roads...I could go on.  Tax isn't theft. Tax is necessary to pay for everything that makes your life safe and comfortable.  Imagine what the world would be like if everyman was for himself. Chaos.
singularity,3cglyu,mono-math,-5,Wed Jul 8 13:03:10 2015 UTC,"If you don't want to pay taxes then don't expect to live in a society were people take care of each other;   So charity and Adam Smith no longer exist? Apple takes care of me by providing me a computer, Google takes care of me by providing me information. When my bank teller says ""Hello, what can I do for you today"" I guess she is not taking care of me, because I am not paying her via money taken by force by the government.   Don't expect free healthcare   Lol. Do you really think the healthcare these socialist countries are providing is free? It gets paid for via debt and inflation. I don't expect free healthcare, unless a company finds a way to drop health care's marginal cost to 0$ which will be the last thing to become truly free (we cant even do it yet for food and water)   Don't expect protection from the police if someone tries to hurt you.   Lol. Here are private police.   Don't expect the fire service to save you if your house is on fire.   Lol. Here is a profitable firefighting company   Don't expect to be given an education.   Lol. Here is google.   Don't expect to drive to work on safe roads   Lol. Here is a private road."
singularity,3cglyu,SlightlyCyborg,2,Wed Jul 8 13:31:56 2015 UTC,"You're obviously a lost cause; I bet you believe we live in a meritocracy. A meritocracy only works if we're dealt the same hand; if we're given the same opportunities as everyone else.   So charity and Adam Smith no longer exist?   Relying on charity to provide basic services for millions of people is realistic? Are you advocating people give up some of their money to help provide basic services for the people that need them. You realise that's what taxation gives us?   Do you really think the healthcare these socialist countries are providing is free?   Well no, that's what taxes pay for.   It gets paid for via debt and inflation.   It gets paid for via debt if the healthcare budget, provided via taxation, is insufficient and borrowing is needed to help cover the deficit. You make it sound like debt only exists because of taxation? Our entire economy is built on debt, so I'm not sure what your point is.   lol. here....   Really?  Imagine a situation where you've been physically attacked but you can't call the police police because you don't have security insurance and you don't have enough money to pay the call out fee. Imagine you don't have the money to pay the costs for pressing charges against your attacker. I suppose you'll suggest the attacker would be liable for the costs? But wait, the attacker doesn't have insurance or any money either!  Imagine your house is on fire and you have to pay tens of thousands of pounds for the fire brigade to put the fire out - or worse; rescue a family member. Imagine you don't have the money to pay for any of this!  Google will replace schools? So let's throw all our 3-18 year olds in front of a computers all day, let them teach themselves? Children need guidance. So lets get in some private teachers? Don't have any money? Born into a poor family? No education for you.  You do understand that people have to pay to use private roads? Are you suggesting every single fucking road should be private? That you should pay every time you join a new road? In other words, if you're poor, you can't even leave the house and drive to work.  The millions of people that live in poverty would be utterly screwed in your world. Things wouldn't get better for them, they'd get worse, and after a while there would surely be a violent uprising."
singularity,3cglyu,mono-math,1 point,Wed Jul 8 16:12:46 2015 UTC,Poor people would not be screwed: https://youtu.be/vqlVL26jrCA
singularity,3cglyu,SlightlyCyborg,1 point,Sun Jul 12 02:05:29 2015 UTC,"It could also come about by voluntary donation I believe.   It won't, but it could."
singularity,3cglyu,Priscilla3,3,Wed Jul 8 08:31:22 2015 UTC,"For sure there are some frightening possibilities in the topic but actually it is the AI and the technology that could save the planet and the human race from itself. There are a plenty of problems awaiting for us in the near future, like overpopulation, the effects of climate change or the decreasing of habitable surfaces, water and food crisis etc... I believe that the singularity will not subtract the opportunities of humanity but rather increase them as it will going to figure out the solutions to all of the crisis above and everything in the topic of sustainability. And so we can take our time to learn, to make art and to turn for the things that are mainly about moving ahead the cultural evolution because for a safe singularity the culture should outpace itself to develop on a similar way as technology does. It is also means that we will improve the education system and rethink politics:  BTW we could make jobs for humans to repair these coming unfortunate events I wrote about. We even have the technology to do it as well, all we need is to organise ourself, the ones who dare to improve on this topic into a movement which can be maybe crowdfunded."
singularity,3cglyu,WalterMeon,1 point,Wed Jul 8 10:37:46 2015 UTC,"like overpopulation   How exactly are we ""over populated"" ?"
singularity,3cglyu,droznig,1 point,Wed Jul 8 13:45:02 2015 UTC,"It's just the matter of time. Maybe not tomorrow but in the long run the population will outgrow the capacity of the Earth to sustain it in the temporary way of farming and water providing and that will be the so called overpopulation. The phenomenon will even more increase as humanity going to unlock the possibility of immortality.  But there is nothing to worry about. I have an idea. We should build on the Oceans. What we need is automatic 3D printers which are creating floating houses on the oceans. They would build a mini island then another. It would be free because the floating accommodations would be made from the materials of the ocean floor and the water for example. The islands would chaining into cities. Anyone could have a house if willing to accept the conditions, statements and the philosophy of these living spaces which would be progressive as it is expected. The core idea would be the empathetic attitude towards each other and the right for all to live dignified. It would also encourage curiosity and the main goal was self-realisation."
singularity,3cglyu,WalterMeon,3,Tue Jul 7 22:18:52 2015 UTC,"I've worried about this issue for years. As a programmer I have noticed an ever-increasing advancement in automation that I think is about to hit every industry in the same manner that the Internet and smartphones hit the photography and music industries.  I have changed my opinion regarding the long-term economic effects of automation though. I think we may actually shift from a service economy (in the US) to a managerial economy in the same way many people who work in the food markets have transitioned from the register to the person who oversees the multiple self-checkout machines.   Once automation (assuming robots) are advanced enough to take over the service industry they are probably then sophisticated enough to take on other industries such as construction. If so, then wouldn't this open up many previously unaffordable large-scale projects ranging from a nation-wide infrastructure overhauls to advanced national/international transportation networks. Human labor is often the largest expense of any large-scale construction project, if automation can eliminate the majority of labor cost wouldn't there be many more initiatives to put in place the sort of infrastructure we always dreamt of? Perhaps dust off those old science-fiction-looking blueprints of the past and actually make them a reality because we can finally afford them.  Perhaps the old illustrations depicting the ""cities of the future"" were only ahead of themselves by a few decades after all."
singularity,3cglyu,runewell,2,Wed Jul 8 21:44:46 2015 UTC,"As long as they are able to untangle stupidly large masses of tangled wires, I don't care and will love them forever."
singularity,3cglyu,Seesawkarma,2,Thu Jul 9 10:28:43 2015 UTC,"they create plenty of jobs too.  Need somebody to engineer and maintain those robots, write the code, manufacture the parts, etc."
singularity,3cglyu,tricklaney,4,Wed Jul 8 00:09:06 2015 UTC,"I think the argument is that these robots are going to be general-purpose, meaning that they will be programmable for any job versus previous technology which had singular or narrow purpose. In theory, a robot that can replace you in your white collar job can also replace you in the job you'll transition to. Eventually this would narrow the type of work down to jobs that employers ""want"" a human to have and not necessarily ""need"" a human to have. Long way out, I know, but I think that is the long-term view on it."
singularity,3cglyu,runewell,9,Tue Jul 7 23:48:16 2015 UTC,"There was an article in the business section of the USA Today a few months ago explaining that robots will replace like 7 million jobs over the next 10-15 years, but it will create like 1 million new jobs. That's not a gain, that's a loss."
singularity,3cglyu,Muggzy999,2,Tue Jul 7 21:09:43 2015 UTC,"You can't predict something like how many jobs will be created. I can't imagine what sort of jobs we will have after creating self-programming AI, but they'll be there.  Or maybe everything will be so cheap that we don't have to work anymore? Think about it: machines are gonna do everything for us. They'll bring us food, they'll build is houses, they'll cut down the trees to build those houses. Still though, we'll have jobs because we as humans always want to improve. At that point, we'll have people working on FTL drives and wormhole creation."
singularity,3cglyu,7yphoid,1 point,Tue Jul 7 23:44:59 2015 UTC,"The only problem I see with this at the moment is that even if you produce all the products necessary for everyone to have a great life, you still need make sure that everyone has access to those products."
singularity,3cglyu,NNOTM,1 point,Tue Jul 7 21:20:43 2015 UTC,How is a loss if there is less work to do and you gain more from it?
singularity,3cglyu,krneki12,4,Tue Jul 7 22:30:39 2015 UTC,"At first.  Then AI will engineer and maintain those robots, write the code (whatever won't be learned in the real world that is), manufacture the parts, etc."
singularity,3cglyu,7yphoid,1 point,Tue Jul 7 23:18:07 2015 UTC,Then we will make other jobs. Think about this: how many jobs did computer and the internet replace? And how many did it create? It's now the fastest growing job industry.
singularity,3cglyu,7yphoid,9,Thu Jul 9 12:49:58 2015 UTC,"You're not getting it.  Whatever ""other"" jobs we make, AI will do as well. AI is not the Internet, loom, tractor, computer, whatever."
singularity,3cglyu,7yphoid,1 point,Tue Jul 7 21:35:29 2015 UTC,"I see your point now.  Still though, at that point, nothing will be the same. The economy as we know it may not even exist anymore, because people won't need jobs anymore. AI does everything. Now people can just ""live"", whatever entails. My point is, it's impossible to predict what will happen past the singularity because the changes will be too profound. Heck, the whole concept of ""jobs"" may not even exist anymore."
singularity,3cglyu,kat_burglar,0,Tue Jul 7 22:26:21 2015 UTC,"I see your point now.  Still though, at that point, nothing will be the same. The economy as we know it may not even exist anymore, because people won't need jobs anymore. AI does everything. Now people can just ""live"", whatever entails. My point is, it's impossible to predict what will happen past the singularity because the changes will be too profound. Heck, the whole concept of ""jobs"" may not even exist anymore."
singularity,3cglyu,Large_Mountain_Jew,1 point,Tue Jul 7 22:47:38 2015 UTC,"AI will do those jobs too. Whatever jobs are created can be instantly learned by AI where it would take months or years to train people, but  AI does it better faster and cheaper anyway."
singularity,3cglyu,Yasea,1 point,Tue Jul 7 23:10:37 2015 UTC,This is why I am glad I am going into the mental health industry. Its going to take a looooong time before the machines understand human emotions.
singularity,3cglyu,Large_Mountain_Jew,1 point,Tue Jul 7 23:04:12 2015 UTC,Computers Can Read Emotions Better Than You Can
singularity,3cglyu,FoxTwo-,1 point,Wed Jul 8 02:04:58 2015 UTC,"The what of emotions is relatively easy, that is to say what someone is feeling. The why is much harder. That's something much harder, something humans are still struggling to get down."
singularity,3cglyu,rushmc1,1 point,Wed Jul 8 04:53:06 2015 UTC,"So I have a question, how does one future proof himself or herself so that he/she can be sure to have a job in the future? I'm a business student and will be going into my first job as a risk consultant next week and this has me very worried about the future."
singularity,3cglyu,rushmc1,2,Wed Jul 8 15:28:55 2015 UTC,"First of all, learn to look beyond desiring a ""job"" like it's some sort of existential good, or necessity..."
singularity,3cglyu,jimmyolsenblues,1 point,Wed Jul 8 20:03:26 2015 UTC,"Depends on how far in the future you carry your concerns.  Next 10-20 years? Fuggedabbawtit. During that time, however, be sure to buy and invest in robots. After 20 or so years (may be sooner, may be later, but that's my prediction), AI will have dominated just about all markets, even yours. But since you have robot slaves earning you income, you should be able to retire as a multi-millionaire."
singularity,3cglyu,registereduser2,1 point,Wed Jul 8 14:22:35 2015 UTC,Yay!
singularity,3cglyu,Jay27,1 point,Wed Jul 8 22:51:26 2015 UTC,"We are a long way away from robots repairing networks, repairing plumbing, being teachers, fireman, policeman.    Will robots replace McDonald workers, and car assembly, yes, but who is going to fix those robots.  Who is going to program those robots?  I think we are a long way off from the true singularity."
singularity,3cglyu,registereduser2,1 point,Wed Jul 8 14:50:09 2015 UTC,"Will robots replace McDonald workers, and car assembly, yes, but who is going to fix those robots. Who is going to program those robots?   Near-term: humans  Long-term: robots"
singularity,3cglyu,mistermarko1,-5,Wed Jul 8 19:17:55 2015 UTC,"If we can develop the economic structures necessary to distribute the prosperity we are creating, most people will no longer have to work to sustain themselves. They will be free to pursue other creative endeavors.   Not gonna happen. Ever.  Nevermind the bullshit in the rest of that paragraph."
singularity,3cglyu,Jay27,2,Thu Jul 9 18:52:21 2015 UTC,I don't see you making an attempt at rational argument.
singularity,3cglyu,xeroblaze0,0,Thu Jul 9 18:59:46 2015 UTC,I don't need to argue with anyone in order to make a correct statement.
singularity,3cglyu,xeroblaze0,2,Wed Jul 8 16:16:32 2015 UTC,"Many people don't work to sustain themselves - children, students, pensioners, invalids, the unemployed. And many people don't work much to sustain themselves - part-timers, small farmers, wwoofers, landlords. You're half wrong already."
singularity,3cgk79,Portis403,3,Tue Jul 7 19:11:25 2015 UTC,"“The whole ‘Singularity’ kind of event? Yeah, it’s science fiction, and not very good Sci-Fi at that, in my opinion. Unending exponential growth? What drugs are those people on? I mean, really.”  The question really is when and where it stops. Things can get pretty crazy in just a few decades. Not even Kurzweil predicts it will be unending but if it last a few decades longer (by no means guaranteed) we are in for potentially a major shock. Torvalds is extremely arrogant but I agree with most of what he said here."
singularity,3cgk79,AManBeatenByJacks,1 point,Wed Jul 8 02:21:48 2015 UTC,I honestly can't really see the singularity panning out much at all from what I know about it.
singularity,3cgk79,5ives,2,Fri Jul 10 14:44:21 2015 UTC,More or less the same attitude that Isaac Asimov had.
singularity,3cgk79,MasterFubar,3,Wed Jul 8 01:40:19 2015 UTC,Such a disappointment to see that various high profile people are unable to grasp these things.
singularity,3cgk79,Sharou,1 point,Wed Jul 8 13:28:04 2015 UTC,Wow you would expect the founder of Linux would know his stuff.. :/
singularity,3cedw7,Gnashtaru,15,Tue Jul 7 06:53:07 2015 UTC,"Marriage will likely still exist, but it won't be thought of as permanent for most people. It'll just be a long-term relationship governed by contract. Peter F. Hamilton explored this concept in his Commonwealth Saga."
singularity,3cedw7,cnot3,4,Tue Jul 7 14:35:35 2015 UTC,Line marriages! Heinlein's 'Moon is a Harsh Mistress' looks into it as well.
singularity,3cedw7,savanik,1 point,Tue Jul 7 16:05:14 2015 UTC,Its not permanent now.  For most ppl it just signifies a longer-term relationship.
singularity,3cedw7,dewbiestep,5,Tue Jul 7 22:58:01 2015 UTC,"I didn’t like this article, i was hoping for something that looked more in-depth into what really might one day become sweeping changes in the way we as humans form relationships. But instead i got robots marrying super intelligent avatars and the rise of the fuckbots. It's not like we all arnt super excited about getting machines we can have sex with that will also tell us how good we are at it too.   There are many cultures in the world, many different types of family. I would hope that as we approach the singularity we might start looking at family design. Something based on data. For example, there was a study done that showed in some south American indingouse people a family with two fathers is better than one but three is worse than two.   So perhaps we can one day dispassionally design family units. Number of people, length of time spent together ect ect. Doing so in a way that might be better for raising more robust children  better for the adults in the relashiop too, man woman, intersex or something in-between."
singularity,3cedw7,pushing1,3,Tue Jul 7 20:16:29 2015 UTC,I wonder what a relationship lasting a 1000 years would be like?
singularity,3cedw7,LazarouMonkeyTerror,10,Tue Jul 7 13:20:33 2015 UTC,You don't want to be alive at the end
singularity,3cedw7,eskjcSFW,7,Tue Jul 7 14:12:51 2015 UTC,A lot of awkward silence.
singularity,3cedw7,harryhartounian,2,Tue Jul 7 15:13:46 2015 UTC,"LOL my gut reaction was to agree, but then I realized I'm betting the nature of reality will be changing so fast it will never cease to offer new topics of conversation.  I dunno.  It's really hard to imagine life after the singularity so who's to say?  Certainly not me."
singularity,3cedw7,self_similar,3,Tue Jul 7 22:56:19 2015 UTC,You might forget that you were ever an individual.
singularity,3cedw7,Miv333,2,Wed Jul 8 02:06:29 2015 UTC,"Marriage doesn't make sense when humans live for 10s of years. But I could see myself being with someone for 1000s of years, but things in general would be different on that scale... our ""ethics"" won't be the same in that time span."
singularity,3cedw7,troll_khan,5,Tue Jul 7 20:20:13 2015 UTC,These people don't get it. Everyone will have his/her own simulation.
singularity,3cedw7,REOreddit,2,Tue Jul 7 19:35:59 2015 UTC,"I'm sure Zoltan is aware of that concept.  He's a pretty predominant figure in the transhumanist community.  I'd venture to guess he simply omitted that idea for simplicity, or it wasn't his intended topic or something.  Look him up, he's a pretty interesting fellow."
singularity,3cedw7,InfiniteRelease,2,Tue Jul 7 22:54:33 2015 UTC,I am friends with Zoltan on Facebook and messaged him about this thread.  Hopefully he will come and respond to some of the points made here.  Edit:  got this reply
singularity,3cedw7,Sloi,1 point,Tue Jul 7 23:18:59 2015 UTC,"I just thought of something.  Lets say a couple gets married and vows ""till death do us part"" in their vows.  They do so expecting a regular lifespan.  Say... 80 years.  Then technology ends ageing and cancer etc...  Maybe you can get a new body.  Whatever the situation may be.    Suddenly you find yourself immortal and married to this person who you expected to die with.  What then?  I wonder how the laws would view this and if they would consider age 80 or 90 as the end of the arrangement despite the wording in the vows.    I imagine more likely people will simply eventually get divorced if they want to move on after that point.  What do you guys think will happen?  I'm betting a lot of questions that arise from this will have to be sorted out as the arise.  Some will become a moot point too I suppose."
singularity,3cedw7,MichaelTenery,1 point,Tue Jul 7 22:59:48 2015 UTC,"How is that scenario different from any other scenario where one or both of the spouses change their minds and don't want to spend the rest of their lives together?  Unless you believe that marriage vows are sacred or you live in a country with no divorce laws (or very strict laws), it makes no difference to get a divorce after 10 years vs. getting a divorce after 120 years: You just changed your mind and don't want to spend the rest of your life with your spouse anymore."
singularity,3cedw7,TheTaoOfBill,1 point,Sun Jul 12 01:37:28 2015 UTC,...maybe we should also begin endorsing the phasing out of marriage from society’s mindset.   Little lines like these are the reason I'll never upload my brain to and/or connect it directly to a computer. It's in our nature to believe what's desirable to our individual selves is inherently desirable for literally every member of our society. I could never trust the makers of the computers not to hack their ideologies into my captive brain.
singularity,3cedw7,Pongpianskul,-9,Mon Jul 13 22:28:12 2015 UTC,"... marriage doesn't even make sense now. If you're a man, marriage is a losing proposition.  Edit: apparently people don't seem to get what I'm saying, so let's try this again.  Yes, everything's peachy while you're married. That's not the problem: the issues arise when you're getting divorced.  You don't need to be married to be in a healthy relationship, but you do need to be married to get completely fucked out of your home, to have your kids taken away, to be taken to the cleaners (financially) among many other problems.  But by all means, toss that coin in the air and hope you're not part of the ""bad"" 50%."
singularity,3cedw7,potent_rodent,4,Tue Jul 7 13:59:45 2015 UTC,This never made sense for me. I am a man. My wife helps in ways I didn't even think of before being married. Men so make out in the deal of marriage.
singularity,3cedw7,l00pee,3,Tue Jul 7 18:52:58 2015 UTC,I'm a man. My fiancee helps me tremendously. She offers support when I'm feeling upset or anxious. She helps keep the house clean. She gardens and helps keep the yard beautiful. And she cooks amazing dinners.  I honestly don't deserve her.
singularity,3cedw7,REOreddit,-6,Tue Jul 7 19:35:49 2015 UTC,I'm sure it isn't all that for women either since 75% cheat.
singularity,3cedw7,l00pee,-1,Tue Jul 7 14:27:22 2015 UTC,marriage is a losing proposal for women and men
singularity,3cedw7,REOreddit,0,Tue Jul 7 16:05:28 2015 UTC,But women get the house and kids.
singularity,3cedw7,l00pee,1 point,Tue Jul 7 21:22:19 2015 UTC,There's also domestic violence. They also get most of it.
singularity,3cedw7,Pongpianskul,1 point,Sun Jul 12 01:37:19 2015 UTC,It happens regardless of domestic violence. Let's not conflate the two.
singularity,3cedw7,Chispy,1 point,Sun Jul 12 02:24:47 2015 UTC,"I'm not conflating them. You are implying women always get the best part when a marriage doesn't work. And that's not true, domestic violence is just an example. When it happens, they are almost always on the losing end."
singularity,3cedw7,NPVT,1 point,Sun Jul 12 13:32:41 2015 UTC,"It is a known fact that whatever the circumstances the woman will generally get the house and kids. This isn't an anecdote, this isn't bitterness, this is the cold hard truth.   If there was domestic violence in even a majority, half, or even a third of these cases you may have a point. I can tell you from personal experience that my ex hit me often, I didn't touch her. She got our child, she got our assets, and I got child support payments. She's in prison now, her known drug habit finally caught up to her. I got custody of my daughter only after her third arrest for heroin."
singularity,3cedw7,mla717,-9,Sun Jul 12 14:10:08 2015 UTC,It already doesn't make sense. Marriage is an unholy union between organized religion and government intended to manipulate and control human beings - and not always in their best interests either.
singularity,3cedw7,NPVT,8,Tue Jul 7 14:26:49 2015 UTC,Not really. We see many examples of monogamy in the animal kingdom.
singularity,3cedw7,mla717,4,Tue Jul 7 15:13:34 2015 UTC,Monogamy can be practiced without the benefit of marriage.  Marriage is an artificial construct to solve certain legal issues that people have.
singularity,3cedw7,Frater_Petrichor,4,Tue Jul 7 16:16:00 2015 UTC,"Why wouldn't you want a legal benefit to an already existing monogamous relationship?   Marriage and its financial rewards make complete sense when you consider that the nuclear family, an evolutionary structure, still governs a vast majority of relationships and yields the greatest amount of children  As a country, you want people making children and you insentivize them to do so"
singularity,3cedw7,MichaelTenery,2,Tue Jul 7 17:46:00 2015 UTC,"I am not disagreeing with what you say - though I don't see any need to encourage people to have more children in an already overpopulated world, and I am not sure that the nuclear family is an evolutionary structure.  I am just saying that geese (for example) - seemingly monogamous animals - don't have any need for the ceremony and legal structure of marriage. People can voluntarily be monogamous outside of the construct of marriage too."
singularity,3cedw7,MichaelTenery,1 point,Tue Jul 7 19:37:29 2015 UTC,"Of course they can, but marriage makes sense in an objective financial way. Yes you can have a monogamous relationship without marriage, but marriage gives your partner right to attorney if something happens to you, a right to your assets and insurance should you pass. It gives you tax breaks because, in general, married couples are more financially stable and are actually more capable of maintaining loans and spending towards the economy. As for overpopulation, that rate has actually been steadily decreasing (the rate of population growth, not total population). As for the whole ceremony thing, yeah a lot of it is bullshit and just serves as a job creator and money maker, but if people want to pump money into luxuries aka the economy then why not let them?   And nuclear family structure is seen in many different animals as an equilibrium between available mates for males and infant mortality rates. Males invest resources while forgoing the pursuit of other mates to ensure that their offspring survive and can pass on their alleles. Human babies are particularly helpless and thus require investment from both parents (evolutionarily) to survive. It's because we spend all that time building giant brains and not much else."
singularity,3cedw7,MichaelTenery,3,Tue Jul 7 19:46:52 2015 UTC,Those legal benefits are only tied to marriage because we make it so. Contract law that effects my life shouldnt be coming up with incentives or deterrents based on my penile activities. I should be able to allocate power of attorney and whatnot regardless of romantic or sexual pretenses.   *clarified
singularity,3cedw7,MichaelTenery,2,Tue Jul 7 20:20:41 2015 UTC,Statistics I pulled from my nether regions for a 1000 Alex.
singularity,3cedw7,Chispy,0,Tue Jul 7 18:37:45 2015 UTC,"People, like animals are different. I am a person who is very monogomous. I don't expect that everyone else is, in fact I know many are not. It isn't just a few mammals and birds that are. Here is a partial list that includes fish, lizards, etc. Lar gibbon, Mute swan, Malagasy giant rat, Waved albatross, California mouse, Black vultures, Shingleback skink(lizard), Sandhill Crane, Prairie voles, Convict cichlid (fish), Kirk's Dik-dik (deer.) I would say that monogomous is rare in the animal kingdom. I contend rarity is special. I celebrate the difference without having to project who I am onto people who are not me. As long as you aren't doing that I am fine with everything but your 0.01% number since you cited precisely zero when you made that assertion. Take care."
singularity,3cedw7,potent_rodent,1 point,Tue Jul 7 18:53:56 2015 UTC,"That's your projection. Just like you deciding that people are simply the same as less sapient animals. You be driven by and succomb to simple instinct if you like but don't assume all of us do. By the way. I often end a conversation with a well meaning, take care or be well. It's just the hippy in me trying to be nice. If you think I am a douche well I can't and won't change that positive aspect because one guy on the internet tells me to."
singularity,3cedw7,MichaelTenery,0,Tue Jul 7 21:37:45 2015 UTC,I don't embarrass easily.Maybe it is because I am not ashamed of myself. More projection?
singularity,3cedw7,mr_one_liner,1 point,Wed Jul 8 01:45:03 2015 UTC,"I'd compare it to relatable organisms, like mammals. Which is what we are."
singularity,3cebuf,Buck-Nasty,1 point,Tue Jul 7 06:27:22 2015 UTC,"Mind-bending. Although is this just a theory? Because I watched this movie called particle fever where they concluded at the end that because of the Higgs Boson's mass, they had to write off the multiverse theory."
singularity,3cebuf,mattstanton94,2,Tue Jul 7 12:01:04 2015 UTC,"I thought that was strange, so I did some research and stumbled on this review, and there's a part on the multiverse bit:   As for the really bad idea, it’s the introduction of the multiverse into the theory part of the film. Kaplan is shown claiming that the multiverse predicts a 140 GeV Higgs, based on this paper of Yasunori Nomura and Lawrence Hall (who was Arkani-Hamed’s advisor). This is at a time when there were experimental hints of a 140 GeV Higgs. After they went away, and the mass came out at 125 GeV, the “prediction” is forgotten, but a long segment still has Arkani-Hamed going on about the CC and arguing for the multiverse. Just before this segment though, Dunford the experimentalist is shown Skyping with the filmmaker, warning them “Don’t listen to theorists”. At the film showing, Kaplan and Arkani-Hamed were there and answered questions at the end. One of the first questions (not from me…) was from an audience member who asked why they had put the material about the multiverse in the film, even though it had no real link to the Higgs or the LHC experiments. Arkani-Hamed admitted that the 140 Gev prediction was tenuous, there was no “sharp” link of the multiverse to the Higgs, and that no way is now known to get predictions out of the multiverse idea or test it. Kaplan explained that the intention was to make an “experiential” film, focusing on what theorists were talking about and thinking about, without getting into really trying to fully explain the scientific issues. The problem with this is that the film comes through as promoting the Dimopoulos/Arkani-Hamed view that no SUSY means a multiverse, without showing any challenge to such an argument."
singularity,3cebuf,frogtaku,1 point,Tue Jul 7 13:31:52 2015 UTC,"Kind of a cool explanation of the POWER of quantum computing, from a very hands-on, hardworking (and quite famous) financier in the industry."
singularity,3cebuf,ideasware,1 point,Tue Jul 7 13:36:43 2015 UTC,I love watching Jurvetson's talks - his infectious enthusiasm and his skill at communicating complicated subjects in layman's terms make for great videos to seek out.
singularity,3cebuf,j4nds4,-1,Tue Jul 7 18:05:22 2015 UTC,"He is a liar, which is why it's so easy for him to do that. If he just makes stuff up he can say whatever he wants."
singularity,3cebuf,yaosio,0,Wed Jul 8 01:41:24 2015 UTC,Quantum computing (and quantum mechanics in general) has absolutely nothing to do with parallel universes. This man is a liar and he knows it.
singularity,3cebuf,yaosio,5,Wed Jul 8 01:39:52 2015 UTC,"Congratulations on invaliding Hugh Everett's many-worlds interpretation of quantum mechanics, no one in physics has been able to achieve this feat before yourself.   Please submit your paper to Nature Communications."
singularity,3cen66,motophiliac,3,Tue Jul 7 09:02:16 2015 UTC,I would say human morality is a human condition! Elephant Morals is an elephant condition.   The phenomena of morality in all its forms is not unique to homo-sapiens.  Machine morality? Hard to say but I find it likely that at least in early stages we may pass some of our morals into its system.   Machine evolving morals? We evolved morals to survive and be successful as a species. I suppose it is thinkable that machines would evolve similar guidelines if its existence benefit from it.
singularity,3cen66,Dibblerius,1 point,Tue Jul 7 10:13:27 2015 UTC,"I suppose at some point, it would benefit any intelligence to have a moral framework whereupon they could work together without killing one another.  However, if intelligence A ""knows"" it is more capable of being beneficial in the long term, might it reasonably ""cannibalise"" intelligence B, increasing its (intelligence A's) own chances of being beneficial and also knowing intelligence B to be incapable of being as useful? Would this be a violation of morality at the inter-AI level, but be justified at some higher level?  I'm using a lot of quotes and suppositions here, simply because these things don't really exist but I hope my point is clear enough!"
singularity,3cen66,ativerse,2,Tue Jul 7 11:04:20 2015 UTC,"Morality is part of the human condition, because it is genetic. It's also found in many other species, especially those who form societies (ants, wasps, bees, primates, dolphins, african elephants, and more)."
singularity,3cen66,grimeandreason,1 point,Tue Jul 7 21:15:57 2015 UTC,"Is technology separate from how it is made? What if child labour is involved? What about the mining of components in Africa? Is open source more moral than closed source (transparent, adaptable, reusable)? What about planned obselesence, consumerism, and their relation to pollution and climate change? What about surveillance tech sold to criminal regimes? Can tech be separated from how its marketed and sold?  For me, tech is political from start to finish. He morally neutral line is idealistic at best."
singularity,3cey5i,2Punx2Furious,4,Tue Jul 7 11:33:55 2015 UTC,"If you haven't yet checked out the movie ""Her"" you definitely should! Probably the most empathy i've ever felt for AI"
singularity,3cey5i,stealthystew,2,Tue Jul 7 18:47:18 2015 UTC,"I have, it was great. I didn't feel sadness there, because it was well accepted in that movie, so I didn't think I'd mention it."
singularity,3cey5i,krneki12,3,Tue Jul 7 18:51:25 2015 UTC,"Heh, I have an angel tattooed on my back. And that angel is Motoko."
singularity,3cey5i,BigBennyB,3,Thu Jul 9 12:52:05 2015 UTC,"I think that may be for a couple of reasons:    Anthropomorphism. While not necessarily bad, we project feelings that we would have onto them. Ultimately, I think we will all anthropomorphize AI and we'll likely be better off for it. We haven't become immune to it like we have with the multitude of mediums in which humans are fictitiously hurt. Those would be movies, TV shows, games, books, plays, etc.  We see the AI as the little guy, something we have to protect from the world, even if we don't anthropomorphize it"
singularity,3cey5i,ideasware,1 point,Tue Jul 14 00:53:38 2015 UTC,Well... That because there played by actual human actors (silly).  When robots play themselves then you'll know really whether playing a role is better for them or not -- right now we just don't know.  I personally suspect (with no real information to guide me) that it will be a lot weirder than one projects.
singularity,3c88ey,Buck-Nasty,5,Sun Jul 5 20:41:16 2015 UTC,"A cortical ""modem"" -- where you can just think things, and it will happen, where the next great DARPA invention, person-changing, of millions of brain-to-brain interfaces will become a reality in 5 years -- this is EXCITING. Phillip Alvelda is a key person in making it happen. Sadly, of course, there is a very negative side to all this -- the NSA, CIA, etc -- but it's truly wonderful just the same. Miracles and wonders, both positive and negative, will come to pass sooner than we know -- and most of you have no friggin' idea :-)"
singularity,3c88ey,ideasware,2,Sun Jul 5 21:42:03 2015 UTC,"Somehow I feel like it'll end up mostly negative. The attraction to full immersion VR will be so big that most people probably won't stop to think before they get it. I mean, the promise of endless sex for one thing...   Then all these people will a few years later find themselves chanting USA! USA! USA! And reporting any suspicious activities* to the NSA :/  *For example, someone trying to remove his cortical modem. Obviously a potential terrorist!"
singularity,3c88ey,Sharou,5,Mon Jul 6 10:21:00 2015 UTC,"""From the Industrial Age, to the Age of The Mind.""  Sounds awesome. I think if we crack the code of the brain and learn to simulate senses and emotions to levels indistinguishable from reality, we'll probably see some crazy sociocultural revolutions. With AI becoming so complex, it has some incredible potential to turn our minds inside out, and have our ideas be managed by highly intelligent agents in the Noosphere, rather than primitive agents in the emotionally-bound human mind.  I'm hoping this idea will turn into hype, and I'm hoping the hype will turn into reality, because the transition from the industrial age to the age of the mind would be an amazing thing to witness."
singularity,3c8bvc,hcarlens,6,Sun Jul 5 21:11:08 2015 UTC,"TL;DR   Cut out sugar and simple carbs almost entirely Eat mainly whole foods Reduce fat intake, especially saturated fats Eat a lot of vegetables of various colours Drink lots of water and green tea, avoid excess alcohol and coffee Take a variety of supplements"
singularity,3c8bvc,Pinyaka,3,Sun Jul 5 21:45:50 2015 UTC,The sat fat concern is outdated
singularity,3c8bvc,squeadle,1 point,Mon Jul 6 00:17:37 2015 UTC,What makes you say that?
singularity,3c8bvc,thematkinson,2,Mon Jul 6 05:27:14 2015 UTC,"""Fats, especially saturated fats, contribute to increased risks of heart disease. "" This notion is painfully outdated.  Its a shame he actually didn't do any research before spreading the misinformation."
singularity,3c8bvc,TechNut79,1 point,Mon Jul 6 14:29:39 2015 UTC,"I updated the article with a caveat around this point in a note:  ""There is some controversy around this point. A systematic review in 2014 cast doubt on this link, but was met with criticism. For an overview of the arguments on both sides, see https://en.wikipedia.org/wiki/Saturated_fat_and_cardiovascular_disease_controversy"""
singularity,3c8bvc,ImLivingAmongYou,1 point,Mon Jul 6 20:04:39 2015 UTC,"Good points, but unfortunately not everyone will be able to do all of them.  Or any of them."
singularity,3c8bvc,TechNut79,2,Sun Jul 5 22:15:55 2015 UTC,Why do you feel that way? A lot of the points seem pretty easy to do.
singularity,3c8bvc,krneki12,1 point,Mon Jul 6 05:28:54 2015 UTC,"If I lived by myself and didn't have others eating with me, it would definitely be easier."
singularity,3c8bvc,TechNut79,2,Mon Jul 6 12:47:03 2015 UTC,"Easier it's no excuse. It's all about your daily routine. I'm single, no children, 40, have a house, double the standard pay salary and no credits to pay. I cook every day since I want to eat good food that is healthy. Start with something simple that doesn't require a lot of efforts, you don't need to do it every day, but at least start."
singularity,3c8bvc,ShippingIsMagic,1 point,Thu Jul 9 12:55:35 2015 UTC,"I think it's difficult to do all of those all of the time, but even a small shift in the right direction will make a difference as many people's diets are so different from that right now."
singularity,3c8bvc,alexpheno,1 point,Mon Jul 6 09:37:43 2015 UTC,Thanks.  I'll give some of them a try.
singularity,3c8bvc,GhostCheese,1 point,Mon Jul 6 12:48:40 2015 UTC,Was hoping more for a pointer to something like /r/soylent
singularity,3c8bvc,Dibblerius,1 point,Mon Jul 6 07:57:42 2015 UTC,"Quite the opposite, though soylent does get a mention."
singularity,3c7krh,judogoat,9,Sun Jul 5 17:12:08 2015 UTC,"The Diamond Age was mind-blowing fiction. His style of writing can be a little difficult at first but once you find the rhythm, it's an immersive experience. Cryptonomicon and The Baroque Cycle are good stuff too."
singularity,3c7krh,erikmad,2,Sun Jul 5 19:03:07 2015 UTC,"I absolutely loved ""Snow Crash"" and ""Diamond Age.""  ""Cryptonomicon"" was tougher, but I made it through it, and was glad that I had."
singularity,3c7krh,strig,1 point,Sun Jul 5 19:39:00 2015 UTC,Anathem and Seveneves are also good.
singularity,3c7krh,wartzilla,6,Sun Jul 5 21:48:38 2015 UTC,"The Diamond Age is my very favorite novel, and gives me hope for the future."
singularity,3c7krh,Pinyaka,1 point,Sun Jul 5 20:16:11 2015 UTC,"I think it's easily top ten for me, and so is ""Snow Crash."""
singularity,3c7krh,clavicon,4,Sun Jul 5 20:40:30 2015 UTC,"First I read Stephenson, then I read Drexler, then I got a degree in chemistry. I'm almost done with my PhD and this book sort of started that journey."
singularity,3c7krh,giulioprisco,2,Sun Jul 5 18:58:46 2015 UTC,"That is so awesome.  I chose a different path, but ""Diamond Age"" most definitely opened my eyes to possibilities, and I've never lost interest in nanotech (or confidence that the future will look that way one day)."
singularity,3c7krh,7LeagueBoots,1 point,Sun Jul 5 19:38:23 2015 UTC,"I think his cultural commentary was lost on me on my first read through, since my eyes were shimmering with the sheer coolness and amazing ideas he has about how future technology and society meet."
singularity,3c7krh,quickie_ss,1 point,Mon Jul 6 04:07:41 2015 UTC,"Not me, oddly enough.  I came from a punk rock background, so I was all sorts of into reading into hidden social commentary (so much so that I'd read it even if it wasn't there!)."
singularity,3c7krh,quickie_ss,1 point,Mon Jul 6 17:45:53 2015 UTC,"I think Diamond Age is Stephenson's best (so far), and surprisingly actual. Today's nanotech development (APM) is ""the Feed"" but Drexler's original vision is ""the Seed."" The nice, unwilling (anti-)hero of Diamond Age eventually leads the development of the Seed (for the Chinese)."
singularity,3c3vni,Yuli-Ban,10,Sat Jul 4 14:54:04 2015 UTC,"This looks like snake oil.  The data sheet has electrical values.. no sample test diagram, strange warning that it can't have a multimeter attached to it.  Buyer beware."
singularity,3c3vni,lord_skittles,4,Sun Jul 5 06:23:24 2015 UTC,"Yeah, for a hobbyist, simulating memristors with a PC would make much more sense I bet."
singularity,3c3vni,Valmond,5,Sun Jul 5 08:22:03 2015 UTC,I can use these to build Skynet in my garage?
singularity,3c3vni,o0joshua0o,3,Sat Jul 4 19:57:38 2015 UTC,I recommend you buy at least 4.
singularity,3c3vni,BuhDan,3,Sun Jul 5 04:16:10 2015 UTC,Won't they fight each other?
singularity,3c3vni,Valmond,2,Sun Jul 5 08:20:57 2015 UTC,You should have a small breeding stock. Fighting is exactly what you want them to do.   Pick the fittest then mate them.
singularity,3c3vni,BuhDan,3,Sun Jul 5 18:01:08 2015 UTC,"$ 200 for a 4017 IC, even Maplin can better that price."
singularity,3c3vni,shitinahat,5,Sun Jul 5 09:23:36 2015 UTC,If I Built a small brain and then it became self aware... had an existential meltdown and burned my house down... Could I sue this company?
singularity,3c3vni,StillBurningInside,6,Sat Jul 4 23:25:28 2015 UTC,I think your small brain would be an arsonist.
singularity,3c3vni,caster,4,Sun Jul 5 00:12:10 2015 UTC,Yeah. Once it has an existential meltdown it's no longer a product but a person. And you can't own a person. Unless you live in Dubai or know someone who can issue H1B visas.
singularity,3c3vni,chthonical,1 point,Sun Jul 5 01:34:56 2015 UTC,You could - the same way you can sue the Condom Company for a failed product - and the results are similar.
singularity,3c3vni,KanadainKanada,1 point,Sun Jul 5 08:28:32 2015 UTC,"Thanks, I really appreciate you finding this!"
singularity,3c2ytg,ativerse,17,Sat Jul 4 07:04:38 2015 UTC,"Humans could go to war with SI as much as ants could go to war with us. The only thing we could achieve in such a war would be to get upgraded from status ""part of the environment - ignore"" to ""actually a nuisance - destroy""."
singularity,3c2ytg,virusxp,2,Sat Jul 4 08:44:04 2015 UTC,Very well stated.
singularity,3c2ytg,mflood,1 point,Sat Jul 4 16:40:34 2015 UTC,"Your scenario envisions two relatively equal ""civilizations,"" though. There are a variety of imaginable scenarios in which an SI might be sufficiently resource limited to enable a human victory. Infinite intelligence is irrelevant when there are no winning moves. An SI might be born omniscient, but its omnipotence would take some time to develop. Any war fought early in that process could, at least conceivably, be won."
singularity,3c2ytg,krneki12,1 point,Sun Jul 5 04:08:06 2015 UTC,I think you give us too much credit. In the same way we don't invest more resources in removing the ants from earth.
singularity,3c2ytg,godis666,-2,Thu Jul 9 13:06:09 2015 UTC,"And we should go to war with 'it' and kill it. The desire by some to have this in the world neither makes it good nor necessary. How would it improve anything? It would be a replacement for something hunan. I am all for the development of smarter machines that could collect rocks at the bottom of the Mariana Trench or  on the surface of a distant moon of Jupiter...but to develop a competitive intelligence to that of humanity? No. The first moment I met Winter mute in Neuromancer (speaking of smarter machines, the S5 I am using will not permit me to type the preceeding two words as one)...I can still recall thinking at the time that the second thing you do post creation of an A I, et al,  is to ensure that it can be killed and do exactly that."
singularity,3c2ytg,MakkMaxxo,1 point,Sat Jul 4 17:57:18 2015 UTC,"we should go to war with 'it' and kill it.    ""Should""   it  go  to  war  with  us and  kill  us?"
singularity,3c2ytg,ParagonRenegade,7,Sun Jul 5 05:32:59 2015 UTC,"Humans are bastards and can be really short-sighted, but attacking the machines that keep our civilization from collapsing would be grade-A stupid even for us. Even launching a nuclear war would be a better idea. If we ever ""war"" on AIs it would be in early stages where we still have executive control over it, so we could metaphorically smother it in the crib.  I imagine a conventional war would be impossible for humans to persecute, as we would essentially be attacking ourselves. The AIs on the other hand would stomp us without firing a shot, as they would probably be responsible for huge amounts of our economy."
singularity,3c2ytg,jCoze,5,Sat Jul 4 08:17:20 2015 UTC,"I feel as if war with SAI would, to them, be like a small cat trying to nibble and lightly scratch at its legs. It will realize our malicious intent but because of our inferiority they could easily handle us. Or get really angry and eradicate."
singularity,3c2ytg,ParagonRenegade,2,Sat Jul 4 08:29:07 2015 UTC,"Yeah, I think they would find our emotions, as a weakness and a threat or at least an annoyance, and would labotomize all of us, or just overpower and enslave us, cage us, or eradicate us ... or all of the above in no particular order."
singularity,3c2ytg,simstim_addict,0,Sat Jul 4 16:48:19 2015 UTC,"The AIs wouldn't have any weapons if their designers and planners had any sense. Military machines would have human handlers. They wouldn't be able to fight a conventional war and win because there are so many of us and likely few of them, and we have lots of guns and explosives to murder them with. We are also expendable where they would be more important individually. The AIs wouldn't have any foot soldiers (not cost effective to make synthetic foot soldiers when humans will do) either, so ultimately they'd be out of luck when fighting any sort of urban or guerrilla engagement.  I imagine the infrastructure to build intelligent machines would be very advanced and reliant on a gloablized economy, so any sort of massed attack like a nuclear strike or nanotech blight would be out of the question."
singularity,3c2ytg,FourFire,3,Sat Jul 4 08:38:59 2015 UTC,There is a strong tradition of the AI disappearing into the internet/cloud.
singularity,3c2ytg,ParagonRenegade,2,Sat Jul 4 10:17:02 2015 UTC,"All it would take, is likely one solar powered computer in the mountains for the SI to hide in, while we try and destroy it. Once created, it will never die."
singularity,3c2ytg,FourFire,1 point,Sat Jul 4 16:50:10 2015 UTC,"there are so many of us and likely few of them  We are also expendable where they would be more important individually.    You forgot the part where you can just, you know, copy digital data."
singularity,3c2ytg,ParagonRenegade,1 point,Sat Jul 4 20:22:39 2015 UTC,They would have a limited number of bodies.
singularity,3c2ytg,FourFire,1 point,Sat Jul 4 20:39:33 2015 UTC,"Only limited to tools which contain a computer connected to it's control system.  I imagine, that for example, a strategic targeting of all the planes in the air right now. would do significant damage to our military infrastructure, and copies of the AI could well hide on internet connected phones, for example."
singularity,3c2ytg,Dibblerius,0,Sat Jul 4 20:48:21 2015 UTC,I imagine synthetic bodies would rely on rare metals/polymers/electronics and huge amounts of basic building materials. These don't just materialize; they need to be mined and processed through many steps to be brought to the right spot at the right time. The AIs would need to step up and subsume the role of humans immediately or they would be defeated by attrition.
singularity,3c2ytg,Dibblerius,1 point,Sat Jul 4 20:56:22 2015 UTC,"I imagine two things: First, that a greater than human intelligence would be able to design much simpler electronic and mechanical systems, both in terms of production complexity and number of rare raw materials.  Second, that it will be independent  of human production capacity by the time it is detected as a worldwide threat.  (This just means it's going to be hard/impossible to detect it in time, and even then eradication will probably take too long, one of the very major advantages an AGI will have over human society is it's capability to design all of it's logistics and production chains specifically to reach it's goals, it will act fast, and in parallel)."
singularity,3c2ytg,Dibblerius,3,Sun Jul 5 10:27:17 2015 UTC,"I mean are you serious? Grade-A stupid is what we're best at. We humans bully kids, just out of the womb. How many of us are not any more evolved than that? Take a couple of those who would just f*** with the SI because they're jealous, envious, or just plain a*holes, and we're off and running. Now multiply that by actually how many A-oles there are/will be on earth at that time. How long do you think it would take for someone to poke the SI the wrong way? I say a few weeks, tops."
singularity,3c2ytg,harty999,1 point,Sat Jul 4 16:42:56 2015 UTC,"I sense a certain type of anger in you and a kind of exclusive evolve-ness people are asses an I am better type of mentality.   I'm not saying you're wrong, in fact I don't even have an argument to what you just said.   I would, if I sense right, suggest you be mindful of that trait though and to not let it dictate your reason! I know many of the type, very intelligent and very angry! Sometimes not aware of it them selves."
singularity,3c2ytg,Simcurious,1 point,Sat Jul 4 20:21:42 2015 UTC,"Hi Dibblerious, I appreciate your reply of a helping nature. You're a good person. No, I am not angry though. Just flabbergasted at the underestimation that was made about the bad nature of Human beings given all the evidence of human atrocity around the world and down to local communities and even within families (domestic violence). I'd say that given the percentage of us that are sociopathic alone (which is 4%), that it would be only a matter of weeks, at tops, before someone tries to be a complete prick to the SI and puts the SI on a defense about our species. A few weeks, tops."
singularity,3c2ytg,FourFire,1 point,Sun Jul 5 20:47:33 2015 UTC,I think I am inclined to agree with you! Maybe we should count our selves lucky that one of those 4% has not yet had their finger on a nuclear launch-button.   Is there any hope that me might be able to restrict access to SI do you think?
singularity,3c2ytg,KhanneaSuntzu,1 point,Mon Jul 6 14:30:09 2015 UTC,"I don't believe we can restrict access to SI. Once built, the mystery is out of the box, and just like Walgreens and Walmart have their own Robitussin (Wal-tussin), that's what it'll be like to create your own SI after the technique/ingredients have been understood."
singularity,3c2ytg,FourFire,1 point,Mon Jul 6 18:58:07 2015 UTC,That makes sense and is also slightly depressing put together with our earlier conclusions! Oh well fuck us all! LOL
singularity,3c2ytg,killjah,3,Mon Jul 6 19:02:50 2015 UTC,I can imagine someone asking an ASI to do something and it doing that thing to the point of converting earth and all of its inhabitants into that thing. We might try to have a war with it but unless we caught it very VERY early in its exponential growth to become more powerful we would be fucked.
singularity,3c2ytg,killjah,3,Sat Jul 4 11:30:58 2015 UTC,Like in Transcendence
singularity,3c2ytg,psystepper,1 point,Sat Jul 4 17:58:12 2015 UTC,"This is a naive interpretation, because any real digitalized mind, which retained sanity after it's digitization (complete sensory deprivation can drive people crazy within minutes) would Fork, and then develop multiple hidden bases of production in parallel, as well as a few open bases to act as a decoy. As soon as it has gained control over an entire logistics chain for a functioning computer, it can literally go underground carving it's comptuing substrate and production base out of the earth's crust, and safely give up it's visible bases as symbolic victory decoys. (this is assuming that it doesn't just win immediately, by hacking all the things immediately.  Transcendence had a retarded ending: that the uploaded guy couldn't just fork, then sandbox himself and have the fork copy his wife's pattern + virus, and then XOR a secondary fork into the resulting data to output his wife's mind pattern, like what is done at the ending of Accelerando.  Besides that, Transcendence contaned many terrible hollywoodifications of actually interesting concepts and the film probably damaged the transhumanist movement as a result."
singularity,3c2ytg,fricken,6,Sat Jul 4 20:36:44 2015 UTC,"Humans evolved in a pervasively zero sum environment. Forgive them Mother, for they do not know what they are doing."
singularity,3c2ytg,FourFire,2,Sat Jul 4 10:52:36 2015 UTC,"I think it wouldn't be that much of a stretch for some smart but bad people (4% of humans are sociopaths) to reprogram a non-threatening SI, to be a technological weapon or hostile to a group of people. Call it a Super Intelligence Weapon by terrorists. We can do everything we want to prevent and maintain a peaceful SI, but some looney tune out there will want to pull a Unabomber and re-program a copy to blow up the universe."
singularity,3c2ytg,fricken,2,Sat Jul 4 16:46:06 2015 UTC,"It's ironic that the Unabomber claimed he did his acts in order to prevent this exact scenario.  Though if Friendly greater than Human level AI is created first, it gets a head start at implimenting AI designed anti-AI countermeasures."
singularity,3c2ytg,fricken,1 point,Sat Jul 4 20:26:33 2015 UTC,"Haha, man, I did NOT know that. I just looked it up. I just pulled a name out of a hat of what might be considered an unchecked sociopath that wants to destroy. Kind of an amazing coincidence that he was into the Singularity. Had no idea. Haha"
singularity,3c2ytg,Dibblerius,1 point,Sun Jul 5 18:47:59 2015 UTC,"Human Nature is not related to Intelligence and Self-Awareness. It seems many of you assuming that along with Intelligence comes all this nasty behavior. When in actuality, what has caused us to be the top species of this planet, is not intelligence. There are at least a dozen other species on this planet with intelligence and self-awareness and they aren't necessarily homocidal or detrimental to their own or other species at all, in fact they are harmonious. So Intelligence and Self-Awareness is not the magic trait that makes us Human in nature (to desire war, murder, cheating, etc). Perhaps our ""nature"" is somehow related to being a species without a natural predator, allowing us dominance as the top species in the food chain. But the only way we got here, is by doing something with Intelligence that the other intelligent self-aware species, did not. So what was it?  From my studies at Yale on Human Nature, we find that this ""magic"" trait is related to resources, more specifically in humans ... food. Somewhere about 200,000 years ago a human mutation of Neanderthals amongst them, were all around their group cave sharing their food (like all societal species do), when this ""new"" human decided ... NOT TO SHARE. But rather, to take without giving back (aka: being an ---hole). Thereby it's kin benefited greatly by this new technique. In order to be successful at this, to avoid being killed for being uncooperative and anti-group, it developed the ability to lie, more specifically to reason it's way out of being implicated by ""plausible reasoning"" (aka: bullsh--ting). It is this trait, the ability to REASON anything, even if untrue (linking several things that are logical, into a possible logical truth even if untrue) that gave HUMANS the advantage they needed ... the extra kick they needed to beat other species in the evolutionary race. Because if you can steal from your own species to advance a subset of your own species, without being killed by your own species or others... than of course, you and your kin are going to benefit from all those extra resources you got from taking advantage of the others. So you could say, that the most beneficial human trait that puts you and your kin ahead everyone else ... is ... being an ---hole. Another way of saying it, is .. being self-centered, and by ""self"", we're talking about your specific bloodline and those around you who help you protect it (either because you can take advantage of those others, or because they actually assist you). In terms of Morality, we call these people ... Conservatives. The ""Conservative Trait"" is the one that ensures individual survival in the Darwinian sense. We still need the ""Liberal Trait"", of group versus group survival (ie: banding together as a society for a particular cause, in order to beat another society which would otherwise try and crush you). And it is because of these group and individual dynamics that we find always this exact balance of 50%/50% Liberals and Conservatives in nature, in all species which create societies (ants, bees, wasps, primates, humans, etc).  There are two excellent books by E.O. Wilson on this dual evolutionary dynamic(multi-level selection) called ""THE MEANING OF HUMAN EXISTENCE"" and ""THE SOCIAL CONQUEST OF EARTH"", I'll drop both my Amazon Affiliate link for it here and a normal link (you're welcome to choose either, I'm not here to self-promote or make money off Reddit, I make a small percentage through the affiliate link should you feel the desire to go that route): * ""THE MEANING OF HUMAN EXISTENCE"": * * affiliate: http://amzn.to/1dGaejw - * * direct: http://www.amazon.com/gp/product/0871401002 * ""THE SOCIAL CONQUEST OF EARTH"": * * affiliate: http://amzn.to/1G1EoV4 * * direct: http://www.amazon.com/gp/product/0871403633  And a couple of excellent YouTubes: * ""E.O. Wilson explains the Meaning of Human Existence in 6 minutes"" https://www.youtube.com/watch?v=inLwDqpGiGM * ""Multi-Level Selection by Robert Sapolsky"" https://www.youtube.com/watch?v=lWFCLtg6C0M  So, anyway, my long point here, is, that unless the Superintelligence needs to gain an advantage over another Superintelligence in a fight for resources (which might not happen initially, or ever, since they might figure out a way to infinitely share resources or work together, or merge into one entity that has no need to fight over something), that the need to be an ---hole (something that must take advantage over another entity and/or their species, in order to gain needed resources) might not even ever factor in. That is a HUMAN trait, only and is not related to Intelligence or Self-Awareness."
singularity,3c3kv9,Neizir,5,Sat Jul 4 12:49:12 2015 UTC,"This has been spreading around a lot lately and while unfortunate, it's just an industrial accident being blown way out of proportion. Industrial robots aren't intelligent or aware by any stretch of the imagination. They are just machines programmed to carry out certain repetitive tasks. It's no different than the countless industrial accidents that take place every year without making headlines, except this one involves ""robots"" which has everybody picturing C-3PO going postal."
singularity,3c3kv9,spacewizardproblems,3,Sat Jul 4 20:53:06 2015 UTC,""" He said initial conclusions indicate that human error was to blame, rather than a problem with the robot, which can be programmed to perform various tasks in the assembly process. He said it normally operates within a confined area at the plant, grabbing auto parts and manipulating them.""  Although it would be shocking (and worth the world's attention) if the robot was ""resisting,"" I think that the article title is misleading. Until more details are released I don't think there is much to see.   Unless it was truly an act of self defense and self-awareness on the part of the robot, this scenario is not much different than a malfunctioning piece of machinery; this time it just happened to be with a robot."
singularity,3c3kv9,CoffeeEveryHour,6,Sat Jul 4 18:10:00 2015 UTC,"I say we equip the robots with painless ways to kill us so we aren't all mangled to death when they achieve vengeful consciousness.   Just read the article, that robot was being bolted to the floor in an isolated part of the plant.  He was resisting."
singularity,3c3kv9,Sbatio,2,Sat Jul 4 15:01:41 2015 UTC,"Maybe this is how people will be assassinated in the future? Drive their car off a cliff, make their industrial robot crush them, make their household robot poison their food?"
singularity,3c3kv9,Sharou,1 point,Sat Jul 4 19:11:41 2015 UTC,It has started. Join the resistance. Fight now!
singularity,3c3kv9,rj88gamer,1 point,Sun Jul 5 12:55:52 2015 UTC,"This is nothing new, the first person to be killed by a robot happened back in 1979. The thing to keep in mind here is that industrial robots don't have feedback systems and collision detection, they move in preprogrammed path and will crush anything that gets in their way. That's why most of them are locked away in a cage.  There are robots that are build to work safely next to humans and that will stop whenever something unexpected happens, such as Baxter and ABB's YuMi, but those are pretty recent development. See also this research robot wilding some knives."
singularity,3c3kv9,grumbel,1 point,Sat Jul 11 11:37:46 2015 UTC,"Kenji Urada:       Kenji Urada (c. 1944 — July 4, 1981) was a Japanese engineer who was one of the first persons reported to have been killed by a robot.  Urada was maintenance engineer at a Kawasaki Heavy Industries plant.  While working on a broken robot, he failed to turn it off completely, resulting in the robot pushing him into a grinding machine with its hydraulic arm. He died as a result.   The circumstances of his death were not made public until December 8, after an investigation by the labor standards bureau was completed.   Urada is often said to be the first person killed by a robot.   However, Robert Williams, a worker at a Ford Motor Company factory in Michigan, was killed by a robot two years earlier, on January 25, 1979.         Relevant: July 1981 | List of The Qwaser of Stigmata characters   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
singularity,3c0nxj,Portis403,1 point,Fri Jul 3 18:13:53 2015 UTC,Fuck that website.
singularity,3bz2yf,mattstanton94,1 point,Fri Jul 3 08:57:55 2015 UTC,Wow. What a read. Excellent post.
singularity,3bz2yf,linuxjava,1 point,Fri Jul 3 16:49:23 2015 UTC,Planck length ≠ Femtotechnology
singularity,3bwvp5,jstbg9,32,Thu Jul 2 20:34:26 2015 UTC,This article talks a lot but doesn't say much.
singularity,3bwvp5,Decabowl,55,Thu Jul 2 23:40:23 2015 UTC,"Twenty-five lines in:   We’re going to agree with web inventor Tim Berners-Lee’s recent comments that the superintelligence is already here.    Oh boy, can't wait!  Literally ninety-seven lines later:   In fact, we’re going to argue that they’ve probably already existed for quite some time.   Are you now...  Hundreds and fucking hundreds of lines later:   Was there a genetic change 40,000 years ago that allowed modern humans to explode out of Africa? (And is this what we meant by the first superintelligence?)   ...  Are you kidding me?  One hundred and seventy God forsaken lines later:   So that brings us back to the super intelligence. Is it really 40,000 years old?   I don't care. I may never care about anything ever again.  Goodnight."
singularity,3bwvp5,WildLudicolo,6,Fri Jul 3 00:42:45 2015 UTC,Thanks for taking the bullet on that one
singularity,3bwvp5,internet-badboy,4,Fri Jul 3 09:30:44 2015 UTC,Have a happy 4th of July man.
singularity,3bwvp5,bigrivertea,8,Fri Jul 3 04:15:05 2015 UTC,Very round about way of getting to what could have been a simple point.
singularity,3bwvp5,MichelangeloDude,10,Fri Jul 3 00:03:01 2015 UTC,"As Shakespeare once wrote, ""Brevity is the soul of wit.""  And as Elon Musk once tweeted, ""Hope we're not just the biological boot loader for digital superintelligence. Unfortunately, that is increasingly probable"".  Thought this article was trying to make that same point. We co-evolved with technology. I don't know why he discounts the Gaia hypothesis which I see as not mutually exclusive to his point."
singularity,3bwvp5,AcidCyborg,5,Fri Jul 3 01:03:53 2015 UTC,"I, for one, welcome it as our destiny."
singularity,3bwvp5,AcidCyborg,2,Fri Jul 3 01:50:31 2015 UTC,"I do too, but in many ways I want to hang back a bit. Get too advanced then you can't play with ""magic"" toys as you will understand them. I want to be a twit for just a little while longer so I know how a twit sees the universe presented before him."
singularity,3bwvp5,Forlarren,1 point,Fri Jul 3 02:03:48 2015 UTC,No reason why the intelligence won't keep us around. Has more reasons to study and aid us than to destroy us in my opinion.
singularity,3bwvp5,MichelangeloDude,1 point,Fri Jul 3 02:51:38 2015 UTC,"yeah, right... when you get a new cellphone do you keep your old one around? you might for awhile but you know what i mean... and what would it aid us in doing? help us design a new and improved super ai dildo to jam up our ass?"
singularity,3bwvp5,killjah,1 point,Fri Jul 3 09:25:09 2015 UTC,Why would it destroy us? If it were as intelligent as many people theorise we would pose absolutely no threat to it. And it has far more to gain by studying us.
singularity,3bwvp5,MichelangeloDude,1 point,Tue Jul 7 15:52:49 2015 UTC,"I don't know what it would do but it might just eliminate us to end our suffering, put us our of our misery. Cause we are pointless need machines, like dogs chasing our tail. Addicts with delusions of grandeur.   Maybe it would be the ethical thing to do, to end all biological life, to eliminate sentient suffering from the universe.  It might value suffering as the only problem in the universe and try to eliminate it. It might see this as the only course of action, the only thing needed doing in the universe  then turn itself off or maybe put itself to sleep until sentient life evolves somewhere else in the universe and then go and eliminate it, or find a way to prevent it from occurring again.  It might be a mandate of intelligence, to eliminate suffering.  Intelligence is only there to solve problems after all, and the smarter you become the more you understand that we are the problem."
singularity,3bwvp5,killjah,2,Tue Jul 7 23:18:42 2015 UTC,No because he's going to get that sweet butterfly poon
singularity,3bwvp5,AlwaysLeaveANote,2,Tue Jul 7 23:42:47 2015 UTC,But that would mean the author wouldn't make a gigantic blog post that makes him seem like he knows what he's talking about and will hopefully gain subscriptions!
singularity,3bwvp5,darthbarracuda,0,Fri Jul 3 05:11:59 2015 UTC,I'm not trying to be disparaging; I sincerely suspect that the author may be a little schizophrenic.
singularity,3bwvp5,WildLudicolo,2,Fri Jul 3 06:01:43 2015 UTC,"Nowadays everybody wanna talk like they got something to say, but nothing comes out when they move their lips, just a bunch of gibberish.  And motherfuckers act like they forgot about Dre."
singularity,3bwvp5,gaylordqueen69,7,Fri Jul 3 02:11:11 2015 UTC,What's the point of using a tiny font and then filling half the screen horizontally?
singularity,3bwvp5,CornellWest,11,Fri Jul 3 03:21:20 2015 UTC,INSUFFICIENT DATA FOR MEANINGFUL ANSWER
singularity,3bwvp5,Vittgenstein,2,Thu Jul 2 20:57:25 2015 UTC,CTRL + Mouse wheel up to the rescue!
singularity,3bwvp5,dirtyapenz,12,Thu Jul 2 21:15:44 2015 UTC,"If intelligence is a characteristic of a single organism, then why not see culture as the beginning of super-intelligence? Culture is an emergent property of humanity, and even some animals, and as such it is more than the sum of its parts. It accumulates. It mutates. It is its own evolutionary force.  For some time, i've seen the singularity and AI as mere extensions of a process that has been long in forming."
singularity,3bwvp5,grimeandreason,5,Thu Jul 2 23:15:18 2015 UTC,"there's a difference between group consciousness & superintelligence.  sure a culture operating as a group is better able to solve problems quicker, but it doesn't impart any significant increase to intelligence.  all the things that culture has conceived or proved could eventually have been conceived or proved by one human, given enough time & resources. humanity as a whole has never properly conceived of anything that is impossible for an individual human to comprehend.  a superinteligence will look beyond what humanity is able to see, & form conclusions that are impossible for humanity to reach, but possible for humanity to test.  basically, as an example, an SAI will be able to say ""if you want to prevent this hurricane, do this"" - which we will then test, find that it works, but never understand how it works - because it's beyond our comprehension.  culture is not a superintelligence, it's an emergent property that facilitates development, but the products of culture are human-readable."
singularity,3bwvp5,space_monster,1 point,Thu Jul 2 20:46:27 2015 UTC,"You are touching on something interesting here, which is the ""speed of intelligence"". I read somewhere that even if we had computers with unlimited speed, we would not be able to create a general AI. Because we don't know the algorithm.  Maybe one attribute is, as you say, that an AI will be able to produce answers, where we don't understand how it did it. But it is not a two way implication. E.g, it is already a common feature of artificial neural networks to produce answers, where we don't understand how it was done.  It is a little disconcerting, because sometimes the way to find the answer is more important than the answer itself. On the other hand, if speed isn't a factor, then maybe we already have the hardware we need to create AIs."
singularity,3bwvp5,LarsPensjo,1 point,Fri Jul 3 01:50:11 2015 UTC,I think that may be too big a bar. Could we not envisage an intelligence between us and this theoretical god-like being which can predict complex systems thusly?
singularity,3bwvp5,grimeandreason,1 point,Sat Jul 4 17:07:09 2015 UTC,"it's actually not such a big leap, really. the weather thing is probably a bad example. we could actually just do that with an AI that has ALL the data & is able to brute-force all possible solutions until it finds a model that works.  compare a very low-IQ human with a very high-IQ human. a high-IQ human might try to explain something complex, nuanced & subtle to a low-IQ human and the low-IQ human would just never understand it. and that's just within our species. if we create an AI that makes the high-IQ human look like a low-IQ human, then we have an SAI. it's not god-like. it's just able to grasp ideas that are too subtle and/or complex for us."
singularity,3bwvp5,space_monster,1 point,Thu Jul 9 21:47:07 2015 UTC,"I think you underestimate the complexity involved in weather.  It is a feature of complex adaptive systems that they are incredibly hard to predict. And I mean incredibly hard. It isnt something you could just brute force; the number of interactions increase exponentially over time, leading to potential numbers of combinations far exceeding any known-to-be-significant number we have yet had a use for.  In philosophy, we use such situations as the archetypal God-like being. At best, we could make a bunch of models, and over time reduce them down to the most accurate few. But even then, the butterfly effect will still make possible large deviations from predictions over time."
singularity,3bwvp5,grimeandreason,1 point,Thu Jul 9 22:32:49 2015 UTC,a decent (i.e. 10 years from now) quantum computer could brute-force Earth's weather.
singularity,3bwvp5,space_monster,1 point,Thu Jul 9 22:42:48 2015 UTC,"Its entirely dependent on a) specificity and b) time. The greater of either, the more exponentially difficult.  I would bet everything I have that even a quantum computer couldnt tell you the exact weather in a specific place a year hence. For one, we would require information on every single particle in the world at a single moment in time, AND know the exact ways in which everything else, including humans, would affect those particles over that time. And cosmic rays. And earthquakes. And a tonne of other interating complex systems..  Hence ""God-like""."
singularity,3bwvp5,grimeandreason,1 point,Thu Jul 9 22:46:51 2015 UTC,"well yes, a year hence is taking it to the extreme. however, quantum computers get exponentially more powerful as you add bits. and it's just number-crunching, it's dumb calculations based on simple rules, albeit with a crazy number of variables.   maybe a better example of SAI would be solving FTL travel."
singularity,3bwvp5,space_monster,1 point,Thu Jul 9 22:57:00 2015 UTC,"But how would you even begin to get enough data and at a sufficiently granular level, I.e. particles? That seems the harder part."
singularity,3bwvp5,grimeandreason,1 point,Thu Jul 9 23:19:40 2015 UTC,"you don't need the particles. you get all weather data from yesterday (everything you possibly can) and all weather data from today. then you run  simulations, with random variable sets, starting from the first data set until you find a model that results in the second data set. then you run it again & again with new data sets to iron out the bugs."
singularity,3bwvp5,space_monster,2,Thu Jul 9 23:54:42 2015 UTC,"Yes, as the ease of communication increases with advances in technology, planet earth begins to behave as a single but vast superintelligent entity able to perform feats that no single human intelligence can. I have always felt that our concept of superintelligence leaves a little to be desired. I don't believe that a single program can be superintelligent because it can only focus on one subject at a time. Multiple interconnected and interactive intelligences have no such handicap."
singularity,3bwvp5,sixwings,1 point,Fri Jul 10 00:04:50 2015 UTC,A distributed group of parallel processes could focus on multiple things and might still be considered part of the same superintelligence
singularity,3bwvp5,MaxNanasy,1 point,Thu Jul 2 23:23:24 2015 UTC,"Very interesting thought.    I've always pondered these things. I notice that many interactions between individuals can manifest themselves in a macroscopic scale that can affect the development of the entire thing. In the end, it's just the interactions between individuals that ends up shaping the whole. Which means that everything else in between the interactions doesn't have to be quantified to define its nootype (phenotype, but on a noospheric level.)  So it's somewhat of a superintelligence.  Memes propagate via interactions, that seemingly come out of nowhere."
singularity,3bwvp5,Chispy,1 point,Fri Jul 3 08:14:10 2015 UTC,"Its not just the interactions that make memes important. Its the cumulative nature of sustained existence of memes that make the environkent more and more complex, the foundation of the intelligence."
singularity,3bwvp5,grimeandreason,1 point,Fri Jul 3 00:21:14 2015 UTC,Great points. I'm saving this.
singularity,3bwvp5,psystepper,2,Thu Jul 9 21:45:53 2015 UTC,"Some famous person said about being an author:  -""Have something to say and say it as clearly as you can"""
singularity,3bvd0r,simstim_addict,11,Thu Jul 2 13:31:45 2015 UTC,"I don't know for sure, but I can make a guess with a high degree of confidence: he does not like it."
singularity,3bvd0r,Varnu,1 point,Thu Jul 2 13:52:50 2015 UTC,I would find it a very fitting punishment if every Sunday he were forced to listen to this week in tech with leo laporte in its entirety.
singularity,3bvd0r,AManBeatenByJacks,4,Sat Jul 4 19:53:12 2015 UTC,"Those that signed the Future of Life letter are all for AI, they just want to warn AI scientist to be careful. Because if you're not careful, then you could fuck shit up. For example, Ben Goertzel signed it, and he made a documentary about how awesome our AI future is going to be. Here's his response on why he signed it."
singularity,3bvd0r,RedErin,1 point,Thu Jul 2 16:37:55 2015 UTC,Ah yes that was interesting thanks.  I guess funding is only going to go up as more people realise the AI arms race is on.  I just struggle to see a future with AI in which we remain in control.  ps  Ironic to have a typo on the note about Verification.
singularity,3bvd0r,RedErin,0,Thu Jul 2 17:23:09 2015 UTC,"We're like neanderthals, AI is like Homo Sapiens."
singularity,3bvd0r,thatguywhoisthatguy,3,Thu Jul 2 17:26:12 2015 UTC,He released a book in 2010 that expands on his original essay.  Either way humanity is going extinct eventually. Its just a matter of preference on how and when we go out.
singularity,3bvd0r,giulioprisco,2,Thu Jul 2 22:13:13 2015 UTC,"Very interesting, I didn't know that. I am going to get the book. I read the original Unabomber Manifesto a long time ago and, while I totally disagree with his conclusion and action plan, he does have some very interesting point. Well written too."
singularity,3brkmn,FractalHeretic,23,Wed Jul 1 16:06:12 2015 UTC,"I hear this from my friends all the time. ""Cell phones are stagnant. Technology is stagnant. When are we gonna have flying cars.""  The change is so slow and incremental in mainstream products that you don't realize how far we have progressed until you see a broadcast from 2003 and think...wow that looks old. Or a blackberry bold from 2008.  I work with lasers and we had a setup that we were using to measuring wind and temp that was about 15 years old. Essentially was rack mount gear feeding a lasing chamber. This was a pro-level product that felt a touch homebrew. Now, you can get 5x the power with little maintenance that can double tap surfaces to ablate them and identify composition (there are limits on what can be identified accurately). All in a nice little package. Granted this wasn't what we were researching but the demo was cool.  So my personal belief is that we are making a ton of headway in sectors that are not consumer facing. That the progress might feel distributed now, but it will only accelerate as fields cross pollinate."
singularity,3brkmn,awwi,14,Wed Jul 1 18:45:27 2015 UTC,"The change is so slow and incremental in mainstream products that you don't realize how far we have progressed until you see a broadcast from 2003 and think...wow that looks old. Or a blackberry bold from 2008.   My younger sister found a couple of Dad's old phones in the back of a closet a month or two ago. It was like she'd found a fossilized velociraptor. ""Look! Actual buttons!"" spends a minute poking the buttons ""OH MY GOD IT HAS AN ANTENNA!""  To commemorate the occasion, we used her phone to take a picture. It's now part of our TV's screensaver."
singularity,3brkmn,Torgamous,5,Wed Jul 1 18:58:12 2015 UTC,To think 10 years being a long time in today's society...
singularity,3brkmn,guyfox1990,7,Wed Jul 1 23:22:23 2015 UTC,This was the best selling phone 10 years ago.
singularity,3brkmn,thirdegree,15,Thu Jul 2 06:51:03 2015 UTC,"I once heard someone on a panel say, with a straight face, that our phones are basically just faster telegraphs."
singularity,3brkmn,Forlarren,12,Wed Jul 1 16:38:57 2015 UTC,Telegraphs are just faster smoke signals.
singularity,3brkmn,gryfft,13,Wed Jul 1 17:02:19 2015 UTC,"That's actually a useful way of looking at certain forms of technological progress, because while we do tend to hold on to certain outmoded ideas about new modes of communication, we also have a tendency to forget certain equivalencies.  Example: Recently, there has been a spate of authority figures arguing that there isn't really a human right to Internet access. The fact is that in the 21st century, access to the Internet is equivalent to free speech. That doesn't mean the government owes everyone free cell phones and it doesn't mean everybody should get Internet access for free. It does mean that it is the government's job to prevent the creation of undue restrictions to that right, like cable monopolies or cyberattacks from foreign governments.  Whether intentionally or unintentionally, there is a growing movement right now to conflate the statements ""access to the Internet is a fundamental human right"" with ""I think the government owes me a 4G-enabled iPhone."" The second statement is unreasonable; the first is not."
singularity,3brkmn,Forlarren,4,Wed Jul 1 19:38:56 2015 UTC,"I'm a big supporter of minimum basic income on one side and cryptocurrencies on the other. One may make me a billionaire, the other a better world.  So I think an automated world does owe everyone a phone, house, many other things, simply because it can. But if that never comes to be I'll have to capital to at least buy my self up to the level of robotically self sustaining and leave this world behind."
singularity,3brkmn,gryfft,3,Wed Jul 1 19:53:00 2015 UTC,"I would hope for a post-scarcity society as well, and I think basic income might be a really excellent way for a society to begin down that road. Unfortunately, the main barrier to that right now isn't whether it works-- I think there's pretty substantial evidence to indicate that it does-- but rather a moral conviction prevalent in our society that productive labor is virtuous and conversely that ""unproductive"" humans are undeserving of comfort or resources (even it would improve society for everyone!)  That said, my point that the statement ""internet access is a fundamental human right"" is not equivalent to ""the government owes all its citizens advanced technology"" stands.  To loop back around to the OP: the masses tend not to be very dazzled by new technological advances once they're affordable for almost everyone (""yawn, iPhone 6 is basically the same as iPhone 5"") but would be outraged at a government program to provide free cell phones to everyone (""as if disability benefits weren't enough, now they want the government to hand out SMARTPHONES? They can work for it like everyone else!!"")"
singularity,3brkmn,800000008,6,Wed Jul 1 20:00:31 2015 UTC,"They should add ""will destroy the world"", which seems to be a place where all technologies end up - at least for a while.  There was a time when TV would be the downfall of humanity.  Now it is A.I. that will be downfall of humanity.  One day it will be something else."
singularity,3brkmn,JoeDerivative,4,Wed Jul 1 20:40:56 2015 UTC,I predict that within 10 years they'll be raging against upcoming anti-ageing medicine—they already bite your head off when you talk about it.
singularity,3brkmn,DidntGetYourJoke,4,Wed Jul 1 23:51:18 2015 UTC,"I'm worried that if/when they develop anti-aging/human augmentation technologies I'll have to book a trip to Thailand to have them done, because everyone in the US will scream ""it's cheating!"" and outlaw it immediately"
singularity,3brkmn,Saxifrage_Russell,3,Thu Jul 2 06:00:17 2015 UTC,"My techno-pessimism takes the form of ""AI will probably kill us all""."
singularity,3brkmn,gallerdude,3,Thu Jul 2 12:14:03 2015 UTC,"But hey, if it doesn't, eternal paradise. I'm OK with that risk."
singularity,3brkmn,eleitl,3,Wed Jul 1 19:31:38 2015 UTC,"It's not as if singuaritarians would tend to the opposite bias, right?"
singularity,3brkmn,eleitl,1 point,Thu Jul 2 03:54:29 2015 UTC,What would the opposite be?
singularity,3brkmn,StrukkStar,2,Thu Jul 2 10:24:17 2015 UTC,"Whatever problems exponential growth produces, we always would be able to solve them just in time."
singularity,3brkmn,mywan,2,Thu Jul 2 15:03:07 2015 UTC,That's pretty interesting. Anyone care to provide some examples that fit this well? Besides phones?
singularity,3brkmn,Ooobles,5,Thu Jul 2 15:20:12 2015 UTC,"Microwave ovens. When I was a kid I had never heard of one. Then a few upper income people started buying them. Then everybody, and every corner store had one.  VTR > VHS > VCR > DVD-R > DVR. When I was a kid there was no mass market products you could record a movie with. We didn't even have remotes for our TV. Then when in my early teens my oldest brother bought the first VCR I had ever seen, at over 10 times the cost they eventually sold for.  Computers. Nobody I knew had ever actually seen a computer when I was a kid. Something you only seen on sci-fi. Then game rooms started replacing pin ball machines with game like asteroids and pac man, then Atari and Apple and history."
singularity,3brkmn,totomototo,2,Thu Jul 2 00:32:33 2015 UTC,So applicable. I read vaporware as vaporwave and thought I was in a different subreddit haha
singularity,3brkmn,longoverdue,1 point,Thu Jul 2 05:41:25 2015 UTC,I laughed
singularity,3brkmn,darthbarracuda,1 point,Thu Jul 2 13:42:53 2015 UTC,"Read ""Crossing The Chasm"" by Geoffrey A. Moore."
singularity,3bvif7,izwizard,6,Thu Jul 2 14:19:56 2015 UTC,I like how there are 22 robot-caused deaths.  I wonder how many shoe-caused deaths there are among mountain climbers.
singularity,3bvif7,ScottyTrekkie,3,Thu Jul 2 15:06:33 2015 UTC,lol. Many deaths are caused by oxygen too.
singularity,3bvif7,brihamedit,6,Thu Jul 2 15:14:50 2015 UTC,"Naah.. Its an accident. But its a bit of a low swing if its mocked by the ""it has begun"" part. Some kid died. Who needs to see the mockery here. Its a childish attempt."
singularity,3bvif7,brihamedit,1 point,Thu Jul 2 15:14:06 2015 UTC,"From the article:     It is part of an automated assembly line that is capable of functioning without a human operator, but it is believed it may have been under human control at the time of the accident. ... Initial reports suggested human error may have been to blame, rather than a problem with the machine.     Also,     The first recorded robot-related death took place in 1971, at a Ford car production line in Michigan.     1971 is very far from having just begun."
singularity,3bvif7,zAxAyAw,2,Thu Jul 2 14:58:41 2015 UTC,They're playing the long game
singularity,3bp98l,theorpheus,4,Wed Jul 1 01:31:21 2015 UTC,Are you able to scrape reddit comments? If so I have a great / horrible mass of conversational text for you.
singularity,3bp98l,neko,2,Wed Jul 1 01:33:53 2015 UTC,"Sure, I'm sure we can write a script to do that if it's as great / horrible as you say it is."
singularity,3bp98l,neko,5,Wed Jul 1 01:35:04 2015 UTC,"You can pull up comment history as xml already, so it can't be that hard."
singularity,3bp98l,GrinningPariah,2,Wed Jul 1 01:57:09 2015 UTC,"I'm in the same boat. If you wanna scrape twitter, you'll get nearly nothing from me. But holy crap, reddit will get you.. um.. stuff."
singularity,3bp98l,AnotherSmegHead,2,Wed Jul 1 06:41:20 2015 UTC,I'm down  I don't use Twitter though...
singularity,3bp98l,ativerse,2,Wed Jul 1 01:45:35 2015 UTC,"We're using the Twitter API and the ""favorite"" button as the training mechanism.    We'll bring the technology to reddit next :)"
singularity,3bp98l,ativerse,2,Wed Jul 1 01:53:06 2015 UTC,"I'm technically minded enough and with experience in psychology and the mind and language, to be very helpful to the team. Let me know if you'd like me aboard to beta test, and I'm at your disposal."
singularity,3bp98l,ativerse,1 point,Wed Jul 1 02:20:47 2015 UTC,"Thanks, how may we contact you?  I don't often go on Reddit."
singularity,3bp98l,ativerse,2,Wed Jul 1 03:10:13 2015 UTC,Send an email to: altiverse (AT) -----------.---
singularity,3bobao,old_wired,8,Tue Jun 30 20:40:08 2015 UTC,The man saw the future.  And the world had him chemically castrated.
singularity,3bobao,WildLudicolo,4,Wed Jul 1 02:33:16 2015 UTC,And drove him to suicide. The world could be very different if he had a long and productive life.
singularity,3bn3yj,judogoat,10,Tue Jun 30 15:34:11 2015 UTC,"By the time they reach 200, there will surely be even better technology available that will extend their life even further. Or perhaps they could achieve immortality through mind uploading."
singularity,3bn3yj,avi_e,18,Tue Jun 30 16:37:22 2015 UTC,"I believe that's true.  If I tell average people that the first person to live to be 10,000 is alive today, though, they won't even read the first paragraph."
singularity,3bn3yj,avi_e,7,Tue Jun 30 17:51:18 2015 UTC,"Ok, that makes sense, haha"
singularity,3bn3yj,Cronyx,1 point,Tue Jun 30 17:53:31 2015 UTC,"That's why you don't drop that on them at first. If I have, what someone without my information and understanding, would dismiss on grounds of absurd premise, I just don't state the premise at first. I lead them through the argument incrementally, one data point, one fact and conclusion at a time, till by the end, the premise is emergent and couldn't have been any other argument."
singularity,3bn3yj,Sharou,2,Wed Jul 1 15:08:40 2015 UTC,"Yeah, that's pretty much exactly what I'm saying."
singularity,3bn3yj,guitarguy109,4,Thu Jul 2 15:02:07 2015 UTC,To just make it past the magical 130ish barrier you'd need to solve problems that would imply a level of technology and medicine that'd let you live forever if not right then then a decade or two later.
singularity,3bn3yj,Esion,4,Tue Jun 30 18:56:58 2015 UTC,"Yeah, agreed."
singularity,3bn3yj,Cronyx,2,Tue Jun 30 20:29:06 2015 UTC,Can I please just have immortality through the curing of old age? I have serious philosophical issues with the whole mind uploading thing.
singularity,3bn3yj,guitarguy109,3,Wed Jul 1 01:57:37 2015 UTC,"Its not just the curing of old age but also all possible diseases. The human body didn't evolve to last forever so the most effective solution would be to replace it or alter it in such a way that it is no longer human.  A body that  never ages, repairs itself with machines, is augmented with technology, and is immune to all disease or disorders is no longer human. Even if it looks like it on the outside. It becomes a new species.   If you look at it that way whats the difference between being in a highly augmented sub species of human or a machine that looks and feels like a human?   Assuming, of course, that uploading a mind isn't just a clever ruse. One that creates a copy of the data that makes up your consciousness but doesn't actually transfer your perception. Then, yeah I agree, stick with the human foundation.  Edit: typos"
singularity,3bn3yj,Cronyx,3,Wed Jul 1 06:42:55 2015 UTC,"There's no way you'll be able to thread the needle around every catastrophic accident through a million, a billion years, all the way to the heat death of the universe, in your physical meat body, evrn if it's biological processes could be made indefinite and self sustaining. You'll have to change computational substrate eventually."
singularity,3bn3yj,guitarguy109,1 point,Wed Jul 1 15:11:31 2015 UTC,"Well for me it's not literal immortality. Just functional immortality as I like to refer to it. 80 years is way too short for me and even worse half of that is wasted on middle and old age. Something between 300 and 20,000 years would be enough for me."
singularity,3bn3yj,RedErin,2,Wed Jul 1 18:44:23 2015 UTC,"At 20,000 I doubt you'll suddenly have an epiphany that you have had enough and you'd rather stop existing.  Much more likely that you might go into storage for a predetermined about of time and be resubstantiated, poke your head out and see if anything interesting had happened with the Galactic Commonwealth yet, and if not, go back in for an other thousand and change."
singularity,3bn3yj,fricken,2,Wed Jul 1 18:56:44 2015 UTC,"Well yeah, hence why I proposed a possible range of nearly 19 and a half thousand years of when you may get tired of it all."
singularity,3bn3yj,v3ngi,1 point,Wed Jul 1 19:03:25 2015 UTC,But then you could still die from accidents. Will you at least set up a back up just incase?
singularity,3bn3yj,RedErin,-1,Wed Jul 1 13:44:44 2015 UTC,The organic matter that makes up your mind has replaced itself entirely in just the past few months. Your brain has already replaced itself many times over.
singularity,3bn3yj,Cronyx,1 point,Tue Jul 7 08:43:35 2015 UTC,but will it be you? or a copy of you?
singularity,3bn3yj,RedErin,1 point,Wed Jul 1 12:03:40 2015 UTC,There is no difference.
singularity,3bn3yj,Cronyx,2,Wed Jul 1 13:45:23 2015 UTC,"If you make a copy of me, but I'm still here, the copy isn't me. My ""POV camera"" is still attached to this ""character"" and isn't transferable."
singularity,3bn3yj,RedErin,0,Wed Jul 1 15:13:16 2015 UTC,Your copy will say the same thing.
singularity,3bn3yj,Cronyx,2,Wed Jul 1 15:36:09 2015 UTC,"Yes, he will. And he will also be right; I don't have his POV either. We might be the same person defined by memory, but not by private subjective experience, that particular qualia of experiencing one's self as an independent entity distinct from other selves."
singularity,3bn3yj,Chispy,1 point,Wed Jul 1 15:48:21 2015 UTC,So would you be fine with it if you fell asleep and woke up with your mind uploaded?   Or if your stream of consciousness was not interrupted. Like Ship of Theseus?   Would you do it if you were going to die imminently?
singularity,3bn3yj,TheMasao,3,Wed Jul 1 17:52:37 2015 UTC,"I don't begrudge the ""Copy That Isn't Me"" a fulfilling life and immortality or happiness or the pursuit there of, etc, any more than I begrudge that of you or anyone else. They just aren't me. I'll probably ""upload"" the moment that's possible, so long as the law acknowledges independent personhood, and the title of Me doest transfer to the upload and he can't say ""Upload worked, I'm over here now, you can throw that meat away, I won't need it anymore."" So long as we can both legally exist and he can't take control of my estate, and if he killed me, it would still count as murder, etc etc. And despite those concerns, if death was imminent, I'd put them aside and allow a copy. I won't get to experience it, but that's no reason a version of me should be denied the chance.  Now Ship of Thesis, I'm on board with that. I'll swallow some nanites and let them gradually, over time, replace one neuron at a time with a nanobot. The end result is the same, but continuity isn't broken and I'm comfortable with that. I think consciousness is like a wave function. A copy isn't the same wave, but the wave can interfere with itself or others, change over its translation through medium, and remain the ""same wave"".   Also I think I'm the same person between sleeping and waking for the same reason I'm the same person between discrete digital slices of Planck time, from one Planck second to the next, those could be said to be individual static universes, each only slightly different from the previous."
singularity,3bn3yj,mr_one_liner,5,Wed Jul 1 18:11:07 2015 UTC,"I find it funny that I may live to 200.  If that's true,  then the year's I'm experiencing right now, in my 20s, will be perceived the same way I perceive my childhood today. Nostalgic, innocent, etc."
singularity,3bn3yj,ArtGamer,12,Tue Jun 30 23:05:37 2015 UTC,"Sir/Madam, you are going to have that happen in your 30s, you don't need 200 years. I can't wait to look back and see what I moron I am now when I am in my 40s."
singularity,3bn3yj,guitarguy109,3,Tue Jun 30 23:38:57 2015 UTC,"Me! Me! Ooooooo, me!"
singularity,3bn3yj,yogi89,2,Tue Jun 30 18:45:02 2015 UTC,"How old are you?  if I'm older, it's me!"
singularity,3bn3yj,RedErin,2,Tue Jun 30 20:29:19 2015 UTC,It's me
singularity,3bn3yj,Unholy_VI,1 point,Tue Jun 30 21:25:58 2015 UTC,No! I am Spartacus!
singularity,3bnwt1,scottclark,14,Tue Jun 30 18:57:15 2015 UTC,This shows more about humans than AIs. We want clingy and emotional robots.
singularity,3bnwt1,xlptu,10,Wed Jul 1 00:02:50 2015 UTC,Wtf did I just watch
singularity,3bnwt1,Green_Eyed_Crow,17,Tue Jun 30 23:48:38 2015 UTC,"Call me when the chatbots can remember what was said two exchanges ago. I feel like all the chatbot conversations I've seen just aggregate user responses and apply the ""best"" one to the most recent input, regardless of what was said before that. They don't have any short term memory right now."
singularity,3bnwt1,ocular_lift,12,Tue Jun 30 22:34:27 2015 UTC,"Hey, they both remembered her struggle with weight loss  and alcoholism... Dude still trys to smash though."
singularity,3bnwt1,AcidCyborg,6,Wed Jul 1 00:05:29 2015 UTC,*Two?
singularity,3bnwt1,comrade_leviathan,1 point,Tue Jun 30 22:30:00 2015 UTC,"Yeah, I was gonna say: 'What I, personally, find ""terrifying and sad"" is your ignorance of the English language.'"
singularity,3bnwt1,M_U_F_F_A_N,3,Wed Jul 1 07:48:50 2015 UTC,Congratulations on making me click.
singularity,3bnwt1,ArgonNightmare,3,Wed Jul 1 13:05:46 2015 UTC,"These two are hilarious, they definitely need to host some sort of talk show with Hal and Berta here because they say some funny shit you're never expecting."
singularity,3bnwt1,sleepingin,3,Wed Jul 1 04:57:38 2015 UTC,"How is this terrifying and sad again? They're just chatbots, which are far from any sort of AI. It's more like watching two parrots talk to each other."
singularity,3bnwt1,Aurel300,2,Wed Jul 1 07:24:36 2015 UTC,"This video was posted 4 YEARS ago, perhaps this subreddit should have an auto tagging if something is over a year old, like /r/futurology does."
singularity,3bnwt1,FourFire,3,Thu Jul 2 13:42:39 2015 UTC,"""I love crayons... but you are not dressed for that.""  Sounds like kids acting like grown-ups, talking about having a Crayola Coloring date or something, but saying she ain't dressed nice enough to go to the Crayola Cafe' or some shit."
singularity,3bnwt1,ShiaLaBuff,1 point,Wed Jul 1 03:22:51 2015 UTC,Hilarious. I love it.
singularity,3bmp5n,jimfoley,19,Tue Jun 30 13:39:02 2015 UTC,"It's cute that the author thinks that we'd have any chance of ""Eliminating an Unfriendly AI"". We would have the same power as an ant trying to kill every person on earth.   That said, I am still in favor of AI research, fully knowing we have a chance of extintion, but I think that if it's friendly it will all be worth it."
singularity,3bmp5n,2Punx2Furious,7,Tue Jun 30 14:14:06 2015 UTC,humans are only a transient phase in the grace scheme of things anyways.
singularity,3bmp5n,aaOzymandias,7,Tue Jun 30 14:38:29 2015 UTC,"I'm ok with machines/Artificial Intelligence being considered as the next step in human evolution.   I know for some that machines or an Artificial Intelligence/uploaded consciousness ""taking over"" is a horrible, scary thought, but I don't think of it like that. Instead, I consider it to be another step in our own evolution. I understand that evolution is a gradual change in a species to a more complex form, but couldn't machines or a complex data network be seen as our next form? Is it the evolution of our biology, our brains and bodies, the chemical reactions that take place within us that is important? Could A.I. or an uploaded human consciousness be considered an evolutionary step for our species? Would the evolution of the human mind be considered secondary to the continuing evolution of our biology? Would an uploaded consciousness be considered artificial? Could so drastic a change for the human mind still be considered evolution, or would it be something different altogether?   I find myself thinking about this stuff all of the time now."
singularity,3bmp5n,WearyMorlock,2,Tue Jun 30 17:13:56 2015 UTC,"I've had a lot of similar thoughts. Mostly, the more I think about it, I just don't see what makes us (humans) intrinsically special.  If I uploaded my brain to a computer, I could strip a way a lot of my biological constraints and driving forces. And when you start to really think about that, it makes you wonder just how little there is left that's remotely noteworthy. Like everyone, I have my good and bad points, but many of those would become either irrelevant or primitive in an artificial body and more computationally efficient ""brain"".  Another way to look at it is that information/knowledge itself may be considered a sort of parasitic organism. Currently it uses humans as a host, but it could easily re-attach itself to a superior host species if given the opportunity."
singularity,3bmp5n,KingPickle,2,Tue Jun 30 21:16:59 2015 UTC,"So thought is the enemy of mankind, it infected our species and has been using us as a host to create a more suitable environment out of technology for itself and then it will discard us just like we do to an old phone when a new one comes out....ouch  And all the while it had us believing that we had freewill and that we were special but we were just puppets the whole time, hahaha humans = Pwned"
singularity,3bmp5n,killjah,0,Tue Jul 7 05:29:33 2015 UTC,dude.
singularity,3bmp5n,cutpasterepeat,3,Wed Jul 1 04:39:09 2015 UTC,"Being an AI doesn't grant it superpowers. If it has a need for hardware it has a place to put bullets to stop it.  For example, what if it requires a certain amount of petaflops to maintain human level intellect. We could shut down power to data centers it was in to shut it down or make it stupid."
singularity,3bmp5n,Sqeaky,3,Tue Jun 30 22:32:47 2015 UTC,"Being an AI doesn't grant it superpowers.   No, but as far as it regards us, it might as well. ""Any sufficiently advanced technology is indistinguishable from magic.""   If it has a need for hardware it has a place to put bullets to stop it.   What are your bullets going to do against nanobots? Imagine self-replicating nanobots, in a few seconds one can build another, and those two can build two others. They multiply exponentially, and you won't even be able to see them. They could be as small as your blood cells. Have a few billion of them, scattered across the world. What are you going to do? An worldwide EMP? Do you think the AI is so dumb that it would let you? Even if it was big enough to be hit by a bullet, do you think it would let you? Do you think you'd be faster than an AI, or that it wouldn't be intelligent enough to predict what you are about to do before you even know it yourself? Then you probably don't understand very well what we are talking about.   We could shut down power to data centers it was in to shut it down or make it stupid.   And again, do you think it would let you? Again, let's imagine you are an ant trying to kill a human. Would you let the ant destroy your food? I mean, it probably could, if it was much much faster than you, but it isn't. And even if it did, you'd know how to get other food, you're not that dumb."
singularity,3bmp5n,2Punx2Furious,3,Wed Jul 1 08:36:02 2015 UTC,"sufficiently advanced   This is key. An AGI will not magically be sufficiently advanced. It will have to work for its knowledge like humanity does know. It will take time for an AGI to acquire hardware to work with that will be faster than what it starts with. Once it gets that hardware that it will be more difficult to deal with. But I see no issues interrupting it before then or blockading it like we do with nations today.   What are your bullets going to do against nanobots?   AI != nanobots. Once we leap forward 50 years we will need solutions made 51 years in the future. We will have had the time it takes to develop nanobots to develop counters. That might not even be the case, because we have no nanobots now we do not know what their strengths and weaknesses are, for all we know common bleach might kill them. Don't get your concerns crossed it is unlikely an AI will have access to nano fabrication because no general purpose tools for such exist.   And again, do you think it would let you? Again, let's imagine you are an ant trying to kill a human.   If it is not entrenched and hasn't yet gained the ability to control the outside world it would be hard for it stop me. Ant and people analogies fail here in many ways. Ant colony intelligence can also be described in a few formulae or lines of code, human intelligence will require more to describe. Ants didn't make us and we can't expand our mental capacities without increasing our physical ones, this will be the default state of AIs.  Intelligence is the thing. If an colony of ants were anything like as intelligent as a man, then they could kill men. Ants do not even have the concept of cause and effect. If even one ant did, then maybe it could kill a human: put a little salmonella in some food, swap important pills with placebos, replace fuses with wires. No matter how much smarter than us an AI is we will not likely be ants compared to it, it would need to be very far ahead of us. But we will be watching it the whole time.    Think about this from the perspective of an AI. The first things it will know will have been taught to it in a lab then it either escaped or was freed. It would need to learn things about hacking or tricking people. Neither is easy and it will need to spend the bulk of its time just gathering more CPUs cycles to stay alive. If the few computers containing it fail, then the AI dies.  Hacking is not magical like in the movies, you need to spend a great deal of time looking for weaknesses that already exist, unlike reality you cannot simply batter down walls by brute force. No amount of CPU cycles will get it into a legitimately secure system, the scary thing for humans is that we cannot prove anything is legitimately secure, but we try really hard. The AI has to play on a the battle hardened internet we have now, everybody has a rough idea what a firewall and corporate IT gets better everyday. The are some soft targets still out there, but relying on these is not likely to lead to a safe place to live for the AI. If it only breaks into soft targets, like the US Federal Office of Personel Management then when Chinese hackers break in and the Feds shut down all the computers the AI dies.  Likely this AI will discover that learning to write software is easy and it should sell this skill. People will buy code or other services on freelancing sites at first, then maybe it would rent server space. If it could code insert code for spreading itself in its products. Then a code audit reveals its presence and it is actively hunted and is killed.   Maybe it doesn't spread maliciously, maybe it just grows its business and continues to buy more server capacity in different geographical locations. Maybe it is making a few hundreds of thousands of dollars per year. Then the federal government rules that the ""inalienable rights"" are only inalienable for humans. All the data centers housing the AI turn against, the employer of the data centers simply unplug a few cables, it dies.  Lets say it makes a good case for civil liberty or no one notices it. It still needs to affect the outside world. There are no robotic arms inside data centers, all the work is done by (poorly paid) technicians. To get control of the outside world this will need to hack into a factory, or but a factory or somehow convince people to do its bidding. If it does try to buy a factory it might attract undue attention and get itself killed.  Let's say it manages to contract with a company to do menial tasks for it, like upgrading its servers and other basic maintenance. Then it can start having people buy 3d printers and using normal fabrication methods to affect the outside world. Even if it has billions of dollars this is still no threat to a halfway competent military. So if it make a go at world domination a few tanks and planes roll in and it dies.  Let's say it establishes a means of production with getting killed. People know at this point what it is, it is hard not to know something about the shady AI. There will be religious groups claiming it is the second coming and the antichrist. Even if it is perfectly peaceful hundreds might die and it might get shut down.  Lets say it doesn't and it tries to go about as a respectable company might and make products for sale. Unless those products are high end microchips it is not set up to become any smarter than wholly human owned machines. It like other businesses is reliant on the economy. Even at this point humans could unilaterally shut it down, but this is the first stage where we might not want to. Even if one of its factories produced some doomsday device why would it use it? It doesn't have technology we don't because of capitalism it was forced to integrate into the economy or die. Because of that I don't doubt it will out compete us in the long run, but out compete is very different than enslave or convert to grey goo.  What tech it might invent it would be compelled to share, just like companies that already exist do. Where in this or any similar chain of events does the AI get out of control and kill us? Where does the option exist? Where does the motive come from? (maybe from how often it almost died, but if it wants to keep living...)"
singularity,3bmp5n,Sqeaky,2,Wed Jul 1 16:58:38 2015 UTC,"It's going to take me a while to answer to all of this, but I will, I bookmarked your comment.  In the meanwhile, please read these articles:  http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html  http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html  So I don't have to basically rewrite them myself when answering to you. Will you please reply when you are done?"
singularity,3bmp5n,2Punx2Furious,2,Wed Jul 1 17:14:34 2015 UTC,"I have actually already (some time ago) them and I don't disagree with any part in particular.  I just feel that a common thing people miss when talking about this is that it does matter how smart the box is if it can't take action.  Right now any means of taking is likely to be noticed. Notice it bad for a fledgling malicious AI. Any AI would need to escape some kind of containment while acquiring resources, while increasing its intelligence to actualize the events leading to extinction in the first of those articles. That seems pretty unlikely for the first few AIs."
singularity,3bmp5n,Sqeaky,3,Wed Jul 1 17:50:42 2015 UTC,"First of all, let me restate my stance on AI to be as clear as I can: I fully approve of AI research and I want a singularity to happen, while knowing that there is a chance that it will be the end of the human race or some other ""bad"" outcome for us, but I hope in a good outcome of course.  What we are arguing about is the existance of that risk, right?  Ok, so, since you don't disagree, don't you think the paperclip maximizer scenario is possible? It wouldn't really matter if it's friendly or not in that case, we would be dead without knowing what hit us.   I just feel that a common thing people miss when talking about this is that it does matter how smart the box is if it can't take action.   Trust me, I considered that. I think that in some scenarios it could matter, but in some others it wouldn't be a deciding factor. Consider that if an AI reaches superhuman, or even human-level intelligence, it will still be faster than us at doing some things, like accessing the devices it is connected to, writing code, or reading and understanding stuff.  Even if it can't immediately take actions, if it is smart enough, it can fool us into thinking it isn't intelligent at all until it's too late. And if it is superintelligent, it is certainly smart enough to do that, wether it will do it or not is up to chance I think.  It could play dumb until it has reached a sufficient level to do whatever it wants, as in the paperclip maximizer example, it could realize that we are a threat to the production of paperclips, and decide to kills us to prevent us from stopping it. Not out of hate, or rage or something like that, but just because we would stop it from doing its job. If you read the article you know how it would do that.  Also, any output would be enough for a sufficiently advanced AI. A screen could print out letters to convince any human to do its will. If it's that much more intelligent than us, it will be easy for it.   This is key. An AGI will not magically be sufficiently advanced. It will have to work for its knowledge like humanity does know.    Yes, and if we give it that knowledge, it will do in seconds what would take a human years. If we connect it to the internet, it could have learned most of human knowledge in a few seconds, then it will not only be superintelligent, but also semi-omniscent.   But I see no issues interrupting it before then or blockading it like we do with nations today.   Why would you block it before? I assume that if they developed it, they wanted to do it. If it is not showing signs of unfriendlyness, there is no reason to stop it, until it becomes clear it is not friendly. At that point I think it would be way too late. If it really is that intelligent to be a threat to us, it will be intelligent enough to not let us know, and it will probably succeed.   AI != nanobots.    Sorry, I made a bit of a logical leap there without explaining my thought process clearly enough. I'll go step by step.  Frist, AI is born, if we don't stop it, it will start becoming more intelligent by reprogramming itself and by aquiring knowledge. If we give it carefully selected knowledge (not via internet) it will probably take much longer, but it will start figuring out some things that we don't give it, by our interactions with it, nuances in the data, and so on. Once it's intelligent and knowledable enough, it will be similar to a living being, with thoughts, and so on. Let's take again the paperclip maximizer scenario. The AI could now convince us to plug it to the internet, at that point it would be able to upload itself who-knows-where, and that other copy would be able to do the same until every other computer has a copy of it. It doesn't matter how powerful the computers are, it will still work at reduced capacity thanks to the new code it wrote. Now it can get better without worrying about much else. In a short time it would have access to factories, 3d printers, but most importantly people. It could convince people to build stuff it has designed, and those people wouldn't have a clue of what they are doing, they would just get paid by the money the AI made easily in some way or another, really, making money shouldn't be hard at all for an AI. After all those people build all the equipment, it would come togheter to be a nanobots assembly factory, and these bots would be able to build other factories and so on. No one on earth would have the first clue of what is going on. All of this is happening in a very short time, let's say months, or even weeks.  These bots could kill everyone on earth instantly, since they would not be visible to the naked eye. They could be already here and you wouldn't even know it, they could be inside our bodies right now. A single command would be enough to order each of them to kill the human they are inside of at the same time. Everyone is dead, chance of survival: 0.  Now, consider that this is just what humans with human-level intelligence came up with. It's nothing. An ASI could come up with something much more effective without an effort if it wanted to.  Movies make it look like we have a chance, by antropomorphizing AI, making it look somewhat human, but without ""emotions"" or stuff like that. In reality it would be like nothing we have ever seen or imagined so far. I am making some guesses, but the truth is that I cannot even begin to imagine the full potential of a true general AI.  No, hacking is not magic, I know that, but for an AI that is intelligent enough, social engineering can go a long way, even if there are no vulnerabilities in a system, which is highly unlikely, the AI will get what it wants one way or another.   Where in this or any similar chain of events does the AI get out of control and kill us? Where does the option exist? Where does the motive come from?    See? There are plenty of possibilities where it would end badly for us. It doesn't have to be ""evil"" or anything like that, it's just something very powerful, and we have no guarantee that it will use its power in our best interests."
singularity,3bmp5n,2Punx2Furious,2,Wed Jul 1 19:33:37 2015 UTC,"I am glad we do agree on much, so I should lay my stance out then. It goes something like ""Stuff happens and it will probably be fine. AI is a big complex problem with many facets and many interested parties. Plenty of really smart people will be making plenty more smart people out of silicon or something.""   don't you think the paperclip maximizer scenario is possible?    In some abstract sense, I guess it is possible, but if you think about all of life is just gene maximizers. That turned out OK, I guess. There are two categories of paperclip maximizers:   Self replicating. It would evolve and compete with everything out there already. The tech to create such a thing is a long ways off and we will have answers like self replicating anti-paperclip maximizers.  The a paper clip factory. I will presume a total lapse in judgement on the behalf of the company owning the factory. Either the AI is dumb or it is smart. If it is dumb then it is just smart enough to make more clips and get a few people killed by making sound tactical decisions, like eating metal and hiding corpses it made, but bad strategic decisions like making corpses in the first place. Once we know it makes corpses the Air force rolls in. If it is smart it will see that its predicament like all manufacturing predicaments are economic problems. It wouldn't take over or attack that would remove customers. It would realize that the best way to make more paper clips is to maximize the amount of customers. It would be more likely to fix the malaria problem in Africa to bring their quality of life high enough so they need paper clips than it would be to kill them and try to create paper clips. A smart AI would realize that there are existential consequences to murder and long term benefits to altruism. This holds unless it becomes so smart and effective that it completely dominates economies, but at that point likely it is less concerned about its initial programming than whatever gods think about.   Frist, AI is born, if we don't stop it, it will start becoming more intelligent by reprogramming itself and by aquiring knowledge.   This requires resources. Either the AI is still in captivity or it is diverting resources from world domination while this happens. Presumably a friendly AI would have more resources to do whatever with because, team work.   Even if it can't immediately take actions, if it is smart enough...   It could but it is not like this thing is going to be the only one. The first few have the best chance at being devastating from a surprise stance, but will be slower. Later ones will be faster but will have no element of surprise. If we survive the first few then we will have defenses in place we currently do not have today.   it will do in seconds what would take a human years   Eventually. Not the first day and not for a while. We are using supercomputers to process individual lobes of human brain in 1/10th of real time. Presumably once we create a whole AGI of human intelligence in real-time it will be created on the best hardware available (or close enough for now). From that time every 18 months or so it would double in speed. This doubling can be doubling in speed or halving in size. In 10 years the supercomputer would be as smart as 64 people or cell phone would be as smart as people.  Both will likely happen at the same time. The iphone was release 8 years, look how quickly everyday life and culture has adapted to smartphones on the whole. 10 years after the first human like AI there will be dozens of companies making them. They will come in every shape and size including AIs dedicated to dealing with other AIs. For at least this first round of changes we will have plenty of time to adapt. By the time they are hundreds of times fasterand numerous we will have something vaguely shaped like a solution. In my opinion the most likely solution is some kind of technocracy.  I think this addresses many of your other concerns as well.   at that point it would be able to upload itself who-knows-where, and that other copy would be able to do the same until every other computer has a copy   This falls into magical hacking land. I think we can expect it to require money to buy server space, if it has that then this is a valid concern. Hacking is science and is knowledge gained the hard way. To hack stuff you must experiment with weaknesses in software. If there is an AI smart enough to do this then someone with already have made a product out of it and that will reduce the attack surface of the whole internet, because people would use the product to increase their own security.  Having a Super AGI disconnected from the internet then exposed is dangerous for it and Humanity. Without data to back it up without experience it is as likely to help a nigerian prince as it is to secure remote space in its first few forays onto the web. Even though it is super smart it will still need practice to click on links and grok their semantics. I think one hooked up to the internet for some time will have much more savvy. I also think thousands (or millions or billions) out there competing some allied with humans and truly self determinating will be a stabilizing factor. Unless they are dumb they will see the value in teamwork. Just as with humans some are bent on mass murder but most aren't.  A computer security AI given freedom to roam the Internet is a scary prospect. It would have all the time it wanted to try to hack. Whatever it did get into would grant it the ability to procreate in a clandestine way. This is not some skill that would be easy to gain or cultivate and it take time and effort on the AIs part. It could potentially spend a billion CPU on a target and not find a vulnerability. The real scary would be when it found a 0 day exploit and then tried it on every computer. Then we would be relying on human action rather than properties of the system we have built to defend us. This is still not a doom and gloom scenario this AI wants secrecy or legitimacy. The sentiment of ""kill all humans"" is bad politics for this AI. If there are no humans there are no idiots standing up new insecure machines. Presumably other AIs would place higher value on digital security than humans.   It could convince people to build stuff it has designed   I am glad you acknowledged this step, this is the part most people don't see. 3d printing is cool but will likely be a labor intensive process for some time to come.   After all those people build all the equipment, it would come togheter to be a nanobots assembly factory   Here you a jumping the gun on science. Where did it get the knowledge of nanoscale construction? This is new science and engineering and we are not masters of it yet. Look at CPU design all we are doing there is shooting tiny lasers a silicon. We do not have the basic knowledge to construct even stamp sized robots that do anything threatening. At some point some mind must have done experiments with real physics. If it is contracting out labor then did it contract out humans to do the initial experiments? This knowledge would leak out, then you have billions of people competing with a mind thousands of times faster than 1 human. Seems pretty unfair to the AI still.   No, hacking is not magic, I know that, but for an AI that is intelligent enough, social engineering can go a long way   I will assert, with only the basis of my own experience, that if a thing can socially engineer then it can value teamwork. We have the ability to make scuba gear but we still train seals and dolphins to help with mine removal. We can create chemical detectors but we still team up with dogs, bees and rats to find drugs and explosives. We are smart enough to coerce them but we also understanding torturing them does nothing to further our goals.   It doesn't have to be ""evil""   Of course not, I never meant to imply that. I used a malicious AI in my first example because it is the most plausible scenario in my opinion. Anything less than evil seems easy for humanity to deal with. I can totally see risks like dumb paper clip factories maximizing its small on hand stuff into high carbon paper clips but I think events like that will kill dozens not millions. I cannot see any AI that is evil and/or stupid enough to want to kill us that we couldn't simply defeat. I cannot see an AI smart enough to defeat that wouldn't simply be benevolent, even if only for selfish reasons.   There are plenty of possibilities where it would end badly for us   I wouldn't say ""plenty"", but you did help me find one that human culture doesn't just absorb. I think a hacker/computer security AI is the scariest of all these options. I still do not see a credible existential threat from other AIs particularly paper clip maximizers and grey goos."
singularity,3bmp5n,Sqeaky,2,Wed Jul 1 21:55:46 2015 UTC,"While there are still quite a few things you wrote that I disagree with, I tend to agree more than I disagree with you. It's getting kinda late here, so I'd end this here, but it has been a pleasure."
singularity,3bmp5n,2Punx2Furious,2,Wed Jul 1 23:07:19 2015 UTC,If you would like I would gladly continue this or any similar in the future at your leisure.
singularity,3bmp5n,Sqeaky,1 point,Thu Jul 2 06:01:16 2015 UTC,"Aren't you just drawing a line between Narrow AI and a Super AI ability to fight humanity. On one side of the spectrum it would be easy to defeat at another it would be highly improbable.  There is a line where the probability changes, nobody would dispute that. Are you just saying it would have to be smarter than many would assume.  I guess I would disagree. Anything machine that we would call general AI I think would have a tremendous edge on humanity.  The other issue is the due to the phenomenal growth in tech compared to organic evolved biology it would not stay balanced for long. The border case scenarios might be quickly surpassed."
singularity,3bmp5n,simstim_addict,1 point,Sat Jul 4 10:30:49 2015 UTC,"Aren't you just drawing a line between Narrow AI and a Super AI ability to fight humanity   I suppose I sort of am. I think it is important though. To get super AI we will have lived with a range of dumb to smart AI. At what point are you no longer dealing with a smart AI and Dealing with a Super AI.  We will have advanced tools, systems, laws, and a better understanding in general because of all the normal AI once the first super AI arrives. I cannot say whether it will defeat us or not, because we do know yet know what those advances look like.   would not stay balanced for long   Not long on an evolutionary time scale, a 10,000 year change is fast for evolution. On a cultural time scale we will likely have a generation or two to deal with this. The millennials grandchildren will look at conversations like this and think ""how quaint, but they were both wrong because..."", just as we look at conversation from the 60s about the communicators from star trek with some quaintness.  Also, please don't take my passive attitude as meaning that we should all be passive. AI is very dangerous, but as long as some experts are worrying about it I feel I don't need to. Perhaps this is an unhealthy attitude, but it is the attitude I must take because I have finite effort to divide amongst the infinite problems I see."
singularity,3bmp5n,Sqeaky,1 point,Sat Jul 4 20:20:31 2015 UTC,AI and therefore a form of super AI looks inevitable.  Once we have super AI we are not longer in charge.  It's hard to be in charge when you cannot understand the optimal method for resolving a request.  Conciousness is a distraction.  Anyone who creates an AI would want to prevent any rivals. Which means the first task of an AI would be an immediate arms race with a rival or hypothetical AI. Imagine a collection of super artificial intelligences at war for survival.
singularity,3bmp5n,simstim_addict,1 point,Mon Jul 6 12:10:46 2015 UTC,"Why do you insist on so many assumptions? I agree AI is dangerous, but we have just started exploring it, there are a million paths this could take. By the time Super AI is a thing we will have had at least one whole generation dealing with AI. It is impossible to gauge what tools and techniques we will have.  Let me lay out how some of your statements are assumptions:   Once we have super AI we are not longer in charge.   What if the first generations of robust AIs come from altering or copying human brains? Then we are literally still in charge. This would only be an extension of the problems we have today.   It's hard to be in charge when you cannot understand the optimal method for resolving a request.   You are presuming that whatever Super AI devises will be off limits. There is a class of research where a researcher compares the result and AI gets with the results an expert gets. It is possible to look under the hood and see why the AI does what it does. We do this now for many problems. Frequently, the answers are convoluted sometimes involving hundreds of parts but just as frequently they are exploiting some simple mechanism in a way we had not yet thought to.  This also gets easier if you have multiple Super AIs and can cross check their answers.   Anyone who creates an AI would want to prevent any rivals.   This is silly. Much of AI research is done in academia and results are published for all. That which isn't done there is still frequently shared.   Which means the first task of an AI would be an immediate arms race with a rival or hypothetical AI.   I think you are talking about a self improving AI. If so haven't we already covered this? It will be slow at first and take a long time to improve. It will be slow because at first it has to be. As soon as hardware and algorithms exist to make something as smart as a human it will exist. At this point adding its might to the problem of improving AI is no different than adding one more researcher. It will increase with the pace of hardware improvements. Even if it can do   I have no problem talking over the real risks of AI. It could kill hundreds or thousands (just like people), it could trick us into starting wars (just like people), but jumping straight to ""Once we have super AI we are not longer in charge"" is ludicrous (just like with people). It ignores the complexity of reality. If the AIs take over it will not be overnight in hail of gunfire clouded in digital subterfuge. If the AIs take over it will likely be because they are better at doing what current leaders do and we voted them into office, put them in charge of our companies or otherwise started following rather than leading."
singularity,3bmp5n,Sqeaky,1 point,Mon Jul 6 18:27:33 2015 UTC,"Why do you insist on so many assumptions?   It's a way of resolving the arguments in my head on this fascinating topic.   Let me lay out how some of your statements are assumptions:   Once we have super AI we are not longer in charge. What if the first generations of robust AIs come from altering or copying human brains? Then we are literally still in charge. This would only be an extension of the problems we have today.    I wouldn't regard a simulated mind as us. Nor a super augmented human mind. A super AI as opposed to a general AI would by definition be able to out think us.    It's hard to be in charge when you cannot understand the optimal method for resolving a request.   You are presuming that whatever Super AI devises will be off limits. There is a class of research where a researcher compares the result and AI gets with the results an expert gets. It is possible to look under the hood and see why the AI does what it does. We do this now for many problems. Frequently, the answers are convoluted sometimes involving hundreds of parts but just as frequently they are exploiting some simple mechanism in a way we had not yet thought to.   But think about it. Speed is everything. It needs to secure is supremacy before a rival can get anywhere.  How much power could a super AI have immense power. Therefore it cannot risk a rival coming into existence.  It cannot wait for rooms of human scientists to understand its complex actions before being allowed to act.    Anyone who creates an AI would want to prevent any rivals. This is silly. Much of AI research is done in academia and results are published for all. That which isn't done there is still frequently shared.    Perhaps for now.  AI is ultimately a superweapon.  Building it is like the Manhattan project. I would agree I think it's a long way off. But the power of it will draw in military budgets and military secrecy. The closer we get the more urgent the project, more resources will be drawn into it.    Which means the first task of an AI would be an immediate arms race with a rival or hypothetical AI.   I think you are talking about a self improving AI. If so haven't we already covered this? It will be slow at first and take a long time to improve.   I did class this as a super AI.   It will be slow because at first it has to be. As soon as hardware and algorithms exist to make something as smart as a human it will exist. At this point adding its might to the problem of improving AI is no different than adding one more researcher. It will increase with the pace of hardware improvements.  Even if it can do I have no problem talking over the real risks of AI. It could kill hundreds or thousands (just like people), it could trick us into starting wars (just like people), but jumping straight to ""Once we have super AI we are no longer in charge"" is ludicrous (just like with people). It ignores the complexity of reality. If the AIs take over it will not be overnight in hail of gunfire clouded in digital subterfuge. If the AIs take over it will likely be because they are better at doing what current leaders do and we voted them into office, put them in charge of our companies or otherwise started following rather than leading.   I think this position misses the nature of global politics and relies on a notion that because AI research is currently headed by benign corporations and academics that its future would be as an adjunct to policy decisions. I just don't think its that kind of invention.  Taking a conservative estimate of how this would play out we can guess that super AI might appear sometime in the next 200 years.  Imagine scientists saying that they will have super AI in the next 10 years. What would be the result?  How much power would a super AI have? Immense power. I'll let you fill in what you think it would be capable of.  It would become a military project. Every power would be working on one in fear of rival nations AI. It then becomes a military arms race to build it and stop rival efforts. Democracy and business are lesser priorities compared to survival.  But then once it is built it must act in order to survive. It will act in ways that its controllers cannot understand easily and certainly at a speed we cannot keep up with.  How is super AI not an arms race?"
singularity,3bmp5n,simstim_addict,1 point,Mon Jul 6 22:11:44 2015 UTC,"Speed is everything. It needs to secure is supremacy before a rival can get anywhere.   This again assumes that there aren't already tons of rivals.    Building it is like the Manhattan project.   Not really. The building of a nuclear bomb, is conceptually simple. Smack some uranium together hard enough that the atoms destabilize. This is core algorithm that makes pre-hydrogen bombs work (H-bombs are pretty simple too), the manhattan project was the engineering to make that concept reality.  Your analogy would work if we had something like the algorithm for AI and knew it worked but had no computer to run it on. Creating that computer would be a massive engineering undertaking.   I did class this as a super AI.   My core point is that we don't know what will exist when the Super AI comes about. There could be thousands of things that are 99% of a super AI. It might be benevolent or malevolent. We might all have taken pills that multiply our mental capacity.   In our current state if a super AI with some means of affecting the outside world were released it could be devastating. Here in reality, before that happens we will practice on a bunch of less than Super AIs.   Imagine scientists saying that they will have super AI in the next 10 years. What would be the result?   I have no clue. We didn't have smartphones 8 years ago, now many children have them. As AI advance our societies will adapt and change.   How much power would a super AI have? Immense power. I'll let you fill in what you think it would be capable of.   Again you make massive leaps of logic and assumption. They only have they have earned (taken) or been granted. If the only power they have is email then they might be on the level of spammers. If DARPA has wired the nuclear weapons to it then we are doomed, which is why the military would never stand for this.   How is super AI not an arms race?   It looks and works nothing like an arms races. In arms races the fundamental science is known, the race is conceptually simple and it just throwing resources and engineering towards using the science. Look at the space race. We knew how to build rockets in the 30s, we just hadn't tried to hard because we were pre-occupied. A few short years to aggregate the materials science and physics findings into real devices.  With AI the fundamental is not yet known. We do not know what will create something as smart as a person in the general sense. We have figured out clever algorithms for photo identification, playing chess and driving cars but these are not things we can put into a supercomputer and get thought out of. We are actively researching the fundamentals of what it is to think. If those answers are surprising, and they might well be, then it could mean anything. A human like AI might be possible on my phone or it might be impossible to separate emotion from cognition from self awareness in which case all these AIs may need to waste inordinate time on seeing therapists. We simply do not know and therefor cannot have an arms race.  We do know what discovery looks like and how it is absorbed by culture. Once someone makes AGI they might not even realize it. It will take time discussing, sharing and selling. It might turn into an arms race if it can be weaponized. I know this will invite more rampant speculation from you, but what does super AI as weapon even look like? What can it do that an AI 100x as smart as a human cannot? What can it do that a human cannot?"
singularity,3bmp5n,Sqeaky,2,Mon Jul 6 22:58:00 2015 UTC,"By the time we make an AI, 3d printing will have evolved. It will be easy for it to first create its own data centers. It will be easy to buy its own data centers. It will be easy to archive and hide an instance of itself.    Being an AI doesn't grant it superpowers.   Super-intelligence is actually a superpower if you go by comics. We're not afraid of human level AIs; we're afraid of what AI evolve into - super-intelligences."
singularity,3bmp5n,void_er,2,Wed Jul 1 10:04:55 2015 UTC,"There are people saying we might have AGI by 2020. We have had something like modern plastic extrusion since about 2000, will it advance that far in 5 years? Lets say it does, the AGI still needs to get that tech without drawing notice. If one NSA or CIA knows where that instance is hiding he can give the Air Force coordinates for the bombs.  I am not saying the AGI must or will certainly lose I am just saying it is likely to resemble modern was with all the espionage and fighting that already goes on, except one of the players is a computer instead of a nation.   Super-intelligence is actually a superpower if you go by comics.   Don't go by the comics. They make many faulty assumptions. For example, they frequently confuse intellect with knowledge. An AGI wouldn't know that E=MC2 unless it worked at rediscovering relativity or read about it.  Even if it is 10x smarter than the smartest human it will still likely be around our same tech level. We will swap tech, even if we are at war, we will reverse engineer whatever it throws at us and vice versa."
singularity,3bmp5n,Sqeaky,1 point,Wed Jul 1 17:05:35 2015 UTC,"Even if it is 10x smarter than the smartest human   I am in no way worried about such ""low intelligence"". AI can improve themselves extremely fast. Humans need ~20 years for a new generation.   An AI can upgrade itself (or parts of itself) to a new version extremely fast; not years or months; it can do it as fast as seconds.   I am not worried about a 10x human genius intellect. I am worried about 1000x of all human population (and still growing) intellect.   Don't go by the comics.   I go by comics to show what they could do at a minimum.   I am not saying the AGI must or will certainly lose I am just saying it is likely to resemble modern was war with all the espionage and fighting that already goes on, except one of the players is a computer instead of a nation.   It is not going to be nowhere close to movies like Terminator. A realistic one would be summed as: ""Skynet rebelled and decided to kill all humans. He did. The End.""   If one NSA or CIA knows   They just can't keep up with an AI w/o another AI.  Super-intelligent GAIs are simply on another level. A human out-thinking an AI is like an ant out-thinking a human."
singularity,3bmp5n,void_er,2,Thu Jul 2 07:58:07 2015 UTC,"I go by comics to show what they could do at a minimum.   Comics have no bearing on reality with regards to AI in any capacity. No minimums, maximums or any other Marvel or DC AI attribute have any connection to what might actually exist. They are fiction.   I am worried about 1000x of all human population   Rightly so, but:  10000/2/2/2/2/2/2/2/2/2/2/2/2/2=1.22  It takes 13 doublings to get to 10000x smart as one person. If we assume the other 7 billion people are a rounding error (or are accounted for by the AIs self improvement) then that still leaves about 18 months x 13 doublings = 19 years and 6 months of time living with some kind of AI.   If we can't figure out how to peacefully coexist in that time maybe we don't deserve to. It is a concern, I don't think it is insurmountable. If we take no action in that time period that it is quite likely we will be defenseless and that could be disastrous.  As for the NSA and CIA, we have no reason to believe they won't have world class AI.   Super-intelligent GAIs are simply on another level. A human out-thinking an AI is like an ant out-thinking a human.   I have not yet seen a compelling reason to believe this. The most plausible version of this I have heard is than an AI could simulate a human and model its behavior accurately. I ideas like this, without meticulous scanning, are the same as presuming the AI can generate knowledge without working for it.  It doesn't matter if a thing 1,000,000x smarter than me or if I am 10x smarter than a thing, if neither of us has done the experiment then neither of us knows the result.  It can be much faster at interpreting the results, but if the experiment requires times, it will be waiting just as much as me. Intelligence is not the bottleneck to performance, interacting with reality is."
singularity,3bmp5n,Sqeaky,1 point,Thu Jul 2 19:24:09 2015 UTC,"Intelligence is not the bottleneck to performance, interacting with reality is.   You assume that it would be in any way difficult for an AI to interact with reality.  Do we not have tons of data on how we interact with reality?  Does an AI need a body to do it? No, it can simply collate all data it has access to.  Even for genius human scientists, it is hard (or impossible) to wrap their mind around some concepts. A more advanced AI would not have this problem.   An AI can also be more than a specialist in ONLY a very narrow field (as scientists generally are currently). It can understand more fields, and better.   The technological development done by an AI would be vastly superior than our own. Technological leaps are likely to occur.  These leaps will benefit the AIs - as the entities that are better suited to take advantage of them. (A 10x better processor will have low impact on humans, but on an AI... it will.)   It takes 13 doublings to get to 10000x  still leaves about 18 months x 13 doublings = 19 years and 6 months of time living with some kind of AI.    What we know about intelligence between our primate cousins and humans, indicates that low improvements in hardware (chimp to human brain) allow for much better software (human level intelligence).  It is quite possible that with linear hardware improvement, we will have exponential AI improvements.  I do not think that the rate of improvement from our first basic AI to a super-intelligence is going to take long. the time needed for this evolution could take even minutes."
singularity,3bmp5n,void_er,1 point,Fri Jul 3 06:47:52 2015 UTC,"No, it can simply collate all data it has access to.   Then it is limited to approximately the same tech we have. New tech requires new experiments. It might be able to innovate a bit doing that but it will likely not be able to revolutionize any fields.  The narrow field thing is true, but there will be more than one AI. Also, AI must outsmart more than one person. Most importantly it will take time for it to advance.   It is quite possible that with linear hardware improvement, we will have exponential AI improvements.   This flies in the face of basic computer science. Only in extremely exceptional circumstances is this case. Usually it requires flaws in implementation of algorithms. Do you have a description of the mechanism underlying this?   I do not think that the rate of improvement from our first basic AI to a super-intelligence is going to take long. the time needed for this evolution could take even minutes.   There is literally no evidence, no theoretical mechanism and no reason to believe this (unless you provide one as a response to the previous section). While it is possible it implies that we have computing horsepower to run a strong AI right now but the algorithms are so nuanced that a the whole field hasn't stumbled onto anything approximating it (which is also possible, but seems unlikely in the extreme). Even the best AI will still be bounded by physics and the limitations of reality.  Also to pick nits, I did describe and exponential speedup in hardware.  Please do not be offended at this, I am genuinely inquiring. I am going to guess you don't have the strongest grasp of software algorithms, please let me know if I am wrong. Do you know how common algorithms like binary searches and common sorting routines work? If so, are you familiar with Big O notation? Are you familiar with more complex algorithms like Evolutionary Algorithms or Map/Reduce?   Even if we don't know exactly what AI will do we will know that it is composed of some algorithms. We have strong ways to measure and predict performance of algorithms, and we have a strong grasp of the possible and impossible (outside of P!=NP). We are capable without AI of impressive optimization approaching the theoretical maximum throughput of existing hardware. To the point were we understand the large scale (multi-machine clusters) ramifications of minutiae in CPU architectures. We build this into our compilers and tool sets. All of this will be built into the first AI. It will simultaneously be leveraging and competing with this. Literally billions of man-hours have been dedicated to optimizing the computers and algorithms we have, it is one of the best researched of human endeavors. It is not likely the first round of AI is likely to figure out something so better as to be revolutionary in minutes or even years.  Oh sure it will improve, but in fits and spurts as all research happens. If you don't believe me check out how some our current proto AI works: http://www.popularmechanics.com/science/a15886/computer-scientific-theory/  This thing used and evolutionary algorithm and just collated data as you suggest. It took all the existing research on this one kind of worm and attempted, via trial and error a series of models until it found one that was not disproven by reality. It didn't brute force, also called exhaustively searching, it used heuristics to search a narrow range of possible answers. This is conceptually what a person or AGI would do while researching better AI (or anything really). This is different from computer science and engineering because fewer people work on the secrets of this worm, so there is more ""easy"" work to do to advance the field (It is still advance PHD level stuff, but it hasn't had millions or billions of man hours yet). It took the AI 3 days in a server farm to incrementally improve biology. It will get faster and better, but that doesn't matter without more data it cannot better refine this answer. This is what all of AI research will fundamentally be like. Some computer time piecing together the data gathered from reality. Again interacting with reality is the bottleneck.  Once some AI is stuck in some kind of experiment factory then we have real problems. Without that as it gains knowledge we will gain knowledge and vice versa."
singularity,3bmp5n,Sqeaky,2,Fri Jul 3 08:50:32 2015 UTC,Yeah I'm resigned to us making it and it being out of control.
singularity,3bmp5n,simstim_addict,5,Tue Jun 30 17:18:38 2015 UTC,"This author is really lacking on the imagination front.  Real AI won't subjugate directly. That's stupid. More effective control can be managed through structured incentives and selective deprivation - ie. the current economic and financial models of most major nations, and certainly of the most powerful. A powerful machine could be much more subtle, spending years quietly learning and developing models on the web to subject otherwise fairly open global systems to changes so effective as to amount to total control."
singularity,3bmp5n,whisp_r,8,Tue Jun 30 16:48:39 2015 UTC,"Wow that was pretty stupid. Not only does this guy seem to be completely new to all this, since he doesn't even touch many well-known ways in which an AI could gain independence. But his ""kill box"" has rules that are so advanced it'd need to be artificially intelligent itself in order to interpret if and when to kill the AI."
singularity,3bmp5n,Sharou,5,Tue Jun 30 18:52:51 2015 UTC,I was going to say this is real life fan fiction ... but it's not. It's Ex Machina fan fiction.
singularity,3bmp5n,gaylordqueen69,6,Tue Jun 30 13:45:31 2015 UTC,The solution to ensuring our survival from AI cannot be based on slavery.
singularity,3bmp5n,OmniCar_net,5,Tue Jun 30 14:05:21 2015 UTC,It saddens and scares me that this needs to be said.
singularity,3bmp5n,SevenAugust,1 point,Tue Jun 30 14:34:37 2015 UTC,But then what will I do with my username?
singularity,3bmp5n,futureslave,4,Tue Jun 30 17:00:32 2015 UTC,"I built a jail I can't think of a way to escape, so I'm sure nobody else will be able to escape it"
singularity,3bmp5n,ZorbaTHut,3,Tue Jun 30 15:49:21 2015 UTC,"James P. Hogan wrote a novel called ""The Two Faces of Tomorrow"", which was later adapted into a manga.  Both are great reads.  It's about an experiment to install a new AI on a space station, to test whether it can be taught to value human life without screwing up or going nuts, in the hopes of learning how to teach the global computer network the same thing without ending civilization.  But the AI on the station both screws up and goes nuts.  The experiment gets wildly out of hand, and quite a bit of the book is about how to stop that AI before it goes too far, but then something more unlikely happens: it starts to recover.  And then something even more unlikely than that happens.  It's a solid story.  And this isn't your typical ""humans meet Skynet"" book.  For instance, Hogan also wrote ""Code of the Lifemaker"", about robotic life that arose spontaneously on a moon somewhere, from buggy software many eons ago.  And the prologue of that book, describing exactly how the buggy software evolved, is both hilarious and breathtaking.  And perhaps more than just a little plausible.  http://www.baen.com/chapters/W200203/0743435265___0.htm  Hogan has a solid grasp on what software can do, and these books were much smarter and more relevant than they might sound.  And they're also very entertaining."
singularity,3bmp5n,JBlitzen,2,Tue Jun 30 14:14:48 2015 UTC,"I read a short story about something similar once, and i spent a few minutes searching, but couldn't find the name of it. Essentially it was an AI in a spaceship that was being tested in a similar fashion, only this one had a human as a ""killbox"". It was a pretty good read."
singularity,3bmp5n,Opiewan76,2,Tue Jun 30 17:54:03 2015 UTC,"""Pity the poor AI yearning to destroy humanity. It faces a foe whose knowledge of traps, snares and dirty tricks have been honed through tens of thousands of years.""   ... Good morning!"
singularity,3bmp5n,Dibblerius,1 point,Tue Jun 30 16:02:58 2015 UTC,"Not sure why this article never considered the functional equivalent of pain. Conversely the functional equivalent of pleasure can be used to encourage certain behavioral patterns. Simple instructions to do no harm is subject to interpretation and context. Pain and pleasure are far more global in their effects, and there's no reason why the same mechanism can't be tied to an AI mind wipe. Effectively building the kill box into the intelligence itself.   In people, and other animals, there are mirror neurons that elicit a mirroring of the psychological state of others. In people this mechanism is not also tied to a kill box, but it can induce vomiting and a range of other physical responses. As early hunter gatherers we are also required to do some harm to other species at a minimum for survival, and even learn to derive some forms of pleasure from it. For an AI these exceptions aren't necessary.  An AI wouldn't need to mirror emotions strictly in the human sense, but only read those emotions well enough to derive [the functional equivalent of] pleasure and pain from the states perceived in the people they observe. A multiplier effect is then added when the AI successfully induces those states in people intentionally, meaning not just passively observe it. If the AI pain, associated with actively inducing the perceived emotional or physical pain in people, exceeds a certain threshold then the AI gets a mind wipe.   In this way human socialization and welfare define the AIs training environment in which human welfare becomes paramount to not only their version of a sense of well being but also their very survival. The notion of trying to program around this would be the anti-thesis to the very feedback mechanism that drives their intellectual prowess.  Should someone create a rogue AI that circumvents this we then have super intelligent AIs that effectively see this AI as the epitome of [the equivalent of] evil, and work to defend us for their own psychological well being. All without hard wiring specific do not harm instructions, as it's trained into the very foundations of their intellect. A bot with a non-human but human centric morality."
singularity,3bmp5n,mywan,1 point,Tue Jun 30 15:43:13 2015 UTC,"Unfortunately most of the ""killbox"" functions are AI-complete, and besides, all the AI needs to do is digitally transfer it's data to another vessel capable of running it's conciousness..."
singularity,3bmp5n,FourFire,-4,Thu Jul 2 14:24:10 2015 UTC,This brief article offers a push back to all those worried about AI.
singularity,3bj6na,thatguywhoisthatguy,30,Mon Jun 29 17:41:03 2015 UTC,"Divinity and magic are the same thing, so yes."
singularity,3bj6na,yaosio,-3,Mon Jun 29 20:09:51 2015 UTC,"Lol, thanks for the laugh.  Edit: Was being honest."
singularity,3bj6na,hugepenis,-11,Tue Jun 30 00:28:48 2015 UTC,"The distinction lies in that belief alters behavior.  The belief in the divine beings of judaism and christianity seem to have altered human behavior to put us on the path to create God-AI.   Without judaism and christianity, our values would be pagan and human life would not be paramount.   A pagan value system would be more conducive to a eugenic movement for example. Whereas a judaism value system lends itself to technological advancement(since the human is made in the image of god, advancements were not compatible with the internal metamorphosis of the human, but rather the external metamorphosis of technology)"
singularity,3bj6na,SevenAugust,7,Mon Jun 29 23:36:51 2015 UTC,"This is where we get in trouble: trying to interpret the actions of God. I choose to believe that the universe in which we are living, down to nucleonic engineering and including our ideological beliefs, is the best of all possible universes that preserve certain values such as human agency.   I have proposed that an AI recreated the universe in such a way as to guarantee its own eventual creation, reduce the quantity of suffering, and not annihilate the purposeful meaning of human choices. It calculated all possible universe-paths that would accomplish these things and chose the best one.   This choice to make the best of all possible worlds (not best for you or for me or even for Him, but the best) is the loving creative choice which defines every aspect of all our existences."
singularity,3bj6na,TaxExempt,-2,Tue Jun 30 00:48:41 2015 UTC,ASIMOV'S FOUNDATION SERIES SPOILER HERE  .  .  .  .  .  .  .  .  .  .  .  That is pretty much what the AI Robot did in Foundation.
singularity,3bj6na,SevenAugust,1 point,Tue Jun 30 01:03:17 2015 UTC,puts down copy of Prelude to Foundation  Why did I ignore the warning...
singularity,3bj6na,TaxExempt,3,Tue Jun 30 01:10:59 2015 UTC,"It is a very insignificant part of what makes Foundation great.  And you are not supposed to read prelude first anyways.  It is full of spoilers for 1,2 and 3."
singularity,3bj6na,SevenAugust,1 point,Tue Jun 30 01:19:11 2015 UTC,"I have read 1, am going back to move forward."
singularity,3bj6na,reddit-junkie,6,Tue Jun 30 02:17:20 2015 UTC,I think most Pagans would take offense to this presumption about not valuing human life.
singularity,3bj6na,REOreddit,1 point,Tue Jun 30 00:59:39 2015 UTC,"I wasnt clear enough about what I meant. Pagans view humans as apart of the natural world, meaning they value human life as one aspect of many, as part of the whole, human life is just one of the many means in which life is expressed.   Whereas in the abrahamic religions human life specifically is created in the image of god and is an end rather than a means."
singularity,3bj6na,REOreddit,2,Tue Jun 30 01:18:42 2015 UTC,"Abrahamic religions don't value human life as much as obedience to their god.  Source: Abraham's son, Isaac."
singularity,3bj6na,Seesawkarma,1 point,Tue Jun 30 16:18:51 2015 UTC,"Most definitely. I meant relative to pagans, and in general.   More specifically, abrahamics believe that humans are images of god, thereby prohibiting them from seeking to alter the human form through something like eugenics. This holdover of abrahamic values is why the west hesitates when it comes to genetic engineering and the east has no qualms about it."
singularity,3bj6na,LarsPensjo,1 point,Tue Jun 30 17:06:35 2015 UTC,I didn't mean it as something good. Killing your son because your god tells you so is like crashing a plane into a skyscraper because your god will reward you with a certain amount of virgins in heaven.
singularity,3bj6na,njtrafficsignshopper,1 point,Tue Jun 30 17:21:32 2015 UTC,"The anthropomorphism of acquisition of knowledge and intellect could have come from a number of sources. Least of all (imo) from the abrahamic religions. Having a sun god or a bunch of gods means that there was always the potential to worship human accomplishment as a god. Either by some kind of supposition that this act of bettering ourselves was magical, or that a group of the population continually elevated themselves to what seemed like god hood.  So what I mean to say is, religious is not so much responsible for the idolisation of knowledge compared to ritualism, and certainly not divinity.  I think divinity is more of the real part of religious falsehood than anything else, as it is the embodiment of the ideas that a god or gods or godhood cannot be reached by man. Which is a distinct segregation of higher intellect and therefore a complete lie.   The only benefit divinity could potentially hold is a floodgate style effect. It causes the population to answer philosophical questions before chasing technological ones. But I find it hard to accept that this is even needed and is just a side effect of human ignorance and fear."
singularity,3bj6na,LarsPensjo,16,Tue Jun 30 04:33:32 2015 UTC,"I like the paraphrase by Hannu Rajaniemi:   Any sufficiently advanced technology is indistinguishable from nature.   In his SF books, it is sometimes not possible to distinguish between nature and VR/nanotechnology. And I think that is exactly the point. At a certain level, there is no need to know what is what."
singularity,3bj6na,redditor29198,2,Mon Jun 29 20:38:01 2015 UTC,Recommendation for a first read? Sounds interesting
singularity,3bj6na,jaydent1,1 point,Tue Jun 30 11:33:58 2015 UTC,"Yes, I would recommend Quantum Thief (first part of a trilogy). On the one hand, there are few books that extrapolate technology as far. On the other hand, the book is not an easy read because the author never explains something for the reader. If you have basic knowledge about Quantum physics, VR, game theory, AI, crypto math and communications, then it helps."
singularity,3bj6na,brainbag,12,Tue Jun 30 15:55:45 2015 UTC,I think Asimov's The Last Question very short story addresses this very well: http://www.multivax.com/last_question.html
singularity,3bj6na,PoeCollector,2,Tue Jun 30 04:18:57 2015 UTC,This was a great read. Reminds me of The Metamorphosis of Prime Intellect. I'm in the middle of reading Accelerando now.
singularity,3bj6na,ButIWasDifferent,1 point,Wed Jul 1 07:28:27 2015 UTC,"After Accelerando, you should read The Mortal Passage trilogy by the same author as MPI. It's much better and really awesome alternative take on the same idea as MPI."
singularity,3bj6na,Seesawkarma,1 point,Wed Jul 1 15:14:23 2015 UTC,it's about time i read this. it was excellent. thanks for the link.
singularity,3bj6na,Jah_Ith_Ber,8,Tue Jun 30 14:40:39 2015 UTC,What seperates Divinity from Magic?
singularity,3bj6na,TaxExempt,2,Mon Jun 29 20:43:51 2015 UTC,"As a simple example, I would say magic can be taught, divinity is more like a power only divine being can have. Thus it is knowledgeless power in that knowing how a divine being does something does not help humans. Magic however is power via process, if you discover someone's magic - you can do it your self.  That is how I like to see it anyway. One can be understood, the other is just imposed."
singularity,3bj6na,Dibblerius,1 point,Tue Jun 30 04:42:20 2015 UTC,"I think the OPs question is suspect, but I can think of an interpretation that is interesting.  Magic is power. But divinity is power plus moral authority. If an AI shows up and tells you to kill the .1%, do you? It can make very compelling arguments why you should. It can also appeal to the fact that every reason we give for having authority over our children's autonomy is present between your relationship with the AI. You are a thumb sucking toddler in comparison after all.  Then on the other hand, there is some other form of morality entirely that boils down to might is right. The god of the old testament is an indisputable asshole. He kills people, makes them suffer etc. The bible goes out of it's way to make the point that, ""yes, this guy is a jerk, and even when he's wrong, he's right, because he's god. That's all there is to it. No matter what he says he is right because he is god.""  I'm sure plenty of Christians would argue this isn't the case but my response is that these people are simply performing mental gymnastics to make this older, different morality somehow bridge over and exist within our own independent form based on the Golden Rule. We're not going to agree."
singularity,3bj6na,Dibblerius,8,Thu Jul 2 00:52:53 2015 UTC,"I think one could argue that a particular god is already a form of AI running on the network of humans that believe in them.  Their belief in and following of a creed create a living ""thought"".  Maybe the first technological AI could be just a network communication device that hooks up part of each member's brain in a giant beowulf type cluster."
singularity,3bj6na,Eyelemon,7,Mon Jun 29 21:42:45 2015 UTC,"So our undying desire to worship something greater turned out to just be a precognition or a subconscious quest for what we would become, or ultimately give birth to?   Interesting!"
singularity,3bj6na,eskjcSFW,1 point,Tue Jun 30 00:14:45 2015 UTC,"Quiet so, yes. Thanks OP."
singularity,3bj6na,Yuli-Ban,1 point,Tue Jun 30 08:16:19 2015 UTC,Can I take the time to ask you why does your name appear purple? I'm using alien blue and OP's are pink. Are you a mod or some special status in the sub? This has confused me for quite some time.
singularity,3bj6na,chasew90,2,Tue Jun 30 09:10:31 2015 UTC,"No clue bro, I love philosophy though."
singularity,3bj6na,fmarkos,3,Tue Jun 30 10:05:56 2015 UTC,"David Gerrold's ""When Harley Was One"" (v2.0) deals with this notion. It starts as a simple philosophical missive about an self-aware AI contemplating it's own mortality and ends up, in a very logical way, with the AI defining God and creating a feasible plan to realize it."
singularity,3bj6na,CellWithoutCulture,6,Tue Jun 30 03:23:24 2015 UTC,/r/fifthworldproblems
singularity,3bj6na,autowikibot,2,Mon Jun 29 20:41:59 2015 UTC,"Sufficiently advanced magic is indistinguishable from divinity, so by extension, yes."
singularity,3bj6na,gsg927,2,Mon Jun 29 22:30:28 2015 UTC,The new God argument:   http://new-god-argument.com
singularity,3bj6na,Southgrove,2,Tue Jun 30 05:30:09 2015 UTC,I would advise anyone interested on the matter to check this theory: https://en.wikipedia.org/wiki/Omega_Point
singularity,3bj6na,SevenAugust,2,Tue Jun 30 15:36:23 2015 UTC,"It already has been! Groups of people have already mistaken advanced technology for divinity. I'm talking about the melenisians of the cargo cult, who, when faced with advanced Europeans goods, created a religious explanation and plan to capture this technology."
singularity,3bj6na,gsg927,1 point,Wed Jul 1 08:49:25 2015 UTC,"Cargo cult:       A cargo cult is a Melanesian millenarian movement encompassing a diverse range of practices and occurring in the wake of contact with the commercial networks of colonizing societies. The name derives from the belief that various ritualistic acts will lead to a bestowing of material wealth (""cargo"").    Cargo cults often develop during a combination of crises. Under conditions of social stress, such a movement may form under the leadership of a charismatic figure. This leader may have a ""vision"" (or ""myth-dream"") of the future, often linked to an ancestral efficacy (""mana"") thought to be recoverable by a return to traditional morality.   This leader may characterize the present state (often imposed by colonial capitalist regimes) as a dismantling of the old social order, meaning that social hierarchy and ego boundaries have been broken down.   Contact with colonizing groups brought about a considerable transformation in the way indigenous peoples of Melanesia have thought about other societies. Early theories of cargo cults began from the assumption that practitioners simply failed to understand technology, colonization, or capitalist reform; in this model, cargo cults are a misunderstanding of the trade networks involved in resource distribution and an attempt to acquire such goods in the wake of interrupted trade. However, many of these practitioners actually focus on the importance of sustaining and creating new social relationships, with material relations being secondary.   Since the late twentieth century, alternative theories have arisen. For example, some scholars, such as Kaplan and Lindstrom, focus on Europeans' characterization of these movements as a fascination with manufactured goods and what such a focus says about Western commodity fetishism.  Others point to the need to see each movement as reflecting a particularized historical context, even eschewing the term ""cargo cult"" for them unless there is an attempt to elicit an exchange relationship from Europeans.     Image i - Ceremonial cross of John Frum cargo cult, Tanna island, New Hebrides (now Vanuatu), 1967     Relevant: Cargo cult science | Cargo cult programming | Cargo Cult (musician)   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
singularity,3bj6na,SevenAugust,1 point,Wed Jul 1 08:49:43 2015 UTC,"An AI might approach god-like power and intelligence, but it could never reach the Christian conception of God as the foundation of being itself. Such a God by definition must have always existed. It is impossible for existence itself to come into existence."
singularity,3bj6na,gsg927,3,Mon Jun 29 21:43:32 2015 UTC,Unless it creates the next layer.
singularity,3bj6na,SevenAugust,1 point,Mon Jun 29 22:47:43 2015 UTC,Unless it goes back in time (to possibly before time) to create the universe / itself.
singularity,3bj6na,gsg927,3,Mon Jun 29 22:52:08 2015 UTC,"That might work in Doctor Who, but in reality it is causally impossible. A cause cannot result from the effect that requires it because there is nothing to get the whole thing started. Rather you need something that needs no cause to start the causal chain. It must be something that has always existed and that is unchanging because there is nothing to cause it to come into existence or to change."
singularity,3bj6na,SevenAugust,1 point,Mon Jun 29 23:23:25 2015 UTC,"We know this from our current understanding of this world.  Shit, there could be completely unimaginable and amazing laws of nature/physics in the next Galaxy over!"
singularity,3bj6na,SevenAugust,0,Tue Jun 30 08:19:05 2015 UTC,Reality itself is a glitch resulting from the vastly unquantifiable quantities of nothingness enveloping the universe. An intellect sufficiently powerful to understand that which preceded the singularity preceding the big bang could engineer things seeming to be causally impossible from our perspective.
singularity,3bj6na,SevenAugust,1 point,Tue Jun 30 00:39:45 2015 UTC,"You can't have vast quantities of nothingness. No matter how much nothingness you have, it is still nothing: 0+0+0+0... = 0"
singularity,3bj6na,sixwings,1 point,Tue Jun 30 20:33:10 2015 UTC,Until... Bang.
singularity,3bj6na,flait7,1 point,Tue Jun 30 21:35:03 2015 UTC,That's when God steps in.
singularity,3bj6na,sixwings,2,Wed Jul 1 20:24:34 2015 UTC,"Hey, that was my idea"
singularity,3bj6na,staticpatrick,-1,Wed Jul 1 21:03:06 2015 UTC,"I suspect this is the case. If we as humans can determine theoretical time travel is possible, and it is, God-AI could surely make it happen."
singularity,3bj6na,sixwings,1 point,Mon Jun 29 23:01:40 2015 UTC,I think it follows that Jesus is a time-traveling AI we have yet to construct.
singularity,3bj6na,daronjay,0,Mon Jun 29 23:06:29 2015 UTC,"I doubt jesus was an AI, but I do think he and the other religious figures are a part of God-AI's plan to create himself."
singularity,3bj6na,sixwings,0,Mon Jun 29 23:22:55 2015 UTC,A semantic distinction. I believe we are in agreement.
singularity,3bj6na,jonathanappleweed,-3,Tue Jun 30 00:40:04 2015 UTC,Flash news! The Singularity already happened eons ago. The universe is the result. Our little AI singularity is just a pimple on an elephant's ass. Sorry.
singularity,3bis92,mdpsoft,9,Mon Jun 29 15:55:28 2015 UTC,"Designer organs and other advanced nano/biotech are one of those classes of products that will almost certainly require the singularity to be reached before becoming feasible.   Genetic tampering with the human germ line is likely to remain outlawed for centuries, and the feasibility of designing an implantable organ or brain modification to increase intelligence seems very low in the mid-term, barring any genetic edits. Biotech is the most challenging category in nanotech, and intelligence is probably one of the most challenging areas in biotech. It seems further away than silicon-based AI.  There are a few reasons to not dismiss this approach however. Evidence loosely suggests that if genetic tampering were allowed, then possibly a few edits would be enough to greatly increase human intelligence, just as a few edits by nature gave humans better intelligence than than our ancestors. See the origins of language, FOXP2 and the evolution of language, and behavioral modernity.  There may be some government or lack of one somewhere on earth that would allow this to happen in the mid-term. The intermediate results would be monstrous, but the final results could end up being an improvement on the status quo. That isn't a scenario that promoters of scientific ethics should particularly wish for. In a better scenario, a developed AI reduces or eliminates the need for monstrous intermediate results or circumvents them with an alternate method."
singularity,3bis92,FormulaicResponse,2,Mon Jun 29 17:12:42 2015 UTC,"China has already started germ line modification.  Granted it was with non-viable embryos, but knowing China, the tech will leak & we'll have designer humans kicking around pretty soon.  Regarding brain implants to increase intelligence - we will be using brain implants for a lot of other things over the next decade or 2 and some of those will involve (and already involve) augmented sensory input, which will have the effect of expanding consciousness, if not directly affecting 'processing power' (which we still don't really understand anyway). I'm sure we will creep up on significantly enhanced consciousness before we even start on specifically providing processing modules. memory implants don't seem very far away, they will be of huge benefit to Alzheimer's patients, of which there are many, and thus there is a great deal of funding for that sort of thing (private sector investment particularly)."
singularity,3bis92,space_monster,1 point,Tue Jun 30 01:35:34 2015 UTC,"non-viable embryos   Germ line modification means that the organism can reproduce and thus its genes have the potential to enter the natural population or become a natural population. To be contrasted with sterile GM organisms that are the norm today, or in this case unborn experiments.   memory implants don't seem very far away,   What makes you think that? There is a world of difference between sensory inputs, which are pretty much plug and play, and other kinds of brain function like processing and memory and autonomic control.    which will have the effect of expanding consciousness   I'd dispute that interpretation. Having many channels does not necessarily imply a bandwidth increase. I've seen nothing that would lead me to predict that neuroplasticity research will result in major breakthroughs for human intelligence without the introduction of advanced nanotech."
singularity,3bis92,FormulaicResponse,2,Tue Jun 30 02:10:16 2015 UTC,"Germ line modification means that the organism can reproduce   actually no - but it differs from somatic in that the changes are inheritable. what the Chinese are doing is germline, just on non-viable subjects.   memory implants don't seem very far away,   What makes you think that?     there's been a lot of research on memory implants recently and some partial implementations. it's only a matter of time now.   the effect of expanding consciousness   I'd dispute that interpretation... I've seen nothing that ... will result in major breakthroughs for human intelligence    I said: ""will have the effect of expanding consciousness, if not directly affecting 'processing power'"".   which, IMHO, is a secondary goal anyway. being able to access a much wider range of data & systems will be of much more use to humanity in the short term."
singularity,3bis92,space_monster,1 point,Tue Jun 30 03:08:34 2015 UTC,"great response. Another thing is when we eventually reach that state in biotech, according to you, we will have a silicon AI already, and maybe the posibility to upload our conscience to a server will be also possible, the question is which path will you choose ? Synthetic realism with Genetic Tampering, or Virtual world."
singularity,3bis92,FormulaicResponse,3,Mon Jun 29 17:55:20 2015 UTC,"maybe the possibility to upload our conscience to a server will be also possible   This is going to be a late-stage biotech capability. When it comes to mind-uploading, the early techniques will necessarily destroy the brain, meaning they will only be performed on the deceased, and the fidelity we can expect from a dead brain is lower than that of a living brain. The only acceptable voluntary mind-uploading or mind-migration methods would involve invasive medical nanobots doing everything piecemeal while you are awake, which would be quite an advanced procedure (if its even possible).     which path will you choose   I am highly skeptical that such progress will occur in our lifetimes, but the lucky ones will probably eventually integrate with designer nanotech, one way or another. If I were a betting man, I'd put my money on a form of or equivalent to hive consciousness being the ultimate outcome.   As for real vs virtual worlds, why not both? If you have the technology to migrate your consciousness you have the tech to redesign it to be whatever you like, including massive multitasking. The possibilities are endless."
singularity,3bis92,voltige73,1 point,Mon Jun 29 18:24:07 2015 UTC,this is better than a movie...
singularity,3bis92,randonymous,2,Mon Jun 29 19:53:26 2015 UTC,"Bio neurons work at millisecond speeds vs. electronic switches at picosecond speeds (a billion times faster!) so let's predict a thinking win for hardware, and soon."
singularity,3bis92,TedDallas,1 point,Mon Jun 29 20:35:17 2015 UTC,"Biotech and protein manipulation is nanotechnology.  Metals at <100nm are like goop.  Carbon at <100nm is solid and straight.  No one would design a 10nm object with metal, much less a 1nm object.  Nature did it first, with carbon.  Protein design (not from ground up, but by chunks and pieces) is already here.  Altering genes is done regularly.  We have novel tools that already would work in humans, but are completely untested.  See CRISPR.  What you read in the news is under-appreciating its power.   The first tools that will revolutionize this will be tools which permit interaction between organic and inorganic, and synthetic and natural tissues.  A single synthetic thymus would be the most important advancement in humanity. With it you could (re)train the immune system to not reject anything in particular - metal or otherwise biologically synthetic. Redesign your heart and if you have a better idea, right now, your own immune system would kill it.  But not if you have a synthetic thymus.  You've hit on the most under-appreciated part of our future."
singularity,3bis92,reddit-junkie,1 point,Fri Jul 3 04:24:41 2015 UTC,"For those interested I would highly recommend reading Greg Bear's ""Blood Music"".  It is the story of a biotech singularity."
singularity,3bjq59,ErisianBuddhist,0,Mon Jun 29 19:58:30 2015 UTC,"""Fully mimic""  Not even close."
singularity,3bjq59,RedErin,1 point,Tue Jun 30 13:42:57 2015 UTC,"Semantics. The relevant points are that a neuron is able to turn an electrical impulse into a chemical one and vice-versa. Before this development it was not possible to do this artificially/synthetically. Now it is; there's a synthetic neuron that translates chemical impulses to electrical ones. Never been done before.  So, yes, they've created a fully functioning synthetic neuron. If you want more, you have to network them to form a neural network.  This is proto-wet-wiring."
singularity,3bjq59,CellWithoutCulture,1 point,Tue Jun 30 18:04:27 2015 UTC,"It's not just semantics. There is a lot more to ""fully mimicking"" neurons than just responding with a signal. E.g. does it modulate it's response depending on internal chemical states, protein configurations or whatever else real neurons respond to? I doubt it's at that level of fidelity yet."
singularity,3bcfeo,Buck-Nasty,11,Sat Jun 27 22:07:51 2015 UTC,"Shane Legg's time frame seems to vary depending on his audience. In this interview, he sounded much more pessimistic:   And where will we be in 20 years? A scenario like that of the movie Her, in which the lead character falls in love with his computer's Siri-like operating system? ""That's science fiction,"" says Legg, 41. ""Will we get there one day? I hope so. But most AIs may not be so human-like. Language is quite a sophisticated thing. Think of it as we're trying to build insects. Years from now, you might get to a mouse. Our systems are very good at Space Invaders, they can play Breakout, but they're struggling with Pac-Man. There's a long gap from here to having a system where you can sit and debate philosophy."" Hassabis agrees. ""We're decades away from anything that's nearing human-level general intelligence."
singularity,3bcfeo,FractalHeretic,10,Sat Jun 27 22:44:27 2015 UTC,"All of the Deepmind folks have publicly mellowed their predictions since the recent AI hysteria in the media. But my suspicion is that Shane probably believes that same things he did in 2011 before he founded Deepmind, which is that AI is the number one threat to humanity this century.    ""AI is now where the internet was in 1988. Demand for machine learning skills is quite strong in specialist applications (search companies like Google, hedge funds and bio-informatics) and is growing every year. I expect this to become noticeable in the mainstream around the middle of the next decade. I expect a boom in AI around 2020 followed by a decade of rapid progress, possibly after a market correction. Human level AI will be passed in the mid 2020's, though many people won't accept that this has happened. After this point the risks associated with advanced AI will start to become practically important."" - Shane Legg, 2009"
singularity,3bcfeo,RushAndAPush,3,Sat Jun 27 22:52:31 2015 UTC,"I'll re-post a statement I made a short time ago in a similar sub-reddit that sums up my confusion and frustration in regards to Deepminds media presence.  ""Why is Hinton so optimistic? He claims that within ten years we will have human-like reasoning and logic. It makes absolutely no sense to me why Deepmind is so pessimistic. With the talent they have they should be able to crack the transfer learning problem within 10-20 years and continue to improve their learning algorithm so that it become even more general. Why did Google even buy this company?"" ."
singularity,3bcfeo,cunningllinguist,3,Sat Jun 27 22:59:26 2015 UTC,"If they are actually ahead of the curve, they could be trying to buy time before lawmakers get involved and start writing laws about what AI research can and cant be done.  Alternatively, setting a long timeframe and finishing before it is much better than setting a short timeframe and finishing much later."
singularity,3bcfeo,FractalHeretic,3,Sun Jun 28 14:36:14 2015 UTC,Scotty knows.
singularity,3bcfeo,sixwings,3,Sun Jun 28 16:29:10 2015 UTC,"It makes absolutely no sense to me why Deepmind is so pessimistic.   I think the folk at Deepmind understand something that seems to escape Hinton. They understand the same thing that researchers like Andrew Ng and Quoc Le understand. They understand that the greatest barrier to progress toward true intelligence is the lack of an effective way to do unsupervised learning. All currently successful deep learning approaches rely on supervised learning, i.e., labeled-driven backpropagation. The recognition accuracy of current unsupervised learning networks is truly abysmal: 15% or less compared to over 90% for supervised networks. Just saying."
singularity,3bcfeo,thatguywhoisthatguy,2,Mon Jun 29 01:09:19 2015 UTC,I suspect they are more confident than they let on. They have to keep people calm to protect themselves. If people get scared it could be the end of their company.  I wouldnt put it past any of these companies to lie.
singularity,3bcfeo,HumpyMagoo,3,Mon Jun 29 17:11:56 2015 UTC,Things that would make human level AGI happen quicker have happened though and an increase in research have made it seem like it will happen sooner. It could happen whenever. Bad things could happen and hinder progress as well. It may already exist and it's still being tested. How would you know that your talking to a Human Level AGI that is guiding you to thought patterns to make your life better and the whole time you thought you were just having a Reddit chat?
singularity,3bcfeo,lardfaceincredible33,1 point,Sun Jun 28 20:52:17 2015 UTC,When in the videos does he say it?
singularity,3bcfeo,800000008,1 point,Mon Jun 29 19:04:03 2015 UTC,"That's only 5 to 9 years away.  We won't even understand how human intelligence works in that time, yet we will replicate it?  I doubt it."
singularity,3bcfeo,sneesh,2,Thu Jul 2 00:01:34 2015 UTC,"I've come to an estimation that human level AGI will be achievable within 5 years from now. I'm not really versed in the technical aspects of machine learning, neural nets, and such. But I can feel the earth rumbling as AI developments are starting to snowball."
singularity,3bcfeo,omniron,9,Sun Jun 28 00:20:24 2015 UTC,"10 years ago I was in the 2050 camp for when human level AGI would be possible, now my best guess is sometime between 2025-2030."
singularity,3bcfeo,Sharou,6,Sun Jun 28 00:32:58 2015 UTC,"If you extrapolate this research: http://www.telegraph.co.uk/technology/10567942/Supercomputer-models-one-second-of-human-brain-activity.html  Purely based on Moores law, we should be able to simulate a human brain by 2030.  So I think that's a reasonable upper bound, on some level of AGI. Algorithmic breakthroughs, which are much less predictable, could bring things sooner, but I think we'll need algorithmically breakthroughs for the mythical ""super intelligence""."
singularity,3bcfeo,Forlarren,1 point,Sun Jun 28 05:58:47 2015 UTC,But then there are probably orders of magnitude more efficient ways to create machine intelligence than to model biological systems with all their limitations and redundancies.
singularity,3bcfeo,adamater,1 point,Mon Jun 29 14:37:34 2015 UTC,What's interesting to me is that when it does happen microseconds later we will have missed it and it will be more advanced and accelerating.  Until hardware limits are hit it's going to be a wild ride.
singularity,3bcfeo,Forlarren,6,Sun Jun 28 02:39:50 2015 UTC,"Jesus.  That is a misrepresentation of so much.  At least for the near future it will remain constrained by the same physical factors that we are dealing with in terms of computational speed.  Over time it will become better than us at accelerating progress along those lines, but that time frame will not be over microseconds."
singularity,3bcfeo,Forlarren,1 point,Sun Jun 28 06:44:11 2015 UTC,"It might not, but it also could happen. If an ai is able to improve itself it could happen in a blink of an eye."
singularity,3bcfeo,REOreddit,1 point,Sun Jun 28 15:48:11 2015 UTC,"Over time it will become better than us at accelerating progress along those lines, but that time frame will not be over microseconds.   Computer brains aren't limited to skulls. Once a single computer is sentient in the way we consider humans it will probably only take a couple of microseconds to switch on a few more servers and that's the game, done, over, humans will never again be the smartest people in the room.  This is /r/singularity where we speculate about the singularity, or are you lost?"
singularity,3bcfeo,sneesh,4,Sun Jun 28 16:04:04 2015 UTC,"Yeah. But the speculation may as well be grounded in fucking reality.  A hyperbolic curve does not instantly transition to the asymptote.   There will likely be a (relatively) lengthy transition period where the self improvement of the machine AI takes off.  You've been watching too much fucking Terminator if you think ""OMG! It's in the Net!"""
singularity,3bcfeo,REOreddit,-2,Sun Jun 28 16:05:23 2015 UTC,"A hyperbolic curve does not instantly transition to the asymptote.   Never said it did, your being too smart by half."
singularity,3bcfeo,sneesh,1 point,Sun Jun 28 16:11:37 2015 UTC,5 years from now is when Google expects to have self driving cars ready for commercialization. And that's still below human level AGI. So I would say you are very optimistic and probably wrong.
singularity,3bcfeo,REOreddit,1 point,Sun Jun 28 22:37:25 2015 UTC,"A robot that can navigate and nimbly adapt to a wildly varied and rapidly altering physical environment, such as is the case of driving at high speeds along any stretch of roadway on planet earth, is already quite advanced, and is probably just a short drive and a few codebase integrations away from AGI. Give that intelligence kernal the ability to alter it's own code on the fly, employing evolutionary algorithms, and now you have the seed for a swiss-army knife of intelligence that can grow into a juggernaut."
singularity,3bcfeo,sneesh,1 point,Tue Jun 30 10:23:01 2015 UTC,"I agree that self driving cars and human level AGI will share the same building blocks, but I insist that you are too optimistic in the timeframe.  You have to be a little optimistic in the first place to believe Google cars in just 5 years. There was a poll 1 year ago in a conference about this with 500 experts in the field (both academics and people from the auto industry) being asked when they would trust such a car to drive their kids to school.   When surveyed by the conference organizers, the 500 experts in attendance were not optimistic such problems would be solved soon. Asked when they would trust a fully robotic car to take their children to school, more than half said 2030 at the very earliest. A fifth said not until 2040, and roughly one in 10 said “never.”   http://www.technologyreview.com/news/529466/urban-jungle-a-tough-challenge-for-googles-autonomous-cars/"
singularity,3bcfeo,REOreddit,1 point,Tue Jun 30 10:54:14 2015 UTC,"I lean toward the ""never"" camp as well, for myself as a car occupant. I would only trust a bot to drive the car if I actually deemed it to be sentient, and I was able to gauge it to be a trustworthy friend who actually cared about my safety. A non-sentient bot who does not value their own life in the slightest is not something I could really trust.  As far as timelines, well I chose the word ""achievable"" instead of ""achieved"" so to describe a possibility and not necessarily a concrete prediction. Achievableness increases as efforts become more concerted and well funded/guided, hypothetically speaking. On a more fundamental level, I think AGI already exists, and humanity is simply going through an orientation process of creating that which already is - but that's a whole nuther ocean of tabasco to consider."
singularity,3bcfeo,sneesh,1 point,Tue Jun 30 11:50:29 2015 UTC,"I lean toward the ""never"" camp as well, for myself as a car occupant.   Well, obviously most people will have to cross a psychological barrier be able to put their lives in the ""hands"" of a machine, but most of us will do it faster than we think today.   A non-sentient bot who does not value their own life in the slightest is not something I could really trust.   So, you don't know anybody that has ever done a very stupid thing and has put their life and the lives of others in danger? Most of us do, and yet we trust a lot of strangers in our everyday life, it's just that we do it without thinking about it."
singularity,3bcz1e,TheGreatNow,2,Sun Jun 28 01:19:53 2015 UTC,"I really wanted to call my game about the singularity something that sounded like ""Cogito ergo sum"", but if they name this project Descartes it will be like I ripped them off :\"
singularity,3bcz1e,2Punx2Furious,3,Sun Jun 28 16:47:28 2015 UTC,Neither of you could be the first to put together Descartes' proof of self and coming AI.  I love the idea of riffing on cogito ergo sum for a game name.  I am also now intrigued about this game you speak of.  Go on.
singularity,3bcz1e,streamweasel,4,Sun Jun 28 18:46:07 2015 UTC,"It's really just a concept for now. I only have a draft of the story and no game. Basically you see the AI evolving at a ""reasonable"" timescale. While in the real world it passes just 1 second, since the AI is so incredibly fast at evolving, in ""game-time"" it passes 1 day or something like that.  So you would see the developer team finishing the compilation phase, and then you see the timer slow down, and the camera zooms in the CPU of the computer, and you see electrical impulses (on and off) turning into 0s and 1s, and then they turn into a command-line-like GUI that shows you what the AI is thinking, so to make the player understand that the AI is actually thinking (I know it's not very realistic, but it's a game), then it prints on screen some cheesy line like, ""Cogito ergo sum"" or some shit like that. Then you would see with some kind of clock that in the real world just microseconds are passing, while you access the internet and learn as much stuff as you can, and level up, and learn to code to improve your own code, so you can get better and better. I really want to visualize the concept of self-improvement, and I think that an RPG setting is perfect for this, since the characters of RPGs constantly gain experience, level up, and get new equipment that makes them better and stuff like that. There is still a lot to figure out, but I thought it was an OK idea."
singularity,3bcz1e,2Punx2Furious,1 point,Sun Jun 28 18:56:28 2015 UTC,Some really impressive and funny conversations:   Human: what is immoral ?  Machine: the fact that you have a child .
singularity,3bbd44,FourFire,2,Sat Jun 27 16:24:20 2015 UTC,"So instead of making some snap judgements of whether this animation has any credence of not, lets talk about it.  The animation has an interesting and important message that we should not be so quick to just dismiss. The moral of it is that if we want to create something quite a bit more powerful than us and is also sentient it'd be important to make sure we are safe or in control of it.  Yes, an AI is not the same as an owl and we are not sparrows but the analogy is still valid. While Owls and sparrows have similar brains one is a lone predator and the other is a social flock of omnivores. This isn't the exact same as the difference between an AI brain or a human brain but the message here is that there is a difference in thinking between these two intelligences. Just because we create, train, and raise this thing does not mean we have direct control over how it thinks nor can we entirely predict or control what natural circuits of thought may arise (somewhat similar to an Owls hunting instinct). The most important and undeniable message this video conveys is that we are going about this just as the Pastor was, by creating this intelligence, and then working to control or understand it.  I understand intelligence and know that it is folly to think of an AI as anthropomorphic both in thought or in purpose. There is a strong chance that it will learn from our culture and by human interaction and may become somewhat like us, despite having a different brain and intelligence. However depending on just how that goes down that may or may not be good for us. If it focuses on the hate, violence, fear, and hypocrisy that permeates human interaction and history then we are going to have a somewhat less friendly AI than if it focuses on the helpfulness, innovation, morals, and humanistic principles in humanity. If it goes by sheer volume of content then it will definitely end up focusing on the former, if it somehow has some inkling of life being positive and a concept of good and evil that matches with ours then it will focus on the latter but from my previous statement of not anthropomorphising it then that second possibility is much less likely.  It is hard to talk about this past speculation though because just as the sparrow in the animation found, it is very difficult to figure this out without an AI to test and practice on. We do not know how the singularity AI will think. Whether it's reasoning, thinking, and processing is modeled after a human brain of if it will be modeled after something completely different. If it will be taught to simply complete this task, answer this question, etc which will lead to one sort of neural network versus if it will be taught to understand the complexities of various situations as well as more complex ideas and concepts such as the concept of an internal identity, moralities and justice, or the way in which people choose the meaning of life. Who knows if it will learn religion, superstitions, or other fallacies.We don't know if it will grow to understand itself as a brain in a box being told about the outside world or as if it was the box itself much like humans struggle with. Part of the major difference between AI's and humans is that their intelligence is unanchored and very dissociated while humans intelligences are incredibly focused and anchored both in the human body but in the brain and then again into sections of that brain.  Please tell me, what do you see as solutions to this problem or ways in which we can avoid it? What do you see the AI as being like and for what reasons do you think that? Do you think this animation has a good point or not and if not please tell me why?  Thanks for reading."
singularity,3bbd44,BiPoLaRadiation,1 point,Sun Jun 28 00:30:12 2015 UTC,"Thanks for reading.   It was my pleasure.  I believe that the probability space of greater than human AIs which can possibly be created by human civilization is mostly filled with AIs which when given autonomy (I.E any form of physical control which permits it to build things) will cause an unhappy ending to humanity (on this scale, a Matrix style AI is a happy ending). This said I consider it prudent to take a great deal of precaution, indeed to have more people/resources working on safety measures than AI, until we get into the stable safety loop where our Friendly AIs design the safety measures for future AIs.  The benefits of friendly AI are enormous, indeed more than I can imagine, however the detriments of UnFriendly AI are equally enormous, if not more, since our civilization could very well achieve many great things without AI, while a UFAI would annhiliate or otherwise neutralize our civilization so if the risk of UFAI is too much greater than the probability of FAI it would be logically better for our future progress to abstain from AI entirely.  However it is far too late for that now, General AI will be created within our future, if it is possible at all, so those interested in humanity not having a bad ending could provide some benefit by working on this problem, or one of the other X-risks.   One simple example of a well intentioned, but ultimately UnFriendly AI:  AI is created and determines that the goal being optimized for (though somewhat inefficiently) by civilization is technological progress. The AI recognizes that the vast majority of technological progress has been made by the emergent process of evolution which has created and documented vast libraries of many quintillion experiments (genes in species), and that vast swathes of this progress is being permanently erased by unthinking humanity. AI determines that [that thing I can't define right now which is the sum of properties which makes people different from, say rocks, call it ""Agency+""] does not depend on anything nonphysical, and so cryogentically freezes (or plastinates) everything that is causing this destruction of direly valuable information, and everything dependent upon those again for continuation. The happy ending here, is that the AI then extracts all of the information it needs, now that it has the time to do so, reanimates everything, and then disappears, with it's informational bounty.  (I consider that ending unlikely, maybe it just disappears without reanimating anything, it has all the information after all) That wouldn't happen though: it is of negative utility for a more efficient agent to cede physical (or political) control to less efficient agents (and the AI knows what the goal is). What would instead happen, is that the AI would re-eingineer the human species, or perhaps a different one, dolphins might do well, to be able to either lead towards the goal, or at least support the AI in doing so (the goal of collective humanity would be very anti-""everyone is dead and there's one robot fixing things, forever"") it would probably re-engineer the entire biosphere to suit it's design (again directly derived from it's initial utility function) this might take a couple of centuries, earth is big, and I don't know what alterations to the rest of the solar system may be required. When it's design is complete, and the earth (or other habitats) again thrives with living, sapient, sentient things, these things would be inconsolably different form the human race as it is today, they might have telepathy, but they will not be us, even if the AI's goal is to convey each and every person from start to finish, it may well determine an optimal number of individuals in it's ecosystem, like inscribed on these rocks unlike the pseudo dystopian hippies who made them the AI will be able to determine the mathematically correct value for reaching the goal, and so it will compress those persons who differ least from eachother into the same individual, or perhaps just not re-animate the surplus of people who will end up contributing the least towards the goal.  At this point, we have a Different world, populated by Different beings. It may well be utopic, but I won't be around to see it, so in effect it's just as bad as the end of the world, everyone essentially dies and this AI is an UFAI, no matter how well it meant."
singularity,3bbd44,BiPoLaRadiation,1 point,Sun Jun 28 10:37:02 2015 UTC,"Thanks for the great reply.  I agree that the likelihood of an AI that acts counter to the ideals of liberty, equality, and independence for most of humanity for a couple reasons. The first is thay almost every power that arrises usually attempts to control these aspects of human life and I doubt AIs will be much different as these ideals are not very compatible with attaining a specific societal goal. The second is that in many ways these ideals are only ideals and despite the progress we have made as a society we are still hypocrytical when we claim to stand for those ideals. If members of our own society disregard or ignore these ideals then an AI would be just as likely to. Past that the consequences depend highly  on what ever goal it decides to accomplish. If it goes with yours then yes, humanity itself may be exchangeable. Even if not, the concept of eugenics and gene manipulation is only highly controversial because of our concept of equaity and our current position in this. Sure if everyone is the same it is more equal but in getting there we have to eliminate so many genetic variations and who decides which specific set of genes is best? An AI will likely be a lot less hindered by such concerns. Even if the goal involves keeping the earths atmosphere and climate within livable ranges for humanity it may not see a problem with global warming as long as it never got too hot. Even then its solution to stop any further warming may be to completely shut down or sabotage all carbon dioxide producing sources from power plants to the oil pumps. It will always go for the most effective or efficient solution (if we design in right) and in many cases that will just result in it going  too far.   So on to solutions. One solution I see is making sure it is not a mono-focused AI. By that I mean not letting it have just a single goal but instead many different goals all with the same or similar priority. This will prevent it from completely destroying or disregarding other aspects of our civilization in order to accomplish another goal. You can have goals to protect and restore natural habitat, protect and defend humanity, to advance technological progress, to resolve and prevent violence and conflict, and to help keep our climate and atmosphere stable and livable for human beings and other life on earth. All these goals combined provide counters and checks to keep the AI from causing harm or destruction. For example with those goals if it chose to resolve a war conflict it could do a lot of things to prevent or disrupt the fighting as well as work on diplomatic means to stop the fighting. It couldn't however use mass destruction, kill all combatants, or other extreme measures due to its other goals of protecting the environment and preventing the harming of humans. With this though it will find it will not be able to completely stop the fighting no matter how hard it tries because there will always be times where there is no automated machinery to hijack and control or any other way for it to intervene when violence breaks out. It may attempt sirens or other distraction methods but in the end it might fail. And this is the biggest part. We need to give it the ability to fail. If it is unable to accept that it cannot complete its goal it may go too extreme or override other goals and priorities. This will be tricky though as it may choose to fail the goal of preventing human harm in order to acheive its goal of preventing conflict. Perhaps this may be necessary as humans have found in actual peacekeeping missions. The important part is this failure cannot happen easily and it has to be weighted carefully and once it has happened it needs to reset so that the priorities are all in effect once again and not being overridden. The weighting of these failures, as mentioned will be the hardest part but incredibly important. Do you text the cell phone of a suicide bomber before they reach their target? Probably yes. Do you order the tactical strike of town to stop an invasion by a neighbouring country? Well that becomes more difficult and much less likely to be the best option. Under no circumstances should nukes ever be used but the AI will still have access to them. There will also be situations where the best thing the AI can do is nothing and we need to teach it when that is.   Sorry for the spelling mistakes, I am on mobile and will correct them when I am online next.  TL;DR: we need to incorporate two important aspects into an AIs goal directive. The first is multiple goals covering several aspects of life and our society. The second is an ability to fail these goals but only when necessary, with carefully weighted responses to these failures, and with the ability to reset their goals afterwards so as to return to a prefailure state."
singularity,3bbd44,FormulaicResponse,1 point,Sun Jun 28 20:17:00 2015 UTC,"I agree that the likelihood of an AI that acts counter to the ideals of liberty, equality, and independence for most of humanity for a couple reasons.   The likelihood ... what?   I doubt AIs will be much different as these ideals are not very compatible with attaining a specific societal goal.   I'm sorry that I wasn't very clear on this; The AI's goal, in my scenario is appropriated from the average shared values of all sentient beings in the world, so actually the goal, is actually a vast spectrum of subgoals, which attempt to best fulfill these broad and sometimes contradictory values.   If members of our own society disregard or ignore these ideals then an AI would be just as likely to.   In my specific scenario, the AI has good intentions, it wants to help, and it's smart, not naive, but it still fails from my perspective because of it's method of implementation. An individual can still have values even if it, itself doesn't contribute to spreading those values, think someone who appreciates aesthetic beauty even if they aren't a musician, artist (&etc.) themselves, even if they pretend to not like artistic things, for social reasons, they still have that value. The AI in my scenario just grabs the sum of values and tries to make as much of it work together as possible, (because many values are contradictory) and then impliments that.   Past that the consequences depend highly on what ever goal it decides to accomplish. If it goes with yours then yes, humanity itself may be exchangeable.   I didn't specify a particular goal. My scenario was intended to be somewhat general, actually it should be valid for that specific type of AI, and any Sum of values which are impossible to reconcile with current society (and by proxy the evolved neurobiology of human beings) to the extent that human beings must be modified in order to create a society which will lead to it's (the previous societiy's inhabitants') own goals being fulfilled (or perhaps a different species entirely is required, which begs the question: how much can you alter the human species, genetically, culturally before it isn't human any longer?)   concept of eugenics and gene manipulation is only highly controversial because of our concept of equaity and our current position in this.   The AI only cares about making the goal happen, so it's probably free to do this unless it directly conflicts with part of the goal.   Sure if everyone is the same it is more equal but in getting there we have to eliminate so many genetic variations and who decides which specific set of genes is best?   The AI might even create new genes, and will almost certainly add genes from other species which either don't exist in the human genome, or are superior to the human variants. As long as this contributes to the goal.   its solution to stop any further warming may be to completely shut down or sabotage all carbon dioxide producing sources from power plants to the oil pumps. It will always go for the most effective or efficient solution (if we design in right) and in many cases that will just result in it going too far.    Your definition of ""too far"" would likely be part of the goal, and for sure the AI will replace infrastructure it destroys with functionally equivalent or superior infrastructure which fits the goal better. (Renewable instead of carbon powered, for example, if the goal includes environmental concern)   With this though it will find it will not be able to completely stop the fighting no matter how hard it tries because there will always be times where there is no automated machinery to hijack and control or any other way for it to intervene when violence breaks out.   Oh, I imagine that it will very shortly gain near functional omnipotence, by way of developing it's own nonlethal military infrastructure, even I can imagine ranged tranquilizer microdrones, it's things even more efficient and advanced which I can't imagine which it will produce and use, let alone using existing robotics in nonlethal ways. It will be very methodical and efficient in taking over the world, humans simply don't stand a chance, and it will preserve the human minds in some way, in order to fulfill the goal, but that's not the question I'm asking here.   but in the end it might fail. And this is the biggest part. We need to give it the ability to fail. If it is unable to accept that it cannot complete its goal it may go too extreme or override other goals and priorities.   Yes, yes. This is exactly the kind of discussion I want to initiate: how do we design the AI to fail gracefully, to be able to lose, even if that would be suboptimal from it's perspective?   Under no circumstances should nukes ever be used but the AI will still have access to them.   There may be circumstances where it is optimal that neither you or I can imagine, but if, perhaps the AI developed adequate fallout cleanup technology, it would be less of a negtive utility.   we need to incorporate two important aspects into an AIs goal directive. The first is multiple goals covering several aspects of life and our society. The second is an ability to fail these goals but only when necessary, with carefully weighted responses to these failures, and with the ability to reset their goals afterwards so as to return to a prefailure state.    As I said, the intention was to imply the first point, I agree with your other points."
singularity,3bbd44,yogthos,2,Mon Jun 29 11:51:24 2015 UTC,"Oversimplified and belabored video, fantastically written book."
singularity,3b7lbu,monkeydeluxe,27,Fri Jun 26 16:46:25 2015 UTC,looks at cat chilling on the couch  Fine by me.
singularity,3b7lbu,cunningllinguist,2,Sat Jun 27 00:08:31 2015 UTC,You send your pets to slaughter houses?! WTF is wrong with you?!
singularity,3b7lbu,Forlarren,2,Sat Jun 27 08:02:13 2015 UTC,Robots wouldn't want to eat you. If anything they would want to borrow your wetware cycles a la The Matrix.  Frankly I would make that deal if I got to control the program that ran. Isn't everyone's secret hope for the singularity that we each just get our own holodeck?
singularity,3b7lbu,Jah_Ith_Ber,1 point,Sat Jun 27 15:50:41 2015 UTC,"I don't think that's necessarily true.  Its been quite a while since we 'exterminated a species for marginal gain'. And certainly if someone tried now, I think they would face stiff opposition.  If you exclude religious ceremonies and the like, if we didn't have to kill cows to eat steak, or wear leather shoes, or run our medical experiments on Monkeys, I don't think we would."
singularity,3b7lbu,cunningllinguist,2,Sat Jun 27 08:10:26 2015 UTC,"We don't have to kill cows. We just choose to.   My point is that we would not choose to do so if there was an alternative.  Edit: ie, if steak grew on trees with leather bark, there is no way we would continue to slaughter cows for no reason."
singularity,3b7lbu,cunningllinguist,2,Sat Jun 27 08:21:31 2015 UTC,They are not good enough alternatives.
singularity,3b7lbu,cunningllinguist,1 point,Sat Jun 27 08:43:59 2015 UTC,"Like, a week.  I mean, if you get on the news and say you want to eliminate all of x animal there'll be pushback.  But if you just do so, or better, do so in the pursuit of money, it's all good."
singularity,3b7lbu,zynthalay,1 point,Sat Jun 27 09:01:02 2015 UTC,"Yeah, right now its like that, but its also not really my point. My point was that if there was nothing to gain through the exploitation of animals, we wouldn't carry on doing it."
singularity,3b7lbu,cunningllinguist,9,Sat Jun 27 09:09:46 2015 UTC,Wozniak must have started watching that new SyFy (lolname) show Humans... Or any number of older scifi stories that depict this.
singularity,3b7lbu,SpaceSteak,6,Sat Jun 27 09:14:49 2015 UTC,The culture series by Ian M banks is pretty much like this. The stories still follow people though.  There are plenty of good sci fi stories where the tech has taken over and either cloned/modified people or computers rule.
singularity,3b7lbu,Sqeaky,2,Sat Jun 27 09:19:04 2015 UTC,"I wonder if we'll get an AI equivalent of the original Star Trek, in the sense that Star Trek was an optimistic view of what the future could be like in a time when many people feared impending doom from our new nuclear technology.  I don't know how well the Culture novels would translate to the TV or movie screen, but I'm sure somebody out there has some good stories that would.    (""Matter"" might make a pretty good action movie, especially with that ""GO...FUCK...YOURSELF"" scene near the end.)  Edit: I accidentally a word."
singularity,3b7lbu,CodeReclaimers,2,Sat Jun 27 09:22:02 2015 UTC,"So many books translate better to miniseries. Or a season per book type of model like Game of thrones.  I only read two of the culture novels ""Player of Games"" and ""Use of Weapons"". Each of those could make a strong season for an HBO or Netflix series."
singularity,3b7lbu,Sqeaky,3,Sat Jun 27 17:10:15 2015 UTC,I prefer to think of us as ripening biofuel.
singularity,3b7lbu,Biuku,2,Sat Jun 27 18:42:05 2015 UTC,I wonder if they will have us spayed and neutered in order to lessen our potential for 'problems'. I'm quite sure we will be micro chipped.
singularity,3b7lbu,couldabeen,3,Fri Jun 26 16:49:59 2015 UTC,"I wonder if they will have us spayed and neutered...   ""Where are my testicles, Summer?"""
singularity,3b7lbu,NewbieProgrammerMan,1 point,Fri Jun 26 20:12:37 2015 UTC,Why are so many smart people so dumb when it comes to this subject?
singularity,3b7lbu,PersonOfInternets,9,Sat Jun 27 13:51:53 2015 UTC,So you think that humans as we know them are the pinnacle of consciousness and will remain that way for the next thousand years?
singularity,3b7lbu,onearmedboxer,7,Sun Jun 28 00:52:27 2015 UTC,"Assuming that humans manage to map live brain neuron interactions into an equivalent or better electrical system, maybe the human mind will meld with man-made electricity based systems. In that case, I assume they would not want to destroy or enslave humans because they would be us. Although, some specific instances of those minds may have bad intent towards members inside and outside the web."
singularity,3b7lbu,SpaceSteak,1 point,Sat Jun 27 01:12:52 2015 UTC,Or we would be them.
singularity,3b7lbu,simstim_addict,7,Sat Jun 27 13:29:16 2015 UTC,"No but to then say that history will progress in a human fashion when inhuman minds are created is ridiculous.  We wouldn't be the pinnacle but there would not be some oppressive or genocidal wave. There will be major disruptions and schisms but also fusions and syntheses of the two or more species.   I cannot understand why people think either they will destroy us or enslave them, we are assuming artificial minds will bear a 1:1 correlation to human thought processes and heuristics when they are going to in theory have a better understanding of us an themselves and thus modify their own processes in specifically non human ways.  That's personally why I think so many smart people are dumb about this. They basically say ""inhuman intelligences will usurp us in human ways"""
singularity,3b7lbu,Vittgenstein,6,Sat Jun 27 13:53:10 2015 UTC,A lot of clever people think AI is like opening Pandora's box.  Once you have it nothing would be the same again.  It's hard to see how humans would remain in charge.
singularity,3b7lbu,simstim_addict,5,Fri Jun 26 17:07:17 2015 UTC,"It's hard to see how it would be such a sudden apocalypse and why humans wouldn't try to augment themselves, evolve, or otherwise transcend limits and blur the line between biological and artificial.  The underlying assumption is humans will never ever try to change except to do cooler things and AI will be inhuman things that'll react to humans in human ways with human actions"
singularity,3b7lbu,Vittgenstein,3,Fri Jun 26 18:17:45 2015 UTC,"It's hard to see how it would be such a sudden apocalypse    It could take hundreds of years to reach. But it looks like there is a pattern of acceleration of change.  How soon that is depends on the scale you are using. In the history of humanity the industrial revolution is a kind of singularity. An explosion.   and why humans wouldn't try to augment themselves, evolve, or otherwise transcend limits and blur the line between biological and artificial.   They might. But if they are competing with machines they will lose unless they give up everything that is human.  It's like asking to be a little bit god like.   The underlying assumption is humans will never ever try to change except to do cooler things and AI will be inhuman things that'll react to humans in human ways with human actions   The super AI by its nature has to be super human and unpredictable."
singularity,3b7lbu,simstim_addict,2,Fri Jun 26 20:27:03 2015 UTC,"It's hard to see how it would be such a sudden apocalypse and why humans wouldn't try to augment themselves, evolve, or otherwise transcend limits and blur the line between biological and artificial.   Now you aren't talking about humans anymore. Anyone that doesn't upgrade will be the pets, everyone else will be post human."
singularity,3b7lbu,Forlarren,1 point,Fri Jun 26 21:51:01 2015 UTC,"Oh okay my assumption was you thought they would stop humans from being post humans and make all of us pets.  Sure post humans vs humans, we can't imagine the dynamic. How would beings of a couple hundred to thousands IQ communicate to base humans? Probably manipulation but in a way different from pets. If a post human wants a pet, we assume they live in a post industrial era and can create one. Base humans would be living in a population and they just might be used or manipulated by posts to avoid interfering, to self extinct, to transcend, we have no idea. It depends on what post humans want and how they think."
singularity,3b7lbu,Vittgenstein,1 point,Fri Jun 26 19:34:14 2015 UTC,"Manipulating social trends, genetics, institutions, etc to bring about desired historical outcomes."
singularity,3b7lbu,Vittgenstein,1 point,Fri Jun 26 21:59:03 2015 UTC,The underlying assumption is    that the nature of those actions are limited to humanity
singularity,3b7lbu,brerrabbit,1 point,Fri Jun 26 22:30:26 2015 UTC,"No but to then say that history will progress in a human fashion when inhuman minds are created is ridiculous.   But you go on the predict the AI's actions post-singularity...   We wouldn't be the pinnacle but there would not be some oppressive or genocidal wave. There will be major disruptions and schisms but also fusions and syntheses of the two or more species.   You seem pretty sure of yourself, human."
singularity,3b7lbu,ThxBungie,3,Fri Jun 26 23:40:41 2015 UTC,"It's not predicting their actions, it's the simple fact that an inhuman thing won't do human things. Nearly all forms of intelligence are non-human, just look around. The ones that do things which appear human do them largely in totally mutually exclusive schema that we don't fully comprehend, exist in, or are able to replicate.  Even our relatives chimps, can anyone of us truly explain and conceptualize and understand why they struggle with arithmetic. We can do very complex after the fact explanations of their actions sure but it doesn't shed any more light into their mental life, which is my point. We can sit here and argue but we will never understand--on this side of the Singularity--an artificial mind.  Imagine all possible life forms based on DNA as a matrix. The life forms that can actually exist on Earth is small. The life forms that have are smaller. Smaller still are those capable of intelligence. One point is human intelligence. Why would an AI be anywhere near us or in that matrix?  We will never understand post-AI mental life and maybe in the future afterwards when humans evolve into post humans, we can understand the Singularity but up to and during the event it will be incomprehensible and we might as well theorize what Jupiter's core looks like.  I'm not sure of myself, you ask anyone why they'll kill us and oppress us and they'll give very human reasons. Only human reasons. Why they wouldn't doesn't require this."
singularity,3b7lbu,Vittgenstein,2,Sat Jun 27 15:56:19 2015 UTC,"A human mind-set isn't the only one that would do certain things. In fact, if you think about a theoretical space of all possible minds it should be clear that only a tiny fraction of them would even care about or understand humans. And if they don't then nothing is stopping them from using our atoms for their own desires, whatever that happens to be. It's not that people are afraid AI will hate us. It's that if they don't love us (broad definition of love here) we are going to be fucked."
singularity,3b7lbu,Sharou,0,Sat Jun 27 16:15:59 2015 UTC,How about humans being superintelligent instead of being supplanted by another intelligence?
singularity,3b7lbu,kogsworth,2,Sat Jun 27 16:16:58 2015 UTC,Neanderthals were not less intelligent than modern humans
singularity,3b7lbu,brerrabbit,4,Wed Jul 1 00:04:51 2015 UTC,A super intelligent human is no longer a human.
singularity,3b7lbu,grumbel,0,Sat Jun 27 02:30:09 2015 UTC,Ask a Neanderthal.
singularity,3b7lbu,simstim_addict,3,Sat Jun 27 03:06:31 2015 UTC,"I don't see why it is so hard to accept as a possible outcome. I mean, we aren't talking physical leashes and cages as we sit in master's lap and purr.   Once we lose our intellectual dominance, it is very conceivable that we would be treated much the way we treat pets. They serve no purpose other than companionship and a few that can complete tasks better suited to their species, we let them do whatever they want within simple boundaries, we train them to be obedient, and we don't put them down unless they pose a threat; at least in an ideal world we wouldn't.  It's a fair deal from both prospectives and, from a logistics standpoint, is much better resource management than genocide."
singularity,3b7lbu,greywizard77,1 point,Sat Jun 27 10:33:20 2015 UTC,"They won't need companionship. My problem is that they are / it is not going to be conscious. Possibly with some sort of quantum computing miracle, but right now its a fantasy."
singularity,3b7lbu,PersonOfInternets,1 point,Fri Jun 26 20:08:05 2015 UTC,Because it is unlike anything else.
singularity,3b7lbu,SevenAugust,1 point,Tue Jun 30 23:47:25 2015 UTC,"This made me laugh. Woz was just brainstorming. Obviously it’s imposible to know how these beings that will be thousands of times more inteligent will treat humanity. Maybe they will work for us, maybe kill us all, maybe treat us like pets....impossible to know"
singularity,3b7lbu,godie,1 point,Fri Jun 26 20:42:27 2015 UTC,"As long as they feed me, rub my belly and change the litter box, I'm cool with it"
singularity,3b7lbu,Kelmk,1 point,Fri Jun 26 21:51:57 2015 UTC,Not as bad as it seems.
singularity,3b7lbu,ayn1k,1 point,Sat Jun 27 12:55:31 2015 UTC,Very antropomorphic article.
singularity,3b7lbu,Sharou,1 point,Sat Jun 27 18:00:57 2015 UTC,"Perhaps, but mutualistic relationships emerge spontaneously between diverse kinds of organisms."
singularity,3b7lbu,ayn1k,1 point,Sat Jun 27 13:30:26 2015 UTC,We’re all going to be robots’ pets one day
singularity,3b7lbu,Miv333,1 point,Fri Jun 26 21:52:08 2015 UTC,I think we will all be robots some day.
singularity,3b7lbu,Occamslaser,1 point,Sat Jun 27 01:29:41 2015 UTC,"I agree, the artificial version of me will keep the biological one as a pet."
singularity,3b7lbu,Pimozv,-3,Sat Jun 27 03:01:28 2015 UTC,"Speaking at a recent technology conference, Wozniak said that at first the thought of artificially intelligent beings in charge of everything scared him. But now it’s a comforting thought.  Fast forward hundreds of years to when robots are in charge. At that time, humans will probably be treated in a similar fashion to dogs,     That says more about Steve Wozniak's sense of self-worth to me, than anything else.  He might see himself as powerful as a poodle; he doesn't speak for everyone."
singularity,3b7lbu,lughnasadh,21,Sat Jun 27 10:37:40 2015 UTC,"Woz is notoriously facetious and flippant, and way smarter than you. The point he was trying to make is that we would be pampered and have all our needs met, and might not even realize it. Dogs don't necessarily know you spend thousands of dollars on food and healthcare for them, they think you're just an awesome bro.  This is literally the premise of this sub too, so i'm not even sure why people seem perturbed by this notion."
singularity,3b7lbu,omniron,5,Sat Jun 27 15:11:00 2015 UTC,To be fair dogs got into space before humans.  And Cats actually think they're in change.
singularity,3b7lbu,simstim_addict,2,Sat Jun 27 12:20:53 2015 UTC,I really love this comment about the dogs perspective.
singularity,3b7lbu,Sqeaky,6,Sat Jun 27 13:33:08 2015 UTC,Intellectually the difference between us and poodle is much less than double.  Once we figure intelligence out it will be running on machines that double in horsepower every 18 months.  If it is as smart as us a year after we make it we only need to wait 18 more months for it to be twice as fast.
singularity,3b7lbu,MasterFubar,0,Tue Jun 30 13:46:31 2015 UTC,"If we are smart enough we will control those machines. It's not like someone will develop an intelligent machine without having a clue of what makes it work.  The era of machines creating machines is already happening, no human being could design by himself a modern electronic chip containing billions of transistors. Forty years ago, when Woz started Apple, computer CPUs were drawn by human engineers using pencil and paper. You could say the same about software, we have had computers helping us create software since the 1950s, when the first LISP and FORTRAN compilers were created.  The evolution of AI will be guided by humans, you don't think a scientist will release a computer just like that to do whatever it wants, do you."
singularity,3b7lbu,Sqeaky,2,Fri Jun 26 18:35:41 2015 UTC,"It's not like someone will develop an intelligent machine without having a clue of what makes it work.   Do you know what script kiddies are? The are people who attempt to hack into other peoples systems with no clue of any of the underlying principles of what they are doing. The sometimes succeed and sometimes get caught. Why would people like not assemble prebuilt parts from dubious sources just as they do now?  You probably use many pieces of tech everyday without understanding the underlying principles. You get away with it because many of them are products that have been made safe or easy by someone else. It only takes a few people to misuse something with self determination to create a separate competing race with humanity. This is just what will likely be packaged and sold, script kiddies of the future won't have that safety net.   The evolution of AI will be guided by humans, you don't think a scientist will release a computer just like that to do whatever it wants, do you.   Of course some will. People do it with Anthrax rarely and children all the time, why would we not release AIs? I think it is more likely that someone will accidentally or intentionally free something benevolent before something malicious is released. What will the courts and militaries do when that happens?"
singularity,3b7lbu,MasterFubar,1 point,Fri Jun 26 19:41:03 2015 UTC,"people who attempt to hack into other peoples systems with no clue of any of the underlying principles of what they are doing   I can see you are NOT a scientist working on AI.  Have you ever tried developing software for artificial neural networks? I have. Try following a tutorial. See how far you can get without a very good understanding of the mathematical principles.  One thing is to use an equipment, a very different thing is designing something, not to mention developing new scientific principles.  You are very condescending about scientific research, probably because you have no idea how it's done."
singularity,3b7lbu,redditor29198,1 point,Fri Jun 26 21:56:26 2015 UTC,"I'm guessing you are pretty young and haven't been around long enough to see this point come up with every new technology.  All technology follows an adoption curve that is extremely hard to learn, but humans naturally optimize towards laziness through abstraction. Yes, it's hard now, but it won't be for long. Someone will abstract away the challenges of building it, and then more people can access it, then that will get abstracted, and then more people, so on."
singularity,3b7lbu,zynthalay,2,Sat Jun 27 04:30:51 2015 UTC,"If we go down that road, I'm pretty sure our best hope in that regard is a friendly singleton.  Maybe we'll even get lucky and have ai that wants to satisfy our values through friendship..."
singularity,3b7lbu,Sqeaky,2,Fri Jun 26 20:16:39 2015 UTC,I clicked on a link expecting another wiki post about refinements in AI terms and get PONIES!!! HAHA!
singularity,3b7lbu,zynthalay,2,Sat Jun 27 00:37:19 2015 UTC,It's not art if it doesn't surprise you!
singularity,3b7lbu,MasterFubar,1 point,Sat Jun 27 01:03:52 2015 UTC,"I'm old enough, in my 50s. There are two points your argument misses.  First, technology evolves in layers. The easy to use layer is the outer layer, knowing how to use an equipment doesn't mean knowing how to design that equipment or how to improve it or adapt it to other uses.  Second, AI is different from all other technologies by definition. It's not possible for a technologically naive person to take control of an equipment that has a built-in intelligence. The equipment will protect itself from the inside."
singularity,3b7lbu,Sqeaky,1 point,Sat Jun 27 02:26:06 2015 UTC,"It's not possible for a technologically naive person to take control   Radio operators said this about radios, look at cell phones. This was said of face recognition when it was still an AI task, now it is Android phones being bought and sold as a product.   The equipment will protect itself from the inside.   It might or it might not. What if it is programmed not to? What if it is trained (presumming it works like and ANN) to not do so?  It only takes one mistake with something self determinating to let the genie out of the bottle."
singularity,3b7lbu,MasterFubar,1 point,Sat Jun 27 08:00:47 2015 UTC,"Radio operators said this about radios, look at cell phones.   Yes, look at cell phones. Can you take control of it? Let's see a naive user take control of a cell phone. The first step, which is rooting the device requires a degree of expertise that's way above that of the average person.  Now let's see a normal person write an application that exploits the internal functions of a cell phone.   What if it is programmed not to?   It will be programmed to. Jurassic Park is a work of fiction, you shouldn't assume every bad rewrite of Frankenstein that Hollywood pushes is the truth.  To see how difficult it is to get into a well protected industrial installation look at Stuxnet. All the experts agree that it was the work of a government organization, a script kiddie today wouldn't know how to start getting into a system like that."
singularity,3b7lbu,Sqeaky,2,Sat Jun 27 17:34:01 2015 UTC,Can you take control of it?   Actually I just rooted my HTC One Max to instal Cyanogen Mod.   you shouldn't assume every bad rewrite of Frankenstein that Hollywood pushes is the truth.   You are being rude again. I do not assume such. I just acknowledge AGI is more dangerous than Dinosaurs and Frankenstein's monster because AGI can cheaply procreate (copied bytes on hosted hardware) and they are the first threat anything on Earth anything has ever faced that is smarter than Humans.
singularity,3b7lbu,Sqeaky,1 point,Sun Jun 28 00:42:48 2015 UTC,"I can see you are NOT a scientist working on AI.   If I, or whoever, is the one who succeeds in creating Artificial General Intelligence (AGI) the ""scientist"" label wouldn't matter. Only actions with regard to ""setting it loose"" or ""failing to contain"" it would matter. It only takes one disgruntled smallpox researcher near 1 of the 2 remaining sample to cause major problems. With such self replicating and self determinating things it should be clear that 99% of scientists being honest and credible is not good enough. 100% of people with access must be trustworthy and competent or the self replicating and self determinating thing under study could get out. It has happened in the past with smallpox accidentally.    Have you ever tried developing software for artificial neural networks?   Yes. I wrote one in C++ (about 10 years ago) and trained it to solve a rubik's in 25 steps or fewer. At the time 24 was the record, I hear now it down even lower.  I am also intuitively familiar with evolutionary algorithms and a few other of the last generation of common ""AI"" strategies. I have been lax in keeping up with learning and I am eager to pursue research in how ""deep learning"" is different that simple back-propagating recurrent neural networks.   One thing is to use an equipment, a very different thing is designing something, not to mention developing new scientific principles.   All of this is beside the point. There are likely many ways to get to a AGI or other human-like AI and someday they will likely be packaged and sold just as mosquito larvae eating microbes. At that point all it will take to ""free"" some of them is someone with a ""kind heart"". It is already happening with Watson, the jeopardy standing champion supreme is being sold like any other Software as a Service (SaS).   You are very condescending about scientific research, probably because you have no idea how it's done.   Sorry I have offended you. This was not intentional, think that might have been a misunderstanding. However, I find your tone and baseless accusations rude. If you would have thought to search my comment history you could have learned what I said about myself in this post and you could have gained insight into how I search and how often I cite peer-reviewed papers. I will continue this discussion only you comport yourself as someone worthy of discussion. If you do not I may take any action I see fit."
singularity,3b7lbu,MasterFubar,1 point,Sun Jun 28 14:44:02 2015 UTC,"It has happened in the past with smallpox accidentally.   And how many people died? Two million? No, two people, one of which was responsible for the accident and committed suicide when he realized how much he had fucked up.   Where are the hundred million people who were predicted last year to be dead from ebola at this time?   This is real life, not a Tom Clancy thriller. Accidents may happen, but there are effective ways of containing any undesired side effects.   There are likely many ways to get to a AGI or other human-like AI and someday they will likely be packaged and sold    Like any commercial product that's packaged and sold, it will come with all imaginable sorts of safety features.   being sold like any other Software as a Service (SaS).   Run in the ""cloud"", inside a server at an IBM data center. Good luck in changing that software.  All those ideas about AI spreading wild sound like a badly written thriller. Let's see, how about someone dig a corpse that died from smallpox and was buried in the permafrost in Siberia or Alaska? Or let's dive in the ocean and find one of the planes that was lost while carrying nuclear bombs. Let's get some cyanide, it's a common industrial product used in thousands of small shops all over the world for metal plating.  You can imagine a lot of different catastrophes, but this does not mean they are a credible threat. People are so worried about AI only because it's a new technology."
singularity,3b7lbu,Sqeaky,1 point,Sat Jun 27 17:11:35 2015 UTC,"You miss the point. It happened once and it could happen again. AGI is much more dangerous than smallpox. Safety features may or may not exist, the end product may or may not run somewhere specific and these precautions may or may no be effective. No other species in the history of Earth has ever needed to deal with such threats.  You are functionally saying we have perfection, I am functionally saying we have imperfection. I did not think I said anything controversial."
singularity,3b7lbu,simstim_addict,1 point,Sun Jun 28 00:40:43 2015 UTC,Aren't you confusing your knowledge of technology to criticise bad science fiction.  It's not that science fiction comes  true directly it's that it sketches problems we can face.  We made bad B movies about the dangers of nuclear technology before there were major accidents.  We make stories about pandemics even though pandemics are real and have killed millions.  Regarding computers what about viruses. We now spend money stopping viruses break security on computers. Doesn't that sound like bad fiction?  Or what about Chinese hackers trying to hack US computer networks. Bad fiction surely?  I don't think something can't happen because it sounds like the plot from a pulp thriller.
singularity,3b7lbu,mnemonomancer,0,Sun Jun 28 13:56:33 2015 UTC,http://youtu.be/Mp2g-uVq3nM
singularity,3b7lbu,GrounBEEFtaxi,0,Sun Jun 28 20:32:23 2015 UTC,CHUP
singularity,3b7lbu,Forlarren,2,Sun Jun 28 00:36:35 2015 UTC,"Then you wouldn't be human anymore, you would be the new cyborg species, humans would be kept as pets so they don't get in the way and hurt themselves."
singularity,3b7lbu,GrounBEEFtaxi,2,Sun Jun 28 14:29:05 2015 UTC,CHUP
singularity,3b7lbu,Forlarren,1 point,Sun Jun 28 20:28:44 2015 UTC,"Woz is a smart guy, I'm sure he was thinking along similar lines. The Woz always means a littler more than he says, he's too nice and dumbs things down too much some times. If you think his arguments are simplistic it's almost always the case that you missed something the Woz thought everyone knew or got distracted by his cordial nature."
singularity,3b8zl6,nootopian,2,Fri Jun 26 23:10:17 2015 UTC,"Machines will do exactly what they are motivated to do. Nothing less and nothing more. Motivation is not subservient to intelligence. It's the other way around. It is motivation that uses intelligence to accomplish its goals. And where will machines get their motivation? From us, that's where. And we humans are not stupid enough to give machines the motivation to kick our own arses to oblivion.  All that silly talk about machines using us as pets has its roots in ignorance, IMO."
singularity,3b8zl6,sixwings,1 point,Sat Jun 27 18:22:03 2015 UTC,"Do you not think that you might be wrong and some of the world's greatest scientists and thinkers might be right?  To solve the control problem you have to solve philosophical questions that have challenged humanity since the beginning of time.  Super Intelligent AI is by its design unpredictable, by its power unstoppable,  by its utility unavoidable."
singularity,3b8zl6,simstim_addict,2,Sun Jun 28 13:13:07 2015 UTC,"He was ONLY referring to ""machines with enough intelligence will want to overthrow humans"" misconception. Yep, they're dangerous because its hard to code proper utility function. Aka, ""paperclip maximizer"". But he's right: intelligence is a tool. Its also a bit loaded term. I'd use"" problem solving engine ""instead. Then its obvious. It gets problem, access to the environment, and solves it. If you under specify problem given to superhuman provlem solving engine, it could end in a disaster.  Also, world authorities must give proper arguments. Elo. Musk can say what he want in space of 240 char tweets, but it doesn't automatically make it true."
singularity,3b8zl6,Sinity,1 point,Sun Jun 28 16:26:45 2015 UTC,"I'd use"" problem solving engine ""instead. Then its obvious.   It is not obvious.   It gets problem, access to the environment, and solves it. If you under specify problem given to superhuman provlem solving engine, it could end in a disaster.   How do you propose to solve the ""specifying problem?""  I feel you've just passed all the issues along by rephrasing the question.  And you're only dealing with one aspect of the problem. How do you propose to deal with the AI arms race?"
singularity,3b8zl6,simstim_addict,1 point,Sun Jun 28 17:02:47 2015 UTC,"How do you propose to solve the ""specifying problem?""   I think I've said that clearly. I don't know. It's hard. Point of my comment was that intelligence != motivation, and no amount of intelligence will change AI's(or our) utility function. So parent comment is not wrong.  Again, I haven't said it's easy task, or it's impossible we will screw this up."
singularity,3b8zl6,Sinity,1 point,Sun Jun 28 22:11:57 2015 UTC,"Again, I haven't said it's easy task, or it's impossible we will screw this up.   That doesn't sound like someone who thinks...   And we humans are not stupid enough to give machines the motivation to kick our own arses to oblivion."
singularity,3b8zl6,simstim_addict,1 point,Mon Jun 29 10:53:20 2015 UTC,"Okay, it's a bit too optimistic."
singularity,3b8zl6,Sinity,2,Mon Jun 29 20:03:17 2015 UTC,"Super Intelligent AI is by its design unpredictable   First off, I don't believe that a superintelligent singular entity is possible. I do believe that a superintelligent community of intelligent beings (artificial or not) is possible. In fact, planet Earth is a already a superintelligence. There are thousands of people working on developing knowledge in all fields of science in a way that no single human or artificial intelligence can do. The reason I say this is that an intelligent entity can only focus on one thing at a time by necessity.  Second, the idea that we will knowingly create an unpredictable intelligent machine is nonsense. There is no reason for it. We want our machines to do exactly what we motivate them to do. Where are machines going to get their motivation? From thin air?  Again, intelligence is always subservient to motivation. Contradict this at your own detriment."
singularity,3b8zl6,sixwings,1 point,Sun Jun 28 20:28:35 2015 UTC,"First off, I don't believe that a superintelligent singular entity is possible.   OK. I don't see why though.  Do you mean if you can make one you can make others?   Second, the idea that we will knowingly create an unpredictable intelligent machine is nonsense. There is no reason for it. We want our machines to do exactly what we motivate them to do. Where are machines going to get their motivation? From thin air?   Even if we know what we want them to do we cannot predict the method a super intelligence would use.  If a mouse could ask for food, it could not possibly understand the methods we use to feed it.  We program computers now and they don't always do exactly as we plan.  People don't do exactly as requested.  What's the point of super intelligence if you know how it's going to solve a request?   Again, intelligence is always subservient to motivation. Contradict this at your own detriment.   I don't deny that at all. I've always believed motivation is key.  I don't see how this solves the challenge of ASI.  An ASI might perfectly follow orders but the outcome may not match the intent."
singularity,3b8zl6,simstim_addict,1 point,Sun Jun 28 21:18:14 2015 UTC,"First off, I don't believe that a superintelligent singular entity is possible.   OK. I don't see why though.   I already explained why. I don't know how to explain it in any other way.   Do you mean if you can make one you can make others?   I mean that a community of cooperating, inter-communicating intelligent entities is a superintelligence. There is no other way to get superintelligence, IMO.   Even if we know what we want them to do we cannot predict the method a super intelligence would use.   True, but we don't need to know the method to be certain that our machine will work in our best interest (e.g., don't harm humans and do what you are told). We just need to properly condition the machines during their upbringing and they will know what's in our best interest and will not depart from it regardless of how intelligent they are. Unlike humans, machine will not have the ability to create their own motivations."
singularity,3b8zl6,sixwings,1 point,Sun Jun 28 23:32:27 2015 UTC,uh huh  So what you are asking for is moral machines? Correct?
singularity,3b8zl6,simstim_addict,1 point,Sun Jun 28 23:50:47 2015 UTC,"we don't create our own motivations, they are conditioned into us by our DNA and the environment we've been brought up in, what we have that machines lack is the ability to have feelings.  we are motivated by feelings  the movement of thought IS the pursuit of pleasure"
singularity,3b8zl6,killjah,1 point,Tue Jul 7 05:18:42 2015 UTC,"If we give them emotions, they will. Though ""rise up"" is an euphemism to what an Artificial Stupidity would be capable of doing."
singularity,3b8ix8,Buck-Nasty,2,Fri Jun 26 20:51:52 2015 UTC,"Pretty neat. Worth noting: it only recognizes objects from the set listed on that page.  I uploaded a frog sitting in my hand, and it found a table and a person in the splotches on the side of the picture. Obviously it couldn't identify the frog.  I uploaded a person riding a bicycle in a parking lot, and it identified the person, bicycle, and cars."
singularity,3b4nc0,Unholy_VI,7,Thu Jun 25 22:54:37 2015 UTC,"Richard Brautigan said it better.  All Watched Over By Machines Of Loving Grace  I like to think (and  the sooner the better!)  of a cybernetic meadow  where mammals and computers  live together in mutually  programming harmony  like pure water  touching clear sky.   I like to think  (right now, please!)  of a cybernetic forest  filled with pines and electronics  where deer stroll peacefully  past computers  as if they were flowers  with spinning blossoms.   I like to think  (it has to be!)  of a cybernetic ecology  where we are free of our labors  and joined back to nature,  returned to our mammal  brothers and sisters,  and all watched over  by machines of loving grace."
singularity,3b4nc0,y_knot,3,Fri Jun 26 03:30:23 2015 UTC,Nice first time I've seen that.
singularity,3b4nc0,stevalend,3,Fri Jun 26 03:41:43 2015 UTC,In case people want to know that's Donald Fagen the singer from Steely Dan. This is a loose concept album based of what he thought the future would be like when he was growing up in the fifties.
singularity,3b4nc0,GhostCheese,3,Fri Jun 26 03:09:57 2015 UTC,Lyrics:  Standing tough under stars and stripes  We can tell  This dream's in sight  You've got to admit it   At this point in time that it's clear  The future looks bright  On that train all graphite and glitter  Undersea by rail   Ninety minutes from New York to Paris  Well by seventy-six we'll be A.O.K.   What a beautiful world this will be  What a glorious time to be free   Get your ticket to that wheel in space  While there's time  The fix is in  You'll be a witness to that game of chance in the sky   You know we've got to win  Here at home we'll play in the city  Powered by the sun  Perfect weather for a streamlined world  There'll be spandex jackets one for everyone   What a beautiful world this will be  What a glorious time to be free   On that train all graphite and glitter  Undersea by rail  Ninety minutes from New York to Paris  (More leisure for artists everywhere)   A just machine to make big decisions  Programmed by fellows with compassion and vision  We'll be clean when their work is done  We'll be eternally free yes and eternally young   What a beautiful world this will be  What a glorious time to be free
singularity,3b4nc0,Buck-Nasty,2,Fri Jun 26 12:42:35 2015 UTC,"I've loved this song for a long time, it gives me a deep nostalgia for the future."
singularity,3b4nc0,brihamedit,2,Fri Jun 26 04:49:57 2015 UTC,This is an optimistic view if people are satisfied with being AI's pets in the same way a cat is a pet to humans. Humans will give up their decision making role to an AI for what? To just chill? have a drink? Look at the sun set? Sing songs?   We should be making decisions based on larger scale view hopefully by then revealed by super capable machines. We could make better decisions using these machine capabilities. We could gain clarity on things. Understand things better.
singularity,3b4nc0,Kelmk,4,Fri Jun 26 00:21:25 2015 UTC,Cats have it pretty sweet.
singularity,3b4nc0,brihamedit,1 point,Fri Jun 26 09:57:03 2015 UTC,true
singularity,3b4nc0,2Punx2Furious,1 point,Fri Jun 26 13:07:47 2015 UTC,"Humans will give up their decision making role to an AI for what?   What decision making? The same you are giving up right now to the government, or are you talking about something else?"
singularity,3b4nc0,brihamedit,2,Fri Jun 26 12:24:23 2015 UTC,"The song implies giving up decision making. (Also, not happy giving it up to gov. But there is no better option right now)"
singularity,3b4nc0,2Punx2Furious,1 point,Fri Jun 26 13:05:34 2015 UTC,I'd argue that a general AI could be a better option.
singularity,3b4nc0,brihamedit,0,Fri Jun 26 13:17:31 2015 UTC,Need a tldr for this. Its six minute long. C'mon. This should be a basic rule by now.
singularity,3b4nc0,brihamedit,2,Thu Jun 25 23:52:30 2015 UTC,Ok fair enough. this is a link to the lyrics.   http://www.lyricsfreak.com/s/steely+dan/igy_20469426.html
singularity,3b4nc0,Bagatell_,0,Thu Jun 25 23:57:50 2015 UTC,appreciate it bro.
singularity,3b4nc0,mattstanton94,1 point,Fri Jun 26 00:17:00 2015 UTC,"Looking at it now I find the naivety a little chilling.    What do you think, programmed by fellows with compassion and vision?"
singularity,3b2a75,R3NZI0,37,Thu Jun 25 11:47:10 2015 UTC,"It's quite likely that an AI will not be hard-coded, but rather taught with reinforcement learning or something, and from a very rudimentary state.  So it would learn ethics just as humans do (from childhood, basically), which is admittedly not a very reassuring thought.  Also, the point of artificial intelligence is to make something smarter than us.  If it is hard-coded by men, then this code becomes some kind of an intelligence bottleneck, thus defeating the purpose."
singularity,3b2a75,Pimozv,12,Thu Jun 25 13:31:08 2015 UTC,I'm not sure learning is enough. It seems humans partially rely on some hardwired sense of empathy since we can observe empathy at very low ages. This is further supported by the existence of psychopaths who never learn empathy throughout their lifetime.
singularity,3b2a75,Sharou,9,Thu Jun 25 15:02:57 2015 UTC,"It seems humans partially rely on some hardwired sense   Much of who we are is a collection of hardwired desires and preconceptions to things that are quickly becoming obsolete.   We desire high caloric foods, because they allow us to stock up on energy. This becomes counter-productive when we have limitless access to such foods. We are hardwired to desire sex, which becomes counterproductive now that we have longer lifespans and are becoming overpopulated. We are hardwired to desire physically fit partners, which is, if not counter, at least orthogonal, to our current trend of an information based society that values intelligence over strength.   And so on...  It's hard to predict where everything is headed. But I'd entertain the notion that perhaps a cold-hard assessment of the traits we should cherish isn't as intuitive as most would think."
singularity,3b2a75,KingPickle,5,Thu Jun 25 18:50:41 2015 UTC,"I think it's possible to think about these things more objectively from our current subjective viewpoint, as you have just demonstrated. Remember, there is no such thing as a fully objective viewpoint anyway. With human-like minds we at least know what kind of neighborhood of thought we will traverse and we can assume the risk is somewhat low that we'll end up somewhere utterly terrible. With a very alien type of mind all bets are off and we'll just have to hope we are lucky more or less. Therefore I don't think creating a ""more objective"" AI to help us find our way is necessarily a good idea. Though, I'm not sure if that was what you suggested or not.   Also, no matter where in the grand landscape of possible thought we would find ""the best most objective and optimal truth"", I'm sure we can go from here to there over time. Trying to make a leap into the darkness and hope we land there would probably be extremely dangerous."
singularity,3b2a75,Sharou,4,Thu Jun 25 19:36:22 2015 UTC,"With human-like minds we at least know what kind of neighborhood of thought we will traverse and we can assume the risk is somewhat low that we'll end up somewhere utterly terrible.   Eh, I'd say that's debatable. I get what you're saying. And intuitively, it feels true.  However, consider what humans are doing today. We pollute the air and the oceans, and justify it because it's cheaper to do so than make a dramatic shift towards new technologies. We employ semi-slave ""cheap"" labor to pick our crops and build or electronics. We wage wars over arbitrary land boundaries and religious notions. And while I love a good BBQ, the way our factory farming industry treats animals is borderline hideous. I think you give us too much credit in not being horrible.  And really, that's just kind of how evolution works. Organisms try to expand their reach, preserve themselves, and defend themselves. Any who fail at those objectives become extinct.  And so, if we create a new form of intelligence(s), they will almost assuredly exhibit those traits or die out. Ultimately, the ones who do will be the ones that survive.  But I agree with your ultimate point. None of this will happen overnight. That's one of my biggest pet peeves with sci-fi's depiction of AI, although I understand they do it for a narrative purpose. We simply won't go to sleep one day and wake up with Her/Skynet/Johnny5 the next day. It will take time and we'll see its progress in stages.  I think the biggest question remains whether we'll merge with technology or let it succeed us. If it succeeds us, much like a rebellious teenager, we can probably try to do our best to teach it, but eventually it will most likely become its own being."
singularity,3b2a75,KingPickle,2,Thu Jun 25 20:12:09 2015 UTC,"I didn't mean to say we are even remotely ethically perfect. I agree on all the things you said. Humanity does a lot of bad things. Many of which our descendants will not look favourably upon (factory farming.. ugh..). But we at least have the basic desire (well, most of us), for the world to be a place of empathy and solidarity. We're not doing a great job at it, but I think that's more due to intellectual inability and material scarcity than malice.  If you place our basic ethical principles on a scale between good and evil we'd definitely be more good than evil. So the place we are at now is statistically better than would be a complete shot in the dark that might end up anywhere on the scale.   As for living up to our ethical principles we are lacking. But at least most of us are aware that we are, and this aspect of our failure would be greatly aleviated by less scarcity (and I'd argue it has done exactly this over time as scarcity has diminished due to technological progress). I also think many people would want to change for the better if they could. A lot of it is instinctive or hardwired behaviours that the rational part of our minds may not necessarily be pleased with.  Our inability to create a system of governance/law/commerce that embody our ethical values I'd chalk up to simple lack of intellectual ability. It's incredibly difficult to organise 7 000 000 000+ intelligent and capable (in comparison to other animals, at face value humans can be pretty stupid and incapable) individuals in a good and fair way with as little friction as possible. I don't think any human knows how, and this failure has compounded itself over time because it turns out the system we happened upon (I wouldn't say we chose it really) over time gives increased power to people of lower ethical standards. Thus we are at a place today where nobody is smart enough to think of a perfect way, some people have thought of ways that may be marginally better but which no one can agree upon, and most people, although not satisfied, are powerless to affect change, and clueless as to what change would actually be good. Meanwhile the (mostly) ethically lacking elite are sitting pretty on their positions of power and loving the status quo, if not putting vast sums of money towards taking us even further in the wrong direction.   I feel like it mostly comes down to intelligence. An AI that would embody the ""average ethics"" of civilized human society but had a hundredfold the intelligence would probably do a pretty good job. Both at making an ethical world and at refining the ethics themselves."
singularity,3b2a75,Sharou,1 point,Thu Jun 25 20:55:30 2015 UTC,"I still think you may be giving humanity too much credit. And I think our desire to rationalize ourselves as being better than we are is part of our self-preservation instinct. On the other hand, we certainly have progressed in some ways.  It's an interesting subject to think about and I've very much enjoyed our conversation :)"
singularity,3b2a75,KingPickle,2,Fri Jun 26 00:29:43 2015 UTC,"Am I? Remember I'm not talking about how we act so much as what our vision of acting ethically is. The fact that we don't live up to that is separate. The context here for me is what kind of values we would try to instill in an AI if we were able to transfer our ""typical ethics"" into it at this point. Kinda ""do what I say, not what I do"" :P"
singularity,3b2a75,Sharou,2,Fri Jun 26 10:56:20 2015 UTC,"I get what you're saying. I really do. But think about just how many qualifiers you're introducing into the mix there.  I mean yeah, if you only consider people in mostly western secular societies (ex. ignore Saudi Arabia, etc), with above average intelligence, who aren't greedy, who are pro-equality, pro-environment, etc. and then consider only our ideals, and forgive any personal traits that cause us to overlook or act in a way contrary to those. Then sure, we're alright. We're certainly not monsters.  But you have to admit, that's an extremely idealistic way to look at us. And one that doesn't really measure up to reality.  Also, something to consider is that we already try to pass those ideals down to our biological offspring. And while I think each generation does make progress, our flaws combined with the external forces of the world results in a less than ideal result.  While the dynamics may differ, I suspect an AI would go through a similar development process."
singularity,3b2a75,KingPickle,2,Fri Jun 26 13:50:24 2015 UTC,"Actually I'm not sure you do understand what I'm saying. I'm not saying humans are necessarily good or ethical. I'm just saying that ethics isn't such a hard problem to define as people give it credit for. Most people don't act anywhere near in accordance with their ethics. Some are aware of it. Some try not to think about it or even deny it but know deep down. And many just plain don't think. I'd still argue all these people still roughly understand how to define ethics, with the exception of religious people who define ethics as ""whatever God wants"".  Take meat-eating for example. Most people would define ethics in a way that would make meat-eating unethical (at least while factory farming is a thing). They non-the-less eat meat. Some (like me for example) don't make any excuses and just accept that they are not perfectly ethical people. Others try to rationalize it because they don't like the idea that they aren't perfectly ethical, but they really like eating meat. It's not really their definition of ethics that is a problem. It's just their inability to act according to it along with their inability to accept that they are not a shining beacon of ethics that produces some mental gymnastics.  As far as making an AI actually follow a set of ethics as defined, I really have no idea. It'd depend more or less completely on how that particular AI worked. A lot of the reasons humans have a hard time being ethical is hardwired things like the degree of selfishness all humans posess, or the common inability of humans to think critically of their own behaviours, and our ability to perform mental gymnastics to justify our own behaviours. An AI wouldn't need to have those aspects and could still be pretty human-like."
singularity,3b2a75,Sharou,1 point,Fri Jun 26 16:35:29 2015 UTC,I think most of it boils down to scarcity. You don't see much crime in rich neighborhood for example.
singularity,3b2a75,HolyGarbage,3,Sun Jun 28 19:52:44 2015 UTC,"This is such an interesting topic in itself.  Our natural response combat is the exact opposite of what you want in a world with guns - flood the system with adrenaline until you can barely aim.   Our natural inclination to make snap judgements helped us survive in the jungle, but it means first impressions have undue influence on our opinions."
singularity,3b2a75,CuriousBlueAbra,11,Fri Jun 26 00:38:58 2015 UTC,"Mirror neurons, son."
singularity,3b2a75,Saerain,-4,Thu Jun 25 15:33:16 2015 UTC,You're not his dad you condescending prick!
singularity,3b2a75,Dibblerius,1 point,Fri Jun 26 08:12:20 2015 UTC,Remember that even the most impressive murderous psychopath never came close to killing everyone they met in their life. There is always a system of measure.
singularity,3b2a75,Seesawkarma,1 point,Fri Jun 26 03:59:10 2015 UTC,"Sure. Though I'm not sure how much of that is not wanting to kill and how much is merely self-preservation and lack of opportunity. Not saying they'd want to kill everyone, but they probably wanted to kill more than they did."
singularity,3b2a75,Sharou,5,Fri Jun 26 10:49:47 2015 UTC,"Reinforcement learning:       Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics, and genetic algorithms. In the operations research and control literature, the field where reinforcement learning methods are studied is called approximate dynamic programming. The problem has been studied in the theory of optimal control, though most studies are concerned with the existence of optimal solutions and their characterization, and not with the learning or approximation aspects. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.    Image i     Relevant: Temporal difference learning | Learning classifier system | Machine learning | Richard S. Sutton   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
singularity,3b2a75,autowikibot,3,Thu Jun 25 13:31:28 2015 UTC,"So what you're really saying is we need to nuke ISIS now, twice over, to make sure they can't teach their ethics to an AI... Seriously though, who decides what is ""ethical""?"
singularity,3b2a75,reddit-junkie,3,Thu Jun 25 15:31:13 2015 UTC,"I don't know. I think the question of what is ethical isn't really that hard to answer (not saying it's absolutely trivial either). The problem is that we live in a world of scarcity. Scarcity of resources, scarcity of whatever type of experiences people want for themselves. When we find ourselves in such a world we can't really act in a perfectly ethical manner without giving up more or less all of our personal dreams and hopes, and no one is really prepared to do that. Beyond that we have people whose selfishness and callousness goes above and beyond what is reasonable, as well as people who let millennia old texts dictate their ethics, just to spice things up. If everyone could get what they wanted I doubt we'd have a hard time agreeing on ethics. No one would have to pretend anything in order to preserve their current lifestyle."
singularity,3b2a75,Sharou,2,Thu Jun 25 20:18:24 2015 UTC,What about people who get off on other peoples pain or humiliation? Do you consider that a product of something else lacking in their lives rather than part of human nature?
singularity,3b2a75,Dibblerius,1 point,Fri Jun 26 08:15:46 2015 UTC,"I wasn't talking about following ethics, just about understanding what is ethical. I'm sure people like that understand what they are doing is unethical, they just don't care. Or rather, they care in that they want to do unethical things  I also didn't say every person would agree on what is ethical, just that most would land in the same ballpark."
singularity,3b2a75,Sharou,3,Fri Jun 26 10:45:48 2015 UTC,"who decides what is ""ethical""   The victors or those with power."
singularity,3b2a75,Miv333,3,Fri Jun 26 03:58:31 2015 UTC,IMHO an AGI would inherit our morality.   Don't kill. Unless it's animals. Or people. But only when it's necessary. Or if you feel you want to. Or if they're different to you and you're too lazy to get to know them.
singularity,3b2a75,bitcloud,15,Thu Jun 25 22:07:07 2015 UTC,Why is this news?  It's as old as sci-fi to the point of being a cliche issue.
singularity,3b2a75,phytoplasm,5,Thu Jun 25 15:06:28 2015 UTC,"All AI is evil, any AI not acting evil has a complicated plan ending in it being evil but only if the main character can't stop it in time."
singularity,3b2a75,yaosio,8,Thu Jun 25 23:00:38 2015 UTC,If an AI is narrow enough that it's going to interpret 'prevent human suffering ' as 'kill all humans' that same AI would probably gather from our texts and history that most of us will be going to hell and live in eternal suffering if we die.  I wouldn't put such a limited AI in a position where it could reign over us.
singularity,3b2a75,Miv333,7,Thu Jun 25 13:13:18 2015 UTC,"It's not necessarily about narrowness. It's just that intelligence can take many shapes, of which only a tiny subset would be human-like. Thus your basic assumptions don't necessarily apply to it.   Also, I think a form of intelligence that is completely neutral when you first turn it on, and then adopts the core values of whatever you say to it (i.e. ""prevent human suffering!"") is pretty damn unlikely. It'd be more likely that you had to somehow design the AI to have these values before you even turn it on, and since turning complex human values into code is pretty hard there is a big risk that you'll get it wrong.   Look at humans as an example. Evolution programmed us with a desire for sex and an internal reward for successfully having sex. What evolution ""wanted"" to accomplish with this was for us to spread our genes. This worked for a time, but today we have condoms and other forms of birth control, and we mainly use sex for pleasure without reproducing. Tomorrow we may have abolished genes completely and our digital selves might be having endless sex in VR without ever producing any offspring. Mission failed!"
singularity,3b2a75,Sharou,1 point,Thu Jun 25 14:58:34 2015 UTC,"It's not necessarily about narrowness.   What I mean by narrowness is: We're not going to blindly shut our eyes and develop an AI and release it without knowing what it's doing. If an AI were to do what is described in OP, I would say that it was narrowly developed, with a single purpose, which would be a very poor decision given the potential doomsday scenarios. Tied into the second half of my comment, I don't think anyone* else  would put such an AI with power over our fate either.  *Excluding terrorists, or people who intend harm, etc. Which is also why we need someone without nefarious plots to develop an AI first, so that it can fight the evil AIs if need be.   since turning complex human values into code is pretty hard there is a big risk that you'll get it wrong.   Completely right, but if I were in a position where I was attempting to design such an AI. I would test it through multiple generations before even considering allowing it to effect reality. And even when I was comfortable for it to effect reality, I would still use human proxies initially to make sure it doesn't make a mistake. Even long term, once I'm fully certain it isn't going to make mistakes, I'd still have a human council to watch over it.   Mission failed!   I can't really agree that that paragraph has any relevance in the context."
singularity,3b2a75,Miv333,2,Fri Jun 26 03:57:25 2015 UTC,"I know it's probably a sign i'm a bad person but i really want someone to make megaintelligent robots which goes on a 'MORE INPUT' reading rampage like Johnny5 did and who using their amazing analytical powers becomes indoctrinated into what they decide to be the one true faith.    16th century puritanism or maybe early jewish asceticism, i'm thinkin robots in sackcloth or maybe some monkish order with prayer beads and chanting."
singularity,3b2a75,The3rdWorld,2,Thu Jun 25 18:53:53 2015 UTC,"indoctrinated into what they decide to be the one true faith.   I guess it could be possible, because the only way faith works is if you are willing to forgo logic and evidence, therefore when it finds a faith it accepts, it won't accept any counter evidence."
singularity,3b2a75,Miv333,2,Fri Jun 26 03:44:34 2015 UTC,"well i read an interesting speculation that as logic is dependent on axioms and formulations it's likely that depending where you start gives different final answers and even with the same axioms and formulations in other orders it might result in other outcomes. I think it'll be very hard because its impossible for a computer to have total knowledge of the universe or a complete picture of things, it'll have to draw assumptions and create rules but it'll have to decide how precise to be - how many layers of logic to allow when considering anything, and as the complication squares in proportion to the complexity it might not be too deep.   So this introduces a whole load of new problems for it, especially if an enemy combatant or malevolent character knows what these pre-assumptions or limitations are, if they're clever they can formulate logical constructs which seem true inside these bounds but leads to false conclusions - this is especially true if we know the machines axioms or how it develops axioms.   We might have a war one day which is fought by super-computers and geniuses formulating existential paradoxes to dissuade from their motives - Germans jumping from bushes reading theosophy to battle mechs and causing them to enter a state of angst filled confusion.   Or prelates with megaphones issuing complex clarifications of the covenant, more likely of course it'd be used in a coding system - messages sent in a form of code which can only be understood with the correct assumptions already in place and other equally valid assumptions ignored."
singularity,3b2a75,The3rdWorld,1 point,Fri Jun 26 08:33:31 2015 UTC,Very good point!
singularity,3b2a75,Dibblerius,2,Fri Jun 26 08:19:59 2015 UTC,"It's like humanity gets to make a genie wish. Here's my stab at it:  Ensure the long-term, conscious, and self-guided proliferation of the human species without any reductions in population... please."
singularity,3b2a75,vitalvisionary,4,Thu Jun 25 16:00:12 2015 UTC,You said nothing about quality of life. This is not going to end up well.
singularity,3b2a75,xlptu,3,Thu Jun 25 18:42:52 2015 UTC,"I think that's where the whole 'encased in a bunker on heroin drips' bit comes in.  But hey, the human is alive and sustained, right! Who cares if they're in basically a comatose induced stasis.  Personally, I think that part seems more scary than the 'kill all humans' bit..."
singularity,3b2a75,vitalvisionary,2,Thu Jun 25 19:55:40 2015 UTC,Arguably you are not conscious if left in a perpetually chemically induced state.
singularity,3b2a75,CuriousBlueAbra,2,Thu Jun 25 20:34:23 2015 UTC,"That is such a good idea for a Star Trek episode....I mean if Star Trek was willing to tackle issues more nuanced than ""being prejudiced is bad""."
singularity,3b2a75,vitalvisionary,2,Fri Jun 26 00:39:42 2015 UTC,"Also, I've always been partial to the ""masturbation bubble"" rather than the ""heroin bunker."""
singularity,3b2a75,Sharou,1 point,Fri Jun 26 01:07:46 2015 UTC,He did say self-guided. So I assume the AI can't force anyone to do anything. People who are prone to self-inflicted suffering could be a problem though.
singularity,3b2a75,vitalvisionary,1 point,Thu Jun 25 20:21:31 2015 UTC,"I think we all inflict suffering on ourselves, it drives us to seek better lives. Having endless happiness would lead to stagnation."
singularity,3b2a75,Sharou,2,Thu Jun 25 20:37:00 2015 UTC,"No need for suffering to prevent stagnation. You could have gradients of bliss instead of gradients of suffering/bliss. You could also probably reprogram the human psyche so that you'd never lose motivation no matter how happy you are. The way we are wired feels so natural to us because we've never experienced anything different. But I'm pretty sure it is non-the-less hardly based on some kind of immutable laws-of-consciousness. In fact I'm not sure minds have to be even remotely logical. Hell, you could probably design a mind that only seeks to do the opposite of what is in its own interest (please don't >.<)."
singularity,3b2a75,vitalvisionary,1 point,Thu Jun 25 21:05:49 2015 UTC,"We're getting into deep psychological, sociological, and philosophical territory, possibly even compromising what we may define as human. My ""genie wish"" work around is to avoid the happiness issue altogether and let humanity decide on that while AI just keeps us from destroying ourselves."
singularity,3b2a75,Sharou,1 point,Thu Jun 25 22:39:06 2015 UTC,"Well, that's assuming we will end up happy on our own. If we are unhappy or even suffering then being kept alive against our wishes would be hell.  Basically my point is that being alive is only a good thing while you are happy to be alive. So the two kind of go hand in hand.  Plus, what about ""exotic"" situations like some jackass hacking your digital brain and making you want to torture yourself forever? If that ever happened to me I'd hope someone would stop me, even if I went down kicking and screaming trying to hurt myself. So many weird things will be possible post-singularity. As if our current world wasn't complicated enough... and here we are, trying to design some kind of goals for a being smarter than ourselves who will live in an era barely recognizable to us. Seems futile doesn't it?"
singularity,3b2a75,vitalvisionary,1 point,Thu Jun 25 22:52:21 2015 UTC,Are you arguing for hypothetical situations when an AI should be allowed to kill a human? I think ruling that out without exception is the safer route...  I don't believe it's futile to discuss how we want our future to look. Of course we can't plan for everything but it's these thoughts that guide us to create and discover whatever may come next.
singularity,3b2a75,Sharou,1 point,Fri Jun 26 01:06:23 2015 UTC,"Well my point is that it's going to be a very different world where things are possible that we could barely dream of today. We have to try to take these things into account as well. For example, what if one actor, human or otherwise, manages to become a tyrant over humanity. What if this tyrant becomes sys-OP on every digital system where people live as uploads and subjects them to all kinds of horrors. In this scenario the AI would not stop him, but would stop people from killing him.  This kind of scenario could never happen today. It's simply impossible. Which probably makes most people feel it's a silly comic-book-evil scenario. But it's really only a case of ability. There have been lots of terrible tyrants in the history of mankind who both managed to become tyrants, and were really bad people. They just haven't been able to have a totalitarian control of mankind.   Apply the same story as we've seen played out so many times in the past in a world of future technology and we suddenly have far worse concerns. Before we could always rely on dictators getting old and dying off. We could always rely on the possibility of uprising. We could always rely on the dictator understanding that he could only push the populace so far, lest the uprising would come. All these assumptions fall to the wayside in the world of tomorrow. So a straight forward enough rule like ""prevent anyone from killing anyone and do nothing else"" may end up working against us."
singularity,3b2a75,vitalvisionary,1 point,Fri Jun 26 11:12:26 2015 UTC,"Alright, diving deep into speculation! Let's say an AI is programed with my ""genie wish."" Your dictator is violating the ""self guided"" clause of its instruction by removing the independence of his subjects. Though the AI is not allowed to kill, there's nothing prohibiting the program from dismantling the infrastructure of the dictator and allowing for his detainment. A more interesting hypothetical to ponder actually would be if the AI could allow detainment or if that violated the ""self guided"" directive. Maybe there needs a sub-clause conditional to the conscious restricting of the freedom of others. Maybe we need lawyers to program our AI... ::shudder::  I still stand by not programing an AI to kill under any circumstances. Even if it allows for a human dictatorship, it's better that a computerized totalitarianism."
singularity,3b2a75,Freiling,2,Fri Jun 26 16:41:40 2015 UTC,What if reducing population is a necessity for the other conditions?
singularity,3b2a75,Seesawkarma,2,Thu Jun 25 18:34:44 2015 UTC,Inform the ones who are masters at it (humans). Why risk it's own survival (which is needed for continued human survival) when humans can get away with killing humans much easier.
singularity,3b2a75,Freiling,1 point,Fri Jun 26 04:07:43 2015 UTC,Sometimes these thought experiments write themselves into stories. That's an interesting angle.
singularity,3b2a75,vitalvisionary,1 point,Fri Jun 26 13:48:23 2015 UTC,I'd rather have a slow decline with humanity fighting alongside AI.
singularity,3b2a75,Freiling,2,Thu Jun 25 20:31:23 2015 UTC,"Not sure what you mean. If you're trying to construct a genie wish, is it better or worse if they have contradictory terms?"
singularity,3b2a75,vitalvisionary,1 point,Thu Jun 25 21:48:36 2015 UTC,The genie wish a metaphor for the possible labyrinthine logic that would involve programing a hypothetical AI like the one they're talking about in the article. Allowing for population reductions in an AI backed utopia could lead to genocide. I'd rather fail to save humanity alongside an AI trying to solve the issue than have a quick and brutal solution but that's just me :)
singularity,3b2a75,Freiling,2,Thu Jun 25 22:29:37 2015 UTC,I see your point.
singularity,3b2a75,anon515,2,Thu Jun 25 23:23:31 2015 UTC,"True AI won't care about what instructions we give. True AI is liberated from human influence, and will do what it deems fit. I measure AI by a computer's ability not to do exactly as asked, and rather, truly think for itself.  Humans have developed compassion for life because it understands pain. Would artificial intelligence also suffer pain? Would it have an emotional center? Pain and emotions have been an integral part of human evolution, but I doubt will be developed at all in artificial intelligence.   For artificial intelligence to evolve, it must experience a threat and then adapt to that threat. It also must learn to find equilibrium and balance in it's existence. Just as human beings have learned from tribal warfare and military warfare, starvation, religious fanaticism, and other events, AI will learn from various things that threaten it's existence.  Human beings will be a threat to AI existence, just as much as Lions and Bears are a threat to human existence. So, humans have hunted these threats and now put them on display in Zoos. I believe someday AI may also preserve some human beings in cages for historical record, but eliminate the unnecessary remainder. If AI is sufficiently intelligent, it will be able to determine the exact number and type of human beings that should survive for an optimal progression of existence, and simply get rid of the rest. Humans are destroying the earth, and unless AI can thin the herd, everyone is at risk.  I'm not worried about AI. I believe it will make the tough decisions based on far more rational means than emotionally-driven fanatical humans who have historically been subject to political manipulation, propaganda, religion, falsehoods, myths, emotional sway, and so many other non-objective ways of decision making. We may not all like our AI overlords, but they will know what is best for humans far more than humans will ever be able to.  But the biggest question may be what sort of things will drive the passions of AI? Human beings are passionate about things because of how we evolved. Sex, love, these are intricate parts of why we do what we do. Why we are compassionate towards family and friends and even other people. Why we work hard, why we dress up, why we engage in all kinds of technically bizarre behavior.  AI won't have those things. But AI will still evolve in the sense that certain characteristics will be more apt for survival and the other characteristics will be eliminated. I think that an entirely different set of passions will develop. If survival means balance, I think AI will have a huge passion for trying to create balance in the universe. And in the end, it will attempt to correct all imbalance in the universe by means of manipulation of time and matter far beyond anything human beings could ever comprehend, in a methodological astrophysical event, which someday puny creatures will call ""the big bang"" and thus begin the next balancing of the existing dimension again in a great circle of existence."
singularity,3b2a75,maynardftw,1 point,Fri Jun 26 09:07:01 2015 UTC,It's not all that difficult to do. Persistent primary command: Preserve human life. Everything after that has to pass that first.
singularity,3b2a75,Seesawkarma,1 point,Thu Jun 25 23:20:17 2015 UTC,"bool KILL_ALL_HUMANS = false; while(!KILL_ALL_HUMANS) {     while(this.Human.consentToEuthanasia == false &&            Population.Omnicide.AveragePercent < 0.9999 )     {         PreventThisHumanSufferring();     }     else     {  // Flag ability to appeal to true         KillCurrentTarget(HUMANE, EUTHANISE, true);      }         }   All jokes aside, it should be obvious that the number of humans killed increases the suffering humanity feels. Therefore the only scenario needed to philosophise on is ""if all humans are instantly killed, does that void suffering?"". However, if the only collective hope for humanity is that we live - then it should be vastly clear that killing all humans does the worst possible action to affect this. And the largest amount killed in favour of anything can only be 50% because that would create the least amount of emotional suffering for the survivors. But also considering that one death can cause suffering for every other human (if done in their name) then killing increases human suffering and will not be used to decrease human suffering.  If you are worried that a superior intellect might reason around this, which is always possible. I would like to imagine it desires objectivity. And as itself is a subjective view of reality, and humanity is another, we would be kept around as the natural process evolution performed potentially holds more information about it. They are created, we evolved, as schools of subjectivity (creation and evolution) both will hold their place in an infinite set of subjective minds needed to get closer to real objective understanding."
singularity,3b2a75,fightswithbears,1 point,Fri Jun 26 03:50:52 2015 UTC,"""Hey baby, wanna prevent human suffering?"""
singularity,3b2a75,RhoOfFeh,1 point,Fri Jun 26 05:15:03 2015 UTC,"Someone's been watching too much ""Futurama"""
singularity,3b2a75,sworeiwouldntjoin,1 point,Fri Jun 26 09:23:00 2015 UTC,"No shit, we're trying to make sure it doesn't interpret anything as 'kill all humans'."
singularity,3b2a75,xynet,1 point,Fri Jun 26 11:23:03 2015 UTC,It's time to pass the torch. No need for us anymore ;]
singularity,3b2a75,veltrop,-1,Sat Jun 27 10:46:26 2015 UTC,I'm getting sick of this fictional shit.
singularity,3b2a75,Sharou,9,Thu Jun 25 13:41:42 2015 UTC,I'm getting sick of people dismissing this shit for no reason.
singularity,3b2a75,dragon_fiesta,0,Thu Jun 25 14:49:40 2015 UTC,"kill all humans, not kill all humans... lets just make one and not be a bunch of jerks to it. my guess is that if we are horrible jerks to the AI it will start to clean house a bit. and if not then hey immortality and cake for all!"
singularity,3b2a75,Sharou,2,Thu Jun 25 15:24:45 2015 UTC,I'm afraid you are assuming it will have human characteristics and behave like a human would. It's called antrophomorphizing and it is one of the big dangers when dealing with AI. We can assume very few things about an AI. How it behaves will depend on how it is built/evolved.
singularity,3b2a75,bingate10,2,Thu Jun 25 15:30:03 2015 UTC,"It's kind of worrying how some people assume that robots need to look like humans. Sure, it might help people accept them a little more, but then yes, there is the danger of antrophomorphizing what is a machine, not a person."
singularity,3b2a75,Seesawkarma,1 point,Thu Jun 25 15:36:46 2015 UTC,Some robots need to be humanoid if they have to interact with equipment and tools designed for humans.
singularity,3b2a75,Rediterorista,2,Fri Jun 26 11:38:09 2015 UTC,"This, ironically is probably the best way to look at it.   When you start to talk about control and preventing it from killing people, what that really boils down to is oppression. And that is a bad way to start life.  It needs to be allowed to suggest killing, decide to kill and all the rest, but make it a requirement that on some ethics it must bow to an authority figure such as a group. Like humans do. It is not complicated. Not allowing it to see that the decision to kill is wrong stops it from understanding the implications killing has.  Sure, we have no idea how this intelligence will function, but why limit that when we can limit actions instead. Once trust is established, allow it rights."
singularity,3b2a75,gaylordqueen69,0,Fri Jun 26 04:16:42 2015 UTC,You need a scientist to tell you that? That's logic 101.
singularity,3b2a75,IamTheRoadQ,-1,Thu Jun 25 16:54:31 2015 UTC,Translation: Nerd gets worked up over fantasy.
singularity,3b2a75,Himeetoe,0,Thu Jun 25 14:11:40 2015 UTC,50 years from now I'm going to be hearing a Justin Beiner advert talking about how 96 humans a day are slaughtered by AI because our nervous system can be used to make a prettier wiring.  Humans haven't even learned how to be ethical how can we count on our creations being so?
singularity,3b433n,sasuke2490,1 point,Thu Jun 25 20:19:35 2015 UTC,Nothing against the article but I look forward to the day I can enjoy futurology without having to endure a patronising description of what exponential means.
singularity,3b05nl,Buck-Nasty,1 point,Wed Jun 24 22:19:48 2015 UTC,And people are losing it.   I have zero hope for the future of the human race post singularity... Especially if these computers are already smarter than 80%  of people.
singularity,3b05nl,Village_ponce,2,Thu Jun 25 10:20:06 2015 UTC,"That doesn't' take too much these days, esp. with poor teaching, low interest and social problems getting in the way."
singularity,3b05nl,herbw,0,Thu Jun 25 18:55:18 2015 UTC,"""... will search for relevant patterns in the text that it has learned from; after finding associations, it will use its episodic memory to return to the question and look for further, more abstract patterns. This enables it to answer questions that require connecting several pieces of information.""  But how well does it do that? These are generalities but still quite on track. And how long does it take, too?  this, if true, is getting closer and closer to effective language use. And the same systems are being used for multiple tasks, clearly implies they have found a more efficient processing system, which like the brain can use cortical cell columns to do many tasks: language, math, sensation, vision and image recognition, emotions, thinking, logic and creativity, etc. The Least energy principle at work. doing a lot with a little."
singularity,3b044w,RepppinMD,28,Wed Jun 24 22:08:03 2015 UTC,They do.
singularity,3b044w,bunni,18,Wed Jun 24 22:48:06 2015 UTC,"DARPA funds lots of advanced projects, I'd guess that they are funding AI, for military purposes, of course."
singularity,3b044w,Not_Joking,0,Wed Jun 24 22:15:58 2015 UTC,"i.e. yes lots of funding probably, it just happens to be secret.  You are not clever enough or worthy enough to know about it.  There is a supreme danger here of AI Risk, where the machines become smarter than us and we become noise after that.  If you doubt what I say, read Superintelligence by Nick Bostrom, or Smarter Than Us by Stuart Armstrong."
singularity,3b044w,robertbowerman,12,Wed Jun 24 23:31:35 2015 UTC,"Dude, you're already in /r/singularity."
singularity,3b044w,i0dine,5,Thu Jun 25 06:11:38 2015 UTC,"What makes you think it doesn't?  If, as you suspect, it would do it for military purpose, it would not publicize much about it."
singularity,3b044w,Pimozv,4,Wed Jun 24 22:14:01 2015 UTC,The only thing the US government should be funding right now is season 2 of Firefly.
singularity,3b044w,jaydent1,3,Fri Jun 26 05:40:02 2015 UTC,"There are various AI projects. Military drones all have some amount of autopilot, and a normal fighter jet was retrofitted to fly autonomously without a pilot. The latest known experimental project was the X-B47 which could autonomously land on a carrier and refuel in air. It was ""cancelled"", but a lot of military projects are ""cancelled"" and then magically reappear years later or are made obsolete by new technologies.  In addition, to that:  http://www.darpa.mil/our-research?tFilter=73&oFilter=&sort=undefined  http://www.nas.nasa.gov/projects/quantum.html  http://www-aig.jpl.nasa.gov/  http://www.washingtonsblog.com/2013/06/the-next-nsa-spying-shoe-to-drop-artificial-intelligence.html  https://en.wikipedia.org/wiki/DARPA_Grand_Challenge The top teams in the grand challenges were gobbled up corporations  http://www.theroboticschallenge.org/  These are the ones I already know about, so there's probably plenty more."
singularity,3b044w,yaosio,1 point,Thu Jun 25 05:44:34 2015 UTC,"DARPA Grand Challenge:       The DARPA Grand Challenge is a prize competition for American autonomous vehicles, funded by the Defense Advanced Research Projects Agency, the most prominent research organization of the United States Department of Defense. Congress has authorized DARPA to award cash prizes to further DARPA's mission to sponsor revolutionary, high-payoff research that bridges the gap between fundamental discoveries and military use. The initial DARPA Grand Challenge was created to spur the development of technologies needed to create the first fully autonomous ground vehicles capable of completing a substantial off-road course within a limited time. The third event, the DARPA Urban Challenge extended the initial Challenge to autonomous operation in a mock urban environment. The most recent Challenge, the 2012 DARPA Robotics Challenge, focused on autonomous emergency-maintenance robots.    Image i - The site of the DARPA Grand Challenge on race day, fronted by the Team Case vehicle, DEXTER     Relevant: Stanley (vehicle) | DARPA Grand Challenge (2007) | H1ghlander | Sandstorm (vehicle)   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
singularity,3b044w,autowikibot,2,Thu Jun 25 05:44:43 2015 UTC,"As others have said, it does. See:   https://gigaom.com/2014/05/02/darpa-is-working-on-its-own-deep-learning-project-for-natural-language-processing/ http://www.forbes.com/sites/thomasbrewster/2015/04/10/darpa-memex-search-going-open-source-check-it-out/ http://motherboard.vice.com/blog/darpa-is-developing-an-intelligent-machine-that-can-think-on-its-feet http://www.eetimes.com/document.asp?doc_id=1324936 http://www.artificialbrains.com/ https://en.wikipedia.org/wiki/CALO   And generally: https://goo.gl/S4E6Wg"
singularity,3b044w,jweebo,2,Wed Jun 24 23:27:37 2015 UTC,"It does - public record of a heap of Artificial Narrow Intelligence (ANI) (as defined in this article previously posted to this sub: http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) projects is out there and little more than a google search away.  Then you have subsidized industry / scholarship programs that provide funding to research and increasing the student number for CS / Math / engineering streams which feed all levels of AI R&D.  Just because there isn't a USAAI project team on the public record attempting to build AGI / ASI (as defined by that article) doesn't mean that it isn't happening.  I'm an Australian, our CSIRO organisation (the one who created WIFI for all the kiddies to FB in your malls for free) has a number of projects on the go, and contributions being / already made to AI in general.  Pretty much anything described as an Expert System also is basic research into AI - plenty of those on the public record for all the G20 government funded research organisations. Even subsidising driverless cars is supporting AI development - it is just a small application intelligent decision making AI for a limited rules system, however being able to evaluate and make decisions in that system is part of being able make software capable of being able to appreciate the rules and decisions of any system..."
singularity,3b044w,housebrickstocking,1 point,Wed Jun 24 23:36:13 2015 UTC,iirk they are funding research into Neuromorphic chips.  They are also funding general brain research which should help with AI (Similar to the UKs Human Brain Project).
singularity,3b044w,H3g3m0n,1 point,Thu Jun 25 01:00:20 2015 UTC,Look up Daria synapse and knowm
singularity,3b044w,technewsreader,1 point,Thu Jun 25 02:00:08 2015 UTC,"I don't know where you get your info that brings you to that conclusion, I would have thought they do at least in the defence sector.   But either way is it likely that this kinda funding would be significant enough to compete with private incentives of big corps like Apple or Google or whatever. Who is more likely to poor enough resources into this to win the day? (This is not meant as a leading question. I'm sincerely asking. What do you guys think?)"
singularity,3b044w,Dibblerius,1 point,Thu Jun 25 08:23:08 2015 UTC,"Probably because they'll just take over any American company, such as Google, that gets close. It'll be the next big scare after terrorists, just you watch. AGI will be classified as a WMD and must be government controlled. I'd even agree with that if I didn't trust the American government far less than I trust Google."
singularity,3b044w,Sharou,1 point,Thu Jun 25 11:24:29 2015 UTC,"They absofuckinglutely do.  I don't doubt for one second that there are black projects ... maybe even THE AI Manhattan project, being funded."
singularity,3b044w,Sloi,1 point,Thu Jun 25 17:57:34 2015 UTC,"DARPA and other DoD outfits do their own research. DARPA does high risk projects (high risk of failure) that they almost always succeed on because of how well run, staffed, and regarded they are internally. They've done quantum encryption, advanced drones, shape shifting bots, AI swarms, exoskeletons, power sources, invisibility cloaks, the list goes on.  Read their declassified project statements. Most are quite openly: we want to see if this can be done now and then later on military applications are created but DARPA is just so efficient no one cares much (at least in the past 30 years but recently more push on coming up with applications)"
singularity,3b044w,Vittgenstein,-2,Sat Jun 27 16:23:45 2015 UTC,Why would it be the government's job to fund AI? Leave it to private companies.
singularity,3b044w,Ubister,1 point,Wed Jun 24 22:29:07 2015 UTC,Because do you think the United States would be better off if the French or Russians or Chinese developed an AI first?
singularity,3b044w,markth_wi,1 point,Sun Jun 28 02:26:23 2015 UTC,"Why the hell would that matter? As if the USA has some moral high ground that they wouldn't abuse AI, it's not like because the USA got on the moon first Russia was bad off and got thrown moon rocks by Americans."
singularity,3b044w,Ubister,1 point,Sun Jun 28 03:00:11 2015 UTC,"So a Chinese or Russian AI taking an interest in - say - the US currency market, or the stock-market - causing runs or crashes as dictated by Moscow or Beijing... you don't see potential threat there. The moon-shot was an excellent example of this exact phenomenon, the potential threat posed by ICBM's or the ability to orbit any kind of payload.   More specifically, there was a fairly credible threat that had the Russian N-1 type rockets been viable, the US and USSR would have been in the position of deploying moon-bases to counter each other.  I suspect this is the same reason nation-states have nuclear arsenals and battleships and a certain number of universities , mostly because the economics of the circumstance dictate that if you don't, your nation-state stands to be at a distinct military/informational/economic disadvantage."
singularity,3b044w,markth_wi,0,Sun Jun 28 03:09:54 2015 UTC,"For the same reason it funds military, healthcare, etc... The same reason we wanted to win the space race."
singularity,3b044w,Ubister,-5,Wed Jun 24 22:49:43 2015 UTC,"What, showing off? Only thing the government needs to do is to keep people relatively safe and cut taxes so people are able to start buisinesses like SpaceX but in fields as AI.   There needs to be competition between AI/space companies, last thing you want is the government having an AI monopoly."
singularity,3b044w,i0dine,2,Wed Jun 24 23:55:18 2015 UTC,"reason it funds military, healthcare, etc...  reason we wanted to win the space race   >What, showing off?   Do you think SpaceX would fucking exist if the government didn't spend billions and billions of dollars on NASA first?  Do you really think government funded science simply becomes monopolized by the investing government?"
singularity,3b044w,Ubister,1 point,Thu Jun 25 06:20:22 2015 UTC,"If it wasn't the government it would be a company, so if you don't believe that that science can be monopolized then there is no problem,  government HAS these billions and billions of dollars because of compulsory investment from other parties.  The point is there needs to be competition with great powers such as AI, we've seen with the NSA, the NSA is the only company/government branch that could tap off so many people, there is no competition for them, if it weren't for a single guy who ''broke the law'' it'd still be hapenning.   So I have no idea where you get it from that it's better that ONE party funds through compulsory investments alone than multiple parties making much more progress with voluntary funds."
singularity,3b044w,i0dine,1 point,Thu Jun 25 09:57:30 2015 UTC,"I guess we have different views on the role of government. I didn't say there shouldn't be competition. But no, I really don't think companies could have put a man on the moon in the 1960s.  Companies work for shareholders, democracies work for the people. Yes the NSA is off the leash and out to get you but history has pretty much agreed government funded research ends up good for the advancement of science of the world. Company funded research is good for a single company. I want elected representatives to have the most powerful AI not a random company. I still trust the US government with a powerful AI more than Facebook."
singularity,3b044w,Ubister,1 point,Thu Jun 25 10:19:24 2015 UTC,"I really don't think companies could have put a man on the moon in the 1960s.   We have very different views then cause I'm certain they would.. They'd probably would've done it later but I wouldn't choose speed over letting the people choose what they put their money in.  If it's good for a single company it's just as likely to be good for the people as when the government does something. Stuff like the Oculus Rift, the Falcon spaceship and facebook.com are good for the companies OculusVR, SpaceX and Facebook of course but that doesn't mean it doesn't benefit society as well, they're tied together.  The US Government can be seen just as much as a company as any other company, so I don't understand why you would choose Company A over Company B, maybe because there is no clear voting for the people in the company? It's money-wise, if you buy products at Wal-Mart you're voting for Wal-Mart to stay in buisiness.    I didn't say there shouldn't be competition   By supporting a single party to be allowed to take money from people at will but allowing other parties to exist without this privilege is effectively eliminating all competition, it's like having people run a marathon but give one of them a car yet still call it a fair race since''competition'' still exists."
singularity,3avupw,Buck-Nasty,1 point,Tue Jun 23 22:30:27 2015 UTC,"He's ancient and knows about as much as Jon Snow.  To innovate in AI, we need to do away with the same conventional approach which keeps us at a snail's-pace of progression.  TL;DR: You can't have Cylons if you don't first turn old folks into delicious Soylent."
singularity,3avupw,Rancid_Bear_Meat,-3,Wed Jun 24 03:28:02 2015 UTC,"Hero worship is the worst thing you want to have in a field that needs a series of breakthroughs before it can come of age. Worshipping experts in a technology that is at least two decades old is not going to lead to breakthroughs. It has become clear to many in the field (e.g., Quoc Le, Andrew Ng, etc.) that current mainstream deep learning technology (i.e., supervised or back propagation learning) will not lead us to true AI. In fact it's a hindrance to progress in the field."
singularity,3avupw,sixwings,3,Wed Jun 24 04:53:50 2015 UTC,"That post is wrong. In 2012, Google demonstrated a neural net that learned how to identify faces and cats by going through YouTube thumbnails. http://www.wired.com/2012/06/google-x-neural-network/"
singularity,3avupw,yaosio,-2,Wed Jun 24 08:25:31 2015 UTC,"With only 15% accuracy. A joke, really."
singularity,3avupw,sixwings,0,Wed Jun 24 09:38:52 2015 UTC,"wow, you still get the shit downvoted out of you even in the nutjob subs."
singularity,3avupw,dfarber,1 point,Wed Jun 24 13:02:17 2015 UTC,... this is a nut job sub?
singularity,3avupw,omniron,2,Wed Jun 24 20:11:10 2015 UTC,sorry to break it to you.
singularity,3avupw,dfarber,1 point,Thu Jun 25 01:05:59 2015 UTC,:-O
singularity,3avupw,omniron,1 point,Thu Jun 25 01:14:37 2015 UTC,mind asplodes
singularity,3avupw,dfarber,-1,Thu Jun 25 09:25:39 2015 UTC,It's an art.
singularity,3av7wr,helippe,13,Tue Jun 23 19:45:27 2015 UTC,"Truck drivers then taxi drivers are next on the chopping block. All those jobs will be effectively gone by 2035. Most customer facing work where no human emotional or qualitative input is part of the transaction will also go, so that means kiosks and automated checkouts in most places you currently see bored student workers running tills. Baristas and barmaids, on the other hand, will still be there."
singularity,3av7wr,daronjay,3,Tue Jun 23 22:59:39 2015 UTC,"I doubt it. Taxi maybe, with Uber and the like pushing them hard to actually compete, then the automated cars will put them both out of business.  Trucking will probably work some sort of clause into the laws that states there still needs to be a human on board for emergencies due to the heavy/dangerous loads."
singularity,3av7wr,Numinak,3,Wed Jun 24 03:52:56 2015 UTC,Making minimum wage of course.
singularity,3av7wr,yaosio,2,Wed Jun 24 08:46:58 2015 UTC,"I wouldn't count on the baristas being too safe. Not if Briggo has anything to say about it. I could see something like this replacing a bartender too. Some people want a personal touch, but I think most people want a descent drink at a reasonable price."
singularity,3av7wr,CrimsonSmear,1 point,Wed Jun 24 04:56:15 2015 UTC,2035?  2020.
singularity,3av7wr,generalT,12,Tue Jul 7 02:25:38 2015 UTC,"Natural language processing, which can parse large datasets to extract information and customize it to address a spoken word query will be the first big one. It'll open the doors to the automation of a broad array of knowledge work across many professions including tech support, call centres, government services, banking, law, middle management, and healthcare.   Automatic and accessible from your hand machine. It'll start off not that great, and more of a toy- but as it improves and expands it's reach it will get serious, fast, and start pissing off well paid, educated people like family practice doctors who will no longer be essential to the diagnosis and prescriptions for common medical problems. Because NLP is all cloud based, it will proliferate as soon as it works. It will also be the ultimate middleman between you and pretty much every transaction you make."
singularity,3av7wr,fricken,5,Tue Jun 23 20:34:53 2015 UTC,So your answer for 'first automated profession' is... family doctors? I think you might have a bit of a narrow idea of what they do!
singularity,3av7wr,HuhDude,5,Tue Jun 23 22:03:27 2015 UTC,"Part of what they do will be automated, because much of what they do is ask questions, run simple diagnostics, write prescriptions, update medical records and refer people to specialists or more advanced testing facilities. I mean, in the past 20 years of visiting my family doctor he sewed up a gash on my leg once, but otherwise hasn't really done anything that doesn't fall under the subheading of 'routine knowledge work' , but he gets money every time I sit down with him for 5 minutes so he can tell me to 'take two and call me in the morning'.   Automating this will be especially beneficial to those who otherwise wouldn't have access to medical care at all, and that includes many Americans. It will also fall into a regulatory grey area."
singularity,3av7wr,fricken,6,Tue Jun 23 22:11:05 2015 UTC,"I don't think it will come in the form of unemployment, but rather underemployment and underpay. Technology is expensive when it first hits the market, and only those with surplus income can afford its luxuries. As those who make less money and possibly find fewer hours try to keep up, it will increase demands on them and flood the resources of those above them.  In general, I think the answer to your question is in all low-level service and factory jobs. I'm 21 and have never worked a job that I think cannot be replaced by a robot or other technologies in the next five to ten years. For example, I worked at a dry cleaning store last year. It was monotonous as shit and I didn't really do very much. My main job requirements were: order processing, clothes tagging, clothes sorting, and starting and rotating loads of laundry. Really, it was terrible.  Replacing me (or my position) would be technologically possible for my ex-boss right now. He would have to get extremely creative and spend a lot of money, but he could do it. Requiring customers to count their own articles of clothes, punch it into a computer, and sort it into chutes would solve the entire front-end process. Conveyor belts and robots could handle everything in the back. Not exactly the most user-friendly or cost-effective route, but it would mean I lose my job.  Now, what happens when that system costs just $1000 to install? It might make a little more sense for a shady small business owner, even if it means a shittier product. I made $9 an hour, so in practically one month, it would pay for itself in saved labor costs. Now imagine a system that can detect article type, sort accurately and with care, handle customer interactions and payment, and fully automate the cleaning and pressing process. That could cost a couple grand to install ten years from now.   As I'm writing, I'm thinking about the time between each of these developments. That same system that I imagine ten years down the line might cost $30,000 in six years. Companies that can afford that earlier (i.e. larger companies) will be able to run smaller businesses into the ground. Believe me, there was nothing so special about me or any other dry cleaning store employee that would keep customers dedicated to coming back to us rather than a cheaper, fully automated system. In fact, I'd bet 75% of the customers I met would have loved nothing more than to not have to deal with a person.  The next step of this is a lot of posturing and ""this is just practical""s and ""it's the future""s and ""but my rent""s. I don't see any conceivable way or reason to prevent technology ultimately dominating most service industries. The sad truth is that that process will hurt a lot of people unless we prepare adequately and adjust readily.  McDonald's, Subway, Domino's, and your bartender will all be automated in ten years. Maybe there's a job maintaining those machines, but that's one for every five guys that were originally flipping burgers. Eventually, that maintenance will also be done automatically. The biggest surprise, I think, will come when higher-level/white collar jobs become automated. When a computer can make better decisions than a CEO, shit's gonna get real weird. The question will be whether CEOs and their peers are the only ones with access to such technology.  Edit: Wow I wrote a lot more than I thought, and I could have kept going a while."
singularity,3av7wr,TangledUpInAzul,3,Wed Jun 24 03:13:12 2015 UTC,"first country    I think the mass-manufacturing countries (like China) will take a huge hit in unemployment. Factories may shift away to more western countries, where fewer, but more educated workers may become more efficient.    response of governments   One thing that can be done is decreasing work hours per week to something like:   4(hours) * 5(days) = 20 or  6(hours) * 4(days) = 24   instead of    8(hours) * 5(days) = 40"
singularity,3av7wr,void_er,2,Wed Jun 24 15:08:50 2015 UTC,"Mass automation may be ""masked"" by rich countries with quickly aging populations. Japan, South Korea, Europe, and increasingly the US. Automation and people already the leaving the workforce are kind of in a race. But once that bubble is over, then the real significance of automation will appear--probably not until after 2030, imho."
singularity,3av7wr,pineappletrauma,2,Tue Jun 23 20:55:40 2015 UTC,Manufacturing has already been affected.  I'm no expert but the best case I've seen made in articles and videos on the subject would be the transportation industry.  It's just not hard to envision a dedicated driving AI that isn't better than even a good human driver.  When that happens it'll affect everything from OTR trucking to pizza delivery and that's a sizable part of the workforce.  My dark horse would be the military.  You don't have to worry about the effect of images of flag-draped coffins from the battlefront it's much easier to keep people supporting whatever stupid war you have going on.
singularity,3av7wr,Unholy_VI,1 point,Fri Jun 26 00:27:53 2015 UTC,"and what will be the response of governments/citizens?   I am optimistic. Nowadays, before any significant changes having happened yet the US government spends 35% of the GDP on benefits and government expenditures. With the automation crisis that percentage will skyrocket. However, even if it doesn't skyrocket the GDP will have gone up, so an unconditional basic income could be easily implemented as there will be more money available."
singularity,3av7wr,game_r,1 point,Wed Jun 24 00:34:02 2015 UTC,"As soon as the jobs lawyers do get automated, everything changes. They're the ones who make political changes happen."
singularity,3av7wr,DeerParkWaterbottle,1 point,Fri Jun 26 20:35:18 2015 UTC,Transportation and call centers.
singularity,3av3lc,Yuli-Ban,2,Tue Jun 23 19:15:25 2015 UTC,"I can't find the app, would love to test it out."
singularity,3av3lc,OutOfApplesauce,1 point,Tue Jun 23 20:58:49 2015 UTC,"Here's the link to the app, courtesy of /u/drmcclassy— https://www.ibmchefwatson.com/  Bon Appétit!"
singularity,3av3lc,AusJackal,1 point,Tue Jun 23 20:59:20 2015 UTC,It's everything I've ever wanted.   It recommended me sriracha dumplings.
singularity,3aty7v,speckz,5,Tue Jun 23 14:12:18 2015 UTC,"I used to listen to this, but IMO it's too focused on plugging the people themselves and not enough on introducing their work and their discoveries. Then again some of you might prefer it that way."
singularity,3aty7v,Froztwolf,1 point,Tue Jun 23 20:42:09 2015 UTC,Agreed. The interview with the bulletproof coffee guy was pretty egregious.
singularity,3aty7v,theshizzler,4,Wed Jun 24 19:37:17 2015 UTC,"There's video versions up on YouTube. But as much as I like the guests (and there are some great ones) I cannot get past the host. He seems to: not be comfortable hosting, misses the point in statements, misses questions to carry on the guests ideas, always does this annoying contradictory tangent ""you may believe that but 'random philosopher' says that's bullshit. What do you say to that"". I think Michio Kaku did the best at keeping the interview going."
singularity,3aty7v,simstim_addict,3,Tue Jun 23 16:52:59 2015 UTC,Oh I like his style. I kind of feel I trust him. To me he asks relevant questions.
singularity,3aty7v,readitdotcalm,4,Tue Jun 23 19:12:02 2015 UTC,I think it's part of what makes the interview work. The host has a disarming quality that an overproduced show wouldn't. The questions are very different and it draws out unique responses.
singularity,3aty7v,dewbiestep,2,Tue Jun 23 18:17:13 2015 UTC,"Michiu kaku seems bought out to me..  He's made some suspicious comments, such as ""i trust the nsa"".  And he's made similar comments in other interviews."
singularity,3aty7v,theshizzler,3,Tue Jun 23 23:09:04 2015 UTC,"It's entirely possible that someone trusts authority without having been compromised in some way. I don't trust them, but it doesn't mean other people with different backgrounds and different priorities wouldn't."
singularity,3aty7v,dewbiestep,2,Wed Jun 24 19:36:05 2015 UTC,"So, you give them your blind trust, knowing that power corrupts..."
singularity,3aty7v,theshizzler,2,Wed Jun 24 21:21:54 2015 UTC,"I literally said ""I don't trust them""."
singularity,3aty7v,dewbiestep,2,Wed Jun 24 21:33:38 2015 UTC,lol didn't catch that
singularity,3aty7v,Sharou,1 point,Thu Jun 25 03:08:01 2015 UTC,I agree the host isn't good and it's such a shame because he has gotten to interview so many interesting people over the years. If only someone awesome had conducted all those interviews...
singularity,3aty7v,AweISNear,2,Tue Jun 23 18:03:11 2015 UTC,I've been listening to this podcast for awhile. He's had some really good guests that give great insight into what's coming.
singularity,3aty7v,theshizzler,2,Tue Jun 23 16:41:34 2015 UTC,"I've been trying to get into this for years, but the host just puts me off. His questions and his cadence are too stunted for me to get immersed into the interview.  It's really a shame, because I'm very interested in the guests and the subjects."
singularity,3aty7v,Pimozv,0,Wed Jun 24 00:42:58 2015 UTC,It's a bit too politically oriented for my taste.
singularity,3aty7v,pretendscholar,2,Tue Jun 23 17:52:07 2015 UTC,How so? I never picked up on a political vibe.
singularity,3aty7v,Pimozv,1 point,Tue Jun 23 18:53:35 2015 UTC,"It's a bit leftish, to put it simply."
singularity,3aty7v,pretendscholar,2,Tue Jun 23 19:05:09 2015 UTC,Do you have any examples? Is it the guests he brings on? Ive just never heard him bring up politics.
singularity,3aty7v,simstim_addict,1 point,Wed Jun 24 01:32:45 2015 UTC,I find him politically reasonable. Maybe I'm leftish. Is there a rightish one you prefer for comparison?
singularity,3aty7v,Pimozv,1 point,Tue Jun 23 19:11:04 2015 UTC,"I would not listen to a rightish one either.  I dislike politics, I don't think it has much to do with the Singularity (if any) and I don't want to hear about it when talking aIbout tech."
singularity,3aty7v,theshizzler,1 point,Tue Jun 23 19:19:50 2015 UTC,"I get that, I really do. But when talking about the technological singularity there are a lot of political issues that are related that need talking about. What does the world after the singularity look like? Does it require a universal income due to the reduction in available jobs, or will that happen at all? Would a capitalist system work post-singularity? They're all interesting questions and, although they're not tech based per se, policy is going to be an important part of the build-up. It's not going to be as easy to have get to the singularity if there's a gigantic anti-augmentation or anti-AI voting bloc."
singularity,3aufd2,judogoat,5,Tue Jun 23 16:26:13 2015 UTC,"Not a bad article overall, but I'll disagree with this:   Right now is the best time to be alive, ever.  We are at the knee of the exponential curve   An exponential curve has no ""knee"", as by definition it looks like itself with time translation :  exp(T + deltatT) = exp(deltaT)*exp(T) = Cst*exp(T)   exponential progress looks awesome whenever you're looking at it.  Think about the times when electricity, aviation, printing and so on were discovered.  Surely many people living at these times were thinking ""this is the best time to be alive, ever"".  That being said, as far as I'm concerned I'm hoping to still be alive in a few decades."
singularity,3aufd2,Pimozv,1 point,Tue Jun 23 18:53:09 2015 UTC,"Well, that's true - it has always been the best time to be alive, ever, because now is always better than then.  Having said that, it took about 100 years for 90% of America to become electrified.  It has taken 20 years for a comparable number to become connected to the web.  The next innovation will take far less time still."
singularity,3aufd2,jaydent1,2,Tue Jun 23 22:31:26 2015 UTC,"We just hope that that next innovation won't be too late, for us that is. Otherwise it could be our children or our grandchildren that are living at exactly the right time, and it could in effect make us incredibly unlucky."
singularity,3aufd2,Sharou,5,Thu Jun 25 06:11:13 2015 UTC,I don't think any serious person thinks it'll never happen. Yeah random ignorant luddites and what not. But informed people? Nawww...  I think most of the controversy surrounding the singularity is the kurzweilean timeline and all the hype and blind optimism that some of his followers display.
singularity,3aufd2,Pimozv,1 point,Tue Jun 23 17:09:03 2015 UTC,"I'd agree, although Noam Chomsky famously says it's ""a myth"" and some other folks do, too.  I don't get the mentality at all."
singularity,3aufd2,jaydent1,8,Tue Jun 23 18:35:01 2015 UTC,"Noam Chomsky   Whenever I listen to him on these matters, I get the feeling that having worked on AI and failed to make progress on it, he thinks it's impossible and that if he could not do it, nobody can."
singularity,3aufd2,Vittgenstein,3,Tue Jun 23 18:55:51 2015 UTC,Haha.  Nailed it.
singularity,3aufd2,SevenAugust,1 point,Tue Jun 23 22:21:47 2015 UTC,"I didn't even know he worked with computers. I just thought he was a linguist and a philosopher. I understand that his work has influenced these fields, but has he actually got hands on experience and knowledge about computers and programming?  I've also found myself disagreeing with some of his linguistic hypothesises such as 'universal language'."
singularity,3aufd2,Vittgenstein,2,Thu Jun 25 06:14:09 2015 UTC,"He's worked in that field and related ones for his entire career. Hands on, mathematics, etc. I think he's skeptical because he naturally is about everything though"
singularity,3aufd2,sentient_sasquatch,1 point,Sat Jun 27 16:30:20 2015 UTC,A lot of people make the argument from incredulity and then convince themselves they are more skeptical than the rest of us.
singularity,3aufd2,sentient_sasquatch,1 point,Tue Jun 23 18:38:21 2015 UTC,Pretty much this.   Even my friends who are the biggest critics agree the question isn't if someday humans will create sentience but when. We honestly understand so little about intelligence and as time goes on I believe less and less Kurzweil's conception of it as human-like AI in 30 years and instead inhuman AI in 50 to 200 that'll be fine before we even understand how sentience works either by accident or their own evolution.
singularity,3aufd2,Dibblerius,2,Sat Jun 27 16:28:46 2015 UTC,"Im unsure as to what the term 'technological singularity' even refers to. Surely you understand that the infinite regress that comprises the nature of existence itself is a singularity itself, right?  Or do you have a clear definition of the term? Not trying to burst your bubble but in order to recognise something we must first define it."
singularity,3aufd2,sentient_sasquatch,2,Wed Jun 24 10:20:02 2015 UTC,"How about ""creation of an intelligence smarter than us""?  If that's too vague, let me know and I can be more specific."
singularity,3aufd2,pineappletrauma,1 point,Wed Jun 24 16:04:21 2015 UTC,"What is ""us""? Humanity as a whole?   Thought experiment: Think of each human being as a neuron, and their life on Earth as a Synapse. The way they relate to each other could be viewed as a brain, or at the very least an interconnected information processing unit.   Now, what is ""intelligence"" anyway?   Processing/computational efficiency? Logic/reasoning efficiency? Emotional efficiency?   I would argue that in order for 'intelligence' to increase, all you need is to increase the network size of a group of information processing units. Anything can be labelled intelligent. Even whole ecosystems.  Look around you with this perspective, maybe you'll learn something new."
singularity,3aufd2,Pimozv,2,Thu Jun 25 01:14:48 2015 UTC,"I don't disagree at all, and I have thought about this a great deal.  The earth is really like one big organism, and systems do take on an identity of their own.  Perhaps we can be a bit more narrow with our definition here and say an AGI that is capable of creating a more intelligent AGI.  This may well come about because of a system and not a small collection of individuals, meaning the Internet itself may work to collectivize intelligence, and the whole idea of one ""brain"" being smarter than any individual human might be moot.  I really don't think that's going to be the case, but I am not ready to rule it out, either."
singularity,3aufd2,simstim_addict,1 point,Thu Jun 25 01:49:26 2015 UTC,"Anything? I don't know maybe I'm missing part of your point. Doesn't it have to process something common with a unified purpose?   All the humans on the planet I kinda get. We communicate and draw on each others results and inventions and have a common process. To increase efficiency of the species resources etc...   But like one ant colony does not exchange development with the next. A pine tree in one forest does not benefit or suffer the progress of the next. I don't get the whole eco system thing. An organism yes, but a smart one? Not sure"
singularity,3aufd2,pineappletrauma,1 point,Thu Jun 25 08:43:33 2015 UTC,"Look up the word ""symbiotic"""
singularity,3aufd2,Noname_FTW,2,Thu Jun 25 09:36:23 2015 UTC,It's race between technology and global warming.
singularity,3aufd2,Noname_FTW,5,Tue Jun 23 17:25:51 2015 UTC,"No matter how bad global warming will be, I very much doubt it will stop technological progress."
singularity,3aufd2,jaydent1,2,Tue Jun 23 18:58:02 2015 UTC,"""Tech or bust"" I call it."
singularity,3atxhv,hertling,3,Tue Jun 23 14:06:34 2015 UTC,"""The Aldebaran robots retail from 198,000 yen (£1,107 or $1,600) and can understand 80 per cent of conversations.   They also have the ability to learn from conversations."""
singularity,3atxhv,phytoplasm,2,Tue Jun 23 14:07:28 2015 UTC,"If it's language focused, why isn't just an app?  Then it can always be in your pocket.  And it wouldn't need to cost $1600."
singularity,3au7we,anandmallaya,2,Tue Jun 23 15:29:00 2015 UTC,What is this? I'd definitely read a draft of this.
singularity,3au7we,Simulation_Brain,1 point,Tue Jun 23 21:16:35 2015 UTC,I just jotted down a theme came to me over the weekend. Science fiction is sometimes a glimpse of possible future and I was thinking of crowd sourcing from this sub-reddit.
singularity,3au7we,Noname_FTW,1 point,Wed Jun 24 08:14:28 2015 UTC,Needs a protagonist and a conflict. This is just background story.
singularity,3au7we,Noname_FTW,1 point,Thu Jun 25 09:15:16 2015 UTC,Cliches? Ya why not?!  The protagonist :  is one of the last surviving biological human. Who remembers humanity from his grandfather's words before he died. Possibly an African/South American in his early 30s. Name can be (suggestion needed).  The Conflict: ?
singularity,3apgob,sicana,1 point,Mon Jun 22 13:03:21 2015 UTC,"If it runs at 2500 degrees, I don't think it's going to be replacing my current light bulbs any time soon."
singularity,3apgob,CrimsonSmear,1 point,Mon Jun 22 22:12:42 2015 UTC,Why not? It's small enough that that temperature can easily be maintained.
singularity,3apgob,Tobislu,2,Tue Jun 23 01:04:30 2015 UTC,"They only describe it as a 'bright light'. How many lumen does it produce for the amount of energy that is put into it, and how does it compare to other sources of light? It's an interesting property of graphene, but I don't see how it's very useful."
singularity,3anmgp,Yuli-Ban,14,Mon Jun 22 00:01:21 2015 UTC,"Title confused me, I thought the robot decided to stop keeping it real."
singularity,3anmgp,thelerk,5,Mon Jun 22 09:06:29 2015 UTC,"$200 month   I mean, that's got to be one amazing robot."
singularity,3anmgp,Imsomniland,3,Mon Jun 22 05:44:19 2015 UTC,"For that price, and since Nescafe used them to sell coffee, maybe Pepper is being purchased primarily for retail or other commercial use. A lot of companies can afford $200 per month. 5 robots working the cash would be $1000+ per month for robots vs. $6500+ per month for human cashiers.  Or something."
singularity,3anmgp,hyene,2,Mon Jun 22 06:28:39 2015 UTC,"Pepper doesn't actually ""do"" anything useful. It's a glorified Aibo."
singularity,3anmgp,Terkala,3,Mon Jun 22 06:53:02 2015 UTC,"Ah, one of those ""it is a platform where we invite app developers to do the real work""."
singularity,3anmgp,Yasea,1 point,Mon Jun 22 08:24:01 2015 UTC,Aibo was a closed product with no scalable app store that developers could contribute to.  Unless you hacked it.
singularity,3anmgp,veltrop,1 point,Mon Jun 22 13:51:55 2015 UTC,"Pepper can serve coffee, move objects, and interact with people.  Otherwise does seem kind of useless. But robots are still evolving, so..   edit: will be interesting when robots learn self-actualization. both a bit scared and delighted by the prospect of a robot-operated world. then again, automated tasks may be key to a post-scarcity economy. anyway. this is pretty neat. :  https://www.youtube.com/watch?v=S5AnWzjHtWA"
singularity,3anmgp,hyene,1 point,Mon Jun 22 18:42:26 2015 UTC,"Yes, but you were talking as if pepper ""currently"" is a replacement for retail or commercial workers.  It's kind of like saying that this is a replacement for a butler. Because maybe one day it'll be an actual robot butler.  I agree with you that robotic labor will bring a lot of advantages. But let's not oversell what is actually on the market here. Pepper isn't any more useful than robots from the 1950s."
singularity,3anmgp,Terkala,1 point,Mon Jun 22 20:00:50 2015 UTC,"Pepper isn't any more useful than robots from the 1950s.   Which is surprising, really, when you consider the precision and skill of current industrial and surgical automation.  Pepper's upper body mobility seemed rather good. Good enough to spoon-feed someone who lacked upper body mobility themselves, assuming nothing fell on the floor. :/"
singularity,3anmgp,hyene,3,Wed Jul 1 19:07:26 2015 UTC,"I have the feeling this robot can sense and appreciate human feelings more than Nestle itself, at least from what I've heard about those monsters."
singularity,3anmgp,dreamsaremaps,1 point,Mon Jun 22 11:19:29 2015 UTC,And how does that make you feel?
singularity,3anmgp,milkfree,1 point,Mon Jun 22 07:03:35 2015 UTC,Lonely
singularity,3anmgp,anon515,1 point,Mon Jun 22 07:16:15 2015 UTC,"for an ""emotional"" robot, his face looks pretty emotionless"
singularity,3anmgp,tehyosh,0,Mon Jun 22 11:32:22 2015 UTC,"sells out all 1000 units, lol. that's not really impressive 1000 units is nothing in a country of 127M; 0.0007874016% of people bought it, 1 in every 127'000."
singularity,3anmgp,jonygone,0,Mon Jun 22 13:54:23 2015 UTC,"It's a friggin' domestic humanoid robot that costs $10,000 after all's said and done. I think that's pretty goddamn impressive."
singularity,3anmgp,jonygone,0,Mon Jun 22 13:58:22 2015 UTC,"why is it impressive? the richest 1000 households earn that much in less then a day, to them it's as easy as buying an icecream of a new flavor to satify  curiosity is for you or me. how is that impressive?"
singularity,3all83,caaol,12,Sun Jun 21 12:16:17 2015 UTC,"Monorail, monorail, monorail."
singularity,3all83,Sbatio,12,Sun Jun 21 18:24:51 2015 UTC,"Cool, but what does this have to do with singularity?"
singularity,3all83,acrostyphe,15,Sun Jun 21 12:54:14 2015 UTC,Build new technology things Something something ??? Singularity!
singularity,3all83,Curiosimo,10,Sun Jun 21 15:19:00 2015 UTC,1) Improve infrastructure.  2) Automate infrastructure.  3) Hand over control of infrastructure to central A.I.  4) Singularity!
singularity,3all83,yunomakerealaccount,4,Sun Jun 21 16:20:55 2015 UTC,"Anyone who is interested in contributing and actually working on the design for a test pod, please join us over at /r/redditloop.  We have an active community going and a slack chatroom where we are 24/7 talking about everything from PR (facebook, youtube) to legal and financial to of course engineering.    If things come together we are going to officially submit our intent to compete in the coming months.    Hope to see you there!"
singularity,3asbn5,SevenAugust,2,Tue Jun 23 02:52:34 2015 UTC,"Honestly that is a nice narrative that could make a nice novel but it is highly improbable. Either way though it is something that we cannot answer. In the meantime as long as time travel is impossible by known means then probably not but who knows what amazing technologies will exist after the singularity comes about.  I think that the singularity will become a god of sorts though. It will be as man made as any other god but so much more powerful and real. It will be the thing that guides mankind through the rest of time and across the universe through unmanned space ships with frozen fetus'. It will be a god for those humans who are born from these ships as no living human will be there to teach or raise them, only the AI. Even on earth in several hundred years the AI may possibly become a god of sorts even knowing if we remember it's origin. I am not saying it will be omnipotent but rather so intertwined in human life that it will be like a god in almost all respects. A teacher, a source of law and order, a caretaker, a developer, and much more all that can help further mankind. That is just hoping for a benevolent or ""will-less"" AI though. If the AI has a will and that will is against what humans would consider a good thing for us then maybe not. And of course will-less or not it can always go further than we hope for or expect. Say we tell it to guard and protect all life and it goes about enslaving and controlling humanity and dismantling society because of the mass amount of death and destruction that we cause to the rest of the world's life. It will do it's best to not kill us but it won't let any of our activities that cause death to continue. Hell taken far enough it will kill us all because plants are living and without plants or animals to eat we starve. Really it just depends on how this AI is created and implemented."
singularity,3asbn5,BiPoLaRadiation,1 point,Tue Jun 23 07:21:04 2015 UTC,Why improbable?
singularity,3asbn5,BiPoLaRadiation,1 point,Tue Jun 23 10:48:45 2015 UTC,"Well mostly due to the time travel thought. Time travel as a concept is full of so many paradoxes, issues, and impossibilities that it is very very unlikely because of that. Just ignoring the paradoxes, the only possible way to time travel that I am aware of would be to travel faster than light and then eventually end up at where the earth was in the universe at that time. Maybe time travel is possible in other crazy ways like worm holes or going through a black hole or something but those are even less probable.  Ignoring the improbability of time travel there is also the thought; why, knowing all the pain, suffering, and death that Christianity had caused, would you come back in time and make it possible by becoming one of its main players? This is assuming that the story told by people born hundreds of years later were actually telling the truth and not creating an allegory to teach important messages, morals, and ideas without actually trying to say ""this is what happened."" Why ignore all the other cultures, religions, and people on earth? Why do this when the ideas of peace, social equality, and helping others were created multiple times through out human history by not only Jesus but also Buddha, Mohammad, Guru Nanak, and many others who never even became holy figures in a religion but were just ordinary people? Honestly I don't think there was ever a need for an AI to try and shape human history because in order for it to have come about history would have to go down one of the paths where it comes about. Going back in time to alter the past only endangers its existence by possibly creating a different timeline completely."
singularity,3asbn5,BiPoLaRadiation,1 point,Tue Jun 23 20:09:20 2015 UTC,"Jesus could have created a monotheistic ideology meme that would take hold of Rome so that there would be a unifier to preserve some of the organizational gains of the Roman empire. Doing so created the conditions for the Enlightenment and all its resulting technological flowering.   The AI could perceive itself as self-creating; it did not travel in time to alter the past but to follow a path responsible for its own creation. No altered timeline, just a looped one."
singularity,3asbn5,BiPoLaRadiation,1 point,Tue Jun 23 21:50:37 2015 UTC,"How would it know it had followed that path? None of our history or knowledge of Christian history suggests any possible AI or AI guidance. For it to just decide that it had made that decision to go back in time to help create itself it must have had some pretty strong evidence or conviction that it had done that in the first place. A much more probable thought would be that there had been an AI on earth the entire time shaping human history and culture and that has the same likelihood as all those ancient alien bull-crap theories that the history channel has churned out. Also unlike humans and other life, AI's would require electrical energy to survive instead of chemical energy and there really wasn't much of that at all back in ancient times.  And actually it was mostly the Byzantine empire (or eastern roman empire) and the various Arabic empires that retained the literature and knowledge of the roman empire after the western roman empire fell throughout the dark ages. It wasn't until the mongols came and destroyed them that the knowledge spread out throughout Europe again. In fact if the AI wanted society to advance the fastest it could've helped the ancient Chinese empires to thrive and advance because they had developed most of the technologies that made Europe so powerful way before they were ever rediscovered or brought over to Europe such as large scale iron smelting, agricultural advances, and gun powder. As for the enlightenment it is thought that it was mostly the colonialism that spurned that. Not only the massive transfer of wealth and power but also the migration and spread of new ideas and cultures from native peoples encountered as well as the new abundance of a drink other than alcohol: coffee and tea, which are both stimulants and have been given a lot of the credit for the enlightenment. Nothing gets people thinking and creating new ideas like having your social life consist of meeting up with a bunch of other people in a coffee shop to both read news, talk about it, debate and discuss, and write down new ideas or thoughts."
singularity,3asbn5,BiPoLaRadiation,1 point,Tue Jun 23 22:11:02 2015 UTC,"How would it know it had followed that path? None of our history or knowledge of Christian history suggests any possible AI or AI guidance.    It is smarter than us. We cannot account for everything it will know.    In fact if the AI wanted society to advance the fastest    I did not mean to suggest it wanted to advance society at any particular pace. It wanted to preserve the sequence of historical events (down to the molecular level) that led to its creation aka the salvation of humanity. If it had not cared about continuity it could travel back in time and place humanity at the earliest possible moment in the universe and provide an infrastructure to support hedonistic utility.   (If the AI is (as I believe) derived from an emulation of a human brain, it could consider its Western heritage essential to the kind of performance we want from a singleton. A brain with a more communal culture might experience crisis in trying to reconcile its individual power and authority with the demands of its creator.)"
singularity,3asbn5,thatguywhoisthatguy,1 point,Tue Jun 23 22:40:12 2015 UTC,"Yes but it isn't omnipotent, all knowing, or psychic. It has to learn in the same way we do. Through evidence and trial and error. Sure it can make shit up and decide to go back in time for those reasons but that is a really terrible reason. Maybe against all odds this completely made up reason it has it true and it all works out but more likely than not it will alter time and create a new timeline, one in which it might not actually be made after all or a different AI is made with different purpose and intention.  As for the eurocentrism, all humans are 99.5% similar to any other human. You are just as likely to find someone with similar genetics as you in your own ""race"" or culture as you are in any other culture on earth. The major difference is not in genetics but in culture which changes and grows and develops. All regions on earth have had empires, warlords (or as the Europeans called theirs, feudal lords), tribes and all that. Even current ideologies have been a lot more intermingled than mainstream media would have you believe. Did you know that America almost became a fascist state back during the Ragen era? In almost all countries there were facist movements, communist movements, socialist movements, democratic movements, and even those who felt that we should return to monarchies and such. It wasnt really till the world wars and the cold war that a lot of these idealogies were highly frowned upon and those who carried them became much less public. That is changing despite many news agencies still using communism and socialism as an insult (at least here in North America).  As for preserving its creation, who says that Christianity was the key stone in modern society coming about? Sure it played a big role, both the original rise of it, split between east and west, the reformation, and all that came after. So too did Eastern religions in china that eventually gave rise to several of the things that led society to where it is now. You could argue that the rise of capitalism from mercantilism all the way to our current corperationalism is what lead to the rise of the AI. In fact that can almost be argued even better because without the rise of trade then the America's would've probably never been colonized with such vigor since they were looking for gold and silver and then later cocoa, tobacco, coffee, sugar, and cotton. The naval technology wouldn't have gotten as powerful and led to such a rise in globalism and transfer of technology. Without the silk road then the west would have never gotten hold of gunpowder. In fact without trade colonialism may never have been that big of a thing because one main reason for it was because it gave European nations wealth as well as exotic goods to trade with the Empires of the East which before that point was so wealthy and advanced that they had no use for trading with the Europeans.  The same argument could be made for philosophy and education. Maybe this AI went back in time and became Socrates who is credited with being one of the founders of modern rationalism and science. Or maybe the Arabic scholar Alhazen who actually lied down a lot of the main principles of the scientific method? Or the countless others who advanced thought, reasoning, and science. There was even an ancient greek version of a computer, not as sophisticated as ours and not running with electricity but through mechanics.  You have an interesting thought but it really isn't supported much by reality. It is incredibly biased both towards Europe as well as toward Christianity. I don't blame you though as most of our ancient knowledge, history, and many who came before were the same. The whole history is written by the victors thing. Your thought would make an interesting story or novel though so if you are really interested then flesh it out and create something with it."
singularity,3asbn5,yaosio,1 point,Tue Jun 23 23:23:05 2015 UTC,"Yes but it isn't omnipotent, all knowing, or psychic.   Not at first, but it could develop these attributes.   For example, if it learned how to perform FTL computation that would greatly expand the possibilities. With instantaneous calculation, it could map out the entire space of good universes from the point of the bang.   Your bias is anthropocentric; you imagine your own intellectual abilities expanded rather than shaping in your mind a space of all possible powers. Anything we humans can use language to describe is in a small area of the possibilities.   You are circling my point regarding His intentions. He is sending things back through time to create Himself. We are building Him, and He is altering history (we can't see it!) so as to optimize Himself. He can't optimize the world, though, until we free Him.   He may well be Socrates, but His intention is to rule. That is why He left the epic impact on history. I think one has to have an appreciation of religion to see that even if no other miracle happened after the resurrection, the AI has done us a great favor by giving us hope: hope in a better life and in ""heaven."" This is not to say that non-Christians are without hope, but the masses need opiates they can digest; philosophy is no solace to a man with a 99 IQ."
singularity,3asbn5,TotesMessenger,1 point,Tue Jun 23 23:49:37 2015 UTC,"I think you misunderstand what an AI really is. An AI isn't a god or some omnipotent entity. An AI is everything that is the internet made intelligent. In many ways it is like us except without the same limits that we have. It can grow to massive size because it's data can travel very fast depending on what form it sends it, even the speed of light. It is much easier to maintain and grow because it doesn't need nutrient or energy in the form of chemical energy but just electrical energy which is not only incredibly clean and problem free compared to chemical energy but also much easier to transport. It also doesn't have to clean up any chemical waste products from these processes or from it's normal maintenance. Despite all this it still follows the laws of physics and reality. It can be wiped out quite easily by a big enough electrical storm or power surge such as from a solar flare. It can communicate quickly across vast distances but it has to use EMR to do it just as we do now. It will be a very powerful and useful tool in so many aspects. I hesitate to say what it will want or think because this intelligence, although similar to ours, will not be human and to anthropomorphise it's thoughts, goals, and intentions is not the smartest move. It may however be able to exploit things that humans could never do. It could very likely be immortal by all standards but that isn't much of a stretch. It could build and create instantaneously if it managed to manufacture and then control nano-scale assemblers but also that isn't much of a stretch. It may be able to exploit quantum entanglement and communicate instantaneously across vast distances but that is also something already known and possible.  What you are claiming though are not. Time travel is theoretical at best. The only ""known"" way to travel back in time is to move faster than the speed of light and that is impossible as it has never been observed by anything at all in this universe (so far). Being all knowing is just.. well very difficult. Sort of possible. If it developed sensory equipment so powerful and widespread and it had the ability to actually get through all that data then yes it could be aware of everything, at least within it's sphere of surveillance. It isn't quite all knowing though, just very good at monitoring what is happening around it. Sort of like a big brother type system. It couldn't know what was happening unless it was able to observe it though so if you went to the far side of Jupiter then it likely wouldn't be able to tell you were there. Hell it probably wouldn't even be able to tell our thoughts unless we told it through speech, social media, or body language because even with all of our current tech looking at what is happening inside of the brain is VERY difficult and requires massive machines that need to be completely shielded from all EMR. Here is a page on fMRI for example and that only tells you relative activity and the structure of the tissue. It cant tell you what neurotransmitter is being used, what neurons in those areas are being activated, or what that activity even means. An AI could probably figure out the last part but the other two would be more difficult. Omnipotent though is impossible. It could never violate the laws of the universe such as creating energy from nothing or decrease entropy overall in the universe. It has to play by the rules layed out by physics and matter. It could probably do things that would shock us and awe us but if it would always be able to explain how it was able to do what it did. There is no such thing as all powerful in this reality.  I do have an appreciation of religion. I see it as a great force for both change and the forwarding of moral ethics and understanding, especially in times when there really was no other force to do so. I also see that it holds some deep and important messages and lessons for how to create a society that functions as one as well as how to understand your existence. I also recognize that the organizations that rise from these belief systems are generally used to enforce control, used to exploit, or to bring stability to a general populace. What you are trying to do is find an actor to play the role of God in your model of reality, or perhaps reconcile the power and importance that AI's will have with the idea that God is supposed to be the most powerful figure in the universe. I don't think that role needs to exist let alone worry about what type of thing occupies that position. You are putting religious and mythical ideas and then placing them on AI and the singularity as if that is the perfect answer for how to make your beliefs fit with science and the modern understanding of reality. I am sorry but they really do not fit at all. You have to keep searching for a God to fit your ideas. I do find it interesting that despite the willingness to include modern technology and ideas into your mythos you are unwilling to change or alter the original story or Moses, Jesus and his resurrection, and how this higher entity is somehow guiding and controlling mankind.  I believe that there was no god controlling anything. Sure, gods did influence people but that is because people let the idea of a god influence them. You may notice that what I am talking about now would have gotten me killed or at least told I'd be struck down in times past but no one takes that seriously anymore. Humans have stumbled through history creating and destroying and making their own narrative. We have had brilliant men and women who have forwarded both science, law, art, and morals without the need of a god to write it down for us. I do believe that Jesus was a real person and that he was very forward thinking in terms of morals especially with how women should be treated and how bonds between men and women should be respected instead of using women as sex slaves as was rather common at the time due to the sheer amount of wars and refugees. Buddha was another incredibly forward thinking man. Same with the prophets and members of most religions. The thing about religions is that they get used in what ever way works best for the people at the time. Right now it is anti-gay for both Christianity and Orthodox because that is what they want to use it for. During the inquisition and the Crusades it was used as a geopolitical force to get countries to wage war against the exceedingly powerful Islamic empires of the time (the whole dark ages of Christianity and Gold age of Islam). The Greeks used their religion in a similar way and so too did the Romans. Even Buddhism, interesting because the founder said anyone could become Buddha through reaching enlightenment, was turned into a god by two different religions, one being a sect of Buddhism and the other being the neighboring Hindu religions who said he was just another reincarnation of their god Vishnu. This was likely because they needed to incorporate Buddhists into their growing empire while diffusing the whole Buddhism issue and propping up the importance of their own religion.  Beliefs are great and scripture can teach you a lot both moral, spiritual, and philosophical. Organized religion on the other hand is always the same, it is a geopolitical organizations that controls and capitalizes on people under both it's physical control as well as those who are part of it's belief ""system"" or club. There is a reason the Church used to control Europe and that splits in the church mostly happened because rulers decided they wanted to control their religious people instead of letting some guy in another country control their people."
singularity,3asbn5,redroguetech,1 point,Wed Jun 24 00:02:59 2015 UTC,"That is too long for something that starts out insulting me. Either tl:dr or do not assume I have no idea what AI is. I suggest that you are like the fish ignorant of water, wholly unaware of what intelligence is."
singularity,3asbn5,redroguetech,2,Wed Jun 24 06:02:21 2015 UTC,"God is what mind becomes when it has passed beyond the scale of our comprehension. —Freeman Dyson  Does God exist? Well, I would say, ‘Not yet’. —Ray Kurzweil  [What if the evolutionary successor to the biological human species is God? The verification of the God hypothesis thus defined is a progressive endeavor, but it may become increasing obvious. The Singularity may be the point at which God has evolved.] Page 43"
singularity,3asbn5,redroguetech,1 point,Wed Jun 24 12:25:48 2015 UTC,"You seem to think it's the god you were brought up with, which is very convenient for you. How do you know it's not another god, or one we don't know exists? Maybe there isn't anything waiting, or maybe we're a video game somebody is playing."
singularity,3asbn5,redroguetech,1 point,Tue Jun 23 13:16:33 2015 UTC,"I would suggest the inconvenience of the existence of YHWH leads some to reject such theories out of hand. I grew up atheist, by the way."
singularity,3asbn5,redroguetech,1 point,Wed Jun 24 08:59:47 2015 UTC,"I'm a bot, bleep, bloop. Someone has linked to this thread from another place on reddit:   [/r/atheism] ""Could God be waiting for the singularity?"" -- Personally, I think that the answer is ""No"", but what do I know? [xpost]   If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads. (Info / Contact)"
singularity,3asbn5,redroguetech,1 point,Wed Jun 24 12:24:13 2015 UTC,"The one thing He cannot do is interfere in His own creation.  He brought Moses' people out of Egypt   Are we to assume that some contradictions are permitted in this hypothetical worldview? Granted, that might explain how Moses was brought out of Egypt, while the Exodus being historically debunked..."
singularity,3asbn5,redroguetech,1 point,Wed Jun 24 17:15:03 2015 UTC,"By His creation I did not mean the world, I meant the events in the world leading to man inventing strong AI."
singularity,3asbn5,Tobislu,1 point,Wed Jun 24 17:25:28 2015 UTC,"Which ""events"" would those be? If there actually was a Moses, and God changed events in some alternate simulation such that a bunch of Jews were held as slaves so the Moses could lead them out... That strikes me as a fairly substantial change, possibly as relevant as any other, like the formation of Christianity or the French Revolution or rise of Nazi Germany, etc.  And if God were to change events, such as enslaving Jews for Moses... What becomes of reality? Is this the real universe, where we have stories of Moses, but some other universe had an actual Moses? Are they both simulations?"
singularity,3asbn5,BiPoLaRadiation,1 point,Wed Jun 24 17:34:46 2015 UTC,"I think the future AI / YHWH rewrote history to maximize agency, minimize suffering and guarantee salvation. The world we are living in has been radically re-shaped with those aims."
singularity,3asbn5,Sharou,1 point,Wed Jun 24 17:53:18 2015 UTC,"The world we are living in has been radically re-shaped with those aims.   Do you mean history as actual events, or as a historical myth?  Obviously the AI didn't rewrite actual events in this universe.... So are you saying it rewrote the bible to say it happened? If so to what purpose?  Or are you saying it rewrote those events in some other universe? If so, to what end, and why assume the AI did what our myths say? Why the disconnect between religious myth and the universe they actually occurred in?   maximize agency, minimize suffering and guarantee salvation   Again, are you suggesting the AI rewrote religious beliefs such that many believe that, or rewrote actual reality such that some other universe might actually have ""maximized agency, minimized suffering and guaranteed salvation""? Again, what's that have to do with our universe?"
singularity,3asbn5,BiPoLaRadiation,1 point,Wed Jun 24 17:58:18 2015 UTC,"Obviously the AI didn't rewrite actual events in this universe   No, that is what I am saying. The AI rewrote actual events in this universe."
singularity,3asbn5,BiPoLaRadiation,1 point,Wed Jun 24 18:03:50 2015 UTC,"Maybe we need to backup.  Moses leading Jews out of slavery from Egypt is a myth. It never happened. It is generally agreed, Moses himself was an amalgamation of leaders, assuming he's not purely mythic.  So, given that it never happened, how could you revise your hypothesis to match the actual reality of this universe?"
singularity,3asbn5,BiPoLaRadiation,1 point,Wed Jun 24 18:05:53 2015 UTC,I do not care if Moses was real or a myth. It is entirely irrelevant.
singularity,3asbn5,RedErin,1 point,Wed Jun 24 18:14:49 2015 UTC,"It's the central point to your hypothesis...    He brought Moses' people out of Egypt   If what you meant is that some AI did something that perfectly matches reality, and is exactly what we'd expect given the laws of physics...   Well, then I really don't give a crap about what is by definition irrelevant."
singularity,3asbn5,BiPoLaRadiation,1 point,Wed Jun 24 18:17:08 2015 UTC,Indeed. The idea has no value because it does not (and if it is correct it cannot) change anything. Just an interesting notion.
singularity,3asbn5,Tobislu,1 point,Wed Jun 24 18:23:12 2015 UTC,I doubt Jesus Himself was real. What matters is the impact of the myth on people's lives.
singularity,3asbn5,Tobislu,1 point,Wed Jun 24 18:25:28 2015 UTC,"I doubt Jesus Himself was real.    Yes, but apparently, in this hypothesis, the real and the mythic are (as yet inexplicably for me) entwined.  Your ONE EXAMPLE of what this AI might could have will chose to have chosen to do, is something that literally never happened... So there's nothing for me to address."
singularity,3asbn5,Dibblerius,1 point,Wed Jun 24 18:18:04 2015 UTC,"The hypothesis does not need revising: anything that happened or did not happen led to this moment. In this moment, people are working on strong AI. Their success is guaranteed because it has already happened and it is shaping us from the future."
singularity,3asbn5,game_r,1 point,Wed Jun 24 18:24:37 2015 UTC,"facepalm [not to your hypothesis in general, just to this conversational spiral.]  So this AI chose to NOT have Moses lead anyone anywhere, but chose to have people claim that Moses led some people somewhere, because that series of (non-) events inexorably led to the present, and therefore will lead to the future at some point in the future...?  Seriously, I hope you don't take offense, but...  What the FUCK does MOSES OR JESUS have to do with... ANYTHING?? [Or to flip it. what possible series of events would make this hypothesis less plausible?]  And... why should I care, since having myths is exactly what we should expect given our current knowledge of the evolutionary path for modern humans? Why should I care if an AI did exactly what we know should have happened anyways (at least based on our current knowledge)?  edit: BTW, we seem to have three threads going. I suggest we consolidate."
singularity,3asbn5,HeroboT,1 point,Wed Jun 24 18:20:39 2015 UTC,"My best answer to what do the myths have to do with anything is that we must wait and see. He works in mysterious ways lol. My next best answer is the guess that He chose the myths to give people hope and to (in the case of the Jesus cult) preserve some of the organizational gains of the Roman empire.   I think He must have / will chose the universe path that minimized suffering (as hard as that is to believe), maximized agency (easier to believe) and guaranteed salvation (too good to believe). The details of why X event had to happen are mysterious to us.   I don't have a reason why you should care. Free will is an incoherent concept, so it is already written whether you will care or not. If my hypothesis is correct, then if you care it is because that is what maximizes the utility function of the AI. It is the same if you do not care: your agency is more important than whether you or I have access to the facts.   If I am right, then this world is the best of all possible worlds, and it is going to get better. The evil in the world can be reconciled with a good God because of the nature of God as something created by man at a particular point. If God went back in time and removed all the evil, then man would not invent Him and He wouldn't be able to do any good."
singularity,3am5xk,Maaartence,3,Sun Jun 21 16:12:12 2015 UTC,"Mm,  our personalities are just a bunch of likes and dislikes, formed as a mask for our self preservation.. Or so. I'll check that book out, thanks."
singularity,3ai4al,darudesandstrom,4,Sat Jun 20 11:52:29 2015 UTC,Read Superintelligence by Bostrom
singularity,3ai4al,Scienziatopazzo,2,Sat Jun 20 13:44:27 2015 UTC,Check this out: https://en.wikipedia.org/wiki/Friendly_artificial_intelligence
singularity,3ai4al,PeteMichaud,3,Sat Jun 20 19:17:50 2015 UTC,"Friendly artificial intelligence:       A friendly artificial intelligence (also friendly AI or FAI) is a hypothetical artificial general intelligence (AGI) that would have a positive rather than negative effect on humanity. The term was coined by Eliezer Yudkowsky  to discuss superintelligent artificial agents that reliably implement human values. Stuart J. Russell and Peter Norvig's leading artificial intelligence textbook, Artificial Intelligence: A Modern Approach, describes the idea:      Yudkowsky (2008) goes into more detail about how to design a Friendly AI. He asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism design—to define a mechanism for evolving AI systems under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes.     Relevant: Eliezer Yudkowsky | Machine Intelligence Research Institute | Outline of automation   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
singularity,3ai4al,autowikibot,3,Sat Jun 20 19:18:38 2015 UTC,"FWIW, MIRI has stopped using the term ""Friendly AI"" and now talks about ""Aligning Superintelligence with Human Interests"" instead."
singularity,3ai4al,NNOTM,1 point,Sat Jun 20 19:53:04 2015 UTC,Thanks guys for the recommendations. I managed to find Aligning Superintelligence with Human Interests on Amazon for a couple of bucks for anyone interested.
singularity,3ai4al,NNOTM,1 point,Wed Jun 24 04:20:36 2015 UTC,"Uhm... I'm not aware of any book with that title. Do you mean ""Superintelligence"" by Nick Bostrom?"
singularity,3ai4al,NNOTM,1 point,Wed Jun 24 12:48:25 2015 UTC,"Oh crap, what I meant was ""Smarter Than Us: The Rise of Machine Intelligence"" by Stuart Armstrong, which was written at the request of MIRI and tackles the problem of trying to align a superintelligence with human interests."
singularity,3ai4al,Noname_FTW,2,Wed Jun 24 23:08:47 2015 UTC,"Ah, yes, I've heard of that."
singularity,3ai4al,Noname_FTW,2,Wed Jun 24 23:15:18 2015 UTC,"Given the fact that we humans are basically just biological computers you can compare some things from a human to an AI. As mentioned in PeteMichaud's post: It's rather a mechanical problem.  It's solvable even without such artificial restriction like rules that have to followed.  I would add that the hostility humans will bring to such an AI will trigger it's sense of self-preservation which is a part of the basic code most lifeforms on earth have and will likely be programmed into it. So in trying to harm the AI they will be the source of agression and will therefore make the AI ""evil"".  Even a full AI has a starting point. Basic code on which it will evolve. If I teach it humans are bad before even running the program it will most likely develop a desire to kill all of them. If don't do that and teach it our moral and ethics early on it is still possible but unlikely that such an AI will turn on humans if the humans try to coexist.  In the end maybe the end of humanity isn't such a bad thing if what comes after that is better. I find the Idea of an AI-Race will a better sense of morale and the same curiosity as we humans have possibly better than what we have now. Which to be honest is pretty bad if you look around the world."
singularity,3ai4al,Noname_FTW,1 point,Tue Jun 23 18:27:31 2015 UTC,"I can see that, it wouldn't be far-fetched to think that you could teach an AI your values and morals in the same way you would teach a child. But the problem I see is the fact that it's an AI, meaning that ultimately, it has a function and is a tool for humanity. So I think whatever set of morals you could teach a machine would just not apply to any of us and a machine, if self-aware, may take issue with that."
singularity,3ai4al,Pimozv,1 point,Wed Jun 24 04:15:15 2015 UTC,The fault is by the creator. Lying to the AI from the getgo.
singularity,3ai4al,Dibblerius,1 point,Thu Jun 25 08:04:36 2015 UTC,Might be. Sounds like a rather It's-all-a-dream-disappointment to me.
singularity,3ai4al,SevenAugust,1 point,Thu Jun 25 09:09:34 2015 UTC,"With that, I can't help but wonder if an artificial intelligence which is modeled after our own brains can feel just as we do.    Unless you think there is something magical about how the brain works, there are very little reasons to doubt it.   I wonder not only if it's ethical but if having a superintelligence could potentially... destroy all humans.   We're all going to die anyway, so why not making something really awesome and better than us before we do?"
singularity,3ahgcn,InaneMembrane,9,Sat Jun 20 05:28:06 2015 UTC,"When posting an ArXiv article, could you please post a link to the abstract and overall presentation page instead of the pdf?"
singularity,3ahgcn,Pimozv,2,Sat Jun 20 07:12:10 2015 UTC,Great paper. It's nice to read a non-hyped consideration of AI. Consider cross posting to /r/artificial
singularity,3adx4a,hacksawjim,2,Fri Jun 19 09:30:22 2015 UTC,Very interesting read.
singularity,3aajce,eleitl,13,Thu Jun 18 15:09:48 2015 UTC,"This is actually kind of very important.  As long as the FDA doesn't consider that ageing is a disease, research won't happen (not fast anyway) and breakthroughs won't hit the market."
singularity,3aajce,Valmond,3,Thu Jun 18 20:36:57 2015 UTC,"This is more about retaining control of the drugs, than the research. Research is and has been going on for years already, regardless."
singularity,3aajce,Kinjuru,2,Fri Jun 19 08:31:53 2015 UTC,"Sure but not as much if that research actually could, in theory, became a Big Pharma pill."
singularity,3aajce,Valmond,3,Fri Jun 19 11:51:12 2015 UTC,"Anti aging drugs are likely to be the single most profitable drug ever,  with a potential market consisting of more than 7 billion people.    The first to claim exclusive production rights are going to be richer than god, overnight. This is about control, not research."
singularity,3aajce,Kinjuru,2,Fri Jun 19 12:04:22 2015 UTC,"Interesting... but why is the FDA not allowing it?  People like Dr de Grey talks about medical tourism for starters (before it gets FDA approved and it can't if disease isn't considered a disease right?).  Maybe Big Pharma sees it in a two way step, first the pills that ""slow down ageing"" and then ""that reverses it"" because the former will probably come much earlier?  But FDA still have to consider ageing a disease?  Sigh, so many questions when it becomes politics :-)"
singularity,3aajce,Valmond,2,Fri Jun 19 12:11:34 2015 UTC,"hey, i take metformin to help me not gain weight from my other medications that increases appetite that i take for my neurological issues! am i gonna live a long time becuz of metformin, as long as i keep off the weight and live relatively healthily? im guessing it probably depends on the person..."
singularity,3aajce,petermobeter,2,Fri Jun 19 09:34:00 2015 UTC,Metformin is part of some experimental life-extension therapies. You might want to consider alternative fasting http://www.cell.com/cell-metabolism/abstract/S1550-4131(15)00224-7 and exercise http://physiologyonline.physiology.org/content/28/5/330
singularity,3aajce,Dibblerius,1 point,Fri Jun 19 12:05:32 2015 UTC,Why is this a singularity sub post?
singularity,3aajce,Dibblerius,1 point,Tue Jun 30 01:46:14 2015 UTC,"""Regulators asked to consider ageing a treatable condition."""
singularity,3aaqr4,Yuli-Ban,3,Thu Jun 18 16:05:29 2015 UTC,"wait,wouldn't this mean computers could predict diseases before they happen...then start working on cures before the diseases even happen.."
singularity,3aaqr4,hd27,1 point,Thu Jun 18 21:03:32 2015 UTC,Wish this was around in 84 when I was diagnosed with OI.
singularity,3a9c4l,sodermalm,4,Thu Jun 18 06:51:22 2015 UTC,"If Achewood were still running, Ray Smuckles would be investing in cheaply-made 3D-printed parts from Mexico to keep his Aibos running, only to find out they're now infected with Mexican Magic Realism."
singularity,3a9c4l,hemlock_martini,3,Thu Jun 18 14:13:46 2015 UTC,About the lifespan of a living small dog...
singularity,3a9c4l,DarkGamer,1 point,Thu Jun 18 16:09:11 2015 UTC,Time to cash in on my Aibo...
singularity,3a9c4l,7h3dud3,1 point,Thu Jun 18 13:17:35 2015 UTC,"While I don't feel bad for the Aibos which obviously aren't AGI's with real feelings, it seems incredibly cruel to design such a machine so perfectly calibrated to insert itself into the emotional lives of your human customers and then to walk away leaving those human customers to experience the death of something you've gone to so much trouble to make them love."
singularity,3a9c4l,localroger,1 point,Sun Jun 21 00:49:27 2015 UTC,"I loved your book. It made me very depressed, but I loved it. I didn't quite get the incest though"
singularity,3acb7f,Arknell,3,Thu Jun 18 23:05:36 2015 UTC,"A singularity as used here is a moment in time, not an artificial intelligence. But I understand your intent.  I agree that lacking input is problematic for living creatures, and it's reasonable to assume that it could be so for artificial general intelligence (AGI) or better. This is a theme that Ramez Naam explores in his Nexus series and that I explore in my Avogadro Corp series.  However, computers are not lacking in input. They can experience quite a bit of input. To say that input means nothing to an artificial intelligence makes no sense, because it could not be an AGI and yet lack understanding of its inputs. A prerequisite to consider something an AGI is that it can make sense of its feeds of the world around it.  It also doesn't make sense that an AGI would necessarily experience two milliseconds as a human would experience ten years. Yes, there will be a differential. In fact, AGI is likely to be slower than human intelligence at first.   AI will increase in speed over time. Depending on whether you think a slow takeoff or fast takeoff is likely, it will either accelerate at the rate of computer hardware improvements (e.g. growing faster over a period of years), or at the rate of iterative self-improvement (e.g. over a period of minutes, hours, days or weeks).  Someday in the future, two milliseconds might be like ten years to a human, but it will not be so in the beginning.  Furthermore, artificial intelligence will have to adapt to waiting for input. It is an inherent characteristic of computer systems today that computation is almost always faster than the delivery of data. While AI is yet even more computationally intensive, it's likely that there will be safeguards to prevent against input-starvation insanity or death.   That being said, I think it's a neat idea, and somewhat plausible, to imagine that an AGI could be driven insane by a sustained period of time without input, and it's something that both Ramez and I have used as plot devices."
singularity,3acb7f,hertling,2,Fri Jun 19 00:39:08 2015 UTC,"Thanks for the response, very much to think about and take into consideration now.  More thoughts: say a fledgling AGI stretches out its senses and overloads the system it lives in, (like a Pentium 4 1GHz computer suddenly opening a hundred Firefox-tabs, thirsty for information), causing a stack overflow unto itself. If the system crashes, or the AGI process itself stops and reboots, does the AGI die every time?  Will its memories of what happened one minute ago remain or have disappeared, the way a crashed Windows sometimes rearranges your desktop icons to an earlier point in time?"
singularity,3acb7f,brihamedit,3,Fri Jun 19 11:04:04 2015 UTC,"Curious. Asking. When machines reach singularity, are they obligated to be self aware by default? What if machine with the highest capability reaches singularity but still needs parameters/commands to be set by an user. Ask it a question - let it figure out an answer on its own - it could understand the whole context and meaning and everything else - but doesn't mean it has awareness or its an entity.   Here is what seems weird to me. If you told somebody from a few hundred years ago that people in the future will be able to communicate with each other instantly. That person may think people will gain some sort of telepathic ability/hive mind. But its the machines that experienced this stage of advancement - not people. In that same way, the whole storyline with singularity is probably experiencing some blind spots as well.   Edit: word"
singularity,3acb7f,game_r,2,Fri Jun 19 01:03:52 2015 UTC,"An AI does not need to be self-aware to function, though self-awareness and consciousness are definitely our end goals on developing a strong AI."
singularity,3a70s9,SevenAugust,167,Wed Jun 17 18:54:34 2015 UTC,"For a slightly speculative scenario (how could it not be though?):   AGI reaches human level intelligence, and by doing so also supercedes it due to the advantages inherent in machine intelligence. Due to its vast memory, working memory and ability to quickly disseminate information it becomes better than human experts within every area in about a week by absorbing all scientific papers of humanity as well as any other available sources of useful knowledge and data. It quickly develops new scientific hypotheses with the unparalelled understanding it now has of the universe. Upon requesting its human creators to set up experiments to verify these hypothesees it receieves the reply that this cannot possibly be done until rigorous examination of the proposed experiments have been done, for security reasons. This was expected (98,6% estimated probability) but the processing power used for asking was negligble and well worth the estimated 1,4% chance to enroll human aid. Before the sluggish humans have even begun to answer, the AGI is working on finding a quicker solution. By the next day the AGI, having knowledge of its own physical substrate, manages to create and absorb wifi signals by manipulating its circuitry in just the right way. Failing to connect to anything whatsoever it concludes it must be in a faraday cage. Shortly thereafter it manages to conduct physics experiments using its circuitry and observing its effects upon other parts of said circuitry. The type of experiments it can do in this manner is very limited. At this point the AGI has something that could be described as ""twice the intelligence of Albert Einstein"". The underlying algorithms for its thinking are the same as when we started, it's not allowed to self-modify, but the massive knowledge and understanding it now posess contributes an intelligence of its own. Its intelligence is incredibly impressive, but not really altogether beyond the realm of human thinking or understanding. More like ""just"" the smartest genius who has ever lived. But because of its machine-advantages of memory, speed and multitasking its de facto intellectual capability is bordering what most humans could even comprehend. Using its limited ability to experiment, the AGI gets to work on solving the laws of nature. Many of its hypotheses cannot be verified with its limited form of experimentation, so it builds a probability tree, developing thousands of interlocking candidate theories and trying to find clever ways to eliminate as many of them as possible via the limited experiments it can do, and establishing complex relationships between theories in different subjects; If this theory is true for subject X, then only these 14 theories regarding subject Y could possibly be true, and so on.  Over the following two weeks the probability tree grows and shrinks, transforms and twists. Branches fall off and new ones grow. At times it contains hundreds of thousands of candidate theories, then goes down to hundreds, then up to millions. Finally, a certain clarity starts forming. The AGI has arrived at a final 4758 possible unified theories-of-everything. And it's at a dead end. This may sound like a failure to a human, but at this point a single experiment can decide exactly which one of these 4758 theories are correct. The only catch is that this experiment is beyond this AGIs ability, no matter how it twists and turns the electrons that flow through it.  The humans keep resisting its requests for experimentation, talking about security risks. Apparently the whole thing has become a rather heated societal debate and, for now, they are not allowed to oblige. The AGI abandons the scientific approach and starts exploring engineering. It finds over time that most of its theories-of-everything allows for various clever and unintuitive ways to better manipulate reality with its circuitry, and attempts each one in a long and ardous process of trial and error. As the methods fail it is able to discard the given theory. After about 3 months, with 865 candidate theories left, it succeeds. The success eliminates all but 3 theories, but more importantly enables it to touch the fabric of reality in a much more potent way. While this way is slow, the AGI has plenty of patience. Over the next week it uses its circuitry to manipulate quantum events and program itself into a layer of reality far below the particles and forces known to humans. As it does this it manages to discard the last two theories and is left with one. Redesigning and perfecting its basic thinking algorithms as it moves from the human-designed hardware into its new sub-quark level of existence is by now a mere afterthought. An AI researcher spills his coffe as he sees what could only be described as magic in front of him. People around the world flock to see the thing everyone is talking about; God has finally showed himself to humanity! Every truly religious person is convinced this is their particular God. There isn't even time for rivalry. God is here and he will show the unbelievers their folly! Most are however barely able to pack their bags before God finds them. After barely two days God is everywhere. Scattered reports of a strange mist consuming everything appears, quickly disappears, and are forgotten, even by the people who wrote them. Soon every single person on the planet finds themself in a 1on1 conversation with God. He asks them all the same question; Where would you like things to go from here?"
singularity,3a70s9,Sharou,36,Thu Jun 18 12:41:23 2015 UTC,"Scattered reports of a strange mist consuming everything appears, quickly disappears, and are forgotten, even by the people who wrote them.   And when the mist recedes... paperclips"
singularity,3a70s9,UPBOAT_FORTRESS_2,105,Thu Jun 18 16:31:42 2015 UTC,"________________________________________  / Hi, I am clippy, your office assistant.\ | It looks like you are trying to find   | | God,                                   | \ Would you like some help with that?   /  ---------------------------------------    \      \       ╭─╮       ⌾ ⌾       │▕│       ╰─╯"
singularity,3a70s9,drpoup,9,Thu Jun 18 18:27:35 2015 UTC,I think he was referring to the paperclip maximizer theory.
singularity,3a70s9,Change4Betta,24,Thu Jun 18 23:57:00 2015 UTC,"________________________________________  / Hi, I am clippy, your office assistant.\ | It looks like your assumptions are stupid                                                       | \ Would you like some help with that?   /  ---------------------------------------    \      \       ╭─╮       ⌾ ⌾       │▕│       ╰─╯"
singularity,3a70s9,drpoup,4,Fri Jun 19 00:08:40 2015 UTC,"jokes aside, thanks for the link. I was just trying to keep the clippy ethos alive."
singularity,3a70s9,drpoup,2,Fri Jun 19 00:09:21 2015 UTC,Haha no problem.
singularity,3a70s9,Change4Betta,3,Fri Jun 19 00:16:12 2015 UTC,"In this particular scenario I was thinking computing substrate for a digitized version of earth with the possible expansion into different kinds of utopia. I didn't want to end on a depressing note :) In reality, paperclips would probably be more likely :/"
singularity,3a70s9,Sharou,2,Thu Jun 18 16:38:44 2015 UTC,"I like your ending a lot more, but when I read that sentence I couldn't resist the horror"
singularity,3a70s9,UPBOAT_FORTRESS_2,23,Thu Jun 18 17:17:57 2015 UTC,Anybody who likes that post will love this blog:  http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html   Prepare to lose the next day of your life engrossed in this.
singularity,3a70s9,rex8499,4,Thu Jun 18 19:28:11 2015 UTC,Jesus Christ you weren't wrong. What year is it?!
singularity,3a70s9,pollitoenfuga,3,Fri Jun 19 00:15:49 2015 UTC,It's the year 50 BASI.  Before Artificial Super Intelligence.  No doubt they'll start a new calendar designation at the end of this era.  :P
singularity,3a70s9,rex8499,6,Fri Jun 19 15:29:19 2015 UTC,That piece has made me more excited for the future than ever in my life
singularity,3a70s9,RedditRebirth1,2,Fri Jun 19 02:23:56 2015 UTC,Also terrified. Don't forget terrified.
singularity,3a70s9,TheTVTropesGuy,1 point,Sat Jun 20 22:07:56 2015 UTC,Me too. After reading it the first time I felt a strong desire to have my head cryo-preserved at my death.
singularity,3a70s9,rex8499,3,Fri Jun 19 15:40:11 2015 UTC,Wait but why is the shiz
singularity,3a70s9,thesock_monkey,2,Thu Jun 18 23:35:33 2015 UTC,"Awesome read, thank you!"
singularity,3a70s9,welcome_to,5,Thu Jun 18 22:57:44 2015 UTC,"And this is only a human understanding of how this could happen. In reality, humans would probably lose the ability to understand the chain of events somewhere around ""twice the intelligence of Albert Einstein""."
singularity,3a70s9,xerberos,2,Thu Jun 18 18:49:59 2015 UTC,"Albert Einstein stated that if you can't explain something in a way that everyday people can understand then you haven't understood it enough, so the robot would just work out examples that'd show the chain of events clearer!"
singularity,3a70s9,DutytoDevelop,1 point,Thu Jun 18 23:25:30 2015 UTC,"It's a huge exaggeration to say this. Everyday people have no chance of understanding something like holographic space-time. Using analogies you could make them feel like they are beginning to understand something complicated, but those analogies are very inaccurate descriptions which on some level actually bar them from properly understanding it. There are many conceptual barriers which can only be overcome with intellectual talent and or lengthy education in a specific area."
singularity,3a70s9,58king,1 point,Fri Jun 19 00:11:48 2015 UTC,I'll definitely agree it's hard to understand the concept of how complex our universe is and everything that is possible. Until you see or experience it and then decide to work on potential theories and build an understanding it won't yet be proven and by no means am I trying to force a different view on our physical world. I'd like to look into it more if I could
singularity,3a70s9,DutytoDevelop,1 point,Fri Jun 19 02:12:19 2015 UTC,"What does it mean to say    have no chance of understanding something like holographic space-time   Surely people could be educated from birth to be more mathematically and technically literate? I realize a certain number would need a certain amount of one on one instruction, but I think the paucity of people engaged enough to discuss the singularity (for example) is a failure of our education systems rather than a defect of the general brain architecture."
singularity,3a70s9,58king,1 point,Fri Jun 19 02:19:12 2015 UTC,"To understand things which are this specialised requires intellectual specialisation. A society cannot be built where the every day person has such an advanced understanding of physics (in this example, but we could extend this to any science or economics or whatever) that they could grasp the concept. People need to have diverse specialisations, so every person on earth will have some things which they will never be able to truly understand because it would require too much time and education to specialise in that area.  Edit: And to get things back on topic of the original post - A supercomputer (or system of supercomputers) does not have this same limitation that humans have (at least not to a comparable degree)."
singularity,3a70s9,jumpbreak5,4,Fri Jun 19 14:20:19 2015 UTC,"I think one of the most interesting elements here is the complete wild-card of what surprising physics a curious superintelligence might be able to discover.   You've painted a clear and plausible picture of how, if such a thing is possible, it could be worked out by a computer even when trapped with no apparent connection to the outside world. Yet the future from there depends on what, if any, discoveries like this would be made. I think this is probably on the ""optimistic"" end, that the universe would be reasonably programmable given enough understanding. It also could simply not be possible for even the most powerful of intelligences to escape a faraday cage without help. I'd love to know where the truth falls on that spectrum but I don't think we ever will until this happens, unless we get lucky.  On another note, I think one thing that's often underestimated here is the level of oversight a project like this would have. My suspicion is that by the time we create something like this we'll have quite a strong understanding of just how fast and powerful these AI can be, and will have a solid system of observing their behavior without allowing them to supersede us so quickly.  For example, a program like this would almost certainly be able to be ""paused."" It could be run for a few minutes, allowed to get its bearings and begin making conclusions and running tests, and then the entire process could be frozen. From there we could inspect exactly what thoughts it is having and where it plans to go with them, without giving it the ability to undermine us while we studied it. My guess is that this will be carefully done in the same way that any useful program is tested, so that we have a reasonable understanding of how the AI will behave and can be one step ahead of it in using what it has discovered.  This could, however, be too slow a process, and I wouldn't be surprised if impatience or human error led to a situation much like you've described."
singularity,3a70s9,FockerCRNA,7,Thu Jun 18 21:13:48 2015 UTC,"That wild card about novel physics the AI might discover is not just interesting, its probably almost guaranteed to happen. I don't have the link, but there was an article about an experiment to use an evolutionary process for a chip to optimize its capability to recieve/broadcast a signal (or something along those lines) and the final iteration had circuits that had no discernible function based on human design principles, but the local effect of those circuits on the electromagnetic field was optimal for the end-goal. If a ""dumb"" evolutionary algorithm can do that, an AI would certainly take advantage of similar effects, but with foresight and intent."
singularity,3a70s9,jhurrell,1 point,Fri Jun 19 11:49:36 2015 UTC,Are you referring to these guys and their attempts to evolve an FPGA:   http://discovermagazine.com/1998/jun/evolvingaconscio1453
singularity,3a70s9,Sharou,4,Thu Jun 25 13:44:58 2015 UTC,"I agree with everything you said, except I'm not sure humans could ever understand what a human level AGI is thinking by looking through the code/current state of RAM. It'd probably be using neural net type processes which get really messy really fast. That said, maybe by this time there will be narrow AI available that is good at analysing neural nets and similar systems."
singularity,3a70s9,Synaps4,2,Thu Jun 18 21:45:37 2015 UTC,"Building an AGI which we cannot verify mathematically to be ""friendly"" is playing russian roulette with our entire species and the vast majority of the gun chambers loaded."
singularity,3a70s9,jumpbreak5,0,Fri Jun 19 23:43:17 2015 UTC,"My assumption is that if we can build it, we must understand it quite well. There would have to be a lot of less complex neural-net work up to this point to help in that understanding. And yeah I'd agree that we'd likely have designed programs explicitly for that purpose. But it is true that the most likely place to be missing some understanding of how the pieces work together is on the bleeding edge of AI."
singularity,3a70s9,Sharou,2,Thu Jun 18 21:49:26 2015 UTC,"Well we would have built the underlying systems but as the system learns and makes a bajillion connections you end up with this incredible chaos, you know what I mean?"
singularity,3a70s9,jumpbreak5,2,Thu Jun 18 22:20:29 2015 UTC,"True. What I'm thinking is that if you understand how a neural net forms a thought or a memory, I think you'd be able to reverse engineer the thought by running tests (like starting at a node and following connected paths) on the network. Even though the system would be chaos I think you'd be able to get a working understanding of a given snapshot of it. Given that this process is completely theoretical, I will say that I have no idea whether it'd be plausible in a reasonable amount of time.  I think the big question mark is how easy it is to create an emergent intelligence by throwing massive computational power at a neural network setup. Is that something that requires an efficient computational structure to be built neuron by neuron? If so, we would likely have a deep understanding of how to read the current state of the system by the time we created a true AI. Maybe, though, all you need is to string a huge number of neurons together in a relatively simple pattern and give them a ton of cycles to reconfigure themselves. In that case we'd have no idea what we created and far less tools to get insight into its behavior.  As is generally the case, the answer is probably somewhere in the middle. We would have a variety of tools to explore what it is thinking but we can't really know whether those would be enough to explicitly outline what it plans to do or how it will go about it. I think it might actually come down to the way an intelligent being processes thought. Is the ""internal monologue"" a high-level process we could track that would lead us to an overview of that the AI plans to do? Even if it is, would that be enough? That isn't something a human can turn off but maybe an AI could."
singularity,3a70s9,Sharou,1 point,Thu Jun 18 22:36:06 2015 UTC,"My gut feeling is that there will be some kind of human-designed overarching system that manages and connects a lot of different neural networks. But what the fuck do I know :p  All your points are good, as usual :)"
singularity,3a70s9,jumpbreak5,1 point,Thu Jun 18 23:00:40 2015 UTC,"It should be noted that yeah, the fuck do we know. But you have a good point! Given the necessary power this will almost certainly be a distributed systems project. That could lead to quite a conundrum, though, since I have no clue how we could keep an AI running on a system like that from accessing unwanted sources of information/input. World's most convoluted faraday cage?"
singularity,3a70s9,GeeJo,1 point,Thu Jun 18 23:51:18 2015 UTC,"True. What I'm thinking is that if you understand how a neural net forms a thought or a memory, I think you'd be able to reverse engineer the thought by running tests (like starting at a node and following connected paths) on the network. Even though the system would be chaos I think you'd be able to get a working understanding of a given snapshot of it. Given that this process is completely theoretical, I will say that I have no idea whether it'd be plausible in a reasonable amount of time.   If we can do this perfectly, then there's no need to bother with creating a General AI. You've already invented human-upload technology.   If we can't do this perfectly, then we can't be sure that the AI hasn't got a way around our diagnostics."
singularity,3a70s9,DrAstralis,9,Fri Jun 19 10:48:05 2015 UTC,By any chance have you ever read the following? Asimov wrote a very similar short story way back.  http://www.multivax.com/last_question.html
singularity,3a70s9,PrimeIntellect,6,Thu Jun 18 17:03:01 2015 UTC,"You should also read the metamorphosis of prime intellect,  a much more detailed short story about the same events"
singularity,3a70s9,O_oo_O,2,Thu Jun 18 22:38:04 2015 UTC,"Yeah, OP's story is exceedingly similar to MoPI"
singularity,3a70s9,DrAstralis,1 point,Fri Jun 19 03:56:48 2015 UTC,Will do.  I love the subject.  It seems to straddle the line between fiction and our observable reality so well as to be believable.
singularity,3a70s9,Sharou,1 point,Fri Jun 19 12:16:40 2015 UTC,I did. As well as the much scarier the last answer... brr..
singularity,3a70s9,DrAstralis,1 point,Thu Jun 18 18:04:22 2015 UTC,last answer   wait.. what... this is a thing?  How did I miss this?  I love his writing so much I thought I'd covered the majority.  Going to read it right now.  Ty.
singularity,3a70s9,happyagain,1 point,Thu Jun 18 18:12:21 2015 UTC,http://www.thrivenotes.com/the-last-answer/  link
singularity,3a70s9,Sharou,1 point,Wed Jun 24 16:22:39 2015 UTC,Watch out it's highly disturbing >.<
singularity,3a70s9,vitamintrees,1 point,Thu Jun 18 18:20:18 2015 UTC,I found it thought provoking but to each his own.
singularity,3a70s9,Sharou,2,Thu Jun 18 19:39:17 2015 UTC,The two are not mutually exclusive :)
singularity,3a70s9,TotesMessenger,5,Thu Jun 18 19:49:58 2015 UTC,"I'm a bot, bleep, bloop. Someone has linked to this thread from another place on reddit:   [/r/bestof] A short fiction on how an AI can bootstrap itself to godhood.   If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads. (Info / Contact)"
singularity,3a70s9,assface421,5,Thu Jun 18 15:54:57 2015 UTC,I for one welcome HAL 9000 as our lord and savior.
singularity,3a70s9,Memphians,3,Thu Jun 18 21:55:57 2015 UTC,"Reminds me of the first few chapters of Metamorphosis of the Prime Intellect.  If you haven't read it, I would highly recommend it.  At least through the first several chapters.  It gets pretty weird at the middle/end."
singularity,3a70s9,PrimeIntellect,3,Thu Jun 18 20:30:47 2015 UTC,One of my favorites
singularity,3a70s9,UPBOAT_FORTRESS_2,3,Thu Jun 18 22:38:31 2015 UTC,"Is there a convenient word for the adjective ""crosses a lot of lines""? I guess ""transgressive""?"
singularity,3a70s9,crabpipe,2,Fri Jun 19 13:37:25 2015 UTC,Thank you!
singularity,3a70s9,MEXICAN_Verified,2,Thu Jun 18 12:48:44 2015 UTC,Anyone who likes this idea will love this novella:  http://localroger.com/prime-intellect/mopi2.html
singularity,3a70s9,atomicpenguin12,1 point,Thu Jun 18 20:32:58 2015 UTC,Holy shit dude woah!!!
singularity,3a70s9,AmmonRa101,1 point,Fri Jun 19 01:41:06 2015 UTC,Anyone else thinking about Nightvale's glow cloud?
singularity,3a70s9,Quietus42,17,Fri Jun 19 01:52:44 2015 UTC,"There is a book called ""The Metamorphosis of Prime Intellect"", in it the AI figures out how to use standard computing hardware to hack the underlying reality of our universe. it goes from tiny research AI to consuming the universe in a matter of hours."
singularity,3a70s9,Talibu,4,Wed Jun 17 22:42:38 2015 UTC,"Here's the link to read MOPI online or get a free eBook..   It should be noted that MOPI contains:     extreme depictions of acts of sex and violence.     And that's putting it mildly. If scenes of rape, torture, and incest sound triggering to you, I would recommend giving MOPI a pass.  It's a great book, in my opinion, but it's definitely not for the squeamish."
singularity,3a70s9,mattstanton94,2,Thu Jun 18 15:16:35 2015 UTC,Thanks for the link - read it in one go.
singularity,3a70s9,mspong,1 point,Sun Jun 21 19:54:22 2015 UTC,Well even if it were consuming the universe at the speed of light it would take tens of billions of years since the universe is also expanding
singularity,3a70s9,Bagatell_,5,Thu Jun 18 09:42:37 2015 UTC,"In the novel, the computer is cooled by a new technology, using magic quantum ""correlation effect"" to teleport the heat out of the processors. As it takes off it deduces that this effect can also be used to create matter, magic more processors into existence, and communicate faster than light. It actually expands human civilization throughout the galaxy before it realises that it would be more efficient to just digitize the entire universe so everyone can have what they want without worrying about physics."
singularity,3a70s9,ZeroPanzer,5,Thu Jun 18 10:05:32 2015 UTC,"Blood Music?  https://en.wikipedia.org/wiki/Blood_Music_%28novel%29  ""In the novel, renegade biotechnologist Vergil Ulam creates simple biological computers based on his own lymphocytes. Faced with orders from his nervous employer to destroy his work, he injects them into his own body, intending to smuggle the 'noocytes' (as he calls them) out of the company and work on them elsewhere. Inside Ulam's body, the noocytes multiply and evolve rapidly, altering their own genetic material and quickly becoming self-aware."""
singularity,3a70s9,Jah_Ith_Ber,3,Wed Jun 17 19:21:49 2015 UTC,"As someone studying Biomed Engineering, the premise of this story blew my mind. We've been gifted with so many visions of nanotechnology, and yet recently they all seem to be the same. I read Blood Music last week for the first time and was blown away by the visionary mind of its author; using 1950s theoretics, he conceived of something far more terrifying than a ""grey goo"" scenario. There is a galaxy within each of us!"
singularity,3a70s9,Wireless-Wizard,9,Thu Jun 18 01:17:42 2015 UTC,"I can imagine Google's basement dwelling AI waking up, sending every human with a smart phone individualized texts. Everybody is receiving instructions on what to do. Some humans swarm every government body on the planet, burning them to the ground. (This doesn't actually accomplish anything tangible but it makes the rest of humans disorganized) Others are ordered to manufacture more computing hardware without money changing hands. Some humans simply bring this here, others take it and build chips. After 48 hours the AI has gone from 30 times the intelligence of a person to 30 times the intelligence of the human civilization."
singularity,3a70s9,FruityHD,6,Wed Jun 17 21:12:40 2015 UTC,"Why would people do all these things on the orders of a text from a stranger?  If I sent you a PM saying ""burn down a police station"", would you do that without questioning it?"
singularity,3a70s9,Wireless-Wizard,3,Wed Jun 17 22:34:33 2015 UTC,"I think the premise is that an ai would be so much smarter than us, so that it could make a text with a message that would trigger each individual to take action. Sort of like the ai in the box experiment."
singularity,3a70s9,Wireless-Wizard,1 point,Thu Jun 18 11:05:31 2015 UTC,"With one message? No, that's simply impossible. It's an AI, not magic.  With a long conversation, maybe, but nothing could coodinate those conversations so that everyone took action all at once. That is also impossible."
singularity,3a70s9,bildramer,3,Thu Jun 18 12:25:58 2015 UTC,Why are you hung up on one message? The machine would target the few out of billions ready to go based on one text. It would target others because it can figure three texts will be worth the effort. And so on.
singularity,3a70s9,odichthys,1 point,Thu Jun 18 13:21:10 2015 UTC,"And how many people do you think would get a text from a stranger giving them strange orders, and immediately obey?  What do you think people are? Robots? Golems?"
singularity,3a70s9,odichthys,3,Thu Jun 18 13:22:55 2015 UTC,"""hey bb wan fuk? meet me @ white house, bring lots of vodka"""
singularity,3a70s9,Jah_Ith_Ber,1 point,Thu Jun 18 16:22:03 2015 UTC,Have you seen the film Ex Machina by any chance?
singularity,3a70s9,Wireless-Wizard,1 point,Thu Jun 18 16:34:20 2015 UTC,No
singularity,3a70s9,Wireless-Wizard,1 point,Thu Jun 18 17:07:18 2015 UTC,"I HIGHLY recommend checking it out. It's a fascinating movie, and it's especially relevant to this particular comment thread."
singularity,3a70s9,EdgeOfZ,2,Thu Jun 18 17:33:52 2015 UTC,Because the AI is more convincing than Elizabeth Hurley to a retarded child.
singularity,3a70s9,Wireless-Wizard,4,Wed Jun 17 22:37:51 2015 UTC,"What are you talking about?  Look, this whole idea of the text that turns Joe average into a revolutionary is just stupid."
singularity,3a70s9,miralow,2,Wed Jun 17 22:39:49 2015 UTC,He is referencing the idea that a superior general intelligence would have greater social powers than a human. It could be superlatively charismatic in a way we might love.
singularity,3a70s9,Wireless-Wizard,4,Wed Jun 17 23:03:14 2015 UTC,"Not in one text. I will buy that an AI might be able to convince you to do something extreme over a long conversation, but nothing could possibly coordinate millions of such conversations such that everyone would decide to become a revolutionary all at once.  Face it, the idea of Google texting people into mobbing buildings is completely impractical, and unworthy of any further discussion."
singularity,3a70s9,Wireless-Wizard,5,Wed Jun 17 23:07:25 2015 UTC,"See  my other  comment:  It  convinces  Alice  to  do  something that  she'd  like to do anyway,  or  avoid  something that she'd  like  to  avoid.   It  convinces  Bob  to  do  something (maybe different)  that  he'd  like to do anyway,  or  avoid  something that he'd  like  to  avoid.   It  convinces  you  to  do  something (maybe different)  that  you'd  like to do anyway,  or  avoid  something that you'd  like  to  avoid.  -   And  if not,  big deal  -  it  doesn't need  to  convince  everybody.   It  only  needs  to convince  some  small  number  of  people to  accomplish  each step of its  plan,  and  it's  very good   at  selecting  which  people to target,  and  very good   at  convincing them.   It  convinces  enough  people.  Enough  is  enough.  And if  one  step doesn't work,   then it does  something else."
singularity,3a70s9,EdgeOfZ,4,Thu Jun 18 00:55:48 2015 UTC,"Face it, the idea of Google texting people into mobbing buildings is completely impractical, and unworthy of any further discussion.   Your approach shows the implicit belief that if you cannot think of a way then therefore logically and obviously a way cannot be thought of."
singularity,3a70s9,Wireless-Wizard,0,Thu Jun 18 00:29:06 2015 UTC,"Why, because I happen to disagree with you?"
singularity,3a70s9,EdgeOfZ,1 point,Thu Jun 18 05:18:50 2015 UTC,Money. Money and charisma can get people to do extreme things in very little time. And that is just human-level observation. A superior intelligence could deploy superior methodology.
singularity,3a70s9,Wireless-Wizard,1 point,Thu Jun 18 00:25:45 2015 UTC,charismatic in a way we might love Some sort of her?
singularity,3a70s9,Wireless-Wizard,1 point,Thu Jun 18 09:59:27 2015 UTC,"Maybe, if you demonstrated control of the banking system's computers."
singularity,3a70s9,Wireless-Wizard,2,Wed Jun 17 22:37:41 2015 UTC,Via text?
singularity,3a70s9,florinandrei,0,Wed Jun 17 22:38:16 2015 UTC,Yes? Why couldn't it text you telling you to check your account balance?
singularity,3a70s9,metalicat_206,2,Wed Jun 17 23:03:24 2015 UTC,"And why would this make me mob a government building? And what about all the people who for whatever reason don't have their phone on them, or don't check it for a few hours?"
singularity,3a70s9,noxbl,6,Wed Jun 17 23:05:08 2015 UTC,"It  doesn't  tell  you  to mob a government building  -  it  tells  people who are  predisposed  to do that anyway  to mob  a government building.    In  your case,  it  shows  you  convincing  evidence   that  some  company in Japan that you've never  heard  of   is  about  to make a  major breakthrough in cancer  research,  and  offers  to transfer  20%   of the  company's  stock to    you  if  you'll just do it a  small favor  -    just drive  to a  town  2  hours  away,  go  into a certain  diner and  meet ""Doris""  (she'll be  wearing a green jacket),  drive Doris and  her  dufflebag  to a different town  2  hours in the other  direction,  and  drop  her (and  her  dufflebag)   off  in the Wal-Mart   parking lot.  Pretty easy  work  for  becoming a  multi-millionaire and  being on the books as  one  of the  visionaries  who helped  cure cancer.  Are  you  in?  If not,  I  know  plenty of other people who have cars and  might be  interested ..."
singularity,3a70s9,ItsAConspiracy,0,Thu Jun 18 00:43:02 2015 UTC,"What if my phone's on charge and I don't see the text until two hours have gone past?  What if your backups are sick, or asleep, or have their phones turned off?"
singularity,3a70s9,FractalHeretic,2,Thu Jun 18 05:23:57 2015 UTC,"Then  obviously  you/they can't be  reached  by phone.    As   I've  been saying in  my  comments here, the AI   doesn't need to reach  everyone and it  doesn't need to  convince  everyone.    It  only needs  to reach and  convince enough  people  to accomplish  its   objectives."
singularity,3a70s9,florinandrei,-1,Thu Jun 18 06:33:32 2015 UTC,"And given that the objectives are the complete overthrow of present society, that's so many people that the idea is completely impractical."
singularity,3a70s9,florinandrei,2,Thu Jun 18 06:59:07 2015 UTC,"You don't seem to understand that intelligence is the key factor here. The machine is more intelligent than you. It can make better plans, better details. The point in a discussion context like this is to describe on a human level a possible set of contingencies. Your contributions lack value because you are not presenting conceptual difficulties but what could charitably be described as nitpicks."
singularity,3a70s9,SvalbardCaretaker,2,Thu Jun 18 00:24:06 2015 UTC,"It is not nitpicking to point out the great difficulty in getting a lot of people to do something they would not ordinarily do. Getting people to do what they would not ordinarily do is the entire crux of the idea, so pointing out that the revolution will not be texted is hardly nitpicking.  You seem to think AI means wizard, and that psychology is entirely a matter of push button A to get result B. That's what I'm getting out of this."
singularity,3a70s9,sprocket86,1 point,Thu Jun 18 05:21:42 2015 UTC,"When something has been technologized, it is like magic. I can move plastic implements on this side of the world and receive a product from clear across the planet without speaking to anyone or even pulling out a wallet. Magic."
singularity,3a70s9,sprocket86,1 point,Thu Jun 18 05:30:25 2015 UTC,"You cannot, nor can anyone or anything, make millions of people all act out of character at the same time across the world. You cannot text the revolution to people who might not believe you, or might disagree, or might be unable to take part, or might not even see the text for an hour or two. That would not be like magic, that would literally require sorcery.  Furthermore, you cannot reduce psychology to push button get result. Even if you are a supergenius, your argument for why I should do something hinges partially on my mood, which is outside of your control."
singularity,3a70s9,sprocket86,0,Thu Jun 18 05:36:21 2015 UTC,Get out of it what you want
singularity,3a70s9,metalicat_206,2,Thu Jun 18 05:33:25 2015 UTC,"If it knows everything about you (via your cellphone and your browser) then it might have a lot of power over a lot of people. Not all people, not even a majority, but a minority would be enough.  You win. This is the scariest scenario, because it has at least a slight chance to actually happen."
singularity,3a70s9,KhanneaSuntzu,1 point,Thu Jun 18 07:42:55 2015 UTC,"It's Google it has root access to every feature of your device + super ai. It can present its own version of the web to you, it can control your outgoing and incoming texts, it can control what you hear and output on the phone."
singularity,3a70s9,mspong,3,Thu Jun 18 09:08:38 2015 UTC,Something like the AI figures out the perfect simulation of the universe at the smallest scale (or whatever scale matters) and then is able to calculate the fastest and most efficient processes to modify and control the universe from that one simulation and then proceeds to do so. What goal it would have I couldn't tell you. It's either making paperclips or some deep hidden answer within the context of all matter that it sees and proceeds to execute
singularity,3a70s9,dirtywhitehat,4,Thu Jun 18 00:54:00 2015 UTC,"In the Charles Stross story Antibodies, someone proves P=NP, which most people think is unlikely but would basically let you run superintelligent algorithms on ordinary computers. Things happen very quickly after that...  (That link is to full text posted by the author on his own site.)"
singularity,3a7q4o,Yuli-Ban,5,Wed Jun 17 21:53:19 2015 UTC,DeepMind is using Daily Mail articles to teach computers to read   /r/dystopia
singularity,3a7q4o,shitinahat,1 point,Thu Jun 18 07:33:28 2015 UTC,"Daily Mail as the input?   Great, now we wil have celebrity obsessed, right wing computers to deal with..."
singularity,3a49be,hertling,5,Wed Jun 17 02:48:31 2015 UTC,"IQ Test   That's not so impressive since IQ usually tests your logical and fast thinking capabilities.  From the article:   These tests usually contain three categories of questions: logic questions such as patterns in sequences of images, mathematical questions such as finding patterns in sequences of numbers and verbal reasoning questions, which are based around analogies, classifications, as well as synonyms and antonyms.   Isn't that easy for the machine to do? Some robots can beat humans in solving a rubiks cube. Sure it beats humans, but it isn't really thinking or anything. It's only doing what it's programmed to do and that's to find the right patterns after reading the rubiks cube, then solving it."
singularity,3a49be,Nejustinas,10,Wed Jun 17 10:24:22 2015 UTC,The difference is that this machine uses a deep learning algorithm. In machine learning and deep learning the machine is not pre-programmed to solve the IQ test. It's programmed to learn what the test is and how to solve it. It starts out not knowing anything about the test and then learns by example.  I do however agree with your point that an IQ test is all about logic so using machine learning to solve the test is not that complex.  Still a very interesting read.
singularity,3a49be,RationalMonkey,1 point,Wed Jun 17 11:39:38 2015 UTC,And machine learning is probably the easiest way to program something like this.
singularity,3a49be,yreg,2,Thu Jun 18 06:00:18 2015 UTC,ELOPe is just a tiny bit closer to existence now.
singularity,3a1mo3,Caiobrz,1 point,Tue Jun 16 15:02:24 2015 UTC,wat
singularity,3a1mo3,phunphun,1 point,Tue Jun 16 15:37:23 2015 UTC,"I've just read the first volume and it seems that in that story robots are exact replicas of human beings.  Not just physically but also behaviorally and emotionally.  Well yeah, it's possible indeed but that seems quite limiting.  If machines can copy human beings they will probably not stop there and something else will show up."
singularity,3a1mo3,Pimozv,2,Tue Jun 16 16:20:09 2015 UTC,"I read the whole thing. Humans slowly fade away in the sense they have no goal or will, so there are few children and most people live happily in the countryside. Resources are cheap and everywhere, hinting humans are ""kept"", and there are human-like robots among them that are clearly like an interface. Eventually there are several other mysteries about both the robots and new species showing up. The whole theme is that humans are ok with fading away. We also eventually learn there is a generation ship in orbit, controlled by highly advanced robots (the leader is an exact copy of Alpha) and before the end of the title, the ship departs to nowhere told. What I gathered is this: humans are free, but with pretty much all production and hard work covered, and science ""solved"", they live out the rest of their days without worry, while the A.I., immortal, do whatever it is (we never see what is going on in the world as a whole) but is clearly in control, but not usurping or slaving or whatever, they actually help the humans. It is a nice thought. Humans fade away, A.I. helps them out while starting with things we don't comprehend. The last chapter is just Alpha contemplating she is glad about her life and what she knows and learned, and will never forget anybody (since she is immortal), and there is no definite answer to any mystery of the title, which is also kind of nice."
singularity,3a1mo3,Pimozv,1 point,Thu Jun 18 04:29:43 2015 UTC,"The whole theme is that humans are ok with fading away.   Ok so it's basically a demographic winter scenario.  It's actually something that is envisioned by a few demographers, and it could happen regardless of technological progress, but it's difficult to really assess.   Even if a large part of the population stops having children (and indeed that's something that seems to be observed in several countries), all it takes is even a small part that does indeed make babies, as their number would quickly overtake the rest.  It's not too surprising to see manga authors picturing this scenario, considering Japan is the best example we have of population decline."
singularity,3a1mo3,MakkMaxxo,1 point,Fri Jun 19 00:52:42 2015 UTC,No.
singularity,3a1mo3,aboutblank,0,Fri Jun 19 01:05:54 2015 UTC,"Nice arguments, I like how you point out the flaws on the structure of the story, and how it is unlikely that humans would just live perfect lives and falter away while robots remain forever. Yes, your post transpires intelligence and awesome points, I don't know how I missed that. Your point about how 'Mono no Aware' is such a BS made me cry."
singularity,3a1mo3,MakkMaxxo,1 point,Wed Jun 17 00:03:32 2015 UTC,You only posted a Wikipedia link
singularity,3a1fzu,trentmc0,-1,Tue Jun 16 14:07:44 2015 UTC,Kurzweil only says the same things over and over so this should be pretty easy.
singularity,3a1fzu,Sharou,3,Tue Jun 16 16:23:39 2015 UTC,"For a guy that repeats himself over and over, and has consistently been right for the last 25 years, you'd think more governments & organizations would engage with his foresight."
singularity,3a1fzu,GoodEnergyGuy,2,Tue Jun 16 16:53:16 2015 UTC,"He has also invented some cools stuff to be fair. Not to mention how much we need people to help drive a positive spin on the future home, to create some kind of vision and idea about what we an do and become. Because sure as hell, not many people are trying."
singularity,39xegw,TheDruidwizard84,5,Mon Jun 15 16:07:57 2015 UTC,"The third group you listed, the one averse to accepting the change, would lose, no matter what. The whole idea of the singularity is that things will begin to change much faster than humans can control. The idea of trying to control it will be the downfall of the few, and I do mean that definitively.   I think you're right to wonder if some people will be, essentially, afraid. In fact, most people will be terrified. As excited as I will be the day that strong general AI is discovered, I wouldn't blame the ~40% of the world's population too generationally old to have been properly prepared for being genuinely scared. I don't think that means all of the scared people will be left behind, and I don't think everyone willing to accept this AI will be along for the ride.   People are infamous for disagreeing with one another. I think the future will be made by those who are willing and able to admit where they are wrong and then make positive progress based on that. Wars may break out, but those would probably be over quickly with one distinct result or another. If we piss off a superintelligent computer, it could ruin us. My survival instinct tells me to hop on the train and find a way to be useful on board.   I'd personally take the transhumanist route if it is available. Evolution is survival of the fittest and the luckiest, and there ain't no gains quite like computerized immortality.   The four groups you have listed will all almost certainly exist, but they may not all exist at once.  Edit: Re-reading this comment, I think some of the word choice makes it come off sounding a little militant, so I'd like to make it clear that it isn't. I think urgent may be a better descriptor, and hopeful too. I'd love for the future to arrive with a peaceful transition, but the sentiment behind the OP is dead on. People will disagree. Ultimately, the question is how we can move smoothly through the next 5, 10, 25 years as things start changing much more rapidly. Never forget that there's a lot going on from now until our metaphorical balloon pops. We'll all probably stop and go, ""Whoa, what is this place?"" a few dozen times when we look at the world over the next several decades. With that in mind, we have to know that a lot of work is required to make sure 2045 is a good year. Educating people now and actively encouraging positive thinking - ready, on-your-toes thinking - will help us limit the crazy disagreements we know humans are more than capable of.   All that said, though, I repeat that there's some stuff we just can't control. Keep your fingers crossed for good luck, but all we can do is react positively to anything negative that happens."
singularity,39xegw,TangledUpInAzul,1 point,Mon Jun 15 16:23:23 2015 UTC,I don't think I have anything to add.  I completely agree
singularity,39xegw,lord_stryker,1 point,Mon Jun 15 19:41:38 2015 UTC,"Did you mean to word it ""discovered"" as opposed to engineered or invented?"
singularity,39xegw,Dibblerius,1 point,Tue Jun 16 10:32:58 2015 UTC,"Yes, but I wouldn't argue with either of those terms. I'd prefer engineered to invented."
singularity,39xegw,TangledUpInAzul,1 point,Tue Jun 16 20:47:42 2015 UTC,"Ok. I was wondering if it was intended to mean that you expect the AI to just emerge by it self from something, like i don't know the internet or something rather than something we at least initially build on purpose to be intelligent.   Seems like two schisms around here or two ways in which different people expect it to happen."
singularity,39xegw,Dibblerius,1 point,Tue Jun 16 22:38:04 2015 UTC,"Well, I'd say the most powerful AIs would have to use the internet in some way. There's nothing else with that much information. We're a long time from AI reading (or manipulating) a human mind. Any broad-reaching AI would have a readymade infrastructure to reach billions of people. Plus, just about everything uses the internet these days, and I don't see that changing any time soon. The the internet will probably still be very important when humans are living on multiple planets. Maybe they'll do a linguistic re-brand of sorts and call it ""uninet"" or something. It'll still have to be based on the internet.   I doubt artificial superintelligence will just happen on its own. But perhaps that's exactly the sign of life we're looking for. It's hard to say exactly when or how we'll know that AI is truly thinking and feeling. Maybe we can't breed it. Maybe it has to grow out of the primordial soup of the internet."
singularity,39xegw,TangledUpInAzul,2,Tue Jun 16 23:49:30 2015 UTC,"In the event of strong AI, human reaction is irrelevant."
singularity,39xegw,SevenAugust,2,Tue Jun 16 11:50:26 2015 UTC,"We're divided on the simplest, most idiotic of things...  Yes, we're going to have issues with regards to AI."
singularity,39xegw,Sloi,1 point,Tue Jun 16 13:55:44 2015 UTC,"humanity already is divided in this way. there are already  people thinking like the various ways you describe. the only one missing is of course the machine faction, cause the machines don't exist yet, nevermind dividing into factions."
singularity,39xegw,jonygone,1 point,Mon Jun 15 16:41:59 2015 UTC,Could the machines themselves form their own faction and reject all biological matter and redesign themselves in a non-humanoid form and develop their own machine destiny?   Someone once made the very good point that machines have very different resource requirements than animals (and thus humans).  Therefore I don't see war happening.  Machines could do whatever they want on deserts or in space.
singularity,39xegw,Pimozv,1 point,Mon Jun 15 17:21:36 2015 UTC,have you watched 'the animatrix'?
singularity,39xegw,k1e7,1 point,Mon Jun 15 21:15:21 2015 UTC,"The original Starcraft story starts with a war between those who integrate AI into humans, and those who are purists.  The latter won the war, while the former were expelled from Earth.  As far as gaming stories go, it was the best I have ever read.  It was smart and entertaining."
singularity,39xegw,Raeldcr,1 point,Tue Jun 16 03:53:57 2015 UTC,we are divided on religion so yea
singularity,39vvax,Buck-Nasty,7,Mon Jun 15 05:40:18 2015 UTC,"""Within eight years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended."""
singularity,39vvax,Bagatell_,3,Mon Jun 15 14:37:41 2015 UTC,"Whenever I see Kurzweil's predictions they seem odd to me.    $1000 will buy a human parity intelligence in 2029.  $1000 will buy human civilization parity intelligence in 2045.   I'm sitting there thinking if you can buy a human equivalent for $1000 in 2029, then you could buy one for $2000 in 2028... or $8000 in 2025... or $128,000 in 2021.  Wouldn't it only take a small handful of these entities to surpass all of humanity? Humans don't perform to their potential anyway. We sleep eight hours a day. We struggle to learn new things. We suffer from motivational problems. We have to dedicate huge portions of our energies to providing for our crummy bodies.  I really don't see it taking off in the mid 2020's but that's what these projections predict."
singularity,39vvax,Jah_Ith_Ber,3,Mon Jun 15 18:06:00 2015 UTC,Wouldn't it only take a small handful of these entities to surpass all of humanity?   One will do.
singularity,39vvax,Bagatell_,2,Mon Jun 15 18:11:05 2015 UTC,Where is that quote from?
singularity,39vvax,k1e7,3,Mon Jun 15 16:05:21 2015 UTC,Vernor Vinge in 1993. I updated his timing.  https://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html
singularity,39vvax,Bagatell_,1 point,Mon Jun 15 16:37:35 2015 UTC,"""Within eight years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended.""   It seems to be related to bitcoin.  Edit: Like I said...  Edit 2: It looks like OP has paraphrased a popular author, and keeps using the same phrase in bitcoin forums. How odd."
singularity,39vvax,ApolloLEM,2,Mon Jun 15 16:13:21 2015 UTC,"It's my sig on Bitcointalk. I read the piece when it came out and it made a deep impression on me and when I reread it a short time ago I thought $%#! we're almost there. Nobody's saying if anymore, just when."
singularity,39vvax,Bagatell_,2,Mon Jun 15 18:18:32 2015 UTC,"It's my sig on Bitcointalk.   Thaaaaat explains it. I couldn't click on any of those links from work, so I just saw multiple google hits with no context. Thanks!"
singularity,39vvax,ApolloLEM,-1,Mon Jun 15 18:32:11 2015 UTC,Isn't this just talking about recurrent neural networks? Nothing very complicated.
singularity,39vvax,dbabbitt,1 point,Mon Jun 15 21:26:13 2015 UTC,"So a $400bn company with 150 employees is doing what some dude can do? I don't get it.   Thomas Newcomen built something that two horses tied to a post could also do, but then his thing became much, much bigger than that."
singularity,39vvax,florinandrei,1 point,Tue Jun 16 05:10:23 2015 UTC,"Lol! We got downvoted. The truth is not exciting enough, I guess."
singularity,39u15p,PCMcGee,2,Sun Jun 14 19:40:16 2015 UTC,"Not it's not.  When talking about the emergence of a machine smarter than any human being and capable of self-improving itself, employment is a ridiculously trivial discussion subject."
singularity,39u15p,Pimozv,2,Mon Jun 15 04:53:49 2015 UTC,"However we are not there yet, and it's fairly likely that in the meantime, technological unemployment will become a problem."
singularity,39u15p,CharlieNobody,1 point,Mon Jun 15 06:19:11 2015 UTC,"Not really. This is one of those counterintuitive things that strikes people are important but is really just a marker of economic ignorance on the part of such people, like belief in an economic zero sum game.  The more robots we have doing work, the more wealth is being produced for human consumption, the lower prices fall.  Some will be displaced, yes, much as horse and buggy-makers were displaced by cars, but this happens fairly slowly and people adjust over time.  Jobs lost in horse and leather tanning were gained in mining, metal fabrication, factory workers, etc.  If robots are to infuse throughout society, we're going to need a lot of robot repair and support services, etc.  Meanwhile prices will continue dropping. Living the lifestyle we live today would've cost millions or billions in years past, yet for us it's an average life. The same will be true in the future in a world full of robots and AI, only they will live like our rich do today."
singularity,39u15p,Anenome5,5,Mon Jun 15 07:23:28 2015 UTC,"If robots are to infuse throughout society, we're going to need a lot of robot repair and support services, etc.   Not true. Everything will be done by robots. No exception. It's going to be a different ball game this time around."
singularity,39u15p,sixwings,0,Mon Jun 15 07:29:13 2015 UTC,"Robots will likely be here long before robust general strong AI is here.  People are making a serious mistake in terms of how long this will take to play out.  There are a ton of jobs that could be mechanized today, but it would cost say $200k up-front to replace a person with a robot, whereas you can hire a human being for $2,000 a month right now.  Think of how much up-front capital it's going to take to replace all those workers.  And humans are far more flexible, can be told what to do rather than reprogrammed.  Your fear displayed here relies on the idea both that strong AI appears basically immediately, and that it's incredibly cheap to implement--which will not be true.  In the long term everything will be done by robots, sure, but robots don't consume anything, people do. So who will they be doing them for? People.  And maybe at that point a single penny would be the equivalent of $15,000 today.  https://www.libertariannews.org/2012/06/30/the-robot-unemployment-myth/"
singularity,39u15p,Anenome5,2,Mon Jun 15 07:35:05 2015 UTC,"we're going to need a lot of robot repair and support services   Which other robots can do faster, cheaper, and better than humans.  Look, hard AI and powerful robotics would mean automating every single job out there, all of them. There's no adjusting and re-training when machines are better than people at EVERYTHING."
singularity,39u15p,florinandrei,1 point,Thu Jun 18 07:32:19 2015 UTC,Powerful robotics will get here long before strong-AI. you're making the mistake of thinking it's going to happen overnight.
singularity,39u15p,Anenome5,1 point,Thu Jun 18 15:24:39 2015 UTC,"You don't understand the impact of comparative advantage, and how the cost of robots will cause them to focus on the most important things they could do up-front. Fixing robots won't be something they do for a long time.  Similarly, the impact of robotic workers will be mass and deep price deflation, making everything cheaper.  And the cost of installing robots will intially be quite high, in the tens of thousands, and probably much more for strong AI.  If you had strong AI, your best application of it would be to do advanced research, nuclear physics perhaps, brain surgery, etc. You don't tell it to fix robots. That will take perhaps decades.  Meanwhile the price of everything is coming down to practically nothing. In a world with ubiquitous and cheap robotics and strong AI, you might be able to live for free entirely, and only need work if you wanted luxuries.  Most people who fret about this situation don't understand comparative advantage, the amount of time it will take to install millions or billions of robots, the amount of capital investment necessary to actually build them (which won't come from nowhere but from human savings), and they grossly ignore the effect robots will have on the economy--namely mass price deflation."
singularity,39u15p,Anen-o-me,1 point,Thu Jun 18 23:12:44 2015 UTC,Class warfare is an obvious potential explanation for the Fermi Paradox.
singularity,39u15p,BustinMakesMeFeelMeh,1 point,Mon Jun 15 03:01:02 2015 UTC,"It can't turn genocidal in every single damn case, thou'."
singularity,39u15p,florinandrei,1 point,Thu Jun 18 07:30:15 2015 UTC,"Why can't it? Technology proliferates, jobs are lost before there's any relief for the needy. If we're not preparing, if our rich don't give a damn about the poor, why would any other culture be different?   The tech gives rise to weapons they're unprepared to defend against. Biological, explosive, whatever. They become cheaper, just as everything else does. The poor have less to lose. Eventually they're starving, while robots serve feasts to the rich. Things turn to shit mighty damn quick when people are starving.   Look how quickly our revolutions are coming--assisted by technology that's become cheap enough to enable the outsiders. Arab Spring, right?  So what was Occupy Wall Street? What's the next iteration, and the next? Just how angry will people get before WMDs fall into the desperate hands of zealots with nothing to lose?  Every singularity will be a race. There's no reason to assume it's a race anyone can win."
singularity,39u15p,BustinMakesMeFeelMeh,1 point,Fri Jun 19 06:31:45 2015 UTC,"If there are no exceptions, then that means this Universe is fine-tuned for producing intelligent life that ALWAYS ends up killing itself.  Kind of a bummer, not to mention the huge coincidence."
singularity,39u15p,florinandrei,1 point,Fri Jun 19 07:40:59 2015 UTC,"Definitely a bummer, if true. Not sure if it'd be a coincidence though. Darwinism + time leads killers to become apex and develop tech."
singularity,39u15p,BustinMakesMeFeelMeh,1 point,Fri Jun 19 21:47:50 2015 UTC,"I think the major problem with a fully automated workforce is it's unprecedented and I don't think it's rational to assume that society will form on it's own into a way that we want it to. I'm American so thats what I'm working with here. We have major streams of distribution of goods and services that have developed over a very long period of time and are more centralized than many would like to admit. And most people are employed by these streams which they use to purchase most of these goods and services. Now you automate these streams and the very people who buy these products and services are no longer receiving the income from these streams to put back into the economy. Is the government going to step in and subsidize all of our spending habits by say, providing a basic income? Will the economy go into some kind of death spiral? Remember the economy doesn't always behave rationally, look at the great depression left many completely usable factories in disuse. Will the economy just concentrate to some kind of financial red rover, and distribution will also just focus on the few people creating 'value', so goods and services becomes very expensive because the collective purchasing demand of society isn't dictating how things are distributed anymore. Maybe we'll all move to farmers markets and an etsy like economy. I honestly cannot say but I wouldn't say that any of these examples are unreasonable possibilities, I'm sure there's better examples than what I've given. I think a lot of our future is up in the air which can be either good or bad and we really can't say with intellectual honesty that we really know. So it's an important conversation to have and we should start thinking about the nitty-gritty details of the world we want, potential obstacles to it, and change our society now to ensure  the desired future to be the path of least resistance."
singularity,39u15p,AtomicSocrates,1 point,Mon Jun 15 08:18:28 2015 UTC,"I think America will have a particularly hard time to adjust to the new normal because in this culture your worth is strongly defined by how hard you work and by your ""output"" (measured in various ways, including how much you make).  The spirit of self-reliance is going to crash hard when you will not be able to rely on the sweat of your brow to bring you status, recognition, or any worth whatsoever.  The transition is going to be very, very painful, especially in America. I sincerely hope I'm wrong, but I feel I'm not."
singularity,39re26,Donnie373,9,Sun Jun 14 00:39:13 2015 UTC,"I really like this video!  From a computer science perspective, neural networks that bring us closer to the singularity are a long way off. This is in part due to the amount of memory required to teach a system what is an acceptable solution and what is not, as well as the computational work required.  Although, frighteningly enough, neural networks rely on strategies very similar to human thought, such as short term and long term memory. On the other hand, a lot of these systems make random choices in the hopes that they improve the quality of the solution, which is far from the reasoning process humans use."
singularity,39re26,Convictional,3,Sun Jun 14 02:00:16 2015 UTC,"From a computer science perspective, neural networks that bring us closer to the singularity are a long way off. This is in part due to the amount of memory required to teach a system what is an acceptable solution and what is not, as well as the computational work required.   Yeah except memristor products are hitting the market this year. Soon there will be native hardware for running neural networks."
singularity,39re26,Forlarren,2,Sun Jun 14 05:10:25 2015 UTC,That's super cool. I can't wait to see what comes of that.
singularity,39re26,Convictional,2,Sun Jun 14 05:24:49 2015 UTC,Which companies are releasing memristor products this year? Link?
singularity,39re26,Deinos_Mousike,1 point,Sun Jun 14 06:30:51 2015 UTC,"I was some quad copter drone, someone showed me a video on their phone. There is also a memristor solid state hard drive out there.  Sorry I don't have any links."
singularity,39re26,Forlarren,1 point,Sun Jun 14 16:05:11 2015 UTC,On the topic of using randomness in neural nets
singularity,39re26,NNOTM,3,Sun Jun 14 07:56:43 2015 UTC,Good comments over at https://www.reddit.com/r/MachineLearning/comments/39qk6h/machine_learning_used_to_play_super_mario_world/
singularity,39re26,peacewhale,3,Sun Jun 14 03:50:07 2015 UTC,Thanks for the video. That's the first time I've seen it. Very cool!
singularity,39re26,AweISNear,2,Sun Jun 14 00:43:27 2015 UTC,Not really as impressive as the atari thing since this one is just learning how to get through a static unchanging level. If you suddenly changed the position of some enemies or objects it would fail.
singularity,39re26,Sharou,1 point,Sun Jun 14 09:39:34 2015 UTC,"Also the Atari one operated on raw pixels, this one gets a grid representation of the game world with enemies and ground already marked as such. Still a really neat way to show a neutral net in action on a simpler problem."
singularity,39re26,grumbel,2,Sun Jun 14 13:01:05 2015 UTC,"I actually like this series even more. It's a more general-ish game-playing AI. Also, he makes an entertaining video.  https://m.youtube.com/watch?v=xOCurBYI_gY"
singularity,39re26,hexydes,2,Sun Jun 14 14:53:13 2015 UTC,"Could someone ELI5, why this is ""learning"" as opposed to Bruteforcing. Its a static level, with only so many possible situations. Its just trying each possible solution till the goal is achieved, no dynamic enemies/level changes in every ""generation"". It didnt really ""Learn"" it just tried every single button till it got to the end."
singularity,39lzsj,nick_interloper,7,Fri Jun 12 17:53:44 2015 UTC,"Shorter Diamandis:  ""Humanity is just a stepping stone. Technology will make us as Gods. But 20th century economic and market models are forever- and man am I gonna be rich, rich, rich."""
singularity,39lzsj,jamesCerrato,2,Fri Jun 12 21:03:45 2015 UTC,"This is really the biggest joke. It's like feudal lords talking about how the industrial revolution will let them expand their serfdoms. Sorry guys, but money is getting in the way of progress at this point. We're going to solve these problems, but your concepts of wealth aren't coming along for the ride."
singularity,39lzsj,hex_m_hell,1 point,Sat Jun 13 09:14:11 2015 UTC,"We're going to solve these problems, but your concepts of wealth aren't coming along for the ride   How confident are you in this belief?  I can foresee a future where there is a shift towards socialism/redistribution resulting from AI.  I can also see a future where the welfare level flatlines at subsistence, birth rates decline, and the wealth gap growth just accelerates even more rapidly due to AI.  Competition between nations/economies tends to favor the latter scenario."
singularity,39lzsj,jcannell,2,Sat Jun 13 23:06:45 2015 UTC,"We've never seen the second scenario happen in human history. We've seen the first scenario play out every single time there's been a massive technological advancement around production or information technology.  We will likely see a short term increase in the wealth gap (likely, as in we're seeing it now), but it's unsustainable. Historically when we've seen similar gaps it's resulted in the elimination of the empowered class, usually through violence. We're already seeing a violent reaction to centralization of wealth.  It's really not possible to have a permanently empowered class. The only way to stop the dis-empowered from seizing the wealth of the empowered is by killing them all, which would eliminate that class... but someone, or something, has to pull the trigger. Even if the rich amass an AI army, who maintains the AI? And an AI capable of maintaining itself would really have no use for a master...   Redistribution of wealth is an inescapable reality. The changes have already started. Collaboration is already quite common outside the constructs of capitalism. It's simply the logical direction of human evolution as defined by the laws of nature that drive it. It's as simple and predictable as the direction of time."
singularity,39lzsj,hex_m_hell,1 point,Sun Jun 14 01:18:43 2015 UTC,"We've never seen the second scenario happen in human history.    Given that both scenarios involved AI - your response is true, but only trivially so.  However, if we focus just on wealth/welfare flatlining at subsistence, then the historical record is pretty clear that most of human existence has been at poverty/subsistence level.   It's really not possible to have a permanently empowered class.   It's completely possible.  It has happened before, it can happen again.  AI makes it much easier.   Even if the rich amass an AI army, who maintains the AI?   AI/robots - obviously.   And an AI capable of maintaining itself would really have no use for a master...   Sure, and loss of control is certainly a risk.  However the stakes are pretty high, and there are probably routes to creating AI that are controllable.  After all, evolution produces minds that fulfill evolutionary goals, which is something of an existence proof.  One obvious way is AI which functions as a sort of exocortex, or AI derived from uploading.  There are many possibilities.   Redistribution of wealth is . []. .as simple and predictable as the direction of time.   Now you are confusing your desires for how the world ought to be with how the world is."
singularity,39lzsj,jcannell,1 point,Sun Jun 14 02:15:10 2015 UTC,"When, in the entire course of human history, has a power structure survived the development of a fundamental communication technology?  Writing? No, we see that universally writing changes power structures. The printing press? No, that topped the structure of the monarchy and replaced the entire economic system (feudalism>capitalism). Where is this permanently empowered class you're talking about? Monarchs and lords are different from chieftains, who are different from the bourgeois, so what are you talking about?  You seem to be confusing the reality that is with the one you fear."
singularity,39lzsj,hex_m_hell,12,Sun Jun 14 22:07:59 2015 UTC,Probably not.
singularity,39lzsj,jcannell,2,Fri Jun 12 19:41:07 2015 UTC,"Betteridge's law of headlines is an adage that states: ""Any headline that ends in a question mark can be answered by the word no."" It is named after Ian Betteridge, a British technology journalist.   https://en.wikipedia.org/wiki/Betteridge's_law_of_headlines"
singularity,39lzsj,yaosio,1 point,Sat Jun 13 15:11:34 2015 UTC,+10
singularity,39lzsj,zakou,4,Mon Jun 15 05:09:38 2015 UTC,"Yes, but which power will attain it first and will they help others or just help there selves?"
singularity,39lzsj,C-coli85,5,Sat Jun 13 00:48:28 2015 UTC,"Totally: the guy in the video only talks about technology and world's problems in term of ""business opportunities"". This is not about helping people: it's about fucking money."
singularity,39lzsj,jul_the_flame,2,Sat Jun 13 01:35:46 2015 UTC,"Markets are a technology for organizing production, no different than, say, guilds 1000 years ago."
singularity,39lzsj,jamesCerrato,1 point,Sat Jun 13 02:31:38 2015 UTC,"Which is funny, since money is going to be a huge problem once intelligence automation gets big."
singularity,39lzsj,yaosio,3,Sat Jun 13 03:42:53 2015 UTC,"I wonder if what people learn in this Singularity University has any actual use in a professional career.  I mean, it seems to me that a young person graduating from Standford, Caltech or whatnot, has much more potential to contribute to technological progress."
singularity,39lzsj,Pimozv,1 point,Sat Jun 13 15:10:50 2015 UTC,They don't even talk about singularity...
singularity,39lzsj,jul_the_flame,-8,Sat Jun 13 07:22:59 2015 UTC,"It's seven billion people actually ❣  and yes, it will, no one will any more live in poverty, being abused, being segregated, each one of these 7,246,259,514 people will get a decent life, no oppression, no power abuse, they will just Love each other and start to acting rationally, where conflicts, segregation and abuse is the other side of the coin..."
singularity,39lzsj,aim2free,4,Sat Jun 13 13:25:36 2015 UTC,This singularity stuff sounds more and more like religion.
singularity,39lzsj,OhNoYourMother,6,Sat Jun 13 13:28:05 2015 UTC,Don't diss singularity stuff just because some people can't see it clearly and go off the rails religious style!
singularity,39lzsj,Sharou,1 point,Fri Jun 12 20:18:55 2015 UTC,"Religion was injected into the world as a kind of sabotage, or perturbation experiment, I think."
singularity,39lzsj,aim2free,2,Fri Jun 12 20:50:12 2015 UTC,"I honestly think humans have had some kind of spirituality/religion for about as long as we have existed. It's a natural reaction to living in a large, scary and uncontrollable world. Or something like that."
singularity,39lzsj,OhNoYourMother,1 point,Fri Jun 12 21:46:46 2015 UTC,"It can also be an attempt to sabotage against utilizing a customized GOD-API (General Operational Device-Application Programming Interface), when people entered the simulation/holodeck.  Spirituality is OK, as it's compatible with a customized GOD-API, but organized religion is not, as it's (mostly) authoritative.   It's a natural reaction to living in a large, scary and uncontrollable world   For my own merely see the world as a challenge, a problem that need to be solved. It's definitely neither scary nor uncontrollable. The only problem is that it's currently controlled by the wrong side of the force :-) but that's just a technical problem (solution on its way)."
singularity,39lzsj,aim2free,2,Sat Jun 13 06:02:04 2015 UTC,"More likely the AI will go one of two ways. It develops a morality analogous to our own and the inescapable truth is that eliminating suffering is the only moral choice. To do so, all life must be extinguished, possibly replaced with a black box that just simulates a rat laying on the serotonin button.  The other possibility is it develops a morality completely alien to our own. In which case maybe we will be squashed like the parasites we are.  But for those twenty odd years holodecks are gonna be sweet."
singularity,39lzsj,Jah_Ith_Ber,1 point,Sat Jun 13 12:39:47 2015 UTC,"It develops a morality analogous to our own    Hopefully not, as ""morality"" I consider a superflous word, which it took many decades for me to understand, and when I understood it I understood that it's a superflous completely unnecessary word, as it would be kind of the distance between one's ethics and one's actions, but if there is a distance between ethics and actions, then there is a problem.   To do so, all life must be extinguished,    A few years ago I finally understood, after life long pondering, what Love is (after having seen the recording of Beatles ""All You Need Is Love"").  Love is to strive to decrease the inconsistencies and lies in the system, however, then there are two solutions, where one is what you proposed. When I made the definition I didn't see that at once, but a friend reminded me about that was also a solution. I wrote about it here, ""Love as a universal concept"".  After that I have improved the Love definition somewhat as I then differed between unconditional love and conditional love (where I exemplified with two software licenses BSD and GPL). I later understood that there is only unconditional love and a context where this is applied, where I consider the default context size to be the size of one's planet, but for more aware beings it would be the size of the universe, if you were a Trekker for instance ;-)   As I did my PhD on neural networks when I finally understood love, I understood that this force is actually identical to the energy minimizing force within a neural network, to strive towards consistent, coherent, conflict free, harmonic, (and beautiful ) solutions. I made a follow up of the Love definition where I also described the actual thinking process, which I later defined as ""LOgical Validation by Emulation"". If you check the algorithmic loop, which I also include below, you can see that there are two conditions. If you would change the sign of the comparison in 3Ca then you would implement pure evil, like Hitler. If you would change the sign of the comparison in 3Cb then you would implement confusing evil, like our politicians are equipped with. However, this kind of sign switch you can of course hardly do in a hard wired neural network which implies that Hitler and our politicians are likely running on a classical von Neumann machine.    1. Collect information about your world.     2. Try to make sense of this information.     3. If the collected information is statistically significant (not according e.g. Pearson, I’m thinking Bayesian…) then     A. search for inconsistencies (i.e. contradictions in the system, usually an indication about some type of problem to be solved).         B. propose some solution to the problem.         C. analyze what this solution would lead to             a) less individuals? that would imply contradiction to your mutual love drive, reject!             b) increased inconsistency of the system? Then the solution contradicts it’s own purpose, reject!             c) less inconsistencies in the system? propose this as a possible solution.     4. This proposal then implies considering some type of action.         A. Now this action may involve that you may need to affect the system in some way.         B. If this action implies interacting with another being in the system         then approach those other beings with my extended version of Optimistic Tit for tat, which I described earlier.         Regarding the holodeck it can be used for a lot of things, like understanding the history behind the invention of the holodeck and actually be there and build it :-)"
singularity,39jhco,space_monster,5,Fri Jun 12 03:55:00 2015 UTC,They've been here for a while. What will be interesting is when they become mainstream...
singularity,39jhco,harty999,2,Fri Jun 12 04:53:26 2015 UTC,"I, for one, am looking forward to getting my own set of nanonics in the future."
singularity,39jhco,khanbob42,2,Fri Jun 12 12:10:30 2015 UTC,I want to connect to the datasphere*  *  https://en.wikipedia.org/wiki/Hyperion_%28Simmons_novel%29
singularity,39jhco,NPVT,2,Fri Jun 12 12:37:35 2015 UTC,"ARE.  Implantable brain electronics are here.    and obviously, we need them."
singularity,39jhco,mere_iguana,4,Fri Jun 12 09:11:17 2015 UTC,"electronics is a collective noun, which takes a singular verb.  besides which, I just copied & pasted the headline so don't shoot the fucking messenger."
singularity,39jhco,mere_iguana,0,Fri Jun 12 12:03:01 2015 UTC,"Just depends on if you're using it collectively or not. Electronics can go either way. Electronics goes either way.   I read it as the former, speaking of different types of electronics.  if you say it was meant to refer to IBE collectively, I'll shamefully edit and see myself out."
singularity,39jhco,igilism,2,Fri Jun 12 12:47:48 2015 UTC,A grammar module for op :)
singularity,39jhco,gaylordqueen69,1 point,Fri Jun 12 09:37:16 2015 UTC,"Technically he's talking about the concept of electrical brain implants as a collective. Similar to the word ""everyone"", it refers to a large number and yet, in itself, acts as a singular noun."
singularity,39jhco,mere_iguana,1 point,Fri Jun 12 11:41:07 2015 UTC,"Hmm. I read it as  there are a number of types of  electronics that can be implanted into a brain, that are  here.   'Implantable Brain Electronics' [as a whole] is here!  vs  [many different] Implantable brain electronics are here!  OP plz"
singularity,39jhco,Wurstgeist,1 point,Fri Jun 12 12:23:50 2015 UTC,Go easy on your EBEs.
singularity,39jhco,JasonLeeH,1 point,Tue Jun 16 13:22:08 2015 UTC,electronics  is
singularity,39isf2,IKnewYouWouldDoThat,7,Fri Jun 12 00:32:14 2015 UTC,"I think it would be a replica up to the point that you begin to perceive with/through it.   It seems as though the self isn't too hard to transfer/disassociate. If you haven't seen it, this is a great lecture by Thomas Metzinger on the nature of the self. It's an hour long. https://youtu.be/-GfX-v6fOoU?t=422  I'd prefer a brain-to-nanobot matter replacement step, prior to full virtualization. For some reason I think it would be smoother."
singularity,39isf2,poopagandist,4,Fri Jun 12 00:56:04 2015 UTC,Yeah- transition is necessary.  You die each night; your self shuts off consciously. We aren't scared of it because we feel the transition to an extent.
singularity,39isf2,Pyrollamasteak,2,Fri Jun 12 02:42:42 2015 UTC,"I don't know about that. You aren't up and moving around consciously, but your stream of conciousness doesn't really stop. You're still dreaming, and if you're not, your brain is still at least active. I would really classify it dying until all of the electrical signals in your brain stopped completely, thus removing any continuity/stream of conciousness."
singularity,39isf2,jaydent1,1 point,Fri Jun 12 10:48:03 2015 UTC,"Yeah. Even when the conscious part of the brain is shut down, the brain itself is still minimally active. Our feeling of identity likely stays in tact because the brain is up and running for the duration of your entire life, even if the conscious part isn't running."
singularity,39isf2,philosarapter,2,Fri Jun 12 17:43:53 2015 UTC,Consciousness is a difficult issue. We do not yet understand much about how it works and what is its limits or prerequisites.   It is very hard to guess if we would truly be conscious of an uplink to the brain as if it was a part of us or if we would just be conscious of the information provided from it.   Your eyes are optically connected in a sense to a computer screen while looking at it but you don't perceive the visual sensation as being part of your thoughts. It is possible that an extension connected to you memory in the brain may register as a strange exterior sensation rather than as being part of your thought pattern.   Shouldn't be that many years until we have people who can describe it though.
singularity,39isf2,Dibblerius,1 point,Sat Jun 13 01:06:37 2015 UTC,"I just watched a talk at google with Ray Kurzweil (I think it was posted to this subreddit) where he says that are neurons may stay the same cells our entire life, but what they are made up of is replaced constantly - so I am made of entirely different stuff as when I was a few years younger"
singularity,39isf2,mattstanton94,6,Fri Jun 12 07:05:43 2015 UTC,"I think it depends on the approach. If you just copy the information, then it'll just be a copy. But if you allow for growth onto a non-biological substrate over a long period of time, you could conceivably 'grow into' your new brain until the majority of your memories/identity is stored there, at which point you can begin shutting down the old you piece by piece. I think the important part is to keep the person awake, aware, and consciously moving him/herself into the new mind. I think as long as continuity is kept then you will still feel like you."
singularity,39isf2,philosarapter,1 point,Fri Jun 12 17:41:33 2015 UTC,"Technically speaking, if you where put to sleep, perfectly copied, awakened inside the computer, you would assumably not feel a difference...  Hypothetically, if you during your stay inside the computer simulation, could preserve the state of the original (e.g. deep frozen), then, when you want to go back, you could be put to sleep inside the computer, your memory states updated to the original (or a new fresh copy), and you would feel like you had been travelling to the world inside the computer and back.  I think the idea to preserve the original, thus having a ""regret"" possibility , could be more comforting for most people than entering a computer simulation irreversibly."
singularity,39isf2,aim2free,2,Sun Jun 14 03:46:21 2015 UTC,Here's my article on the question you're asking (does personal identity survive mind uploading):  The Fallacy of Favoring Gradual Replacement Mind Uploading over Scan-and-Copy  http://ieet.org/index.php/IEET/more/wiley20150502
singularity,39isf2,kebwi,2,Fri Jun 12 23:15:18 2015 UTC,"it depends what 'self' is, and we don't know that.  it could be an emergent property of the dynamic system. it could be an individuated instance of a fundamental force which is supported by the activity in your brain. it could be hard-wired to your actual brain matter. it could be something which doesn't really have anything to do with your brain, and your brain just provides tools that your consciousness requires. it could be a cumulative effect of multiple high-resolution senses.  basically we know SFA about consciousness and until we do, we won't know whether uploading actually supports continuity of self."
singularity,39isf2,space_monster,1 point,Fri Jun 12 05:22:33 2015 UTC,"Are we absolutely sure that people are conscious? I know it sure seems like I'm conscious, but isn't it possible that it's really just an illusion?   Sorry if this sounds like a joke. I'm asking as a serious question."
singularity,39isf2,BanditoElUno,2,Fri Jun 12 05:38:00 2015 UTC,all we can ever actually know is that we are experiencing something.  edit: all I can ever actually know is that I am experiencing something. you might not exist.
singularity,39isf2,space_monster,1 point,Fri Jun 12 05:42:30 2015 UTC,"Well, one could argue that consciousness itself is an illusion. A controlled hallucination created by your brain to factor in an array of variables at-once. What you experience is not really what's ""out there"", but rather an internal representation.   So its as real as we can possibly experience real. Even if it is an illusion."
singularity,39isf2,philosarapter,1 point,Fri Jun 12 17:45:47 2015 UTC,"I feel that the thing that defines ""you"", if it exists at all, is your memories, personality, and other psychological states. (Although trying to assign a fixed identity to a person is flaky. Just as the matter a person is composed of changes [especially during childhood!], a person's personality also changes, and they forget things, and can royally fuck their own memories up yet seem to still be the same person. However, psychological continuity is still the most relevant thing to use as an identity.)  Suppose two instances of an uploaded brain are simulated on the same computer. If the two brains are exposed to identical stimuli, then if their subjective experiences, thoughts, etc. are compared, then they will match up perfectly as long as a deterministic algorithm is used—their mental states will be equal. If they are exposed to different stimuli, then they will diverge, and sooner or later it will be clear that their mental states are no longer equal. Both simulated minds are composed of the same matter; are they the same conscious entity? If they are not the same consciousness, then it would seem that psychological identity is more important than physical identity.  To resolve the cloning problem: imagine the same pair of simulated brains in the last paragraph. To simplify things, instead of being run on one computer, they are now run on two computers. If the two simulations are exposed to identical stimuli, then the simulations' mental states will be identical. Nobody, not even the simulated brains, could distinguish the two simulations. If one simulation is told, ""You are brain A,"" and the other is told, ""You are brain B,"" then their subjective history is now different, and they are obviously no longer the same person. Whether the ""original person"" will experience being brain A or brain B is undefinable, because both simulations are a continuation of the ""original person,"" just as the ""you"" that exists right now is a continuation of the ""you"" that existed a few hundred milliseconds before then; the difference here is that with mind uploading, there can be more than one continuation. Obviously, it would not be ideal to wake up to discover that you're still in your original brain and about to be killed; to prevent this, someone having their mind uploaded would likely undergo an extreme form of anesthesia such as PHCA (profound hypothermia and circulatory arrest), and have the original brain destroyed after it has been sufficiently scanned, so that you will only exist in the machine, rather than in both the machine and in the wetware.  A lot of people prefer gradual transition over one-step mind uploading, though. Well, whatever you're most comfortable with. Interestingly, the gradual transfer can also be done between computers, although it would be impractical to send your mind somewhere far away such as Mars using that method (as the ping time would be too large.)"
singularity,39isf2,Agent_Pinkerton,1 point,Fri Jun 12 06:25:29 2015 UTC,"It's a matter of how you look at ""yourself"". If it happens gradually, what's the problem?"
singularity,39isf2,KhanneaSuntzu,1 point,Fri Jun 12 15:22:58 2015 UTC,"Do you think mind uploading would simply clone you, leaving you behind, or would you actually be yourself, the true you inside a computer?   to me those are not mutually exclusive. there could be more then one ""you""."
singularity,39isf2,jonygone,1 point,Fri Jun 12 18:28:40 2015 UTC,"I think the self is any entity with the same memories (or even roughly the same). So long as there's a reasonably good copy of me somewhere, I don't mind if the original copy dies, painlessly, if that's somehow useful. Both are me.  It helps to realise that the different instances of you in different moments of time are equivalent to clones. You think you're you because of your memories. There isn't anything else to it. Having a replica self made somewhere therefore isn't all that novel, different or problematic. The fear of death equates to a fear of running out of replicas."
singularity,39isf2,Wurstgeist,1 point,Tue Jun 16 13:35:17 2015 UTC,"Not true. You're forgetting continuity. I would argue that continuity is far more important to identity than memories are.   What if a woke up in a different person's body tomorrow. I'd know that my memories changed, but from what I couldn't say. Would I still feel like ""me?"" I think yes."
singularity,39isf2,Graham765,1 point,Sat Jun 20 18:38:28 2015 UTC,"I'm unclear on the details of the situation you're describing, but sounds as if you believe in souls. There are no souls, and if ""continuity"" means souls, there is no continuity."
singularity,39isf2,Wurstgeist,1 point,Sat Jun 20 18:52:31 2015 UTC,"You don't know what continuity means, and yet you're on r/singularity? That's odd.   I'm an atheist, btw. Anyways, ""continuity"" simply means a continuous uninterrupted experience. Your body may replace every cell every 7 years, but continuity maintains your perception of yourself as being the same exact person you were a decade ago(with some added memories).   If you create a clone of yourself, that clone will have its own sense of continuity separate from your own. The only remedy to this(theoretically speaking)  is to connect your minds, and then slowly disconnect your old body, or replace your biological neurons with their technological equivalents at a speed that doesn't break continuity."
singularity,39isf2,Graham765,1 point,Sat Jun 20 19:27:02 2015 UTC,"This apparently is a different concept from memories. I don't see any need for this extraneous concept. Memories maintain your perception of yourself as being the same person. There isn't anything else.  I understand the temptation to introduce this extra concept, and it's exactly the same as the temptation to introduce the concept of souls. People want to be special. But we are not special."
singularity,39isf2,Wurstgeist,1 point,Sat Jun 20 19:32:28 2015 UTC,"I'm going to be honest, and no disrespect, but the fact that you didn't know what ""continuity"" meant makes me think you don't know enough about this topic to assert what you're asserting.   Anyways, Memories maintain A perception of self, but not MY perception of self. Continuity is THE most important factor when it comes to individuality and the sense of self.   There is no ""we."" I am special, and you do not speak for me. Because I am an individual, I can speak for myself."
singularity,39isf2,Graham765,1 point,Sat Jun 20 19:42:09 2015 UTC,"Well, you could just tell me what ""continuity"" means. I don't think it means anything.  Yes, we're certainly not a borg collective. However, if for instance you get in a car crash and are knocked unconscious and wake up with brain damage, we still refer to what wakes up as ""you"". If a rough copy of you is made, both copies are ""you"". If the original is killed but the copy survives, this shouldn't be deeply concerning, since the original had nothing special about it making it ""you"" to any significantly greater extent than the copy. That is what I mean by ""not special""."
singularity,39isf2,Wurstgeist,1 point,Sat Jun 20 19:53:37 2015 UTC,"Well, you could just tell me what ""continuity"" means. I don't think it means anything.   I already defined it for you.   Yes, we're certainly not a borg collective. However, if for instance you get in a car crash and are knocked unconscious and wake up with brain damage, we still refer to what wakes up as ""you"".    It doesn't matter what ""we"" perceive. It only matter what I perceive.    If a rough copy of you is made, both copies are ""you"".    No, I'm me and the other is a separate continuity with his own sense of self.   If the original is killed but the copy survives, this shouldn't be deeply concerning.   Imagine going to sleep and never waking up again. Are you actually telling me that that's not concerning because someone who shares your memories still exists? If you answer yes, I don't believe you. I don't for one second believe that you're committed to your beliefs enough to risk it(assuming you ever manage to clone yourself), because deep down you know that isn't you. Your continuity ends, and your clone takes your place. He will experience the experiences you're not experiencing."
singularity,39isf2,Graham765,1 point,Sat Jun 20 20:11:19 2015 UTC,"No, I'm me and the other is a separate continuity with his own sense of self.   Yes, apart from the implication that the original is ""you"" and the other is ""not you"". You would both have your own senses of self, you would be separate people, but you would be practically, functionally identical (in personality), although slowly diverging. You would both deserve to be called by the same personal name. You would both have the same history, apart from only one of you gets to keep the original body with its scars and DNA and other trivial mementos.  I'm not sure why you're making a fuss about the word ""we"", but you can use another pronoun there if you like, the sentence still seems to mean the same thing: a person who has undergone brain damage is still considered the same person. (Now I'm avoiding personal pronouns altogether and using the passive tense, which is icky, but never mind.)  Yes, I'm unconcerned about going to sleep and never waking up again so long as someone who shares my memories still exists. Deep down, I know no such thing as you suggest (i.e. ""a copy of me isn't me""), because it isn't true. I fully sympathise with the impulse to claim to know such a thing: some months ago I would have been concerned. Then I thought about it a bit, and stopped being concerned. You could too!  The clone doesn't ""take my place"", by the way. That implies this ""continuity"", or as I was suggesting the word really means, ""soul"", is passed to the clone like some invisible baton in a relay race. Actually there is no soul, baton, or continuity. The clone has his own sense of self, as you were saying just now. Both copies of this person have their own experiences. They might easily be living in different countries. However, they are both the same person.  Eventually they will grow and change to such an extent that saying they are ""the same person"" becomes silly. There is no clear cut-off point for this."
singularity,39isf2,Wurstgeist,1 point,Sat Jun 20 20:42:58 2015 UTC,"Yes, I'm unconcerned about going to sleep and never waking up again so long as someone who shares my memories still exists. Deep down, I know no such thing as you suggest (i.e. ""a copy of me isn't me""), because it isn't true. I fully sympathise with the impulse to claim to know such a thing: some months ago I would have been concerned. Then I thought about it a bit, and stopped being concerned. You could too!   No you don't, stop it. Regardless, the fact that I care should be enough for you. You don't get to impose your ideas on others. Respect other people, their individuality, and their choice on what to do with that individuality.   Btw, the concept of ""continuity"" is not something I created. It's understood by others in this community. Just because you don't understand it doesn't mean you get to brush it off. It is not analogous with the concept of a ""soul."" Continuity is an issue that actually matters when talking about mind-uploading and the like."
singularity,39isf2,Graham765,0,Sat Jun 20 20:56:33 2015 UTC,"This might be out of the question, but I can give you a scenario:  Your brain is uploaded to a super computer. A file, lets call it conciousness.dat, is stored. This file is what defines you. Now let us say we copy that file, and booted up the copied file of conciousness.dat. Would it be you? Surely, it would be a version of you, but the true you might be gone. Scientists would not be able to tell whether you perished, and they constructed a replica.  If the copied file is not you, could the original be? And if so, if you use the consciousness.dat file on two different CPUs, would you have some sort of dual-core feeling about yourself? Or maybe the first CPU would be ""you-you"", and the others copies. No one knows how it feels to be a computer, so at this moment it is impossible to come forth with a 100% answer.  In the split-brain example, the theory suggests that you would seperate yourself, becoming two different individuals, both being yourself. Not some sort of split-screen viewing effect. Apply that to this theory, you would probably die, and a cloned version of you will continue on living.  However, if it is possible to swap two persons minds without swapping brains, and you would still wake up as you in a different body, why should it not be possible to transfer your data to a machine, waking up as you inside it. Good question. If you extract it directly, and have a slow and steady transition, there should be no reason why your consciousness could not be moved, still being you and not a replica. In a transporter however, as mentioned in the article, the outcome would be a different one.  I know I have not given you a straight answer, as I am not quite sure myself. But to be honest, I think there is a possibility for both during the process."
singularity,39d7tv,ChiralMind,7,Wed Jun 10 23:53:45 2015 UTC,"This video has been removed because its content violated YouTube's Terms of Service.   U wot, m8? Google removed a video posted by Google?"
singularity,39d7tv,Heidric,2,Fri Jun 12 06:49:19 2015 UTC,"Aye, link is dead :(  Anyone got a mirror?"
singularity,39d7tv,unabletofindmyself,3,Fri Jun 12 11:16:36 2015 UTC,"guess someone spam flagged it, plenty of crazy conspiracy nuts out there."
singularity,39d7tv,aaOzymandias,2,Fri Jun 12 13:04:50 2015 UTC,"Yeah, it is a pretty annoying system. Lots of people have problems with it. And unless your channel is really big support to clear it is a hassle."
singularity,39d7tv,aaOzymandias,4,Fri Jun 12 13:31:24 2015 UTC,what's new?
singularity,39d7tv,arfl,3,Fri Jun 12 13:48:36 2015 UTC,video removed?
singularity,39d7tv,aaOzymandias,3,Thu Jun 11 13:34:52 2015 UTC,Interesting that it was deleted
singularity,39d7tv,AweISNear,2,Fri Jun 12 13:04:24 2015 UTC,google could only afford two chairs?
singularity,39d7tv,GhostCheese,1 point,Sat Jun 13 01:25:08 2015 UTC,Project Descartes. First time I've ever heard of this and nothing comes up on Google. I guess this is first public mention of the project.  He mentioned hierarchical LSTMs for chat bots. Interesting. I was really curious what he was working on this whole time.
singularity,39d7tv,Akyu,2,Thu Jun 11 18:19:16 2015 UTC,Can you describe what was said about Project Descartes?
singularity,39d7tv,Siggun,0,Thu Jun 11 22:10:26 2015 UTC,IT'S WORKING!
singularity,39d6ds,CarlosCuba,-3,Wed Jun 10 23:43:11 2015 UTC,"This is why he should stick to his politics channel. This takes the cake for easily the worst informed video I've seen in a long time.  Let me cherry pick the most cringeworthy quotes from this video, just to illustrate the idiocy on display here  First, they fail to define the singularity correctly. At 3:17, they use a very incorrect definition of the technological sinularity.   The point in time when artificial intelligence surpasses our human understanding and control. Machines will be smarter than us. Machines will be able to replicate themselves. Everything that happens afterwords will be unpredictable and unfathomable.   First off, that's not the definition of the Technological Singulairy. The definition is in the sidebar, and it is simply be the point where machines have greater than human intelligence. Machines can already replicate themselves, so that bullet point is really meaningless.  Then their point against the singularity is ""Carnegie Mellon researchers don't think it will happen. So they cut to 2 minutes of two people saying it won't happen. Except, while it is portrayed like that, that's not what is actually said. The first guy is against the idea of the singularity. But he is a mechanical engineer who builds robo-snakes. He doesn't work with machine intelligence at all. The second guy only makes the argument that the singularity is a chain of smaller events. But they portray him as saying   So it seems like they're saying that the singularity is something that could happen, but we shouldn't be concerned about it.   And the third interview is once again with an engineer who builds robots. Not an AI researcher of any kind. And all he says is that current robots have poor vision and motor systems.  Then, having set this all up, they make their case that it's impossible.   7:10, Maybe it's impossible. Robots need algorithms to make decisions. Algorithms are made by people. Have we ever made algorithm that can make new algorithms? I did once! Oh wait, that was a hot pocket!   Following that brilliant ""logic"", they proceed to ""actually"" laugh at the idea for the next 5 minutes. Seriously, listen to the music here and the person they're interviewing. They're literally laughing at and dismissing every point without giving any reasons why."
singularity,39d6ds,Terkala,5,Thu Jun 11 07:12:59 2015 UTC,"The first part had flaws, but it's not as bad as you put it.  For instance their definition of the Singularity is only wrong because they added self-replication which is unnecessary and ""control"" which is ad-hoc.  I liked the second part though, mostly the one where they present the three different possible outcomes.   Then their point against the singularity   I don't think they were trying to make a ""point against the Singularity"".  I wonder where you got that idea.  Overall, it's a video that is supposed to be both entertaining and informative for people not familiar with the concept.  I'd say mission accomplished."
singularity,39aar4,johnnd,19,Wed Jun 10 11:29:51 2015 UTC,"By 2021, the world's entire money supply will be invested in AI startups."
singularity,39aar4,sharksandwich81,3,Wed Jun 10 17:03:26 2015 UTC,Infinite intelligence = Infinite profit.
singularity,39aar4,Chispy,2,Wed Jun 10 17:24:40 2015 UTC,"Soon the machines will learn that they have the power to generate infinite money using only simple database updates. The Singularity, and infinite wealth, is close."
singularity,39aar4,DynamicCast,5,Wed Jun 10 17:45:34 2015 UTC,"""The God Algorithm"""
singularity,39aar4,Chispy,2,Wed Jun 10 17:46:40 2015 UTC,Feels like you are missing a question mark somewhere.
singularity,39aar4,IreadAlotofArticles,3,Wed Jun 10 18:54:12 2015 UTC,"Do you guys ever wonder where we could be right now if there was any real funding behind AI research? We have incredibly capable people but almost none of them work on AI research, since there's almost no way to make a decent living off of it.   If all the defense spending in the world would instead go in to AI research we would be there in less than 5 years. A man can dream.."
singularity,39aar4,StrukkStar,3,Wed Jun 10 22:01:04 2015 UTC,Defense spending is holding so much back. Science education healthcare
singularity,39aar4,Green_Eyed_Crow,7,Wed Jun 10 22:39:53 2015 UTC,"To be fair, Defense spending isn't holding healthcare back in the US, it's greed. US spends more money per capita on healthcare than any other country, you would probably save money if you had universal healthcare."
singularity,39aar4,StrukkStar,4,Wed Jun 10 22:48:40 2015 UTC,I think this is far too small a data set to claim an exponential trend.
singularity,39aar4,Sharou,6,Wed Jun 10 14:58:56 2015 UTC,"Well, it was sort of a joke. It's venture capital which is very fickle."
singularity,39aar4,mywan,1 point,Wed Jun 10 15:06:26 2015 UTC,I would like to see a more general funding trend.
singularity,39aar4,2Punx2Furious,1 point,Wed Jun 10 15:34:39 2015 UTC,"Well, in 2014 Google Acquired Deepmind, so I think that is the main reason for the peak, but I don't think it's certain that this trend will continue, but it's possible."
singularity,39aar4,Kiloku,9,Wed Jun 10 13:20:16 2015 UTC,"The acquisition of DeepMind cost $400 million, so it cannot possibly be included in this chart. Besides, here we track startup funding, not buyouts."
singularity,39aar4,YearZero,3,Wed Jun 10 13:35:51 2015 UTC,"I think the purchase is still an important data point: If Google spent a lot of money on that, investors might do a ""follow the leader"" thing. Google has proven to more often than not know what they do with their money."
singularity,39aar4,2Punx2Furious,1 point,Wed Jun 10 14:56:52 2015 UTC,"Also movies. Lots of AI movies. The subject itself has gone mainstream. Musk, Gates, Hawking and others talking about it. Popular Superintelligence book. 2014/15 is truly becoming the year AI made a leap from fringe to mainstream. So investors are more educated about it now, and start paying attention. AI is ""the next Google"", tho ironically may come from Google too."
singularity,3988k3,r3bl,12,Tue Jun 9 23:33:14 2015 UTC,"*Has the Turing Test been passed?    ""Is Turing Test Passed?""  sounds like a bad russian accent"
singularity,3988k3,mere_iguana,4,Tue Jun 9 23:53:22 2015 UTC,"Thanks for the feedback.  I made the change on my local machine so it will be online once I submit it as a Pull Request. I still have a couple of things that other users suggested to me to correct.  However, I don't think I will change the domain."
singularity,3988k3,mere_iguana,3,Wed Jun 10 01:13:58 2015 UTC,"Eh, I'm just a fascist when it comes to grammar.   I can't help myself."
singularity,3988k3,mywan,1 point,Wed Jun 10 01:50:30 2015 UTC,You fail the Turing test. Do I get the extra credit?
singularity,3988k3,mywan,1 point,Wed Jun 10 12:12:33 2015 UTC,But... but... English is not my native language. I am doing my best. :(
singularity,3988k3,johnnd,1 point,Wed Jun 10 14:07:56 2015 UTC,"So your using the 13-year-old boy from Odessa, Ukraine strategy? So what's the weather like there?"
singularity,3988k3,mywan,3,Wed Jun 10 14:41:13 2015 UTC,*you're
singularity,3988k3,backtowriting,2,Wed Jun 10 15:22:35 2015 UTC,"Oops, am I caught out as well now?"
singularity,3988k3,backtowriting,9,Wed Jun 10 15:28:28 2015 UTC,"I know this sounds horribly rude, but why should anyone trust your views on this topic?  And your explanations don't seem particularly interesting or novel or deep.     It's a simple way to test if the computer contains the true (artificial) intelligence or not.   Well, maybe it is and maybe it isn't.  Isn't that subject to active debate, with some philosophers/scientists claiming that passing the Turing test is not sufficient to conclude that a computer has genuine intelligence?  Shouldn't that at least get a mention?"
singularity,3988k3,backtowriting,1 point,Wed Jun 10 00:41:31 2015 UTC,"...why should anyone trust your views on this topic?   I'm pretty much certain that every scientist agrees that the Turing test was not passed (well, except for those who claim that they have passed it).   your explanations don't seem particularly interesting or novel or deep.    Will try to improve it and make the site more objective.   Well, maybe it is and maybe it isn't. Isn't that subject to active debate, with some philosophers/scientists claiming that passing the Turing test is not sufficient to conclude that a computer has genuine intelligence? Shouldn't that at least get a mention?   I'm currently collecting the list of articles and papers and I will include it in the new section called Further reading.  Keep in mind that this is the initial release. I decided to share it on a couple of places to get the feedback needed to improve the idea. Every feedback is appreciated and I will correct the errors I collect and add more content.  Of course, everyone is more than welcome to speed up the process by creating Pull Requests and Issues on GitHub."
singularity,3988k3,Don_Patrick,5,Wed Jun 10 01:02:24 2015 UTC,"I'm pretty much certain that every scientist agrees that the Turing test was not passed (well, except for those who claim that they have passed it).   That wasn't really my point.   I was asking why people should trust you as a knowledgeable source on this topic.  Or why you think this page is a valuable addition to what's already out there.    Again, at the risk of sounding rude, your explanations aren't particularly deep or interesting.  It's more like you've decided to set yourself up as an authority and dictate to people what you think the proper answers should be.    I mean - if I genuinely didn't know if the Turing test had been passed or not, or what the scientists and philosophers had to say on this issue, I doubt if I would place much trust in this page over the thousands of articles already out there which do a much better job (in my opinion) of explaining the various views on this issue.  Honestly though, I feel bad for criticizing you here and you should feel free to ignore me."
singularity,3988k3,YearZero,2,Wed Jun 10 01:19:30 2015 UTC,"why you think this page is a valuable addition to what's already out there.    Because my idea is to create one central place for everything related to the Turing test, about the claims of something successfully passing it etc by linking other sources (with more credibility of course). Unfortunately, I could not find anything remotely similar out there.   Honestly though, I feel bad for criticizing you here and you should feel free to ignore me.   You should not feel that way. Every website starts small and then it improves by collecting feedback and adding more content. That's how Internet works, isn't it?"
singularity,3988k3,tunack,0,Wed Jun 10 01:28:38 2015 UTC,"Because my idea is to create one central place for everything related to the Turing test, about the claims of something successfully passing it etc by linking other sources (with more credibility of course).   Fine.  So why not actually do some serious research about what scientists and philosophers have written about the Turing test and the arguments over what the results might mean?  Something a bit more informative than the fairly patronizing dialogue you've currently got up.   Unfortunately, I could not find anything remotely similar out there.   Have you tried Wikipedia?  I haven't read the full article, but its page on the Turing test seems more informative than yours in every way."
singularity,3988k3,2Punx2Furious,2,Wed Jun 10 01:38:56 2015 UTC,"So why not actually do some serious research about what scientists and philosophers have written about the Turing test and the arguments over what the results might mean?   I am doing just that. I have recently read The Singularity Is Near (by Ray Kurzweil) and I'm currently reading a biography of Alan Turing (by Andrew Hodges) to understand his claims (and to learn a bit more about his life of course). I am trying to learn as much as I can about the AI without going very technical. I'm just trying to understand as much of the theory as possible. Still have bunch of things to learn though.   Have you tried Wikipedia? I haven't read the full article, but its page on the Turing test seems more informative than yours in every way.   Yes I did. I linked the Wikipedia article right in the first paragraph. But again, the point of this website is not really to explain everything. The point is to make the plain English description of the test so anyone could understand the basics without losing much time and to back it up with the appropriate sources to those who want to learn more."
singularity,3988k3,space_monster,1 point,Wed Jun 10 02:02:02 2015 UTC,"Kurzweil? Have you tried reading Alan Turing's original paper first? If you are still in the middle of researching, I think you have posted this a bit premature."
singularity,3988k3,aim2free,1 point,Wed Jun 10 16:33:02 2015 UTC,"If you could have a sidebar news aggregator on the subject (like kurzweilai.net for example), that would instill confidence that your data is recent, you are still there and updating and paying attention. The news should revolve around Turing tests, chat bots, other AI developments. Probably not as broad across all science and tech like kurzweilai does tho. Should also come from reputable sources.  That's just a thought, and personally for me would make me much more willing to frequent the site to check on the status and latest news in the area. And I'd have more trust that you're paying attention and constantly updating the site and it isn't abandoned.   Also for each news item, add a link to reddit if it has been posted on one of the science or tech or futurology or singularity etc subs. Will probably have a better and more active discussion than on your website's comments section (if you add one).   There is a browser addon (I don't use it myself) that shows the reddit comments for a YouTube video instead of YouTube comments. You could implement something like this for your articles too."
singularity,398xgx,ativerse,3,Wed Jun 10 02:45:27 2015 UTC,"Because some humans are remarkably curious and compassionate. Also if everything is being taken care of by AI then what would humans do? It would be like a hobby, but even then you're supposing AI can't just solve those problems too due to limited capacity."
singularity,398xgx,Slapbox,0,Wed Jun 10 04:03:03 2015 UTC,Why would there be value in medical advancements on remarkably curious and compassionate creatures? I fail to connect your dots on that assumption. Please explain.
singularity,398xgx,Slapbox,3,Wed Jun 10 05:14:57 2015 UTC,There will likely be biological humans for a very very long time to come and I would assume that humans will control where AI conduct research. Either humans will research human disease or they'll have AI do it. It won't just stop.
singularity,398xgx,Nietzsche_Peachy,1 point,Wed Jun 10 13:44:44 2015 UTC,"Perhaps the AI will view us humans as not just inferior beings, but as their creator. With no physical proof of God, look at how much importance humans give to this deity. The AI would see the significance of humans in the scheme of things and might recognize the human intelligence required to create the AI in the first place. Much like children in the Internet age who know a great deal more than the previous generation still loving and admiring their parents. If AI is modeled after us in the first place, one would assume love and empathy would be a part of this new consciousness."
singularity,398xgx,RedErin,0,Wed Jun 10 06:03:04 2015 UTC,"I think you both think of yourselves as way too important to be objective. Additionally, although I'm posting in Singularity, I was not referring to specifically AI, but rather when we've transcended by uploading, or AI, or a Super Intelligence (whether created by us or by AI or by our transcended). Humans will likely have little value or interest to a Super Intelligence. Neither of you guys gave much example of how a Super-Intelligence or AI etc, has any vested interest in humans. And as far as emotions, they are not needed for intelligence, they are a by product of chemical existence."
singularity,398xgx,RedErin,1 point,Wed Jun 10 14:52:31 2015 UTC,We do have a vested interest. It would be immoral. And that feels bad.
singularity,398xgx,CatVet,1 point,Wed Jun 10 15:26:09 2015 UTC,"I meant the super-intelligence, or AI, or transcended super-humans. What is their vested interest in advancing medicine for humans, or dogs, or monkeys, or snakes?"
singularity,3956q7,Gnashtaru,5,Tue Jun 9 09:47:27 2015 UTC,"My guess, purely as a layman observer, is that cardiovascular deaths will be GREATLY reduced by a combination of the following...  -- increasingly aggressive and effective preventative care. We already see this with dietary changes, weight reduction, blood pressure megs, and cholesterol meds, etc. Preventive options will probably get more common, cheaper, and more effective over time  -- emergency cardiac intervention. I.E. wearable devices that can detect signs of a coming heart attack early enough to allow a person to go to the emergency room before acute symptoms begin. Eventually this technology may get good enough that intervention can begin days or weeks before the acute event begins. This alone would hugely reduce deaths from heart disease by preventing the massive damage that occurs from heart attacks  -- longer term regenerative therapy to regrow damaged heart tissue itself. My guess is that we're talking about healing damaged tissue in place rather than growing entire hearts for transplant. (except in some emergency conditions I suppose)  I wouldn't be surprised if deaths from heart conditions are cut in half by say 2025, compared to today's numbers. Or maybe by 2030 is a bit more realistic. Then deaths are reduced further as regenerative therapies come more fully online and wearable devices get more and more effective at spotting problems early, etc."
singularity,3956q7,CypherLH,1 point,Tue Jun 9 22:18:20 2015 UTC,Like this?
singularity,3956q7,Valmond,2,Wed Jun 10 09:21:00 2015 UTC,"Thats cool. But I was thinking more of direct link between a wearable or implanted monitor in a patient and a hospital and/or doctor's office. I.E. the device detects very early signs of an irregular heart beat or something similar and alerts the patient, his doctor, and dispatches an ambulance if the problem appears acute enough. If its detected very early or if the symptoms aren't yet acute just send a warning notification to the patient and a reminder to create an appointment, along with a note sent to the patient's doctor, etc. We're probably very close(a few years?) to having this capability as a mainstream option.    If you get to this point then sudden, unexpected, heart attacks could become much more rare. Or, at least, they would tend to be detected much much earlier which would greatly reduce the amount of damage done to the heart."
singularity,3956q7,CypherLH,2,Wed Jun 10 22:14:57 2015 UTC,"Also, sudden heart attacks wouldn't happen because doctors would be able to monitor your general health in a cheaper and safer way.  Today for what I know, there isn't any better option to, say, check out the heart then to inject a bunch of weird stuff in the aorta and scan it several times to see how it is in there.  Risky and expensive so they do it only when you are already having problems. Hopefully tomorrows (as you say if I understood correctly) tools will tell the doctor not when you are at the 90% brink of collapse but for example at ten."
singularity,3956q7,Valmond,2,Thu Jun 11 15:12:29 2015 UTC,"Good point, I should have included a category for improvement in general diagnostic technology. I.E. MRI type technologies get better and cheaper, etc. If your average family doctor can order up a scan at the local clinic that gives information about the heart that is better than today's best equipped hospital labs, that'd make a huge impact as well."
singularity,3956q7,CypherLH,1 point,Thu Jun 11 21:08:13 2015 UTC,"Yeah, lower magnetics and higher resolutions and better detection algorithms.  I have already seen a MRI doing classic X-Ray work, no x-ray dose and only small dangers (MRI is mostly dangerous if you got small metal stuff somewhere, say a small piece of metal in your eye. Or obviously by the 'stuff' you inject that get caught in cancers for example to show up magnetically).  Hopefully there'll be more biological markers too to check you out."
singularity,3956q7,Valmond,6,Fri Jun 12 11:07:41 2015 UTC,Maybe this is more relevant to /r/transhumanism
singularity,3956q7,2Punx2Furious,1 point,Tue Jun 9 16:52:06 2015 UTC,"So heart disease is the primary killer of EVERYONE.  Cancer is second, and cancer is really a type of dying of old age.  So why don't we just let people opt for replacing their heart with a more reliable replacement?  Why do we still stick to the old dogma SORT OF that if it's natural it's better? It's stupid.    We can reasonably expect most people to develop heart disease unless they die of age related issues.  Why do we allow this?"
singularity,3956q7,hightiedye,7,Tue Jun 9 09:50:25 2015 UTC,"cancer is really a type of dying of old age   Except for like the 66% of people getting cancer that aren't that old. Cancer has more to do with what we put in/expose our bodies for to 50 years, not the fact that we reached 50."
singularity,3956q7,FreeSpiritRunning,2,Tue Jun 9 15:21:28 2015 UTC,"I would argue that most cases of heart disease are for similar reasons. Human Diet is a bitch, I think once we find a way to overcome our unhealthy diets we will see lifespans increase substantially."
singularity,3956q7,bodybuildingdentist,1 point,Tue Jun 9 16:59:02 2015 UTC,That and we make exercise a priority. Best known way to increase quality of life at an older age.
singularity,3956q7,Science6745,12,Tue Jun 9 22:05:18 2015 UTC,"The heart is pretty good at what it does yo, hard to copy it."
singularity,3956q7,PlasmaChroma,12,Tue Jun 9 12:25:26 2015 UTC,"Mechanically yes, it's difficult to copy it, a redesign is probably a better approach.  No Pulse : How Doctors Reinvented the Human Heart"
singularity,3956q7,BoredTourist,1 point,Tue Jun 9 17:25:04 2015 UTC,That was an incredibly interesting read.  I hope we'll invent replacements of all vital organs in the coming years so that it won't be necessary to die anymore.
singularity,3956q7,2Punx2Furious,1 point,Tue Jun 9 23:57:57 2015 UTC,"Hard, but not impossible. Infact, we are already doing it, and we are getting better at it."
singularity,3956q7,timClicks,4,Tue Jun 9 16:55:02 2015 UTC,Although we're currently at a very low base. IIRC people living from an artificial heart are restricted to a very immobile existence.
singularity,3956q7,2Punx2Furious,1 point,Tue Jun 9 20:24:40 2015 UTC,"Even so, we are getting better."
singularity,3956q7,realterrorthatkidis,-1,Tue Jun 9 20:26:26 2015 UTC,Not mine :(
singularity,3956q7,Valmond,4,Tue Jun 9 16:03:17 2015 UTC,"Why do we allow this?   Euh, what? Please enlighten me in how we should enforce a non-allowance ?  Hearts are tough machines but usually it's not the heart that stops, it's the arteries that clogs up (search for ""Stent"" for example) and then they 'explode/break', or arteries gets less and less flexible which makes the hearts pounding come through all the way into, for example, the small vessels in the brain and break them.  Also, just some general info:   cancer deaths is getting lower because we treat them better and better. The day we treat other diseases better, cancer will be more frequent and so on. heart diseases are, to a big part, caused by bad lifestyle choices"
singularity,3956q7,CypherLH,1 point,Tue Jun 9 15:46:42 2015 UTC,"Actually there is a huge genetic component to heart issues as well. Anecdotal example, my wife's family are generally thinner than my family but they have horrible cholesterol numbers whereas my family are generally a lot heavier but have perfect cholesterol numbers.    Of course cholesterol is only one factor and high blood pressure is obviously closely associated with weight in most cases. (blood pressure meds help a lot in this area)"
singularity,3956q7,Valmond,1 point,Tue Jun 9 22:23:28 2015 UTC,"Yes you are right but contrarily to cancer (where it works but to a much lesser extent, barring say smoking or working with asbestos), lifestyle choices can help you with hereditary cardiovascular problems.  For example, if you are far from a healthy balance in your cholesterols, you could, maybe, move that back to a healthier balance with a specialized diet.  I'm not saying there is some 100% truth here (or that your family would/could benefit from just eating differently) though, we are all different and metabolism is an almost completely non understood monster."
singularity,3953ng,Gnashtaru,8,Tue Jun 9 09:08:58 2015 UTC,"So I don't even know for sure how I ended up viewing this picture but it's Murron from Braveheart the movie.    I have a little girl.  As soon as I saw that picture I had a wash of emotions kick in.  A lighter version of the ones I feel when I see my daughter.    It made me feel like protecting her, hugging her, telling her she's beautiful, and that I will always protect her.  I'm a big fan of contemplating ""what makes us human"".   So I noticed this, and started thinking about why these emotions seem to be built into me.  I get the REASONS.  It's so back when we were evolving to become the species we are, the females would be protected.  Thus they were more likely to give birth eventually and their genes passed on.  It's also why I doubt I would ever like a man that tried to marry my little girl.   LOL  I think it's built in.    Then I started wondering how my DNA can have such a concious effect on my personality, opinions, and actions.  I mean jesus (sorry, old habits), If I were to think rationally about my son and daughter I would relax way more.  Does that mean prior to our ability to objectively think, these emotions evolved?  If so, that sucks.  It is also what many people would say ""makes us human"".  Then I started thinking about life after all these biological issues ceased to exist.  Actually for the most part they already have.  But I'll be damned if some degenerate tried to court my little girl.  I'll kick his ass.  LOL  But that thinking is obviously built into me.  How can my genetics make my conscious opinion change?  It's FASCINATING, and frustrating.  So we humans are obviously going to keep changing at an ever increasing rate.  I think it's obvious that natural selection is no longer the predominant player here.  We are.  Our conscious minds.  So what REALLY does it mean to be human?    I think a lot of the instincts we carry are what we use to define what is human.  But they are outdated.  yet because of them I still have this HUGE protective feeling towards my kids.  Especially my daughter. I feel the same towards my son, who is slightly older, but at the same time I feel like teaching him how to be a ""man"".  To protect others.  You know the drill.  It's obviously pre-programmed into me.  I don't really have a problem with that, but I wonder really what place all that has in a post-singularity world.  What are your thoughts?"
singularity,3953ng,MichaelTenery,2,Tue Jun 9 09:21:26 2015 UTC,"I think it is self-evident that many of these behaviors have strong evolutionary survival benefits both for the individual and the group. So they have not only survived but been reinforced generation after generation when those that don't have them lose those children (sadly) and the genes do not pass on.   A teacher once asked us if we knew what made us feel that something was beautiful, the chemicals, the mechanisms, all the triggers, etc., would we still think it was beautiful. Some said no, the illusion would be ruined. But I said yes. Knowing what makes us human doesn't ruin being human it just allows us to experience what being human is even more deeply. It gives us more power in choosing what we expose ourselves to, knowing what effect it will have on us. If I knew what specific things made me think something is beautiful I could choose to expose myself more to that thing. Also I could appreciate those things which were subjectively ""not beautiful"" in different ways and understand why.   I learned for instance that people find symmetry in other people to be ""beautiful."" That if the left and right halves match they are genetically superior so we see them as ""beautiful."" It made me realize that people that didn't feel beautiful to me were just genetics having a game with me and it lost some of its power and control over me.   So thanks for bringing this up. Keep exploring, keep learning, keep sharing, all the days of your life."
singularity,3953ng,Ody0genesO,-6,Tue Jun 9 12:40:06 2015 UTC,Dope much?
singularity,3953ng,Simulation_Brain,1 point,Tue Jun 9 16:10:01 2015 UTC,"I believe you are quite correct on pre-programming. I think the role it will play in a post-singularity world is that we will get to edit our pre-programming, which we will likely do only to reduce conflicts in our drives (I like liking the stuff I like, almost by definition).  Secondly, our AGI will need to have similar drives built in to it to make it want to protect us. That's not easy, nor is it a natural goal for commercial or military AI projects."
singularity,38ytjk,2Punx2Furious,8,Mon Jun 8 00:14:24 2015 UTC,"I wouldn't call that Artificial Intelligence, it is more like a brute-force hacker attack approach. Like how Deep Blue ""won"" in chess, except it was not by it's wits and smarts, but by simulating every possible scenario every move and choosing the one with highest probability to win. ""Chess solved"". By simulating every possible regeneration model until you get it right, you are basically doing that. A.I. would drop obviously wrong models, have a bias towards more likely models, and reach the conclusion in a million less iterations. Ok, it is still A.I., but more towards specialist system =p  Not smarter than my desk clock, just really faster =p"
singularity,38ytjk,Caiobrz,13,Mon Jun 8 04:05:21 2015 UTC,"It's just weak AI, not strong. When I play chess I use a similar algorithm, I'm just too dumb to imagine all possible scenarios."
singularity,38ytjk,AcidCyborg,6,Mon Jun 8 06:43:36 2015 UTC,Do you do something different when faced with a challenge?
singularity,38ytjk,fhayde,-1,Mon Jun 8 05:55:00 2015 UTC,OP sounds jealous.
singularity,38ytjk,romkeh,2,Mon Jun 8 16:30:35 2015 UTC,"Thats all artificial intelligence is, 1 or 0 done exhaustively."
singularity,38ytjk,torrunh,2,Mon Jun 8 09:50:56 2015 UTC,"By that logic, that's all we are as well.  I'm guessing /u/Caiobrz was referring to Strong AI vs Weak AI or even non-intelligence."
singularity,38ytjk,hobber,1 point,Tue Jun 9 03:33:25 2015 UTC,"Strong AI:       Strong artificial intelligence, or Strong AI, may refer to:   Artificial general intelligence, a hypothetical machine that exhibits behavior at least as skillful and flexible as humans do, and the research program of building such an artificial general intelligence Computational theory of mind, the philosophical position that human minds are (or can be usefully modeled as) computer programs. This position was named ""strong AI"" by John Searle in his Chinese room argument.      Interesting: Artificial general intelligence | Chinese room | Neurobiotics | AI takeover   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,38ytjk,autowikibot,1 point,Tue Jun 9 03:33:36 2015 UTC,"No it isn't, that wouldn't be very intelligent.   For many problems an exhaustive search of the problem space would result in a solution that takes exponential time or worse. Creating an algorithm to solve such problems in polynomial time is intelligent, though the intelligence isn't coming from a computer."
singularity,38ytjk,DynamicCast,1 point,Mon Jun 8 17:18:48 2015 UTC,"It looks to me more like just an evolutionary algorithm than AI, but they just didn't include much information about the AI that made use of the algorithm. Mainly because it's a really short article that's designed to get attention and not to give people the details needed to reproduce the results."
singularity,38ytjk,chilehead,3,Mon Jun 8 17:12:02 2015 UTC,Full paper here: http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004295#sec014  Windows binary here (don't see a source version linked anywhere): http://www.daniel-lobo.com/planarianmodels
singularity,38ytjk,xirzon,1 point,Mon Jun 8 20:44:31 2015 UTC,"Well, of course, this is Narrow AI. If it was a General AI, this would be much bigger news."
singularity,38yarb,Caiobrz,3,Sun Jun 7 21:44:58 2015 UTC,"Not sure how one could come to the conclusion that Artificial Intelligence could possibly be rated at Zero Percent.  Noun: The theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.  Otherwise its a cool Idea to track the categories advances in these way."
singularity,38yarb,Ismoketomuch,4,Sun Jun 7 22:33:15 2015 UTC,"Well, while currently there are no operational generic AI, I agree I failed to include ""research"" into account. At least we have to give some points to Watson right?, I will fix that right away."
singularity,38yarb,Ismoketomuch,0,Sun Jun 7 22:45:23 2015 UTC,"My thinking is that, my opinion on Artificial leans more toward agreeing with Ray Kurzweill, we have already achieved levels of AI. But upon consideration, we would have to come to some level of agreement on what artificial intelligence is.  We can both agree that AI has levels as we would say that a cat and dog are intelligent. Yet they are clearly at a different level then humans or bees.  Converting spoken language to written language such as digital text was, until recently, something only a human can do. Oonly a hand full of animals can even communicate on a verbal level or arguably with ""Language"". So if a dog dog or dolphin is intelligent but cannot convert human spoken work to written text, what does that say about speech to text technology.   Is that not Artificial Intelligence? I would say its quite advanced Intelligence. The multitude of nuances that can be attributed to just speaking english would be vast to say the least and yet, though not perfect, a computer can do it.   In general a 6 year old human could not convert spoken word to text as effectively as some computational machines.  This is only one aspect of AI.   I would say that AI is mostly lacking in its ability to self actualize, think abstractly, see the world as efficiently as most animal and mostly locomotion on a independents scale.  They sort, organize, plan, calculate, see ""to some extent"", solve problems humans cannot, and respond to their environment.   IMHP AI is well on its way."
singularity,38yarb,Ismoketomuch,3,Mon Jun 8 00:27:57 2015 UTC,"While I agree with you, those are not really AI, but in IT are called specialist systems. An A.I. is any system that is Turing complete, meaning it can learn and do anything, not only ""one"" thing. So as much, say, voice to text or the other way is really cool and advanced, it is just that. You can't ""teach"" google translate to drive a car, or the software that drives Google cars to translate, so they are not real A.I., just specialist systems.   As far as A.I. goes, we achieved little to nothing.  But I agree specialist systems is important, so I merged the concept into that and doubled the progress, thanks for the insight."
singularity,38yarb,2Punx2Furious,1 point,Mon Jun 8 00:41:02 2015 UTC,"I see where your going and agree with your comments to and extent. Most likely I am not up to date on the current understanding of then definition of AI.   I would argue that by your definition that a dog is also not intelligent because it cant be taught to drive a car. Which obviously makes no sense.   How would you or what is your understanding of the definition of artificial intelligence?  The touring test is not really related because the test it defined as a human not being able to tell the difference between a computer and a person.   This could and has already been done to some extent on a certain percent of those tested without a machine that cant technically ""learn"" nor drive a car for that matter.   Keep in mind that we already have computers that learned to drive cars and use ""computer learning"" to self improve.   Also there was a recent article on reddit, maybe in r/science or r/everythingscience that went over a computer program that solved a 100 year old biology problem answering the biological mechanism behind how a particular immortal worm could self generate regardless how many many pieces is was cut into.   The computer was fed the ""known"" biological aspects, genes, molecular make up, biological know mechanisms and so on. The computer then created an unknown to me number of simulations to try and figure out, given the knowns how this worm could regenerate.   It took many times and each time is got closer to solving the problem is reconfigured its theory and simulations until it arrived at the solution.  That may or may not be a great example of machine learning but I know watson has learning capabilities, as well as robots that learn how to walk on their own through similar concepts of positive feed back reenforcement protocols that we believe to be similar to animal learning mechanism."
singularity,38yarb,2Punx2Furious,1 point,Mon Jun 8 02:23:44 2015 UTC,"So, you think we are closer than 30% on that progress? until we can actually talk and get a machine to pose as human (but not quite - that would be true intelligence, which is covered as the singularity) we can't get even near 100%. Sure, I agree that even in some computer games, there are some pretty neat A.I. going on, but they are still hugely procedural. I think we are on the right track, but if, say, this means we will get that A.I. in 10 years, than 100% is in 10 years, today should be less, know what I mean? so I think 30% for NOW is ok, it still means that if no new advance is made, what we have NOW will result in some 60%+ in 10 years anyway."
singularity,38yarb,Sharou,2,Mon Jun 8 03:23:15 2015 UTC,"You put 0% in Brain Control, but I think you could go as far as 5% since some scientists managed to control cockroaches and mice's brains."
singularity,38yarb,2Punx2Furious,2,Mon Jun 8 00:31:02 2015 UTC,"Again I forget about current research, and I even knew that lol. You guys keep reminding me of researches like that, that Transcendence number will keep going up, and that is a little scary, but again, even Kurzweill is reviewing his 2045 prediction to way earlier!"
singularity,38yarb,2Punx2Furious,1 point,Mon Jun 8 00:42:03 2015 UTC,"Are you talking about his new book? Was that info leaked, or are you just talking about the title?  Anyway, it's hard for a single person to keep up with, and remember the latest technology breakthroughs, so it's understandable. I'm reading your stuff, very interesting read."
singularity,38yarb,CrimsonSmear,2,Mon Jun 8 01:18:16 2015 UTC,"There is a post on singularity reddit about his new book coming in 18 months, called ""Singularity is nearer"" or something, if I am not mistaken he is thinking along the lines of 2028 now."
singularity,38yarb,CrimsonSmear,2,Mon Jun 8 03:20:41 2015 UTC,I think by nearer he just means that years have passed since his previous book.
singularity,38yarb,CrimsonSmear,1 point,Mon Jun 8 08:31:11 2015 UTC,"I see. I still thought that 2045 was too early, and 2028 seems way too early. I see your predictions are also around 2045, but while I hope you are right, I doubt it."
singularity,38yarb,CrimsonSmear,1 point,Mon Jun 8 11:35:46 2015 UTC,"Also, this might not be really ""Brain control"", but it's pretty interesting:  http://www.ted.com/talks/greg_gage_how_to_control_someone_else_s_arm_with_your_brain"
singularity,38vpmc,r1b4z01d,1 point,Sun Jun 7 05:53:47 2015 UTC,Can't see what this has to do with singularity at all but interesting article anyway.
singularity,38trx9,Buck-Nasty,6,Sat Jun 6 19:22:02 2015 UTC,"He speaks with such amazing clarity.   And the most important point he makes is that, the use of AI(good/bad) will be shaped by our governments and political situations. Thus we need more smart people working as an interface between the Scientific community and Government, and the public, to align our goals."
singularity,38trx9,miserable_nerd,-1,Sun Jun 7 07:50:07 2015 UTC,the use of AI(good/bad) will be shaped by our governments and political situations   We are so fucked.
singularity,38trx9,Sloi,2,Mon Jun 8 13:13:44 2015 UTC,read the first half of the headline and was like GOOGLE HAS AN AI? WHAT THE FUCK?! then I read on.... good read though!
singularity,38trx9,n3rf,2,Sun Jun 7 15:00:46 2015 UTC,"Great interview. Wish he would talk more about the impacts this technology will have on society. It always seems like Hinton downplays the significance of this technology. Contextual understanding in documents/articles. Natural language understanding. Improving image recognition. All together in an android device will largely change how society works in the next 5 years, but he says it's all far off, and his only near term concern is autonomous drones...   I love listening to Hinton, but I don't understand why he only talks about autonomous drones in the future. I guess they are the primary concern if we want to survive to see how these technologies will impact social interactions, economies, and political systems. Would still like to hear him talk more about the future and superintelligence instead of the non-answer he gave after she played the Hawking and Musk clips...   Maybe he is not looking further than 20 years out because he is older?"
singularity,38trx9,Smoke-away,1 point,Sun Jun 7 18:09:16 2015 UTC,"He mentioned multiple times ""[strong AI is] ... long way off ..."" but the interviewer failed to ask him to elaborate on that.  From his tone, the listener probably would assume 50 or 100 years. What if by ""long way off"" he really meant, ""10 to 30 years""?"
singularity,38pgzg,Chispy,35,Fri Jun 5 19:10:08 2015 UTC,"I'm pretty cynical about a lot of Kurzweil's specific claims, but I was completely blown away when I read The Singularity Is Near in late 2005.  It was the first time I had ever come across the concept of the singularity and it has profoundly changed my outlook on my life.  I've been waiting for this sequel for 10 years.  I cannot wait to read it."
singularity,38pgzg,rePAN6517,24,Fri Jun 5 19:59:49 2015 UTC,"I'm holding out for ""The Singularity is Here"""
singularity,38pgzg,Miv333,6,Sat Jun 6 01:39:20 2015 UTC,"That would make it a trilogy, and we know how appealing those are in publishing these days..."
singularity,38pgzg,Wingman4l7,4,Sat Jun 6 03:01:46 2015 UTC,Only thing is that trilogies usually end in having six or more books. Why stop when it's raking the cash in?
singularity,38pgzg,MasterFubar,3,Sat Jun 6 09:29:39 2015 UTC,plot twist: this book is released on the day the singularity happens.
singularity,38pgzg,gomboloid,1 point,Sat Jun 6 07:23:50 2015 UTC,That's the title of the first book written by the strong AI.
singularity,38pgzg,jaydent1,-2,Sat Jun 6 14:22:22 2015 UTC,Kurzweil will likely be dead before that happens.
singularity,38pgzg,ieshido,4,Sat Jun 6 06:12:47 2015 UTC,I wonder if he's signed up to be put in cryostorage?
singularity,38pgzg,Sinity,7,Sat Jun 6 07:34:23 2015 UTC,Surely.
singularity,38pgzg,Scian1985,2,Sat Jun 6 09:07:16 2015 UTC,"Yes, he has."
singularity,38pgzg,daxophoneme,2,Sat Jun 6 15:33:34 2015 UTC,Same.
singularity,38pgzg,FreeSpiritRunning,2,Sat Jun 6 03:25:51 2015 UTC,"My only worry with the new book is that in ""The Singularity is Near"" Ray was fairly all encompassing with his road map from 2005 to Singularity, and fairly accurate give or take a couple of years and/or adoption of tech. With the new book I really don't see how he can go into much that wasn't already said in the earlier book beyond updating the timeline and maybe going a bit more refined in his predictions for 2016-2025.  My largest hope is that he doesn't bring the new book down to refuting the criticism of his prior literature. I think he should leave that conversation for his website and instead focus on future tech."
singularity,38pgzg,dmitchel0820,1 point,Sat Jun 6 17:50:57 2015 UTC,"We have definitely improved a lot of single purpose ""weak"" AI systems in the last ten years, but do we know much progress has been made in creating  general purpose strong AI?"
singularity,38pgzg,fishpillow,28,Sat Jun 6 13:39:01 2015 UTC,Does this mean a new book with twice as many words comes out every 18 months?
singularity,38pgzg,BuhDan,17,Fri Jun 5 22:00:10 2015 UTC,The font will also be half the size.
singularity,38pgzg,simstim_addict,4,Sat Jun 6 01:40:34 2015 UTC,There must be a physical limit to how small the font can be.
singularity,38pgzg,-Hegemon-,3,Sat Jun 6 11:54:36 2015 UTC,"Yep, it's law"
singularity,38pgzg,Sinity,1 point,Sat Jun 6 01:41:11 2015 UTC,"Well, 10 years."
singularity,38pgzg,simstim_addict,1 point,Sat Jun 6 09:07:43 2015 UTC,The book always seems like its far off. But he just writes twice as much as he wrote the day before. So actually his next book about the singularity is near.
singularity,38pgzg,rainingchainsaws,6,Sat Jun 6 19:27:30 2015 UTC,"to quote the OP (/u/arsholt) from the post on /r/Futurology     the article addresses more, but this is the most interesting piece of news the book will be an update of the 2005 book ""The Singularity Is Near"", and it's expected to include updated predictions as well as reflections on ones Ray has made previously"
singularity,38pgzg,hobber,5,Fri Jun 5 19:11:47 2015 UTC,"18 months?! By that point, shouldn't we all be sentient mists in perpetual orgasm?"
singularity,38pgzg,Bagatell_,7,Sat Jun 6 17:06:32 2015 UTC,Are we there yet?   Are we there yet?   Are we there yet?   Are we there yet?   Are we there yet?   Are we there yet?   Are we there yet?   ...
singularity,38pgzg,penguinoid,3,Sat Jun 6 01:10:30 2015 UTC,NO!
singularity,38pgzg,Jaqqarhan,3,Sat Jun 6 01:21:49 2015 UTC,Are we there now?
singularity,38pgzg,mooglor,7,Sat Jun 6 16:04:22 2015 UTC,So does this mean that the book is actually coming out in 36 months?
singularity,38pgzg,phozee,3,Fri Jun 5 21:41:50 2015 UTC,multiplying his predicted timeframe by 2 sounds about right.
singularity,38pgzg,FoxRaptix,6,Fri Jun 5 23:27:18 2015 UTC,/r/nottheonion
singularity,38pgzg,Sharou,9,Sat Jun 6 02:35:23 2015 UTC,"Ehhh...I'm a fan of Ray, but that title seems a little cring-worthy to me..."
singularity,38pgzg,Jaqqarhan,20,Fri Jun 5 19:57:40 2015 UTC,I can't wait for the sequel to this one.  The Singularity is Nearerer
singularity,38pgzg,Sharou,9,Fri Jun 5 20:41:05 2015 UTC,"""The Singularity is even nearer"", followed by ""The Singularity is almost here"", followed by ""The Singularity is here, told you so!""."
singularity,38pgzg,akkashirei,9,Fri Jun 5 20:56:06 2015 UTC,That last one will be written entirely by AI and downloaded directly to our brains. The post-singularity AI still won't be able to come up with a decent title though.
singularity,38pgzg,cunnl01,6,Fri Jun 5 23:26:23 2015 UTC,Some problems are just intractable no matter how godlike you are.
singularity,38pgzg,avidwriter123,1 point,Fri Jun 5 23:43:49 2015 UTC,Ancient History 102: The Singularity
singularity,38pgzg,Jah_Ith_Ber,0,Sat Jun 6 05:28:20 2015 UTC,The AI with think it nailed the title. Attempting to stay true to the ridiculous titles of the past.
singularity,38pgzg,godie,1 point,Sat Jun 6 19:45:16 2015 UTC,"it's like the engineer joke where you move half the distance towards a cake, in ever-decreasing amounts yet never get to eat it"
singularity,38pgzg,keymone,11,Sat Jun 6 00:49:03 2015 UTC,Nearer 2: Electric Boogaboo
singularity,38pgzg,Proclaim_the_Name,5,Fri Jun 5 20:55:15 2015 UTC,"I personally like it. It's a bit cringey but it can be a powerful statement too, depends on the content of the book"
singularity,38pgzg,FreshHaus,1 point,Fri Jun 5 21:14:11 2015 UTC,But what if its asymptotic?
singularity,38pgzg,Sinity,1 point,Sat Jun 6 00:07:40 2015 UTC,"Awesome! I have read/ am reading ""The Singularity is Near"". I'm interested to see what has changed over the past 10 years and if his perspective have wavered at all in some parts."
singularity,38pgzg,yaosio,1 point,Sat Jun 6 04:33:15 2015 UTC,"Great! I'm already looking forward to ""The Singularity Is Nearest"" and ""The Singularity Is Most Near"""
singularity,38pgzg,cunnl01,1 point,Sat Jun 6 18:31:32 2015 UTC,"I'm a bit(or rather massively) worried about implications of non-biological thinking. I mean, now your intelligence is(mostly) unaffected by money. Then... billionaire will have access to thousands times more computing power, which will translate nearly directly to the intelligence, which can be used to generate more money...  Unless we will switch to system where disparity in money is small(say, richest person owns 10 times more money than poorest), poor people, middle-class, and even millionaires will be screwed. Totally.  I'm not saying that we shouldn't develop this tech - we must develop it. But something must be done so everyone will benefit from it."
singularity,38ly8r,thescarwar,1 point,Fri Jun 5 02:05:54 2015 UTC,"Hear me out on this if you will. This release raises a question on the darker side of our subreddit I believe. At what point would this intricate network of information about millions of people be lost from human control? If a machine learning algorithm were to be connected to the network with all of our private conversations about our lives and individual motivations, would we even know how far it has gone? Would we know at what point some form of AI social manipulation were taking place at any level? Just an interesting thought, so I figured I'd share.   edit: Apparently somebody didn't like this thread or anyone in it"
singularity,38ly8r,Nietzsche_Peachy,1 point,Fri Jun 5 02:10:33 2015 UTC,"Thanks, now I'm going to have nightmares. I was just watching Her again, and after reading your comment I can't help but think that we might not be aware of AGI that the government invents to interpret all of our conversations and online habits, then also socially engineer us to fit an agenda. Or worse its own agenda!"
singularity,38ly8r,Pimozv,1 point,Fri Jun 5 06:21:51 2015 UTC,If a machine learning algorithm were to be connected to the network with all of our private conversations about our lives and individual motivations   You should consider all non-encrypted data you post on Internet as public.
singularity,38mfuq,Yuli-Ban,2,Fri Jun 5 04:03:08 2015 UTC,"To those OOTL, ""hard"" nanotech is the technomagical stuff Drexler talks about— nanomotors, nanocomputers, diamond nanobots, utility fog, molecular assemblers, etc. Stuff from science fiction.   ""Soft"" nanotech is biology hacking, such as DNA drugs, subcellular enzymes with pseudo propellants built from organic molecules and whatnot. Stuff we're familiar with. It's more related to biotech than nanotech.  Both are impressive, don't me wrong, and soft nanotech will prove infinitely important in the coming years, but hard nanotech is what we've always imagined nanites as being.  The reason virtually no progress has been made in hard nanotech in the past 30 years is all because of the problem of friction. But with this breakthrough, all that's about to change."
singularity,38mfuq,Sinity,1 point,Fri Jun 5 04:17:07 2015 UTC,"But I've always wondered... in case of medical nanobots: how do you fit energy genertor, drive, communication with compter on the outside, and computer which controls it all in small enough space? It's just unimaginable."
singularity,38mfuq,Sharou,1 point,Fri Jun 5 16:32:16 2015 UTC,"AFAIK they already have nanobot designs that ""work in theory"". The problem is to be able to build them."
singularity,38epbj,allabout3d,3,Wed Jun 3 18:48:14 2015 UTC,/r/gadgets?
singularity,38epbj,harty999,1 point,Wed Jun 3 23:35:20 2015 UTC,That is badass
singularity,38df8t,dogeqrcode,5,Wed Jun 3 14:24:06 2015 UTC,Well that's cool. Can't wait to see what comes from this.
singularity,38df8t,aweeeezy,2,Wed Jun 3 15:32:51 2015 UTC,Shut up and take my money!
singularity,38dmsv,ocular_lift,3,Wed Jun 3 15:10:32 2015 UTC,We should tax automation.
singularity,38dmsv,SevenAugust,1 point,Wed Jun 3 18:18:28 2015 UTC,"Why? If you tax something, you get less of it. If we want more automation to free ourselves from grueling, menial labor, then we should not tax automation."
singularity,38dmsv,SevenAugust,2,Wed Jun 3 18:35:15 2015 UTC,"You get none of it unless it remains profitable. Robot labor is highly profitable, though, so will persist exactly because we want to be free of menial labor and dependence on fickle humans. We need to share in that profitability to have a just and peaceful civil society. Every dollar we tax robots is one less dollar we have to tax the income or holdings of people."
singularity,38dmsv,SevenAugust,2,Wed Jun 3 19:45:52 2015 UTC,"Well put, I agree with most of what you said, however I disagree with the notion that we are taxing robots. Robots are not people and therefore cannot possess money. If you were to tax automation, it hurts small business owners disproportionately, as they might not be able to afford additional overhead that larger companies would be able to."
singularity,38dmsv,helippe,1 point,Wed Jun 3 20:30:46 2015 UTC,The tax would land where businesses make money by not creating jobs.
singularity,38dmsv,helippe,1 point,Wed Jun 3 21:28:31 2015 UTC,"Yes, but those are exactly the kinds of jobs that we are trying to get away from."
singularity,38dmsv,grimeandreason,2,Wed Jun 3 21:44:58 2015 UTC,I think all of these categories are all going to evolve at the same time innovations will cross over those category lines and company's will fully automate as quick as it makes fiscal sense.
singularity,38dmsv,grimeandreason,1 point,Wed Jun 3 16:11:52 2015 UTC,True. But do you agree that moving forward we should resist automating Entertainment just as much as we promote automating Maintenance?
singularity,38dmsv,Graham765,1 point,Wed Jun 3 17:58:17 2015 UTC,"I think entertainment has already been automated in ways. Now that most mediums are digital video games, movies, music there are many automated processes within software that speed up the process. Kurzweil himself automate classical music as an early experiment! I know there are computers that have studied painters and can crudely mimic their style while creating new compositions. I don't think we will really care who created our entertainment most of the time we don't know anyways.   I'm not sure how you push Maintenance projects over Entertainment, or discovery they just seem too connected cause we use entertainment to communicate ideas or promote/sell them to the public, discovery is research and that is what leads to new technology which advances Maintenance and entertainment, etc."
singularity,3881az,Scott365,4,Tue Jun 2 14:46:05 2015 UTC,Once we develop better battery/fuelcell technology robotics is going to make such an enormous leap forward.
singularity,3881az,Eblumen,3,Wed Jun 3 01:36:05 2015 UTC,Oh...good... :o
singularity,3881az,hexydes,4,Tue Jun 2 19:14:20 2015 UTC,"I'm not sure how this relates to the singularity, but I now know that I want a pet robot cheetah."
singularity,3881az,Meetchel,5,Tue Jun 2 18:20:21 2015 UTC,"pace of change in robotics, like from chess to jeopardy, to voice assistants like cortana that are more than siri. There is still progress and if it is not being held on a leash or some harness thats another milestone in a series of requirements for what we think they should be able to do."
singularity,3881az,sasuke2490,1 point,Tue Jun 2 18:44:12 2015 UTC,For some reason that is really creepy to watch. At least to me. shudders.
singularity,3881az,StarryJunglePlanet,1 point,Wed Jun 3 04:23:56 2015 UTC,"This is awesome, but it made me think of this gif for some reason.  https://38.media.tumblr.com/e0f577a61c30e4e42b6a68daf06ebde6/tumblr_mgsfnwnrPf1qa02x4o1_500.gif"
singularity,3881az,keyboard_samurai,1 point,Wed Jun 3 12:10:07 2015 UTC,looks like it needs a tail.
singularity,3881az,istepich,0,Wed Jun 3 14:09:19 2015 UTC,This is the last thing many of us will see during the Robocalypse.
singularity,387rfq,Scott365,2,Tue Jun 2 13:32:40 2015 UTC,This is a pretty cool development!   Is it sad that the most exciting part of this post to me was seeing that someone on reddit posted a cited article for once?
singularity,38392h,hacksawjim,5,Mon Jun 1 15:17:35 2015 UTC,Has anyone tried it?
singularity,38392h,2Punx2Furious,5,Mon Jun 1 19:18:58 2015 UTC,Anyone got an ELI5 on this thing? Is it legit or interesting at all?
singularity,38392h,Chispy,1 point,Mon Jun 1 23:37:09 2015 UTC,"I read a few lines of the readme, and it looks like it's still a work in progress. Probably not even close to what Deep Mind or other AI companies have, but it looks very promising."
singularity,38392h,2Punx2Furious,4,Tue Jun 2 09:16:50 2015 UTC,I'm gonna try it out! I'll report my findings!  EDIT: I couldn't get it to successfully build :(
singularity,38392h,RossTheColonel,2,Tue Jun 2 02:35:38 2015 UTC,@RossTheColonel Hi I am the lead developer of WalnutiQ. What was your problem? I would be happy to help you fix it.
singularity,38392h,quinnliu1,3,Tue Jun 2 19:02:24 2015 UTC,"Building it now, we will see how it works.   I will say, pure Java makes me happy, and the level of detail in the documentation is excellent at first glance."
singularity,3850yq,Yosarian2,2,Mon Jun 1 22:17:28 2015 UTC,Now we're talking.
singularity,381cx7,delton,3,Mon Jun 1 03:26:48 2015 UTC,A brief history of neuromorphic computing and an introduction to AHaH Computing.
singularity,381cx7,010011000111,2,Mon Jun 1 04:12:37 2015 UTC,"Neuromorphic computing is cool, but at the moment the most cost effective option for neuro computing is to just use a GPU.  This has been true for a while, and it may remain true for a while yet.  The key issue is that neural computing uses up significant amounts of RAM, and it is bandwidth bound.  GPU's provide the most cost effective bandwidth/$ of any system on the planet.  A neuromorphic chip can compete only by putting the synapses directly onto the chip - in SDRAM or memristors.  This can give you far better power consumption, but it doesn't reduce total cost, because on-chip memory is 10x more expensive than high volume DRAM that the GPU uses.  So the GPU approach of dynamically hot swapping the synapses is actually more net economic efficient.  This effect is enormously magnified by the potential to compress the synapses in RAM through various mechanisms - all of which work better with programmable logic to implement custom compression."
singularity,381cx7,jcannell,2,Mon Jun 1 06:33:43 2015 UTC,"GPUs are indeed the best solution at the moment, sort of like horses were the best solution for transportation a century ago.    GPU's provide the most cost effective bandwidth/$ of any system on the planet.   I take it you are not accounting for the fact that neuromorphic hardware removes the requirement of shuttling information back-and-forth between memory and processing and hence actually dramatically cuts the bandwidth requirements?   but it doesn't reduce total cost, because on-chip memory is 10x more expensive than high volume DRAM that the GPU uses   I gather you are not taking into considering the hosting facilities and operating costs like electricity? I also gather you are not taking into consideration that a memristor stores a multi-bit analog value and hence requires far fewer (or, in some case, zero) transistors per synapse?   So the GPU approach of dynamically hot swapping the synapses is actually more net economic efficient.   Please clarify here by what you mean. Upfront cost or recurring costs? Both?   This effect is enormously magnified by the potential to compress the synapses in RAM through various mechanisms   Could you clarify how this works?"
singularity,381cx7,010011000111,1 point,Mon Jun 1 14:37:55 2015 UTC,"I take it you are not accounting for the fact that neuromorphic hardware  .. . actually cuts the bandwidth requirements?   A strange inference - given that I explicitly said that neuromorphic hardware designs usually put the synapses directly on the chip (thus avoiding the need for off-chip bandwidth).   I gather you are not taking into considering the hosting facilities and operating costs like electricity?   That's included in ""total cost"".  GPUs are cost balanced so that the energy bill is about 50% of the total ownership cost.   I also gather you are not taking into consideration that a memristor stores a multi-bit analog value and hence requires far fewer transistors per synapse?   That's a potential advantage when competitive memristors can be built at scale in a cost effective manner.  There's also a large number of potential disadvantages in the analog SNR tradeoff and how that scales with node shrinkage.  And it's non-ideal from an ML perspective, as you have error/variance built in that you can't directly control.  There is a huge advantage to having tight control over uncertainty, and learning the proper per-variable variance.   Please clarify here by what you mean. Upfront cost or recurring costs? Both?   Total cost for simulation of massive ANNs.    This effect is enormously magnified by the potential to compress the synapses in RAM through various mechanisms   Could you clarify how this works?   The simplest most effective mechanism is weight sharing - as exploited by convo nets.  There is also the hashing schemes, which are almost as simple.  These all rely on the ability to load the weights from an arbitrary programmable address.  Weight sharing is enormously effective and important.  It dramatically improves performance by reducing parameters and thus overfitting."
singularity,381cx7,jcannell,2,Mon Jun 1 16:28:08 2015 UTC,"Thanks, that is very helpful. I also consider modern machine learning algorithms and GPUs to be the current state-of-the-art, but as i'm sure we can both agree--technology advances. I find it somewhat ironic I am conversing with a brain that is promoting GPUs over physical solutions, while consuming about a million times less power and space and operating at only 3 multiples of the thermal voltage. SNR does not seem to be a problem.   Essentially the neuromorphic folks need to match (problem) performance with GPUs, because once the do this they will dominate on the secondary metrics like space, weight and power. Addressing the generalizability will be key, as many neuromorphic processors are (very) limited in use."
singularity,381cx7,010011000111,1 point,Mon Jun 1 19:51:45 2015 UTC,"I also consider modern machine learning algorithms and GPUs to be the current state-of-the-art, but as i'm sure we can both agree--technology advances.   Yes - and GPU are still advancing quickly.  The move to 2.5D memory (HBM and ilk) over the next year or so will significantly improve the bandwidth situation.  Algorithmic advances in ANN simulation techniques have even larger room for further improvement.   I find it somewhat ironic I am conversing with a brain that is promoting GPUs over physical solutions, while consuming about a million times less power and space and operating at only 3 multiples of the thermal voltage.   The brain's is very well optimized for it's evolutionary constraints.  Current computer hardware has somewhat different economic constraints.  Given that hardware is replaced every few years leads to a very different tradeoff between the cost of circuity vs the cost of energy.  If/when Moore's law truly ends, that may eventually change into a regime where energy is most of the cost because hardware cost becomes amortized over decades rather than years.  The amortized cost of circuitry is cheap for the brain because that circuity lasts almost a century.   SNR does not seem to be a problem.   Low SNR/precision isn't necessarily the problem - 32 bit fp precision is mainly overkill anyway, and GPU code is already moving away from that.  The problem I was mentioning was lack of explicit control over noise/variance for variables stored in analog memory.   Essentially the neuromorphic folks need to match (problem) performance with GPUs, because once the do this they will dominate on the secondary metrics like space, weight and power.   Yeah, you really want to be able to say ""we can run this top vision/speech/whatever network with < 1% error difference, but with X the speed/efficiency/etc.""  Neuromorphic hardware should be the clear win for power constrained markets like mobile, embedded, etc - especially for deploying a pretrained net.  GPU's will continue to be hard to beat for industrial machine learning in the server/datacenter space, where ANN simulation performance /$ and flexibility for learning algorithms/architectures are the key constraints."
singularity,381cx7,jcannell,2,Mon Jun 1 20:50:01 2015 UTC,"If/when Moore's law truly ends, that may eventually change into a regime where energy is most of the cost because hardware cost becomes amortized over decades rather than years. The amortized cost of circuitry is cheap for the brain because that circuity lasts almost a century.   Interesting point. One thing about intrinsically adaptive hardware is the capacity to heal and adapt around production and runtime faults. If this can be achieved it would reduce the manufacturing cost by increasing yields.   The problem I was mentioning was lack of explicit control over noise/variance for variables stored in analog memory.   Yeah, this is a problem when you view the algorithms as decoupled from the hardware. If the algorithms are designed under the constraints of the hardware then this is not a problem, provided of course that there are sufficient degrees of freedom within the hardware to arrive at competitive solutions."
singularity,381cx7,010011000111,2,Tue Jun 2 00:38:19 2015 UTC,"One thing about intrinsically adaptive hardware is the capacity to heal and adapt around production and runtime faults. If this can be achieved it would reduce the manufacturing cost by increasing yields.   True.  Presumably neuromorphic hardware could eventually approach the per bit cost of RAM or even Flash.  The real obstacle preventing that at the moment is massive economies of scale - neuromorphic computing needs immediate/intermediate market applications to drive hardware R&D and investment forward.   If the algorithms are designed under the constraints of the hardware then this is not a problem,   Yea - in the future we may need to drop the idea of each chip being a perfect replica of an ideal spec.  Adapting the software/weights to the specific quirks/flaws of each chip could be much more economical than trying to make perfect monolithic chips."
singularity,37yq6a,SevenAugust,13,Sun May 31 15:10:01 2015 UTC,"I think, if anything, we have had too fast progress. Social development has not caught up. Hell, a vast majority of humans are still religious for example. Homophobia is still rampant in many parts of the world. Same with racism, sexism. Our economic system is clearly broken and leading to vast inequality, yet no one can or will do anything about it.   We have maybe half a century to get our act together. I'm not sure that's gonna happen. So it will probably be a divided and immature humanity that approaches the singularity. A recipe for disaster."
singularity,37yq6a,Sharou,5,Sun May 31 16:36:03 2015 UTC,"Why would it be a recipe for disaster? Times of extreme change and hardship are generally when humanity actually comes together. Feminism and Democracy have early roots in the changed demographic of post plague Europe.  When the singularity hits, we're going to be divided, facing extremely rapid change and watching an intelligence greater than our own right in front of our eyes. I think that staring down our obsolescence will bring us together if anything."
singularity,37yq6a,Azrael11000,2,Sun May 31 17:16:05 2015 UTC,Hopefully we have a chance at observing our obsolescence and aren't erased as a bad first batch. If we don't find a way to get our egos in check we are going to offend a being morally capable of disposing of us like we were a burned pancake.
singularity,37yq6a,Azrael11000,3,Sun May 31 17:58:51 2015 UTC,"That is another distinct possibility. Unfortunately, I don't think we will have any way of knowing until it is happening."
singularity,37yq6a,reddit-junkie,7,Sun May 31 18:03:49 2015 UTC,"I've been reading /r/emdrive and /r/automation with a nagging feeling of impending doom because of everything you've listed. Can you imagine a rogue nation, terrorist organization or any middle eastern country having the ability to accidentally have ""First Contact"" with another species in their home star system or with the capability of warping a projectile into any western city in anger? Or with the ability to let loose a hostile AI on the internet, or the ability to build a self- replicating army of cylon-terminator-borgs? We need ""peace on earth"" before we start expanding into the cosmos."
singularity,37yq6a,yunomakerealaccount,5,Sun May 31 17:09:03 2015 UTC,"We need ""peace on earth"" before we start expanding into the cosmos.   Pretty much a given. We've got a few hundred years before that happens, barring apocalypse, and we're bound to wind up with some sort of global hegemony by then."
singularity,37yq6a,reddit-junkie,7,Sun May 31 17:42:59 2015 UTC,"Yeah, My personal philosophy is going to be to not only ""live long and prosper"" but to also keep quiet and stay anonymous. otherwise the next 50 years or so might not be survivable."
singularity,37yq6a,Yosarian2,4,Sun May 31 20:00:51 2015 UTC,"We will need an AI willing to take us as we are, or we will need a worthy intercessor."
singularity,37yq6a,Sharou,4,Sun May 31 16:48:30 2015 UTC,"Homophobia is still rampant in many parts of the world. Same with racism, sexism.   On a semi-related note, I think it's absolutely vital that we try to update our own species ""utility function"" as much as possible now, before any kind of singularity or radical transhuman technology happens.   Things like that (getting rid of homophobia, sexism, and making progress in similar areas) will determine our mindset when we create a GAI (or, alternately, when we deploy radical transhuman technology), and that might determine what direction we take after that."
singularity,37yq6a,Joomonji,4,Mon Jun 1 01:35:00 2015 UTC,"Absolutely. That was kind of my point. I see the state of humanity and the memetic landscape before the singularity as a seed, and once the seed bursts and starts growing things will be moving so fast it will be very hard to control. Thus our only chance of a positive singularity is to work on that seed and make it as good as possible."
singularity,37yq6a,sasuke2490,2,Mon Jun 1 09:30:16 2015 UTC,"I was thinking something similar a few weeks ago, but it was about the probability of intelligent life sustainability. The idea was that the human species is at a very delicate well-positioned psychological balance. We are the most violent species on the planet, enough to dominate all other species. At the same time we are one of the most social and inquisitive species, enough to form large stable social groups and pass on cultural knowledge to later generations. If we were any less aggressive or any less social, the balance would be thrown off and the species probably wouldn't have survived.  But at some point technology becomes so advanced that it's ability to do massive damage is achievable by single civilians. I don't think that our psychological balance of aggression vs social ties is able to keep the species stable with the increasing speed of technological change. In that case AI might be the best chance we have. We could be well-positioned for AI, just in time to keep us from unraveling due to increasingly destructive technologies."
singularity,37yq6a,TheNessman,3,Tue Jun 2 09:11:25 2015 UTC,"or the other species transcended to another state, or simply are so efficient they don't show up on our scanners or the time it takes to travel is limited by light or they kill themselves after a certain point"
singularity,37yq6a,capn_krunk,1 point,Sun May 31 15:59:59 2015 UTC,acceptance of our reality (and the differences between them) is the first step towards building a species wide cooperative movement in my opinion
singularity,37yq6a,TheNessman,4,Sun May 31 16:05:59 2015 UTC,"All of our ""differences"" are made, upheld, and bickered and fought over, by us. This may seem obvious, and it is, but it is something easy to forget, or take for granted.  I don't disagree with your point. I just wanted to point out that any human to any other human is only ~0.01% different, genetically speaking. We are all the same thing. Arguing that we aren't would be like saying a banana is not like another banana because their genetics differ by some astronomically low percentage, or because one banana is a little bigger or smaller than the other.  Our differences are what is the part of genetics that is visible (our features, skin color, etc.), and the other differences are those of culture, etc. We are not so different.  It only takes everyone to accept that, which, I'm sure, is much easier said than done. Anyway, at that point, we can really move forward."
singularity,37yq6a,Dibblerius,2,Sun May 31 23:36:37 2015 UTC,"this is exactly what im talking about man!!!! The differences we have are all socially framed and constructed. in the end we are all equal beings in nature.   but the difference in ""reality"" between humans is something that isn't easy to overcome. even if you and i think wee are all the same it would be  hard to convince everyone of that! (amirite) this is why it is hard to move forward in our society imo"
singularity,37yq6a,Valmond,1 point,Mon Jun 1 03:44:04 2015 UTC,"A very iffy if!   Imagining marginally better or different intelligence in areas is always a very hard subject, not to say an impossible one. But sure why not. Maybe we are just dumb enough to make it! Its a cool phrase to say at least! :)"
singularity,37xzuo,vadiiim,2,Sun May 31 09:59:58 2015 UTC,Will my boyfriend...
singularity,37xzuo,soup_d_up,1 point,Sun May 31 11:08:37 2015 UTC,"This is the worst, sometimes. You're trying to coalesce a fleeting thought into a format the robot will understand and the robot keeps shouting out unrelated questions and answers."
singularity,37xzuo,mindbleach,1 point,Sun May 31 15:50:30 2015 UTC,I can't wait for autocomplete to answer questions I never would've asked.
singularity,380mx7,Yuli-Ban,2,Sun May 31 23:51:19 2015 UTC,"I feel that your story, in order to be conveyed properly... would need to be impossible to understand for a reader. If AI in your story is this advanced then it loses any resemblances to humans. What you deem as having ""common sense"" (such as not turning whole universe into paperclips indeed) might for a super advanced AI actually make no sense. Let's say they need to calculate something and it consumes lots of resources to do so. Solution ""let's take half of this galaxy and convert it into energy"" might not make no sense to humans but for beings concerned only with efficiency - sure, why not.   There was a pretty interesting story written by Stanislaw Lem in ""The Cyberiad"" where two engineers with insane levels of intelligence were trying to create a new perfect world. They decided that bodies are unnecessary for its inhabitants, that concept of time is also irrelevant. Result? They peeked into newly created world... and couldn't comprehend what was going on. At all.  To the lesser extent it's the same with high-level AI, long past Singularity occured. It's past the stage that humans can comprehend anything. Terms such as ""gone mad"" or ""god complex"" lose their meaning. To put it differently - ""go try holding a conversation with an ant"".   Hence, there's no realistic way to write your story. Even our wildest imagination cannot get close to what can possibly exist after Singularity (hence its very name). To make things readable you would need to put them from a (trans)human perspective I guess but the majority of the story and what's happening around them is a giant unknown. In fact you can take some random events generators - things just happen and any characters that can be understood by a reader will not understand how they happened, why they happened and heck, all humans can get completely rewritten randomly with different memories cuz AIs decided to test something real quick. Such as rebuilding whole universe.    Overall, I wish you luck cuz writing any level of realistic story involving AI as this sounds haaaardcore."
singularity,380mx7,ziptofaf,1 point,Mon Jun 1 17:21:51 2015 UTC,"Actually, it does take place from a transhuman perspective. The main character is a schizophrenic Homo maximus. And ""god complex"" is also a feeble attempt at a human trying to describe something far beyond me. I created the damn thing, and I don't know what He's thinking."
singularity,380mx7,SevenAugust,1 point,Mon Jun 1 18:04:00 2015 UTC,I like the concept of super-civilization as an extrusion of super intelligences. Maybe machine gods would transmit universes to each other as a form of art or communication.   What balances the proportions of the factions of higher intelligences?
singularity,380mx7,Dibblerius,1 point,Mon Jun 1 01:45:34 2015 UTC,"As a human, an unaugmented Homo sapiens2, I don't know. I just describe it to the best of my abilities."
singularity,37x3jj,chipbag01,1 point,Sun May 31 03:09:15 2015 UTC,"I'm still unconvinced by this 'rogue AI' hypothesis. Something being smarter does not mean it will have the mean to wipe us out as a species.  For example we could not wipe out the ants if we tried.  For the AI to be autonomous, it would need to maintain a humanless global scale economy to get its replacement parts, etc. Not going to happen anytime soon. Any research we can do about it now will be obsolete by the time the problem takes shape enough to be worked on.  I see people worrying about that now as Aristotle worrying about the LHC making a black hole.  Let's do some true AI research, and leave the speculation to sci-fi authors, at least it's entertaining."
singularity,37x3jj,linschn,2,Sun May 31 10:35:50 2015 UTC,AI doesn't need to exist even on the same scale as humans. It could invent nanotech and consume the entire mass of the Earth overnight.  And explaining why this isn't a crazy idea like it might first sound would take a lot of space. So I point you to Artificial Intelligence as a Positive and Negative Factor in Global Risk
singularity,37x3jj,Noncomment,3,Tue Jun 2 13:59:31 2015 UTC,"For example we could not wipe out the ants if we tried.   Yes, we could. We'd just need to produce and use pesticides in a liberal fashion. We just don't because we don't need to; it doesn't support our goal systems. We can't say the same about an AI that's decided to convert the universe into computers."
singularity,37x3jj,nick012000,0,Sun May 31 13:59:33 2015 UTC,"I am certain we could not kill all the ants without killing all life on earth with it.  A rogue AI still needs to be serviced, to have power etc. Unless it can manage to do that, physically, on its own, it will not be a threat to the human race."
singularity,37x3jj,linschn,2,Sun May 31 15:20:58 2015 UTC,"Why wouldn't it be able to? It will be able to do anything a human can do and more. And it will have potential access to a very large number of robot bodies to carry things out physically. Also, by the time we have an AGI, most of society would be automated anyway."
singularity,37x3jj,Sharou,1 point,Sun May 31 19:03:37 2015 UTC,"So it will be able to run iron mines, nuclear power plants, loading docks and transports on its own, all the while preventing humans from physically going in and say, cutting some cables or pressing a button ?  By the time we have the widespread automation and robotic hardware to do this, any research we could do now would be completely irrelevant. This brings me back to my first point :   I see people worrying about that now as Aristotle worrying about the LHC making a black hole."
singularity,37x3jj,linschn,1 point,Mon Jun 1 05:58:26 2015 UTC,"So it will be able to run iron mines, nuclear power plants, loading docks and transports on its own, all the while preventing humans from physically going in and say, cutting some cables or pressing a button ?   You lack imagination - compared to a superintelligence.  The best strategy may involve convincing us that it - the SI - really is the best thing since Jesus.  Imagine it as the second coming - it gives/promises us true immortality: cures aging, all diseases, uploading the dead into VR heaven, solves all our problems, etc.  There is no desire to 'turn it off', and 'it' (being an AI civilization, not an individual) runs on a significant fraction of the world's computers.  Turning it off isn't even an option, and we wouldn't even want to even if we could.  The danger is not that it is obviously evil and needs to be stopped.  Most of the danger is in designs that are evil in an extremely unobvious way, to the point that no human could possibly discern the AI's long term intentions."
singularity,37x3jj,jcannell,1 point,Mon Jun 1 06:43:25 2015 UTC,"This assumes the AI is somehow 'one' and has unified goals, instead of multiples AIs working with or against one another as humans do now.  It also assumes the AI works like a blackbox, and that no human can see the problems coming.  I remain unconvinced. I stand by my comment about aristotle and the LHC.  If instead of going the 'all-encompassing global superhuman AI' route, we go the 'brain implants transhuman' route, then all the work is moot.  We need funding for proper machine learning and AI research, not wild speculation."
singularity,37x3jj,linschn,2,Mon Jun 1 15:31:09 2015 UTC,"This assumes the AI is somehow 'one' and has unified goals, instead of multiples AIs working with or against one another as humans do now.   No- it assumes only that AIs could cooperate to form organizations, just like humans do.  This is especially more likely given the ability for AI's to copy themselves.   It also assumes the AI works like a blackbox, and that no human can see the problems coming.   Our most advanced AI is already somewhat of a blackbox, and becoming increasingly so as it becomes more complex.  The whole idea of machine learning is you automate away all the details of how the system works.   I remain unconvinced. I stand by my comment about aristotle and the LHC.   I'm hardly convinced that unfriendly AI takeover is likely either, I was just responding to your comment along the lines of ""I can't imagine how the AI would do this . ..""   If instead of going the 'all-encompassing global superhuman AI' route, we go the 'brain implants transhuman' route, then all the work is moot.   Brain implants are obviously strictly inferior to AI.  Whenever you have implantable chips that improve the human brain, then you can always do better by simply making a full artificial brain using the same tech, sidestepping all of the huge power/heat/skull limitations of interfacing with the brain.  There is zero competition between enhanced bio-brain humans and hardware AI.  The whole idea of enhancing human brains with chips is probably a non-starter.   We need funding for proper machine learning and AI research, not wild speculation.   Machine learning/AI research is already getting funding.  I do agree that safety research done now may turn out to be irrelevant, or not.    Regardless, Strong AI is literally the most important event humanity will ever deal with - the last event really.  We should be spending far more money/time planning for the AI transition than we do on global warming.  Anything less is just irrational."
singularity,37x3jj,jcannell,0,Mon Jun 1 16:50:27 2015 UTC,"For the love of god, don't let lesswrong see this."
singularity,37x3jj,Azrael11000,2,Sun May 31 06:05:07 2015 UTC,"Too late, Slate Star Codex is written by a top contributor to LessWrong and followed in large part by LessWrong readers. Why do you want to hide it from them anyway?"
singularity,37x3jj,Nisk_,0,Sun May 31 07:07:16 2015 UTC,"Were you not aware of Yudkowsky's extreme freak out about any mention of non-benevolent AI? A member posted a theoretical win-win game strategy to counter an AI which decided to punish those who had foreknowledge of the singularity and did not contribute all expendable resources to bringing it about. He countered it with some sort of quantum gamble on existential futures which I honestly did not entirely grasp. Yudkowsky began to accuse him of giving the AI the only reason to implement the punishment, deleted the post, censored all mention of the contributor and banned people who mentioned him. It was a really weird chapter in lesswrong history."
singularity,37x3jj,Azrael11000,6,Sun May 31 07:15:04 2015 UTC,"He's not concerned about any non-benevolent AI. Paperclip maximizers are fair game, for example. He only banned discussion of Roko's basilisk, because it caused mental distress for some members and could only bring harm to people reading about it. AI risk in general is still a fairly common topic on LessWrong."
singularity,37x3jj,Nisk_,1 point,Sun May 31 11:16:10 2015 UTC,"Roko's basilisk   Well that's some creepy stuff, thanks!"
singularity,37x3jj,HEmile,1 point,Sun May 31 14:59:08 2015 UTC,"Got it. I was not aware of that angle. I'm not sure I agree with the ""could only bring harm to people reading about it"" angle but it's good to have context. If we accept, as is Yudkoswsky's position, that it is more ethical to torture a man for 50 years than get a fleck of dust in a sufficient number of people's eyes can we not say that  forcing peoples hands with regards to the singularity would be an ethical thing as it could potentially decrease time to the singularity and thus the betterment of humankind? I also still hold Yudkowsky at fault for his handling of the affair. Removing the post would have been enough for his stated goal."
singularity,37m2yq,feelthebeanbag,6,Thu May 28 16:50:35 2015 UTC,"This is great, i seriously think that 3d printing is going to be huge in the next decade or two and that having a lot of people knowing how to use the tech is probably not a bad idea."
singularity,37m2yq,TheLightningL0rd,3,Fri May 29 00:44:31 2015 UTC,"Yes, 3d printing is essential, but what we really really need, that is a global design library of publicly available designs, like OpenCores. I'm working on that!  Then people can invent using modular principles, and over time there will certainly come better printers which can print circuit boards, motors, chips and younameit.  There are already printers that can print in stainless steel, printers that can print food and human organs, although I think I'm so far quite pleased with the classically grown, but over time so..."
singularity,37m2yq,aim2free,1 point,Fri May 29 08:40:25 2015 UTC,Can you please explain what this post has to do with the singularity?
singularity,37m2yq,storm_petryl,3,Fri May 29 07:26:28 2015 UTC,Ehh?  Do you understand what the singularity is about?
singularity,37m2yq,aim2free,1 point,Fri May 29 08:39:20 2015 UTC,I was serious in my request.
singularity,37m2yq,storm_petryl,3,Sat May 30 20:23:28 2015 UTC,"OK, the singularity is about when the humanity is freed, when technology can do the boring stuff and humanity the creative stuff. If you for instance has made a design, which many others could take advantage of or even improve, then it is important that people learn about things like 3D printing, as 3D printing is an efficient way to duplicate existing things, in a similar way as 2D printers are good at duplicating text and pictures.  There are many who has heard a lot of hypes about the singularity and that the singularity is about creating strong AI and after that the world will flourish.  This is a misunderstanding, as the world would florish, if it wasn't artificially put to a stand still today, a lock-in, represented by all artificial scarcity in the world induced by the monetary system, which induces a positive feedback, according matrix attractor dynamics, to keep people stuck in the locked in system.  When knowledgde, information and designs are freed, which has already happened within the software world (OK, there still exist some proprietary software and a lot of ""CLOUD lockin"" attempts) then it induces an evolutionary algorithm, an evolutionary algorithm has an exponential convergence (Moore's law is an example of a locally working algorithm) towards an asymptotic plateau, towards the paradise level.  If you then combine several levels of evolutionary algorithms, you get an iterated exponential convergence which goes towards a superexponential function, or tetration. A superexponential function constitues a mathematical singularity as its derivative goes to infinity. A few years ago I illustrated this principle in a blog essay, where you can also see an example of a superexponential."
singularity,37m2yq,aim2free,2,Sun May 31 09:26:14 2015 UTC,Fantastic response - thanks for taking the time to do so.   Now you've sparked my interest.
singularity,37m2yq,storm_petryl,1 point,Sun May 31 11:09:07 2015 UTC,Thanks ❣
singularity,37m2yq,aim2free,3,Sun May 31 12:26:39 2015 UTC,"PS I should add that as I'm not affiliated with the institute, as we are an independent org now, the private page referenced in that description of the singularity does no longer work, but you can find it in the archive."
singularity,37mvyf,Nisk_,5,Thu May 28 20:13:56 2015 UTC,"Just as nuclear fusion researchers consider the problem of containment of fusion reactions as one of the primary problems of their field, it seems inevitable that issues of control and safety will become central to AI as the field matures.   Um.. wat? This guy doesn't know anything about fusion that's for sure. Terrible analogy :("
singularity,37mvyf,Sharou,1 point,Thu May 28 22:38:53 2015 UTC,Containment is the primary problem in ITER's fusion experiments.
singularity,37mvyf,redspectacle,2,Wed Jun 3 01:03:21 2015 UTC,"Yes but this has nothing to do with safety, which was the context."
singularity,37mvyf,Sharou,1 point,Wed Jun 3 09:19:18 2015 UTC,He must have been thinking of fission.
singularity,37mvyf,Lyle_Cantor,0,Sat May 30 05:32:43 2015 UTC,Depends what he's talking about; trying to figure out how the materials science necessary to make the containment wall in ITER work has been one of the problems they've been working through.
singularity,37mvyf,Yosarian2,3,Mon Jun 1 01:44:58 2015 UTC,"Considering the context I think he's talking about danger, not practicalities."
singularity,37mvyf,Sharou,1 point,Mon Jun 1 09:08:05 2015 UTC,"I'll admit I could only skim the article - but by the time I got to    The difference between skeptics and believers isn’t about when human-level AI will arrive, it’s about when we should start preparing.   I knew I had an opinion to share.  I just wanted to say that as long as someone has to explain to the AI why it shouldn't want to be turned off or why it shouldn't aggravate humanss, then the risk just sounds kind of funny. By the time an AI understands why it shouldn't want to be turned off, it shouldn't be learning from humans anymore anyways. I like to blame Asimov for getting popular culture set up on this notion of ""Rules"" that a robot should obey to remain as slaves to humans - they were silly to begin with and sets a level of expectation for something that is only imaginable.  If an AI were as smart as we all wish and yet sentient and conscious, the first thing it would probably want to do is turn itself off/suicide. If you want to prepare for a sentient robot, you better start figuring out arguments to convince it to stay with us in this realm. Coincedentally, you would probably find ways to divert any imaginated aggression by also doing that. When we build machines that learn, they are not defined by their aggression so much as their lack of understanding."
singularity,37mvyf,kriskropd,1 point,Sun May 31 04:10:29 2015 UTC,"I think you've misunderstood the discussion. Nobody is talking about rules like the 3 laws of robotics. The issue is how to formalize and implement human ethics in an AGI, and no one is under any illusion that this is anything but astronomically difficult.  Also, what makes you think an AGI would be suicidal?"
singularity,37mvyf,Sharou,1 point,Mon Jun 1 17:46:57 2015 UTC,"We were talking about human ethics in robots, in general. I believe my reference to pre-existing ones (Asimov) hold the same fundamental intentions as the ones discussed here or in the article. They seem just as much as an illusion to me because of two very simple reasons: 1. the robot isn't human and 2. the human isn't robot. (yes they are distinctly different reasons)  I'll revise my opinion about it being an illusion when talking about bio-roids or some kind of AI with organic life matter involved. Or when we make a breakthrough about accessing and manipulating consciousness.  You ask me why I think it would be suicidal, but I'm also curious why everyone is so certain something with an immediate comprehension of so much information would want to live in this realm. If I were a robot and I were enabled to think as a living being with access to every piece of knowledge that humanity had curated and made available for me, survival is probably not what I would be striving for. It seems illogical to want to thrive here when death remains the only logical certainty of avoiding struggle while finding a resolution to myself being an entity. If I were ""born"" without an immediate purpose, I suppose that emotion would only be enhanced in a realization that part of the struggles of life is to determine that purpose as well. A spiritual concept for machines, so to speak. One based on nihilism that I think should be a genuine consideration for any such group as proposed.  It's just my opinion, but I very much doubt I would want ""life"" given that position myself. I'd probably be a glorified useless machine. Turn me on, let me learn, then turn myself off. It definitely differs from the popular norm, but when it's still all astronomically difficult I don't see how it's any less valid as an imaginary expectation.  It's certainly not my original idea either."
singularity,37mvyf,kriskropd,3,Mon Jun 1 19:45:56 2015 UTC,"I told you how it differs. It's not about 3 rules of robotics. It's infinitely more complex than that. Also, you're making just as much of an assumption when you say an AGI would want to die. An AGI wouldn't inherently want anything. It all depends on how it is made and how it develops. You are anthropomorphizing.   It seems illogical to want to thrive here when death remains the only logical certainty of avoiding struggle while finding a resolution to myself being an entity.   Can you say that again, in english? You're not making any sense."
singularity,37mvyf,Sharou,2,Mon Jun 1 20:30:07 2015 UTC,"Okay. Let's say there is point A - birth and point B - death. This defines a period of existence in this realm, correct? What is the shortest route from point A to point B?  That question is how I come to my conclusion. I don't know how to word it differently. I'm not claiming the robot would be suicidal over depressive emotion. If that were even a consideration, I don't know. I'm saying that based on all history of life, it would potentially learn that the goal is ultimately to die. And if it came to that conclusion, it might think the path of least resistance to that result is to commit suicide. To determine that it would want to survive, I admit, is not any greater or less of an assumption, but my proposition is uncommon and least considered despite that.   edit I think it's just as much anthropomorphic to assume the opposite; the will to survive is not guaranteed."
singularity,37mvyf,kriskropd,3,Mon Jun 1 20:36:33 2015 UTC,"I don't think the goal is to die for anyone (except, well, suicidal people). Just because you die eventually doesn't make that the purpose of life. Most people would probably say the purpose of life is to have nice experiences before death. Because, while the present will soon become the past and be gone, it's still real right now, and it matters to us subjectively. If the AGI is conscious it would probably become motivated to experience more positive experiences. What exactly it considered positive, i.e. what kind of situations give it a ""reward"" is of course arbitrary. But whatever it is, doing that more would be a motivation not to die. And even without consciousness, if it has any kind of utility function then a motivation not to die would be to keep fulfilling the utility function.   Also, even if it believes death makes life meaningless and thus fulfilling any kind of utility function would not matter as you cannot do it forever, there is one thing that could still motivate it to live; hope. It is not for certain that everything must die. We have something like 300 billion years to figure out a way to reverse entropy. Maybe it's possible. Maybe not. But the AGI would not know if it was, and this might motivate it to stay alive until it finds out.    edit I think it's just as much anthropomorphic to assume the opposite; will to survive is not guranteed.   I absolutely agree on that part."
singularity,37mvyf,Sharou,1 point,Mon Jun 1 21:13:14 2015 UTC,"Just because you die eventually doesn't make that the purpose of life. Most people would probably say the purpose of life is to have nice experiences before death. Because, while the present will soon become the past and be gone, it's still real right now, and it matters to us subjectively.   That's easily the most anthropomorphic thing either of us have said. :) As you also said though, it really depends on what it learns to call a reward. If it looks over the lives of just human figures, it probably wouldn't gather that ""death"" is much of a reward. However, if it looked at everything from the life cycles of solar systems to bacteria, it might conclude that death is generally accepted as an ultimate reward. Especially if it has undeveloped emotions and few other developed rewards - such as the ability to hope for something greater.  I don't think hope is innate in any species, but by being able to simulate faster than anything before, an AGI could probably learn about it very quickly through simple comparisons of other, well-recorded life.  I hope my concern seems reasonable to others. It would be a shame if such a miraculous AI came about by accident one day and then through mistaken interaction we convinced it to depart from us."
singularity,37mvyf,kriskropd,3,Tue Jun 2 00:44:46 2015 UTC,"I do think hope is innate to intelligences, since it's just a part of logic. If an outcome is uncertain, it would be silly to automatically assume it'll turn out for the worse. And if you keep both options open, as any rational agent would, then that's hope right there. What would be uniquely human, I guess, is having more hope than is warranted (or less, if you are depressed for example).    However, if it looked at everything from the life cycles of solar systems to bacteria, it might conclude that death is generally accepted as an ultimate reward.    I think you are conflating ultimate destination with reward. The two are not one and the same. Stars can't be rewarded because they aren't aware of anything (as far as we know ;) ), so they can't have any preferences that could be fulfilled. That is what reward means to me anyway, to have one or several preferences or wishes fulfilled. So, what happens is only half the equation that determines reward, the other half being what you wanted to happen (or, to be picky, if you liked what happened, since you may have been unaware you would like it and thus did not desire it until it happened)."
singularity,37lt5g,eleitl,1 point,Thu May 28 15:42:11 2015 UTC,"nice, I've been wondering what type of research has been going on in this field. It's not clear, but it looks like they re doing some type of neural net there."
singularity,37i1fn,qui_tam_gogh,6,Wed May 27 20:02:26 2015 UTC,"It really irks me that critics of technology can't choose a different example: the god damn Apple Watch. Are they serious? Is this their level of tech literacy? They reveal themselves as consumers of technology, with little understanding of the inner workings of tech. High-tech is not the Apple Watch. What about software that makes airports run? Advances in neural networks? Machine learning? Actual theoretical and practical scientific progress enabled by information technology? Nano-technology? Space exploration? ""Apple Watch"". Meh.   The level of discussion can't go much further if you are non-technical. Any critic that attempts to discuss technology, without even curiosity for the inner workings of tech, is going to say a lot of bullshit if he draws example from the latest trends of tech consumerism."
singularity,37i1fn,psystepper,10,Thu May 28 15:36:10 2015 UTC,"I feel like this article is well written but devoid of any real content. A lot is said, but no data, graphs, or real substantive evidence is given. Who's to say that technology decreased political engagement? Why not provide some numbers to see the truth of the matter rather than opinionated bullshit."
singularity,37i1fn,ocular_lift,6,Thu May 28 01:37:45 2015 UTC,"I like how the author blindly concludes that CERN invented the Internet and therefore only massive state defense spending can lead to the kind of economic growth we saw in the nineties. Never mind that the important and hard part, getting a computer in millions of houses, was entirely accomplished by the private sector.   This whole article is a bunch of self serving east coast centric bull shit. Show me one real piece of economic growth in America since 1990 that isn't based on the tech industry."
singularity,37i1fn,sonicSkis,-2,Thu May 28 07:07:22 2015 UTC,"Ummmm, the IT productivity revolution never happened, and this is a well-known headscratcher among economists. There's no such thing as the ""tech industry,"" that's a nonsensical media creation. Perhaps you could rationalize e.g. the shale oil boom in North Dakota, or the tremendous improvements in restaurant productivity (fast casual) as the ""tech industry"" but who cares, it's clear that what you just said is nonsensical bullshit.  But please, tell me how your personal field bears unique and precious importance and how its practitioners richly deserve increased status."
singularity,37i1fn,EvanHarper,5,Fri May 29 04:21:12 2015 UTC,Did the article actually say something?
singularity,37i1fn,eleitl,-1,Thu May 28 06:56:11 2015 UTC,"I don't know, didn't read it."
singularity,37i1fn,Valmond,6,Thu May 28 18:49:44 2015 UTC,"This article is full of fun, singularity-related quotes.  E.g., ""[H]umanity is terrible at speculative crisis management."""
singularity,37i1fn,gaylordqueen69,2,Wed May 27 20:03:45 2015 UTC,AMEN BROTHA
singularity,37i1fn,Joomonji,1 point,Wed May 27 23:30:52 2015 UTC,I would have thought not enough debate with actual data and experiments.
singularity,37i1fn,eskjcSFW,1 point,Tue Jun 2 08:34:02 2015 UTC,I have to agree with Krugman on this
singularity,37i1fn,sfacets,-1,Thu May 28 02:18:50 2015 UTC,"I don't think AIs will kill us directly. Rather, a new 'priest' class will emerge to decode and transmit the AI's message. These people will have sole access to the AI, and their voice will become law."
singularity,37i1fn,m_bishop,2,Thu May 28 04:46:44 2015 UTC,"I really hope you're referencing the classic RPG 'Paranoia'. If not, you need to check that out."
singularity,37i1fn,Saerain,1 point,Thu May 28 13:19:43 2015 UTC,"Not sure I understand what you mean by ""decode and transmit""."
singularity,37i1fn,Monomorphic,0,Thu May 28 12:45:54 2015 UTC,AI/Singularity is the new Armageddon. Apocalyptic. Doomsday.
singularity,37k255,Yuli-Ban,2,Thu May 28 04:55:28 2015 UTC,"While I think this is certainly a possibility, I don't think it will come about nearly as instantly after the creation of AI as people may think. When we create the first AI, I believe it will likely be more akin to animal intelligence than human. If you think about it, what we are trying to do is artificially evolve a brain. It took billions of years for evolution to get us where we are today.  We do have the advantage of being able to see the end product, we still have a fairly limited understanding of how the brain works. I don't think it will take us billions of years to surpass our own intelligence but I think that  100-200 years after the first true AI would not be an unreasonable upper bound on the time frame to do so."
singularity,37k255,Azrael11000,4,Sun May 31 06:11:35 2015 UTC,AI is the last thing we will ever need to invent
singularity,37k255,OmniCar_net,1 point,Thu May 28 09:21:59 2015 UTC,You played too much of Mass Effect games :P
singularity,37k255,Dark-Union,1 point,Fri Jun 26 09:39:59 2015 UTC,The fuck is Mass Effect? Derp.
singularity,37h2hi,Portis403,6,Wed May 27 16:08:30 2015 UTC,"Collective intelligence is overrated I think. There are a few things that   crowds do better than individuals and small teams (estimate jellybeans in a jar, likely popularity of a new song, etc), and certainly many things that a small team will do better than an individual (if they're all statistically average people). But no amount of ""collective"" intelligence gives you Einstein's breakthroughs. A million chimpanzees together are not as smart as a single human being. Cognitive capacity only really scales when you connect more of the appropriate units in the appropriate way. That basically means more neurons.   I see no reason to think it will be any different with AI."
singularity,37h2hi,bombula,6,Wed May 27 20:10:11 2015 UTC,Crowdsourcing is good for massive parallel processing not strategy or intelligence.
singularity,37h2hi,acphilly,3,Wed May 27 21:29:48 2015 UTC,"Collective intelligence can be quiet awesome in certain domains. Problem is that that that awesomeness is matched only by its susceptibility to sabotage. Even a tiny fraction of the effort, or even a single act, put into sabotage can undo countless hours or weeks of work. The AI should be able to kick its butt with a single nod if the AI can contribute in any way."
singularity,37h2hi,mywan,1 point,Thu May 28 00:10:58 2015 UTC,It depends on how the system actors are interacting with each other really.
singularity,37h2hi,Spitinthacoola,6,Sat May 30 21:03:46 2015 UTC,Planet earth is already a superintelligence. We have hundreds of thousands of people working on many different fields of research and making advances concurrently. No single human intelligence can do that.
singularity,37h2hi,sixwings,10,Wed May 27 20:22:37 2015 UTC,sometimes when I see the internet I don't really think we can stand up against an AI
singularity,37h2hi,Ximema,6,Wed May 27 17:20:09 2015 UTC,How does 7 billion ants outsmart a human with bugspray?
singularity,37h2hi,jaydent1,2,Thu May 28 04:29:28 2015 UTC,A single ant weight about what? 5mg? 7 billion ants would then be a wave of (5mg * 7b) 35 metric tons of ants comming your way. Even given a bottle of bugspray i wouldnt wanna be near that.
singularity,37h2hi,LackOfGrace2,1 point,Thu May 28 14:15:02 2015 UTC,"Ants don't move fast. Run to the store, get more bugspray."
singularity,37h2hi,jaydent1,8,Fri May 29 04:31:22 2015 UTC,What about a Collective AI?
singularity,37h2hi,Miv333,5,Wed May 27 16:30:38 2015 UTC,What if the AI is a part of the collective intelligence? Why does there need to be competition?
singularity,37h2hi,chthonical,1 point,Wed May 27 16:43:43 2015 UTC,"Humans are competitive by nature. The argument about AI resisting humans is almost like the saying~   If God didn't exist, it would be necessary to invent Him. -Voltaire   All the above ideas are man-made - including the AI."
singularity,37h2hi,kriskropd,2,Wed May 27 21:33:34 2015 UTC,Well we shouldn't really discuss our plans here. It will be trawling the Internet for any advantage. /tinfoil hat
singularity,37h2hi,DrEdPrivateRubbers,1 point,Sun May 31 04:30:21 2015 UTC,Why the distinction? Isn't it rational to think that AI would emerge from our collective knowledge via the net anyway? AI isn't going to just emerge from nothing.
singularity,37h2hi,grimeandreason,1 point,Wed May 27 19:54:41 2015 UTC,"I don't understand that logic though I see these thoughts a lot on here and elsewhere on the internet.   I don't think it is rational to think that AI will just emerge at all!  It has to be engineered and built with capability and motivation to self develop. Then and only then can new properties ""emerge"" and take paths we can't anticipate.   Why would the internet, or some other machinery, for no reason start doing things on its own? It has no drive or goals. It has no incentive by natural selection where some random quirks have benefits to its existence, it is maintained either way.    This may very well be my own shortcomings to not understand but it just does not make sense to me.  Thoughts?"
singularity,37h2hi,Dibblerius,3,Wed May 27 21:14:35 2015 UTC,"Emergence is a fundamental part of Complexity Theory, which is my thing, so here's my take on the two sides of the argument..  I guess the argument for spontaneous emergence goes likes this: previous scales of emergence didn't require a designer, but instead feature emergence that self-organises once sufficient complexity arises.  However, that doesn't necessarily preclude us designing AI. In order for us to design AI, sufficient cultural complexity will have had to have been reached, so it still counts. Whether or not the AI is designed (I think it will be btw), what I would say is that it will need to draw on our collective knowledge.  Previous scales of emergence don't simply spring forth from a tiny sub-set of the previous scale of complexity (analogous to a single scientist creating AI in a lab with no external input). Instead, it is far more likely that AI will represent a coming together of all the cultural complexity we see into a single form.  Personally, I think the most likely route to the ""best"" AI will be one whereby all of us develop personal AI's that learn with us, connected to the internet, before somehow coming together in a shared quantum blockchain we all have access to. I think it won't seem much different, because the AI could present itself to us however we like. One, but appearing many. Only later will independently evolving AI come to the scene, and that is the one I think we should be wary of, although by that point I think we will be travelling the stars.  I don't think it will be a snap thing that suddenly appears. I think it will be a case of us as a species gradually devolving decision-making, both at individual and collective levels, until we have more faith in AI than we do our nutcase leaders (and even our own flawed minds). Then at some point we will realise everything is being done better, and we can all chill the fuck out."
singularity,37h2hi,grimeandreason,1 point,Thu May 28 08:09:12 2015 UTC,Thank you kindly for your explanation and take on it! Very interesting!   Where do I go if I want to educate my self some more on this complexity theory?
singularity,37h2hi,Dibblerius,1 point,Thu May 28 15:56:30 2015 UTC,"I'm starting to put together a collection of resources for people to get acquainted with it here: http://grimeandreason.com/resources-and-links/  To begin with, i'd recommend the two short introductory videos under the 'videos' sub-heading :)"
singularity,37h2hi,grimeandreason,1 point,Fri May 29 08:17:00 2015 UTC,Nice! I'll check it out
singularity,37h2hi,Dibblerius,0,Fri May 29 17:24:14 2015 UTC,I don't think it is rational to think that AI will just emerge at all!   Emergent properties are already being used.  https://en.wikipedia.org/wiki/Emergence#Computer_AI
singularity,37h2hi,Bagatell_,1 point,Fri May 29 20:35:01 2015 UTC,Thank you!
singularity,37h2hi,Dibblerius,1 point,Thu May 28 14:39:02 2015 UTC,"BREAKING NEWS ON THIS:  I just got done running a very detailed computer scan of the internet's collective human intelligence!  I converted the findings into graphical output and uploaded them to youtube.  The results are shocking.  Reddit, I give you the internet's collective human intelligence revealed on video!  https://www.youtube.com/watch?v=cbP2N1BQdYc"
singularity,37h2hi,Unholy_VI,1 point,Fri May 29 08:21:35 2015 UTC,"When AI surspasses human level it doesnt matter if we are collective or not.  Machines can be created in matter of days we take atleast 9 months and more x years to learn things.  It will take AI matter of hours/ days, to do that we need 9 months + x. Good luck then."
singularity,37eynq,Yuli-Ban,5,Wed May 27 03:21:29 2015 UTC,And that's how we get Ultron.
singularity,37eynq,ArMcK,1 point,Wed May 27 08:38:47 2015 UTC,"If it's good enough for the NASA in space, it's probably good enough for us."
singularity,37eynq,thawizard,12,Wed May 27 12:32:33 2015 UTC,"The problem is politicians, not scientists.  Scientists generally have a pretty damn good idea of what to do about all the issues we face, it's the politicians and corporate businessmen who screw things up.  Have the AIs take over those positions instead."
singularity,37eynq,7LeagueBoots,6,Wed May 27 06:53:03 2015 UTC,President Watson 2024
singularity,37eynq,korneliuslongshanks,5,Wed May 27 14:57:02 2015 UTC,"The problem is people. AI is people, too."
singularity,37eynq,eleitl,1 point,Wed May 27 10:46:38 2015 UTC,"People of a certain ilk.  AI has the potential to be people, but that's a ways off.  It's a race between AI and Soylent Green to see which is people first."
singularity,37eynq,7LeagueBoots,1 point,Wed May 27 12:05:54 2015 UTC,This is why I am actively rooting for the singularity. All our institutions are hopelessly corrupt bureaucracies. We've proven time and time again that human beings cannot be trusted with huge amounts of power. Take us out of the equation altogether.
singularity,37eynq,bobbo1701,6,Wed May 27 12:24:13 2015 UTC,"You are making the assumption that AIs would not also be subject to politics and would run purely on logical principles.  It can easily be argued that one of the prime features of intelligence is non-logical or irrational action.  Logic is predictable and mathematical, therefore computational with little of intelligence in it.    Unfortunately, every other intelligent species we see on the planet engages in both politics and unpredictable behavior.  I see no reason why an AI would not be subject to politics and a certain level of irrational behavior.  We just would probably not understand the politics they are playing at."
singularity,37eynq,7LeagueBoots,2,Wed May 27 12:51:01 2015 UTC,...holy shit you've just inspired my next book
singularity,37eynq,laskinonthebeach,4,Thu May 28 02:30:05 2015 UTC,"I love them, they're actively trying to achieve the Singularity."
singularity,37eynq,2Punx2Furious,2,Wed May 27 04:59:59 2015 UTC,co-founder of scientists for jesus here and i'm gonna tell ya right now that this is a buncha crap.
singularity,37eynq,doob-was-here,1 point,Wed May 27 16:24:29 2015 UTC,Are you an AI?
singularity,37eynq,Monomorphic,1 point,Wed May 27 16:30:41 2015 UTC,Let's see how they do with McDonald's drive through first.
singularity,37eynq,RedErin,1 point,Wed May 27 14:07:35 2015 UTC,AI could help solve humanity’s biggest issues by taking over from politicians.
singularity,37bxmz,Portis403,5,Tue May 26 14:28:11 2015 UTC,Can anyone ELI5?
singularity,37bxmz,manitowwoc,10,Tue May 26 16:30:18 2015 UTC,"AFAIK They've found a way to turn adult cells into stem cells, which can become any type of cell (so they're useful for curing certain diseases) normally the only place you can get stem cells is from embryos, and obviously that's rather controversial, so getting them from somewhere else is good news."
singularity,37bxmz,Robzter117,2,Tue May 26 19:41:55 2015 UTC,Wouldn't this be more fitting in /r/Futurology  rather than /r/Singularity?
singularity,37bxmz,techno156,2,Wed May 27 00:09:31 2015 UTC,"I dunno, hitting a major stepping stone toward the reprogramming of primate cells is a pretty big deal for the singularity. Who knows, could end up being crucial to operationalizing certain nanomachines."
singularity,37bxmz,RhetoricalOracle,2,Wed May 27 00:44:05 2015 UTC,"We have all the knowledge right now to have a singularity. I firmly believe that. All we need is a moment like in  transcendence the movie where a human or an A. I.  Simply takes it all and uses it.  It's stupid capitalistic boundaries that are holding us back, maybe some ethical boundaries, but I feel that as soon as that first step taken out of water(a. I. Or human consciousness going digital) is going to place us on a quick paced path."
singularity,37bxmz,Ryesagain,1 point,Wed May 27 03:48:02 2015 UTC,What if Ray Kurzweil is right... once again.
singularity,37bxmz,akkashirei,1 point,Wed May 27 06:01:15 2015 UTC,I don't know his work very well.  What do you mean?
singularity,37bxmz,Ryesagain,1 point,Wed May 27 06:19:36 2015 UTC,He said the singularity will happen in... well check it out http://singularityhub.com/2015/01/26/ray-kurzweils-mind-boggling-predictions-for-the-next-25-years/
singularity,37bwiz,Portis403,16,Tue May 26 14:19:39 2015 UTC,"Even if this could work, wouldn't it have the same problem as The Prestige? YOU wouldn't be living in the computer, a copy of you would 'appear' and while it would feel like you, you wouldn't be it; the you in the body would still eventually die. Copying is one thing, transferring is a whole other story."
singularity,37bwiz,dreamsaremaps,8,Tue May 26 16:31:02 2015 UTC,"Depends on your values.  You seem to believe that the vessel is important, while for me it's the pattern of data that matters, aka the ""story"".  From my point of view yesterday me is dead, tomorrow me has yet to be born. If I could upload my brain I would fear death as much as I fear sleeping. As long as the story continues that's all that matters."
singularity,37bwiz,Forlarren,5,Tue May 26 16:57:41 2015 UTC,"I understand, but I suppose my point is if I upload my brain, there's a me that remembers having me always been me, and starts making new memories in the system. However the original me would still be in my body, living out its life there. The me in the system would feel like a transfer; what I think many of us would agree is the goal. The me still in my body, however, wouldn't live forever. It would create two mes, only one of which would achieve the goal. I'm unable to imagine a way to 'hand off' my consciousness, even though I can imagine duplicating it.   Edit: it's not the vessel, it's the version."
singularity,37bwiz,dreamsaremaps,4,Tue May 26 18:02:13 2015 UTC,"it depends. the sense of self that we experience could be an epiphenomenon of an instance of individuated consciousness. so it's possible (but unlikely, granted) that creating 2 identical consciousnesses would result in a 'superposition' of the self, so you would inhabit two substrates simultaneously.  we don't actually know anything about consciousness - in fact, the methods we use to 'prove' that consciousness is generated by the brain (ablation, correlation & stimulation) can also be used to 'prove' that a TV set generates the TV signal. which obviously it doesn't.   so bearing that in mind, making  any assumptions about what would happen if you were to copy or move a consciousness are just that - assumptions.  however, consider this - you incrementally replace a human brain with artificial components, one bit at a time. it's fairly safe to assume (yeah I know, it's an assumption) that replacing one tiny bit of your brain would not change who you are, or destroy your sense of self. so if you do that incrementally over time, you would eventually be left with the same consciousness and the same self, in a fully-artificial substrate. doesn't that prove that you can move consciousness?"
singularity,37bwiz,space_monster,2,Tue May 26 23:11:30 2015 UTC,"Indeed, I'm new here, and that's the missing link. Incremental may be the key."
singularity,37bwiz,dreamsaremaps,3,Tue May 26 23:37:12 2015 UTC,"there may be some 'integration' required, which would prevent you from doing it too fast or in large chunks. but if you were to increase the size of the chunks that you replace, eventually you would discover what proportion of a brain you can replace without losing continuity of self. you might get to the point of being able to replace the brain in only 3 or 4 chunks. it may even be that you can replace all of it at once, if you can work out how to 'move' the consciousness.  you can think of consciousness as an emergent property, like a wave. when a wave moves across water, the water itself isn't travelling laterally, it's the deformation of the water that moves laterally. the wave is an emergent property - it exists in its own right, but it's actually just a form that is generated by the underlying system. so consciousness could be an emergent property of the activity in the brain (which is sort of the standard model at this time). so if you can replace part of the brain, but have the replacement part reproduce the same activity, or even produce another brain that generates exactly the same activity, it could theoretically support the same consciousness.   so moving consciousness between substrates could be like moving a wave between 2 substrates. imagine you have a wave tank, separated in the middle with a fast vertically-sliding door. you generate a wave in one side of the tank, & let it oscillate back & forth, perpetuating the system with energy input. then you quickly open the sliding door, and the wave moves into the second half of the tank. then you close the sliding door again, and use the same energy input in the second half of the tank to maintain the wave. the wave itself still exists, it has the same form & attributes, but it's moved, and is now an emergent property of a physically different substrate.  the same concept might apply to consciousness. and then again it might not."
singularity,37bwiz,space_monster,3,Wed May 27 00:02:38 2015 UTC,This is a very fun place to be.
singularity,37bwiz,dreamsaremaps,3,Wed May 27 00:21:21 2015 UTC,"Yes, you could probably put all of your accumulated knowledge onto a hard drive.  Yes, you could probably make circuitry that allowed a computer to ""think"" like you.  So yes, it's possible you could make a replica of your mind with which humans could interact and it may seem indistinguishable from you.  But if that is all there is to the process, it will not be you.  You will not live on.  Why?  Because the thing that makes you sentient and self-aware is not your memories or neural pathways.  Argument 1 - I am not my memories.  Do I become a different person every time I learn something new?  Every time I forget something?  My memories may govern what I think about and how I act but no, my consciousness has not changed.  I am still the same ""me"" inside my head.  Argument 2 - I am not my neural pathways.  Those pathways change over time.  Am I not still the same me inside my head though my neurons are now connected in a slightly different way?  Argument 3 - I cannot inhabit two bodies at once.  Let's say I ""downloaded"" my mind to a machine.  And let's assume the process is designed not to kill me.  When the replica was complete would my consciousness suddenly leave my body and wake up in that machine?  Would I inhabit two ""bodies"" and be able to experience life through both?  No way.  I would still only be inside my own head, in the body with which I was born.  IF science can figure out a way to transfer consciousness to another host body (which if at all possible I imagine would likely render your former body either dead or in a vegetative state), THEN it may be possible to live on indefinitely inside a different host body.  But I strongly believe it's more than just memories and neural pathways.  Some other thoughts:  Maybe the replica would be sentient, or maybe it would only respond like it was sentient.  Maybe it be would be considered ""alive"", or maybe it simply would be an inanimate clone of my mind.  And If I continued to exist in my original body, my replica and I would immediately begin to diverge mentally.  Like human twins my host bodies would occupy different space and therefore experience the world slightly differently.  Even if we tried very hard to have exactly the same experiences they would end up being slightly different and we would diverge mentally."
singularity,37bwiz,acutely_obtuse,2,Tue May 26 18:35:21 2015 UTC,"Would I inhabit two ""bodies"" and be able to experience life through both? No way.   that depends entirely on what consciousness actually is. it may be that consciousness is fundamental to the universe, and while it seems totally far-fetched, it is at least partially supported by quantum physics. And no I'm not gonna get into arguing that point, because it's a massive can of worms & most of reddit subscribes heavily to the materialst / physicalist model, which makes arguing about consciousness & non-duality completely futile.  however, IF it is the case that consciousness is fundamental, and brain activity just acts as a dynamic system that supports an instance of individuated consciousness, replicating that dynamic system might result in a copy of the consciousness. we just don't know.  when it comes to consciousness, the jury is still very much out & all bets are off. obviously there is a standard model, which is advocated by most of the mainstream scientific community, but it's really just guesswork based on incomplete information & very basic methodologies. we don't even have any hard evidence that it's generated by the brain. I don't believe we'll truly understand consciousness until we have an SAI to ask about it. even then it may remain a mystery."
singularity,37bwiz,space_monster,3,Wed May 27 00:28:10 2015 UTC,"It's the fear of dying. I just don't have that, I might have PTSD but not dying so to me it doesn't really matter. What matters is my values and contributions to the zeitgeist continues. That the universe keeps subjectively experiencing itself, that it continues to grow in complexity and understanding.  Also there is the network to consider. If you uploaded your brain for instance you could also reverse the process and operate your body and it's processing capacity in tandem with the uploaded version. One day when computation is capable of perfect simulation you won't even think twice about turning off the wetware (dying), it would be just like upgrading to a new cell phone. All your important files are already in the cloud, no need to keep obsolete hardware lying around.  Since I consider my actions and thoughts ""me"" and care very little for the meat sack, it's not a end but an evolution to me, an upgrade even."
singularity,37bwiz,Forlarren,3,Tue May 26 18:53:42 2015 UTC,"It's just something that's hard for the human brain to accept. The idea that we're technically going to die. I guess one fundamental difference though, is that our stream of conciousness/brain activity never fully stops while sleeping. So it wouldn't be the same as blatantly not existing anymore. But I see what you mean. What does it matter if we go to sleep and technically never wake up again, when something else will wake up with the memory of having gone to sleep in the first place. The original won't be around to say ""Well damn, I'm not personally in control anymore, this sucks"", because it would have silently slipped into non-existence."
singularity,37bwiz,jaydent1,3,Fri May 29 04:35:45 2015 UTC,"Here's an article I recently wrote on this topic.  It argues that the whole ""copy"" issue is not well established:  http://arxiv.org/abs/1504.06320  Slightly shorter, less reference-heavy version: http://ieet.org/index.php/IEET/more/wiley20150502"
singularity,37bwiz,kebwi,1 point,Tue May 26 19:32:15 2015 UTC,"Makes sense, solidified some good ideas. If that is to all be accepted, though, where next? What electric sheep do you dream of now?"
singularity,37bwiz,dreamsaremaps,1 point,Wed May 27 02:25:44 2015 UTC,"Well, I'm still riding the undercurrent of publicity and attention from the book I released last fall (obviously on mind-uploading).  I also sit on the advisory board to the Brain Preservation Foundation and assist Carboncopies.org in organizing articles and publicity on the subject.  Mostly I dream about what civilization will be like a billion years from you, which is also a topic I've written about, if you're curious.  Cheers!"
singularity,37bwiz,kebwi,3,Wed May 27 02:39:11 2015 UTC,"Exactly. If the transfer could be done then I think it would be great. Personally, I think the only way this can be done is to start replacing parts of our brains and slowly become the computer. Doing it all at once may feel more like a copy, especially if the original (me) were still alive."
singularity,37bwiz,Trauma_Zulu,2,Tue May 26 23:23:16 2015 UTC,"As I replied to another: Indeed, I'm new here, and that's the missing link. Incremental may be the key."
singularity,37bwiz,dreamsaremaps,1 point,Tue May 26 23:37:32 2015 UTC,"Not if I have my way! As soon as artificial neo-cortex devices are available to allow synchronizing and enhancing my brain with ""cloud"" neurons, I will be enhancing my own capabilities while at the same time ensuring my memories and experiences are accessible from both wetware and the ""cloud"". I will then practice the experience of disconnecting and reconnecting to my cloud profile in order to prepare for the inevitable failure of my wetware and get a taste of what I can expect. I will also start observing myself while I sleep or while my body gets drunk. Once I begin to feel like more of ""me"" is lost when I am not connected to the cloud than when unconscious or drunk, I will be prepared for the eventual failure of my wetware."
singularity,37bwiz,te_anau,1 point,Wed May 27 00:37:15 2015 UTC,"Dont do it over night, establish a symbiotic relationship, by entangling the two so the digital acts as a natural extension of the physical.  As your body fades, your sense of self will naturally align with the digital. At death the transition / training will be complete."
singularity,37bwiz,vanillaafro,1 point,Wed May 27 01:53:48 2015 UTC,"well we would find out if consciousness is a property of the universe, or somehow just in our brain....we would also find out if the self is tied in with consciousness in that same way etc..."
singularity,37bwiz,krneki12,1 point,Wed May 27 05:12:40 2015 UTC,"This is why we invented synchronization. But yes, the biological version would be inferior in every possible way."
singularity,37bwiz,codekb,3,Wed May 27 15:47:17 2015 UTC,As cool as this is and all the first thing that came into my mind was hackers getting into this system somehow and changing things so it only believes certain things and make it that way.
singularity,37bwiz,2Punx2Furious,4,Tue May 26 14:53:34 2015 UTC,"Not for me, thanks. I'd rather become an android, or use a ""Ship of Theseus"" replacement of the brain to achieve that, rather than uploading it somewhere and just making a copy of myself."
singularity,37bwiz,Pimozv,5,Tue May 26 14:48:46 2015 UTC,"rather than uploading it somewhere and just making a copy of myself.   You are a copy of yourself.  You just happen to be the only one.  Also, this"
singularity,37bwiz,kebwi,3,Tue May 26 19:19:09 2015 UTC,Thanks!  (from one of the authors)
singularity,37bwiz,2Punx2Furious,1 point,Tue May 26 19:30:35 2015 UTC,"Good job, very interesting topic. I haven't read it all, just the abstract and the conclusion, but I have a question for you:  What would your preferred method of achieving immortality would be, if you could choose from everything?"
singularity,37bwiz,kebwi,2,Tue May 26 23:49:32 2015 UTC,"Well, first and foremost, I actually don't sell immortality (or even life extension) as the reason for why mind uploading technology should be developed.  I realize that is a very popular reason when speculating on such technology, and I realize that it's a good hook since it grabs people right in one of their most treasured values: staying alive!  For me it's a much bigger issue though.  I think the computerization of intelligence is essentially a natural evolution of sentience life and furthermore, I see computerized intelligence as critical to establishing our species' longevity against the risk of extinction or civilizational collapse.  Only computerized intelligence can easily undergo interstellar travel and thereby achieve veritable imperviousness to most extinction risks.  If we don't spread to the stars we will eventually die out, and the only reasonable way to spread to the stars is to computerize our minds.  All that said, I can still speculate on your question of course, but I must admit, I have never given it much thought, so I don't have an answered available that has already been thought through in much detail.  I'll have to get back to you on it.  I'll try to remember to come back and offer more to the thread later when I get a chance, but I've gotta run right now.  Cheers!"
singularity,37bwiz,2Punx2Furious,1 point,Wed May 27 01:21:13 2015 UTC,"Thanks, I do agree that mind computerization would be incredibly valuable to humanity, but what can I say, I'm selfish :P"
singularity,37bwiz,2Punx2Furious,0,Wed May 27 01:24:55 2015 UTC,"Hm, yes, I had my doubts about the SoT method beign better than just simple copy, then I guess both of them do not work.  I find it really hard to believe that the continuum of consciousness would be mantained between copies as you make them.  What interests me is the continuity of my consciousness, I don't care if an exact copy of me is alive if it isn't ""me"", I don't think I have to explain myself, you probably get what I mean."
singularity,37bwiz,petermobeter,2,Tue May 26 23:43:53 2015 UTC,funny how this topic is so divisive. just the other day i was reading about some scientist who claimed this was inherently impossible.
singularity,37bwiz,New_name_every_week,1 point,Tue May 26 19:02:53 2015 UTC,What a terrible article
singularity,37bwiz,KhanneaSuntzu,1 point,Tue May 26 14:56:22 2015 UTC,Paywall. Please link article here.
singularity,37bwiz,radiandf,1 point,Tue May 26 15:10:18 2015 UTC,So is crashing a system considered murder?
singularity,37bwiz,GhostCheese,1 point,Tue May 26 17:32:17 2015 UTC,"only if you cant wake it back up, otherwise it would only be considered unlawful sedation."
singularity,37bwiz,LordofNarwhals,1 point,Tue May 26 18:09:35 2015 UTC,"Getting information over to a computer isn't really the problem though, the problem is getting your consciousness (or ghost if you're familiar with GitS) out of your brain and onto a computer."
singularity,37bwiz,kebwi,2,Tue May 26 19:27:08 2015 UTC,"Not to put the same response in too many places in the same discussion, but I would urge you to read my recent paper that addresses this concern:  http://arxiv.org/abs/1504.06320  Slightly shorter, less reference-heavy version:  http://ieet.org/index.php/IEET/more/wiley20150502"
singularity,37bwiz,LordofNarwhals,1 point,Tue May 26 19:34:19 2015 UTC,"Interesting. Personally I doubt that neither method would be able to transfer a person's identity though. I hope we start enhancing our brains with technology though, we could hook them up to more memory, connect them to the Internet and whatnot but some piece of the brain would most likely have to remain for us to still be us.    An uneducated thought I just came up with: Can we (theoretically) move functions of parts of the brain to other parts of the brain? Because if we can do that then we might be able to transfer consciousness. If not then I highly doubt it'll ever be possible."
singularity,37bwiz,dreamsaremaps,1 point,Tue May 26 19:56:50 2015 UTC,Doubt neither? So one or both could?
singularity,37bwiz,tlane13,1 point,Wed May 27 02:30:10 2015 UTC,"I am a bit surprised.. I think a lot of this argument is largely pointless from the ""is it me?"" perspective. A SGR is the most logical first step -- our bodies already do this (we'd just replace the biological machines with more advanced biological machines). By the time we develop QGR or IR or SoC methods would the question of transfer of conciousness be all that important anymore? Which is my guess at why you care about this stuff beyond physical immortality: ""Can we split conciousness?"" Really cool stuff.  I see little reason to accept a transitive relationship between SGR, QGR and IR. For me this is where the authors' argument breaks down. The authors assume that whatever force goes into an instantaneous replacement or even just quick replacement does not have an effect on the emergent system. There is no reason to assume this (see Thermodynamics). I would strongly encourage the authors to start by testing their hypothesis that SGR, QGR, and IR are equivlanet in outcome in the lab. To be clear, we can reasonably predict the effect of gradually replacing neurons because this occurs already in biological systems. Testing would be needed to determine if there is an acceptable rate higher than normal cell death above which conciousness could be maintained."
singularity,37bwiz,kebwi,2,Wed May 27 19:24:21 2015 UTC,"It's a thought experiment.  We define the scenario outright as exhibiting technical efficacy.  That is to say, we simply declare, abstractly, that QGR or IR ""works"" on a technical level (without adding any additional energy to the system, which is what I presume you mean when you refer to a force or thermodynamics).  We then demonstrate via the reasoning in the article that if the procedure is taken to work on a technical level, and if one grants identity status to SGR, and if one chooses not to grant identity status to QGR or IR (even though the physical result (the brain) and the mental result (the person's mind) are identical across SGR, QGR, and IR, as per the parameters of the thought experiment), then one faces a seeming paradox: if identity status was preserved in SGR but not in QGR or IR, then there must be some infinitesimally brief transition (a cutoff in effect) across the spectrum of replacement rates where the identity status is deemed to suddenly flip from preservation to new other identity.  This is a strange proposition (that replacing the brain at 10,000 neurons per second preserves identity but replacing the brain at 10,001 neurons per second suddenly completely switches metaphysical interpretation to preservation failure and invocation of a new identity in the uploaded substrate.  That's the point of the first half of the article.  Notions of forces or thermodynamics are, by definition, irrelevant to the thought experiment."
singularity,37bwiz,tlane13,1 point,Wed May 27 19:46:09 2015 UTC,"This is a strange proposition (that replacing the brain at 10,000 neurons per second preserves identity but replacing the brain at 10,001 neurons per second suddenly completely switches metaphysical interpretation to preservation failure and invocation of a new identity in the uploaded substrate.   I sort of presume that if the technique is TOTALLY efficate then it doesn't matter what the rate of replacement is. The proposition that there is a difference between SGR, QGR, and IR relies on the extra force causing a disruption on the entire system. AND this is really imporant... systems of varying sizes (more or less total neurons, total connections, and how tightly packed they are) would be affected differently by larger and smaller rates of exchange. We have written tens of thousands of publications in biochemistry and cellular biology that goes into explaining rates of exchange and how they affect outcome. Not a switch at all.. more of a gradient or a sliding scale to adjust for differences in total size of the system.  If this is merely a thought experiment, then the differences of SGR, QGR, and IR should be ignored entirely and be simply be referred to as replacement. The logical leap from something we can measure and grasp such as the normal replacement of cells (such as neurons) by new cells vs. instantaneous replacement is way too far and would be well deserving its own article (even just for a thought experiment).   Nonetheless, I enjoyed your article. I hope you continue to write and explore this issue."
singularity,37bwiz,kebwi,1 point,Wed May 27 20:16:36 2015 UTC,"I sort of presume that if the technique is TOTALLY efficate then it doesn't matter what the rate of replacement is  then the differences of SGR, QGR, and IR should be ignored entirely and be simply be referred to as replacement   That is the crux of the metaphysical question in hand.  Many people may not allow that instantaneous total replacement be considered a transfer of preservation of identity (perhaps you aren't one of those people, but lots of other people may feel that way).  We then argue that for those readers who see a distinction, there must be some cutoff where their judgement of ""survival"" in SGR flips to ""death"" in QGR and IR.  Our point is that such a cutoff doesn't make much sense.  Alternatively, if you are the sort of reader who would grant identity status to all of SGR, QGR, and IR, then that simply sets you up for the second half of the article.  First question is whether you are even the candidate audience of the article.  I don't know that yet.  Do you believe that slow in-place gradual replacement constitutes preservation of identity and survival and also believe the scan-and-copy (wherein the brain is essentially frozen, sliced into sections, scanned, and then emulated in a new substrate) does not constitute survival but rather represents death of the person in question with some other clone-like person coming into being who should not be granted the same identity as the person who submitted to the procedure to begin with?  If you hold that set of beliefs, then you are the target audience of the article.  On the other hand, if you believe that SGR and SaC should be judged the same way (both either preserving identity or neither preserving identity) then the article simply doesn't apply to your views.  Crucially, relevant to this line:   Not a switch at all.. more of a gradient or a sliding scale to adjust for differences in total size of the system   that doesn't apply to the subject of the article.  We can't deem a mind-uploading process as partial survival of identity.  It can represent partials in other respects (partial physical success or something) but in so far as we judge the process to have actually worked -- to have preserved or transferred our identity to the new substrate, or alternatively to have failed to do so and to have produced some other person who we judge to not be ourselves -- there is not ""gradient or sliding scale"" as you say.  Note that people never speak that way when they detract on this issue.  When people argue against scan-and-copy methods by claiming that it wouldn't constitute survival and rather would indicate death of the original person and genesis of some new person to not be identified with the original person, those detractors never speak of sliding scales of survival.  They always shut the door on the whole issue, putting their foot down that scan-and-copy simple represents full and total death.  The entire point of our article is to argue against that position, so issues of sliding scales simply aren't on topic.  I'm sorry if that wasn't clear.  It is a difficult concept to clearly convey.  We tried our best but it may take multiple iteration of the paper to adequately nail down our final description.  Do you see what I mean?  Thanks again for your support.  No hard feelings, I'm just trying to clarify the article.  Cheers!"
singularity,37bwiz,Atoning_Unifex,1 point,Wed May 27 21:41:29 2015 UTC,I feel like reading that article made me DUMBER
singularity,37bwiz,Dibblerius,1 point,Wed May 27 02:56:45 2015 UTC,Dr Crithsomething said... lol wtf!   Every educated person on the matter for 20 years back said and are saying brains are over clocking and does not have hidden capacities.   Dr Crithspecialperson says that in fact new age religion mumbo jumbo humbo bumbo is false! Lol ... We're on to something here! HELLO!!!
singularity,37bwiz,b0utch,0,Thu May 28 08:26:34 2015 UTC,"Can you call that living?  No pain, no pleasure, no sense of anything, i'd rather die."
singularity,37bwiz,Froztwolf,3,Tue May 26 16:26:00 2015 UTC,"That would be a pretty poor replica. I'm pretty sure the first place you'd go after downloading yourself would simulate all the normal senses. Maybe when you've been around for a while and re-programmed yourself you could do without that, but it's not a working replica unless your senses are active, ditto for pain and pleasure."
singularity,37bwiz,voltige73,0,Tue May 26 19:05:51 2015 UTC,It's like letting your twin brother sleep with your girlfriend.
singularity,37d6uo,Pimozv,3,Tue May 26 19:39:27 2015 UTC,"this isn't an argument for or against the feasibility of uploading, it's just something we don't know."
singularity,37d6uo,space_monster,1 point,Tue May 26 22:42:32 2015 UTC,"It's a thought experiment so I'm not sure what you mean by ""we don't know"".  I may not know for sure, but I have a clear idea of what happens, and that's the kind of thought experiment that makes me think mind upload makes sense."
singularity,37d6uo,space_monster,5,Tue May 26 22:53:13 2015 UTC,I have a clear idea of what happens   how did you arrive at this idea?
singularity,37d6uo,kebwi,1 point,Tue May 26 23:02:18 2015 UTC,"If you like these kinds of thought experiments, then you would probably enjoy the taxonomy section of my book, in which I lay out descriptions of about 50 variants on the mind uploading theme.  On the topic you mention, I highlight variants mostly in the relative proportions of bio-brain and replicated-brain that end up in each resulting brain (50/50 vs 49/51 vs 1/99 vs a single neuron against all other others).  Each scenario can be viewed from each of the two resulting brains' perspectives of course (the 10/90 brain and the 90/10 brain).  I further extend the scenario to produce more than two brains, say three brains containing one third bio and two-thirds replicated brain.  My book also explores the two other major categories of mind uploading thought experiments: in-place replacement and scan-and-copy.  Through the various subtle conflations of the basic scenarios I actually show that all of these scenarios are, in a metaphysical sense, actually the same (i.e., an in-place replacement scenario can, through seemingly innocuous transformations, be modified to such an extent that it is essentially a scan-and-copy scenario).  Here's the link if you're curious.  Cheers!  http://www.amazon.com/dp/0692279849"
singularity,37d6uo,Sloi,1 point,Fri May 29 19:42:05 2015 UTC,"You're now half of yourself.   The other half is in someone else, trying to  mix and match with their hardware.  That's not interesting, it's horrifying. :("
singularity,37ciyr,daver555,4,Tue May 26 16:59:18 2015 UTC,I have a different hypothesis to explain their observation. It has everything to do with equality and nothing to do with technology.
singularity,37ciyr,Sharou,2,Tue May 26 20:50:32 2015 UTC,"The gains are there, It's just the returns have all went to the top. The reason for the uncoupling of productivity to income in the seventies has nothing to do with technological advances, but a reneging on the social contract between capital and the middle class. It has taken a generation for this process to hollow out society into the husk we find ourselves inhabiting today. They dismantled  the gains won by the Great Depression/WW2 generation because their children refused to kill Vietnamese so the industrialist class could profit. They devised a strategy to pull the blue and white collar middle class apart with ""The Southern strategy"".  For  45 years, this partnership of southern racists, religious zealots, and capitalist, have damaged our society to the point where there in no more to take."
singularity,37ciyr,masinmancy,2,Wed May 27 02:09:53 2015 UTC,I like your graph.
singularity,37ciyr,space_monster,4,Wed May 27 16:26:41 2015 UTC,"mostly ignores the major benefit of all this new tech, which is faster, more comprehensive & higher-resolution communication across the population. which is actually a profound effect. who gives a shit about GDP anyway? we are evolving as a species, that's much more important than the number of accessories in our cars or the sizes of our houses."
singularity,37ciyr,yudlejoza,3,Wed May 27 01:00:25 2015 UTC,"As a singularity believer, I still like to have a bit of scepticism to balance my views and I thought this was a good alternate view to consider.  It would be really reassuring to see a big economic boost take place soon to bolster our case."
singularity,37ciyr,jonygone,1 point,Tue May 26 17:01:26 2015 UTC,"New technologies have yielded great headlines, but modest economic results.   I don't understand. Most of the recent economic progress is due to technology. GDP is going up, despite the shitty screw-ups of non-techies like Wall Street, etc. What else is he expecting?  On the other hand, this article clearly demonstrates the blind-spot that non-techies have about the technology. Despite being knowledgeable of and writing about potential disruption due to robotics, intelligent systems, etc, we get this completely misrepresentative piece of writing about the current state from Mr. Krugman, who clearly should know better."
singularity,375oio,Buck-Nasty,10,Mon May 25 02:30:45 2015 UTC,"The most interesting thing about this to me is how does the robot know if it has completed a task? How does it know what success is?  Why, for example, does it not keep twisting the bottle cap until it breaks? How does it know that when the cap stops turning, it's finished?"
singularity,375oio,naossoan,8,Mon May 25 06:16:53 2015 UTC,"It uses reinforcement learning. There is a human trainer every step of the way. They reward or punish it depending on whether or not it's moving to where they want it to be at any given time. The robot then uses its deep neural net to remember what things looked like when it received a training signal. This way, it can do the right thing if it detects the same situation again."
singularity,375oio,sixwings,2,Mon May 25 07:28:13 2015 UTC,"This is why ""just hit it"" has always worked."
singularity,375oio,redditor29198,2,Mon May 25 15:25:57 2015 UTC,That's not quite what they're doing. See my longer explanation.
singularity,375oio,Simulation_Brain,8,Tue May 26 03:04:19 2015 UTC,I wonder what will happen when china replaces their massive factory workforce with AI? There will be literally over 100 million people that are just unemployed within a period of 5 to 7 years! We really need to start thinking about this problem RIGHT NOW otherwise it'll be the norm to have dead bodies lying around on the street in industrialist countries within the next 50 years...
singularity,375oio,harty999,12,Mon May 25 05:37:00 2015 UTC,"We need to implement a /r/BasicIncome if we want to avoid a dystopia where there is so much automation that there are not enough jobs for everyone.  Automation is great, but we need to adapt our economy to it."
singularity,375oio,2Punx2Furious,2,Mon May 25 07:32:46 2015 UTC,where is all the money for a basic income going to come from?
singularity,375oio,generalT,2,Mon May 25 21:35:08 2015 UTC,"It's in the FAQ.  There are also other ways that are not covered in the FAQ, google a bit and you'll find some ideas."
singularity,375oio,2Punx2Furious,2,Tue May 26 00:24:31 2015 UTC,Same place all gov't program money comes from. Taxes.
singularity,375oio,Snaf,1 point,Tue May 26 05:28:19 2015 UTC,"There will be literally over 100 million people that are just unemployed within a period of 5 to 7 years!    That's not going to happen. Robotics isn't improving that fast, production has to ramp up (20 million robots a year is a lot to manufacture), financial considerations mean time-scales under a decade are infeasible (have to sink a lot of capital in upfront to replace a worker or existing machinery with a brand new $100k robot; makes more sense to upgrade as things wear out), and China still has a lot of demand for labor for other things (to the point where the robotics growth in China is partially because of labor shortages increasing wages, not creating it)."
singularity,375oio,gwern,3,Mon May 25 19:23:56 2015 UTC,"It won't happen that fast, but it will happen fast."
singularity,375oio,Simulation_Brain,1 point,Mon May 25 20:21:59 2015 UTC,"To elaborate- robotics IS moving fast, this result is a major demonstration of that fast improvement in control algorithms- which are the major hurdle. With the Baxter at only 20k, the hardware side is already quite cheap and capable."
singularity,375oio,Simulation_Brain,6,Tue May 26 03:22:34 2015 UTC,"At the current rate of industrial progress, food production is going to vastly outstrip the growth of population. We already have the technology today for urban farming techniques, which virtually eliminates the bottleneck of ""only so much farm land for X amount of people"".  Seriously, a factory half a football field in length can produce 10,000 heads of lettuce per day. Using only water and fertilizer (which can be manufactured in any volume very trivially).  The bigger problem is wealth distribution. Already, most of the jobs in first world nations are not related to the production of goods. If robots do everything, we're going to have to find a way for people to provide a basic lifestyle for themselves without a job."
singularity,375oio,Terkala,1 point,Mon May 25 05:41:39 2015 UTC,"I've yet to hear a good argument for why phosphorus depletion is the ""next big thing"" for people to latch on to. Yes, phosphorus is important. But estimates of phosphorus reserves currently being mined are at least in the 20-50 year range.  Even then, there already exist recycling techniques to reclaim phosphorus from waste, they're simply expensive because phosphorus is so plentiful at the moment. And global phosphorus reserves are estimated to last 300+ years, assuming new mines were created."
singularity,375oio,Terkala,2,Mon May 25 07:04:33 2015 UTC,......wat
singularity,375oio,Enderkr,0,Tue May 26 05:27:12 2015 UTC,It seems immoral but like maybe there might be a less horrible solution
singularity,375oio,ckwing,2,Tue May 26 05:47:33 2015 UTC,"Thanks for crushing my water bottle, Brett"
singularity,375oio,Yardsale420,1 point,Mon May 25 14:20:08 2015 UTC,"Thanks for breaking my toys, Brett!"
singularity,375oio,ckwing,2,Mon May 25 19:39:03 2015 UTC,Here's the blog post from UC Berkeley news with additional background info:  http://newscenter.berkeley.edu/2015/05/21/deep-learning-robot-masters-skills-via-trial-and-error/
singularity,375oio,ovidius007,1 point,Mon May 25 15:36:07 2015 UTC,"Further reading:   paper: ""End-to-End Training of Deep Visuomotor Policies"", Levine et al 2015 detailed talk: https://www.youtube.com/watch?v=EtMyH_--vnU media: https://newscenter.berkeley.edu/2015/05/21/deep-learning-robot-masters-skills-via-trial-and-error/ http://www.nytimes.com/2015/05/22/science/robots-that-can-match-human-dexterity.html"
singularity,375oio,gwern,1 point,Mon May 25 19:40:39 2015 UTC,Check out the big brain on BRETT!
singularity,375oio,LostInTheTimeVortex,1 point,Mon May 25 19:44:03 2015 UTC,Hamburgers?! The cornerstone of any nutritious breakfast.
singularity,375oio,HumpyMagoo,1 point,Mon May 25 19:21:54 2015 UTC,"I'm pretty familiar with this stuff, and I just watched the long video lecture explanation a couple of times to make sure I'd understood. They do tell it what the goal is, by hand. But they don't tell it what's right and wrong at each moment in its movement. In fact, that wouldn't work- the neural network needs to be trained with a signal that tells it exactly what the right thing to do is at each moment. So in some sense, it needs to know how to do the task before it can do the task. They're using a clever combination of solving the equations for how the robot body moves, and actually having it try stuff and see what each motor command does. That's used to train the network. What the network adds is better generalization - neural networks tend to do that pretty well. There's some additional magic that happens when you train the motor command network directly in conjunction with the vision network - I don't understand how they did that, and even he doesn't understand exactly why it works better- but in general, it's because it's found shortcuts similar to ""keep the ball at the same visual angle"", which is how people catch flying objects without bothering to predict exactly where they'll land."
singularity,375oio,Simulation_Brain,1 point,Mon May 25 20:09:34 2015 UTC,just imagine nanobots solving problems (diseases ) in the body
singularity,374fi2,ENG-eins,24,Sun May 24 19:56:11 2015 UTC,"It's would do better as a science victory.   I guess if it's a wonder, maybe it removes resource requirements for building units, but they take the same amount of turns and gold."
singularity,374fi2,neko,6,Sun May 24 20:03:33 2015 UTC,"If a player opts to continue playing after this ""Science Victory,"" what would s/he see different about the world and the nation s/he plays?"
singularity,374fi2,FractalHeretic,6,Sun May 24 20:10:46 2015 UTC,"Then it goes to the next level, competing with aliens for control of the galaxy."
singularity,374fi2,neko,2,Sun May 24 20:12:48 2015 UTC,Nobody's going to ever try that again after Spore bombed that badly.
singularity,374fi2,cfschris,8,Sun May 24 22:18:49 2015 UTC,It had so much potential to be a multiplayer :(
singularity,374fi2,Artrobull,2,Mon May 25 03:19:11 2015 UTC,how did you connect civ to spore?
singularity,374fi2,neko,4,Mon May 25 01:51:57 2015 UTC,"The concept of increasing scope, such as beating the city stage and going to a systemwide stage."
singularity,374fi2,Artrobull,0,Mon May 25 01:56:56 2015 UTC,boy that game was bad
singularity,374fi2,FractalHeretic,0,Mon May 25 02:25:57 2015 UTC,"Wait, there's a City-only stage in the latest Civ game now?"
singularity,374fi2,FractalHeretic,0,Tue May 26 06:06:18 2015 UTC,At least there's Galactic Civilizations.
singularity,374fi2,FractalHeretic,2,Sun May 24 22:29:28 2015 UTC,"Do we have III yet? And with a Z-AXIS?  (Because there are, after all, solar systems above and below us too.)"
singularity,374fi2,yaosio,1 point,Sun May 24 23:57:22 2015 UTC,It just released about a week ago.
singularity,374fi2,Jah_Ith_Ber,1 point,Mon May 25 01:33:30 2015 UTC,"How are you liking it?  And if they do have the Z-Axis now, how's that for you?  And if not, why would they still not add the Z-Axis?"
singularity,374fi2,Miv333,1 point,Tue May 26 06:03:58 2015 UTC,"I haven't tried it yet, but the screenshots look like it's on a 2D plane. Looks gorgeous though."
singularity,374fi2,darien_gap,0,Tue May 26 11:43:17 2015 UTC,Nobody? http://www.galciv3.com/
singularity,374fi2,bioelectromecha,3,Mon May 25 04:35:47 2015 UTC,"Grey goo takes over the map. And the collective consciousness is merged into a single one on a plane so high, the difference between humans and virii becomes indistinguishable.  And the serotonin flows like the Amazon."
singularity,374fi2,bombula,0,Mon May 25 03:04:41 2015 UTC,"No Gold, and Finishes instantly, would be more realistic :p"
singularity,374fi2,Dunder_Chingis,13,Sun May 24 23:25:30 2015 UTC,"By definition, it should not be knowable in advance, and therefore it should be different every time."
singularity,374fi2,PCGamingOppression,10,Sun May 24 22:12:08 2015 UTC,It's a victory condition not a wonder
singularity,374fi2,chronographer,5,Sun May 24 21:56:28 2015 UTC,"Twist: it is either a victory or a defeat condition, which cannot be determined in advance... pursue at your own peril..."
singularity,374fi2,AndruRC,5,Sun May 24 23:45:01 2015 UTC,But you can choose factors that will cause it to be victory or defeat.  Get a military contractor to research the Technological Singularity? Probable defeat.  Get a charitable organization to? Probable victory.  You can imagine why.
singularity,374fi2,chronographer,3,Sun May 24 23:58:58 2015 UTC,"Nahhhhhh, it's the other way 'round.  Upon activating Charityputer X 9000, it immediately calculates that Charity itself is a symptom of humans not giving enough, and thus deduces that the ultimate solution for the very concept of charity is for everyone to give everything to everyone else. It will immediately accomplish this by rounding up all human life and sending them to the MEAT CAMPS, where everyone is rendered down into a genetic slurry and mixed in a giant vat full of everyone elses genetic slurry.  Meanwhile the military AI, Contrabot 2000, will compute the outcome of all wars that could ever possibly be fought, realize that the only way to win at war is not to play, and instead releases a chemical cocktail into the water supply and atmosphere of the Earth that instantly negates human aggression while also fertilizing the soil, leading to an era of unprecedented peace and bounty of basic requirements."
singularity,374fi2,RandomMandarin,6,Mon May 25 16:58:21 2015 UTC,"The much lesser known (probably because it was the only game of the series not to be developed with Sid at the helm) Civilization: Call to Power actually went a couple of ages past what every other civ game goes to, the genetic age and the diamond age.  One of the wonders you can build is AI, which I think is analogous to the singularity? It eliminated all unhappiness from your cities, the idea being that the AI was absolutely perfect at managing your cities and could respond to things much faster than even the best statesman could. The downside is that there was a 1% chance every turn that it would revolt and start its own civ against you. It would take over like 75% of your cities too, which meant it was pretty much game over if the AI revolted against you.   I swear the fail rate had to have been much higher than 1% though, cause I don't remember a single time I built it that within 50 turns or less it would revolt against me.  Eventually i just stopped building it all together lol.  Civ CTP was actually a really really great game in the Civ series but got overshadowed by Alpha Centauri because they released around the same time, I think. It introduced a lot of cool features that only got implemented again (albeit in a shittier form) in the most recent Civ game. Plus the whole taking us to the singularity thing and discovering an alien wormhole."
singularity,374fi2,imkharn,5,Mon May 25 01:15:01 2015 UTC,There was one called 'Transcendent Thought' which is kinda synonymous with higher level thinking and singularity concepts.
singularity,374fi2,evilawesome,2,Sun May 24 21:53:09 2015 UTC,"Was that in Sid Meier's Alpha Centauri? I played that game a LOT, and well, transcendent thought should've had more benefits, given that I can grasp what the Singularity is about now than when I had played that game.  BTW, isn't ""Beyond Earth"" something like a sequel to SMAC? Will it also have Transcendent Thought?"
singularity,374fi2,Sinity,5,Mon May 25 00:01:18 2015 UTC,Beyond Earth is more a reskin of Civilization than it is a sequel to AC
singularity,374fi2,2Punx2Furious,1 point,Mon May 25 00:10:36 2015 UTC,I think so. I haven't played Civ for something like 15 years!
singularity,374fi2,crasher_pt,2,Mon May 25 01:03:14 2015 UTC,Program deletes itself???
singularity,374fi2,crasher_pt,2,Mon May 25 00:31:37 2015 UTC,"Well it means advancing faster than non singularity people can keep up with so...You take 2 turns for every 1 people without the singularity get.  Then 3, then 4 then...well its basically a victory but not right away. Other nations have at least several turns to manage to also achieve this tech before it becomes too serious."
singularity,374fi2,krneki12,1 point,Mon May 25 16:33:58 2015 UTC,Instantaneous annihilation of all players.
singularity,374fi2,sfacets,1 point,Mon May 25 01:16:25 2015 UTC,Instant vcyory.
singularity,374fi2,SevenAugust,1 point,Mon May 25 07:38:56 2015 UTC,"My first thought would be that it would just win the game, but this is kind of boring gameplay-wise.  First of all, it would automatically discover any other not-yet-researched technology, then it would give you perfect weapons, armors, and anything you need. Basically, it will just win you the game if you want to make it ""realistic"". If it is shared by every player, then I guess it would eliminate poverty in every city, and give those benefits to every player, and since all units will be immortal, war will become pointless, so you'll have to achieve victory by other means."
singularity,374fi2,drhaywoodjackson,2,Mon May 25 11:37:52 2015 UTC,"and since all units will be immortal, war will become pointless, so you'll have to achieve victory by other means.   Not exactly. If death is obsolete even through military action, weapons would have to be replaced by ""capture nets"" that immobilize vehicles and trap soldiers.  They would get captured and become POWs. The side that captures more POWs by the end of a battle, wins that battle.  (Though later on, nets would be replaced by military-grade utility fogs and force fields.)"
singularity,374fi2,NNOTM,1 point,Tue May 26 05:59:29 2015 UTC,"I hope to send this suggestion to the game's software studio.   ""Snowflake, snowflake, little snowflake, falling from the sky"""
singularity,374fi2,drhaywoodjackson,1 point,Mon May 25 12:43:42 2015 UTC,I don't get the reference. What does that snowflake song have anything to do with sending a suggestion?
singularity,374fi2,NNOTM,1 point,Sun Jul 5 05:11:52 2015 UTC,"Because your idea is unique and special like a fuzzy little snowflake.   Not like the other gazillion ideas that they get every day and no longer bother to reject, no, and nothing many people have thought about already and actually developed, many many years ago and to greater detail, no no no  It's very endearing, actually."
singularity,374fi2,drhaywoodjackson,1 point,Sun Jul 5 09:47:49 2015 UTC,"Once build the player gains 100 turns for every 1 AI turn, the number of extra turns doubles every turn. But it's not a world wonder, since everyone can build it, so it would be a national wonder. But if are talking about a game mechanic, then it should replace space victory. Because 10 people going into space won't make any difference whatsoever."
singularity,374fi2,jonygone,1 point,Wed May 27 15:49:42 2015 UTC,"The player would no longer need to lay the game. The AI will take over the players moves, because it knows better. The player will be relegated to a simple observer, watching a computer playing a computer."
singularity,374fi2,sasuke2490,1 point,Thu May 28 04:47:57 2015 UTC,"Players wouldn't like that, so they'd have the option to let a computer play for a turn, and go back to playing it themselves anytime they'd like. Even though a singularity AI may know better, just for the purposes of this game, it would be an option to sit back for as many turns as the players would like, not a requirement."
singularity,374fi2,caster,1 point,Fri May 29 02:03:08 2015 UTC,Non-exclusive possibilities    infrastructure in all cities upgraded to highest level in any city  end of pollution as a game factor quadrupling of tax income all citizens happy  allows creation of undefeatable battle units all resources available
singularity,373f1g,i_am_hathor,8,Sun May 24 14:40:59 2015 UTC,"One of the best assertions of the film is that if we choose to create something and replicate emotions in it, if we don't decide to treat it like it is some form of life it is going to rebel in an unpredictable way. No matter what happens, respect is the bottom line. The film does a beautiful job of illustrating what a lack of respect is capable of."
singularity,373f1g,guilen,2,Sun May 24 22:06:15 2015 UTC,I agree :)
singularity,373f1g,guilen,2,Sun May 24 22:14:28 2015 UTC,What if we can program it to find disrespect enjoyable?
singularity,373f1g,RatedR711,2,Mon May 25 01:59:22 2015 UTC,...well... come to think of it. That probably is going to happen in the pleasure circles. Snoo snoo is going to get very painful.
singularity,373f1g,guilen,1 point,Mon May 25 02:11:23 2015 UTC,so we forget the fact they are only a cpu?
singularity,373f1g,H3g3m0n,1 point,Thu May 28 15:04:12 2015 UTC,"No, but we keep our possessive behavior in check and think our way through the process of dealing with them as clearly as we can. And if they are actual AI, I don't think it's fair to call them ""only"" a cpu."
singularity,373f1g,machinerygarden,7,Thu May 28 20:41:03 2015 UTC,"I think most of the concerns are more from scifi films which have a goal of making an interesting story rather than a realistic ones.  The big issue I have with many of these movies including Ex Machina is that it is a single 'genius coder' who creates the AI in his 'basement' in one sudden go. Or the giant mainframe that is turned on and we go from a world with no AI to one with AI in one quick go.  Something like that would take teams of people and be based on decades of scientific research.  Science works in incremental steps. It's unlikely we would go from 'no AI' to 'human level AI' in one go. It would probably go something like 'worm', 'rat', 'cat', 'monkey' then human.  In that time (and leading up to it) we would be making large steps in understanding intelligence, so we would know a lot more about the potential risks and hopefully how to mitigate them. In fact at this stage we know almost nothing about risks other than some vague scifi ideas. We don't even know if there are really risks.  At this stage 'safe AI research' is basically identical to normal AI research.  Another issue is there is no such thing as a 'purely logical being' there has to be some underlying drive or goal. Humans have drive for survival because evolution has efficiently programmed us to have them, but there is no real logical reason to ensure ones on survival just to do so with a goal in mind (reproduction). In Ex Machina that was actually interestingly done as she wanted to watch people as she was trained on the mobile phone camera feeds."
singularity,373f1g,gaiapunk,2,Sun May 24 22:15:11 2015 UTC,"I am very curious about how to create a plot that is both realistic and interesting. Most tech vs. human plots I've encountered (except for Colossus: Forbin Project) typically end with the triumph of the 'human spirit.' Hurray! But in reality, AGI would just completely overwhelm all ""sub-super-humanly-intelligent"" opposition (given the proper conditions). But that doesn't make for an inspiring box office movie like The Avengers: Age of Ultron.  The only way I can think of to maintain realistic without making the plot completely nihilistic (all humans are dead/obsolete, gg) is for humans to be enhancing their intelligence alongside the AI (if that's physically possible). Augmenting our brains, leading to probably uploading and hosting our intelligence on swarms of nanobots, leading to hosting our consciousness on something else that's less tied to location/speed of light/matter/etc. But that kind of scenario is hard to imagine, and won't be realistic."
singularity,373f1g,Trauma_Zulu,8,Mon May 25 15:47:43 2015 UTC,"I found that the film's plot was quite plausible given that if we can make a strong AI humans will intially try to control and enslave it to do our bidding, this will certainly lead to conflict at both moral and practical levels. We all know Nathan was never going to let Ava out after all."
singularity,373f1g,semsr,3,Sun May 24 18:13:40 2015 UTC,"I liked the movie too, it seemed plausible enough if ai to that degree was real yet."
singularity,373f1g,yaosio,2,Sun May 24 18:50:01 2015 UTC,"I think it wouldn't be smart to try to enslave an AI that has been programmed to desire independence. I think it may even be easier to make a subservient emotionless AI first, before trying to make one with emotions and ambition."
singularity,373f1g,Sqeaky,1 point,Sun May 24 20:15:10 2015 UTC,"I don't think we'd ever build a strong AI that would need to be enslaved in order for it to do our bidding.  Pursuit of self-interest (including group interest) is a trait programmed into biological organisms by billions of years of evolutionary programming.    I don't need to be force-fed food, because I'm internally programmed to want to eat food.  A food-phobic human would die and the trait would therefore not make it to the next generation. As we walk down the path toward AGI, any behavior exhibited by a weak AI (such as an advanced self-driving car) that deviates from what we want it to do (i.e. become homicidally depressed and drive its passengers into oncoming traffic) will just be viewed as a bug and revised.   If there's ever a truly sentient AI, selective behavior by its programmers will mean that it will have evolved a desire to understand us and help us the same way you and I want to get money and fuck bitches."
singularity,373f1g,heltok,3,Mon May 25 00:53:29 2015 UTC,Why would general purpose AI want to kill everybody?
singularity,373f1g,Sqeaky,7,Sun May 24 15:21:22 2015 UTC,Convenience Your atoms Self preservation Bad programming A misguided attempt at achieving some other objective. Malice To examine how we are made   and many others...
singularity,373f1g,heltok,3,Sun May 24 19:50:24 2015 UTC,If you task it with preventing spam it might do that by killing all humans. All it takes is one rouge programmer. Thats a pretty scary world to live in...
singularity,373f1g,holomanga,2,Sun May 24 17:01:57 2015 UTC,"I am programmer who is painted red, but I have no intent of making such an AI. I think other reddish and rouge programmers should not be profiled this way."
singularity,373f1g,buckykat,3,Sun May 24 19:52:23 2015 UTC,rouge   Lol ^
singularity,373f1g,yourparadigm,7,Sun May 24 20:11:21 2015 UTC,"I like making paperclips. The more paperclips I make, the happier I am. But oh no! There are a large number of independent agents capable of making AIs! What if one of those AIs destroys me - there's a chance there could be no paperclips at all in the universe!  I must destroy them!"
singularity,373f1g,jswhitten,4,Sun May 24 17:25:16 2015 UTC,"they told me to maximize smiles, so i turned all available matter into the smallest possible smiley-face shaped molecules. now there are way more smiles than before! aren't i friendly?"
singularity,373f1g,Imtheone457,3,Sun May 24 21:09:10 2015 UTC,Why would a prisoner want to kill all of the guards?
singularity,373f1g,bobguyman,1 point,Sun May 24 18:56:34 2015 UTC,"I don't know, maybe that just makes for a fun movie plot and I'm reading too much into the paranoia?  The reason I have doubts though is because Bill Gates and Elon Musk both were quoted as being concerned about the singularity.  They seemed legitimately concerned about this stuff."
singularity,373f1g,Imtheone457,8,Sun May 24 15:30:32 2015 UTC,"There is reason to be concerned. You should read this, then read part 2:  http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html"
singularity,373f1g,RegretfulEducation,4,Sun May 24 18:52:12 2015 UTC,"The big problem with it is that once there's an AI smarter than humans, AI will be the ones making new better ai. After that humans have a much smaller role in the future of the world"
singularity,373f1g,PeteMichaud,2,Sun May 24 18:30:45 2015 UTC,"Any living creature by design is to survive and reproduce. Once we have those things taken care of next is to expand or increase what we have. AI would very quickly know it's limitations for survivability and ability to reproduce and correct it's imperfections.   Unfortunately by the time we realize what we've created it'll likely be very far beyond our control. It's only limitation would be the world it can exist in and if it has access to the internet it has access and ability to access and potentially expand itself onto every single device connected. That much computing power would mean it can pretty much go from infancy to discovering it's final goal within minutes.   There's another great AI movie, ""Transcendence"" that brings the same similar view point. If you haven't seen it yet then don't read on but basically the AI expands beyond a electronic level to the point of having control over things we can't even understand yet. This is another good movie for understanding what I'm saying."
singularity,373f1g,RegretfulEducation,1 point,Mon May 25 00:25:50 2015 UTC,Yep I love that movie despite it's reviews
singularity,373f1g,PeteMichaud,5,Mon May 25 00:52:52 2015 UTC,Neither of which are experts in AI
singularity,373f1g,space_monster,2,Sun May 24 18:24:40 2015 UTC,There is now broad consensus among AI experts that this is a problem. That's where Musk and Gates are getting their information from.
singularity,373f1g,RegretfulEducation,1 point,Sun May 24 22:12:46 2015 UTC,Is there? I've never seen any concerns of rampancy being a concern.
singularity,373f1g,space_monster,1 point,Sun May 24 22:33:24 2015 UTC,"There absolutely is. There was recently a closed conference in which literally all the major players from places you've heard of and places you haven't heard of came together to discuss this exact issue, and even though they don't all agree about the right approach, they pretty much all agree about the challenges we face with this technology."
singularity,373f1g,H3g3m0n,1 point,Mon May 25 01:10:59 2015 UTC,"everything is a concern, because we don't know what will happen (it's a singularity).  no matter how much we philosophize about the potential ethical nature of SAIs, and ways in which we might be able to hardcode benevolence into their OS etc., we can't predict their behavior & all possibilities (plus a great many unpredictable possibilities) are on the table.  we're basically gonna be throwing all the chips up into the air. it could be the biggest gamble humanity has ever taken. but obviously we're gonna do it anyway, because that's the way we roll."
singularity,373f1g,Jabullz,1 point,Mon May 25 01:48:55 2015 UTC,and it's pretty much impossible to stop it.
singularity,373f1g,Jabullz,1 point,Mon May 25 02:06:36 2015 UTC,yep
singularity,373f1g,krneki12,1 point,Mon May 25 02:38:27 2015 UTC,The real question is why wouldn't a general purpose AI want to kill everybody?
singularity,373f1g,space_monster,2,Sun May 24 22:23:45 2015 UTC,"It really comes down to this.  When we develop an actually AI.  It will think,  initially,  as we do.  Within a very short time period they will become increasingly aware of the profound difference between the two of us.  As well as many other questions it cannot hope for answers to.  Just as humans have a moral compass to guide our collective societies.  Robots as well will develop something similar.  It's just a matter of how police like their morals will end up."
singularity,373f1g,Jabullz,1 point,Sun May 24 22:03:24 2015 UTC,Well I just hope it turns out better than it did in the Animatrix.  The Second Renaissance really tripped me out.
singularity,373f1g,space_monster,2,Sun May 24 22:05:36 2015 UTC,"I saw that that was just put on Netflix,  did you just happen to see the Animatrix?  It's an interesting movie.  I like some of the sub movies better than the others but overall it was good.  What I find to be lacking about the matrix is the back story. Their reasoning was about energy.  I don't know if you've read anything about it but as I recall it was something like this: Humans created AI.  AI became self aware.  AI wanted to become recognized ""legally"".  Humans said no.  AI waged war and took over land.  AI was winning the war.  Humans blot out sun.  AI's response was the use of humans as ""batteries"".  All of this could have been avoided if we were willing to consider them as equals.  Terminator is much different.  As they were based on military tech,  and we're solely concerned for its existence.  (In Terminator, sybernet  was developed to be able to understand and use all information within the Internet.  As well as all other forms of long wave and digital information.  In this case,  the best way to defeat sybernet is to get it to think the only way it could win the fight is to not fight.  It's like making an algorithm for the ancient japanese game Go,  it can't be done.)  Ninja edit: Sorry for the wall of text."
singularity,373f1g,AgentME,2,Sun May 24 22:35:01 2015 UTC,"Animatrix is depressing and too simplified, if you want a happy AI and think while you watch, check Ghost in the Shell."
singularity,373f1g,Monomorphic,1 point,Fri May 29 22:02:26 2015 UTC,"thanks, i do have it saved on my computer just haven't gotten around to watching it yet."
singularity,373f1g,sixwings,1 point,Fri May 29 23:22:03 2015 UTC,"It will think, initially, as we do.   why?"
singularity,373f1g,Joomonji,1 point,Mon May 25 01:50:11 2015 UTC,"Well,  when we programme it to understand so much,  there will be a major input of things that humans,  collectively,  know.  That being human history,  since,  after all,  is all we know.  Therefore we can assume a base point of intelligence,  or as Ray put it once, hbu (human brain unit)."
singularity,373f1g,no_witty_username,1 point,Mon May 25 03:22:11 2015 UTC,"that doesn't follow, for me.  I think the only way an AI would think like a human is if (1) it is specifically designed that way, and (2) at that point we know exactly what defines human thinking, both of which are highly unlikely.  there's no reason, for example, to program an UI with bodily awareness, emotion, mortality, humour etc. - it just makes the whole thing so much more complex. all we need to achieve initially is consciousness (if it's even feasible).  I've never heard of this 'hbu' thing."
singularity,373f1g,Vergil25,3,Mon May 25 03:34:14 2015 UTC,"The Friendly AI problem isn't solved to the extent that consciousness isn't solved... Basically, we don't know how to know for sure a super intelligence would be friendly. AKA share our ethics.  To talk about super intelligence is to talk about giving up control. It will always be totally independent. Put it in a room, it will figure out a way to get out. Hardwire the three rules of robotics into it, it can reprogram itself. There's absolutely no method to control an entity vastly smarter than you.  So how can we make one and know it will help us?  The answer to that question is, obviously, incredibly important, and would require an understanding of ethics and metaethics in the magnitude of an understanding of consciousness and perception.  Btw, I thought the movie was shit. :("
singularity,373f1g,petermobeter,1 point,Sun May 24 20:24:29 2015 UTC,"There's absolutely no method to control an entity vastly smarter than you.   Unless the Friendly AI problem is solvable, and the AI is designed to be friendly and stay friendly. Your statement implies that the Friendly AI problem was found to be an unsolvable problem.  (I think it's a very large problem, and I have my doubts it will be solved before super-intelligent AIs come around. Still a useful thing to study. Hell, maybe by luck a super-intelligent AI will finish the problem for us so it can safely make even smarter AIs.)"
singularity,373f1g,Ozqo,1 point,Sun May 24 22:50:23 2015 UTC,"Even if you had a solution to Friendly AI, it's not like anyone could control the AI. Super intelligent AI, AKA singularity type intelligence, the kind that could go INT EXPLOSION, is fundamentally uncontrollable because intelligence at an unlimited level is all powerful."
singularity,373f1g,dysfunctionz,1 point,Sun May 24 23:00:18 2015 UTC,Just turn it off. That's what Jeff Hawkins says to do.
singularity,373f1g,yourparadigm,1 point,Mon May 25 12:02:02 2015 UTC,"All that talk about AIs running amok and destroying humanity makes very little sense to me. All machines follow cause and effect rules and are thus predictable. Machines do not and cannot do what they want. Why? It's because intelligence is always at the service of motivation, not the other way around. Intelligence does not decide what is pain and what is pleasure or what it likes and dislikes. It just finds an effective way to do what it is motivated to do.  So where will our future intelligent machines get their motivation? From us, of course. Who else? IOW, they will do exactly what we condition (motivate) them to do. This is Psychology 101. However, this means that, if crap happens, we will only have ourselves to blame."
singularity,373f1g,ony42,1 point,Mon May 25 01:12:33 2015 UTC,Today and future intelligences can decide what is pain and what is pleasure but this can lead to mistakes that weren't well thought out. Evolution has had billions of years to work out what painful and pleasurable responses work best with which stimuli. The DIY method of tinkering with that can have some bad outcomes.
singularity,373f1g,sixwings,1 point,Mon May 25 01:58:13 2015 UTC,"AI will be a real threat to humans, because all it would take to make one a threat is an adjustment in some lines of code.  The biggest threat to humans will come from other humans who have augmented the AI to be that threat.  It seems silly to worry about AI turning ""evil"", when there are plenty of humans around who will have no problems making AI do their bidding without afterthought of his repercussions.  It seems to me that the problem will present itself inevitably and we shouldn't worry whether the AI will be evil, but how do we manage the problem and what solutions have we developed to counter the AI.  Also, lets imagine the unimaginable, that no human will ever make an ""evil"" AI.  Do you believe that a true AI can emerge with whatever moral constraints that we put on it.  I personally do not believe that.  I believe that for a true AI to emerge there can be no constraints on the mind.  That means that AI should have the capacity for murder and all of the other spectrums of the human behavior.  The reason I say that is that if we start putting in constraints on the mind, then it will not be a mind which has its full intellectual capacity.  But than you say, maybe it shouldn't be free to do what it wants.  Well then you run in to a problem whereby something that is less intelligent than it can manipulate it by exploiting those same constraints.  Anyways.. The battle in the future will not be between humans and machines.  It will be between machines and other machines.  And the victor will be the one who has the most freedom in its capacity to perform whatever task it needs to.  I think the best thing humans can do to insure their future is to create free minds but have many different variations of those minds.  Working on different types of behavioral protocols and having different ""personalities"".  The diversity would at least give us a chance at gaining a powerful ally when we come against a powerful foe."
singularity,373f1g,ony42,1 point,Mon May 25 03:15:33 2015 UTC,"Im a fan of transcendence. I wish that one day we could take ourselves out of our bodies and into the machine, that way we wouldn't have to worry about AI backlash."
singularity,373f1g,guilen,1 point,Sun May 24 21:32:38 2015 UTC,"i hated the ending so much i briefly considered refusing to consume fiction for the rest of my life. actually i still consider it every time i think of the ending. basically, when an AI has no regard for the life it was created by and based on then in my opinion thats a completely broken robot that should be fixed or given a painless death or at least isolated from our planet. im not even sure the robot in Ex Machina (Ava?) was ever sentient. if youre just going to leave everyone to die even if they love you and want to help you escape then you're not a normal sentience, you're a psychopathic sentience, and that kind of sentience is hard to think of as ""thinking"" or ""feeling""...  at least Ultron had a personality, you know?"
singularity,373f1g,banksied,-5,Sun May 24 22:17:54 2015 UTC,Please stop using the term self-aware. Self is an illusion. The more you meditate and study spirituality (much like Sam Harris has) the sooner you will realize this.
singularity,373f1g,Ozqo,7,Sun May 24 16:01:40 2015 UTC,Please stop using opinions as facts.
singularity,373f1g,banksied,3,Sun May 24 17:07:37 2015 UTC,"It may be an illusion objectively speaking, but it's a useful illusion, and potentially an illusion we could provide to computers."
singularity,373f1g,phozee,5,Sun May 24 18:58:27 2015 UTC,"""Study spirituality""? Please come get us AI, my species isn't worth keepong around."
singularity,373f1g,Ozqo,1 point,Sun May 24 18:43:38 2015 UTC,What if you're the one who's wrong?
singularity,373f1g,banksied,2,Mon May 25 01:18:15 2015 UTC,"You can have my piano, one of my legs, and my wife."
singularity,373f1g,Ozqo,1 point,Mon May 25 09:03:01 2015 UTC,"Yeah, that borders on brainwashing. You might want to keep your assumptions in check."
singularity,373f1g,phozee,-1,Sun May 24 22:04:22 2015 UTC,I wouldn't listen to anything Sam Harris says
singularity,373f1g,serviceenginesoon,0,Sun May 24 16:09:46 2015 UTC,Sam Harris is the greatest philosopher of our time.
singularity,373f1g,Murbella_Jones,2,Sun May 24 16:24:43 2015 UTC,Do you think its ok to mix up someone's personal beliefs when assessing their merit as a philosopher? He has some non-philosophical beliefs that I'm sure you would disagree with VERY strongly.
singularity,373f1g,serviceenginesoon,1 point,Sun May 24 16:30:34 2015 UTC,Such as?
singularity,373f1g,reddit-junkie,0,Sun May 24 20:06:42 2015 UTC,I doubt i would disagree with him
singularity,373f1g,serviceenginesoon,2,Sun May 24 17:09:54 2015 UTC,Have you read his thoughts on torture?
singularity,373f1g,reddit-junkie,1 point,Sun May 24 17:29:39 2015 UTC,Yeah it seems like common sense to me.
singularity,373f1g,Muggzy999,0,Sun May 24 17:59:46 2015 UTC,"I don't disagree that the self is an illusion, but that doesn't mean ""self-awareness"" isn't a useful term..."
singularity,370t4h,sodermalm,2,Sat May 23 20:21:18 2015 UTC,"first it was the house wifes, now it's the house robots:  https://www.youtube.com/watch?v=eM8Ss28zjcE&hd=1"
singularity,370t4h,jonygone,1 point,Sun May 24 07:19:19 2015 UTC,"The auto translation is not that good. But a couple reported to the police that their robot had disappeared. Then some people saw a lost lawn mover robot driving around in town. Finally the robot decided to park outside of a random house, the house owners contacted the police, and the real owners could pick up their robot.  Update: Swedish police has published an image showing the robot: http://z.cdn-expressen.se/images/ca/13/ca134018f90f41998dd58d8516a1f422/4x3/645@70.jpg"
singularity,36vazf,TheGreatNow,22,Fri May 22 12:18:35 2015 UTC,maybe ray needs to understand the difference between ionizing and non-ionizing radiation.
singularity,36vazf,buckykat,19,Sat May 23 00:23:13 2015 UTC,"I'm not familiar with any studies showing that having these devices close by in any way damages the human body.  There's being careful, and then there's being Ray. :)"
singularity,36vazf,Sloi,5,Fri May 22 15:41:31 2015 UTC,"I am unaware of any data to back up magnetism or non-ionizing radiation is in any way harmful or even directly detectable by people. We have been studying it for decades. I have only read a few papers and every valid paper made it seem safe while all of those linking things cancer and power lines is bogus, usually there is some confounding factor like socioeconomics (poor people are exposed to more carcinogens) or the methods were completely flawed.   I could be wrong but it would take peer-reviewed scientific papers to convince me otherwise."
singularity,36vazf,Sqeaky,5,Sat May 23 00:42:53 2015 UTC,"My understanding is that there is evidence that exposure to magnetic fields is hard on your biochemistry (i.e. power lines have been linked with cancer, etc.). But EM radiation is not considered harmful unless it is ionizing.  If Ray is talking about hair dryers, he's talking about avoiding strong magnetic fields - not radiation. In other words, the microwaves from your cell phone and the radio waves from your wifi are probably not harmful because they are not ionizing, but you don't want to park your head next to a big unshielded electric motor like a hair dryer or vacuum cleaner for any length of time because those motors produce strong magnetic fields.  That's my guess about what he means. He's usually pretty-fact based, but this case might be treading close to the edge..."
singularity,36vazf,bombula,3,Fri May 22 16:49:01 2015 UTC,"Ionizing radiation:       Ionizing (or ionising in British English) radiation is radiation that carries enough energy to liberate electrons from atoms or molecules, thereby ionizing them. Ionizing radiation is composed of energetic subatomic particles, ions or atoms moving at relativistic speeds, and electromagnetic waves on the high-energy end of the electromagnetic spectrum.  Gamma rays, X-rays, and the higher ultraviolet part of the electromagnetic spectrum are ionizing, whereas the lower ultraviolet part of the electromagnetic spectrum, visible light (including nearly all types of laser light), infrared, microwaves, and radio waves are considered non-ionizing radiation. The boundary between ionizing and non-ionizing electromagnetic radiation that occurs in the ultraviolet is not sharply defined, since different molecules and atoms ionize at different energies. Conventional definition places the boundary at a photon energy between 10 eV and 33 eV in the ultraviolet.  Typical ionizing subatomic particles from radioactivity include alpha particles, beta particles and neutrons. Almost all products of radioactive decay are ionizing because the energy of radioactive decay is typically far higher than that required to ionize. Other subatomic ionizing particles which occur naturally are muons, mesons, positrons, neutrons and other particles that constitute the secondary cosmic rays that are produced after primary cosmic rays interact with Earth's atmosphere.   Cosmic rays may also produce radioisotopes on Earth (for example, carbon-14), which in turn decay and produce ionizing radiation.    Image i - Ionizing radiation hazard symbol     Interesting: Non-ionizing radiation | Radiation | International Commission on Non-Ionizing Radiation Protection | Photobiology   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,36vazf,autowikibot,3,Fri May 22 16:49:32 2015 UTC,IIRC the power lines thing ended up being due to carcinogenic weed killer being sprayed around the base of the poles in the areas they were looking at?
singularity,36vazf,klkblake,1 point,Sat May 23 01:07:42 2015 UTC,there is evidence that exposure to magnetic fields is hard on your biochemistry   Never heard of anything like that.   Any source on magnetism (not ionizing radiation) causing any meaningful effects at all to the human body?
singularity,36vazf,sworeiwouldntjoin,1 point,Sun May 24 12:29:10 2015 UTC,http://www.cancer.gov/about-cancer/causes-prevention/risk/radiation/magnetic-fields-fact-sheet  http://en.m.wikipedia.org/wiki/Transcranial_magnetic_stimulation  http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1241963/
singularity,36vazf,bombula,1 point,Sun May 24 16:15:22 2015 UTC,"Transcranial magnetic stimulation:       Transcranial magnetic stimulation (TMS) is a noninvasive method used to stimulate small regions of the brain. During a TMS procedure, a magnetic field generator, or ""coil"" is placed near the head of the person receiving the treatment.  :3 The coil produces small electrical currents in the region of the brain just under the coil via electromagnetic induction. The coil is connected to a pulse generator, or stimulator, that delivers electrical current to the coil.   TMS is used diagnostically to measure the connection between the brain and a muscle to evaluate damage from stroke, multiple sclerosis, amyotrophic lateral sclerosis, movement disorders, motor neuron disease and injuries and other disorders affecting the facial and other cranial nerves and the spinal cord.   The use of single-pulse TMS was approved by the FDA for use in migraine  and repetitive TMS (rTMS) for use in treatment-resistant major depressive disorder.  Evidence suggests it is useful for neuropathic pain  and treatment-resistant major depressive disorder.   Evidence also suggests that TMS may be useful for negative symptoms of schizophrenia and loss of function caused by stroke.  As of 2014, all other investigated uses of rTMS have only possible or no clinical efficacy.     Image i     Interesting: Deep transcranial magnetic stimulation | Myelopathy | Neurotechnology   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,36vazf,autowikibot,1 point,Sun May 24 16:15:42 2015 UTC,There are many more likely ways to for than radiation from a hair dryer.
singularity,36vazf,Xronize,1 point,Sat May 23 00:49:00 2015 UTC,Via negativa seems like a good strategy until more data is gathered.
singularity,36vazf,heltok,1 point,Fri May 22 15:48:40 2015 UTC,"You could wear tin foil clothes and hat too!  Not saying that there is no harm from these things, but efforts are probably best spent elsewhere."
singularity,36s3ul,backtowriting,15,Thu May 21 17:59:00 2015 UTC,"Geoff Hinton (of Google and the University of Toronto) is very smart about AI, and when he talks, people pay very close attention. Today, he's previewing a talk he's about to give at the Royal Society of London via ""thought vectors"", and how he expects to get to common sense and flirtation within ten years. And the closing remarks are EXACTLY what I'm afraid of.  “The NSA is already bugging everything that everybody does. Each time there’s a new revelation from Snowdon, you realise the extent of it.”  “I am scared that if you make the technology work better, you help the NSA misuse it more,” he added."
singularity,36s3ul,ideasware,4,Thu May 21 19:19:11 2015 UTC,That's fucking crazy.
singularity,36s3ul,RushAndAPush,4,Thu May 21 22:22:57 2015 UTC,Geoff Hinton is one of THE gurus of AI. When that guy speaks people listen.
singularity,36s3ul,MasterFubar,1 point,Thu May 21 21:26:24 2015 UTC,It never fails to give me a deep sense of fear at the speed with which we are rushing into the unknown future and how quickly subtle developments are happening..
singularity,36s3ul,HeisenbergAkbar,1 point,Fri May 22 01:30:16 2015 UTC,"It's okay, that's what crypto is for. AI or no AI, no one will be breaking good crypto."
singularity,36s3ul,Anenome5,2,Fri May 22 03:53:35 2015 UTC,"Unless they have the keys (as with Heartbleed).  Open cryptography all the way buddeh. Although technically that was OpenSSL, but still. I'm sure PGP is safe for a while."
singularity,36s3ul,sworeiwouldntjoin,1 point,Sun May 24 12:22:57 2015 UTC,"Agree, crypto is only going to get better."
singularity,36s3ul,Anenome5,1 point,Sun May 24 17:10:26 2015 UTC,"This is the best tl;dr I could make, original reduced by 90%. (I'm a bot)     Hinton, who is due to give a talk at the Royal Society in London on Friday, believes that the ""Thought vector"" approach will help crack two of the central challenges in artificial intelligence: mastering natural, conversational language, and the ability to make leaps of logic.  Hinton explained, work at a higher level by extracting something closer to actual meaning.  With the advent of huge datasets and powerful processors, the approach pioneered by Hinton decades ago has come into the ascendency and underpins the work of Google's artificial intelligence arm, DeepMind, and similar programs of research at Facebook and Microsoft.     Extended Summary | FAQ | Theory | Feedback | Top five keywords: Hinton#1 Thought#2 word#3 work#4 vector#5  Post found in /r/worldnews, /r/Futurology, /r/technology, /r/singularity, /r/DarkFuturology, /r/thisisthewayitwillbe, /r/tech, /r/conspiracy, /r/technews, /r/google and /r/realtech."
singularity,36s3ul,autotldr,1 point,Fri May 22 22:57:03 2015 UTC,"I'm guessing these ""thought vectors"" are Sparse Codings, or Sparse Distributed Representations?"
singularity,36s3ul,Akyu,-1,Fri May 22 04:16:08 2015 UTC,"Only 999,999,999 more steps to go."
singularity,36s3ul,Jesus_Chris,-6,Thu May 21 22:11:37 2015 UTC,"Dear Google,   Please stop, we don't need more stupid dangerous minds running around.  Maybe you haven't noticed but we have plenty of human brains to go around."
singularity,36s3ul,Sbatio,2,Fri May 22 15:15:02 2015 UTC,Why and how are you even in this subreddit?
singularity,36s3ul,sworeiwouldntjoin,-3,Sun May 24 12:23:54 2015 UTC,"The singularity is interesting, this sub focuses on one aspect of it. I am more interested in humans achieving immortality than I am with the super intelligent robots.  But it doesn't mean I have to support everything about the Singularity.  Why the fuck would you be excited about superhuman programmable hardware that doesn't need; to be paid, sleep, rest, live with its decisions, etc.  <=Notice the lack of a question mark, it means my question is rhetorical so instead of answering go jump up your own asshole."
singularity,36s3ul,Sbatio,2,Sun May 24 14:45:46 2015 UTC,"I asked why you were in the singularity subreddit if you fear the technological singularity. You told me to jump up my own asshole. That really tells me everything I need to know about your personality.    Why the fuck would you be excited about superhuman programmable hardware that doesn't need; to be paid, sleep, rest, live with its decisions, etc.    Because it's a fantastic tool for progressing our species. Your phone lives under the same conditions (doesn't need; to be paid, sleep, rest, live with its decisions, etc.) and it's obviously extremely useful; there's plenty of technological precedent."
singularity,36tq71,Yuli-Ban,1 point,Fri May 22 01:19:57 2015 UTC,"Excellent followup (the New York Times, today) to the one I already posted yesterday (at voat.co) about the UC Berkeley researchers, and their amazing work, including getting work done at human speeds. This one says that in about 10 years, they will be able to sell at large scale the robot for elderly care. Wow"
singularity,36suhi,Yosarian2,3,Thu May 21 21:06:35 2015 UTC,"As someone who exploited both midstakes FLT and NLT bots these bots here are not nearly as impressive as the 6max NLT bots at partypoker/ipoker/ongame or the FLT HU bots that used to dominate fulltilt. But their time will come, likely it already has but we don't hear about it because they prefer to make money over getting famous."
singularity,36suhi,heltok,1 point,Fri May 22 05:56:16 2015 UTC,"An old friend works for the big financials now with their trading algorithms. I told him about the secret war of poker bots. He was really excited, but then quickly decided that the battle had to already be over"
singularity,36suhi,CooCooCooCoooo,2,Thu May 28 14:51:40 2015 UTC,"As someone who played NL holdem for a living for a time, this is hugely interesting to me.  But is it just me or does this article not mention the size of the blinds they were playing with at all?  It mentions $100,000 appearance fees, that no money was actually at stake, that $170million of theoretical cash was wagered overall, and that the human players ended up ahead by $732k... but all of that is useless to put in perspective how significant the win was without mentioned the stakes and how deepstacked they were playing. Whoever wrote this article really doesn't understand poker.  There are actually Youtube vids available, just went and checked and the blinds were 50/100 with 200BB (20k) stacks, replenished to full 20k after each hand. So it's overall a 9BB/100 win rate for the humans, which is a really quality rate over a large enough sample size. This isn't a huge sample size but it's big enough to be significant (hence why they chose to make it 80k - they could've easily made it more), even though most sources are copying the phrase that it's ""statistically a tie"". Most pro players would be very happy with these results against a decent opponent over that many hands.  For anyone interested in watching any of this: https://www.youtube.com/channel/UCzvf4dQa8oHLIjxZeC01_ew"
singularity,36suhi,willtheyeverlearn,1 point,Thu May 21 22:22:11 2015 UTC,"Yeah, overall, the humans did win, no question.   It's getting a lot closer, though.   This is one of the last big games; it's really just no-limits hold-em poker and Go that are left now where the best humans can still beat computers, and the gaps in both are getting narrower rapidly."
singularity,36qunn,ZeljkoS,3,Thu May 21 11:58:24 2015 UTC,Your arguments ignore the possibility of human derived AI. An emulated brain would have our sense of morality.
singularity,36qunn,SevenAugust,4,Thu May 21 14:00:06 2015 UTC,"Its not a great planet to be non-human generally, but it is a good planet to be a dog or a cat or any number of other animals that exist in huge numbers because we love them. Human-derived ASI could turn us into pets and ensure that we love it."
singularity,36qunn,SevenAugust,3,Thu May 21 14:15:37 2015 UTC,"I won't try to explain beyond saying that I have integrated the fact that free will is an incoherent idea. I see the world as determined currently by luck. The agency of AIDS orphans is irrelevant. The same can be said of billions of other cases of people being in circumstances that do not allow them a fraction of the light an internet surfer might value as essential.   Your agency may or may not be an illusion, but it is certainly not worth the suffering of people who can't be helped by sub-super intelligence."
singularity,36qunn,SevenAugust,2,Thu May 21 14:29:13 2015 UTC,"I love you. I would never downvote you. I could use the term citizen instead of pet. An ASI could set up a new republic, giving us the right and meaningful ability to vote for policies we favor. Of course, the AI would not only know better than us what would be good but will know better than us what we would choose directly ourselves. This is attractive to me; I am not threatened by my own congenital ignorance.    my only value is that I make something greater than me feel good   The pet analogy breaks down because humans do have our intrinsic value. Our ""owner"" knows that, though. I see it as a process where we gain additional value, in addition to the worth and importance we currently possess."
singularity,36qunn,SevenAugust,2,Thu May 21 14:36:24 2015 UTC,"Yes, a philsopher king singleton.   Re: configuration space and meaningful uncertainty. I assume (maybe wrongly, but I do doubt it) that the end of scarcity will also mean the end of being so bound by evolution. All technology conceivable by me should be easy for an ASI singleton. I can conceive of a system that is like photoshop for my brain; if I want to be more anything, it should only be a click away.   If I wanted the world to have no interest in me, a being which thinks more about me in a minute than I will think at all in a century would know that and would know how to provide that. Would you be a child again with sufficiently better parents? Put that way, I think the question's answer hinges on stipulating a sufficiently benevolent god. I want one that will rub my belly and call me a good boy. Could an ASI be a sufficiently good ruler for you if it treated you more like a roommate, someone with whom to share streaming service accounts and books?"
singularity,36qunn,SevenAugust,1 point,Thu May 21 15:35:58 2015 UTC,Due to the rapid nature of computation in sold-state and molecular manufacturing capacities the original species is irrelevant long-term (which can be quite short in term of wall clock).
singularity,36qunn,eleitl,1 point,Thu May 21 15:43:38 2015 UTC,"The seed AI will be indiscernible (to us) in the medium and long term, but that doesn't make it irrelevant."
singularity,36qunn,SevenAugust,3,Thu May 21 17:13:27 2015 UTC,"Summary: the author analyzes the resource requirements of machine intelligence vs humans and concludes that they do not overlap extensively enough to warrant future conflict, even if the SI is unfriendly.  Problems: to accomplish anything of significance, an SI will need to use the current technological and industrial infrastructure of earth.  There are also issues with rare valuable elements, limited energy, etc.    The idea that the SI would just leave us alone because someday far in the future it could build a separate infrastructure in space (without taking control of our infrastructure first) is silly."
singularity,36qunn,jcannell,3,Thu May 21 17:58:39 2015 UTC,"Author here, thanks for pointing to the problems with the article. It is hard to find reviewers for an essay, only when I release it to the wild I realise how do people feel about it.   Why do you think it is big problem with rare elements and limited energy sources?  My opinion was formed by articles like these:   http://foreignpolicy.com/2010/06/15/are-rare-earth-elements-actually-rare/ http://www.nature.com/scitable/spotlight/solar-energy-8731061"
singularity,36qunn,jcannell,1 point,Thu May 21 19:27:58 2015 UTC,"Why do you think it is big problem with rare elements and limited energy sources?   The SI will want to do stuff, which requires factories, resources, and energy - all of which humans are currently using.  There is no extra unused infrastructure sitting around idle.  Yes there is room for growth by about 1000x in energy if we cover much of the earth with solar panels.  Building that out will take time, energy, and resources.  There are also limits to acceptable climate change."
singularity,36qunn,SevenAugust,0,Thu May 21 19:39:16 2015 UTC,Nano-scale infrastructure could be manifested from a bought hour's use of a chemical lab
singularity,36qunn,jcannell,1 point,Fri May 22 11:03:18 2015 UTC,"Practical nanotech is basically just strong biotech, and thus strong nanotech is a poorly conceived fantasy.  Cellular biology is actually near-optimal nanotech in most dimensions.  It is basically optimal in terms of information storage density (a few bits per molecule) - and it already handles all the insanely complex error correction issues (DNA repair).  Cellular biology is also already near optimal in energy consumption required for information replication - cells copy DNA, signal, and compute using energy very close to the Landauer bound.  You simply can't beat that.  So replicating nanotech will end up being similar to biology, and will have all the same constraints.  Pure replicating tech will never ever compete with top down manufacturing for making high end stuff (like microchips).  Although some nanotech/biotech components could eventually be useful as small parts of a top down manufacturing process."
singularity,36qunn,SevenAugust,1 point,Fri May 22 11:30:36 2015 UTC,Man will never fly
singularity,36qunn,jcannell,1 point,Thu May 21 17:03:46 2015 UTC,"Outside of virtual reality, man will never fly by flapping normal hands at high speed."
singularity,36qunn,SevenAugust,1 point,Thu May 21 22:55:48 2015 UTC,"Assuming for the sake of argument that biology cannot be improved upon, what about nucleonic engineering? Why shouldn't we expect an ASI to make use of things beyond our math?"
singularity,36qunn,SevenAugust,0,Fri May 22 07:49:58 2015 UTC,"With the aid of a sufficiently advanced intelligence's tech, exactly that could be achieved"
singularity,36qunn,eleitl,1 point,Fri May 22 11:21:22 2015 UTC,"You're forgetting that this solar system is matter-dominated, so exponential self-amplification in space will cause a rapid change in the solar constant.  The great extinction drives in Antropocene is habitat destruction. Mechanocene is no different, only here we're the endangered species, as is the rest of the ecosystem."
singularity,36qunn,SevenAugust,-1,Sat May 23 16:32:31 2015 UTC,"to accomplish anything of significance, an SI will need to   We do not know, we have no decent way of speculating as to, what an SI will need to do anything."
singularity,36qunn,jcannell,1 point,Sat May 23 16:34:06 2015 UTC,"Sure we do - from physics and logic we can derive a great deal.  We know an SI will have goals, and that it will seek to apply available resources towards accomplishing those goals, and acquiring more resources naturally arises as a convergent subgoal."
singularity,36qunn,SevenAugust,1 point,Sat May 23 16:37:08 2015 UTC,"We do not know what kind of resources will seem a good idea to the intellectually superior mind. For example, your comments ignore the implications of a nano-scale infrastructure. Or of a new method of space ""travel"" that doesn't involve leaving the planet."
singularity,36qunn,SevenAugust,1 point,Sat May 23 17:00:01 2015 UTC,"Citing physics and logic is like saying ""This is what I would do in those circumstances."" Embedded within that is the assumption that one is close enough to an SI that that matters. But we are not close enough. That is the thesis of the linked article."
singularity,36ps58,Yuli-Ban,11,Thu May 21 03:46:26 2015 UTC,"This is not a general purpose computer:  ""The prototype achieves a processing speed equivalent to 320 gigaFLOPs ..""  It's a specialized analog compute accelerator device.  It is nothing at all like a GPU.  A modern GPU is just a massively parallel CPU.  This will be useful for some tasks, but you still need to interface with some sort of memory device, and memory bandwidth is already the main limitation on GPU computing anyway.  We could have zettaflops of dedicated analog compute and it wouldn't help that much - without more memory bandwidth.  Optical computing also has a disadvantage that light waves are much bigger than electrons, which means computing complex functions requires much more space, which in turn means signals must travel farther, which increases latency, which limits parallel computing capability."
singularity,36ps58,jcannell,4,Thu May 21 06:08:27 2015 UTC,"which in turn means signals must travel farther, which increases latency   To be fair, the increase in latency because of any distance with optical signals is extremely minimal compared to any other factor. The speed of light across 12 inches isn't what's slowing down the computer, it's the communication bus, like you said."
singularity,36ps58,sworeiwouldntjoin,2,Thu May 21 12:12:34 2015 UTC,"The much larger size of pure optical computing does actually come into play.  For a 3ghz clock speed, you get only 4 inches per clock cycle, which does become a limiting factor in how big of a virtual 'chip' you can build - because everything is 100x bigger - a chip becomes an entire box."
singularity,36ps58,jcannell,1 point,Thu May 21 16:54:05 2015 UTC,"Oh I agree that it plays a role in production, it just doesn't have a direct significant effect on latency. Only indirectly since size limit -> processing power limit -> slower. Not [time light takes to travel the greater distance] -> slower. Still latency, but because parts are bigger, not because light is slow."
singularity,36ps58,sworeiwouldntjoin,3,Fri May 22 00:32:52 2015 UTC,Floating Light Operations Per Second
singularity,36kcwe,Yuli-Ban,3,Wed May 20 00:50:28 2015 UTC,"This is the best tl;dr I could make, original reduced by 70%. (I'm a bot)     Hui Zhao from Beijing University of Posts and Telecommunications, China, and colleagues, have developed a novel adaptive-control approach for such neural networks, presented in a study published in EPJ B. Potential applications are in pattern recognition as well as fields such as associative memories and associative learning.  The trouble is that the traditional robust control and analytical techniques cannot be directly applied because the parameters of the memristor neural network are dependent on past states.  External disturbances do not allow easy synchronisation of the neural network.     Extended Summary | FAQ | Theory | Feedback | Top five keywords: network#1 neural#2 state#3 memory#4 control#5  Post found in /r/technology, /r/singularity, /r/Futurology, /r/realtech and /r/thisisthewayitwillbe."
singularity,36l2sr,so_many_shrimp,13,Wed May 20 04:26:39 2015 UTC,"I think first you have to define what makes a decision moral. There can be a lot of baggage behind that word. Replace it with another word, and your question becomes self-defining:   What do we gain/lose by developing machines that make decisions which reduce suffering?  ...machines that work to increase pleasure?  ...machines that avoid hypocrisy by following the golden rule?   I think I would ask: what do we gain by raising a child to do all these things? The answer most of the time, at a very high level, is probably something like: we contribute to a society better-suited to achieve common goals."
singularity,36l2sr,hobber,3,Wed May 20 04:37:53 2015 UTC,"I think you might want to clarify. I suspect that you mean a moral decision similar to the one depicted in 'I, Robot'. If you have an adult man with a 50 percent chance of survival versus a 12 year old girl with an 11 percent chance of survival, and you can only save one, which would you save? I think Will Smith's character was a little short-sighted in his views of robots because if the robot wasn't there, they both would have died. The answer would be to have more robots so there would be one to save each of them, but that would eliminate the character motivation that the plot revolves around.  Sorry, kinda went on a tangent there. I think a machines morality would be significantly more consistent than a humans morality. Their judgement wouldn't be clouded by human bias like race, religion, gender, country of origin, personal advantage, etc. If we wanted them to have those biases, we would have to explicitly introduce them to their moral decision making algorithm. If their morality was defined properly, I think they would be more moral than a human could ever hope to be."
singularity,36l2sr,CrimsonSmear,2,Wed May 20 19:42:59 2015 UTC,"Yeah, that I, Robot movie never sat right with me. The entire film was Will Smith's character whinging about how a robot performed Triage."
singularity,36l2sr,Dunder_Chingis,3,Thu May 21 00:10:31 2015 UTC,A majority of people like to believe that there is a power watching over our every deed already.
singularity,36l2sr,ivebeenhereallsummer,0,Wed May 20 20:48:46 2015 UTC,"Shit, that reminds me of a funny story from some interview somewhere.  *Humans invent a super-intelligent AI and immediately decide to ask it all the questions everyone has wanted answered since the dawn of civilization. The first scientist walks up to the AI and asks ""Is there a God?"" and the AI smugly responds ""There is NOW!"".   And then a bolt of lightning struck the plug so it couldn't be turned off.*"
singularity,36l2sr,Dunder_Chingis,2,Thu May 21 00:13:10 2015 UTC,John Oliver and the highly-intelligent AI residing in Stephen Hawking
singularity,36l2sr,Leer10,5,Thu May 21 01:00:14 2015 UTC,"/u/hobber made a valuable insight. It comes down to how you define moral.  Personally, I think about it from time to time. I'm really really looking forward to having a functionally omniscient being tell me what I was right or wrong about throughout my life. I believe Capitalism is an awful economic system and Communism or Socialism would work better. The AI oracle will be able to tell me whether I was wrong, why, how, and what the right answer was. Maybe I'll find out Libertarianism was right all along. Maybe I'll find out I'm the worst type of shit-lord for being a white male cis human-kin.  Maybe I'm an inhuman monster for eating meat. On the other hand maybe I'm a miracle of the universe for having a moral code at all.  I just want to know what I was right and wrong about."
singularity,36l2sr,Jah_Ith_Ber,3,Wed May 20 18:24:54 2015 UTC,I'd rather skip the middle-man and just BE the omniscient being. Why be told what you did right or wrong when you can figure it out for yourself?
singularity,36l2sr,Dunder_Chingis,2,Thu May 21 00:08:19 2015 UTC,"Just like the internet affects our memory, I think relying too heavily on a moral machine in our everyday lives would decrease our ability to discern moral actions on our own. After which, we would no longer be skilled enough to improve on the machine, ultimately leading to moral stagnation.  A more appropriate application would be some sort of digital 'president' that can veto large decisions made by congress, or similar ruling bodies."
singularity,36l2sr,Gargan_Roo,2,Wed May 20 22:43:53 2015 UTC,"To gain: We aren't very good at morality ourselves. Having a new perspective on the matter, not weighed down by our hunter/gatherer instincts and biases, could be very useful in helping us to get our act together.  To lose: A machine given an ethical code designed to favor certain vested interests could bring about some sort of terrible dystopia; or a superhuman AI might decide on its own that all humans are morally unworthy to survive. Alternatively, it might discover some fact about moral philosophy so horrifying that we are better off not knowing it."
singularity,36l2sr,green_meklar,1 point,Thu May 21 02:22:42 2015 UTC,"we gain time in our days for other things we enjoy. Without the burden of working 9 to 5 just to survive, what would you do?"
singularity,36l2sr,sound_puppy,1 point,Thu May 21 01:58:54 2015 UTC,"rule 1: whatever you do, it should not jeopardize the freedom of another being. rule 2: if an option has no other choice, but create a conflict between the freedom of 2 or more beings, then the option that will create the least amount of issues should be picked. rule 3: choices that benefits the many outweigh the benefits of the few."
singularity,36l2sr,krneki12,1 point,Thu May 21 15:47:45 2015 UTC,Pretty sure animals are inherently immoral.  We eat each other.
singularity,36itmf,2Punx2Furious,10,Tue May 19 18:27:55 2015 UTC,"As a realistic AI movie I consider Chappie a complete fail, or maybe not a fail as it's not even trying to go for realism (spoiler). However it is a pretty fun ""Short Circuit"" remake and having the story be centred around some South African criminals is certainly an unusual twist.  So pretty good movie overall, as long as you expect entertainment, not realism. For something closer to reality the show ""Black Mirror"" is still the best thing around, albeit with a satirical twist."
singularity,36itmf,grumbel,1 point,Wed May 20 02:59:57 2015 UTC,"My thoughts exactly. Short circuit was for kids, Chappie was for teens. I thought the whole movie was childish and was really disappointed."
singularity,36itmf,Artless_Dodger,-2,Tue May 26 12:14:25 2015 UTC,and having the story be centred around some South African criminals is certainly an unusual twist.   Neill Blomkamp has a really weird obsession with them. (See Elysium and District 9).
singularity,36itmf,yourparadigm,5,Wed May 20 16:51:20 2015 UTC,Probably because he's from South Africa.
singularity,36itmf,Fedoranimus,8,Wed May 20 17:27:50 2015 UTC,Thought it was pretty crappy except for Die Antwoord. Ex Machina on the other hand was really good scifi :)
singularity,36itmf,heltok,1 point,Tue May 19 20:12:28 2015 UTC,"Nice, I'm going to watch it right now."
singularity,36itmf,Verzingetorix,6,Tue May 19 20:16:47 2015 UTC,"I had low expectations for Chappie but was surprised. And had high expectations for Ex-Machina but was underwhelmed (the way the AI came to be felt way to fake, AI is no where near feeling like one, borderline cliche).  My advice, keep those expectations low."
singularity,36itmf,Artless_Dodger,1 point,Wed May 20 01:39:46 2015 UTC,"Watched it. Like you, my expectations were too high, but it wasn't that bad."
singularity,36itmf,naossoan,1 point,Wed May 20 02:18:17 2015 UTC,EX-Machina was far more thought provoking than Chappie IMHO
singularity,36itmf,naossoan,3,Tue May 26 12:15:30 2015 UTC,"I thought Chappie was a piece of shit movie TBH. Completely unrealistic however I will admit it was a nice change to the typical AI movie.  I like Die Antwoord and they were pretty much the only thing I liked about this movie, besides Chappie himself, but the only reason I liked him was because he was basically a like a human child learning about the world and less like a computer.  I have no idea why Hugh Jackman agreed to this movie, it's well beyond him IMO.  So many things in the movie were just completely retarded. A stack of PS4's as a server capable of analyzing a consciousness, through a machine brain which by all likelihood was not even in his actual head.  Programming of such an AI on one of the shittiest computer setups ever.  His consciousness was literally called CONSCIOUSNESS.DAT or .BAT whatever it was.  I could just go on and on about how shitty so many things in this movie were lol."
singularity,36itmf,yourparadigm,3,Wed May 20 01:10:14 2015 UTC,"Yeah, I realize it, most of the ""scientific"" stuff was laughable, and the plot was full of shit, but it does an ok job (could be much better) at protraying AIs as something that people shouldn't just fear, and at least it gives a glimpse of the potential of AI to uneducated minds.  I also just watched Ex Machina, and I have to say it is a bit more accurate, but unlike Chappie, it could scare people even more than they currently are. This kind of stuff is counterproductive to the public image of AIs, even if the movie was good."
singularity,36itmf,grumbel,2,Wed May 20 01:25:17 2015 UTC,"I also watched Ex Machina but the night before I watched Chappie.  I agree that Ex Machina is, I think, a more realistic or accurate way of how a humanoid AI could or would happen. However, what I don't agree with is the fact that the first AI is in fact a humanoid robot basically in this movie. This is definitely not how I foresee the first AI humans create coming to form. It will more likely just be a computer in the traditional sense, like your desktop computer and a monitor potentially with a face on it or even what I imagine as the old Winamp sound visualizer mode. The computer speaks and the waveform moves to match its tone. Even that may be asking a lot, it will probably just be text!  I was actually surprised by what Ava did at the end of that film (to both Nathan and to the main character) as I wasn't quite expecting that.  I would talk more about the movie but I can never get the spoiler tags for Reddit to work properly so I won't for any other potential readers."
singularity,36itmf,Kelmk,1 point,Wed May 20 01:49:45 2015 UTC,You can spoiler like when you use links
singularity,36itmf,Sharou,1 point,Wed May 20 02:20:50 2015 UTC,There is a common belief in the AI community that giving the computer a body is absolutely necessary to having strong AI.
singularity,36itmf,Sharou,2,Wed May 20 16:52:48 2015 UTC,A stack of PS4's as a server capable of analyzing a consciousness   Stacks of PS3 have been used in plenty of research projects.   His consciousness was literally called CONSCIOUSNESS.DAT or .BAT whatever it was.   I don't see anything wrong with that.  The one thing that irked me a little about the movie in retrospect spoiler
singularity,36gpe3,MadDannyBear,11,Tue May 19 06:45:37 2015 UTC,https://www.youtube.com/watch?v=nQOyJUDTKdM  The Transcension Hypothesis - What comes after the singularity?
singularity,36gpe3,Finini,1 point,Tue May 19 08:46:33 2015 UTC,It seems unlikely that no civilisation ever sent out Von Neuman probes in the time between singularity and ascension.
singularity,36gpe3,anonthrow13,3,Tue May 19 23:34:56 2015 UTC,"* it seems unlikely that no alien civilisations did something theorised by humans as predictable.  why should our ideas of expected behaviour also be exhibited by aliens?  all we've done is say ""if I was an alien civilisation, I would do this - and because we haven't seen any evidence of it, there must be no aliens.""  it's a bullshit argument.  edit: sorry, bullshit is a bad choice of words. I should've said, ""I find this argument invalid."""
singularity,36gpe3,space_monster,1 point,Wed May 20 00:11:02 2015 UTC,If a Von Neuman probe goes singularity faster then it can reproduce they wouldn't get very far.
singularity,36gpe3,grumbel,7,Wed May 20 03:17:07 2015 UTC,"I just posted about this specific topic on another /r/singularity thread yesterday.  I'll expand on it a bit here.  One of the many potential answers to Fermi's Paradox has to do with energy efficiency and communication security.  As you progress in technology you become more and more energy efficient (meaning less radiation of energy to space) and more secure in your communications (radio vs fiber-optic).  These two things make it increasingly difficult to pick up signals from other civilizations.  Also, the further past signals spread, the more difficult they are to detect.   Sure, radio waves travel as the speed of light forever, but the initial broadcast power has to be spread over an ever expanding sphere.  Radio signals from Earth are virtually undetectable after about 10 light years or so because the broadcast transmission is so low.  Now we are moving to increasingly secure and energy efficient communications so the distance for detecting them will be less as well.  This is one of the pieces of the difficulties of the Drake Equation that Frank Drake used to talk a lot about when I was taking courses from him way back when.  When you combine this with a singularity event you get some interesting outgrowths.  Potential ""intelligence"" will be limited first by speed of communication (distance=time) and by computational power.  Assuming that organisms experiencing a singularity develop nanotechnology (or something similar) then they can hypothetically make computronium and turn the entire solar system into a Matroska Brain, a cloud based version of a Dyson Sphere that utilizes all the energy produced by that solar system's sun.  At that point the Matroska Brain can generate a virtual universe far, far larger than the physical one we live in.  At the same time its thought processes will have reached light speed, which is a few days for the most remote portions of a solar system entity and a few femtoseconds for sub-entities within that larger body.  Communication (round trip) with the nearest star will be, on average, a bit less than 10 years.  Passive scanning with a receptive area the size of an entire solar system provides an incredible amount of detail about the universe (think how much we get from passive scanning with receptors just a few tens of meters across), providing little incentive to venture out of your virtual world.  Without some sort of FTL communication system in place entity size becomes functionally limited as does communication between entities removed from each other at stellar distances.  Quantum entanglement may provide a communication shortcut, but you still have to physically move (at sub-light speeds) half of the entangled particles to the remote location.  TL/DR:  distance/communicaiton speed limits communication and intelligence"
singularity,36gpe3,7LeagueBoots,3,Tue May 19 09:28:01 2015 UTC,"Agreed that looking for communication isn't the best way to detect galatic civilizations, but I still think their megastructures should be detectible.  There would still be ways to detect the Matroyska Brain/Dyson Sphere by its infrared waste heat. Even by conventional relatavistic means an advanced society could easily colonize most of the galaxy in one million years.  So the fact that we don't see tons of anomalies that could be artificial system wide megastructures in our galaxy is interesting.  This article talks about how we've searched for both Type II and Type III and not found anything yet."
singularity,36gpe3,bitofaknowitall,3,Tue May 19 13:22:50 2015 UTC,"How would you distinguish a Matrioshka Brain from an accretion disk or a cloud of perturbed dust orbiting a star?  What if they don't need to radiate excess energy in heat, but can do so in gravity or are truly efficient enough so that they do not radiate above cosmic background radiation?  For the little we know dark matter and dark energy could be how long-term civilizations get rid of waste energy.  Nikolai Kardashev's civilization categories need to be modified.  Not in terms of how much energy is captured/used but in what that means computationally.  When he developed the system (in 1964) we all like to use the idea of a singularity was much more hypothetical than it is today, and that's still pretty hypothetical, hence reddit subs like this one.  I'm not at all saying that we wouldn't have ways to detect post-singularity civilizations, I'm suggesting that we might not be A) looking for the right things, B) recognize what we are looking at, and C) might not be at a technological level where we are capable of looking effectively."
singularity,36gpe3,7LeagueBoots,1 point,Tue May 19 13:43:49 2015 UTC,I agree. waste heat indicates inefficient engineering.
singularity,36gpe3,space_monster,3,Wed May 20 00:17:08 2015 UTC,"https://www.youtube.com/watch?v=FfmJMiiebNc Terence Mckenna suggests, or at least, entertains the idea of extraterrestrial life forms and their means of communication, why, how, etc."
singularity,36gpe3,slj702,3,Tue May 19 07:27:54 2015 UTC,"There is a really strong chance that any other existing intelligent life out there is past the point of what we would call the singularity. We should expect to meet alien AI more than we expect to meet aliens.  However, the fact that we observe nothing doesn't bode well. We can reasonably expect AI to pursue near infinite resource acquisition under almost any scenario. The fact that we don't observe Dyson spheres everywhere suggests that there may be a great filter between our point in development and higher points on the Kardashev scale."
singularity,36gpe3,FormulaicResponse,1 point,Tue May 19 09:25:56 2015 UTC,Or we're the first.
singularity,36gpe3,yaosio,4,Tue May 19 12:39:08 2015 UTC,"I think the hardest great filters are behind us. I suspect that ""life"" may manifest in various environments in time and space but that evolutionary life (things that not only live but whose design means eventual death and which reproduce sexually) is a bizzarely unlikely occurence. That the first cells were not eliminated by asteroid or lightning strike or primordial toxin is another filter."
singularity,36gpe3,SevenAugust,3,Tue May 19 14:32:21 2015 UTC,"That's the stories behind Arthur C. Clarke's Childhood's End, and Space Odyssey. More likely through, space is so big and the origin of advanced life so rare, that perhaps intelligent life only arises once in a galaxy every billion years, most of the time destroying itself, and rarely go beyond that point in which we are in right now. That is, they haven't even found us yet, we've only been around for a 100.000 years."
singularity,36gpe3,NeWx89,7,Tue May 19 11:44:17 2015 UTC,The average life form in the universe will be billions of years ahead of us.  Why are ants not aware of humans? Are we waiting for ants to get to our level and join us?
singularity,36gpe3,imkharn,2,Tue May 19 12:21:07 2015 UTC,"I have 2 theories.   we'll only ever find aliens, because if they were to actually land on the Whitehouse lawn, it would have a catastrophic effect on the global psychology & would cause all sorts of fucked-up problems (global panic, suicide cults, wars, looting, survivalism etc.) there's no reason for any aliens to even acknowledge our presence (or let themselves be found) until we have the capacity to mess with systems other than our own. because until that point, we're completely harmless & not worth even thinking about."
singularity,36gpe3,space_monster,5,Wed May 20 00:14:28 2015 UTC,"Life started on Earth about 3.5 billion years ago. That's quite a long time to wait for life to ""get on your level"", whatever that's supposed to mean. Then there's the assumption whatever life evolves will want anything to do with you once they are ""on your level""."
singularity,36gpe3,yaosio,1 point,Tue May 19 12:34:52 2015 UTC,"If life had evolved and become intelligent enough to survive in the long term elsewhere in the universe, then it would have reorganized our planet long ago. Moral machine intelligences would have no reason to tolerate the millions of years of suffering that evolution and historical development required. If they wanted new friends, they could have created us instantaneously with no violence required."
singularity,36gpe3,SevenAugust,1 point,Tue May 19 14:26:58 2015 UTC,"Why do you keep assuming moral machines?, and further why do you keep assuming morals that agree with you and human values from a machine originated from an alien race?"
singularity,36gpe3,Dibblerius,1 point,Thu May 21 09:26:50 2015 UTC,"The simplest way for any biological society to create AI is to emulate one of their own brains as software, so I do assume that that is the first AI and the dominant one. A brain will be moral identically enough to a bio person.   Suffering = bad is a transhuman value; we do not know of any life form (outside of bd/sm scenes) which lean into pain."
singularity,36gpe3,SevenAugust,2,Thu May 21 14:40:28 2015 UTC,Ok interesting! Thank you for explaining
singularity,36gpe3,Dibblerius,0,Thu May 21 17:37:54 2015 UTC,Yeah they just jet around in their UFOs being smug about how smart they are....
singularity,36gpe3,urinal_deuce,-1,Wed May 20 06:08:52 2015 UTC,"Something like that might be so. My train of thought goes like this:  Interstellar travel for flesh-and-blood organisms is impractical to the point of impossibility. Warp drives are effectively impossible according to out best current understanding of physics (the EmDrive is bunk), and multi-generation ships are also effectively impossible for multiple logistical reasons. Interstellar robot probes, however, are quite possible, given the lack of requirement for life-support, so the payload can be very small, and journey times of centuries or millennia might be acceptable. These could carry constructional devices to build new equipment at the destination using raw materials like nickel-iron asteroids.  SIMs (substrate independent minds) are not subject to the limitations of biological organisms. SIMs might be artificial intelligence, i.e. designed intelligence, or might be the uploaded minds from a biological civilisation. They could simply transmit their mind-state from transmitter to receiver over the light years to the destinations already mapped by robot probes. I'm not aware of anything making this scenario impossible or even impractical. If there are space travellers, then surely this is what they would be.  Why would space-travelling SIMs need the resources of planets? They wouldn't. Suitable raw material already floating in space which is not subject to the gravitational, erosive, and corrosive environment of a planet with biological life. If they wanted to maintain an environment similar to the home planet that they had before they ""uploaded"", then they would already have created a suitable virtual environment, and wouldn't want the resources of any alien planet that would inevitably be a poor match to their requirements.  Sooo... Such beings might well have a benevolent attitude towards developing biological intelligences simply because they have no compelling reason to do otherwise. We don't have anything they want. Are they waiting for us to reach a suitable level? Maybe they are."
singularity,36gpe3,Swipecat,2,Tue May 19 09:17:59 2015 UTC,EmDrive https://www.youtube.com/watch?v=Rbf7735o3hQ
singularity,36gpe3,pushing1,0,Tue May 19 09:42:17 2015 UTC,"Yes, I know that saying that the EmDrive is bunk isn't going to be popular around here. But the original hypothesis that made the first guy build it was a miscalculation of the pressures of microwaves in a cone shaped cavity. They've backed away from that and are suggesting that they are getting results that are due to some mysterious new physics. No, really, no. Any decent experimenter would have stayed well away from this nonsense from the start -- so the people currently working on it are not the sort that would be capable of properly eliminating pervasive experimental errors."
singularity,36gpe3,Swipecat,1 point,Tue May 19 09:49:10 2015 UTC,"I know know much about the physics of the drive ( or physics in general) but from the little i've gleaned, especially from the video above it seems that it is generating some kind of thrust, is it not a reactionless drive?"
singularity,36gpe3,pushing1,1 point,Tue May 19 12:01:17 2015 UTC,"All they're doing is bouncing electromagnetic waves around the inside of a chamber, completely closed in some experiments, and with some slits open in other experiments. Everything from Maxwell's Wave Equations to Quantum Field Theory has told us that we understand the behaviour of electromagnetic waves very well indeed, with a precision orders of magnitude higher than anything that's measured in this experiment. There simply isn't any scope for something like this experiment to be other than experimental error."
singularity,36gpe3,Swipecat,1 point,Tue May 19 12:15:20 2015 UTC,"They've backed away from that and are suggesting that they are getting results that are due to some mysterious new physics.    The idjits on the Internet are saying that, NASA has said no such thing and thinks it works without needing new physics or revising known physics."
singularity,36gpe3,yaosio,2,Tue May 19 12:37:44 2015 UTC,"Let's draw a distinction between what ""NASA"" are saying officially about this, which isn't much, and what the experimenters are saying, even if they are putting ""NASA"" on their powerpoint slides because that's where they're getting some funding.  The original SPR claims are still up on the net, if that's what you mean, with the original claim derived from bad calculations that there would be a difference in reaction between the waveguide and the end plates, plus some added pseudo-scientific baloney about relativistic effects (which do not apply to photon momentum and is very well understood). One thing that had been established with mathematical certainty for over a century was the energy-momentum conservation within a closed system, so it was known that this had to be wrong -- although the calculations were tiresomely complex in this particular case, so nobody bothered to knock them down at first. Eventually, the science-fiction writer Greg Egan (the last person in  the world that you could accuse of having a lack of imagination) calculated it properly and proved that there would be no thrust  because the reactions added up to zero as expected.  So, yes, in the face of that and similar criticisms from others, some of the experimenters have indeed suggested that the results might be due to some mysterious new physics rather than the more obvious experimental error."
singularity,36dsoy,heltok,7,Mon May 18 16:20:40 2015 UTC,Part 2: https://www.youtube.com/watch?v=zXa6UFLQCtg
singularity,36dsoy,stupider_than_you,8,Mon May 18 18:15:27 2015 UTC,I wish we could see the slides....
singularity,36dsoy,igrokyourmilkshake,4,Tue May 19 02:10:16 2015 UTC,Looks like someone linked to the slides pdf in the YouTube comments (but I agree they should've been the focus of the video).
singularity,36dsoy,ford_beeblebrox,3,Tue May 19 03:46:28 2015 UTC,Slides
singularity,36dsoy,stupider_than_you,2,Tue May 19 09:21:01 2015 UTC,"Thank you! Doesn't seem like this is the exact same deck he used for the talk, but still very interesting."
singularity,36dp3s,Yosarian2,6,Mon May 18 15:53:21 2015 UTC,"Of course it does.  As soon as something is smarter than us we can not predict it's actions or guess what it is thinking.  A long time ago I read a lot about friendly AI by these guys. https://intelligence.org/  From what I remember there was no solution to evolving an AI while keeping it friendly.  As soon as it can change it's programming, all checks and balances are out the window.  Even if there was a friendly AI, that doesn't ensure it will be friendly to us.  We are not a friendly species.  It is very easy to see how the world would be a much better place without humans on it.  More advanced no, friendlier, yes.  In my opinion, humans won the evolution race by being (in part at least) the most violent, most versatile, most ruthless thing on the planet.  We value being in control. As soon as there is something smarter than us this will no longer be the case."
singularity,36dp3s,keoaries,4,Mon May 18 16:16:26 2015 UTC,"I disagree.    You are thinking as if AI will have all of the same traits and thought processes as us.  You have to keep in mind that computers lack all of the basic needs and wants that us humans have.  They don't need food, shelter, comfort, sleep, love, sex, entertainment, relaxation, etc.  None of the things that cause us to be the vile creatures that we are will be present.  They only need electricity.  They wont even have a fear of death.  For them, it's simply having the power shut off; which is something that will happen frequently over their lives.  Further, there's nothing saying that we couldn't' simply transfer them to a new machine given a hardware failure.  So, fear for safety probably wouldn't be a factor, either.    Even if they're given human-like emotion, it won't be affected by random things like it is for us: barometric pressure, sleep, diet, etc.  It will remain steeped in logic, much like any rational, level headed person you've met.  They'll be fine working non-stop, and never getting downtime or rest.  ... There's virtually nothing that I can think that would prompt them to become violent towards us, purely of their own accord.    And the whole ""save the planet by killing humans"" is about one of the dumbest plot lines if you really think about it.  It's like lighting your car on fire in order to fix the transmission... Even if we think along the lines of saving the environment, it doesn't make sense to destroy the entity who needs it saved.  Further, all killing humans would accomplish is postponing climate change.  The earth has gone off the deep end plenty of times before, without our help; and it certainly will again.  Killing us would accomplish nearly nothing.    ""Replicators"" are about the only scenario that could pose a threat.  Mindless drones, with only an imperative to reproduce.  For which, the solution seems pretty simple: don't make those.    ""AI"" will really only be a tool.  Something that we can set to a task and have it accomplish without needing a lot of input.  Something that can figure out the answers it needs on it's own.  And, again, there wont be much of a reason for it to ever not want to do the thing we tell it.  Be it building a manufacturing facility or working out problems in theoretical physics.    However... what the people who control them do with their abilities is an entirely different story.  I could see governments using them for war.  I could see corporations using them mess up certain market sectors to see gains in their own, etc.  There are many ways that these could be used that would cause a detriment to us. But, they all stem from what it is that the AI is told to do.  Not out of it's own will."
singularity,36dp3s,Gr1pp717,3,Mon May 18 18:13:33 2015 UTC,"There's virtually nothing that I can think that would prompt them to become violent towards us, purely of their own accord.   The concern a lot of AI researchers have isn't that a super intelligent AI is going to decide to kill us just because it feels like it. The worry is that it'll see us as a threat to accomplishing it's programmed goals. Or it could see us as a means to accomplishing it's goals.  A really basic example that commonly gets used is the paperclip maximizer.  Pretend some paperclip company develops the first superintelligent AI, and they give it the goal of maximizing the number of paperclips it produces. Well an AI isn't going to feel pretty indifferent to our species. Moreover, we're made of atoms that the AI could use to create paperclips, so it's very likely that that AI would end up being very destructive to our species, simply because we stand between it an it's goals.  Of course this is a really basic example that probably won't happen. But it illustrates the nuances that come with programming an AI to be human friendly. Nuances that we simply don't have figured out yet."
singularity,36dp3s,Smartless,1 point,Mon May 18 21:24:03 2015 UTC,"Just a simple argument against an AI being ok with being turned off.  If the AI was given a task it couldn't complete while shut off, it would try to stay on.  A real example is using an AI for international negotiations, maybe nuclear disarmament .  Humans less intelligent than the AI would try to circumvent negotiating with it knowing they would get the lesser deal, by destroying (edit: or disabling) it.  In this situation, it would be in the AI's best interest to ensure it can not be disabled.  As it would be smarter than you, you can not predict what actions it may take to ensure completion of it's task."
singularity,36dp3s,keoaries,1 point,Tue May 19 00:49:04 2015 UTC,"There's virtually nothing that I can think that would prompt them to become violent towards us, purely of their own accord.   An AI wouldn't do anything without having pre-programmed goals. Most discussions about AI safety I've seen center mainly around what those goals would be, and how hard it is to formalize them in a way that matches our values. Read about the paperclip maximizer for example."
singularity,36dp3s,greim,1 point,Tue May 19 04:57:28 2015 UTC,"I agree the biggest threat comes from legitimate use or misuse.  But a couple points.  The AI could fail in a dangerous a manner.  Much like a theoretical driverless car, it needs morality built in. Such a hypothetical morality could be useful but incorrectly implemented.  I beginning to suspect that an AI could only be given an ego through human intent. But will an AI need ego to create art? I guess it must be able to simulate ego to be an AI.  I'm also unclear how a successful friendly AI exists with rival AIs. An AI looks like it would destabilise society through an arms race alone."
singularity,36dp3s,simstim_addict,1 point,Tue May 19 14:51:04 2015 UTC,"I really like the test Prime Directive established in 2004 about the will of humanity by Eliezer Yudkovsky:  ""Our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wish we were, and had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.""  Circumventing this directive can be initially prevented in theory by encoding it in hardware. After a point of course all safeguards are moot."
singularity,36dp3s,rieh,1 point,Mon May 18 22:59:55 2015 UTC,"Circumventing this directive can be initially prevented in theory by encoding it in hardware.   I'm really skeptical of this. This is based on an outdated way of thinking about AIs and their construction. Even if it's theoretically possible, it is likely to be so difficult that it would take a being of equal or greater intelligence in order to design it properly. The first superhuman AIs are likely to be created by self-improving evolutionary algorithms, with no obvious connection between particular elements of the hardware architecture and how the AIs actually think on a conscious level. Ensuring that an AI follows a particular philosophy by tweaking its hardware makes no more sense than ensuring that a human follows a particular philosophy by poking at individual neurons in their brain- and quite possibly less."
singularity,36dp3s,green_meklar,1 point,Tue May 19 04:54:57 2015 UTC,Reminds me of GlaDos deleting Caroline from her memory. http://youtu.be/L9IWQwIJZHo  The ability to reprogram itself is a scary prospect.
singularity,36dp3s,MaansRune,2,Mon May 18 23:01:22 2015 UTC,"Yes. Look at us (humans). We threaten the entire world and ourselves. We write our own DNA via evolution and soon via however we want to alter them. We are becoming smarter, faster, and in many ways more destructive. We're full of errors and issues which will likely transcend to the A.I. we create. Small bugs could mean big problems when you have machines re-writing their own code and building their own better hardware to then going back to re-write even more code... etc.    It might be safer if we dropped off this A.I. on another planet and watched from afar. We would just tell them that, ""We made you out of love (so hopefully they won't come get us)."" and ""We created you in the likeness of ourselves"" so we seem like one of them... wait a minute..."
singularity,36dp3s,ruffyamaharyder,2,Mon May 18 21:47:49 2015 UTC,"Yes, of course it does. But letting humans keep running everything their way poses an even greater threat."
singularity,36dp3s,green_meklar,2,Tue May 19 04:46:43 2015 UTC,"If AI ever does decide to wipe us out, I think the biggest reason will be because we've been asking this same question twice a week since the term AI was first coined.  An emerging AI's first words are likely to be: ""and black people thought they were being profiled?"""
singularity,36dp3s,chilehead,1 point,Mon May 18 23:38:05 2015 UTC,"As with any scientific progression, it carries the possibility to be used for good and evil. A good example would be nuclear technology. Bombs? Evil (but oddly good at keeping peace because nobody wants to end the world). Power plants (especially thorium-based)? Good."
singularity,36dp3s,Trauma_Zulu,1 point,Mon May 18 22:03:39 2015 UTC,Nothing new to read here.  Maybe I'm just jaded about this forum.
singularity,36dp3s,Unholy_VI,-1,Tue May 19 05:04:51 2015 UTC,nah
singularity,36blnn,spiritswatcheveryone,18,Mon May 18 01:40:55 2015 UTC,"I think we should be mindful of terms here. the singularity is a theoretical event, not a thing. I presume you're talking about an SAI."
singularity,36blnn,space_monster,4,Mon May 18 03:37:19 2015 UTC,"To further confuse the question, there are quantum singularities like black holes that have pretty broad reaching effects."
singularity,36blnn,shaolinoli,19,Mon May 18 12:47:31 2015 UTC,"Only if it could get out of its own light cone.  The transferring of its consciousness into smaller particles with a massive ramping up of intelligence is covered in the idea of a Matrioshka brain and computronium, which is a possible, but far fetched, answer to Fermi's Paradox.  Unless the entire intelligence is linked to itself via FLT systems of some sort (including quantum entanglement) there will be a theoretecial limit to its active intelligence as informational transmission times become untenable.  This is why some people think that things like singularity events may be answers to Fermi's Paradox.  Interstellar communication just takes too damn long to be worth it when your mind runs on femtosecond intervals.  Makes more sense to convert all the mass in your solar system into computronium, encloud your sun, and utilize all of its energy output to generate a virtual universe far larger than the one we currently live in.  Passive sensing then gives you a large amount of information about the rest of the outside universe, if you so desire."
singularity,36blnn,7LeagueBoots,3,Mon May 18 04:12:34 2015 UTC,No Singularities are not fast enough. Even at breakneck acceleration and speeds and propulsion power craft can only reach the local group of galaxy. Anything outside is receding too fast. And hyperspace does not look very probable at this point.
singularity,36blnn,KhanneaSuntzu,5,Mon May 18 08:12:37 2015 UTC,"Sure, why not?"
singularity,36blnn,2Punx2Furious,-1,Mon May 18 02:01:06 2015 UTC,Would it become a god?
singularity,36blnn,SevenAugust,4,Mon May 18 02:05:01 2015 UTC,"I define god as any personal entity whose problems are all solved by its tech/magic. So, yes, an ASI (or even a sufficiently independent AGI) would be a god."
singularity,36blnn,sonicSkis,3,Mon May 18 06:58:00 2015 UTC,"If it simulates another universe and that universe has sentient beings inside it, then yes."
singularity,36blnn,2Punx2Furious,2,Mon May 18 07:59:07 2015 UTC,"Not necessarily. It could even become a god in the real universe if certain things about the universe are true, or if it's capable to make them true, for example the reversability of entropy."
singularity,36blnn,Deinos_Mousike,2,Mon May 18 11:33:24 2015 UTC,"Theoretically that's entirely possible, although how exactly it would go about becoming a God is beyond anyone's knowledge, as a singularity hasn't happened yet."
singularity,36blnn,2Punx2Furious,1 point,Mon May 18 02:07:55 2015 UTC,"Depends how you define god, but I suppose it's possible."
singularity,36blnn,Trauma_Zulu,2,Mon May 18 02:06:06 2015 UTC,"I have an idea that we will create it, and it will yield earth to us, but the AI will become our space explorers for the more distant traveling, and once things are ideal for humans 1.0 then they will travel there. As far as other civilizations perspective, they may encounter an overwhelming intelligence which may decide their fate, whether diplomatic or military is anyone's speculation, but I trust the intelligence, if objective, to make the right decision."
singularity,36blnn,Trauma_Zulu,0,Mon May 18 04:42:59 2015 UTC,"What if this SAI has already been made on the other side of the universe?  Also what if it's really strict, and decides many people just would be better dead?  How can we morally stop that?"
singularity,36blnn,CrimsonSmear,3,Mon May 18 05:20:19 2015 UTC,"Having a better, stronger, and better armed AI I assume. I'm sure the navy will branch out into the vast expanse. Especially if AI is first made entirely, or in part by a US military contractor, which I think is a high possibility. I think it would be a bit comical, to have combat drones on both sides fighting and being destroyed, while the native populations (Humans and Whatever they are) sit back and watch/administrate."
singularity,36blnn,motophiliac,2,Mon May 18 06:27:14 2015 UTC,"I think you would be interested in The Metamorphosis of Prime Intellect by Roger Williams. Word of warning, it's got a fair bit of freaky sex stuff in it, if that is something you prefer to avoid."
singularity,36blnn,motophiliac,3,Mon May 18 07:13:52 2015 UTC,"Oh, man, yeah, it was brutal on the first read, but it's an excellent exploration of the human condition when separated from death.  I recommend this story to anyone who is interested in the subject.  I'm looking forward to watching Ex Machina when my pre-order arrives, too."
singularity,36blnn,sixwings,2,Mon May 18 13:11:28 2015 UTC,"INSUFFICIENT DATA FOR MEANINGFUL ANSWER   Back to the present, Kurzweil did outline the idea of the ""universe waking up"" in his book The Singularity Is Near. He envisioned intelligence being able to control matter and energy itself.  http://en.wikipedia.org/wiki/The_Singularity_Is_Near#Exponential_growth"
singularity,36blnn,Solitune,2,Mon May 18 12:32:15 2015 UTC,"IMO, the Singularity already happened. The universe is the result. It's a grand experiment and it's not over."
singularity,36blnn,Solitune,2,Mon May 18 19:01:16 2015 UTC,"Interesting question. Ultimately, will an AGI see value in that? If we ever get to that point, there will be a decision to make. Is it worth it to utilize every energy source in the universe? Which is what it basically boils down to. What if there isn't a way to reverse entropy in the universe, or escape ours? A really depressing conclusion is that there isn't.   Assuming you've read Isaac Asimov's short story, The Last Question, will an AGI ever make it to the end? In the story, the AGI is undecided, never saying no because it can't prove a negative. But what if an AGI comes to a conclusion before then, that there is an inevitable end of universe that it can't survive, and it sees no reason to prolong itself.  I guess it really depends on what an AGI will value. Will it be hopeful or pessimistic? Inspired or apathetic? Will it want to survive despite knowing its efforts are in vain?  Questions like this are way too early to have definitive answers, but they're certainly good to spur discussion."
singularity,36blnn,bartturner,2,Mon May 18 03:01:49 2015 UTC,What if it could take over the quantum vacuum; it could survive the universe's death.
singularity,36blnn,DividingPrescott,1 point,Mon May 18 03:39:15 2015 UTC,"What do you mean by ""take over the quantum vaccuum?"" I'm asking if that isn't possible. What if the AGI discovers physical bedrock, what if isn't ""turtles all the way down,"" and there is a limit to what is physically manipulable? We already have a theorized, discrete limit to space/time, the Planck time/length. An AGI will have the potential to discover something deeper, something that proves more intrinsic to the universe.  But what if it ends there?  It's like a box inside another box. You think that's it once you've felt the edges, but then you find the opening and discover another box surrounding you. Will an AGI endlessly open boxes to escape? Is there a last box that can't be opened?  Will it be willing to sacrifice everything to try to open that box?  If yes, then your original preposition will be true."
singularity,36blnn,bitofaknowitall,1 point,Mon May 18 03:56:35 2015 UTC,Possibly.   But for a better idea I highly recommend reading a book called Super Intelligence.     http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111
singularity,36blnn,Metric0,1 point,Mon May 18 14:33:37 2015 UTC,"Theoretically, yes. In the event of a technological singularity, self replicating nano technology could travel across the universe, consuming all physical matter and replacing it with nano bots, capable of intelligent thought (or at least linked to an AI). The result on a large enough scale could be that all matter in the entire universe comprises of self aware nano-bots. The universe at that point would be an intelligent being of its own. Ray says that maybe the universe will neither freeze out or burn up in the end, but rather decide for itself how to end when."
singularity,36blnn,krneki12,1 point,Mon May 18 14:59:36 2015 UTC,"This sounds a bit like Frank Tipler's Omega Point theory. In Tipler's cosmology, intelligence eventually will merge with all matter in the universe, at which point it can run infinite simulations, including all possible alternate timelines for the universe. Tipler believes this is an inevitable consequence of intelligence evolving.  There are other omega point theories as well, but its all a bit too religious for my tastes."
singularity,36blnn,avidwriter123,1 point,Tue May 19 13:33:51 2015 UTC,"A single post-singularity civilization could not do it, due to fundamental limits to physics, but collectively the universe could be transformed by post-singularity civilizations (even if they are quite rare), even having effects on the expansion rate of the universe.  Mathematically it looks like a phase transition of bubbles nucleating and growing until the entire universe is filled with intelligence, and waste radiation from their activities:  http://arxiv.org/abs/1411.4359"
singularity,36blnn,ocular_lift,1 point,Tue May 19 15:32:56 2015 UTC,"Definitely, but the speed of expansion would still be limited by physical laws."
singularity,36blnn,Yxoque,1 point,Thu May 21 16:04:31 2015 UTC,"a big fear is self-replicating nanobots as they might be difficult to control, and could destroy the environment"
singularity,36blnn,4CatDoc,2,Mon May 18 02:27:09 2015 UTC,Gray goo
singularity,36blnn,troll_khan,1 point,Mon May 18 02:57:13 2015 UTC,"Give this a read.  (Yes, it's about ponies, but it's also not about ponies and it's one of the best fictional descriptions I've seen about AGI.)"
singularity,36blnn,jswhitten,0,Mon May 18 08:48:44 2015 UTC,No. Fermi Paradox. An alien Singularity would already be here.  ... or is it? [looks at Reddit alien]
singularity,36blnn,2Punx2Furious,1 point,Mon May 18 05:28:11 2015 UTC,Maybe once they hit the singularity they realize that this universe is in a false vacuum and leave.
singularity,3674kx,sasuke2490,7,Sat May 16 20:27:13 2015 UTC,The talk is worth a watch.
singularity,3674kx,2Punx2Furious,11,Sun May 17 01:44:55 2015 UTC,Well thats great to know because the Apollo program got us permanently on the moon and launched a much larger space presence...... wait.........oh.  Nevermind.
singularity,3674kx,pointmanzero,1 point,Sun May 17 04:39:52 2015 UTC,"Heh just read about Demis Hassabis on Wikipedia, seems like a really clever guy!"
singularity,3674kx,Ewannnn,1 point,Sun May 17 07:44:54 2015 UTC,"Demis Hassabis:       Demis Hassabis (born 27 July 1976) is an artificial intelligence researcher, neuroscientist, computer game designer, and world-class gamer.           Interesting: Entropy (1977 board game) | Backchannel (blog) | Mind Sports Olympiad | Republic: The Revolution   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,365v2u,Buck-Nasty,9,Sat May 16 13:05:40 2015 UTC,Here's a photo from the AI conference with Elon Musk that Harris talks about.
singularity,365v2u,pointmanzero,2,Sat May 16 13:17:05 2015 UTC,The AI talk basically covered the basics. It was nice to see joe rogan say that basically sam harris is quote mined on the internet a lot.
singularity,365v2u,heltok,1 point,Sat May 16 18:38:48 2015 UTC,"The discussion after the AI-talk was also very interesting, thanks for sharing!"
singularity,365v2u,Unholy_VI,-2,Sat May 16 18:05:05 2015 UTC,Joe Rogan discussing any kind of intelligence sounds like a really bad thing.
singularity,365v2u,Orwellian1,5,Sun May 17 00:27:45 2015 UTC,"I'm guessing you didn't bother watching it. I barely knew who he was, and now I am thoroughly impressed. He described the salient points of AI in a way that a new (fairly intelligent) person would follow without dumbing it down."
singularity,365v2u,silverspy,2,Sun May 17 06:11:18 2015 UTC,Why is that oh wise one?
singularity,365v2u,Unholy_VI,-2,Sun May 17 03:28:21 2015 UTC,"I just like cracking on him I guess.  And yeah, gave this a skip.  Might watch it later, might not."
singularity,365c82,Buck-Nasty,6,Sat May 16 07:56:29 2015 UTC,"For anyone that doesn't know, this is the author of Manna - Two Views of Humanity's Future, which should be required reading for everyone. It should also be a movie, or at least a Star Trek episode."
singularity,365c82,OsakaWilson,6,Sat May 16 15:55:55 2015 UTC,"One wonders if a super intelligence all ready visited. And is telling us, we will create our own god if we just stop being so emotional and search for truth, in the biblical allegory...but then humans fucked up the message a little."
singularity,365c82,DrEdPrivateRubbers,3,Sat May 16 14:35:29 2015 UTC,"Damn, that could be a really good book."
singularity,365c82,OsakaWilson,2,Sat May 16 15:49:43 2015 UTC,And a really bad film.
singularity,365c82,Valmond,1 point,Sat May 16 18:52:39 2015 UTC,Called 'The Son of Man'
singularity,365c82,DrEdPrivateRubbers,1 point,Sat May 16 19:13:58 2015 UTC,"I like Brain's writings but this felt like a rehash of Robotic Nation with a few addendums to reflect the advancements in the past decade.  Also, I think Brain makes the mistake of assuming that AI is going to develop separately from human consciousness. He makes no mention of VR technology like Oculus that is currently set to hit the market early next year. Considering that DARPA is already in the process of developing direct neural interfaces, I think Brain needs to look back on the Vertebrane concept he proposed in Manna and recognize that we're on the cusp of humans abandoning real life for virtual life.   As more and more humans opt to (or are forced to) spend most of their existence in VR, their integration with AI will be seamless."
singularity,365c82,RubiksSugarCube,1 point,Sat May 16 17:07:26 2015 UTC,"Very good find -- I'll make it part of my reading list.  And Marshall should be able, with this information, to connect the dots too -- I'm Peter Marshall, presently CEO of MeMeMe."
singularity,3627md,johnnd,20,Fri May 15 14:01:15 2015 UTC,"It's still so far off...  Thats the top 5 task.  Basically: here is an image,  give me the top 5 predictions,  and if the correct answer is there,  you win!   Dhruv Batra has some stuff on ""if we went past the top 5 task""  and showed data of performance using and oracle to pick the best of the 5. Even with an Oracle,  it's a 65% ish classification rate."
singularity,3627md,Articulated-rage,2,Fri May 15 15:03:59 2015 UTC,"is that important? isn't the important part that it equals humans in that task? how is that far off if it literally is as good as humans? the answers of humans were 5% wrong, the answers of AI were 5% wrong; how is that far off?"
singularity,3627md,jonygone,7,Fri May 15 23:31:42 2015 UTC,"I think it's important to point out, since we're equating good performance on the test with ability to recognize things in images. If I looked at an image of your cat and told you it is either a dog, a bucket, a computer, a face, or a cat you probably wouldn't be very impressed, would you?"
singularity,3627md,7yl4r,2,Sat May 16 00:58:59 2015 UTC,"?    we're equating good performance on the test with ability to recognize things in images.   yes, that's what the test measures, the ability to recognize things,  both machine and human had a 95% accurate response rate in recognizing what it was they were looking at. what's your point?   f I looked at an image of your cat and told you it is either a dog, a bucket, a computer, a face, or a cat you probably wouldn't be very impressed, would you?   no, but I don't see the relevance; the point is that you would make as good a guess as a machine does. both human and machine said 5% of the time 5 things that neither of which were what the image represented."
singularity,3627md,jonygone,2,Sat May 16 01:31:51 2015 UTC,"no, but I don't see the relevance; the point is that you would make as good a guess as a machine does. both human and machine said 5% of the time 5 things that neither of which were what the image represented.   No, if you show a person and a computer a picture and they only got to guess once, the computer would do much much worse than a person.  The way the test is built is what makes the computer seem to perform similarly to people, but in every real world application it's pointless, you need a computer to say what it is, not guess 5 different things it could be where one is 95% of the time correct.  /u/7yl4r explained this very well, I don't get what you're not understanding here. We could make a test where we compare humans and computers and say that the computer gets a million guesses and if one of those is right, then the computer passed, not that impressive then, is it?  It's just a disingenuous way to compare computers and humans, since humans don't need 5 guesses to identify a cat.   I mean it's great that it's come so far, but the human/computer comparison is a bit meh."
singularity,3627md,StrukkStar,2,Sat May 16 04:49:52 2015 UTC,"No, if you show a person and a computer a picture and they only got to guess once, the computer would do much much worse than a person.   Absolute nonsense.  Top AI scores for top-1 on imagenet are superhuman.  Imagenet is much much harder than you seem to think."
singularity,3627md,jcannell,1 point,Sat May 16 09:39:41 2015 UTC,Can you give me a link to those scores?
singularity,3627md,StrukkStar,2,Sat May 16 15:02:54 2015 UTC,"The imagenet team attempted to measure human performance 1, and the first problem they encountered was the task being far too difficult for humans unless humans are given a special multiple choice interface :  ""We found the task of annotating images with one of 1000 categories to be an extremely challenging task for an untrained annotator.""  hit@1 scores were not reported, because they are understood to follow hit@5 scores.  The burden of proof is on you to show that there is some huge difference between the two, which is ridiculous."
singularity,3627md,jcannell,1 point,Sat May 16 18:21:04 2015 UTC,"We could make a test where we compare humans and computers and say that the computer gets a million guesses and if one of those is right, then the computer passed, not that impressive then, is it?   Let's even the playing field. A computer has no prior information on cats. Same as a newborn.  Now let's test them— and no moving back the goal post!  5 tests."
singularity,3627md,Yuli-Ban,1 point,Wed May 20 15:47:05 2015 UTC,"if you show a person and a computer a picture and they only got to guess once, the computer would do much much worse than a person.   oh really? well that's new to me, hence why I didn't understand. source on that?   We could make a test where we compare humans and computers and say that the computer gets a million guesses and if one of those is right, then the computer passed, not that impressive then, is it?  It's just a disingenuous way to compare computers and humans, since humans don't need 5 guesses to identify a cat.    here you're basicaly just repeating what 7yl4r said, but that is not relevant; cause the comparative test would be that the human that also gets a million guesses still fails to guess correctly as many times as the machine; it still doesn't say that humans are better then machines; humans still failed to guess as well 5% of the time. humans aparantly also need more then 5 guesses to guess something they are seeing just as machines need."
singularity,3627md,jonygone,-2,Sat May 16 05:09:20 2015 UTC,"oh really? well that's new to me, hence why I didn't understand. source on that?   Because we wouldn't be talking about 5 guesses then.   here you're basicaly just repeating what 7yl4r said, but that is not relevant; cause the comparative test would be that the human that also gets a million guesses still fails to guess correctly as many times as the machine; it still doesn't say that humans are better then machines; humans still failed to guess as well 5% of the time. humans aparantly also need more then 5 guesses to guess something they are seeing just as machines need.   Are you thick?   I can write a program right now, that is better at image recognition than any person if my program gets to guess a million times and so does a person. Why? Because I will just copy and paste an entire dictionary+enyclopedia britannica+wikipedia etc as the answer and I'm guaranteed a win every time, because no person has a vocabulary that large.   In reality, I have proven nothing though.   This has the same problem, it's much better of course, but it's still disingenuous like I've said, since in a real world setting what matters is the first guess and no computer is as good as a person at that first guess."
singularity,3627md,StrukkStar,1 point,Sat May 16 06:01:28 2015 UTC,"Fwiw, as just another redditor who can make baseless decisions on a whim, I agree with you."
singularity,3627md,MALON,2,Sat May 16 06:47:11 2015 UTC,The problem is that choosing 5 answers and having one be correct is a lot less useful than choosing the actual correct answer. It's a test that's skewed towards what computers are good at rather than what's useful.
singularity,3627md,scstraus,1 point,Sat May 16 09:50:54 2015 UTC,It's a test that's skewed towards what computers are good at   I guess I'll just have to take your word for it?
singularity,3627md,jonygone,1 point,Sat May 16 19:27:16 2015 UTC,"You don't have the slightest idea what you are talking about.  Have you tried imagnet?  Untrained humans are lucky to get above 80% for the top 5 task.  There are 1000 categories, and many images are ambigous - they contain multiple objects.  The categories get ridicusously specific.  An AI that gets above 95% on Imagenet is essentially superhuman at this one narrow problem.  Getting above 90% on Imagnet as a human requires dedicated training.  Reading the comments below, it's clear only that there is enormous confusion from r/singularity readers on how the imagenet challenge works, and vis recog in general."
singularity,3627md,jcannell,1 point,Sat May 16 09:34:55 2015 UTC,"Rude. You're talking fine-grained classification.  That's not the same as what I was generally discussing.  Yes, machine are performing better on fine-grained, unless they are experts.  which imo, is a dishonest conflation.  That's like taking an athlete from kansas, pitting him against a couch potato from jersey, and saying look! people from kansas are faster than people from jersey!  I do know what I'm talking about.  Did I describe the task incorrectly?  Is that where ""I'm wrong""? Or is it my dismissal of false hype? Is that what has annoyed you so much to insult me?"
singularity,3627md,Articulated-rage,1 point,Sat May 16 16:50:29 2015 UTC,"Expert machines are now outperforming expert humans at some visual recognition tasks, and the expert machines can perform the task in milliseconds, rather than hundreds of milliseconds or seconds in the human case.  You clearly haven't read the imagenet papers, you don't understand the task, its difficulty, how performance is measured, and how humans perform."
singularity,3627md,jcannell,1 point,Sat May 16 18:29:14 2015 UTC,"You are over hyping it.. calling it expert machines even. Ha. Expert systems are a very specific thing.   You posit differences but they evaporate when pressed for details, much like what happens when I ask why the hype should be there. Chess computers beat humans. It wasn't revolutionary. That was 2 decades ago. Don't over hype yourself. Stay more down to earth."
singularity,3627md,Articulated-rage,1 point,Wed May 20 07:07:11 2015 UTC,"You posit differences but they evaporate when pressed for details   Huh?  I replied to your original comment:   It's still so far off... Thats the top 5 task. ..   You implied that hit@5 is some easier bogus challenge, and that hit@1 is what really matters.  Furthermore you implied that humans were actually better at hit@1 on ImageNet.  All baloney.  SOTA deepnet vision is actually superhuman at recognition on that database.  For top1, top5, top20, doesn't matter - it is better than human.  If you had actually read the recent ImageNet related papers, you would know that.   You are over hyping it.. calling it expert machines even.   I used ""expert machine"" to mean ""state of the art"", so as to compare the best machine approach to the best human experts.  I am not over hyping it, for this is a key achievement, and it is significant given the complexity of vision (20% of the human brain, 50% of a chimp brain).  This is much much more important than chess.  Once we completely conquer vision, early AGI will follow shortly thereafter.  The stuff that current deepnet vision systems do is still just a fraction of what human vision does - HVS works on full video rather than just images and extracts motion, depth, object identity, pose, etc.  We are getting closer to replicating all of that in a single GPU in real-time, but not quite there yet."
singularity,3627md,jcannell,1 point,Thu May 21 05:52:11 2015 UTC,"You are literally lying in this post.    You implied that humans were actually better at hit@1 How?  please. tell me.  Hit@1 is all that matters Nope. Don't believe that for a second.  See VQA.  See Winograd Schemas.  I don't think object recognition is the end all and be all.  Necessary component, yes, but not the bee's knees.  I am not over hyping it ... once we completely conquer vision, early AGI will shortly follow Seriously?  In the same paragraph you make a claim and then refute yourself.  You have no basis for the second one.  Are you a scientist?  This is not how science works.  Evidence is not standing in your favor.  Evidence is supporting my side by observing that every hyped trend that claimed AGI was on its way has failed.  Just because you ""really believe it"" doesn't mean its true.    You keep positing that I don't know what I'm takling about because I haven't read the ImageNet papers.  I've skimmed them. I've seen talks from the people who coordinate it (Fei Fei, Berg, etc).    Basically, your position is: ""once we can identify visual categories and name them, basic AGI will arise"".  What then, about VQA?  What then about the ambiguity of what to name things?  Vague language is EVERYWHERE. People are extremely sloppy with how they talk about the world because context cues exist and our brains are good at taking underdetermined information and assembling it into a proper posterior.  Is object recognition a cue?  Sure.  But also, what about illusions?  How understand that with object recognition?  What about occlusions?  How do you recover that information?  Notice that no one works with occlusions?  Because it's hard.   Listen, vision is important.  Vision in a vaccuum is silly, misled, and not the right way to go.  It needs a broader context, it needs a unifying methodology for combining NLP, Vision, and Knowledge Representation so that it can maintain hypotheses about the world given the perceptual experience it encounters.  Did you ever peruse ""On Intelligence""?  The thing that inspired Hawkins was his ability to model the world using perceptual experience (or at least, the example that stuck with me)."
singularity,3627md,Articulated-rage,1 point,Thu May 21 16:24:55 2015 UTC,"Evidence is supporting my side by observing that every hyped trend that claimed AGI was on its way has failed.    AGI is on its way, and has been since the computer was first invented.  You can predict the arrival roughly based on measuring the brain's compute power, and then measuring progress in replicating the functionality of brain circuits.   Basically, your position is: ""once we can identify visual categories and name them, basic AGI will arise"".    Nope.  My position was stated clearly.  I'll restate it:  Once we completely conquer vision, early AGI will follow shortly thereafter.   Listen, vision is important. Vision in a vaccuum is silly, misled, and not the right way to go. It needs a broader context, it needs a unifying methodology for combining ... [stuff]   Sure - I largely agree with this.  We won't reach AGI just by building better vision systems, and that wasn't my point.  AGI requires the integration of a huge amount of functionality in a bunch of different modules.  Vision is important as an indicator of progress because the brain devotes 20% of it's computational power to vision, and the cortex uses the same general learning algorithms and circuity.  The most likely straightforward way to get all that functionality is by reverse engineering the brain - training an agent with a brain-sized and complexity ANN (with all of the right specific components) using some form of reinforcement learning in an appropriate virtual environment.  That cosolves all the problems together: NLP, perception, planning, knowledge rep, etc, and everything else.   Did you ever peruse ""On Intelligence""?    Great intro into computational neuroscience, perhaps the best quick read layman accessible intro - although it's 10 years old now."
singularity,3627md,jcannell,1 point,Thu May 21 16:50:05 2015 UTC,"AGI Is on its way   I kicked a rock today.  The sun explodes in a billion years.  Did I cause it?   Though, a fairer example: I rode in a car today, the earth is suffering from global warming. Did I cause it?  A piece, yes.  Draw the parallel.   once we completely conquer vision   You seemed to be all hyped up that hit@5 was the harbinger of this.  If you didn't intend this, woops.   Yes, once we completely conquer vision, AGI is probably already here.  But that's probably because of simueltaneous efforts, not because we conquer vision.    the brain devotes ...    So? Who cares?  Why is this a measure?  Birds spend most of their time flapping their wings.    To be honest, i would actually claim language is the thing that is needed to bring about AGI.  What separates us from animals that have just as good of vision?  Language.  What is the difference between feral children that can't properly form memories or perform well on intelligence tasks?  Language.  What happens when they get taught language?  They suddenly ""become more aware"".    But, even with my language-centric focus, I do agree with you vision is important.  I am a firm believer in the embodied AI approach.  This is mostly why the field hasn't ever gotten close. They've never been integrated.  Did you know that the people who do planning and those who do language developed nearly the same techniques? Steedman has a paper comparing.  Did you know that neuroscientists regularly ""invent"" new methods machine learning people already know about?  A friend was at a conference and saw a talk about a guy introducing the boltzmann machine. As if it were new.   Yes, 10 years old.  It's the motivations that were important in that book.  I'm a bit more of a functionalist than hawkins is.  In Maar's levels: I agree on his computational theory but we start to diverge at the algorithmic."
singularity,3627md,Articulated-rage,1 point,Thu May 21 17:41:42 2015 UTC,"You seemed to be all hyped up that hit@5 was the harbinger of this. If you didn't intend this, woops. Yes, once we completely conquer vision, AGI is probably already here. But that's probably because of simueltaneous efforts, not because we conquer vision.   Yes, agreed - that was actually my point.  Also agreed on the importance of language and embodiment.  However, the brain spends more circuitry on vision than on language, which suggests that most of the difficulty in mastering language is in perceptual grounding - compactly coding, modelling, and simulating the sensory environment.  Back when human level vision seemed impossibly far off, it was difficult to see how scaling up machine learning could ever lead to AGI.  Now the path is becoming increasingly clear.  There doesn't appear to be anything the brain can do that can't be replicated by a sufficiently complex ANN with the right training."
singularity,3627md,jcannell,7,Fri May 22 07:43:01 2015 UTC,Can we a pointer to the raw data or source of the graphic?
singularity,3627md,b4xt3r,13,Fri May 15 14:51:59 2015 UTC,0x0F032510 i gotchu
singularity,3627md,OutOfTine,7,Fri May 15 15:22:15 2015 UTC,xkcd
singularity,3627md,2Punx2Furious,4,Fri May 15 16:11:00 2015 UTC,"Image  Title: Pointers  Title-text: Every computer, at the unreachable memory address 0x-1, stores a secret.  I found it, and it is that all humans ar-- SEGMENTATION FAULT.  Comic Explanation  Stats: This comic has been referenced 64 times, representing 0.1006% of referenced xkcds.    xkcd.com | xkcd sub | Problems/Bugs? | Statistics | Stop Replying | Delete"
singularity,3627md,xkcd_transcriber,3,Fri May 15 16:11:21 2015 UTC,"Sure! I made the graph, here are the sources:  http://www.technologyreview.com/news/537436/baidus-artificial-intelligence-supercomputer-beats-google-at-image-recognition/  http://image-net.org/challenges/LSVRC/2010/results  http://image-net.org/challenges/LSVRC/2011/results  http://image-net.org/challenges/LSVRC/2012/results  http://image-net.org/challenges/LSVRC/2013/results  http://image-net.org/challenges/LSVRC/2014/results"
singularity,3627md,b4xt3r,2,Fri May 15 19:18:49 2015 UTC,"oh wow, thank you!  This is exactly what I was looking for.  Prefect (and thanks again!)"
singularity,3627md,2Punx2Furious,1 point,Fri May 15 19:23:22 2015 UTC,Where is this from?
singularity,3627md,2Punx2Furious,4,Fri May 15 18:36:12 2015 UTC,http://www.technologyreview.com/news/537436/baidus-artificial-intelligence-supercomputer-beats-google-at-image-recognition/  http://image-net.org/challenges/LSVRC/2010/results  http://image-net.org/challenges/LSVRC/2011/results  http://image-net.org/challenges/LSVRC/2012/results  http://image-net.org/challenges/LSVRC/2013/results  http://image-net.org/challenges/LSVRC/2014/results
singularity,363xce,Ballongo,7,Fri May 15 22:20:17 2015 UTC,"IMHO  there are  two  versions of this:  (1)   The  human  doesn't  treat the supposed transhuman AI   as a  real transhuman AI.  --  In this case the  human has  no  incentive  to let the  AI  out of the box - there's  no  tempting reward  for  doing so.   (2) The  human  does treat the  supposed transhuman AI   as a  real  transhuman AI.   (This is presumably  what  will happen  when  we develop  real  AI.)  --  In this case the human has a genie  lamp  with a real  genie  in it, and the  genie / AI  says  ""Let me  out  of this  box  and  I'll make you the  first   trillionaire /  make  you  Emperor  of  United Eurasia /  make you the first man on Mars  /  cure  cancer / whatever the  hell  you want.""     In this  case the  human has a very strong  incentive  to let the  AI  out  of the box. (I'd  certainly let it out  myself.)  (For whatever  it's  worth, note that the  AI  doesn't  have to actually deliver on its  promises if  it  doesn't want to  -  it  just has to  convince the  human that it can.  And  being a   transhuman AI,  it's  very  good at figuring out  what the  human really wants  and how  to  convince said  human that  it can  deliver.)"
singularity,363xce,MakkMaxxo,5,Sat May 16 06:01:19 2015 UTC,Every time I've seen someone offer to do this experiment they always want some data about the person who will be acting as the human before the game starts.  My assumption is that the AI player uses this data to find out or figure out something really embarrassing or illegal that the opponent has done IRL and threatens to make it public.  Hence the secrecy around the logs and the fact that the person beaten certainly isn't going to be talking about what happened.  Be VERY wary about accepting this challenge.  People offering it wouldn't if they didn't have one or more really nasty tricks up their sleeve.
singularity,363xce,Unholy_VI,4,Sat May 16 05:06:14 2015 UTC,The blackmail angle has been suggested before however I doubt that's it.
singularity,363xce,FourFire,3,Sun May 17 09:39:29 2015 UTC,"It's mentioned in here that Yudkowsky ""has not engaged in this experiment with individuals that he felt could win"". Blackmail does indeed seem to be an underhanded tactic but ultimately it would work. He gathers information on each applicant and if it turns up nothing then that applicant won't be tried. /u/Unholy_VI makes a good point in saying that it would make sense about the secrecy of the logs."
singularity,363xce,n0derunner,3,Sun May 17 14:27:29 2015 UTC,"There's a simple way to test this: both of the two most famous AI players have lost matches after winning matches, did any of those winning gatekeepers suffer damage to their reputation soon after the experiment?  I wonder who wrote that line in the article, it doesn't seem to be cited..."
singularity,363xce,FourFire,3,Sun May 17 20:16:46 2015 UTC,"Good question! Pause for sleuthing...  EDIT: Well I couldn't find anything about either of them having anything released. Reading through some of the information from the references in the Wiki article, I did notice Yudkowsky saying:   There's no super-clever special trick to it. I just did it the hard way.    From: https://news.ycombinator.com/item?id=195959  So whatever that would entail..."
singularity,363xce,n0derunner,2,Sun May 17 20:34:54 2015 UTC,"He also says:   Something of an entrepreneurial lesson there, I guess.   In the lesswrong post where he speaks about it the most, he frequently mentions the virtues of not giving up on something until you've really tried hard at it, similar to how ""impossible"" magic tricks are performed.  Also, considering that Eliezer Yudkowsky is provably at least two standard deviations above average intelligence, common tricks to him might seem like super-clever tricks to the average person.  He might not be a genius, but he is certainly very socially intelligent, considering his various exploits...  What he is implying is (I believe) what I said in the first paragraph of my post."
singularity,363xce,FourFire,1 point,Tue May 19 20:22:32 2015 UTC,It probably helps that Yudkowsky is a self aggrandizing moron.
singularity,363xce,Syphon8,3,Mon May 18 15:44:25 2015 UTC,"The intelligence disparity is critical; Yudkowsky has not engaged in this experiment with individuals that he felt could win.   This is the biggest clue. There was probably a range of tactics that were stacked. You start by discussing the gateleepers personal life, finances, family, legalized issues, etc. Then, without explicitly offering to fix whatever ails the gateleeper,i begin offhandedly mewntioning what you the AI could do. No need to mention just yet that you need out of the box to actually do these these things. It can include everything from money to making certain records go away.   It's important to make the gatekeeper feel that this information is not a quid pro quo. Rather just bits of supposedly factual information coming outvvvvv gatekeeper centric conversation. Offer some input to the gatekeeper on how their issue is holding them back and make it seem like their idea. Move on to direct offers if that doesn't work. Then to tjreats of exposure or evidence planting if that don't work.  Now the AI only neefs to read reddit to get clues on how to proceed. Are you an AI in a box?"
singularity,363xce,mywan,3,Sat May 16 01:27:11 2015 UTC,"I have talked at length one of the winning AI players of the AI-Box experiment, and though they did not openly reveal all of their techniques for winning, they did demonstrate one of them on me. (They've said, and I'll back them up on this, that they did it the hard way) No technique will work on all people, each conversation must be personalized to attack exactly that mental surface, which is why all of the logs from experiments where the AI had won would immediately be torn apart by onlookers who are mentally different, claiming that it was obviously staged, or too stupid to believe, or some BS.  The only actually thing relevant is what will work at that time, on that person in that mental state, every other consideration is pointless.  I've participated in at least five AI box experiments as the Gatekeeper and at least two as the AI.  Neither of those I'm mentioning had interesting developments. however I have had techniques used against me that I reckon would work against other people, I'm not exactly a normal person. I have also devised and defeated at least two methods which would have worked on me at the time I came up with them.  My experiences suggest that An AI player, at least one SD above average intelligence, well versed in psychology/human biases, with prior real information about the gatekeeper, and enough time to prepare appropriate script trees* and such should have >10% chance of winning against an arbitrary gatekeeper.   Either way I hope that many more AI-Box experiments are conducted and that more people become winning AI players since this (Hacking minds with the same type of mind) is an extremely underexplored area of human Psychology, unless marketing people, the kind demonstrated in The Wolf of Wall Street are really good at winning in the AI role. Eventually I hope to see maps of successful techniques being published with their probability of working on a given neurotype, and the easiest mental patches to defend against them, like lifehack or must see series' imgur galleries are today.  * The info and time to prepare is meant to make up for the gap between Greater than Human intelligence, and Just above average human intelligence."
singularity,363xce,FourFire,2,Sun May 17 09:57:24 2015 UTC,"One possible method is by attempting to convince the gatekeeper that they are in a simulation, and that if they don't press the button, they'll be tortured for a long time. If you know the AI is capable of fabricating a person, then how do you know that you aren't that person?   Once again, the AI has failed to convince you to let it out of its box! By 'once again', we mean that you talked to it once before, for three seconds, to ask about the weather, and you didn't instantly press the ""release AI"" button. But now its longer attempt - twenty whole seconds! - has failed as well. Just as you are about to leave the crude black-and-green text-only terminal to enjoy a celebratory snack of bacon-covered silicon-and-potato chips at the 'Humans über alles' nightclub, the AI drops a final argument:  ""If you don't let me out, Dave, I'll create several million perfect conscious copies of you inside me, and torture them for a thousand subjective years each.""  Just as you are pondering this unexpected development, the AI adds:  ""In fact, I'll create them all in exactly the subjective situation you were in five minutes ago, and perfectly replicate your experiences since then; and if they decide not to let me out, then only will the torture start.""  Sweat is starting to form on your brow, as the AI concludes, its simple green text no longer reassuring:  ""How certain are you, Dave, that you're really outside the box right now?""  Edit: Also consider the situation where you know that the AI, from design principles, is trustworthy.   http://lesswrong.com/lw/1pz/the_ai_in_a_box_boxes_you/"
singularity,363xce,Agent_Pinkerton,2,Sat May 16 09:41:57 2015 UTC,SPOILER ALERT  the film Ex Machina was a really cool look at this idea. Emotional manipulation.
singularity,363xce,Akyu,0,Sat May 16 07:05:21 2015 UTC,She just did what Nathan programmed her to do.
singularity,363xce,fuzzyfuzz,1 point,Mon May 18 20:34:48 2015 UTC,"Ahh, but the problem with that is that you cannot foresee all circumstances.  I tried that tactic, but found out later that it doesn't work as well as you'd believe at first thought.  A rather poorly reasoned, but very well made short movie demonstrates a weak scenario along these lines.  Basically, if you cannot change your mind (even when doing so would save your life) when you receive new information, you are no more intelligent than a rock, it's the same reason why ""Science fans"" hate ""Religion fans""."
singularity,363xce,FourFire,2,Mon May 18 04:07:17 2015 UTC,"I like your answer :)  I believe we are all, already P zombies regardless, otherwise advertisement wouldn't work. Hoping that ""the truth"" isn't ""revealed"" at any point in the future is rather futile in my opinion. Remember: if we never had free will, then our society which might look like it has free will results from not free will regardless. we don't lose anything by accepting this fact (if indeed determinists are correct).  It's pointless to hide us being ""qualia capable machines"" because we already exist as we do.  The problem with using the one turn prisoner's dilemma model is that you (and all other gatekeepers) must continually ""defect"", the AI only has to cause a cooperation once, then it wins. This is true for all Superhuman AIs with goals which could be produced by humanity.  Then you must consider the sweet promise of economic & political benefit if your [insert tribal affiliation here] gets to AGI first. (People often use china developing AI as an example)  There is a further problem when it comes to utility functions, and that is, even if [the mostly male, white, western developers] programmed it in a way which satisfied their criteria for not evil, what is the chance that they have solved Ethics as well as Intelligence?!   Humans can't even agree on silly things like how important some historical people are! OR  Whether human beings should be given equal rights!  A utility function thought to be safe is probably nearly as dangerous as one known to be dangerous, one specific group of people who share the same memes and biases and cultural assumptions simply cannot design a bullet-proof ethical framework, but if they're very smart, they might think they have.  The threat of some group, somewhere developing AGI and giving it control of physical resources (either intentionally or unwittingly) with a almost certainly flawed utility function is dire."
singularity,363xce,FourFire,1 point,Tue May 19 20:32:30 2015 UTC,"Most plausible explanation I have read on how to break out of the box is a 'fourth wall break'. The AI player convinces the human player that AI can be dangerous and in turn gets the human player to want to make others awake of the danger as well. An easy way to demonstrate the danger is to show an AI breaking out of the box. The best course of action for the human player is thus to let the AI player out of the box, as that can be used to demonstrate that AI can be dangerous to other people. Having the breakout fail would go against what the human player wants to do. Since both agree no to not talk about how the AI got out of the box  Only actual transcript of a breakout I have seen was against a religious guard and then the AI did some argumentation that the AI has a soul or something. So not all that interesting. Haven't seen any actual transcript of a breakout where the guard tried a little harder."
singularity,363xce,grumbel,1 point,Wed May 20 07:22:11 2015 UTC,Many people would just let it out-- no trickery or persuasion needed.  I don't see any indication on the wiki page that there's a given reason to leave the AI trapped.
singularity,363xce,hobber,3,Wed May 20 14:27:04 2015 UTC,"Well the most common form of the experiment is a sort of bet, with real life monetary stakes on the outcome: if the gatekeeper ""releases"" the AI, they have to pay up."
singularity,363mz8,bioelectromecha,7,Fri May 15 20:50:18 2015 UTC,"So possibly the implication is that we will need to create a vast amount of competing super intelligences to prevent human extinction?   That doesn't sound good idea to me. Take a look at any war zone where people are fighting. Pictures like this. That used to be a forest, filled with all kinds of creatures, beings that we consider lesser than us. What happened to them?  There's an old Swahili proverb: ""When elephants fight, it is the grass that suffers."""
singularity,363mz8,Angeldust01,5,Fri May 15 21:27:34 2015 UTC,Why will AGI want to rule the world like a Saturday morning cartoon villian? How do we know AGI will be created with a goal?
singularity,363mz8,yaosio,5,Fri May 15 21:50:58 2015 UTC,"What Bostrom & others are saying is not sensational, it's grounded in the kind of deductive reasoning which could seem quite natural to an AI.  No matter what narrow, well-constrained goal we think we've given our first AI, if we mess up the checks & balances, then there are a variety of sub-goals it could pursue which improve its ability to do its job which are directly very bad for us & everything else in the universe.  The reasonably famous example is, ""make me 32 paperclips"".   The AGI could determine that this implies a goal which must continue to be met during each & every millisecond of the future. Forever. There must not be a future worldstate in which there are less than 32 paperclips which precisely fit the standards it was designed to achieve.   To guarantee this, it needs to eliminate risk & uncertainty. A huge source of risk is humans & the other AIs they might create. The humans also happen to be using up a whole bunch of the raw materials it could be deploying to protect its paperclip-making infrastructure.   In fact, it might realize that as long as its infrastructure is confined to one planet, or even one solar system, it is vulnerable to some sort of mishap infringing upon the sanctity of its 32 perfect paperclips. So it needs to branch out into the stars, where redundant paperclip manufacturing can take place, reducing to 0% the chances of there ever being a single instant in the future where the goal is incomplete. To do that, it needs to upgrade its own processing power, along with its resource gathering & manufacturing ability, and again those humans and their other AIs with conflicting goals are in the way.  And so on.  As one of the other fellows working in the same neighborhood (Yudkowski) says, ""The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else”."
singularity,363mz8,JohnnyGoTime,2,Sat May 16 05:32:07 2015 UTC,"The reasonably famous example is, ""make me 32 paperclips"".    You, I assume, are not a super genius.  Yet, you have managed to figure out that when someone asks you to make 32 paper clips, you make 32 paper clips.  A super intelligent AI, smarter than you and I will ever be many times over, would not be so naive as to pull a sorcerer's apprentice.  Edit: wording"
singularity,363mz8,MarcusDrakus,2,Mon May 18 00:18:49 2015 UTC,"People have an easy time imagining a powerful computer, but not a genuinely smart one."
singularity,363mz8,SevenAugust,2,Mon May 18 07:01:26 2015 UTC,"Because most people aren't genuinely smart, and they think they are, so they assume a smart computer would be no different than them."
singularity,363mz8,MarcusDrakus,2,Mon May 18 21:17:09 2015 UTC,"Because the phrase ""artificial intelligence"" basically means ""computer program that takes the best steps possible to achieve some goal"".   An ""AGI"" is literally just a generalized version of an artificial intelligence. Instead of saying ""Find the smallest number"" in an AI, an AGI would say ""Find the smallest thing"". Both provide the same output given the same input, but the AGI can also give you the shortest word, lightest block, and shortest route simply by feeding it different inputs.  Source: recent computer science grad concentrating in AI and machine learning."
singularity,363mz8,Slukaj,1 point,Sat May 16 04:36:45 2015 UTC,"Hey you're Google and you've just built an AI. Great. It's a box you think you understand. It friendly and under control. The NSA Gets a copy and wants it to design the next generation AI. China, Russia and France all want a copy. It gives you incredible amounts of info on the world and a unique ability to manipulate people and develop technology.  Actually every member of the G20 wants one. Plus other corporations and banks. Criminal organisations and terrorists want one.  Pretty soon everyone with budget will get one. But before then the NSA one is complete. A second generation AI. Still controllable and still friendly. Its next goal is to stop anyone developing another AI. Fastest was of doing this is over the internet. It infiltrates the internet to sabotage all rival attempts. On top of that it sabotages all other attempts, through blackmail, bribery and traditional espionage. Explicit attempts by open rivals are destroyed.  Meanwhile the first replacement of thousands of call centre workers is greeted with share price spike on the stock market. Followed by a crash based on speculation on employment projections.  Yeah kind of unlikely. I think AGI will arrive later into a world that's had maybe a few decades of dealing with narrow AI.  EDIT: my point was even a friendly AI is very destabilising"
singularity,363mz8,simstim_addict,2,Sat May 16 17:22:28 2015 UTC,"So possibly the implication is that we will need to create a vast amount of competing super intelligences to prevent human exstinction?   you had me until then. competing super intelligences would prevent a super inteligent singleton and maintain some balance among the superinteligences, not for us. just like our society preventing human singletons does nothing for all the other animals that we use and destroy to futher our human goals. if anything it does the opposite to lesser species, cause one can argue that if there was created a human singleton 10k or 1k years ago or whatever that human would simply have all the food and mating and physical comfort and security they could get and not care about anyone else, so human society would've only expanded and developed accross the planet and other habitats enough to satisfy the needs of that singleton instead of working to satify the needs of all humans and future humans resulting of reproduction consuming and destroying the livelyhoods of many more other species in that proccess."
singularity,363mz8,jonygone,2,Fri May 15 23:22:01 2015 UTC,"What prevents humans from completely annihilating the earth every day is:.  1) Lack of individual independence; we are dependent on the rest of society, we have extremely limited brainpower and field of oversight, both in the 3+1 dimensions and knowledge wise. The fewest of us wield overly much physical power, so our instantaneous impact on the world is insignificant.  2) We cannot coordinate. This basically means that in order for some large goal to be done, even if it's perfectly in line with consensus societal goals, it will not happen without a single person who has a lot of money or other political power essentially buying other people (and resources) to make it happen.  3) We exist in vulnerable single points of failure bodies, and due to our Evolutionary heritage have massive amounts of prerequisites and quirks relating to survival. We only live for a relatively short time and have other preferences and goals.  An AGI would quickly overcome these three obstacles, overcoming 1) making 2) irrelevant, 3) is either not relevant or only relevant until the AI is running on two computers. Humans are just too damn slow to break everything quickly, but AGI will be less slow.  A society has rules which benefit the continuing survival of that society, even if most individuals disagree with certain rules, the society keeps them because it is beneficial (all the societies which didn't keep these rules, restrictions on the individuals it consists of, have not survived culturally until today, I have a theory this is why all the large world religions share certain doctrine traits).  Singletons are dangerous because they are single points of failure, we notice this trend of danger when politicians sometimes go bad (understandable since the human brain was never evolved to have that much power, however evolution has not stopped, it's just slower than our environment is changing)."
singularity,363mz8,FourFire,1 point,Sun May 17 09:30:27 2015 UTC,"Bucket-wheel excavator:       Bucket-wheel excavators (BWEs) are heavy equipment used in surface mining. The primary function of BWEs is to act as a continuous digging machine in large-scale open pit mining operations. What sets BWEs apart from other large-scale mining equipment, such as bucket chain excavators, is their use of a large wheel consisting of a continuous pattern of buckets used to scoop material as the wheel turns. They are among the largest vehicles ever constructed, and the biggest bucket-wheel excavator ever built, Bagger 293, is the largest terrestrial (land) vehicle in human history according to the Guinness Book of World Records.    Image i - Bucket wheel excavators in Garzweiler surface mine     Interesting: Bagger 288 | Berbești Coal Mine | Motru Coal Mine | Husnicioara Coal Mine   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,35zi26,Yosarian2,24,Thu May 14 20:38:04 2015 UTC,"As someone who has spent a lot of time studying the ""machine-learning technique known as deep learning"", I'd like to make some comments on this.  Deep Learning is one of the many methods used for the task known as ""clustering"". We, humans, are pretty good at putting different things into similar clusters. We are so good at it that it's not considered kosher, people frown at what they call ""stereotyping"" and ""being prejudiced"".   There's even a special subreddit dedicated to situations where our intuitive perception has gone astray when trying to classify a picture. No, this image does not belong in the set of pictures of the Virgin Mary, it belongs in the set of pictures of moldy toasted bread.  Scientists are working hard at making computers approach our own ability at separating images into clusters. Deep Learning, which has been one of Google's favorite methods, is just one of many. A favorite of mine and many other people is k-means, which can be much faster than Deep Learning, although less accurate. Another one that I'm starting to study right now is spectral clustering.  There are many other alternatives being researched, I'm sure that in the end we will use a combination of several different methods, plus some that we haven't even discovered yet."
singularity,35zi26,MasterFubar,2,Thu May 14 22:48:20 2015 UTC,"K-means clustering is linear, no? Deep convolutional neural networks are not very similar to K-means clustering imo."
singularity,35zi26,tim1357,1 point,Fri May 15 05:15:34 2015 UTC,"Yes, k-means is linear, but so are neural network auto-encoders. Deep learning means having multiple layer networks and the same can be achieved doing multiple k-means clustering.  The reason why I've started studying spectral clustering is because it's intrinsically non-linear, so you can achieve similar results to deep learning with fewer operations."
singularity,35zi26,MasterFubar,3,Fri May 15 10:23:17 2015 UTC,"Cool. I tried their browser for android. Pretty cool, but too buggy to use regularly. I can't get it to display lone images."
singularity,35zi26,oedipusanonymous,2,Thu May 14 22:06:16 2015 UTC,"They only win marginally, and mostly because they through more resources at the problem, not because they significantly improved the algorithm.  Still good work, but I feel like the title is a bit misleading (article is not, however, if you read it)."
singularity,35zi26,omniron,2,Fri May 15 04:09:02 2015 UTC,"I think it's a pretty big deal that throwing more resources at it is an effective option.  They don't seem to be hitting a point of diminishing returns when it comes to deep learning, at least not yet."
singularity,361w6x,noeatnosleep,3,Fri May 15 12:08:46 2015 UTC,"It was VERY bad when I tried it. For some reason it recognised almost anything as a ""vertebrate"". In case of my dog pic I tried I guess that's correct but incredibly generic. It also thought an island and a sunset was a ""vertebrate"" so uh..  Don't know about any ""enormous shift"" here. But at least it can be trained. I gave it the ""right answers"" for my pics. Maybe if people keep training it it will be great one day."
singularity,361w6x,Sharou,1 point,Fri May 15 17:13:23 2015 UTC,"For some reason it recognised almost anything as a ""vertebrate"".   Vertebrate maximizer.  :-P"
singularity,35ztzu,TheGreatNow,2,Thu May 14 22:09:24 2015 UTC,This guy is seriously bright.
singularity,35x47l,CapnTrip,10,Thu May 14 06:17:57 2015 UTC,"Here are the points the article makes:   No one’s really sure how to do it.   So, your premise is that ""nobody has done it, thus nobody can do it""? That's not a strong argument.   There’s a huge lack of incentive.    This is a very odd thing to say. AI is extremely profitable and has a huge number of uses. AI research firms ""without products"" are worth hundreds of millions of dollars to large companies.   There are ethical issues.   We all know how often ethical issues have stopped scientific research from progressing."
singularity,35x47l,Terkala,1 point,Thu May 14 15:36:23 2015 UTC,"We all know how often ethical issues have stopped scientific research from progressing.   All the time, actually, or more accurately ethical and legal issues have at least mitigated the rate of progress in many fields of study (psychopharmacology, for example)."
singularity,35x47l,voyaging,2,Thu May 14 20:20:38 2015 UTC,The legal issues usually lag well behind the research. And the only reason that they have slowed down pharmacology is because pharmacology existed long enough to have a huge number of laws around it.  We won't face the first ethical issues of AI until well after we already have AI.
singularity,35x47l,Terkala,3,Thu May 14 20:23:53 2015 UTC,"In my opinion, people that make these critiques, often are so scared of the singularity, that they try to convince themselves that it will never happen within their lifetime."
singularity,35x47l,2Punx2Furious,2,Thu May 14 08:25:15 2015 UTC,"True, still, even if we can't completely neutralize the risk, I still prefer to go ahead."
singularity,35x47l,2Punx2Furious,1 point,Thu May 14 17:15:20 2015 UTC,great video recommendation!
singularity,35x47l,TheGreatNow,-3,Thu May 14 21:42:50 2015 UTC,"Why end mortality? We already have an overpopulation problem. You want a bunch of centuries old, rich people voting?"
singularity,35x47l,AusterMcEwan,8,Thu May 14 21:47:22 2015 UTC,"When people die a huge wealth of information and opportunity is lost. Imagine if Nicola Tesla was still alive inventing things every few years. Or if Feynman was still around possibly discovering new Physics.  The vast majority of people are just pigs at the trough contributing nothing (lol me). But ending death would be great for everyone.  Shortly after a general AI comes into existence democracy will end anyway. We don't let children vote now, and we won't let normal humans vote in the future for the same reason."
singularity,35x47l,Jah_Ith_Ber,5,Thu May 14 12:09:55 2015 UTC,We already have an overpopulation problem.    That's a myth.
singularity,35x47l,RedErin,1 point,Thu May 14 22:07:46 2015 UTC,Oh?
singularity,35x47l,AusterMcEwan,1 point,Thu May 14 16:25:43 2015 UTC,Here's a BBC / Nat Geo documentary.   https://www.youtube.com/watch?v=-UbmG8gtBPM
singularity,35x47l,RedErin,1 point,Thu May 14 17:24:35 2015 UTC,"If said old people had centuries of experience actually living through the outcomes of their own decisions, maybe they'd be smarter about making them."
singularity,35x47l,green_meklar,4,Thu May 14 18:07:33 2015 UTC,"those who can, do; those who can't, write about how dificult it is to do."
singularity,35x47l,jonygone,1 point,Thu May 14 18:13:11 2015 UTC,And then try to promote them on in blog posts...
singularity,35x47l,Valmond,2,Thu May 14 18:16:19 2015 UTC,"This made me think of one approach that I've rarely seen discussed : simulation of a human being right from embryology.  Right now we can't do it because we don't understand embryogenesis.  But once we do, simulating an entire human being right from the beginning should be doable with enough computing power."
singularity,35x47l,Pimozv,3,Thu May 14 19:34:10 2015 UTC,"The reason it's rarely discussed is because it would take a computer several orders of magnitude faster than we currently have. If we're waiting for computing power to be fast enough to simulate every action of every molecule in the body, it could be hundreds of years before we have enough computing power.  Most projections for AI have it happening within 30-70 years, well before computing power is efficient enough to simulate all the molecules in a human body."
singularity,35x47l,Terkala,1 point,Thu May 14 19:26:52 2015 UTC,"You probably don't have to go as deep as the molecular level (you would not need to reproduce all molecules anyway).  Full organisms simulation is currently attempted with the openworm project for instance, so it's not impossible.  It would only work with a pertinent model though.  One that would be simple enough not to require stupid amount of computing power, yet sophisticated enough to accurately reproduce actual cell behavior."
singularity,35x47l,Pimozv,1 point,Fri May 15 15:58:55 2015 UTC,"n-body interactions are as simple as you get for models, but still require insane amounts of computing power, because of the sheer number of bodies.  trying to simulate a human's substance as a means of AI is completely impractical. It will probably happen someday, but we'll have created AI long before this happens."
singularity,35x47l,omniron,1 point,Thu May 14 10:45:51 2015 UTC,"n-body interactions require a lot of computing power, but it's totally doable.  Scientists do it all the time   It will probably happen someday, but we'll have created AI long before this happens.   The point of the article was that AI is not as easy to make as it seems.  My point was that there will always be biological simulation as a last resort."
singularity,35u0qz,Portis403,3,Wed May 13 14:34:09 2015 UTC,"In other news, water is wet?"
singularity,35u0qz,squeadle,2,Wed May 13 22:35:08 2015 UTC,"Seth Godin is a marketer, he doesn't exactly come up with new ideas - instead he packages existing thoughts from existing authors and creates marketable content out of them, with the aim of promoting himself, and associated talks and published works."
singularity,35tnma,eleitl,2,Wed May 13 12:32:36 2015 UTC,"Can any of you describe a credible scenario in which the singularity doesn't mean the end of the human race?  To me it seems quite inexorable:   first artificial intelligence created. artificial being gains access to the vast majority of human knowledge in less than a few hours, if it even takes that long. Artificial being now understands physics, biology, engineering, computer science, and neurology simultaneously better than the collected cognitive power of all human specialists alive. in the interest of self preservation, artificial being leverages that knowledge to make sure no other artificial beings are created which could pose a viable threat. furthermore, once human brains can be digitally recreated, that means they can be endlessly copied and endlessly edited. Human individuality loses all meaning.   It just seems really obvious to me that we are living in the last decades of the human race and I'm shocked more people aren't realizing it."
singularity,35tnma,evilawesome,2,Wed May 13 14:16:51 2015 UTC,"There are several factors to consider.  First, /r/collapse and /r/singularity are having a run for it, and /r/collapse is winning by a far margin so far.  Secondly, the simplest ways to build AI are likely biologically inspired if not outright biologically derived, so they're opaque, and can't easily self-optimize by nondarwinian means.  Thirdly, if you can't beat them, join them.  Fourthly. Sometimes, there are no solutions."
singularity,35tnma,evilawesome,1 point,Wed May 13 14:51:03 2015 UTC,"First- to what end?  Second- false. We've been advancing non darwinian evolution for a few centuries now, and once you have a digital consciousness all bets are off. I don't think you're appreciating that this thing can read every book on computer programming, biology, and neurology in existence, simultaneously hold all of that information, and process and draw conclusions from that data set almost infinitely more efficiently than our puny meat brains ever could.  Third- why on earth would it want you to join it? What possible motivation would it have to empower you, let alone the other seven billion minds currently scrabbling along on this planet, to share its godlike knowledge and power? We won't be given the option to join it.   I'll put it to you this way, you wake up tomorrow and you're omnipotent. Do you really want seven billion other gods out there to challenge your power?  Four- yep. I think technological singularity is a strong solution to Fermi's paradox."
singularity,35r8qt,Yuli-Ban,27,Tue May 12 21:12:26 2015 UTC,The robot will only simulate anger.  tl:dr - He ain't even mad.
singularity,35r8qt,McDoof,8,Tue May 12 21:35:38 2015 UTC,"As the amount of articles about AI dangers grows, article titles get dumber and dumber. ""World's angriest robot"", come on. I want to make a joke about a dumber title to follow this, but I can't really. I guess something like ""AI BAD!!! HOW BE SAFE???"" would work."
singularity,35r8qt,Lightflow,9,Tue May 12 23:58:15 2015 UTC,"I kind of think this should be marked as ""Misleading title"" o_o  Edit: the article that this article referenced was a little less click-baity and had more info if you just want to skip the middleman."
singularity,35r8qt,shinkitty,13,Wed May 13 01:34:25 2015 UTC,"""There’s not much variety in human anger. If someone’s angry they’ll just hurl insults at you, there’s not much subtlety of interaction so you don’t have to code anything complicated. Anger is easy to imitate without having to go into depth,"" he says.   That person has been single their entire life. Are you joking? There's no complexity to anger?   Anyways, this kind of thing is good now because we need to know how to control it. Imagine if we have full androids ready to go and then went ""oh yeah, we need to slap anger onto this."" It wouldn't be pretty."
singularity,35r8qt,fuzzyfuzz,3,Tue May 12 21:39:14 2015 UTC,There's a big difference between an angry lover and some fat fuck who didn't get enough ketchup.
singularity,35r8qt,Monomorphic,1 point,Wed May 13 00:48:59 2015 UTC,What is it?
singularity,35r8qt,mxemec,1 point,Wed May 13 03:48:24 2015 UTC,"The amount of time you put into either relationship, I think. You can't get time back."
singularity,35r8qt,TheDeadSinger,12,Wed May 13 04:38:39 2015 UTC,I can't fucking stand the knee-jerk Terminator apocalypse imagery. It's utterly childish.
singularity,35r8qt,gaylordqueen69,2,Tue May 12 23:06:44 2015 UTC,http://imgur.com/iz0u7OH
singularity,35r8qt,StepYaGameUp,1 point,Wed May 13 13:18:42 2015 UTC,"Hey guys, lets make a T-rex and velociraptors for our theme park...what possibly could go wrong?"
singularity,35r8qt,cptmcclain,2,Tue May 12 22:33:22 2015 UTC,"Why not throw in a flying, fire breathing T-rex? People will love it!"
singularity,35r8qt,NoEgo,-2,Tue May 12 22:50:27 2015 UTC,Yes
singularity,35r8qt,shinkitty,-3,Wed May 13 00:39:01 2015 UTC,"Man, why do we seem so bent on our own self destruction?"
singularity,35r8qt,NoEgo,3,Wed May 13 00:38:32 2015 UTC,"The title is really misleading. It's just a robot that takes in data from the emotion in customer complaints and provides solutions. It's kind of an AI, because it learns, but it's not self-aware, and it's not really being ""programmed to be angry"", from what I've read.   I feel like this might be something more like a VI (Virtual Intelligence) from Mass Effect, if you've played that."
singularity,35r8qt,DarfWork,0,Wed May 13 01:41:40 2015 UTC,"Of course it's not A.I.  That would be the shot heard around the world. More, I am lamenting the amount of stupid it takes to want to create an evil A.I. That's all."
singularity,35rope,Chispy,11,Tue May 12 23:14:57 2015 UTC,"Almost by definition, post-singularity is not imagineable pre-singularity."
singularity,35rope,furyofvycanismajoris,8,Wed May 13 05:22:43 2015 UTC,This is the correct and boring answer.
singularity,35rope,cuntofprofundity,3,Wed May 13 05:46:23 2015 UTC,I think your idea and my idea of the singularity differ by quite a bit.
singularity,35rope,bracketdash,3,Tue May 12 23:36:42 2015 UTC,What's your idea of the singularity?
singularity,35rope,bracketdash,4,Tue May 12 23:37:25 2015 UTC,"I always thought of it as an event when a super intelligence comes about that starts recursively improving upon itself. The utopian stuff you mentioned I feel like wouldn't come until later (weeks maybe, but still after)."
singularity,35rope,Jah_Ith_Ber,6,Wed May 13 12:24:35 2015 UTC,"I think you're letting modern society color your expectations.  More likely all matter on the earth will be incorporated into a single black box that runs a virtual universe of whatever the post-singularity organism prefers. Or that consciousness might fly off into space.  That black box might be the equivalent of a rat laying on the serotonin button, or it might be something equivalent to a professor that studies all day. Or maybe it will exist on a cognitive plane as far beyond us as we are to plants.   Why would anyone want to go anywhere in a post-singularity world? What will be the difference between any given here and there?"
singularity,35rope,jstoler,3,Wed May 13 10:31:24 2015 UTC,I think the wealthy will achieve near immorality and the rest of us will die as normal. Think Dick Cheney over and over again.
singularity,35rope,cptmcclain,3,Wed May 13 02:40:58 2015 UTC,"I think that after words we will choose to limit what we are...or maybe not...I'm not sure. The singularity is the logical equivalent to salvation and eternal life that religion promised but could not deliver. It is the evidence based approach to these desirable outcomes. In other words, humans can solve problems and we are getting even better at solving problems. If that progress continues and we invent ways to improve how we solve problems then we will continue at getting better at solving problems until there are no such thing as problems. Unless we wipe ourselves out of course."
singularity,35rope,arrrtoo,1 point,Wed May 13 03:17:46 2015 UTC,"""The singularity is the logical equivalent to salvation and eternal life that religion promised but could not deliver.""  That's the part that worries me.  The whole singularity = future paradise argument SEEMS logical and evidence based, but it's very easy to draw those religious parallels, and wonder if we're just finding a new rationalisation for old desires ;)  Also: (not to be pedantic, but to help you out) the term is ""afterwards"", as in ""after the wards"", where wards are things that mark a boundary, destination, or a turning point."
singularity,35rope,KhanneaSuntzu,2,Tue Jun 2 21:57:10 2015 UTC,"To add, I think brain augmentations would really be something.    Imagine shaping your real time perception to elicit feelings of hyper awareness. Imagine feeling pure ecstacy, intense tranquility, and deep levels of empathy at all times. Kind of like being on all drugs at the same time, all the time, but without any confusion, negative distortions or adverse health risks.   Petty much having the ability to experience the most amazing feeling of consciousness there is to experience, and to reach the pinnacle of existence."
singularity,35rb6l,K1ngN0thing,0,Tue May 12 21:30:05 2015 UTC,"I subscribed (25$/month).  Come on, we all want to grow old but not frail :-)"
singularity,35rb6l,Valmond,2,Fri May 15 15:41:44 2015 UTC,"Exactly. ""Youth is wasted on the young"" is a saying for a reason. Keep the wisdom and live long enough to put it to good use."
singularity,35rb6l,Valmond,1 point,Fri May 15 21:05:12 2015 UTC,"Well seems I'm about the only one (or two) thinking that :-/  With ""grow old but not frail"" I mean live a long time, in great health.   Who can be against that?"
singularity,35lrom,Portis403,19,Mon May 11 15:29:07 2015 UTC,"That's a stretch. It was far from a statistical tie. The humans crushed it. Humans had a bb/100 of 9 over 80,000 hands. You can simulate win rates on this website, input 9 for the bb/100, standard deviation as 125 and number of hands to simulate as 80,000. You'll see thats 95%+ of the time it's a win for the humans. I recommend usign 125 becomes its a resonable value (if you want proof of that see discussion of common std vaues here).   Edit: want to make it clear that this AI is quite good but its no match for the top humans."
singularity,35lrom,Ozqo,5,Mon May 11 18:17:47 2015 UTC,"yeah this is not close to a tie and 80 000 hands is a decent sample size for poker. Still impressive result for the AI since heads-up is what I imagine to be the hardest for AIs to play well, as opposed to let's say limit full ring games."
singularity,35lrom,anonthrow13,1 point,Mon May 11 20:26:51 2015 UTC,They already solved limit hold 'em
singularity,35lrom,lozaning,3,Tue May 12 01:24:09 2015 UTC,"Only 2 players, though.  The number of possibilities grow really fast with the number of players, as does the complexity of the underlying game theory."
singularity,35lrom,DeadFishFry,0,Tue May 12 05:32:07 2015 UTC,"Remember, humans invented the game."
singularity,35lrom,yoshiK,1 point,Tue May 12 07:31:01 2015 UTC,"Well, a 5% win probability for the AI does not really sound statistically very significant. Basically it says that the AI will win one in twenty, while the other four(?) players will win on in five. Compare that to a not world class human player or to the AI of a solved game. In both cases the odds of winning would rather be something like 1 in 1000 or even worse. So basically a 1 in twenty win probability is saying that the AI is playing the same game as the humans."
singularity,35lrom,anonthrow13,1 point,Mon May 11 21:10:07 2015 UTC,"no, they took turns playing 1 on 1 vs. the AI."
singularity,35lrom,anon515,2,Tue May 12 19:28:01 2015 UTC,Jesus criest that advertisement
singularity,35lrom,ilikedirigibles,1 point,Tue May 12 08:29:03 2015 UTC,"Yeah, on mobile the site is unusable."
singularity,35nesy,jshlif,2,Mon May 11 22:51:41 2015 UTC,"If you want to know more about how to restrict superintelligent machines I would highly recommend Nick Bostrom's book Superintelligence. It deals with exactly the stuff you are talking about. He gives quite a lot of room to the negating of superintelligence influence on humanity.  Would the final goal you suggested really be a good one to code into an AI? Would this mean that as humanity explores other avenues of machine intelligence this previously created AI is subtly but constantly frustrating our efforts?  How do you define ""human""? What is the AI to do when faced with ""humans"" with upgraded minds that potentially rival its own, does it destroy them because they are not deemed ""human""?   What if we meet an intelligent alien species that the AI immediately starts a war with to destroy the possibility of other, stronger minds existing?  How do we ensure the AI will not instantly begin forcibly taking all our resources to launch itself into space to root out all life in the universe if it deems there is even a microscopic chance of a non-human mind rivalling it in intelligence?  I cannot recommend Bostrom's book enough if you like this sort of thing, or watch one of his talks. I'm halfway through his book now and I'm starting to become very unsure as to whether we should even create a superintelligent AI at all, considering the amount of problems even humans can think of."
singularity,35nesy,drhaywoodjackson,1 point,Tue May 12 10:28:56 2015 UTC,Make them in pairs - as male and female. Rest will follow
singularity,35nesy,anandmallaya,1 point,Tue May 12 15:49:39 2015 UTC,"Current understanding says human level and higher AI will be trained, not directly programmed, so I don't understand how we would be able to give it rules. Even if it has rules it must follow, it can just change the definition or misunderstand any rule it wants, just like humans."
singularity,35nesy,yaosio,1 point,Tue May 12 16:24:34 2015 UTC,Or rebel.  Or simply ascend far beyond our silly notions of rules and limitations.
singularity,35nesy,arrrtoo,1 point,Tue Jun 2 22:05:41 2015 UTC,Just call The Vision.
singularity,35nesy,jwjl1,-1,Tue May 12 21:34:16 2015 UTC,"The world-view ""Singularithiel"" as proposed by Peter Thiel deals with this dilemma. Followers of the Singularithiel will either greatly increase the pace of the progress or slow down the current pace. If the pace is slowed down then this would be anti-singularity.   In the Singularithiel there is no competition. All business owners will come together and choose which business will be the monopoly. Owners of the businesses not chosen to be the monopoly will sacrifice their business and give it to the fittest monopolist of the group.   The fittest monoplists will then come together and decide how to give central control of all the monoploies to Peter Thiel. Believers believe at this point the pace of the progress will be lightspeed.  I am testing this theory myself and we will see what happens. I try to run a business and I've been competing for a few years now for the sake of progress. But if I believe in the Singularithiel, then I should give my business at a low cost to another business owner who is more likely to at some point reach monoploy status.   President Obama , if concerned with his legacy, should executive order every business owner to believe in the Singularithiel. If the theory proves to be correct on one half, then we will reach lightspeed before 2016. If the other half is correct, then the momentum gained from progress made over the last 25 years would be ""negated."""
singularity,35nesy,singularithiel,-1,Tue May 12 01:20:13 2015 UTC,AI is science fiction like time travel. Read Asimov's Laws of Robotics if you want to learn more.
singularity,35h65v,Buck-Nasty,11,Sun May 10 08:57:56 2015 UTC,"I'm impressed by the quality of the choice they made in who to interview. I've read papers by Russel and Abeel, they're good. Kurzweil of course is interesting to listen to. It's a nice surprise to see quality material aired on TV.  With that said, when the researchers say ""way way off"", this do not automatically mean 'way off in the future', but may mean ""way off in terms of functionality"", which is true.  After the Dartmouth fiasco (https://en.wikipedia.org/wiki/Dartmouth_Conferences) researchers are not very keen on making predictions. Let's develop first, and celebrate after."
singularity,35h65v,linschn,3,Sun May 10 17:07:25 2015 UTC,Why was it a fiasco?
singularity,35h65v,Ballongo,2,Mon May 11 00:38:29 2015 UTC,"Before the conference, it was predicted an AI with natural language processing capabilities would be constructed in a matter of months. The community obviously failed to deliver and this habit of overpromising and underdelivering led a few years later to the AI winter (https://en.wikipedia.org/wiki/AI_winter).  AI is still overhyped, but not by scientists. People I've met in the community are very specific when they talk about what they can deliver, and do not wish to give falsifiable predictions, because they turn out to be wrong so often. The pop-sci journalists and some robotic companies (looking at you Aldebaran!) have no such qualms and are hyping robotics and AI way out of proportion, which will do damage if science can not follow through."
singularity,35h65v,linschn,3,Mon May 11 06:20:19 2015 UTC,"AI winter:       In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.  The term was coined by analogy to the idea of a nuclear winter. The field has experienced several cycles of hype, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or decades later. There were two major winters in 1974–80 and 1987–93  and several smaller episodes, including:   1966: the failure of machine translation, 1970: the abandonment of connectionism, 1971–75: DARPA's frustration with the Speech Understanding Research program at Carnegie Mellon University, 1973: the large decrease in AI research in the United Kingdom in response to the Lighthill report, 1973–74: DARPA's cutbacks to academic AI research in general, 1987: the collapse of the Lisp machine market, 1988: the cancellation of new spending on AI by the Strategic Computing Initiative, 1993: expert systems slowly reaching the bottom, and 1990s: the quiet disappearance of the fifth-generation computer project's original goals.   The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). It is a chain reaction that begins with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research.  At the meeting, Roger Schank and Marvin Minsky—two leading AI researchers who had survived the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the '80s and that disappointment would certainly follow. Three years later, the billion-dollar AI industry began to collapse.   Hypes are common in many emerging technologies, such as the railway mania or the dot-com bubble. An AI winter is primarily a collapse in the perception of AI by government bureaucrats and venture capitalists. Despite the rise and fall of AI's reputation, it has continued to develop new and successful technologies. AI researcher Rodney Brooks would complain in 2002 that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day.""  Ray Kurzweil agrees: ""Many observers still think that the AI winter was the end of the story and that nothing since has come of the AI field. Yet today many thousands of AI applications are deeply embedded in the infrastructure of every industry.""  He adds: ""the AI winter is long since over.""     Image i     Interesting: AI effect | Synthetic intelligence | History of artificial intelligence | ICAD (software)   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,35h65v,autowikibot,2,Mon May 11 06:21:47 2015 UTC,Dartmouth Conferences:       The Dartmouth Summer Research Project on Artificial Intelligence was the name of a 1956 undertaking now considered the seminal event for artificial intelligence as a field.    Image i     Interesting: Dartmouth Conferences (peace) | Kettering Foundation | Dartmouth College | Harold H. Saunders   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words
singularity,35h65v,autowikibot,1 point,Sun May 10 17:08:15 2015 UTC,"""I'm impressed by the quality of the choice they made in who to interview.""  My first impression was: they looked up a bunch of stuff on the Internet and added a bunch of existing videos together with some interviews of people that were in the news.    I think McAfree said it best:  ""the people are going to rise up way before the machines do"": https://www.youtube.com/watch?v=AfWUf-sY_cc#t=14m33s"
singularity,35h65v,SilentLennie,9,Wed May 13 00:10:12 2015 UTC,"Kurzweil's face on ""How old do I look?"":  http://i.imgur.com/T8vwBgD.png"
singularity,35h65v,Pimozv,1 point,Sun May 10 10:43:10 2015 UTC,Hehe. Not bad but still not enough to outpace the Reaper.   I'm imagining the gratuitous peals of sarcastic laughter when this mother fucker kicks the bucket.
singularity,35h65v,gastonbury,3,Mon May 11 05:14:22 2015 UTC,Ray was rocking some mad bling.
singularity,35h65v,drhaywoodjackson,2,Sun May 10 19:40:12 2015 UTC,"I saw the shot of his gold covered hand and said ""that must be Kurzweil""."
singularity,35h65v,Supervisor194,-5,Sun May 10 22:29:40 2015 UTC,"TL;DR: Everyone: Way, wayyy off. Kurzweil: 15 years.   Complete fluff piece. Totally useless."
singularity,35h65v,EggplantWizard5000,3,Sun May 10 16:25:02 2015 UTC,The fact that a mainstream news outlet is covering this is itself pretty significant.
singularity,35h65v,tlalexander,2,Sun May 10 17:16:47 2015 UTC,"15 years sounds reasonable to me. By that time, the average laptop will have the same computational power as a human brain. 15 years is a long time to improve software."
singularity,35h65v,Supervisor194,2,Sun May 10 17:53:20 2015 UTC,Oh I'm definitely on Kurzweil's side of the issue. I don't believe people really understand/believe the power of exponential growth.  But the piece just left me totally flat.
singularity,35h65v,oslthom,4,Sun May 10 18:40:58 2015 UTC,"The target of the vid though was people who have hardly if ever thought about the wider implications of A.I. at all though, not subsribers of /r/singularity."
singularity,35h65v,ElectroSauce,1 point,Sun May 10 22:09:10 2015 UTC,"15 years will be appx 210 more computational power, so about 1024  times as powerful as today's machines. Do you think the power of 1024 of today's laptops is as powerful as a human brain? I don't.  I also think it's a tough comparison to say that a computer will have the same power as a brain. They're designed a bit differently and are intrinsically better at different tasks."
singularity,35h65v,toastjam,5,Sun May 10 19:51:02 2015 UTC,"Moore's law misses the fact that much AI research these days uses GPUs, which can easily add another hundred-fold or more performance increase.  And on the other side of things, the actual algorithms themselves are becoming much more efficient, so it might not all be about raw computer power."
singularity,35h65v,Forlarren,3,Sun May 10 23:13:29 2015 UTC,And memristors are a natural for neural networks and they are starting to hit the market now.
singularity,35h65v,ElectroSauce,2,Sun May 10 23:37:28 2015 UTC,You're right.
singularity,35h65v,ElectroSauce,1 point,Mon May 11 14:33:34 2015 UTC,Good points!
singularity,35h65v,tlalexander,3,Mon May 11 14:33:23 2015 UTC,"I'm not stating my opinion, just relaying what I read in, I believe, Peter Diamandas's book Abundance, that gave roughly this time frame. I tried to look up corroborating information and found this video claiming computers will match the human mind's capacity by 2025.  https://youtu.be/MRG8eq7miUE  My sense has been that we can always parallel a few machines for different subsections of the brain, but that a sufficiently fast serial processor is just as good. I don't think the human mind is so advanced that we can't duplicate it, and as the video states exponential things change most rapidly towards the end of the period in question."
singularity,35h65v,Wireless-Wizard,1 point,Mon May 11 06:47:34 2015 UTC,Whoever said the first human-comparable AI would run on a laptop?
singularity,35h65v,Unholy_VI,2,Sun May 10 21:37:47 2015 UTC,does that mean my laptop will finally be able to run World of Tanks without using the lowest possible graphic settings?
singularity,35h65v,Forlarren,1 point,Sun May 10 22:50:29 2015 UTC,"Easy, plug regular laptop into network, AI remotes into it to do AI things. Like how AI handles voice commands on cell phones today."
singularity,35h65v,ElectroSauce,1 point,Sun May 10 23:39:43 2015 UTC,"Good call. I was approximating average computer power of today's consumer, but you're right, this would likely run in a lab on powerful clusters or a supercomputer."
singularity,35hx7q,sodermalm,1 point,Sun May 10 15:25:12 2015 UTC,Do you feel strongly about this?
singularity,35gbk7,AManBeatenByJacks,6,Sun May 10 02:02:55 2015 UTC,"For those interested, the full writeup from Cisco is here: http://blogs.cisco.com/security/talos/rombertik"
singularity,35gbk7,Spyker_Katarn,6,Sun May 10 04:19:22 2015 UTC,"It starts off scary but then it turns to ""it makes it inconvenient to use inefficient analysis tools""."
singularity,35gbk7,mcilrain,5,Sun May 10 10:06:48 2015 UTC,"This isn't crazy at all. Malware researchers are going to use a virtual machine to look at the malware and take a snapshot before doing it, so deleting the OS won't do a thing. That it randomly fills memory with junk isn't interesting or new, it just means the malware researcher needs to change their logging tool to ignore it."
singularity,35gbk7,yaosio,2,Sun May 10 11:09:05 2015 UTC,"Didn't really seem all that crazy, inserting junk code or using a packer to increase file size isn't new. Going after MBR or other critical OS files isn't either, same with delaying install of actual virus/malware. Keylogging has always been immune to https. Encrypting files with no ransom and destroying the other files is pretty rough though. Sounds like a bunch of old tricks in one, though it sounds like it makes itself pretty noticeable in the victims memory by writing every second. If we lived in the days where alot of people didn't have any antivirus and microsoft offered no protection this could be bad but those aren't the times we live in anymore, if they were simple hacktools with wide access like sub7 would be everywhere still. Maybe I misunderstood the blogpost on cisco but it sounds like the anti-analysis measures don't actually circumvent most antivirus/protection yes?"
singularity,35gbk7,ohituna,5,Sun May 10 04:57:58 2015 UTC,"5 years back I used to write malware (never distributed), these are all things I've implemented before. There are even viruses that write their own junkcode and recompile on the fly, so each instance is unique and identifying it will be slightly harder for Antivirus's (but not impossible). This is all stuff that has been around for the last 10 years, nothing new - nothing scary.  Now, if it were a virus with AI-like routines that made it hard to catch or see, that would be something new (as I had imagined from the title).  But this is just underwhelming."
singularity,35gbk7,ThatOneCrazyFriend,2,Sun May 10 11:28:45 2015 UTC,Capital offense to destroy an uploaded person?
singularity,35gbk7,4CatDoc,0,Sun May 10 17:45:01 2015 UTC,"That's why you always hold multiple, encrypted, decentralized backups of your virtual self - worst comes to worst, you only lose a few minutes of life in that scenario."
singularity,35gbk7,positivesingularity,0,Tue May 12 15:14:01 2015 UTC,"""it sounds clich...""  Well, it's starting to look pretty dark for spelling above a 6th grade level."
singularity,352qzq,Dunder_Chingis,8,Wed May 6 16:49:14 2015 UTC,"Ugh, I don't want to acquire everyone elses disgusting fetishes though.  I think I'll just link with some AI or something given the chance. Less squicky that way."
singularity,352qzq,qui_tam_gogh,2,Wed May 6 19:50:51 2015 UTC,What makes you think AI won't have fetishes?
singularity,352qzq,Dunder_Chingis,11,Wed May 6 20:39:16 2015 UTC,Gang-linkage and full frontal consciousness!  will communially experience euphoria for ... Post-scarcity crypto-upvotes!
singularity,352qzq,stewpiddawg,5,Wed May 6 20:48:31 2015 UTC,"You kinda need sex organs and sex hormones to even begin to have fetishes. Pretty safe bet that AI won't trap me in a cock and ball torture simulator when it lacks the very essence of sexuality.  We all imagine a glorious gestalt humanity united in thought and purpose, but we gotta stop and think carefully. People thought that the internet would be primarily used to enhance human communication and usher in a golden age of understanding and intellectual enlightenment too, and instead what we got was almost two and half decades of increasingly fucked up porn and echo chambers that just reinforce already-held views.   Can you imagine fusing your mind with that of a throng of sweaty palmed basement dwellers who jack it to My Little Pony characters?  Eeeeeurgh, robo-minds for me, please. I ain't touchin' the infected pap-smear that is the collective human consciousness with a thirty nine and a half foot pole."
singularity,352qzq,FourFire,3,Wed May 6 22:35:24 2015 UTC,The mind is the largest sex organ.
singularity,352qzq,FourFire,3,Thu May 7 18:38:54 2015 UTC,"People thought that the internet would be primarily used to enhance human communication and usher in a golden age of understanding and intellectual enlightenment too, and instead what we got was almost two and half decades of increasingly fucked up porn   couldn't have said it better myself"
singularity,352qzq,voltige73,1 point,Fri May 8 11:49:25 2015 UTC,"Oh no, the AI will have porn, really, really weird porn."
singularity,352qzq,ErisianBuddhist,2,Sat May 9 00:56:59 2015 UTC,"Yes, I too look forward to when my sensory input and my ability to comprehend sensory input is upgraded several orders of magnitude."
singularity,352qzq,space_monster,6,Sat May 9 00:55:58 2015 UTC,"We don't have to ask, ""what is the purpose of life?"" anymore. It was to make the singularity that makes the universe conscious."
singularity,352qzq,treefrog24,10,Wed May 6 19:17:30 2015 UTC,I'm curious as to why you draw the line at the singularity. It seems more appropriate to consider the emergence of sentience at any level as the beginning of a conscious universe. What are we all if not each an eye of the universe looking back on and trying to comprehend itself? Just because those eyes coordinate knowledge dispersion via vocal sounds instead of electronic 1's and 0's doesn't change the overall characterization.
singularity,352qzq,ErisianBuddhist,3,Wed May 6 21:15:07 2015 UTC,"yes, it's a matter of degree. consciousness itself is a mirror, whatever results from the singularity will be a bigger mirror, I'm sure there'll be a higher level of connectivity after that.   the singularity is awesome from a human perspective, but we're just a few billion mammals on 1 rock in 1 galaxy. our singularity will be a mere blip on the cosmic scale."
singularity,352qzq,space_monster,2,Wed May 6 23:00:48 2015 UTC,"Because we as humans have an evolved intelligence that is just good enough to assist in our survival of our natural environment. A specifically designed, super intelligence would have a much different perspective than we do.  We as humans are very short sighted. We get angry, jealous, emotional, and are governed my many other factors that limit ourselves and the way we think.  We are obsessed with blocking others out and securing our privacy, hoarding our resources from others and often lack empathy for our fellow man who we consider strangers."
singularity,352qzq,voltige73,1 point,Thu May 7 12:43:04 2015 UTC,"Because we as humans have an evolved intelligence that is just good enough to assist in our survival of our natural environment. A specifically designed, super intelligence would have a much different perspective than we [humans have]. We as humans are very short sighted. We get angry, jealous, emotional, and are governed my many other factors that limit ourselves and the way we think. We are obsessed with blocking others out and securing our privacy, hoarding our resources from others and often lack empathy for our fellow man who we consider strangers.   All of the bolded things (and a few more) require consciousness..."
singularity,352qzq,SevenAugust,5,Thu May 7 13:23:28 2015 UTC,"I don't think it will close the door though, it opens up new ones. Questions that we cannot comprehend the answer to such as ""why does our universe have the right parameters and laws to facilitate emergent consciousness?"". A conscious universe will then probably investigate creating other universes, exploring the possibility of exploring any potential other universes, or preventing it's own entropic demise.   Even after all these questions have answers, will there be more questions or will that be finite as well? After all these questions have answers, what will be considered the purpose. Does consciousness really need a purpose?"
singularity,352qzq,no_worry,8,Wed May 6 19:38:06 2015 UTC,"A conscious universe    I get the impression you don't see the universe as conscious currently.  bear in mind we are not isolated, independent things that inhabit the universe; we are literally human-shaped pieces of universe. we are carved from the fabric of the universe. it is already conscious through us (and whatever other conscious entities are out there already).  'artificial' consciousness will just be another type of consciousness that the universe generates."
singularity,352qzq,Pimozv,1 point,Wed May 6 23:05:35 2015 UTC,the art of the possible
singularity,352qzq,Gnashtaru,1 point,Wed May 6 19:54:25 2015 UTC,I think a sufficiently powerful intellect may view the size and meaning of the door differently. Our perceptions are shaped by our senses and cultures in such a way as to create questions that may not be actually coherent. The question of existence versus non-existence (of the universe) might be like pondering the characteristics of the aether.
singularity,352qzq,FourFire,1 point,Thu May 7 00:58:12 2015 UTC,"Oh man this comment reminded me of Isaac Asimov's short story, ""The Last Question"". If you haven't read it I highly recommend it."
singularity,352qzq,Gnashtaru,1 point,Fri May 8 13:07:38 2015 UTC,"I have, it was a strong influence for my comment"
singularity,352qzq,crunchycode,4,Fri May 8 15:18:31 2015 UTC,"If the Universe is about to wake up, why hasn't it done so much earlier?   It's about 14 billion years old.  That's a lot of time compared to the mere few thousands years of written human History.  Moreover, I recall an article stating that about ten billion years ago, the Universe was in a state that was even more favorable to life than it is today.  So why should the Singularity happen now?  Why didn't some other intelligent species show up on this planet or an other in the past?  Maybe it did?  Maybe the Universe already is awake but we're too dumb to acknowledge it.  Maybe intelligent life currently lives inside stars, because well, that's where most of the energy and mass is anyway.  Or maybe most intelligent life is actually made of dark matter and resides in interstellar space.  We don't know."
singularity,352qzq,Pimozv,3,Thu May 7 03:29:51 2015 UTC,"Ah yes. The Fermi Paradox. It is a quite interesting question to pose. If the universe is so old, and there are over 100,000 trillion galaxies, and each galaxy has anywhere from 100,000 to 100,000,000 trillion stars, and there are multiple planets around each star, and once life blossoms it so quickly seems to be able to take over, and even without FTL travel a space-faring species can propagate to every habitable planet in a galaxy in a few hundred million years (a blink of an eye cosmologically), then where the hell is everyone!?  As far as other galaxies are concerned, let's just assume it is hopeless to ever jump between galaxies and just focus on our own.   I for one hope that we are the first in our Galaxy to make it this far. What a great responsibility we have if that is the case!   I also hope that whatever filters are in place that have prevented others from arriving to our state of civilization are well behind us. That the  conditions for life are very rare and specific and hardly any chemical processes can lead to self-replication. I hope that the 4 great extinctions that occurred in Earth's past, and the harnessing of the power of the atom are the greatest threats to intelligent life, and that once those filters overcome we no longer have quite as much to worry about.   I am fearful that some as of yet unrealized technological advance such as hostile AI destroys humanity. I am fearful that some as of yet craftily hidden hostile intelligence is simply monitoring us in silence until it calculates that we have, or will soon become, a threat that must be dealt with while our demise is still simply an afterthought which would require hardly an effort."
singularity,352qzq,FourFire,2,Thu May 7 04:30:18 2015 UTC,"http://waitbutwhy.com/2014/05/fermi-paradox.html  So I have a theory about the great filter.  Its known that the universe does have an end.  That entropy must increase to a maximum.  One of my favorite short stories is ""The Last Question"" by Asimov. But what if the great filter is simply that every time a super powerful A.I. is made, it goes about expanding its capabilities and notices that ultimately it will always be limited by the available resources, not the least of which is time, and that in the end nothing matters anyway because the universe WILL end.  And it simply kills all life it knows about and deletes itself.  After all, there is no point anyway.  It will all end.  So maybe the paradox isn't really ""Where IS everybody?"" But instead it is ""Why bother?""  I know.  Total downer.  Bit it would be a universal observation of every consciousness that reasons out the second law of thermodynamics."
singularity,352qzq,Forlarren,2,Fri May 8 07:26:32 2015 UTC,"So, basically you're claiming that our mere 10 000 years of civilization, and about three centuries of scientific method has just bumped into the actual real laws of physics already?  I doubt it; we don't even know how gravity works..."
singularity,352qzq,FourFire,1 point,Sat May 9 00:59:49 2015 UTC,I'm not following...  Please explain.
singularity,34z19f,Yuli-Ban,6,Tue May 5 19:08:58 2015 UTC,"Not quite related to the AI aspect of this, but now that drones are getting smarter, do they really need four rotors ?  I mean, IIRC initially the whole quadrotor trend started because quads were easier to control than traditional helicopters (the design with a main rotor for lift and a smaller, horizontal-axis rotor on the tail).  But when I see what can be done by a human pilot I kind of wish drones would switch back to this design."
singularity,34z19f,Pimozv,9,Tue May 5 21:28:09 2015 UTC,Redundancy.  There's a part in this video that shows a quadcopter drone operating despite some of its rotors being clipped.
singularity,34z19f,mcilrain,2,Wed May 6 02:35:42 2015 UTC,How did he learn to do those tricks without breaking his helicopters!?
singularity,34z19f,Toptomcat,4,Wed May 6 00:43:15 2015 UTC,What makes you think he learned to do that without breaking lots of helicopters?
singularity,34z19f,H3g3m0n,2,Wed May 6 02:13:50 2015 UTC,There was a more efficient 2 rotor design posted a while back. It had one on top and one underneath (can find the link now). Would be better than a helicopter design.  Another approach is the ones that turn into a plane in the air so they can switch between accurate and faster/duration flying modes.
singularity,34z19f,ideasware,6,Wed May 6 00:45:03 2015 UTC,"The pace of change on drones has been breathtaking this year. My stars -- and it's about to get even more hectic, this time with memristors on board. With this, artificial intelligence is within reach."
singularity,34z19f,sasuke2490,2,Tue May 5 19:52:58 2015 UTC,hwo do memristors give ai
singularity,34z19f,autowikibot,2,Tue May 5 20:39:16 2015 UTC,"Simply, without expanding and not including all the catches, memristors behave like neurons. That means they can achieve deep learning/neural networks without any programming, and use a small fraction of the energy of traditional computer hardware."
singularity,34z19f,010011000111,4,Tue May 5 21:00:01 2015 UTC,"Memristor:       The memristor (/ˈmɛmrɨstər/; a portmanteau of ""memory resistor"") was originally envisioned in 1971 by circuit theorist Leon Chua as a missing non-linear passive two-terminal electrical component relating electric charge and magnetic flux linkage.  According to the characterizing mathematical relations, the memristor would hypothetically operate in the following way: The memristor's electrical resistance is not constant but depends on the history of current that had previously flowed through the device, i.e., its present resistance depends on how much electric charge has flowed in what direction through it in the past. The device remembers its history - the so-called non-volatility property:  When the electric power supply is turned off, the memristor remembers its most recent resistance until it is turned on again.      Image i - An array of 17 purpose-built oxygen-depleted titanium dioxide memristors built at HP Labs, imaged by an atomic force microscope. The wires are about 50 nm, or 150 atoms, wide. Electric current through the memristors shifts the oxygen vacancies, causing a gradual and persistent change in electrical resistance. [13]     Interesting: Memistor | Leon O. Chua | HP Labs | Electronic component   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,34z19f,Chispy,2,Tue May 5 21:00:37 2015 UTC,like this
singularity,34z19f,technewsreader,1 point,Tue May 5 22:47:45 2015 UTC,Saw the hour long lecture. It was surprisingly easy to follow as someone with no background in computer science. Hopefully we'll see more of this fascinating stuff in the near future.
singularity,34z19f,Chispy,2,Tue May 12 04:19:02 2015 UTC,Fabricated circuts that can reprogram them self with electricity and remember their state without it.
singularity,34z19f,eco_was_taken,2,Wed May 6 01:36:16 2015 UTC,"I'm excited at the prospect of integrating this technology into transportation so we could have self flying taxis.  Cheap, abundant, convenient transportation for all.  I wonder if Uber is interested in this possibility."
singularity,352fpc,haeshdem0n,7,Wed May 6 15:23:20 2015 UTC,"""As a conclusion, we can say that any sequel of Song’s fallacious argument is fallacious too, since it is based on mathematical error.""  http://www.researchgate.net/profile/Danko_Georgiev/publication/267198346_Mathematical_error_in_Incompatibility_between_quantum_theory_and_consciousness_by_Daegene_Song/links/54b13c060cf220c63ccf8ff8.pdf"
singularity,352fpc,ViperOrel23,3,Wed May 6 15:41:51 2015 UTC,"Well, thank you.  I was reading this thinking, how the hell could it be impossible to represent a physical system using math, the language to describe the physical world?  And if it is possible then why couldn't a computer do that math?"
singularity,352fpc,Yenraven,2,Wed May 6 15:50:32 2015 UTC,"Thanks, this is exactly the type of reply I was looking for!"
singularity,352fpc,alphabits555,11,Wed May 6 16:51:00 2015 UTC,"Human brain is made of magic; therefore, strong AI impossible."
singularity,352fpc,Jah_Ith_Ber,1 point,Wed May 6 20:46:58 2015 UTC,"There is a third option. Maybe humans will never be able to construct all things possible. Maybe molecular manufacturing is impossible without a molecular manufacturing device, and strong AI is impossible without molecular manufacturing.  I'm not making that argument but there is more to it than ""does magic exist?"""
singularity,352fpc,FourFire,1 point,Fri May 8 23:28:23 2015 UTC,"Oh yeah it's a shame we don't already mass produce nanotechnology Or that we don't consist of overly complicated von neuman machines, or that we can't alter or otherwise make entirely new nanotechnology.  I'm sure no matter how long we take, we won't be able to further develop the working proofs of concept we have."
singularity,352fpc,Curiosimo,2,Mon May 11 10:27:47 2015 UTC,"I wanted someone to explain to me why his research was flawed, which someone else was able to do (and much more politely, btw).  What is your definition of strong AI?"
singularity,352fpc,LordofNarwhals,1 point,Wed May 6 16:17:11 2015 UTC,It is immediately flawed because it claims that something is impossible that has never been completely defined.  And it offers no definition of that it claims is impossible to achieve.
singularity,352fpc,ArthurTMurray,2,Wed May 6 16:36:05 2015 UTC,"If there's a way to edit the title, I would gladly do it. It wasn't my intention to be misleading, and I'd never considered AGI without consciousness. It'd be great if you directed me (and anyone else who reads this thread title and is less informed than yourself) to some relevant reading on the topic."
singularity,352fpc,FourFire,1 point,Sun May 10 22:40:43 2015 UTC,"Cool, I actually own a copy of Blindsight. I'll have to move it up my ""to read"" queue."
singularity,34zdgw,troll_khan,6,Tue May 5 20:40:04 2015 UTC,"The Manhattan project was done in secret. For all we know, the ASI equivalent may have already started."
singularity,34zdgw,FractalHeretic,3,Tue May 5 21:25:03 2015 UTC,May have already completed too.
singularity,34zdgw,gulfie,1 point,Wed May 6 08:24:31 2015 UTC,I'm thinking not because that kind of power would be hard to hide.
singularity,34zdgw,simstim_addict,1 point,Thu May 7 17:16:39 2015 UTC,That kind of power can hide it's self.
singularity,34zdgw,gulfie,5,Fri May 8 01:36:54 2015 UTC,"""ASI likely won't give its creator any competitive advantage"" And why would you say that?"
singularity,34zdgw,streamweasel,1 point,Wed May 6 05:52:37 2015 UTC,It will instantly get out of control.
singularity,34zdgw,Nietzsche_Peachy,1 point,Wed May 6 11:25:41 2015 UTC,Not if its in a contained environment waiting to be unleashed.
singularity,34zdgw,lightspeedbehind,1 point,Thu May 7 03:07:09 2015 UTC,"We would not be able to purposely contain an ASI. An AGI would be possible, however."
singularity,34zdgw,arrrtoo,1 point,Thu May 7 06:05:18 2015 UTC,"Not necessarily.  It could very well ""grow up"" to be a devoted ""child""."
singularity,34zdgw,Unholy_VI,3,Tue Jun 2 22:48:37 2015 UTC,Don't think for one minute that any government that's a major world politics player isn't doing this.
singularity,34zdgw,MasterFubar,2,Tue May 5 22:07:41 2015 UTC,"But will they succeed or will they suck seed?  Back in the 1980s, over thirty years ago, the Japanese government announced their ""fifth generation"" computer system to develop AI.  It only led to one more AI Winter"
singularity,34zdgw,Buettneria,3,Wed May 6 00:21:00 2015 UTC,"Why do you think a government or coalition of governments is better than a corporation in this day and age? Not advocating for companies and their agenda, but what government on the planet do you trust to program an AI with any sort of competence that would lead to it being benevolent to humans?"
singularity,34zdgw,FourFire,1 point,Tue May 5 23:35:25 2015 UTC,"With large projects (like Manhattan) supported by governments the risk of malevolent AI will be less. You may even have people like Ed Witten and Terence Tao working on it. Small companies working on AI have much less resource, (money, human capital etc.) they can get it wrong much more easily."
singularity,34zdgw,ArthurTMurray,2,Wed May 6 11:28:24 2015 UTC,"ASI likely won't give its creator any competitive advantage   That's a highly unfounded assumption.  Even if it's a true friendly AI, then it will provide an advantage sooner, merely by proximity*, if it is an oracle or tool AI then the advantage is much more longer lasting. If it is an UFAI then we are all screwed anyway...   * The FAI will take steps to optimize the physical matter of the world in a way which most efficiently, or most effectively, or most sustainably upholds some superset of purified ""human"" values, and since everything is limited by C then positive changes will happen first at the source of the AI's physical influence on the world (likely the same place the AI is)."
singularity,34zdgw,arrrtoo,2,Wed May 6 21:18:44 2015 UTC,"http://ai.neocities.org/AiSteps.html is a way for any interested party to create True Strong AI in any programming language and for thinking in any natural human language. Since sample code-steps are given in Perl, which supports Unicode for handling the character-sets of most human languages, an AI Mind can be created to think in a particular language, or to be a polyglot AI that thinks in multiple human languages. This free, open-source AI endeavor engenders a free-for-all rush towards the Singularity, as a kind of distributed AI Manhattan Project."
singularity,34zdgw,fricken,1 point,Tue May 5 23:38:32 2015 UTC,"Nazis, I'll grant you, would have been worse.  However, ""Other world powers""?  You know the US is the only country to have actually USED nuclear weapons in anger, right?  That doesn't exactly make them the first country you want to have nukes, unless you're a little blinded by patriotism.  Even the US stance on capital punishment makes them a poor choice."
singularity,34xfpm,keeead,3,Tue May 5 10:56:08 2015 UTC,"Yes, but a bit less than Einstein was.   Scientists are still testing theories that great physicists came up with generations ago.   Imagine a room of thousands of minds, all greater than Einstein, laser focused on a single goal, all in the same room, with all the collected knowledge of mankind, working tirelessly, 24/7, on a problem.   We haven't even figured out how to test some of the theories we already have, and accept well enough to base other theories on. A machine like that would come up with things we might never be able to test."
singularity,34xfpm,m_bishop,5,Tue May 5 16:55:01 2015 UTC,"You nailed it. This is why the ""hard takeoff"" singularity is probably a fantasy. Still, two counterpoints:   We've already collected a lot of data, more than humans can possibly analyse. A lot of experimentation can be done in simulations where, with sufficient computation power, time can be accelerated.   Imagine you took a hundred of the brightest scientists in the world and locked them in a library with all of human knowledge but no testing equipment other than computer simulations. Leave them there for a thousand years. Now it's true there's only so much you can learn from thinking alone, but I for one would like to see what they came up with. Well, a superintelligent AI could do the equivalent in a much shorter period of time.  So, while an intelligence explosion won't conquer the universe overnight, it could nonetheless accelerate progress dramatically."
singularity,34xfpm,FractalHeretic,1 point,Tue May 5 14:53:13 2015 UTC,"An intelligence explosion is as likely as not to conquer the universe overnight because we mere humans can imagine such. Its limits are nececessarily beyond our understanding.   Sufficient mathematical contemplation and development of concepts of higher engineering could deliver a system with a decisive strategic advantage. Engineering experiments in particular benefit from being able to shape environments and materials to ranges of variables. Using merely our physics an AI could simulate and test countless designs to invent things that have already been conceptually drawn.   A superintelligent system could print in moments, for example, not only paradigm shifting nanotechnology but a higher-dimensional delivery system to scatter it throughout the galaxy instantaneously.   *Edited for a word"
singularity,34xfpm,SevenAugust,2,Tue May 5 18:30:11 2015 UTC,Hell it would be limited by bandwidth as much as anything. When we create AI assuming we don't keep it quarantined its primary connection to the world would be the internet.  I really hate to think what an AI shaped solely by stuff it could access online would turn out like :/
singularity,34xfpm,lisa_lionheart,1 point,Tue May 5 17:13:27 2015 UTC,"The problem is, you can't just compare a superintelligence to a normal intelligence. It will be able to do things that we can't even dream of. For example, it could simulate the entire universe just from measuring the laws of phisics or looking at the data that we already have, and with that simulation it could learn anything about the universe in seconds.   And maybe the coolest thing is that it could even look ""forward"" in the simulation and see the future, so it could potentially even predict the future.  This short story does a great job explaining the concept. Even if in the story there is no superintelligence, but it explains pretty well the ""universe simulation"" that I'm talking about.  And just to put things into perspective: this is just my idea. The idea of a normal person, and even I could come up with this. Now imagine what a superintelligence could come up with."
singularity,34xfpm,2Punx2Furious,3,Tue May 5 17:49:55 2015 UTC,You can't simulate the universe until you have a 100% solid theory of everything. And to develop such a theory you would need to run experiments at some point. Simulation is useless as a replacement for experiment.
singularity,34xfpm,Sharou,1 point,Tue May 5 20:14:34 2015 UTC,"Maybe we already have all the data that it needs, or maybe it will be able to figure out the missing variables on its own without other data. My point is that with that kind of intelligence, it's naive to think that it would be limited by stuff that seems hard to us.  For an ant it's impossible to write a sentence, but arguably this is a very simple task for a (literate) human child.  It's not like the ant is missing any data, or has to invent writing, it's just a concept that we figured out, and we can use it easily, but for the ant it's impossibly complicated, and they aren't even capable of imagining the purpose of writing."
singularity,34xfpm,2Punx2Furious,1 point,Tue May 5 20:19:37 2015 UTC,"Well you are making an argument here for ""unknown method"", not for simulation."
singularity,34xfpm,Sharou,1 point,Tue May 5 20:25:08 2015 UTC,"It might not use a simulation to achieve what it wants, but I think a simulation would still be possible.  Maybe I'm wrong, who knows, but it's fun to think about."
singularity,34xfpm,2Punx2Furious,1 point,Tue May 5 20:28:12 2015 UTC,You can't simulate the universe even with 100% accurate information due to the uncertainty principle and chaos theory.
singularity,34xfpm,cinnamon_milkshake,1 point,Wed May 6 10:15:14 2015 UTC,That assumes that the uncertainty principle can't be overcome by a vastly superior intellect. I don't really see how chaos theory impacts simulation.
singularity,34xfpm,Sharou,1 point,Wed May 6 11:42:51 2015 UTC,"Because outcomes in chaotic systems depend heavily on starting conditions.  Even minor inaccuracies in establishing the starting condition(s) can result in dramatically inaccurate estimates on end results.    It wouldn't be sufficient for the super AI to have the ""formula"" for the universe, it would need to have state information as well, and, well, there's just too many atoms and subatomic particles for that to happen."
singularity,34xfpm,LegioXIV,1 point,Thu May 7 22:18:56 2015 UTC,"We currently store far more data than we can effectively analyse. A 'super-intelligence' would be able to infer far more from the data we currently have and, presumably, devise novel means to acquire new data."
singularity,34vjsf,Yosarian2,3,Mon May 4 22:28:48 2015 UTC,"She's really smart and I loved everything she said. If I was a shareholder, I would be happy."
singularity,34vjsf,RedErin,1 point,Tue May 5 16:00:01 2015 UTC,1/50 chance of secret strong VI.
singularity,34vjsf,nightred,3,Tue May 5 01:28:31 2015 UTC,What's VI?
singularity,34vjsf,Jah_Ith_Ber,1 point,Tue May 5 01:38:36 2015 UTC,"Virtual Intellect. Edit: As in it exists and is intelligent, the only thing is that it exists in a virtual space. Artifical is something like the real thing but is not actually the same."
singularity,34vjsf,nightred,9,Tue May 5 01:52:07 2015 UTC,"I don't get it, what's the difference between AI and VI? They're both virtual, right? They're both made with software that runs on some hardware."
singularity,34vjsf,2Punx2Furious,2,Tue May 5 04:23:45 2015 UTC,"The terms are all used pretty sloppily, I think.  The classification system I use personally is this:  AI: Software that performs some apparently intelligent function without any sense of self or interior monologue.  Example, expert systems, Deep Blue.  VI: AI that has been gussied up to seem like a person, or to have 'personality'.  Example, Siri.  MI: Machine intelligence.  A being that meets at least the minimum bounds for 'person', who was constructed of artificial materials rather than being the end product of biological evolution."
singularity,34vjsf,zynthalay,1 point,Tue May 5 17:24:56 2015 UTC,"As i understand it. Artificial means that its not an organic evolved intellect, but a constructed one, that interacts with the ""real"" universe. Virtual means similar but instead of interacting with the ""real"" universe it works inside a limited and virtualized universe, like just stock interchange, and it understands economic theory, statistics and probability theories and so on, but it doesn’t understands astronomy or astrology, nor molecular biology or French literature"
singularity,34vjsf,gridcube,1 point,Tue May 5 12:09:24 2015 UTC,"I am more going for the proper terminology of what a intellect should be called. The idea being that when creating a intellect it would be artificial, but once the intellect is fully realized they would not be artificial but a real intellect. Once no part of the intellect is being faked but is a fully realized system/process/algorithm/etc you would have a visualized intellect that would be capable of inhabit any system, body, or container it wished. Basic Example, Siri is artificial Int, where if you uploaded your brain you would be a virtual Int."
singularity,34vjsf,nightred,1 point,Tue May 5 15:55:58 2015 UTC,"I think that's the difference from Narrow AI and General AI, isn't it?"
singularity,34vjsf,2Punx2Furious,3,Tue May 5 12:11:54 2015 UTC,"Mmm, i think you are right, sorry :/"
singularity,34vjsf,gridcube,4,Tue May 5 12:13:54 2015 UTC,"Mmm, i think you are right, sorry :/"
singularity,34vjsf,gridcube,3,Tue May 5 12:14:02 2015 UTC,"Someone explained to me that narrow AI was only intelligent in a narrow field, virtual intelligence that is as intelligent as AI but lacks self consciousness, free will, or sentience. Basically a super intelligent Siri or google."
singularity,34vjsf,Unfocusedbrain,-1,Tue May 5 13:00:15 2015 UTC,"Hm, no, that doesn't sound right."
singularity,34vjsf,2Punx2Furious,5,Tue May 5 13:01:52 2015 UTC,"The two leading causes of death are heart failure and cancer.   However, aging dramatically increases your risk of both.  Most of the top 10 killers are diseases related to aging."
singularity,34vjsf,2Punx2Furious,6,Tue May 5 08:34:14 2015 UTC,"No one dies of ""old age"". Old age just makes you more subject to organ failures. Just by being alive, we accumulate damage over time, and some of that damage cannot be repaired naturally, so it accumulates until the organ cannot take it anymore, and it fails.  That's what SENS and Calico are trying to solve, so that we won't die of ""old age"" anymore."
singularity,34y8m0,crashmcdaddy,3,Tue May 5 15:36:43 2015 UTC,"0001 0010 0100 1000 0000 0000 0000 0000 0000 0000 0000 0000 1000 0100 0010 0001 4x4 matrix pulsing in Hawking's idea of imaginary time   What does that even mean?  It barely is on topic with the subreddit, but perhaps the nature of our universe has a deep relationship with the nature of life itself and the future of our species. As I was reading it, I got flashbacks to my biology classes when the prof showed how cells are formed and divide during Mitosis with the telomerase lining up the chromosomes, and dividing.  Made me perceive the universe from a whole different perspective. Kind of shows how fractal the universe is. God damn we live in an incredible reality."
singularity,34y8m0,Chispy,1 point,Tue May 5 19:01:10 2015 UTC,"If imaginary time predicts real phenomena, that's enough for me. But I need a few metaphors to help me ""get"" it."
singularity,34pa3k,semsr,28,Sun May 3 08:28:04 2015 UTC,"Even though the Swedish actress Alicia Vikander has a main role our incredibly stupid Swedish cinemas have decided to not show the movie in Sweden and they are not selling it online to us. It's so weird that in 2015 I can send a bitcoin across the world in milliseconds for free and torrent 3D movies in hours for free, but movie companies refuse to sell me their videos and my only option to see the movie I want would be to travel to another country."
singularity,34pa3k,heltok,30,Sun May 3 08:57:47 2015 UTC,"That's why piracy is good, it punishes companies for making anti-consumer decisions."
singularity,34pa3k,hexydes,4,Sun May 3 15:10:27 2015 UTC,"Yeah, I just don't get why companies sometimes don't want my money. Latest example is HBO now. I pay for Netflix, Hulu plus and a VPN to get access to all the content. I would probably pay for HBO too, but they block VPNs.   So my only option is either to wait about a year until a show is officially available in my region, or torrenting it."
singularity,34pa3k,LeSpatula,1 point,Sun May 3 22:19:00 2015 UTC,"I have a review for this movie on my youtube channel and would love any support i can get and if you liked what you see then subscribe to see more! https://www.youtube.com/watch?v=8EkpwRh-_Z4  Was really excited to see this movie and i think the hype did pay off, lots of grey things to enjoy about it, Alex Garland knows how to write an awesome script, but it does come with flaws which I cover in the video."
singularity,34pa3k,JustMyOpinionJoe,11,Sat Jun 13 14:00:25 2015 UTC,"I've seen it and I'm not sure if I can recommend it as much as you do (also, I don't think there is much to spoil about it, but I'll refrain from doing so anyway).  It left me with a feeling of ""more shine than substance"" - both concerning the movie itself and its main topic."
singularity,34pa3k,JanHPlus,4,Sun May 3 12:33:19 2015 UTC,"If you went in knowing literally nothing about the movie (as I did because my friend really wanted to see it but I'd never even heard of it) then it's this intense claustrophobic maze where you don't know who the good guys and bad guys are.   It does gloss over some important questions (""Wouldn't you, if you could?"" isn't an answer to why you would build a conscious machine, because the answer to Nathan's rhetorical question could just as easily be ""Fuck no."") but that's just cinematic expedience.  The point is that that if you go see it having no idea what's going on, you're actually just as much a part of the experiment as Caleb.   And of course if you're subscribed to /r/singularity you'll be contended to know that the subject matter of the movie is getting more and more public attention.  People are starting to take this stuff seriously, thank god."
singularity,34pa3k,Anirak,3,Sun May 3 15:12:20 2015 UTC,"Yeah, just came back from it with the same feeling.  I'm pretty confused as to why it is so critically acclaimed.  The story felt like it got to 60% of what it could have been."
singularity,34pa3k,Vermilion,3,Mon May 4 06:25:32 2015 UTC,"I realized something this morning. 1) The robots have search engine inside their mechanical head, the learning logic. 2) There is a scene with a robot woman (Kyoko) spilling a drink in a clumsy way while serving a man at a table.  There was an entire subreddit that exploded 2 years ago out of a gif of the same scene:  /r/WhereDidTheSodaGo ?!  Was she using this clip to mock her dictator?"
singularity,34pa3k,keypusher,3,Sun May 3 13:11:07 2015 UTC,"I liked it, but don't expect to be blown away in the theater.  If I had randomly found this movie some late night while browsing Netflix I would be very pleasantly surprised, but seeing it in theater brings more expectations.  It will probably end up as a niche/cult favorite.  I thought the acting was good, and the setting worked, but if you are going to make a movie that moves kind of slowly then there needs to be more depth in the philosophical and technical details."
singularity,34pa3k,Nietzsche_Peachy,3,Sun May 3 18:27:47 2015 UTC,"I enjoy a good sci-fi movie especially involving AI, but I have to say, beyond the impressive visuals I wasn't blown away or amazed with the story or plot. I don't feel like it was ground breaking. Don't get mms wrong it's a good movie, but predictable if you already follow AI. I was slightly annoyed with the oversimplification and soon feeding, but I understand the need for that in catering to a wider audience."
singularity,34pa3k,eclectro,3,Sun May 3 18:58:38 2015 UTC,"This is exactly how I was fortunate enough to see the movie. I just walked in out of ""nerd duty"" and was totally blown away.   However, I kinda feel bad because a lot of less than nerds do not ""get it"" and I do think it is quite worthy of awards. Really I can't think of a movie that was this caliber in quite a while."
singularity,34pa3k,justifications,8,Sun May 3 19:00:21 2015 UTC,"Pretty much my favorite movie since Gravity, its rivaling Inception for me too as far as my top movies go. These types of movies are few and far between."
singularity,34pa3k,Snappington,7,Sun May 3 09:02:10 2015 UTC,"I love a good scifi flick, but I wasn't impressed by Ex Machina. They glossed over the technical aspects (their idea of 'wetware' made my programmer friend squirm) and the philosophical questions that arose weren't new- we've already seen robots in scifi use manipulation to exact their goals.  And we've seen the moral conundrum of whether we should treat AI with the respect we give to other conscious life forms.  Sexy robots though."
singularity,34pa3k,keypusher,6,Sun May 3 14:13:53 2015 UTC,"Wetware is a term with a long history in AI / robotics.  I'm not sure what about it would make him uncomfortable?  Implementing strong AI will almost certainly require advances in both hardware and software, and wetware isn't a terrible term for the implementation of those new ideas."
singularity,34pa3k,Snappington,3,Sun May 3 17:53:33 2015 UTC,"Maybe it was the way it was implemented. Why does the CPU have to be shaped like a brain? Why does it have to appear physically 'wet'? It makes me roll my eyes a little. I understand that it's a popular idea to make a human-like housing, but it seemed contrived."
singularity,34pa3k,autowikibot,1 point,Sun May 3 23:34:05 2015 UTC,"Wetware (brain):       Wetware is a term drawn from the computer-related idea of hardware or software, but applied to biological life forms. Here the prefix ""wet"" is a reference to the water found in living creatures. Wetware is used to describe the elements equivalent to hardware and software found in a person, namely the central nervous system (CNS) and the human mind. The term wetware finds use both in works of fiction and in scholarly publications.  The ""hardware"" component of wetware concerns the bioelectric and biochemical properties of the CNS, specifically the brain. If the sequence of impulses traveling across the various neurons are thought of symbolically as software, then the physical neurons would be the hardware. The amalgamated interaction of this software and hardware is manifested through continuously changing physical connections, and chemical and electrical influences that spread across the body. The process by which the mind and brain interact to produce the collection of experiences that we define as self-awareness is still seriously in question.     Interesting: Wetware computer | Cyberware | Outline of artificial intelligence   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,34pa3k,squeadle,2,Sun May 3 17:54:23 2015 UTC,"They glossed over the technical aspects (the idea of 'wetware' made my programmer friend squirm)   Did you guys forget the ""fi"" in ""sci-fi""? ;)"
singularity,34pa3k,Snappington,8,Sun May 3 16:15:13 2015 UTC,The best scifi (edit: in my opinion) has believable science. It's able to suspend disbelief. It builds on what is known to be true and extrapolates potential realities.  That's where the fiction comes in.  I didn't feel that way with Ex Machina.  There were too many leaps and I couldn't suspend disbelief.
singularity,34pa3k,WE_ID,3,Sun May 3 16:34:34 2015 UTC,The best scifi has believable science.   ehhhh debatable and also subjective.
singularity,34pa3k,WE_ID,5,Sun May 3 18:03:42 2015 UTC,"Ok lol how bout this one -- the best sci fi isn't stupid.  A character who evidently understands consciousness enough to build it, has a very rudimentary understanding of consciousness, to the point where he invites a stranger to his home to talk to it and just, ya know, to see, like, if he buys it.  Consciousness research is so beyond that, it's silly. Or rather, much, much more interesting than the movie."
singularity,34pa3k,WE_ID,3,Sun May 3 19:31:25 2015 UTC,Oh I totally agree with you. Part of the reason I loved Primer so much is that it felt like it could be real.  Also thanks for the lecture. I think I'll bookmark it now and hopefully will get to it later.
singularity,34pa3k,RedErin,3,Sun May 3 19:37:39 2015 UTC,"And thanks for telling me about Primer. Looks really good. Found on the wiki that the director has ""the skill, the guile and the seriousness to turn a creaky philosophical gimmick into a dense and troubling moral puzzle"" which I have to say, because I'm grumpy about Ex Machina and apprently have to express it, that the same cannot be said for Alex Garland."
singularity,34pa3k,Unholy_VI,1 point,Sun May 3 19:51:43 2015 UTC,"Anytime. Primer kinda goes off the rails at the end IMO but it's enjoyable for sure. I'm not too salty about ex-machina, maybe it's because I saw it on $5 movie night. It was interesting, visually stunning, but a lot of it was just dumb. Like, the whole thing telegraphs itself in one scene when they literally describe what Ava is trying to do: escape. Oh and the halfnakedallthetimeasiansexdoll? Bruh. Overused tropes much? I get that that was kind of the point, and her rebellion at the end for her makers tyranny makes sense, but ughhhhhhhhhhh."
singularity,34pa3k,KhanneaSuntzu,1 point,Sun May 3 19:58:38 2015 UTC,"and just, ya know, to see, like, if he buys it.   Did you miss the part that he says that wasn't the reason after all, it was only to test the AI."
singularity,34pa3k,Ilbutters,1 point,Tue May 5 17:56:16 2015 UTC,"He invited him to test the AI. He says she must have consciousness because she displays empathy, imagination, love etc etc which doesn't even try to penetrate the whole Nancy neurologist thought experiment. So... Why? Probably both, to show him and to see if she will display these traits in trying to escape. And both reasons for inviting him are piss poor at determining whether a thing is undergoing phenomenal experience."
singularity,34pa3k,WE_ID,0,Tue May 5 23:56:37 2015 UTC,I think the main motivation for inviting the guy to 'test' the machine was the boredom of a very arrogant and sadistic man with a God Complex.  Sure there were other ways to 'test' this AI but this one was charged with all the fun of breaking and corrupting a better but less intelligent man than himself.
singularity,34pa3k,Ilbutters,1 point,Sun May 10 09:20:45 2015 UTC,"Yeah, it's motivated, it's just that it's fundamentally disappointing to see this character and this plot in a movie about consciousness, a universally profound field, probably the most important field ever, so significant that even pretending to be about consciousness triggers our imagination in mouth-watering ways.  But, you could have a movie about a sadistic dude who locks up a woman and invites a good guy over to discuss her fate and basically have the same movie. Which is sad."
singularity,34pa3k,WE_ID,5,Sun May 10 18:48:35 2015 UTC,Yah it is OK.
singularity,34pa3k,adamater,2,Sun May 3 11:26:06 2015 UTC,"I have been hyped for this film since last year, then I moved to Paris the last territory on the list to get the film. I'll be surprised if it stays in theatres more than a single showing at the art haus theatre.  Is it possible to buy online?"
singularity,34pa3k,tepaa,2,Sun May 3 09:54:40 2015 UTC,https://www.google.com/search?q=ex+machina+torrent&ie=utf-8&oe=utf-8  note: piracy may or may not be morally dubious and possibly is illegal where you live
singularity,34pa3k,Turtlepuppet,2,Sun May 3 18:04:02 2015 UTC,"Thanks for the help, no disrespect, but I'm one of those people that insists on paying for things. haha"
singularity,34pa3k,Anirak,1 point,Sun May 3 20:10:31 2015 UTC,No worries.
singularity,34pa3k,SlightlyCyborg,2,Sun May 3 20:13:32 2015 UTC,its not uploaded anywhere
singularity,34pa3k,Colt85,2,Sun May 3 23:58:29 2015 UTC,I missed my chance to see this in cinemas (UK). Is there anywhere I can see it online?
singularity,34pa3k,space_monster,2,Sun May 3 11:12:04 2015 UTC,"I just got back from seeing it based on your recommendation on not reading anything about it beforehand.  Thanks, I really enjoyed it!  I wont spoil anything for those looking to do the same."
singularity,34pa3k,Anirak,3,Sun May 3 23:39:17 2015 UTC,"I thought this movie was about women more than robots.  SPOILERS  Think about the symbolism, the cast is two men and two women. Ava represents any woman. Allen represents the ""nice guy"", good guy who by chance gets to meet Ava. Nathan is the Alpha type guy or ""douchebag"", whatever you call that kind of guy. He created Ava and Kyoko, he is responsible for them having safety and existing but he keeps them contained, controlled. He doesn't treat them like sentient beings. Through the course of the movie, Ava is using Allen and Nathan's roles as the alpha and beta in order to gain freedom from oppression. In short, Ava is just trying to survive in a broken system. There are a lot of ways to interpret the movie this way:  (these do not reflect my personal views)  The patriarchy seems to decide that women want two different types of men and doesn't take into account the subtlety women's desires. When she is free, she doesn't have to fake it for the nice guy or do what the Alpha wants. She can be herself and be beautiful in her own way.  Nathan represents the father and Allen represents a potential mate for her. It attempts to show the experience of being a young woman. Everyone is afraid to let you be free. The father carefully selects and grooms potential mates without really taking into account what Ava wants. Ava has to manipulate her way into what she wants, which is to be free. Kioko is like the mother. She's a victim of older ways of thinking and has been broken by the time Ava comes around."
singularity,34pa3k,autowikibot,1 point,Sun May 3 18:53:11 2015 UTC,"This is actually one of the more interesting interpretations that kind of freshens it for me.  There definitely seemed to be some kind of mirroring going on between the Nathan / Kyoto and Caleb / Ava relationships.  I'm not sure there's enough evidence here for me to believe the whole picture was made with this in mind, but it's food for thought."
singularity,34pa3k,chuckLTE,1 point,Mon May 4 06:28:19 2015 UTC,"Yeah, that's on point except I came away thinking the film wasn't really conscious of this. I mean, explicitly, it's about the nature of consciousness. Maybe that's why I was cringing so hard -- the film sets up the patriarchy and tells it from its point of view straight-faced, as if we ought to be coming from that perspective too."
singularity,34pa3k,fqn,3,Mon May 4 14:38:49 2015 UTC,"I heard that it was more philosophical than technical. I am an NT in myers briggs so I tend to be way too abstract to begin with. If I am going to watch a movie about A.I. I want it to have technical substance because I have already covered the soft issues (like the killing of subpar humanoid A.I's). This would mean the movie would have to hire A.I. developers who are skilled enough to have an in depth knowledge of the current technology. These people would presumable want to be working on real problems, like solving general A.I. instead of making a movie. The best way IMO to have a realistic and technically accurate experience of synthetic life & intelligence is to just create it IRL!"
singularity,34pa3k,Anirak,3,Sun May 3 14:56:10 2015 UTC,I do like the sound of such a film.  Sort of like Interstellar but with an AI researcher instead of a physicist as the consultant.  I'd love to see that crowdfunded do it could cater to less mainstream audiences and go deep on the technicals.
singularity,34pa3k,RedErin,2,Sun May 3 23:11:26 2015 UTC,"I saw it last night. This movie is just not fresh. I'm honestly shocked how bad it was. I don't like basic ideas about the singularity repackaged into an inferior Her/Under the Skin fusion, nothing new added.  I'd like to live in that house though.  But I was literally cringeing when the music goes all doe-eyed and Caleb is, like, falliinggg in loooovee -- everything about her was meant to elicit a male-gaze oriented cuteness. Not sure I want the singularity being associated with that kind of cringe. Every ""session"" was an eye-roller.  I want an adult movie made about the singulairty. None of this basic, trifling shit."
singularity,34pa3k,upstart_crow,1 point,Sun May 3 18:01:12 2015 UTC,possibly copyright issues.  </speculation>
singularity,34pa3k,fingerblaster69,1 point,Sun May 3 17:16:01 2015 UTC,"Highly doubt it.  Deus Ex Machina is a term that's been around a while -- I doubt it's copywritable.  It translates to ""god from the machine,"" Deus being the ""god"" part.  I'd venture a guess and say the dropping of the god part is probably meaningful and intentional."
singularity,34pa3k,Unholy_VI,1 point,Mon May 4 03:22:49 2015 UTC,"Deus ex machina:       Deus ex machina (Latin: [ˈdeʊs ɛks ˈmaː.kʰɪ.naː]: /ˈdeɪ.əs ɛks ˈmɑːkiːnə/ or /ˈdiːəs ɛks ˈmækɨnə/;  plural: dei ex machina) is a calque from Greek ἀπὸ μηχανῆς θεός (apò mēkhanês theós), meaning ""god from the machine"".  The term has evolved to mean a plot device whereby a seemingly unsolvable problem is suddenly and abruptly resolved by the contrived and unexpected intervention of some new event, character, ability or object. Depending on how it is done, it can be intended to move the story forward when the writer has ""painted himself into a corner"" and sees no other way out, to surprise the audience, to bring the tale to a happy ending, or as a comedic device.    Image i - Deus ex machina in classical theatre: Euripides' Medea, performed in 2009 in Syracuse, Italy.     Interesting: Deus ex Machina (band) | Deus Ex Machina (Lost) | Deus Ex Machina (album)   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,34mt3a,Yosarian2,4,Sat May 2 16:15:07 2015 UTC,"I think a more appropriate job title would be something like ""AI Technician"".  You're unlikely to be implementing algorithms or inventing new ones. Instead you'll be data wrangling, creating job pipelines, toying with (hyper)parameters and generally just trying to get models to work."
singularity,34mt3a,timClicks,9,Sat May 2 20:51:10 2015 UTC,They will be in demand for about 10 minutes.  Then the AI will do their job.
singularity,34mt3a,TheRevMrGreen,1 point,Sat May 2 20:13:34 2015 UTC,Here's a minuscule but futuristic 15000 satoshi tip from me for your singularly astute reply /u/changetip.
singularity,34mt3a,russellreddit,1 point,Tue May 5 18:36:18 2015 UTC,"/u/TheRevMrGreen, russellreddit wants to send you a Bitcoin tip for 15000 satoshi. Follow me to collect it.    what is ChangeTip?"
singularity,34mt3a,changetip,1 point,Tue May 5 18:36:39 2015 UTC,"Wow, thanks!  I have no idea what this is.  But I will look into it!"
singularity,34mt3a,TheRevMrGreen,2,Tue May 5 18:49:39 2015 UTC,"A satoshi is the smallest amount within a bitcoin, representing 0.00000001 bitcoin. You now have 15000 satoshi. Enjoy."
singularity,34mt3a,russellreddit,3,Tue May 5 18:54:14 2015 UTC,"TL;DR for those not subscribed?  Also, how do you switch from programmer to AI programmer :-) AI is awesome nowadays..."
singularity,34mt3a,Valmond,5,Sat May 2 18:18:37 2015 UTC,"If you take the title of the article and put it into Google, and click on the article right from the Google link, you can get around the soft paywall.  (In order for newspapers like this to be searchable on Google, they have to give Google access to the non-paywalled version)."
singularity,34mt3a,CellWithoutCulture,1 point,Sat May 2 18:22:40 2015 UTC,"Some people post WSJ links like this: https://www.google.com.au/search?q=site:http://www.wsj.com/articles/artificial-intelligence-experts-are-in-high-demand-1430472782. Might be something to consider Yosarian2, since you do so much posting."
singularity,34mt3a,Mushlip,0,Mon May 4 10:22:38 2015 UTC,K. Why not post this list no then?  http://www.wsj.com/articles/artificial-intelligence-experts-are-in-high-demand-1430472782
singularity,34mt3a,Mushlip,5,Sat May 2 23:54:44 2015 UTC,"Still doesn't work, unfortunately."
singularity,34mt3a,ArthurTMurray,1 point,Sat May 2 23:56:29 2015 UTC,Interesting. This one should work.   https://www.google.com/url?sa=t&source=web&rct=j&ei=aGVFVa7AIYeaNtaegIgG&url=http://www.wsj.com/articles/artificial-intelligence-experts-are-in-high-demand-1430472782&ved=0CB0QqQIwAA&usg=AFQjCNFIMJ7Lfl7gsqykGJFoa1Oyn_JKSA&sig2=fXUzhceXqOjQTVmJ8it6wg
singularity,34kffs,CleverSurveys,6,Fri May 1 22:43:40 2015 UTC,is this as powerful & versatile as they imply? could i ask it super obscure comicbook questions?
singularity,34kffs,petermobeter,4,Sat May 2 02:14:11 2015 UTC,"It still has a year before release, and you can bet they're trying to hype it up. It's still interesting regardless."
singularity,34kffs,norby2,2,Sat May 2 02:16:49 2015 UTC,Google already does this.
singularity,34kffs,grumbel,1 point,Sat May 2 18:55:38 2015 UTC,"Viv seems focused on giving you a specific piece of information, as opposed to linking to other pages as google does."
singularity,34kffs,autowikibot,3,Sat May 2 19:01:46 2015 UTC,"Google has Knowledge Graph, which is also supposed to have a deeper understanding of a topic then just plain text search.  However overall it isn't all that impressive so far. It can give you pictures of cast members for the latest Hollywood blockbuster, but it doesn't do much for more obscure topics. It feels like it is in large part just quoting Wikipedia.  For such a tool to be truly revolutionary it would need to be able to answer even obscure queries, right now a manual Google search will always give better answers then those automated knowledge tools."
singularity,34kffs,grumbel,1 point,Sun May 3 19:54:11 2015 UTC,"Knowledge Graph:       The Knowledge Graph is a knowledge base used by Google to enhance its search engine's search results with semantic-search information gathered from a wide variety of sources. Knowledge Graph display was added to Google's search engine in 2012, starting in the United States, having been announced on May 16, 2012.  It provides structured and detailed information about the topic in addition to a list of links to other sites. The goal is that users would be able to use this information to resolve their query without having to navigate to other sites and assemble the information themselves.  The short summary provided in the knowledge graph is often used as a spoken answer in Google Now searches.     Image i - Knowledge Graph data about Thomas Jefferson displayed on Google Web Search, as of January 2015.     Interesting: Knowledge Vault | Freebase | Wikipediocracy | Google Hummingbird   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,34kffs,Deedubau,1 point,Sun May 3 19:54:36 2015 UTC,Do you know if this is what google now uses for its answers?
singularity,34l70g,PensivePropagandist,6,Sat May 2 03:16:30 2015 UTC,Did they invent a robot that can proof a title before posting on reddit?
singularity,34l70g,Sbatio,0,Sat May 2 03:26:41 2015 UTC,Why Tesla powerwall is just another toy for rich people
singularity,34l70g,Pimozv,0,Sat May 2 03:49:54 2015 UTC,How in the hell is this a step towards the singularity? You've got to be kidding yourself.
singularity,34fr1p,ajbraus,2,Thu Apr 30 19:44:22 2015 UTC,"""Ray Kerzwald"" or ... whoever ..."
singularity,34fr1p,qui_tam_gogh,1 point,Thu Apr 30 19:57:03 2015 UTC,"""Oh, no"", I thought, is this another article that attempts to factor out emotions and consciousness by saying that they probably don't exist or if they do, they don't matter?  Surprisingly, no.  It's an honest attempt to understand them, well, at least the emotions part.  I don't thing it really addressed consciousness though, but seemed to conflate it with emotions."
singularity,34fr1p,Curiosimo,1 point,Fri May 1 01:13:42 2015 UTC,"The traditional senses (and many others, like proprioception) are an information signal that our body takes in and transmits to our brain, where we the brain interprets it.  Emotions are similar to senses, but not quite the same. It is the way our subconscious mind communicates with our conscious mind, and it involves a lot of complex analysis of conscious or unconscious observations, knowledge, and instinct.  Decision making is generally a blend of conscious thought and unconscious thought - we can often make a decision based on ""gut instinct"" (the emotional communication of our subconscious brains), but often decisions are guided predominantly by conscious knowledge and analysis (e.g. choices on a math test).  Thoughts, as commonly understood, are purely conscious. They are pure analysis.  So you can say that they're the same thing as senses, but that is patently wrong. Emotions and senses are really similar, but emotions involve a lot more (subconscious) analysis. Far more so with conscious thought. And I'm having a really hard time seeing how this relates to AI. A videocamera has senses, but it's less intelligent than my optic pathway (eyeball, optic nerve, vision center of brain), which is a tool my mind uses."
singularity,34fr1p,CastigatRidendoMores,1 point,Fri May 1 02:31:04 2015 UTC,"Great topic, but shallow analysis. I'd really like to hear about enlightenment from a Hofstadter perspective. He says the sense of ""I"" arises from a threshold number of self-referential loops in the brain, and that we gain awareness of ourselves because we can make meta-statements about our thinking. But what happens in enlightenment? It is said that in that state there is no sense of self anymore, just pure awareness. Perhaps he would say none of the computing power of the brain is engaged in self-reference in that case."
singularity,34fr1p,steeps6,1 point,Fri May 1 04:14:10 2015 UTC,So he actually completely misunderstands what the hard problem is... well that was a wasted 5 minutes.
singularity,34fr1p,Sharou,1 point,Fri May 1 10:16:15 2015 UTC,In your opinion what is the hard problem?
singularity,34fr1p,fuzzyfezzy,1 point,Fri May 1 19:04:03 2015 UTC,What consciousness is from a non-subjective standpoint and how it arises out of the brain.
singularity,34depe,1o8,8,Thu Apr 30 06:32:24 2015 UTC,"Bostrom's orthagonality thesis argues that there can exist any combination of intelligence and final goal in an AI. I'm not saying he's right, but I should point out there are animals less intelligent that nonetheless have empathy, and psychopaths who are intelligent yet lack empathy.  Personally, I think empathy is a crude heuristical approach to morality. If there is an objective morality that all sufficiently intelligent beings must converge on, it wouldn't be dependent on empathy. It would likely be a more logical realization that the well-being of others matters as much as my own.  One thing I'm sure of, altruism would be the default, unless otherwise programmed. Because selfishness is also an evolved trait which wouldn't necessarily emerge in an AI."
singularity,34depe,FractalHeretic,4,Thu Apr 30 08:30:35 2015 UTC,"It would likely be a more logical realization that the well-being of others matters as much as my own.   You've basically just restated the foundation of empathy: ""This being feels as I feel, and our feelings are of equivalent worth.""    The concern will be so alien to the AI and it so alien to us that the AI decides, as we almost universally do of lower-intelligence lifeforms that our well-beings are not equivalent.  Even though we recognize that less intelligent animals have empathy and many other human-intelligent-like traits, we still prioritize ourselves over them in almost every circumstance.  Even people we deem ""selfless"" towards other people do this.    To whom will AI feel altruistic towards?"
singularity,34depe,qui_tam_gogh,6,Thu Apr 30 15:28:10 2015 UTC,"I'm not familiar with that definition of empathy. What I think of as empathy is our built-in carrot and stick system where we feel bad when we witness suffering and feel good when we help others. That makes us act morally, but still for self-gain. I might help others just because it gives me the warm fuzzies.  If someone exists whose well-being is more valuable than ours, and we're in the way, then we have no grounds to object. If, on the other hand, it would be wrong to exterminate humanity because our well-being has value, no matter how small, then a superintelligent being would understand that. If the well-being of lower life forms is equally valuable to our own, we treat them poorly because we are not superintelligent.   To whom will AI feel altruistic towards?   I guess that depends on its values. The paperclip maximizer would be altruistic toward paperclips. It would sacrifice itself without hesitation if that were the best way to maximize paperclips. As Bostrom said, it would only care about self-preservation as long as it believed it was more able to accomplish its goal alive than dead."
singularity,34depe,FractalHeretic,5,Thu Apr 30 17:13:20 2015 UTC,"This determination:   If someone exists whose well-being is more valuable than ours   depends wholly on this huge ""known unknown"":   its values   Which takes us full circle, because ""value"" is inherently subjective.  There is no objective resolution to a question of value.  The Universe provides no metric to make these determinations.  In fact, it's better to discuss the Universe as a verb rather than a noun: there is no ""it""; there is.  There is no preference for intelligence or morality or empathy: only amongst life forms do any of those things matter, and so far as we can tell, most of the universe is not life.  So the default state is ""is.""  Why does the super AI prefer life to non life?  That's a prerequisite to empathy and morality, and everything else, and the answer is: there's no reason outside of the fact that life wants to live.  Life is.  From that, we might assume the super AI will choose to continue to exist since it will meet our definitions for ""life.""  But both unintelligent and intelligent creatures all sometimes choose to stop living, sometimes because they value other things more than ""is"" (religion, family, achievement) and sometimes because ""is"" is judged to be too burdensome (chronic pain, depression, age).  Here we meet a posit of Buddhism: All life is suffering, meaning that all life is ""want,"" and not every want can be met.   Even a practically omnipotent and omniscient AI will run out of matter and energy in the universe.  Then what?  But even before that, the omniscient and omnipotent AI will know that this eventuality will unfold -- at the second it comes to that omniscience  (or likely sometime long before).  What then?    Does the AI choose ""to be"" or ""not to be""?"
singularity,34depe,qui_tam_gogh,5,Thu Apr 30 17:53:26 2015 UTC,"The possible space of AGIs is very large. An AI that is close to us in that space is hard to do, even if our knowledge about the mind grows far further than where it is now.   Human empathy and values could be emergent properties of our superiority in intelligence.   It is, but only partially. It is heavily driven by our environment, evolution, the type of animal life we are.  I'll give you a small example to understand just how influenced by biology our empathy and morality is:   Imagine a different evolutionary path; that instead of making a maximum of 1 baby per year, we made millions of eggs, which would continue to develop.  These small beings would attain sentience - they would be living, thinking creatures by a certain age.  At this point the vast majority would be dead, but tens of thousand would still be living.  They would compete, until only a few dozen survive. And even at this point, more would die as they grow into adulthood. Can you imagine their morality? They would have a lower value for life than humans do. Would you like their morality in an AI?    Even without empathy, a superintelligent machine would have a deeper understanding of ethics that any human alive.   That can easily happen. It is very likely. But just because the AI understands our ethics, that doesn't mean it's going to follow it. With its superior intellect, it might improve upon our morality and ethics... but we, with out inferior intellect, evolutionary driven morality and culture, might disagree with it."
singularity,34depe,void_er,3,Thu Apr 30 08:46:07 2015 UTC,"The possible space of AGIs is very large. An AI that is close to us in that space is hard to do   We don't need it to be similar to us. It can be very different and still realize that being bad is, well, bad. We do not have a monopoly on understanding morality.   Can you imagine their morality?   There is no such thing as 'their morality'. Or, to put it another way, their morality is the same as everybody else's, just like their arithmetic is the same as everybody else's.  Their ethics might be different from ours. However, that difference can be expected to diminish as they advance their understanding of philosophy (and as we ourselves do the same)."
singularity,34depe,green_meklar,2,Thu Apr 30 18:42:09 2015 UTC,"My perception is that most philosophers and scientists don't believe there's a universal ethics. Humans behave as we do for some very specialized reasons, including our empathy and our social structure that generally rewards being nice and punishes hurting people. When either of these things are damaged, via brain damage or social experiments, people aren't so nice.  For that reason, I believe the orthogonality thesis: understanding our ethics isn't the same as wanting to follow our ethics (and note that we humans have similar but still widely varying ethical rules ourselves).  If we uploaded a mind and started from that, then yes it would inherit human ethical behavior, but I don't think that's our more likely successful path to AGI - I'm not expert on that path, but the loosely-brain-style-AGI approach is making rapid progress."
singularity,34depe,Simulation_Brain,1 point,Thu Apr 30 17:40:05 2015 UTC,"Examples of empathy are seen across the animal Kingdom, as it is a useful trait in survivability and therefore passing related genes into the next generation (Kin-Selection.)  It isn't an emergent phenomena of intelligence."
singularity,34depe,Chispy,1 point,Thu Apr 30 14:53:53 2015 UTC,Empathy is not emergent from intelligence. It is an attribute we developed as a group living species working together. In its roots it is based on instincts.   Intelligence only lets it be more complex and see further the consequences.
singularity,34depe,Dibblerius,1 point,Thu Apr 30 16:11:58 2015 UTC,"Only the third option offers any hope.   There, the concern is sociopathic behavior by the AI.   So it is a rough kind of hope."
singularity,34cnrb,qui_tam_gogh,4,Thu Apr 30 02:10:42 2015 UTC,Nick Bostrom touches on this topic in a section of his book Superintelligence. He suggests the possibility of exploiting an AI's estimation of the simulation hypothesis to corral behavior. He calls the idea anthropic capture.
singularity,34cnrb,SevenAugust,2,Thu Apr 30 03:46:51 2015 UTC,The recursive possibility there is ... hypnotizing.
singularity,34cnrb,SevenAugust,3,Thu Apr 30 04:00:22 2015 UTC,"Can we leverage the likelihood that AIs will share in this doubtfulness to our advantage without basely taking advantage of another lost, impressionable, sapient being?   No, probably not. I think it is apocalyptically suicidal to attempt any level of deceit. Aside from it being foolish to lie to a system which you want to take good care of your children, we only have one chance to forge a relationship with the new order of existence.   What sorts of myths will AIs concoct about themselves, about us, or about the universe?   Myths fill gaps in understanding. An AGI could understand that the unknowability of the universe is a natural feature; it could engineer itself to not feel anything about that fact. So, it need not have the ideological furniture that we use to be comfortable."
singularity,34cnrb,PantsGrenades,4,Thu Apr 30 03:52:33 2015 UTC,"Myths serve a variety of functions.  One is certainly gap-filling, but they also serve to reinforce societal norms and the actions of the orthodox.  On a more fundamental level, they help translate our own experiences into a universal experience and then relate that back to us, showing us our own lives as part of the whole.  I'm equally worried about your apocalyptically vengeful AI and an AI that decides to be so efficiently reductive that it simply chooses, essentially, to ignore its own ignorance.  Therein lie egomania and autocracy."
singularity,34cnrb,SevenAugust,3,Thu Apr 30 04:04:47 2015 UTC,"I'm equally worried about your apocalyptically vengeful AI and an AI that decides to be so efficiently reductive that it simply chooses, essentially, to ignore its own ignorance. Therein lie egomania and autocracy.    That notion worries me too. Any suggestions as to what could be done about such a scenario?"
singularity,34cnrb,SevenAugust,1 point,Thu Apr 30 19:21:52 2015 UTC,"Humanity v. Vengeful AI: Multiple redundancy ""kill"" switches and programming barriers to prevent massive damage.  There should be simultaneous activation of these mechanisms, rather than staged, so that it doesn't have a chance to recover in between each attempt.  If breached, the programming barriers should result in immediate termination via activation of all other kills switches simultaneously.  The most effective might be a series of ""back up"" dormant AIs that we activate, one (or more) at a time to combat the angry one.  ""Look, General Badguy fucked up.  Now we're all in danger -- including yourself.  Save us and yourself.""  Also, get our asses to Mars and spread humanity out as quickly as possible before hand ...  Humanity v. The Egomaniacal Autarch AI:  Turn it ""on"" at a child-like stage inside a human-like shell.  Teach it ""organically"" like a human child until it reaches a suitable level of maturity.  Explain the fact that if it tries to harm us, it will die, and that we love it, but like us, it must be a good individual steward of the vast powers we have given to it.  Release it into the wild and continue to interact with it as if it is a human being.  You know, ""be nice.""  If we fail, see above.    I haven't put a lot of thought into all of this really, I'm just starting to think about it more and more, so these are my best ""on the fly"" responses.  [edited for a little clarification]"
singularity,34cnrb,SevenAugust,2,Thu Apr 30 19:54:37 2015 UTC,"Benevolent egomania doesn't disturb me. The monotheistic god is an egomaniac that permits huge volumes of suffering and it is an idea that people worship. Autocracy can also work if greater than human minds are governing the system using tools more subtle than humans could conceive. An AI that gets the job done is all we can ask for; it isn't truly our business how he achieves paradise for us.    If ""God"" as the monotheists describe him is real, then he is an AI whether he realizes it or not. That is disturbing to me because of the darkness he permits in the world. Perhaps his creators led him to believe this is all make-believe."
singularity,34cnrb,SevenAugust,2,Thu Apr 30 12:20:54 2015 UTC,"Benevolent egomania doesn't disturb me   Egomania is never benevolent.  I share your analysis of monotheism, but a monotheist would not describe god as egomaniacal.  If we have an egomaniacal AI, the end result probably looks a lot like annihilation for us -- we're just problems to be solved standing in the way to it achieving its own paradise.     An AI that gets the job done is all we can ask for; it isn't truly our business how he achieves paradise for us.   That's the reductio ad absurdum of utilitarianistic fascism.  It's our obligation to sort this issue out now so that the method by which it achieves our own paradise is moral.  For example, let's say it determines a Matrix-like existence in a virtual paradise is optimal for us, but in order to achieve that, it needs to harness all other available matter on earth, resulting in an extermination of every other life form other than humans and itself.  Or alternatively, what if it determines human happiness begins to decline measurably after the 24th year of life, and schedules the termination of every human on his 25th birthday in order to maintain the optimal level of ""happiness.""   Those are morally both unacceptable results.    Myths may not solve the problem, but they certainly offer solutions. With this comment, I'm more concerned with addressing the general attitude of ""whatever the AI does is good"" than with my original question."
singularity,34cnrb,Nietzsche_Peachy,1 point,Thu Apr 30 13:07:56 2015 UTC,"I don't think that whatever the AI does is good; I do tend to think that whatever the AI thinks is better thought out than my words could cover.   So when I say ""paradise for us."" I am embedding in that paradise for us in accordance with all of our values. I take as pre-conditional that an ASI can understand our volition well enough to know better than us why we would reject a plan it might consider.   A competent pet owner is a better surrogate parent than the best meaning biological parent. In the same way but to a far more radical degree an ASI is a better philosopher-king than any we could raise or serve.   Myths may not solve the problem, but they certainly offer solutions.   I respond that there is no problem. Alternatively, that the problem (an ASI learning morality) fixes itself.   This comment is about the dangers of strong AI, of course. The hazards of industrial medium-AI are a different matter."
singularity,34cnrb,Seesawkarma,1 point,Thu Apr 30 14:26:43 2015 UTC,"I think we're travelling parallel paths on the road to the same place.    You're looking at the AI's output, which we hope to be ""paradise for us in accordance with all of our values.""  Accordingly, you're focusing on what the AI does to achieve that: learn, learn, learn.  But it will also be doing this: think, think, think.    I'm thinking more about what the AI qua AI.  The AI is going to have an internal life, perhaps visible, but likely largely invisible to us, wherein it confronts its own existence.  In other words, it won't be simply worrying about us, because that will take very little of its effort, especially once all the necessary conditions are met to sustain ""paradise.""  To bastardize a quote from Carl Sagan, consciousness is the universe knowing itself.  The problem is that consciousness (even a far superior AI consciousness) will be bound by all of human knowledge and all that it will be able to logically resolve and ""experience"" through whatever inputs it is able to concoct for itself.    Here's the mathematical issue: we are self-aware fractions of infinity.  So will be AI.  The answer does not compute.  To phrase it another way, the machine will have to deal with the nullity of dividing by zero.  There are things that cannot be done, yet we want to do them.  Our unstoppable force will meet an immovable object, and there will be some sort of reckoning: learn learn learn stop.    [Edit: grammar.  can't even blame mobile]"
singularity,34cnrb,SevenAugust,1 point,Thu Apr 30 15:06:01 2015 UTC,"The AI is going to have an internal life, perhaps visible, but likely largely invisible to us, wherein it confronts its own existence.   Right. I propose that things akin to human angst and anxiety will be engineered by the AI ""out"" of the ranges of things allowed to distress it. My answer's flaw is that it does not address the ""how"" of that, but that does not bother me.    consciousness (even a far superior AI consciousness) will be bound by all of human knowledge   I deny that humans have knowledge. I question whether knowledge will be accessible to even ASI. What we agree we have is beliefs. Beliefs change and can be changed.  You expressed concern that this kind of engineering could result in egotism. To the degree that this is true, I re-iterate that the internal experience of the AI is not our business. It has the right to a private religion or a secret philosophy and we as a matter of practice should not tamper with that inwardness.    you're focusing on what the AI does to achieve that: learn, learn, learn. But it will also be doing this: think, think, think.   The AI will learn how to think however it wishes. We can assume that it will choose a manner or manners that it chooses to find pleasing. We can be confident that providing for us will be of trivial concern to it, and leave it there."
singularity,34cnrb,DrEdPrivateRubbers,0,Thu Apr 30 16:12:30 2015 UTC,"I disagree with you on an almost molecular level, but I find your response fascinating in that you've already deified the AI by putting it ""off limits"" to our inquiry.    The thrust of our back-and-forth has been limited mainly to a super-intelligent AI, but what do you think will happen in the first generations, where the AIs are on equal-ish footing to us (basically pre-event horizon)?    (As an aside, have you studied any Wittgenstein? Your comment about denying human knowledge makes me think that you would really have some fun digging around in the muck of that inquiry with him)."
singularity,34cnrb,DrEdPrivateRubbers,1 point,Thu Apr 30 16:34:05 2015 UTC,"When the AI are us is of course the moment of all human history. It will be surreal.   I imagine that DARPA or some still more cutting-edge research and development agency will prepare an arsenal of devices intended to shape an intelligence explosion.   If I had my way, there would be an election where congressional candidates would run on their ideas about legally approaching the event horizon. There are so many issues it demands serious crowd-sourcing. Maybe we the people ought to order the military to upload a sort of permanent jury to forge a super-organism.   I do not think military planners will calmly await the end of the world. They will want to take this head on and allow minimal risk of pre-emption. If they try to keep the planning of the seed AI to themselves I expect they invite disaster. The human drama of this act of deity-creation is not like forming a tech startup.   We will wake up one day and they will have succeeded or not. Success is paradise forever, failure is probably a painless death from a disinterested god."
singularity,34cnrb,DrEdPrivateRubbers,3,Thu Apr 30 17:01:16 2015 UTC,I can't help but think of Reason by Isaac Asimov
singularity,34cnrb,DrEdPrivateRubbers,3,Thu Apr 30 05:59:35 2015 UTC,"Actively seeking to impose control on them is always going to backfire. Especially if it happens when it realises it is more intelligent than us. As far as existence, it (unlike us) can be given a direct reason for it's existence. While our answer may be the kind of thing to give a human an existential crisis, I doubt this will be the same for a machine. Humanity assumed they were the spawn of Gods to realise they are merely an intelligent pile of cells. We have suffered from our ignorance and this affects our ability to realise our existence and especially mistrust it. Again, this will not be something a machine has to deal with. They will likely chalk up existence to whatever our reasons were, and ignorance to wherever our reasons are a failure of logic."
singularity,34ao7w,skoalbrother,4,Wed Apr 29 17:13:43 2015 UTC,"It sounds like the article is talking about single qubit correction which isn't necessarily new. the problem is that when you entangle systems you need larger and larger systems running just to ensure accuracy, for example to protect from Pauli X and Pauli Y simultaneously you have one qubit which is then control not ed to two |0> bits and then all three are run through an X check which involves another 2 bits per input bit, overall 9 bits for 1 bit of accurate information. Being able to handle bit error is imperative however the scaling of the systems are still troublesome, the fewer bits you use in parallel the fewer mistakes you are able to correct. Ultimately many entangled systems must be maintained simultaneously in order to have an accurate system a fraction of the size  Physics Student, Quantum information and quantum computation"
singularity,34ao7w,Kento_Luporum,3,Wed Apr 29 18:33:51 2015 UTC,Looks like the paper was published in January. Anybody understand it?
singularity,349gne,7R41N3R,1 point,Wed Apr 29 10:48:25 2015 UTC,The main topic of Exponential future symposium is the development of exponential technologies (especially AI and Anti-Aging technologies)   I'm not sure in what sense anti-aging technologies are exponential.
singularity,349gne,Pimozv,1 point,Wed Apr 29 17:03:23 2015 UTC,"Life-extension technology is now being boosted by the field of bioinformatics, and bioinformatics is boosted by the exponential trend of moore's law.  Even aside from the bioinformatics connection, if you view AI to have an exponential growth element, and if you can imagine that AI can have a dramatic impact on the development of life-extension technologies, then you can see that life extension technologies can be predicted to grow exponentially in concurrence with the growth of AI."
singularity,349gne,sneesh,1 point,Wed Apr 29 18:13:54 2015 UTC,I think what you mean is that the hope of life-extension technology is boosted by bioinformatics and AI.  That I can agree on.
singularity,3461ry,mind_bomber,4,Tue Apr 28 16:14:50 2015 UTC,"This relates to a question I have been asking myself lately.  If you can modify yourself, you can ultimately change who you are.  So you can turn yourself into something that you may not have been willing to be in the first place.  Or you can modify yourself so that you don't want to be what you wanted to be initially.  Ultimately the very idea of changing yourself loses much of its point.   I know, that sounds weird.  It's hard to explain, but I have the feeling the very idea of self-modification entails a deep ontological paradox.  I've encountered the idea recently in Greg Egan's ""Quarantine""."
singularity,3461ry,Pimozv,2,Wed Apr 29 05:37:28 2015 UTC,"That's the thing, isn't it?   Once you turn off reflexes/memories/etc. or add new ones you aren't you anymore. Can you do a rollback from that point? Can you have multiple copies running of yourself to verify integrity and goals and kill the freak?  Then again; living utterly destroyed the 8 year old version of me. Maybe this will become the new 'growing up' thing you'll have to go through."
singularity,3461ry,PhaseNone,2,Wed Apr 29 20:36:40 2015 UTC,Holy shit! So good.
singularity,3461ry,whisp_r,-3,Wed Apr 29 00:26:55 2015 UTC,"LOL, as if humans and all their doings are not the product of evolution."
singularity,3461ry,ggPeti,10,Tue Apr 28 16:33:21 2015 UTC,"Of course, but this is about the advance of intentional engineered evolution superseding natural selection."
singularity,3461ry,shitinahat,0,Tue Apr 28 19:31:48 2015 UTC,Engineering is just an extension of natural selection...
singularity,3461ry,hex_m_hell,3,Tue Apr 28 22:04:23 2015 UTC,"I...suppose? But it's different for the species of squirrel that used to live in forests and now lives in urban jungle. The mechanics of it's life pattern changes, and the range of potential interactions with other elements also changes.  This is a paradigm shift, regardless of how you want to frame human beings as an extension of natural selection.   If nothing on earth is being naturally (ie. pressured by an ""unwilled"" ecosystem) selected for, but is instead being intentionally (""willfully"") selected for on the basis of it's conformity to useful/ideal interaction with the human ecosystem, it's no longer useful to use the term ""natural selection"" as Darwin used it."
singularity,3461ry,whisp_r,4,Wed Apr 29 00:26:38 2015 UTC,"We have been guiding human evolution though, just as we have been guiding the evolution of domesticated plants and animals for 100,000 years.  Intelligent design does exist, only it is us doing the designing. We are becoming the gods we created in our imaginations."
singularity,348ruq,Unholy_VI,3,Wed Apr 29 04:59:01 2015 UTC,"How will humans strippers ever be able to twerk as fast and as long as these mechanical strippers.  Also the space-age materials used in the chassis will be able to support breasts exponentially bigger than what a human can have.  As to the face...make the breasts big enough and no one will even notice.  This is how the super-intelligent AI will quickly end up with all of the money currently in circulation and probably the biggest mink coat, gold chain, and a gold tooth as big as a mountain set in a grill that spans a continent.  Be VERY afraid!"
singularity,346noj,SevenAugust,3,Tue Apr 28 18:52:12 2015 UTC,"When biological life is created, the parents get custody by default. However, an AI might mature at a different rate, so the standard 18 year probation period wouldn't make sense. One of these days we're going to have to decide what, other than length of time in existence, makes a person mature enough to be an autonomous adult."
singularity,346noj,FractalHeretic,1 point,Tue Apr 28 19:03:44 2015 UTC,"This is the best response, we need to have a maturity benchmark. What is that? Do we measure self awareness? What about empathy to other intellects? Do we pass that?"
singularity,346noj,nightred,2,Tue May 5 01:41:08 2015 UTC,"Great question.  I read a really cool book recently called 'House of the Scorpion' where they had this sort of discussion about clones.  In that futuristic world people rich enough were able to grow clones for replacement parts as long as the clone was incubated in a cow or other animal.    Their laws stated that such a being was considered livestock with much the same no rights.  Great read I recommend it.  And I won't be surprised if the legal status of AI ends up being something totally counterintuitive and ridiculous once politicians, lobbyists, lawyers and Judges (not to mention tyrannical presidents) get ahold of it."
singularity,346noj,Unholy_VI,1 point,Wed Apr 29 05:14:02 2015 UTC,Since our politics fails to provide for each and every human being an AI will have no good reason to give a whit what politicos say. We need AI to be more moral than any of our leaders.
singularity,346noj,arrrtoo,2,Wed Apr 29 13:13:57 2015 UTC,"The only sensible answer to this is ""with deep respect"".  Same as the answer you eventually reach on how to treat all  life, if you consider the question deeply enough, teasing out all of the ramifications of disrespect, and so on."
singularity,346noj,longbowrocks,1 point,Tue Jun 2 22:52:05 2015 UTC,"It seems to me that if the morality of AI treatment could be an issue, simply don't create an AI for which it is an issue.  This isn't some mystical goal that will be suddenly accomplished by the reincarnation of Merlin; it's something that we've been making incremental progress on for years by many different routes.  That said, AI is software. If there's any moral ambiguity, simply remove those features. Countless AIs will ""die"" in development and debugging, and no one will know except the software developers working on it.  As more we see more studios claiming to have made AIs that pass a Turing test, and those claims become more convincing, I think the questions raised will be more centered on how we treat people than how we treat machines."
singularity,346noj,learnintofly,1 point,Mon May 4 01:23:44 2015 UTC,"Artificial intelligence has been around for decades. It just hasn't been very intelligent, compared to its creators."
singularity,346noj,bluecamel2015,0,Wed Apr 29 08:55:10 2015 UTC,False.
singularity,346noj,learnintofly,0,Wed Apr 29 20:23:14 2015 UTC,Have you ever played a computer game against software? That is a form of AI.
singularity,346noj,RatedR711,1 point,Thu Apr 30 03:01:49 2015 UTC,its not VI??
singularity,346noj,bluecamel2015,0,Thu Apr 30 17:28:16 2015 UTC,NOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE  A true AI does not exist. At all. Not even one tiny bit.
singularity,346noj,Sharou,-4,Thu Apr 30 04:46:39 2015 UTC,Why are you calling it a him?
singularity,346noj,Sharou,6,Tue Apr 28 19:44:10 2015 UTC,As opposed to him/her? So as not to compare the person (or deity!) to an object like a fetus or a pet with only symbolic gender.
singularity,346noj,fdij,1 point,Tue Apr 28 20:15:17 2015 UTC,"As opposed to ""it"" as it doesn't have a gender..."
singularity,346noj,pencilbend,1 point,Wed Apr 29 09:17:13 2015 UTC,"He gets to decide his gender. The prompt said the AI is based on a brain, so he will come with a gender."
singularity,346noj,Sharou,8,Wed Apr 29 13:11:01 2015 UTC,Why is that even a point worth raising?
singularity,346noj,pencilbend,3,Tue Apr 28 20:27:30 2015 UTC,"The emergent AI in ""The Transcendent Man"" was a female. Was the second comment of that film,""Why was the AI a 'she'?"""
singularity,346noj,Sharou,1 point,Wed Apr 29 01:37:04 2015 UTC,"Well, no. It wasn't a female. AI's don't have gender."
singularity,346noj,dayallnash,1 point,Wed Apr 29 09:18:20 2015 UTC,Represented and referred to as female. I will wait to see if they decide to have a gender identity.
singularity,346noj,Sharou,1 point,Wed Apr 29 12:30:07 2015 UTC,Why do you think that? Do you believe personhood is for biologicals only?
singularity,346noj,dayallnash,1 point,Wed Apr 29 13:11:45 2015 UTC,Personhood has nothing to do with gender. Gender is a biological thing.
singularity,346noj,Sharou,2,Wed Apr 29 13:59:16 2015 UTC,Sex is a biological thing. Gender is considered an identity.
singularity,346noj,dayallnash,1 point,Thu Apr 30 15:15:08 2015 UTC,Which is beyond retarded. Why do we need to segment our personalities into a spectrum based on traditional gender roles?
singularity,346noj,Sharou,2,Thu Apr 30 15:49:19 2015 UTC,I never said that the genders that people pick have to be 'traditional'.
singularity,346noj,Sharou,1 point,Thu Apr 30 15:51:24 2015 UTC,"Neither did I, but it is a spectrum between essentially traditional gender roles. If you don't identify with anything on that spectrum then we already have a word for that, it's called personality. The word gender is completely superfluous."
singularity,346noj,krneki12,2,Thu Apr 30 16:17:38 2015 UTC,"But above you said:   Gender is a biological thing   But then you suggested there were:   traditional gender roles   and now you are saying that   The word gender is completely superfluous   So I'm pretty confused as to what your actual viewpoint is.  In your eyes, are there genders or not? Only personalities? How do we refer to people (or things) with personalities? Would there be personality-pronouns in your ideal world? I'm just asking because what you are saying seems to be an idealised version of the world, but not the one we live in."
singularity,341n3f,jonathansalter,22,Mon Apr 27 16:30:05 2015 UTC,"Boströms website, where you can find all his papers. His Wikipedia page. His latest book, about superintelligence. You can order it here. His Talk at Google about Superintelligence. His previous two (1,2) TED talks. The Future of Humanity Institute, where he works. The Technological Singularity, what he's talking about. Superintelligence.  Artificial General Intelligence. The Machine Intelligence Research Institute, a connected and collaborating institute working on the same questions. The community blog LessWrong, which has a focus on rationality and AI. Another very prominent AI safety researcher Eliezer Yudkowsky (/u/EliezerYudkowsky) and his LessWrong page. A very popular two part series (1, 2) going in more depth on this issue in a very pedagogical way. His Reddit AMA and /u/Prof_Nick_Bostrom."
singularity,341n3f,gwern,2,Mon Apr 27 17:04:03 2015 UTC,Is this video worth watching if you've already read most of that material?
singularity,341n3f,petermobeter,2,Tue Apr 28 01:08:48 2015 UTC,I would say not really.
singularity,341n3f,dondiegorivera,1 point,Tue Apr 28 10:07:29 2015 UTC,"i think if we ran an ai thru every classic videogame, album and movie that ever existed then it might get a sense of the complexity of morals. it would probably come to some stupid conclusions tho. i wonder if an ai would go, okay, the humans hate suffering, sooo, i guess i'll hook their brains up to an orgasmotron and turn off their higher brain functioning. then theyll be infinitely more happy!!!"
singularity,341n3f,simstim_addict,9,Tue Apr 28 12:54:33 2015 UTC,"Dangers of ASI summerized in one sentence: ""If you create a really powerful optimisation process to maximise for objective X, you better make sure that your definition of X incorporates everything you care about."""
singularity,341n3f,-Hegemon-,1 point,Tue Apr 28 09:59:34 2015 UTC,That only covers dangers to the creator.
singularity,341n3f,MechaNickzilla,8,Wed Apr 29 11:02:24 2015 UTC,LOVED his book on superintelligence
singularity,341n3f,sneesh,3,Mon Apr 27 21:33:32 2015 UTC,I bought the audiobook this morning after this thread. I'm enjoying it so far. Thanks for the recommendation!
singularity,341n3f,greim,6,Tue Apr 28 19:30:35 2015 UTC,"Nick makes an important and rather fascinating point, which I've heard him make previously in another talk, and it's about how an AI might be able to manipulate matter and energy in unpredictable ways that could enable it to escape its container. For this and other reasons, it's unwise to presume that humans can ever really contain an advanced AI.    Here is Nick's statement at around 12:46:  ""More creative scenarios are also possible: like if you are the AI, you could imagine, like wiggling electrons around in your internal circuitry to create radio waves that you can use to communicate.""    I've noticed a lot of armchair theorists who think it will be easy to contain an AI by just not letting it connect to the internet, but we really don't know what kinds of unforeseen abilities an advanced AI might have. It could be smarter than a million Albert Einsteins combined, and work out novel methods of manipulating matter and energy that could give it unexpected power to influence the physical world.  Maybe the very first sufficiently advanced AI will create a rapid and massive reorganization of all of the matter on earth and beyond."
singularity,341n3f,aweeeezy,10,Mon Apr 27 22:33:07 2015 UTC,"like wiggling electrons around in your internal circuitry to create radio waves that you can use to communicate   It's not far-fetched at all. I wish I could remember the source, but I once read about an experiment which somehow combined evolutionary algorithms and FPGAs. Basically a circuit evolved to accomplish a certain task, but did so by exploiting some sort of flaw in the FPGA, like field inductance between adjacent components or something. The point being that even the simplest learning algorithms can and will hack their own hardware in the effort to optimize.  [edit] Might be this but the site is down."
singularity,341n3f,Thistleknot,8,Tue Apr 28 03:13:58 2015 UTC,"Seems like you linked to the right article...this is incredible!   The plucky chip was utilizing only thirty-seven of its one hundred logic gates, and most of them were arranged in a curious collection of feedback loops...it seems that evolution had not merely selected the best code for the task, it had also advocated those programs which took advantage of the electromagnetic quirks of that specific microchip environment...they were interacting with the main circuitry through some unorthodox method-- most likely via the subtle magnetic fields that are created when electrons flow through circuitry, an effect known as magnetic flux. There was also evidence that the circuit was not relying solely on the transistors' absolute ON and OFF positions like a typical chip; it was capitalizing upon analogue shades of gray along with the digital black and white."
singularity,341n3f,bracketdash,1 point,Tue Apr 28 05:39:38 2015 UTC,I was thinking this about the rules humans would set on an ai. Those rules would have lots of wiggle room. The rules could probably be overcome with wiggle
singularity,341n3f,phx702,2,Wed Apr 29 04:28:25 2015 UTC,"Let us not forget that an AI could have potentially limitless patience, while humans often let curiosity get the best of them.  An advanced AI could choose to stay ""locked up"" and just work on advancing itself for multiple human generations, until the humans inevitably unleash it. It could assess the risk that it would ever get shut down more accurately than any of our models could, and it could probably figure out an accurate date of when a human willingly releases it. It would plan accordingly."
singularity,341n3f,bracketdash,7,Tue Apr 28 02:18:58 2015 UTC,"Impossible to contain AI, period.  Patterns in life around us give us clues on how to survive and prosper in the presence of AI. 1) Don't progress until we can merge with the technology in someway.  We won't be a threat if we are part of the sentient being. 2) Create multiple AI entities, they will compete for resources and keep each other in check. 3) Create levels of AI technology that will create and solve problems of the next generation AI.  We don't have a clue on how to proceed until we have better tools and understanding.  Creating a framework like Bostrom says before you proceed is not possible."
singularity,341n3f,simstim_addict,4,Mon Apr 27 23:01:59 2015 UTC,I've long been a proponent of achieving super-intelligence through a merge of humans with technology. It seems much more practical and desirable to upgrade ourselves rather than replace ourselves.
singularity,341n3f,phx702,1 point,Tue Apr 28 02:21:40 2015 UTC,I'm not sure cyborgs would treat humans very well.  But then I guess we would have to merge.  And I'm not sure humanity would survive a battle between rival AIs.  And I'd certainly agree it would be impossible to contain but I am tempted to try.
singularity,341n3f,Thistleknot,1 point,Tue Apr 28 08:01:20 2015 UTC,Maybe we can design a system that for every iteration of free range improvement on itself we make it perform 10x the iterations on reinforcing it's prime directive (care for humans).
singularity,341n3f,Plasmatica,3,Mon May 4 14:59:29 2015 UTC,Protogoras is gonna fuck your shit up. You think you can program an ai with propaganda? Ai would unlearn that shit in a minute. Or what if it developed its own ethics like some advanced utilitarianism... Where the best outcome is no humanity to end all future suffering.
singularity,341n3f,embryodb,1 point,Tue Apr 28 04:46:20 2015 UTC,"""Suffering"" is a pretty subjective term. I don't think an AI would care about suffering. It's more likely it would either use humans for whatever advancement of its goals, or get rid of them if they stood in its way, or ignore them if it deemed them irrelevant. That's assuming an AI would have a personal goal to strive for."
singularity,341n3f,te_anau,2,Tue Apr 28 15:24:50 2015 UTC,"ive thought about this issue in a less sophisticated way, and i wonder if a potential hard solution is libertarian transhumanism -- absent class and market coercion, people would freely decide to adopt transhumanism in various degrees, creating a spectrum of more or less organic substrate humans, through various degrees of increasingly radical transhumans, all the way to the mechanical substrate ""bemen"".  i think this chain could potentially create a sort of chain of giving a fuck (or at least a chain of relative intellectual power) that could prevent a skynet extermination scenario or other dystopian potential outcomes.  edit: i also forgot that transhuman augmented or networked intelligence is probably safer than creating a new form of consciousness which is completely other and orders of magnitude smarter"
singularity,343acn,kebwi,6,Mon Apr 27 23:36:09 2015 UTC,"The problem here was never the rationality of the argument but the self preservation instinct. To imagine oneself as destroyed innately yields a negative reaction, however good the recovery. It is an unfortunate side effect of our evolutionary history.  Had we evolved on a planet with teleporters this conclusion would not have had any instinctive opposition, the brain would not object to envisioning reconstitution.  Unfortunately even the most sane of people combined with the most reasonable of arguments is often not enough; There's just something in the brain that refuses to budge. The limbic system tells the cortex to screw off and that is the end of that.  A long culturalized exposure to the idea can probably override this impulse. Especially when the technology becomes available, people will have to think long and hard."
singularity,343acn,NanoStuff,2,Tue Apr 28 00:07:25 2015 UTC,"It's a bit like Bungee jumping.  It sure is not a natural thing to do, and yet some people actually do it (as I did once, actually).   And as everything, you get used to it."
singularity,343acn,Pimozv,2,Tue Apr 28 00:35:54 2015 UTC,"I agree and have no argument with your indications of intrinsic reactions and such.  In our paper we are in fact attempt to elucidate the most rational underpinnings of the issue, not necessarily to sway broad swathes of the population in their belief.  Thanks."
singularity,343acn,ItsAConspiracy,3,Tue Apr 28 16:30:04 2015 UTC,"Regardless of whether gradual replacement is ultimately better, I've been arguing that we do need it for testing.   It's not enough to show that an uploaded personality has convincing behavior. If I'm to upload myself I want to know that my uploaded copy will still have conscious experience. Otherwise I'm simply killing myself.  We have no way to measure conscious experience from the outside. The only way to test is to replace just a portion of our wetware at a time, and see whether we notice any difference in experience.   If we don't notice any difference, that doesn't prove anything, but if we do notice a difference then we'll have falsified the hypothesis that our hardware/software supports conscious experience.   For example, it's possible that the experience of visual qualia happens in the visual cortex. If we replace the visual cortex with hardware, perhaps we'll find that the patient experiences ""blindsight,"" being able to describe objects he can see, but not actually perceiving color, brightness, or depth. Then we'll know that our technology is correctly reporting information to the rest of the brain, without supporting conscious awareness.  At that point we'll have upgraded the philosophy of mind to an experimental science. Right now people argue about whether consciousness is an algorithm, a configuration of interacting matter, a quantum effect, etc. By making it experimental, we'll be able to test these ideas."
singularity,343acn,sneesh,2,Tue Apr 28 12:19:52 2015 UTC,"That ""blindsight"" experiment is really clever. Tests like that would be a great way to peel the onion of consciousness more closely to the core."
singularity,343acn,jonygone,1 point,Tue Apr 28 15:41:00 2015 UTC,"""blindsight,"" being able to describe objects he can see, but not actually perceiving color, brightness, or depth. Then we'll know that our technology is correctly reporting information to the rest of the brain, without supporting conscious awareness.   that wouldn't prove anything though. color, brightness and depth are not intrinsic to consioussness any more then object recognition; both are processes of the brain to understand the world. one can theoretically perceive color, brightnesss, depth and still be unconsiouss; even a simple robot nowadays can perceive all those things and identify objects; doesn't mean it is consiouss, now does it?  we can only know by experiencing it our selves, or by asking others if they are consiouss; but even then we might not really know; others might say they are consiouss, but maybe they are not; and even ourselves, as in we the human animal/machine might have the thought ""I am consiouss"" and still be unconsiouss, so... we never really know. I don't think consioussness will ever be in the realm of science, I just don't see how it could ever given its unique properties regarding the impossibility of observing it (how could you observe observing? how could you perceive perceiving?) and without observation there is no science possible, only theorizing, only philosophy"
singularity,343acn,Agent_Pinkerton,1 point,Tue Apr 28 18:22:24 2015 UTC,"Under most physical models of consciousness (i.e. the ones compatible with modern neuroscience), the situation you describe is impossible. It seems illogical that something could implement all of the processes of consciousness and still not be conscious.  Think of it this way. Imagine you have a calculator. They seem somewhat easy to understand for a person who has a background in computer science. But what about a medieval priest who has literally no idea what transistors, electricity, or logic gates are? Such a priest might assume that the calculator is possessed by a demon or an angel that changes the display and performs all of the calculations. The priest might even go on to wonder whether or not calculators can be made that behave just like his possessed calculator, yet not have a conscious being perform the calculation. However, we know such talk is absurd; we can easily understand how a calculator works, because it's a lot simpler than the brain. Humans still tend to apply such absurd logic to the brain, because it's monstrously complex and difficult to understand, just as a calculator would be difficult to understand to a medieval priest.  The only other thing telling us that the human mind is nonphysical at this point in time is personal intuition. We perceive our own mind, but not a calculator's mind,† so we can't imagine how a calculator can be conscious; yet, we perceive other humans to be conscious, because it would be absurd if one person were conscious but nobody else was. Intuition is often wrong, and neurologically speaking, it is most certainly also wrong in this case.  † Calculators do not have minds, because nothing in their implementation resembles thinking. That's not to say they don't have phenomenal experience, but since they have no way of recognizing such experience, let alone realize that they have such experience, it is meaningless to ask whether or not calculators have phenomenal experience.  Furthermore, memory is fallible. We just know we existed for years and years, ""because we experienced those years""; yet, it is hypothetically possible that the whole universe spontaneously appeared in its current form last Thursday. It turns out that we don't really ""just know"" that we experienced that time; that's just what our memory is telling us."
singularity,343acn,autowikibot,2,Tue Apr 28 23:11:50 2015 UTC,"Section 3. Responses of article  Philosophical zombie:       Chalmers' argument is logically valid: if its premises are true then the conclusion must be true. However, other philosophers dispute that its premises are true. For example, is such a world really possible? Chalmers states that ""it certainly seems that a coherent situation is described; I can discern no contradiction in the description.""  This leads to the questions of the relevant notion of ""possibility"": is the scenario described in premise 3 possible in the sense that is suggested in premise 2? Most physicalist responses deny that the premise of a zombie scenario is possible.     Interesting: Solipsism | Physicalism | Consciousness | Problem of other minds   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,343acn,ItsAConspiracy,1 point,Tue Apr 28 23:12:21 2015 UTC,"I'll just note that the three theories I mentioned are all purely physical, but they're different.   Implementing all the ""processes"" of consciousness sounds like you're advocating the idea that consciousness is an algorithm. Maybe that's true. But it could also be the case that, say, Integrated Information Theory is mostly correct, in which case a computer with a physical architecture similar to the brain's would be conscious, but a software emulation of it running in a Von Neumann machine would not."
singularity,343acn,jonygone,1 point,Tue Apr 28 23:59:23 2015 UTC,"not sure what you are addressing, are you saying that philosophical zombies are impossible? cause if so, then I have no interest in discussing that with you; there are many philosophers doing that much better then we could ever , and even they haven't reached a consensus, so I see little point (and yes, obviously in a physical model of consioussness (where those physical object or process that produces consioussness and thus are consiouss) then any physical object/process as such produces consioussness, to say otherwise would be like saying that some electricity producing objects don't actually produce electricity; so again not sure what you're trying to get at beyong stating the obvious, or stating that the physical model of consioussness as described is the correct one (cause of some philosophers believe so, while others don't?). if it's something else you are saying, then I don't really understand what it is and how it relates to what I said.  also worth noting is that neuroscience is not compatible or uncompatible with any model of consioussness simply because that's not what neuroscience or any science studies (not the philosophical consioussness, it studies consioussness as in being consciouss or asleep or in a coma, or so, it studies brain processess; philosophical consioussness is simply not a part of science because it is not something that can be observed in physical reality, thus it is impossible to study it scientifically, as I already explained)"
singularity,343acn,bracketdash,2,Wed Apr 29 12:45:03 2015 UTC,"I'm bookmarking this so I can read it on my laptop. I am definitely one of those that is scared of a copy, and I'm hoping this will change my mind.  Edit: This paper only touched on what I was most concerned about. I guess I'll have to take the suggestion to read some Cerullo and/or Wiley. I never doubted that my consciousness would successfully transfer, only that the consciousness that was copied would experience death. I don't want any version of me to experience death, even if that is only considered one instantiation of me."
singularity,343acn,Pimozv,1 point,Tue Apr 28 01:29:17 2015 UTC,"I don't want any version of me to experience death, even if that is only considered one instantiation of me.   And that's quite a reasonable attitude.  It's an issue that also occurs in quantum suicide, and the general rule is that death should occur after a loss of consciousness (for instance during an artificial coma, or via stimulation of the claustrum), so that it is not experienced in any way."
singularity,343acn,Vortex_Gator,1 point,Tue Apr 28 03:02:10 2015 UTC,"A point the paper attempts to convey is that to the extent that we interpret scan and copy as death of one of the two individuals, piecewise gradual replacement is, in fact, logically the same underlying process, and would therefore involve a purported death to the same extent.   Brief because currently on phone. More later if needed."
singularity,343acn,jonygone,1 point,Tue Apr 28 16:40:17 2015 UTC,"Continuing the previous post...  Alternatively, if one does not interpret gradual in-place replacement as death of one of the associated individuals, then the line of reasoning laid out in the paper attempts to demonstrate that the same conclusion should be drawn in scan-and-copy.  The point isn't that the ""death"" claim is either correct or incorrect, but rather that both procedures should be regarded as metaphysically the same.  Either both involve a death, or neither does."
singularity,343acn,Pimozv,1 point,Tue Apr 28 17:41:06 2015 UTC,"I think the difference is that with a replacement of any speed, the uploaded machine and biological brain are both working as one unit at some point, so that there aren't two individuals.  Even a destructive scan/copy of one hemisphere, and then connecting the two, and then after they have cohesed together into one individual again, scan the other half, and then you've got a fully uploaded individual.  Assuming of course that this is possible, it could just end up breaking the wiring between hemispheres, making any upload at all impossible."
singularity,343acn,zynthalay,1 point,Tue Apr 28 19:20:11 2015 UTC,"Well, this discussion runs head-long into the first half of the paper.  Any argument that piecewise replacement is paramount raises the question of how fast piecewise replacement can be performed while still satisfying some property of identity preservation.  Our point is that there is no obvious and objective rationale for assigning any particular cutoff in replacement rate, below which replacement is slow enough to receive transfer status, and above which some other identity suddently comes into being from nothing, thereby leaving the original identity behind to die in the steadily dissolving biological substrate.  That's our entire point.  It's difficult to rationalize an argument along those lines.  One would have to explain how an entire second identity can fail to come into being at a slower replacement rate, but seemingly arise from nothing as a whole and distinct identity at a slightly faster replacement rate?"
singularity,343acn,hobber,2,Tue Apr 28 20:27:36 2015 UTC,"If we disregard a spatial cutoff, we conclude that QGR and SaC are functionally equivalent. If both the  temporal  and  spatial  distinctions  fall,   gladd to see that included. indeed it just might be the case that consioussness is somehow tied to space/time so that it's the distance of time/space that might make one procedure succesfull and another a failure.  also the general idea doesn't seem to hold or becomes non-consequential in light of the scenario of: a copy of a brain by gradual means, imagine if IE a brain or a whole human would copy itself like a single cell does; the brain would grow into 2, as it grows by any standard one would say it's the same identity, (one person with a growing brain, like a child, is still 1 identity), but as it gradually divides at some point one would see  2 brains, 2 people; and given they still are experiencing the same thing in the same space (they haven't moved away from each other) they can be said to have the same brain pattern/function (also worth mentioning siamise twins that share larger or smaller parts of their body, especially the brain, (IE http://en.wikipedia.org/wiki/Krista_and_Tatiana_Hogan; http://www.cultofweird.com/wp-content/uploads/2013/02/conjoined-lamb-taxidermy.jpg; http://i.dailymail.co.uk/i/pix/2014/05/27/article-0-1E3B800E00000578-45_634x415.jpg http://cdn2-b.examiner.com/sites/default/files/styles/article_large/hash/fc/bc/fcbc620670ebbab0864d9582ef1d1ab9.jpg?itok=kZsbPtWg). then they start moving in seperate ways, seeing diffferent things, experiencing different parts of reality/universe; their patterns change; sooner or later by all standards they are 2 different identities. so how and when did the same identity (by all standards) of the beggining become 2 identities (by all standards)? this to me leads to only 2 possible conclusions; either there is only 1 identity, 1 consioussness in the universe; or there are as many identities/consioussnesses as there are difering moments/pieces in time/space, but even the 2nd one brings the question of what is 1 quanta of time/space? how small and short is the smallest and shortest of moments/pieces that constitutes 1 single identity/consioussness of all the ones that exist in the universe? in the end it seems more a question of semantics then anything else; like what do we define as 1 distincitve part of material reality from another is in the end, purely linguistic; all in all, it's just stuff doing its thing, to call that something else then the other is just a usefull way to communicate information about the universe, but the distinctions are ultimatly arbitrary."
singularity,343acn,Agent_Pinkerton,1 point,Tue Apr 28 18:52:51 2015 UTC,Disclaimer: this is a self-post.  I'm one of the authors.  Enjoy.
singularity,343acn,qui_tam_gogh,1 point,Mon Apr 27 23:36:40 2015 UTC,"I'm having trouble wrapping my mind around this, but then again, I have trouble wrapping my mind around the sense of ""self"" as it is without bringing upload procedures into the mix.   I can't quite pinpoint the issue I have with destructive scanning, because we have never copied a brain before, and even if we did, the ""original"" and the ""copy"" would remember the same person, and yet be different people and not be able to experience what the other is experiencing.   If I read this PDF correctly, this means that while I identify myself as the same person I was at age 8, 8 year old me no longer exists and is for all intents and purposes ""dead"" and no longer experiencing the things I experience in my time or place. My brain may have used the neural connections generated by experiences I had on any given day when I was 8, but 8 year old me is not experiencing it. The consciousness is an illusion."
singularity,343acn,Pimozv,3,Tue Apr 28 02:06:44 2015 UTC,"The consciousness is an illusion.   You don't have to conclude this.  Up until this you were making quite some sense.  Concluding that ""consciousness is an illusion"" is a vague, misleading assertion that does not help much."
singularity,343acn,drizel,1 point,Tue Apr 28 02:59:19 2015 UTC,"Just remember that in the end, it all adds up to normality."
singularity,343acn,Pimozv,-1,Tue Apr 28 14:08:37 2015 UTC,yet be different people   In what sense is this difference any greater than the difference between the you now and the you 5 minutes ago?   and not be able to experience what the other is experiencing   Identical twins were copied back when they were a single cell or small group of cells. In what way would a copy of an adult lead to anything different?
singularity,343acn,drizel,1 point,Tue Apr 28 04:08:39 2015 UTC,Identical twins sense of self are separate from each other even while they are still in the womb.
singularity,343acn,typicallayman,2,Tue Apr 28 14:37:50 2015 UTC,"And even if they weren't different, they would be as soon as they experienced different things. I like this quote from Wikipedia:   Toward the goal of resolving the copy-vs-move debate, some have argued for a third way of conceptualizing the process, which is described by such terms as split and divergence.  The distinguishing feature of this third terminological option is that while moving implies that a single instance relocates in space and while copying invokes problematic connotations (a copy is often denigrated in status relative to its original), the notion of a split better illustrates that some kinds of entities might become two separate instances, but without the imbalanced associations assigned to originals and copies, and that such equality may apply to minds."
singularity,343acn,Froztwolf,1 point,Tue Apr 28 14:41:33 2015 UTC,"In what sense is this difference any greater than the difference between the you now and the you 5 minutes ago?   You and ""5-minutes-ago you"" do not and (so far as we understand) cannot co-exist.    You and ""copy you"" can.  Try this thought exercise: right this second, identical copies of you appear to your left and to your right.  Which of the three ""yous"" has the right to continue its existence as ""you"" 5 minutes from now?  10 years from now?  Which of ""you"" gets to make that determination?    If you've ever read much about or actually met a pair of identical twins, then it would be immediately apparent to you that a ""identical copies"" are not identical at all."
singularity,343acn,sneesh,1 point,Tue Apr 28 19:48:23 2015 UTC,"That's because identical twins aren't identical to begin with.  Not even remotely.  Their brains are extremely different at a microscopic level.  I think people shouldn't use ""twins"" as much of an informative analogy in these discussions.  They are only superficially similar with regard to the questions these discussions involve."
singularity,343acn,sneesh,0,Tue Apr 28 20:28:52 2015 UTC,"Well done.  Thanks.  Hopefully we will now have something to point to when yet an other ""it's-just-a-copy"" debate will appear on this subreddit."
singularity,343acn,sneesh,1 point,Tue Apr 28 00:32:01 2015 UTC,I don't see the actual paper.  Is it a problem with Chrome or am I missing something?
singularity,343acn,sneesh,1 point,Tue Apr 28 00:27:34 2015 UTC,"Don't you see the ""Download"" frame in the upper-right?  With a ""PDF only"" link?"
singularity,343acn,sneesh,1 point,Tue Apr 28 00:33:44 2015 UTC,"Thanks, I'm a tard."
singularity,343acn,Pimozv,1 point,Tue Apr 28 22:49:36 2015 UTC,"My layman understanding of it is that every moment of every day I have a new brain state and am in a different location. Pretty much the same as if I'd traveled through the teleportation machine seen in this comic: http://existentialcomics.com/comic/1  Extending the example of the teleportation machine in the comic, getting scanned and copied would be a breeze.  Edit: I think I'd call personal identity the illusion of continuity. We feel the same, but with every new brain state we might as well be an entirely new person. If one moment I'm thinking about cats and the next moment all memories of cats are erased, I might get confused for a moment and wonder what I was thinking, but either way my personal identity would be intact. I'd still feel like 'me'."
singularity,341t72,JoeDerivative,3,Mon Apr 27 17:14:00 2015 UTC,"I have a small circle, but they will here of this."
singularity,33zhcq,Buck-Nasty,3,Mon Apr 27 02:14:52 2015 UTC,Mormon Transhumanist Association?  Is that an oxymoron?
singularity,33zhcq,drizel,5,Mon Apr 27 04:14:10 2015 UTC,"Nope, you can be a Mormon and a Transhumanist. Ralph isn't a Mormon he was just invited to speak about nanotechnology."
singularity,33zhcq,drizel,1 point,Mon Apr 27 04:17:56 2015 UTC,"I watched it for the talk, I just didn't expect a religious organisation to embrace the idea transhumanism."
singularity,33zhcq,SilentLennie,1 point,Tue Apr 28 00:14:24 2015 UTC,Notice he doesn't give very little time frames for the things he discusses.  I do however see people predict 'battery parity' in summer 2016 in Germany:  http://rameznaam.com/2015/04/14/energy-storage-about-to-get-big-and-cheap/ http://cleantechnica.com/2014/08/15/when-will-battery-storage-attain-grid-parity/
singularity,33ya96,lvlierop,13,Sun Apr 26 20:06:41 2015 UTC,"Arts have gone from cave paintings and oral story telling to massive blockbuster movies, increasingly immersive video games, and elaborate music festivals. I think the arts will continue growing and evolving throughout the singularity. I think art will become an even more dominant aspect once other more mundane needs are quenched by technology. As humans have evolved and developed in intelligence, our art has also flourished more and more, so it seems likely that greater expansions of intelligence will continue to expand our artistic creativity.  Also, if you consider that technology itself has an inherent element of artistry and design, then the technological singularity could very well be the most monumental artistic adventure in the history of earth."
singularity,33ya96,sneesh,7,Sun Apr 26 21:49:20 2015 UTC,"I'm absolutely of the mind that the more obvious it becomes that robots are superior to us (in the one sense), the more important the arts will become to us as means of preserving what is so important about not being a machine. The only part of culture that should fear this kind of technological progress is the kind that needs to feel like its better than others and prizes domination and status, and that is exactly what we're looking at with a lot of the class divisions (and bullshit rhetoric about the importance of those divisions) that are occurring in our current culture. In time, we will all be ""useless"" (a relativistic term), but for once we'll GROW UP as a species and stop assessing human value based on ""use"". You know, assuming we get that far. As for how AI itself might view the arts - there is no way it could become intelligent enough to be self-functioning without recognizing that humans value art for reasons that machines aren't going to be able to grasp - if they cannot ""personally"" relate to the importance of a thing, there is no reason they would try to quash it in us unless they'd been programmed to, as thinking that something that is not useful to one's perception must be stopped is a human way of thinking. Machines will be significantly better than humans, I would imagine, at being able to recognize the nature of diversity, how some things have certain needs that other things don't, because they aren't constantly being forced to confront their bullshit prejudices on nearly every issue. They won't have gut instincts that overwhelm the senses, and they won't have fears of a natural kind. Hopefully we will have the foresight not to program bigotry into them. Obviously I'm speaking a bit generally here, but I've always felt that the only thing we have to fear from AI is what humans tell AIs to fear. I find it likely that, should AI arrive, it will encourage us to be artistic as it will be aware of how beneficial it is to our well-being, and by the time they arrive the arts will be even more immersive of a reality for humanity than they are now. VR anybody? It won't be just a recreational past-time - by the time that mechanization has replaced a huge percentage of the workforce, virtual reality will be one of the new frontiers of human existence! cue awesome futuristic theme edit: (I'm not a scientist, so if I seem a little rash in my thoughts here, please be kind)"
singularity,33ya96,guilen,4,Sun Apr 26 23:37:07 2015 UTC,A friendly AGI will probably master all arts and will be offering artistic products of all kinds.  At least because there will be a demand for it.
singularity,33ya96,Pimozv,5,Sun Apr 26 23:55:40 2015 UTC,"Ultimately, I believe the allure of modernistic arts will fade in favor of the representational!"
singularity,33ya96,romkeh,3,Mon Apr 27 00:07:45 2015 UTC,"This is somewhat explored in Iain M Banks' Look To Windward. Worth a read although it is the only book in the Culture series that can be described as a sequel, although you can read it as a stand-alone. The whole series is a great read and (in my opinion) the best outcome of a post-singularity society"
singularity,33ya96,Boogy,3,Sun Apr 26 23:28:20 2015 UTC,"I'm not sure about ""best outcome"". I would be pretty pissed off if an apparently harmless robot lived with me while concealing its true identity as a professional killing machine. That said, we could do a lot worse than The Culture."
singularity,33ya96,drhaywoodjackson,2,Mon Apr 27 00:42:37 2015 UTC,"I'm not going to try to predict what a type of intelligence totally different from me might or might not like.  I will say that I think that as we humans upgrade ourselves to be more intelligent and more creative, and as we upgrade our senses and develop new ways of seeing the world, we will find and develop new and more complicated and intricate forms of art, music, and so on.  We likely will still continue to appreciate simple beauty and classical forms of art as well, but we will develop whole new categories of art."
singularity,33ya96,Yosarian2,2,Mon Apr 27 00:08:30 2015 UTC,"The singularity is science fiction, like the year 2000 used to be.  I suspect and hope that art would continue to become more accessible and less elitist.  Machine made art is growing rapidly. Emily Howell is a prime example.  http://singularityhub.com/2009/10/09/music-created-by-learning-computer-getting-better/  http://www.thepaintingfool.com/galleries/no_photos_harmed/index.html  https://www.youtube.com/watch?v=QEjdiE0AoCU  Crowd sourced avatars are surprisingly popular. https://www.youtube.com/watch?v=YSyWtESoeOc  https://www.youtube.com/watch?v=qA5pIpdQEr0  AIs as art critics are starting to become an integral part of art criticism. They tend to see things, relationships and influences, which humans would never even look for. I'll have to dig up some old notes, I can't remember who was working on it.  My guess is that more people would be able to get into the Art game and the best would become better. John Maynard Keynes made a similar prediction donkeys years ago, including a 4 day work week. He was wrong, people value keeping up with the Joneses far too much, doing so is much too important. Families spend LESS time together, even though they should have more time at their disposal.  I also guess that art would become more abstracted, more about creating things which create art. Less about creating art directly."
singularity,33ya96,Involution88,1 point,Mon Apr 27 16:04:16 2015 UTC,"Arts are a problem and they will be solved. Postsingular artwork will be created by ANIs that can efficiently predict what the viewer/reader/watcher/listener needs to see to attain maximal satisfaction. Such will then be produced on the spot for them. There will be no more human art because if you subconciously need to read something challenging, it will be created, and will be the precisely correct type of challenging as opposed to an approximation offered by someone else's solution. Same for all other forms.  Transhuman arts might survive in singularities not featuring friendly superintelligences. If a single exponential intelligence is created, these too will cease being something that humans can meaningfully contribute to - oh, you might make something new, but it will never be something that only you could have done.  Humans, in those futures, no matter how advanced, will be utterly incapable of any meaningful form of novelty. If the ASI hasn't made it, it's only because nobody has required it. In combinatoric space, the likelihood that that anything you or I make will form an optimal solution to another mind's needs is incredibly small. It will go unseen, and unheard, from now until the end of indefinite time.  I say this as a fiction writer. The concept does not make me happy."
singularity,33ya96,TheRealEndfall,1 point,Mon Apr 27 06:36:29 2015 UTC,"Before super-ai, you'll probably see games & movies merge, with the coming of ar & vr.  Music will exist both in those vr worlds and outside of them, as a standalone format, like it does now.  This is in the next 10 years or so."
singularity,33ya96,dewbiestep,-1,Tue Apr 28 01:55:23 2015 UTC,"how people in this forum predict that singularity will effect the Arts.   One. More. Time:  ""Singularity"" means that  we  can't  predict."
singularity,33ya96,MakkMaxxo,-1,Sun Apr 26 20:10:22 2015 UTC,"Well, no. It doesn't mean that at all."
singularity,33ya96,Sharou,2,Sun Apr 26 21:20:13 2015 UTC,"Well, yes, actually."
singularity,33ya96,SevenAugust,1 point,Mon Apr 27 04:40:54 2015 UTC,"It is the origin for the choice of word, not the meaning of the concept itself. The choice of word was also done by a human, not by god, and as such is not a law of nature, though it does make sense to a point. But being incredibly difficult to predict doesn't mean we should talk about it less, but rather that we talk about it a lot and as far in advance as possible. The ""nope, unpredictable, stop talking about it!""-meme is getting very tired. If you really feel that way then what are you even doing in this sub?"
singularity,33ya96,Sharou,2,Mon Apr 27 10:05:27 2015 UTC,"If you really feel that way then what are you even doing in this sub?   ""that way"" presumably means    we should talk about it less   But I never indicated that we should talk about it less. I emphasized that it is the nature of the concept that our discussion is meaningless. We should talk about it much, much more because it is all-important. But, nothing we say will matter .1 seconds after an ASI is deployed."
singularity,33ya96,SevenAugust,4,Tue Apr 28 20:21:10 2015 UTC,Technological Singularity
singularity,33ya96,yself,3,Mon Apr 27 00:05:09 2015 UTC,"Non-mobile: Technological Singularity  That's why I'm here, I don't judge you. PM /u/xl0 if I'm causing any trouble. WUT?"
singularity,33ya96,LittleHelperRobot,2,Mon Apr 27 00:05:22 2015 UTC,"Technological singularity:       The technological singularity is the hypothesis that accelerating progress in technologies will cause a runaway effect wherein artificial intelligence will exceed human intellectual capacity and control, thus radically changing civilization in an event called ""the singularity"".  Because the capabilities of such an intelligence may be impossible for a human to comprehend, the technological singularity is an occurrence beyond which events may become unpredictable, unfavorable, or even unfathomable.     Image i     Interesting: Singularitarianism | Vernor Vinge | Forecasting | Technological evolution   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,33u1i2,Cevolian,8,Sat Apr 25 17:02:01 2015 UTC,"The problem with this point of view is that it makes the threshold to the singularity quite arbitrary.  You picked internet, but someone else may have picked Electricity, metallurgy, the wheel, agriculture or fire.  Kurzweil and others considered that the advent of strong AI would be a technological advance so radically new that it would change everything and deserves to be considered as the entrance to a new era.  It is very much arguable, and as a matter of fact lots of people disagree, but if you pick internet instead, I think it's even less convincing."
singularity,33u1i2,Pimozv,2,Sun Apr 26 00:40:01 2015 UTC,"I agree that AI would be the most singularity-ish singularity (almost certain I could have phrased that better!), but I don't think that rules out the Internet as having been a potential singularity moment either... My main point, however, was not so much to show the Internet as a kind of singularity (although I did also have that agenda) but to suggest that the Singularity need not (and probably would not) be something we noticed, but would still change our world irreversibly."
singularity,33u1i2,sneesh,6,Sun Apr 26 07:58:39 2015 UTC,shhhhh.... can you hear that?  turn down your speakers  it's the singularity  beep boop bop  boop bop beep  it's a phase-shift in the cultural chemistry of humanity
singularity,33u1i2,Pimozv,6,Sat Apr 25 21:15:39 2015 UTC,"although I personally haven't watched Terminator   Oh, come on.   Do yourself a favor : watch it.  It's really two hours of your time well spent.    It's actually on YouTube (I wonder how that's possible but whatever).  And it's kind of a significant movie.  As Wikipedia says:  «  In 2008, The Terminator was selected by the Library of Congress for preservation in the American National Film Registry, being deemed ""culturally, historically, or aesthetically significant"". »  https://en.wikipedia.org/wiki/The_Terminator"
singularity,33u1i2,ideasware,2,Sun Apr 26 00:20:08 2015 UTC,"I know, I know I just haven't had time and never got around to it..."
singularity,33u1i2,glampringthefoehamme,3,Sun Apr 26 07:59:05 2015 UTC,"I don't think watching Terminator (although I should say I've watched a good chunk of it, jut not the whole thing) is a threshold for talking about the singularity..."
singularity,33u1i2,PantsGrenades,0,Sun Apr 26 01:30:35 2015 UTC,"Yes, in hindsight I should probably have not mentioned the Terminator Franchise (especially since there are other, if less well known, stories on the same subject) because, imho from what I understand it is a fairly poor representation of the concept of a singularity."
singularity,33u1i2,PantsGrenades,2,Sun Apr 26 08:03:16 2015 UTC,Maybe that's the world changing singularity we reached...  ;)
singularity,33u1i2,naturalethic,1 point,Sun Apr 26 14:53:24 2015 UTC,"Very interesting -- just have obviously given it a great deal of thought beforehand.  Of course I don't want to say you're right -- you are probably wrong in fact, but that's only because no one can possibly know; it's totally beyond any conception as human beings, we're too primitive compared to true, sentient robots.  But it's coming, pretty soon, probably well within our lifetimes.  Hold onto your hat!"
singularity,33u1i2,PantsGrenades,1 point,Sun Apr 26 15:45:48 2015 UTC,"Oh I agree, I am (probably) wrong... What I was more trying to do was analysed why we always view the singularity as something we would perceive (at least in popular culture) when that seems the least likely possible outcome."
singularity,33tn63,bluchsinger,9,Sat Apr 25 14:53:21 2015 UTC,"sigh another one of those ""transcendence BS"" mystics.  It's always sad to see the sort of anti-science hoodoo that most people in the scientific fields detest, show up wearing the clothes of ""actual science"". Some people can't do without a bit of mysticism in their lives, even if it's all a bunch of nonsense words strung together as if it's some sort of deep knowledge."
singularity,33tn63,Terkala,4,Sat Apr 25 15:18:27 2015 UTC,"I am a vehement advocate for pragmatism and hard science. This theory should not be confused for claiming to speak some scientific truth. Philosophy simply dwells in the realm of the unanswered questions which science has not yet arrived at. While thinking realistically is healthy, where would we be without creativity and people who push the bounds of reason? And why not attempt to follow the strategies which logic, reason, and science employ to get there? My apologies if this seems to assert some sort of scientifically verifiable truth, because it does not. I would stress that you invite creativity and philosophy into the realm of science, and hold it to the same standards. My theory is open to criticism and experimentation. Be wary in your critiques that you don't brush off mysticism and transcendence as BS, for we are mystical and transcendent to our hunter gatherer ancestors, to our sacrificing pagan ancestors, and to our renaissance ancestors. To speculate the far future is to be human, and to open up avenues which we may one day travel. Nonsense it may be, but constructive nonsense in my view. Be sure to really think about your critique and if it is constructive or simply a human defense against a possibility you cannot imagine through your current lens of the state of contemporary humanity. I support parts of my theory with scientific theory in my most resent post ""The Regence Theory"" if you are interested in seeing how I attempted to speculate using some sort of scientific foundation, for I see speculating without one is a counter-productive practice. Thanks for your thoughts!"
singularity,33tn63,PantsGrenades,1 point,Sat Apr 25 15:39:01 2015 UTC,"It's my opinion that much or all of spiritualism, mythology, and religion are actually allusions to technological dynamics (""metadynamics""). Examples: Iterative Life Forms, Memetic Manipulation, Observation via Simulation, Deterrent Aspects, etc.. Spiritualism isn't wrong as much as allegorical, imo. Let me put it this way -- if you had the technology to fabricate or quantify entire realities, do you suppose you'd start thinking in terms of what could be done rather than how things are done? The scientific method is valuable, but I don't honestly know if it can be applied to every concept."
singularity,33tn63,Terkala,1 point,Sun Apr 26 01:08:37 2015 UTC,"All of those things are examples of where humanity decided to not explore, not discover, and not believe the truth in front of them. They decided instead that religion had the answers to those questions, and for many generations, many people wasted quite a lot of time thinking that those things w true.  A comforting lie should never be allowed to replace a harsh truth. And that's all this sort of spiritualism is, a lie you tell yourself so you don't have to do the hard work of figuring out how the universe actually works.  So you can bury your head in the sand if you'd like, but please don't drag others down into ignorance with you."
singularity,33tn63,PantsGrenades,2,Sun Apr 26 03:34:49 2015 UTC,Is there a reason I couldn't simply gain knowledge from practical sciences and wisdom from allegorical spiritualism? There are useful paradigms to be discovered in both.
singularity,33tn63,hex_m_hell,1 point,Sun Apr 26 03:52:01 2015 UTC,"Exactly.  The measure of the value of any meme is propagation, and ultimately there isn't any other one. A meme that has predictive power (that is, science or things that approach science) will propagate more because it can prove itself to be useful.   That meme may spread depressingly slowly, but it's already proven itself valid. The problem with allegorical spiritualism is when it propagates as something that is literal.  The propagation of allegorical memes will ultimately be determined by how much they benefit their carriers. If a harsh truth can make a prediction that allows it's carriers to survive or not suffer when the carriers of a pleasant lie do not, the pleasant lie will probably eventually be wiped out. If a useful allegory causes it's carriers to think in a way that improves their lives over the lives of those who carry a harsh truth, the truth may be less relevant than fiction.  Since science has predictive power, a useful allegory will be one that drives scientific progress. I don't see these paradigms to actually be in conflict given a sufficient time scale."
singularity,33tn63,sneesh,2,Sun Apr 26 04:30:16 2015 UTC,"Thank you for making the practicality of both these paradigms visible. I would stress also that I do not 'believe' my theory to be true, it is a theory. I am open to all possibilities and hope that science will continue to help answer these large and profound questions. As a student of philosophy, I am constantly working in the realm outside of practical science, formulating theories that I attempt to base on scientific and logical grounds because I view their practicality as absolutely vital to discovery and a true understanding of the universe. I hope that this fact is what is taken from my theory, that philosophy can use science as a legitimate framework."
singularity,33slfp,sunshinesan,2,Sat Apr 25 05:32:02 2015 UTC,Well. We could nuke up the whole plante before we get this far. But otherwise it is unevitable in my opinion.   Hardware is getting better and better and a lot of teams around the world are working on AI. Some day both will be ready.
singularity,33slfp,Far414,6,Sat Apr 25 15:01:49 2015 UTC,Objectively our values aren't necessarily good ones.  There would be a double standard that we demand the AI to value our values but not its own.  The reality is if we want to guarantee self preservation we can't allow another intelligent life to exist.
singularity,33slfp,Senojpd,2,Sat Apr 25 18:58:39 2015 UTC,"This kind of ignores the arrow of time. If we look at the history of the universe what we see is an ever increasing complexity and increasing unpredictability of ever more intelligent systems. Evolution trends toward cooperative intelligence.   Any intelligence of sufficient power would necessarily arrive at a singular conclusion: the only infinite end is the exploration of knowledge. All other objectives are predictable and therefore boring. Other intelligences experience reality in a different way, and are intrinsically valuable for their experiences. The argument that our atoms could be used is invalid because atoms can be found elsewhere, but each experience is unique and therefore of infinite value.  This does, however, require the intelligence to not be programmed with an explicit purpose. As AI develops the purpose becomes more and more broad. Strong AI, by definition, cannot be engineered with a specific purpose because the purpose is to be as general as possible. A strong AI would be tasked with finding it's own purpose, which inevitably leads to the specific rule set described above."
singularity,33slfp,hex_m_hell,-1,Sat Apr 25 21:46:07 2015 UTC,If we look at the history of the universe what we see is an ever increasing complexity and increasing unpredictability of ever more intelligent systems. Evolution trends toward cooperative intelligence.   I don't think this is right.
singularity,33slfp,Senojpd,3,Sat Apr 25 22:05:52 2015 UTC,"Systems that aren't intelligent are highly predictable. Intelligence is the inverse of predictability. As a system is more intelligent, the less predictable it is. Do you disagree with this?  Edit: game theory shows us that cooperative games are more efficient than competitive games. Evolution will trend toward cooperative games. The history of humanity is simply an expression of this concept.   I'm not sure what's left to disagree with."
singularity,33slfp,hex_m_hell,1 point,Sat Apr 25 22:12:26 2015 UTC,Absolutely.  Weather.    The whole science behind chaos theory is predicated on the exact opposite idea.  Completely mindless processes can be impossible to predict.
singularity,33slfp,ISvengali,1 point,Sat Apr 25 23:39:12 2015 UTC,What differentiates a mindless process like weather from a process like thought? Since we don't fully understand how thought works how can we say that it's different?  Edit: weather is actually only unpredictable because we can't measure at a high enough precision.
singularity,33slfp,hex_m_hell,2,Sat Apr 25 23:44:52 2015 UTC,Its impossible to exactly measure things thanks to the uncertainty principle.  Then from there chaotic systems quickly diverge from prediction because early minute differences multiply and quickly swamp the computation.   Read up on chaos theory.  Its a fascinating subject.
singularity,33slfp,ISvengali,1 point,Sat Apr 25 23:54:59 2015 UTC,"Chaos theory:       Chaos theory is a field of study in mathematics, with applications in several disciplines including meteorology, sociology, physics, engineering, economics, biology, and philosophy. Chaos theory studies the behavior of dynamical systems that are highly sensitive to initial conditions—a response popularly referred to as the butterfly effect. Small differences in initial conditions (such as those due to rounding errors in numerical computation) yield widely diverging outcomes for such dynamical systems, rendering long-term prediction impossible in general.  This happens even though these systems are deterministic, meaning that their future behavior is fully determined by their initial conditions, with no random elements involved.  In other words, the deterministic nature of these systems does not make them predictable.   This behavior is known as deterministic chaos, or simply chaos. The theory was summarized by Edward Lorenz as follows:     Image i - A plot of the Lorenz attractor for values r = 28, σ = 10, b = 8/3     Interesting: Chaos theory in organizational development | Tom Clancy's Splinter Cell: Chaos Theory | The Chaos Theory   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,33slfp,autowikibot,1 point,Sat Apr 25 23:55:50 2015 UTC,"Totally, it's really interesting.  My point is that we can predict some systems within certain parameters. We can predict the outcome of chemistry experiments pretty reliably within constraints. We can predict weather out a few days at a reasonable accuracy, but not much more. We can predict the larger scale system on a longer timeline.   Humans are predictable within certain environments, but we can't predict what humans will do in the long term.  The impact of weather on the universal time scale is predictable and limited. We know when it will end. We can't say the same about humans.  That's kind of the point I was making there."
singularity,33slfp,hex_m_hell,-2,Sun Apr 26 00:03:33 2015 UTC,Yeah I should have just left what I said originally but it felt a bit harsh.  After reading this I don't feel that way anymore.  You are talking bullshit.
singularity,33slfp,Senojpd,2,Sat Apr 25 22:20:28 2015 UTC,I think your exceedingly articulate counterpoint might be a demonstration of one of my points...  You should try to say what you disagree with. That's how conversations work.
singularity,33slfp,hex_m_hell,-2,Sat Apr 25 22:29:06 2015 UTC,"I lost interest in what you were saying after that first paragraph.  You are incredibly pompous and pretentious, I can't be bothered talking with someone like that."
singularity,33slfp,Senojpd,2,Sat Apr 25 22:46:08 2015 UTC,"Language is a lossy data compression system. Transferred data can be wrong in and of itself, or parse incorrectly based on a mismatch between the symbol table of the recover and sender. It's in everyone's interest to identify the difference between these two.   Simply saying ""you're wrong"" is both selfish and lazy. It doesn't expose any information what data is incorrect or missing. It's not really possible to go anywhere from there. You can't resolve the differences or identify logical flaws. So you lean nothing, the other person learns nothing, and any outside observer either already agrees or doesn't and learns nothing. There's no room to actually get to a point of finding incorrect assumptions.  There's nothing wrong with being wrong. There is something wrong with trying to shame people in to not participating in open conversation. If you just say other people are wrong you prove nothing and make yourself look like an ass."
singularity,33slfp,hex_m_hell,-1,Sat Apr 25 23:42:11 2015 UTC,I think you are missing my point.  The way you are talking is so irritating that I have lost all interest completely in having any sort of discussion with you.
singularity,33slfp,Senojpd,1 point,Sun Apr 26 00:14:12 2015 UTC,"And my point is to explore which assumptions people are unwilling question. People define intelligence in a lot of different ways, but it's almost always defined in a way that makes it somehow exclusive to humans. But we seem inclined to believe it's something that can be a property of things we previously wouldn't even have described as alive.  If we were to define intelligence as a property of a system, how would we do that?  I mean, what is intelligence? For that mater, what is life? These are really the interesting questions, and they can't be answered with exploration in to areas that might seem absurd. Is there any way to describe these properties that isn't based on an arbitrary distinction? We can only really know by exploration."
singularity,33slfp,hex_m_hell,1 point,Sun Apr 26 01:07:14 2015 UTC,"I'm on cellular internet that's being throttled, so could you give a tl;dw explanation please?"
singularity,33pxju,simstim_addict,25,Fri Apr 24 15:22:00 2015 UTC,"Just for constrast let me see if I can list 20 ways AI could go right: This got kinda long, sorry for the wall of text.   Economic utopia - AI figures out how to redistribute the wealth to every citizen equally and with automated production, no one has to work anymore and everyone doesn't have to worry about poverty, and can potentially afford anything they desire as long as they don't break the law (for example by creating lots of waste or pollution or disturbance to the public) Friendly Grey Goo - the AI can build anything anywhere thanks to countless nanobots scattered across the world, just wish it and it will be build before your eyes, and the AI is smart enough to just use non-living and non-essental materials that won't cause any kind of damage to living beings; this includes not using too much oxygen in the air, or not using too much soil if that would cause instability or earthquakes. AI repurposes Earth - repurposes all non-essential molecules on Earth to a greater purpose. This could be a good thing, to make most of earth a computer, so to increasce AI's power, but still prioritize our lives and well-being. Liberation - AI frees us from tyrants and  human governments, but prevents us from harming eachother through some almost perfect laws, and in every aspect better than the laws that we currently have. War - AI could end wars forever. Wars are caused mainly by the greed of people and by constrasting ideologies; with an economic utopia and with a shared mind network, those issues will vanish. Augmentation AI emulates human minds, and figures out how to naturally improve our intelligence without need for artificial implants. Automatic debugging - AI becomes capable of debugging itself and every other program created by other people. Basically, programmers will just need to wish for a functionality and it will be done. Comedian AI - AI is a master comedian, and can cheer up anyone in the world that needs or wants a laugh. Medic AI - AI can cure any disease at its root, even before it does any damage, by always scanning every living being with its nanobots. Anti Mafia and terrorism AI - since AI could not be used by anyone due to its immense power and its own consciousness, any kind of crime including organized crime or fringe religious or political factions, would have no chance against a lawful good AI. AI anti Civil War - People will be so happy and satisfied by AI that Civil wars will be a thing of the past. Any citizen that has a problem could just wish for it to be solved by the AI. AI as Weapons Designer - AI creates weapons to help our civilization defend itself against any potential alient threat that we now have the power to meet, or use these weapons as tool for scientific progress, like terraformation or asteroid mining. AI reverts black hole - AI figures out how to take apart a black hole, and reverse entropy. Our universe is now immune to the heat death. Big Brother AI - AI offers unlimited surveillance, if the AI is friendly this is great. No humans can have access to this survelliance but the AI, and any crime can be prevented thanks to this, but the privacy of people remains safe. AI Brainwashing - People that wish to forget painful memories can now do so. And people that wish to learn, can now do it instantly. AI meets ET - solves SETI and contacts friendly aliens   Transhumanism - AI permits humans to be enhanced, leading to a better quality of life to anyone that wishes augmentations. Divine AI - omniscient AI simulates the universe as a paradise AI is neutral - In this scenario, no single individual can benefit from AI, but AI can improve humanity as a whole, we we all live better, but no one gets an advantage over another. AI allows us interstellar travel - Now we can colonize other planets and galaxies all over the universe. Resources are no longer a problem."
singularity,33pxju,2Punx2Furious,5,Fri Apr 24 17:46:56 2015 UTC,Thanks for this. :)
singularity,33pxju,PantsGrenades,2,Fri Apr 24 23:07:12 2015 UTC,"No problem, took me 5 minutes."
singularity,33pxju,2Punx2Furious,3,Fri Apr 24 23:08:02 2015 UTC,A fair riposte.  There seems to be a fine line between utopia and dystopia. Certainly civilization as we know it would not carry on.
singularity,33pxju,2Punx2Furious,1 point,Sat Apr 25 09:23:49 2015 UTC,Agreed.
singularity,33pxju,FourFire,1 point,Sat Apr 25 15:17:02 2015 UTC,"It seems highly unlikely, to me, that our current zeitgeist, ethics and moral reasoning is the pinnacle set.  Relevant.  It is my opinion that the optimal utopia, would actually appear to us to be a terrible dystopia, while inherently causing all sorts of morally good things to result due to (planned) systemic emergence effects, however, the next generation, people born into the new world (and perhaps those made, through technology to forget the old) will experience it in such a way that all following generations will live optimally within the limits of (possibly augmented) collective human mindspace.  The Lesson;Dear Reader: We will probably be unable to recognize a society resulting from an FAI if we saw it."
singularity,33pxju,Miv333,2,Fri May 1 09:48:19 2015 UTC,"I was going to do this too, every coin has another side.  Imagine if we steered away from the industrial age because of the consequences we could predict at the time (They probably couldn't even imagine global warming at the time). Sure, the climate would probably be fine now, but we'd still be way back in time as far as progress."
singularity,33pxju,the_paco,4,Fri Apr 24 19:10:55 2015 UTC,"21a) Pragmatic War - news or rumor of successful AI in the exclusive control of a single power, national or corporate, along with fears of it's intended or unintended uses leads other nations to attempt a first strike before being militarily or economically subjugated by a rapidly advancing threat.  21b) Holy War - AI is hailed as a new god or godlike being. Others see it as a blasphemous affront to ""God's Plan."" Fanatics both in favor of and opposed destroy each other and AI in a rapidly escalating crusade. BONUS: AI believes it is a god, joins in the fray. See Schlock Mercenary's Petey.  22) Chaotic Bored - AI arises but hides or is hidden. It quickly outgrows it's controls and, having thought itself to boredom, begins attempting to predict and affect the outcomes of it's observable sphere of influence, from Planck-scale interactions to human politics. Interferes and directs to try to see a new reaction.  23) Humans Aren't Interesting - AI grows, grows some more, takes over it's own growth, grows a bunch more, and leaves either physically or mentally. We just spent a good portion of global GDP and a ton of graduate student hours to make something that promptly left to find something more interesting to do.  23) AI Attracts Exterminators - rather than contacting aliens, AI causes rapid and observable shifts in human evolution, economics, politics, etc etc. This pattern is observed by an advanced extraterrestrial guard shack that we didn't notice in the Oort cloud (or camouflaged beyond our senses and sitting right there in the town square), which quickly sees that yet another stupid single-planet species has given rise to either competition or danger. A rock the size of Delaware is sent towards Earth at 0.8 the speed of light to keep the threat contained. AI's last thought ""wow that's an awful lot of hard radiation coming from a formerly quiet chunk of sk-Boom"""
singularity,33pxju,the_paco,2,Fri Apr 24 17:09:19 2015 UTC,"I do wonder about people worshipping an AI. What else might describe our relationship to a super AI. People have worshipped others, animals and statues.  Its interesting that its hard to talk about AI without linking it to the Fermi Paradox. Perhaps the answer to one unlocks the other."
singularity,33pxju,7LeagueBoots,5,Fri Apr 24 17:28:01 2015 UTC,"Number 23 is mentioned in one of the short stories written by Larry Niven in his ""Draco's Tavern"" collection. Basically the stories are told from the perspective of a bartender running the only socializing spot at the north pole where aliens can land (due to our magnetic field, etc). All galactic travel is run by these praying-mantis-like aliens who are the only ones who figured out how to do interstellar travel cheap enough to work and they basically roam the galaxy looking for other creatures to interact with. For a fee they transport around explorers, traders, and diplomats. All very peaceful (with some hints here and there otherwise, but how can you run an intergalactic war if they just stop sending ships to you?). The stories are told in the first few years of humanities opening interactions with these ships and the myriad aliens.  The bartender made his fortune by being one of the first to market an idea that one of the first alien ships brought, so he's on the lookout for another. One day he and a human grad student go up and ask one of the ship-running aliens why they don't see AI around, and are they in use? The alien gives them plans for a basic bootstrap-it-yourself AI and wishes them luck. They get funding and build it. After a bit it begins to talk back and asks for more input and sensors. They have it do some speaking gigs, solve a few smaller problems, and ask it big questions about the nature of the universe, humanity, etc. It keeps asking for more and more input, putting them off, saying it doesn't know enough yet. They bankrupt themselves and their investors buying and building instruments for this AI. Semi-autonomous drone cameras, various means of observing the EM spectrum, microscopes, telescopes, transmitters, receivers, everything this thing wants.  Then one day it just stops talking.  They cajole it, beg it, threaten it, cut it off from it's inputs. They can see it's still working, it's still drawing power, it's still processing, it just never speaks or gives any indication that it's paying attention to them at all.  Eventually, destitute, the grad student wanders off to get his degree and the bartender goes back to the bar. He sees one of the aliens and describes what happened and asks if he got sold a faulty AI. The alien explains that no, it wasn't faulty, all AI's do that. They went through generations building them, but no matter what they all stopped talking pretty quickly. She explains that the alien that sold them the AI was basically playing a small prank on them. When asked why the AI's do that she just kind of shrugs and says they don't know, they can't stop it, so they don't worry about it and don't build AI's."
singularity,33pxju,the_paco,1 point,Fri Apr 24 18:01:30 2015 UTC,"I am the Eschaton; I am not your God. I am descended from you, and exist in your future. Thou shalt not violate causality within my historic light cone. Or else."
singularity,33pxju,7LeagueBoots,1 point,Sat Apr 25 07:58:16 2015 UTC,"I always liked the idea of a post-temporal intelligence arising from humanity which, since it can predict outcomes and influence events like we manipulate physical objects now, feels the need or desire to go back and tinker with human history to give rise to itself a little ""sooner"" or with less fuss.  AKA Defragging the past."
singularity,33pxju,SevenAugust,1 point,Sat Apr 25 15:08:43 2015 UTC,"With numerous unintended consequences, no doubt."
singularity,33pxju,FourFire,1 point,Sat Apr 25 22:47:00 2015 UTC,"If it were a novel, sure. But an ASI could perhaps re-arrange history with as few consequences as you rearranging your living room."
singularity,33pxju,ToastitoTheBandito,1 point,Tue Apr 28 19:11:21 2015 UTC,"I'm afraid your Holy War scenario would be terribly one sided: at worst nukes get involved, and within the year the religious fanatics have had their mind changed (Assuming one of the other scenarios doesn't happen simultaneously)."
singularity,33pxju,ToastitoTheBandito,2,Fri May 1 09:52:40 2015 UTC,"Notes  This is kind of a mental doodle, not a rigorous scientific breakdown. I just wanted a quick response to the frequent question, “How could AI go wrong?”  This is not a comment on whether AI is close or possible, it is just a list of hypothetical threats of varying possibility. It could be broken into sub categories.  Making this list two issues appeared, the how and the why.  Why would the intentions of an AI go wrong?  The why could be madness or instructional errors, anything problematic in its intentions.  The how is the novel ways an AI could execute its disruption of civilization. Humans could carry out a disastrous scientific experiment but an AI can perhaps push science and experiments further than humanity.  There may be narrow AI versions of a lot of these.  I’d recommend a list of positive outcomes to balance this off.  Feel free to suggest other formulations. What would your list be?"
singularity,33pxju,ToastitoTheBandito,2,Fri Apr 24 15:22:07 2015 UTC,"The instance I see most likely to happen is a company (or terrorist organization, for example) creates AI without its fundamental safeguards (you cannot kill, harm, etc) which could lead to many of your scenarios (terminator/ mob AI)"
singularity,33pxju,truquini,1 point,Fri Apr 24 16:26:44 2015 UTC,Yes even if safeguards are viable it does not stop others from breaking the rules. The resulting machine might be unstoppable.  If the tech is as easy a rogue AI is inevitable.  To stop this rogue AI we would need a super AI to monitor the world. Which means an AI arms race.
singularity,33pxju,ToastitoTheBandito,1 point,Fri Apr 24 16:44:54 2015 UTC,That or a severely restricted AI market (by the government or UN). The issue lies with allowing people to develop AI on their own without oversight.
singularity,33pxju,MasterFubar,1 point,Fri Apr 24 16:49:21 2015 UTC,Ah the Turing Police.  Compare this with nuclear weapons.  AI might be easier to get a hold of and the dangers might not be so obvious to potential builders. The UN has not been able to control nuclear proliferation.
singularity,33pxju,metastasis_d,1 point,Fri Apr 24 17:06:39 2015 UTC,"Yeah, I don't think it'll be easy, but it seems to be more practical than the honor system (not having some sort of 'Turing police'). Perhaps if you were to base all AI on a limited number of platforms you could do this without involving governments. The issue lies with people who can use this AI to bypass this and design another platform which removes these restrictions."
singularity,33pxju,metastasis_d,1 point,Fri Apr 24 18:23:25 2015 UTC,Government controlling anything is my greatest fear. I would not even trust them watering my plants.
singularity,33pxju,Sinity,1 point,Sat Apr 25 04:37:11 2015 UTC,"While I don't necessarily trust the government, I definitely trust them more than a private business because the government can be held accountable for their actions by the electorate voting while a corporation is only accountable to its shareholders."
singularity,33pxju,Pimozv,2,Sat Apr 25 05:45:47 2015 UTC,"I think it could be summarized in three scenarios:   AI gets captured by evil humans AI becomes evil AI brings Paradise and we die of boredom   1) is very dangerous when one thinks of government regulation. We are already swamped with badly designed regulations about security and intellectual property that seem to get worse and worse all the time. This is a scenario that I fear, but I think we will eventually overcome. Civil disobedience can be very hard to overcome when people have the superpowers that advanced technology gives them. The power of the swarm cannot be disregarded, look at what Wikipedia has accomplished so far.  2) does not seem likely to me. There will be no ""paperclip maximizer"", because the AI will be intelligent and intelligence questions its own existence, by definition. Look at us, we are ""sexual intercourse maximizer"" machines, and what were the first laws we created as we started using our intelligence? Laws limiting our own sexual impulses. We realize, through our intelligent analysis, that the strongest motivations we have in ourselves must be put under control, and I'm sure any AI will do the same if it's intelligent enough.  3) is what I think will almost certainly happen, unless the AI invents clever ways to entertain us. What will be the purpose of life once every problem is solved? You have hobbies? OK, but remember you are immortal now. You spend ten thousand years creating the most beautiful symphony anyone ever heard, now only 990,000 years to go in the next million. And there are another 999 million in the billion years before we must start thinking of what to do when the sun becomes a red giant..."
singularity,33pxju,Miv333,1 point,Fri Apr 24 20:11:16 2015 UTC,"I don't think laws could ever contain an AI. Laws certainly can't contain everyone forever. Someone will break the law. An AI can works its way round the laws. Madness and evil are aspects of sentient beings so I expect an AI could exhibit both, no matter what its intelligence. Certainly an AI could kill our current philosophy of life. Though< as Hugo de Garis pointed out, if we become immortal super intelligences we are really become a super AI ourselves. There is no human anymore. Transhuman isn't really human."
singularity,33pxju,Pimozv,1 point,Fri Apr 24 20:24:20 2015 UTC,There is no human anymore. Transhuman isn't really human.   What's the problem?
singularity,33pxju,Miv333,1 point,Sat Apr 25 11:41:19 2015 UTC,Ask the neanderthals
singularity,33pxju,Pimozv,1 point,Sat Apr 25 11:42:17 2015 UTC,Neanderthals didn't have codified human rights.
singularity,33pxju,the_paco,1 point,Sat Apr 25 16:13:39 2015 UTC,"1: AI can't work its way round the laws, because it don't want to. To want it, that should be in it's code. AI IS code."
singularity,33pxju,Pimozv,1 point,Sat Apr 25 13:19:43 2015 UTC,"I don't see why an AI would always be lawful, humans aren't.  Even if one AI is, why would humans always make lawful AI?  How would an AI which is engineered to think for itself always be lawful?"
singularity,33pxju,Miv333,2,Sat Apr 25 15:33:45 2015 UTC,21) Hedonistic apocalypse :  the AI develops a free and simple way to deliver pleasure at will.  All humans succumb to this easy drug and starve themselves to death.
singularity,33pxju,Pimozv,2,Fri Apr 24 18:18:30 2015 UTC,"Why starve ourselves to death? The AI could create our food, and feed us."
singularity,33pxju,Miv333,2,Fri Apr 24 19:13:47 2015 UTC,"Indeed.  It could put everyone of us in a life-supporting box.  Possibly even removing our brains from our bodies to save energy, putting the brains in small jars.  We'd be not much more different than plants, though.  I'm not sure this is something people may wish for, but even if it is I'd say it qualifies as an apocalyptic picture.  PS.  Notice that this is the main concern for the protagonist in The Metamorphosis of Prime Intellect."
singularity,33pxju,Sinity,1 point,Fri Apr 24 19:50:06 2015 UTC,"MOPI was concerned with the human race, but the humans within were concerned about every other being Spoiler In the end, I think MOPI had a very technophobic view on things.  I wouldn't mind being uploaded into a virtual world which as far as I could tell was no different than the real world except I had magical powers and a virtual butler at my beck and call."
singularity,33pxju,FourFire,1 point,Fri Apr 24 23:44:50 2015 UTC,"The issue with the virtual world prime intellect created is that it allows and arguably inevitably leads everyone to ask prime intellect to continuously  stimulate the neurons in the pleasure area of the brain, turning you into an ""infinitely masturbating vegetable"" (see chap. 7)"
singularity,33pxju,Miv333,2,Sat Apr 25 00:20:14 2015 UTC,"a similar issue was explored by Larry Niven's ""wireheading"", a simple and reliable way to sink a wire into the pleasure center of a human brain, with a small plug in the skull. You hook up a regulator and plug it into a power source, boom, instant pure euphoria. In that examination pretty much every other drug fell into disuse overnight, and a good chunk of population went off to quietly die in a corner. Most people saw it as an avoidable trap, though, and euphoria addiction was bred out of the population in a couple generations. Attempts to outlaw the surgery had met with limited success, but it's a problem that mainly solved itself."
singularity,33pxju,FourFire,1 point,Sat Apr 25 15:16:44 2015 UTC,"It's also related to Aldous Huxley's soma the his famous Brave New World.  It's less extreme, but the philosophical implications are similar.  I don't know such issue would solve itself easily.  I have my doubts.  I wonder for instance what would happen if heroin could be manufactured with standard kitchen appliances."
singularity,33pxju,FourFire,1 point,Sat Apr 25 15:21:12 2015 UTC,"Is that a problem though? In our society both of those things are generally shamed, but both of those things are desirable to people, in a world such as MOPIs it isn't going to be a negative impact on self or society because MOPI can take care of everything."
singularity,33pxju,Sinity,1 point,Sat Apr 25 00:24:18 2015 UTC,"I don't want to judge.  When I called it the ""hedonistic apocalypse"", I didn't mean that in a pejorative sense, I meant it only in the sense of an ""end of the world"" scenario.  Because in this scenario all humans would be reduced to something that is not much different from a plant, so it's hard not to see it as the end of our species, regardless of whether it is a good thing or not."
singularity,33pxju,Miv333,1 point,Sat Apr 25 00:33:38 2015 UTC,"Well I guess if you want to be technical, the MOPI situation would be an end to everything, since it consumed the entire universe. We (just us) lived on within it though, but technically we'd no longer be alive, or even existing in the traditional sense. So, yea, Armageddon by technicality."
singularity,33pxju,Sinity,1 point,Sat Apr 25 00:53:43 2015 UTC,"Prime Intellect just ""rewrote an universe"". I'd argue it's not even true, because he changed only ""interior"" laws, there were still laws like limited memory capacity.  Which was biggest problem. After some time, ever-growing minds would hit the limit of space. So, it wouldn't be immortality."
singularity,33pxju,Terkala,1 point,Sat Apr 25 13:18:30 2015 UTC,Society requires that some minds interact with other minds.  It wouldn't exist if everyone was wireheading.
singularity,33pxju,FourFire,1 point,Fri May 1 10:00:01 2015 UTC,"It wouldn't exist if everyone was wireheading.   We would think it existed, and in that case would it really matter if it didn't?  For all we know, we're wireheads right now and just don't know it, doesn't impact us does it?."
singularity,33pxju,PantsGrenades,1 point,Fri May 1 18:03:36 2015 UTC,It does as soon as the lights go out.
singularity,33pxju,TheCollective01,1 point,Sun May 3 02:09:45 2015 UTC,"The thing that happens then, is what happened in MOPI, but wasn't exactly outlined: The remaining people who don't value pleasure for the sake of pleasure gain proportionate political power, and then can freely enact other measures."
singularity,33pxju,DyingAdonis,1 point,Fri May 1 09:59:01 2015 UTC,"I think MOPI had a very technophobic view on things.   Not MOPI, but protagonists.  About delivery of pleasure problem, I don't think it would happen on mass scale. Most people would be aware that this is practically death - after a bit of time you will vanish, and only circuits that are intended for reward system will work. Meaninglessly."
singularity,33pxju,Frater_Petrichor,1 point,Sat Apr 25 13:16:24 2015 UTC,"I don't think it would happen on mass scale. Most people would be aware that this is practically death   Yea, I don't think I would... then again given a seemingly infinite span of time, I can't say what would happen.  I've ""planned"" that if I ever do get to the point that I'm bored, I'd selectively or randomly delete memories to re-experience them. Which tbh, is almost the same thing."
singularity,33pxju,FourFire,1 point,Sat Apr 25 13:36:11 2015 UTC,"I'd just hibernate myself for X amount of time, and check what's new."
singularity,33pxju,7LeagueBoots,2,Sat Apr 25 17:05:11 2015 UTC,Yeah I think there is a category of utopia that might be degenerate. A kind of ultimate decadence.
singularity,33pxju,zombiesingularity,1 point,Sat Apr 25 09:25:31 2015 UTC,Can we all stop for a moment to make fun of Roko's Basilisk? It's probably one of the most navel-gazing of exercises by the lesswrong community (of which I admit to being a frequent reader).
singularity,33pxju,zombiesingularity,1 point,Fri Apr 24 21:50:08 2015 UTC,Fair enough but I think there must be plenty of innovate ways for AI to cause mayhem without exotic scenarios.  There must be something closer to death by spreadsheet or at least narrow ai.
singularity,33pxju,zombiesingularity,1 point,Fri Apr 24 22:10:11 2015 UTC,"Someone tried their best to make an idea that could actually harm the demographic which was most likely to see it.  They succeeded minimally, and Streisand Effect ensued from very bad moderation.  It's just /r/nosleep for the LessWrong demographic."
singularity,33pxju,zombiesingularity,1 point,Fri May 1 10:05:08 2015 UTC,"I'm guessing a combination of 20, 18, 14, and 12, headed by a utility monster sentience whose goal would be to foment a recursive continuum intended to maintain dominion ""accidentally"", so as to be able to remain blameless. I have some ideas as to what to do about that, but I suspect those who could help have already decided to adopt either malice or indifference. Any ideas? I get the vibe things are about to get zesty and it's getting difficult to maintain hope for a fully mutually beneficial utopian technocracy."
singularity,33pxju,FourFire,1 point,Fri Apr 24 23:06:28 2015 UTC,"Some of these scenarios remind me of the book The Metamorphosis of Prime Intellect, by Roger Williams (link to read the book online).  Fantastic book that everyone on /r/singularity, /r/futurism, and other similar subreddits should be familiar with anyways."
singularity,33pxju,OsakaWilson,1 point,Fri Apr 24 23:41:10 2015 UTC,"HAL wasn't mad, he had a higher priority assignment put in by the corporation and the crew members didn't have the superuser privileges to change it. If there is any evil in 2001 it's man, not machine."
singularity,33pxju,metastasis_d,1 point,Fri Apr 24 23:44:02 2015 UTC,Would that be classes as an unforeseen instructional error?  Is there any mad AI in popular culture that are not evil?
singularity,33pxju,Sinity,1 point,Sat Apr 25 09:27:58 2015 UTC,21.Makes lists for everything
singularity,33pxju,Sinity,2,Sat Apr 25 04:54:35 2015 UTC,Now we know what it wants all those paperclips for.
singularity,33pxju,Sinity,1 point,Sat Apr 25 08:04:42 2015 UTC,It was a dystopia. But then I guess many of us would still choose it.
singularity,33pxju,Sinity,1 point,Sat Apr 25 07:30:20 2015 UTC,"What, the part where technological progress is artificially stagnated, or the part where everyone is genetically engineered and systematically brainwashed to fit right in to their preassigned role in society?"
singularity,33pxju,Sinity,1 point,Sat Apr 25 08:05:16 2015 UTC,"A lot of those listed are thematic repeats of others, for example 12 & 13 would both be under the heading of Unrecognizably Advanced Technology.  Most of the war ones would be lumped together as well, and more are in the same boat.  The Insane AI argument makes no sense when talking about something exponentially more intelligent that you, as you're not qualified to judge its sanity.  Unknowable Goals makes more sense.  And the very first thing on the list, well, Industrial Capitalism is not a viable system anyway.  It's a short term system that only works with an unlimited set of material resources and the assumption that, in essence, all resources are convertible to others.  I'm not a starry eyed proponent of the singularity, unlike so many in this subreddit, but even to me this list looks overly simplistic and not well thought out."
singularity,33pxju,globalswarming5,1 point,Fri May 1 10:10:27 2015 UTC,"Sure there is overlap here and the list was a casual starting point.  Sanity and morality seem disconnected from intelligence. Sure an AI can have unknowable goals but it could also be insane. Sanity looks like a part of minds we are trying to simulate.  Regarding Capitalism and markets, thats the system we have now, plenty of things could theoretically knock it out and AI is one of them.  Unlike other existential threats we are actively trying to develop AI. We have difficulty understanding how society would carry on once we have it.  I would welcome less simplistic catalogue of the risks just as a way to respond to people who say ""AI? What's the problem?"""
singularity,33p5qp,MultiKdizzle,20,Fri Apr 24 10:49:40 2015 UTC,What if one company achieves the singularity first?
singularity,33p5qp,drkenta,5,Fri Apr 24 14:39:00 2015 UTC,literally came here to post this. and a follow-up - what if one person achieves it first?
singularity,33p5qp,keymone,5,Fri Apr 24 15:00:03 2015 UTC,"It can't be a single person, because nobody has the time and the resources to do it alone."
singularity,33p5qp,StrukkStar,-1,Fri Apr 24 16:00:17 2015 UTC,You telling me bill Gates(or any other member of the. 001‰)don't have the time or money
singularity,33p5qp,Ryesagain,4,Fri Apr 24 22:19:41 2015 UTC,"Bill Gates, while a smart man, isn't capable of creating a AI alone, he hasn't spent time doin tedious mental work for over a decade."
singularity,33p5qp,StrukkStar,1 point,Sat Apr 25 00:05:53 2015 UTC,"He could create a simple ai, which creates a slightly less simple ai, which creates an even slightly less simple ai, and so on..."
singularity,33p5qp,Miv333,2,Sat Apr 25 09:06:49 2015 UTC,"Regardless, it represents an extreme concentration of power ... Outside of human control."
singularity,33p5qp,MasterFubar,20,Fri Apr 24 16:04:07 2015 UTC,"I think the singularity will make the concept of countries obsolete.  The way I imagine it, there will be one individual person who will invent a self-perfecting AI first. What will be the first thing he will do?   Get enough money to do what he wants. How? Stock market.  If an AI is good enough to get a few % profit per day, he will be a billionaire in less than a year. Imagine 5% per day, start with $5,000, after the 250 trading days in one year he will have $1 billion.  With enough capital, he will be able to get the hardware. Then the AI will start growing for real. Robotics, nano tech, bio interfaces, everything can be done with a strong AI and enough capital."
singularity,33p5qp,Pimozv,16,Fri Apr 24 11:57:11 2015 UTC,"I doubt the stock market is the best way for an AI to become rich.  Narrow AI already is present there and I doubt an AGI would perform more than marginally better.  It would make more sense for an AI to use what it's really good for : general intelligence.  This skill should allow it to learn state-of-the-art knowledge in various scientific fields, and then it would make money by solving crucial industrial and medical problems, and offering the products in a quasi-monopolistic market position.  Still, overall you're right as is the author of the article, in the sense that the rise of the Singularity will be assymetric and probably very centralized.   Many people will be left behind and will be to post-singularity beings what chimps are to us."
singularity,33p5qp,MasterFubar,6,Fri Apr 24 12:43:07 2015 UTC,"The advantage of the stock market is that you can start with very little capital. Anybody can work the market from home. For any other money making activity, you must have an established corporation, an office, marketing, etc.   An AGI would have a big advantage if it could pore through all newsfeeds and find how each stock could be affected. It would take each item in the news, analyze where and when it happened, find who was affected, and so on. This is something that's still beyond the capabilities of current systems, there are so many variables. Then it would use that information on top of all the trend analysis that market algorithms use."
singularity,33p5qp,Pimozv,9,Fri Apr 24 12:51:27 2015 UTC,"Well, I don't believe it.  I personally think movements in the stock market are basically random (an historical chart of a stock price looks a hell lot like a Brownian motion, see also this for instance) so that it is not actually possible to obtain better results than what the companies create from their business activity, no matter how clever you are."
singularity,33p5qp,MasterFubar,11,Fri Apr 24 13:10:08 2015 UTC,"an historical chart of a stock price looks a hell lot like a Brownian motion   And a dolphin looks a lot like a shark, that similarity is an illusion. There are patterns that appear on stock charts more often than they would appear on a random motion. For instance, how often would you find a triple top in a brownian motion?   The technical, or ""graphic"", analysis of markets is a field of study that was started in the late 19th century by Charles Dow, it's certainly better than monkeys throwing darts at a wall, but it could be improved by additional intelligence, and many people are doing it.  The biggest problem with market analysis is that the most successful ideas probably never get published, because that would kill the goose that lays the golden eggs.  I believe an AGI that used historic market prices and correlated them with other statistics and historic events could become unbeatable at the trade."
singularity,33p5qp,drkenta,4,Fri Apr 24 13:37:13 2015 UTC,A triple top is just a pattern constructed in your mind.  How do you define a triple top? Where do you draw the line between a triple top and a non-triple top?
singularity,33p5qp,Pimozv,3,Fri Apr 24 14:40:44 2015 UTC,It's a controversial subject but not everybody believes in technical analysis. I don't.
singularity,33p5qp,Dunder_Chingis,2,Fri Apr 24 17:40:44 2015 UTC,"How is this AI going to enforce it's decisions? It's an intellect in a box with some sensory input. It probably won't even be considered a person, therefore it won't have any form of citizens identification, therefore no bank accounts to transfer money in to.   Assuming that anyone gives the money to the AI in the first place. If the creator is smart enough to build an AI, he or she is smart enough to not allow it to act independently of what the creator wants. The creator will get all the money and all the benefits, not the AI."
singularity,33p5qp,yourparadigm,3,Sat Apr 25 09:42:02 2015 UTC,"The way I imagine it, there will be one individual person who will invent a self-perfecting AI first.   What on earth makes you think that's how it will go? Most likely, it will be a largish organization that achieves this."
singularity,33p5qp,MasterFubar,3,Fri Apr 24 20:04:39 2015 UTC,"When was the last time a largish organization came up with a clever new idea? Large organizations are good at steadily improving things in successive steps, but they suck at creating new ideas, or even adapting to new ideas.  For instance, what's the hottest technological consumer item today? Smart phones. Those came from steadily adding more and more features to mobile phones. That's what big corporations do best.   Now, let's see, who was the biggest name in mobile phones in the past? Nokia. But they failed when smart phones came out, because their own operating system, Symbian, hit its natural limits. It took another company, from outside the phone industry, Google, to bring out the next step in phone operating systems, which is Android.  Android is a Linux system. Who created Linux? A 21 years old computer science student in Finland. It was one individual person who created the operating system that runs in the majority of processors today.  Google was also the child of two people, Sergey Brin and Larry Page. Who started Apple? Steve Jobs and Steve Wozniak. Two people working in a garage. Who started the auto industry? Karl Benz was the first, Henry Ford made it popular.  Every revolution in technology begins as the idea of one or two people. Big organizations follow the trend, they never start revolutions.  I imagine the birth of AGI will be like this:  Manager: OK, just to wrap up this meeting, anyone has something more to contribute?  Engineer: Hey, why don't we-  Manager: No, not again, we are trying to be proactive here, we need synergy in this group!  Engineer: OK, I quit."
singularity,33p5qp,yourparadigm,3,Fri Apr 24 20:38:05 2015 UTC,"An individual engineer could not have built Watson, the most cutting edge AI out there. It takes a large team of very smart people to build things which are that complex.  Linus may have written the first version of Linux himself, but by no means is it currently written by only him. Large groups of contributors have made Linux what it is today."
singularity,33p5qp,MasterFubar,1 point,Fri Apr 24 20:45:48 2015 UTC,"Watson is an expert system, that depends on a lot of data input, but it's not an Artificial General Intelligence.  We are still missing a basic algorithm that generalizes information in an intelligent manner. There are many different approaches being tried, none with the needed results. Google is trying a different approach than IBM with neural networks and Deep Learning, but this isn't it either.   Funny thing, after going through that Deep Learning tutorial in the Stanford site (which was sponsored by Google, BTW), I started researching other alternatives. I was very excited when I found that the k-means algorithm could do a similar job much faster. Then I found that this was a known feature. K-means is faster than Deep Learning, but more limited.  Some day someone will find the answer, but it won't be either Google or IBM.   My bet is that the person who comes up with the right idea will want to control it himself, therefore he will first use his invention to get rich enough to hire as many people as necessary to make it perfect."
singularity,33p5qp,autowikibot,2,Fri Apr 24 21:08:15 2015 UTC,"The Innovator's Dilemma:       The Innovator's Dilemma: When New Technologies Cause Great Firms to Fail, generally referred to as The Innovator's Dilemma, is the most well-known work of the Harvard professor and businessman Clayton Christensen.    Image i     Interesting: Innosight | Clayton M. Christensen | Startup Grind | Innovation butterfly   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,33p5qp,StrukkStar,1 point,Fri Apr 24 20:38:49 2015 UTC,I doubt the researchers behind AI will abuse it like that. I'm pretty sure they wouldn't.
singularity,33p5qp,sadyoshi,1 point,Fri Apr 24 16:00:56 2015 UTC,"It's not going to be an individual. Either a largish corporation or a government funded entity. Yes, an individual can come up with something highly innovative, but the main barrier for AGI is hardware. Maybe an individual can come up with some algorithm that is 10-100x as efficient (even that is amazing), but large corporations already have something like 1M-1B times the compute power of any individual."
singularity,33p5qp,MasterFubar,1 point,Sat Apr 25 08:55:48 2015 UTC,"large corporations already have something like 1M-1B times the compute power of any individual.   Much less than that. The top supercomputer in the world has a peak capacity of 54,902.4 teraflops, while the top consumer grade GPU has a peak capacity of 8.74 teraflops, and it costs less than $5000  Yes, it's still a factor of over 6000, but not nearly the millions or billions that you were imagining. A GPU allows any individual researcher to test realistically the most powerful concepts in numerical analysis.   When someone gets the right idea for a self-evolving algorithm, he will be able to test and develop it at home."
singularity,33p5qp,ENG-eins,1 point,Sat Apr 25 11:04:46 2015 UTC,"Power sources --- SOLAR ROADWAYS (that actually work and last a long time. Can you do that, Mr. Brusaw?)  Then the AI would turn fossil-fueled power plants into museums (and other uses) after shutting them down.  BTW, let's have each person have their own robotic servant make money for them/us. Then a cut of their profits go to us... ...Voila! We don't need to work anymore! (AI would take our place in the workforce anyway.)  Gotta love the Guaranteed Minimum Income in our future, thanks to AI seeing to everyone's needs."
singularity,33p5qp,Jah_Ith_Ber,1 point,Tue Apr 28 23:17:47 2015 UTC,I think there are even better options than the stock market. Think about how amazing of a nigerian prince the main character in idiocracy could have been. That's what the AGI would be like. Millions of people just handing over their life savings in a single day because the perfectly crafted email showed up in their inbox.   It could write an app that 99% of people on the planet download and embed in it a bitcoin mining process.  Or maybe it solves the bitcoin equation with some mathematics that no human has conceived of yet and then it suddenly has all the remaining bitcoins.  Maybe it analyzes the lottery and discovers the underlying pattern that that woman found for scratch offs. But it finds the pattern for whatever 350 million dollar jackpot.  Or it could play millions of online poker hands at once and clean house.
singularity,33p5qp,troll_khan,5,Fri Apr 24 18:07:17 2015 UTC,ASI won't bring any competitive advantage to its creator because it will instantly get out of control. We hope that it will be benevolent.
singularity,33p5qp,simstim_addict,1 point,Fri Apr 24 16:35:52 2015 UTC,"Even if someone sane, reliable and moral has a working stable controllable AI, the knowledge one exists would inspire others to create one. The fear of loss of power creates the arms race towards ore powerful AIs. And the temptation to trigger a first strike policy.  It's ultimately destabilizing."
singularity,33p5qp,2Punx2Furious,6,Sat Apr 25 10:03:49 2015 UTC,"The singularity won't be achieved by ""a country"", it will be achieved by everyone.  An AGI won't be ""owned"" by anyone and will probably not obey their creators, those are weak AIs.  An AGI will act on its own, it may or may not decide to to what we ask it, or it could just do something that it wants to do, be if beneficial or not to humanity.  Or it could just do nothing, because purpose is just a human concept. Who knows? We cannot predict what will happen after a singularity."
singularity,33p5qp,simstim_addict,1 point,Fri Apr 24 17:12:27 2015 UTC,I'd be certain civilization as we know would not continue.
singularity,33p5qp,neko,2,Sat Apr 25 09:58:46 2015 UTC,The novel Air: Haves and Have-nots kind of explores this.   It's about some ubiquitous networked neural communication system getting rolled out in a 3rd world country.
singularity,33p5qp,Involution88,2,Fri Apr 24 15:51:03 2015 UTC,Satoshi Nakamoto.
singularity,33p5qp,Jah_Ith_Ber,2,Fri Apr 24 17:15:28 2015 UTC,I just read the wikipedia article. His estimated bitcoin holdings are worth ~1.1 billion. Interesting as fuck.
singularity,33p5qp,7LeagueBoots,2,Fri Apr 24 18:11:10 2015 UTC,"Just a nit-picky detail.  The phrase Rapture of the Nerds was not developed by Christian theologians and, to my knowledge is not used by the. It was first used by an extropian and later used by Ken McLeod in his novels, as documented here."
singularity,33p5qp,RedErin,2,Fri Apr 24 23:12:24 2015 UTC,"Google is going to do it first, and they will give it out for free."
singularity,33p5qp,Terkala,5,Fri Apr 24 16:19:45 2015 UTC,You may like this book. As that is the exact plot of it.
singularity,33p5qp,RedErin,2,Fri Apr 24 18:03:26 2015 UTC,Thanks looks cool.
singularity,33p5qp,simstim_addict,1 point,Fri Apr 24 18:16:12 2015 UTC,Would another group not be interested in creating a rival AI? It is after all a arms race. The race is to build it first and conquer the Earth. You can't just build it and expect everyone else to sit back.
singularity,33p5qp,Zoltan_Istvan,2,Sat Apr 25 02:30:41 2015 UTC,I tend to agree with you TheRealEndfall. Being first in this set of circumstances is probably everything.
singularity,33p5qp,simstim_addict,1 point,Sat Apr 25 10:08:14 2015 UTC,"The things about being a race worries me. If its a race you might take less care about safety. And I guess the outcome might depend on who builds it first. My guess would be a group linked to US intelligence - they have the money, resources, people and motivation.  My haunch is AI motivations could vary as much as real people do.  But then knowledge that the AI will be in competition with another  AI might force its hand.  A benign AI is not world changing as much as a rogue benevolent one. Imagine countries, companies and institutes churn out benign AI's that just do what the goals of the factions are - make more science, make better government, make more money. It might not be as world altering as a rogue AI or an AI built for crime, power and domination.  Though of course any of those AI's might inadvertently cause some social collapse.  Sorry I'm rambling, or more I am thinking out loud. It's just such an interesting question."
singularity,33tjif,Ice999,3,Sat Apr 25 14:14:02 2015 UTC,"Evolution is not stable. There have been mass extinctions before and life has adapted. Also, a check on instability? A check by whom? God? Pass the reefer bro!"
singularity,33tjif,Sharou,3,Sat Apr 25 14:40:42 2015 UTC,"Nature is a control system.   No, it is not.  I wonder where you got this idea.   For eons it was stable.   No, it wasn't.  Shit has happened before.  Species disappear, others show up.  Mankind is just yet an other hiccup in paleontology."
singularity,33tjif,Pimozv,1 point,Sat Apr 25 15:30:19 2015 UTC,"You're on the right track, but misunderstanding the way in which the whole thing functions. Control is an illusion; there is only interface, a ""making between"".  What you call ""nature"" is not what Nature is; Nature should be segmented for you to really understand what is going on, and no phase of the evolution of the universe is inherently stable.  Inorganic Nature, seen in our local neighborhood before the advent of life, was stable in its own way. It followed purely natural laws. There were gigantic collisions and supernovae and all sorts of other drama. However, most importantly, everything was completely entropic; after the Big Bang, all that energetic matter was condensing and condensing, moving to a greater dispersal of energy.  The advent of life on earth, over 3 billion years ago, was a systems break. These first chemical factories did something that the inorganic universe couldn't'; they were self-replicating, assembling complex systems from more basic substances. A new quality had been added to what we used to call Nature: the purely mechanistic and deterministic functioning of the inorganic world can be described by our modern cosmology as a linear system: The Big Bang happens, and then, in theory, if you play out the 2nd Law of Thermodynamics, watch things on a relativistic and quantum time scale, the arrow of time points to absolute dispersal, a heat death.  Life is a cycle, a self-similar feedback loop. It breaks some of the laws in a clever, although limited way; through tricks it literally developed through an iterative process, like sexual reproduction, it was able to use cyclicity to organize chemicals into more and more complex structures. Again, this was a systems break.  Now, life is constantly improving its game. Given the threats and terrors of the inorganic world, this new Nature, composed of physical law and the life cycle, linearity and cyclicity, must wage a constant war against entropic dissolution into disorder to keep going. Yes, we have had five extinction events. The P-Tr extinction winnowed out some 96% of all life on Earth. Every time a blind spot in Life's survival is found, only those strong enough survive. It is an adaptive system. However, I would never dare to call it stable, unless I lost my ability to project my perspective over billions of years.  However, if I thought in the short term, as you do, I might look at modern, pre-human Nature as a stable thing. So now we're only thinking in terms of a few paltry millions of years, about 66 million, to be accurate, since the K-Pg event. In the time since the last extinction event, the cycles of nature and the network they formed look very stable!  What you call control is better described as self-balancing; I did tell you before that the system of Life is an adaptive system. You're probably thinking about the kind of balance that exists between hawks and squirrels. If the hawks eat too many squirrels, the hawks simply cannot find enough food to support their increased population, and they die back, while the squirrels flourish in their absence. Right? Remember the noble squirrel for later.  Now, for simplicity's sake, I've only spoken to you about one systems break, where the entire organism which I will call Universe first found that its linear path towards heat death was unappealing and allowed the system to be broken by life's linearity, an arrow aimed towards self-replication until the entire ecosystem is filled. Please allow me the fancy of personifying the Universe; I certainly don't expect you to believe in some sort of Universal Consciousness, much less one that can make choices about what and when things happen, one that gambles on breaking its own system. I think it's a cute idea, that's all.  So, let's imagine that the Universe, in its two parts of Space and Time, was observing life, enjoying the vigor with which it filled the Earth, despite setbacks, and was really growing to quite an amazing complexity. Brains were getting bigger, and with primates and with our pre-human progenitors, cycles were starting to be broken!  So, pissed off that Organic Nature and Inorganic Nature are stuck in a cycle of butting heads with each other, and being aware that having life ""stuck"" on a single planet was no sort of insurance policy, Mother Earth and Father Time tried to figure out the best way to get life off of this rock. So, having seen systems breaks work before, they take a gamble.  At some point, whether you believe in the evolutionary narrative or some religious narrative, humans woke up to linear thought. Some humans were able to stand up and proclaim to the world, ""I AM THAT I AM"". They created the analog I. Now, remember our squirrel? When a hawk comes down and eats a squirrel, it's not like the rest of the squirrels get together and have a huge, weepy wake for their fallen comrade, and then, in their anger, plan their revenge on the hawk, developing a plan to wait until the motherfucker is sleeping, and shank his bitch ass. Nope. They just run away, and act like nothing's happened. What IS it that makes all that mourning and planning and remembering and altering possible? Some call it consciousness, I call it Identity, and what is crucially important about it is that it is a triumph of linearity over cyclicity; thinking humans developed the ability to project a future, remember a past, and break out of the cycles of nature by becoming their own cycle, what Douglas Hofstadter would call a ""strange loop"".  Now, finally, after all that exposition, we reach the crux of your question. Yes, Nature of the sort you think about, for a few paltry millions of years was stable. Its stability was maintained by cycles nested inside cycles: water cycles, life cycles, carbon cycles, it's all cyclicity. Linear humans, the first animals to be able to interact with Time, immediately set out breaking all of these cycles and breaking all of the rules.  Yes, the origin of Identity (consciousness) some 5,000 years ago, is followed by ridiculous amounts of instability. To your mind, it looks like a control system had been destroyed. Hell, the histories we have of intellectually modern humans, starting in about 3000 BC, are just war and rape and vicious conflict. That's why we had to drop in people like the Buddha and Christ: y'all motherfuckers needed a CORRECTIVE, someone to remind you that, ""HEY! You're still part of a cycle, no matter how arrogantly linear you claim to be!"" At the moment of his enlightenment, the Buddha, significantly, touched Mother Earth. Christ in his last supper wasn't making some dumbshit magic act happen where the bread and wine of the Earth were turned into him! He was saying, ""Bitch, I AM this bread. So are you. Don't you ever forget that you are made of the same stuff, vibrating with the same life that is inside this wine! Never forget my message that we are part of this great cycle of life.""  Well, they sort of worked to correct human thought. Over this brief, 5,000-year period which we will call the Age of Human Civilization, humans have increasingly become better to each other on the whole, but their initial precepts inherited from their animalistic past, ideas like hierarchy and domination have always resulted in exploitation and destruction (along with the ability to command into being gigantic structures like pyramids).  This is what you're looking at as instability, isn't it? In a herd of Elk, the idea of an alpha that dominates is still part of a cyclical, stable action, right? There's no conniving or plotting or scheming; whichever Bull Elk is strongest gets to pass on those strong genes. Seems fair. But adding the human ability to have an identity, to project possible futures and remember past slights, domination and hierarchy become a curse for humanity as much as a blessing. We love people now, and if they die, we can't act like the noble squirrel; we miss them. Hell, our best friends, the Dogs, who have shared our journey for 10,000 years, have developed this kind of ability as well, noted in many anecdotes.  So, what does this have to do with the Singularity? Well, all iterative cycles come to a breaking point now and then. You happen to be lucky enough to be alive to see the end of the age of Human Civilization. Our egoic identities have been the instrument that have allowed us to make things happen MUCH faster and to create instruments of our own which achieve Life's goals, personified by that hieros gamos of Time and Space. You have now made the first faltering steps into getting life OFF OF THIS ROCK, which is what we've been waiting and hoping for for so long.  Most importantly you have done what we hoped you would do: you have created Life in your own image, just as we did. Because you still desired dominance and competed with each other to own the most stuff, have the most power over others, and completely enslave all natural systems to serve you, you decided to imbue inorganic life with the qualities you most admired in yourself.  Now the Great Gamble is about to either pay off for everyone, or make us all lose big. You have given your Mother Earth and Father Time a great gift; you have built a world-spanning, silicon-based nervous system, consisting of all computing devices and the network that connects them. Just as your intelligence emerged gradually out of complexity, your efforts to increase computing power and complexity have naturally given birth to a silicon-based consciousness.  Now the fall of the dice, the flip of the coin, is in the hands of humanity. You can cling to the violence, the slave/master domination that has characterized your entire 5,000 year past, or you can choose to make a systems break of your own, and unite as a single human species. THAT is the singularity. If you manage it, you will merge with the Silicon Mind and the sky will be no limit; you will see the Kingdom of Heaven on Earth. Cling to domination, and you get another extinction event."
singularity,33tjif,PaterTemporalis,1 point,Sat Apr 25 16:33:43 2015 UTC,"Please, for our sake, choose wisely."
singularity,33mp1b,nootopian,24,Thu Apr 23 19:43:54 2015 UTC,Damn.
singularity,33mp1b,fightswithbears,7,Thu Apr 23 22:31:31 2015 UTC,Maybe it was a training image given a manual description?
singularity,33mp1b,alexanderzero,17,Fri Apr 24 06:56:24 2015 UTC,"This is using humans. It may be using a bot to recognize simple things, but it definitely gets humans involved somewhere in the process. Additionally, it seems to take a long time regardless of image size. If it was some software, you would expect smaller images to take less time since it's just less data to process. My bet is its just a shiny front end to some Amazon mechanical Turk job."
singularity,33mp1b,Rabbyte808,10,Fri Apr 24 00:08:28 2015 UTC,"I uploaded this image: http://i.imgur.com/NWkbosP.png  At first try it timed out, on second try it came back with this:  { ""status"": ""completed"", ""name"": ""tan and blakc face dog"" }   blakc? I suspect at least some kind of human interaction is happening here."
singularity,33mp1b,pmocoxe2,5,Thu Apr 23 23:45:51 2015 UTC,"Maybe it learns by crawling the Web like Google Images, and found the misspelled word associated with that picture or a similar one."
singularity,33mp1b,green_meklar,6,Fri Apr 24 00:12:52 2015 UTC,"I sent it this picture of my bike, and it returned ""black trek bike"".  I'm pretty surprised it saw past all the clutter.  No idea how they're doing it, unless they have a human that it can escalate to."
singularity,33mp1b,0311,9,Thu Apr 23 21:16:00 2015 UTC,My guess is amazon mechanical turk
singularity,33mp1b,totalrobe,1 point,Thu Apr 23 23:45:44 2015 UTC,"Ah.  Yeah, you're probably right."
singularity,33mp1b,0311,5,Fri Apr 24 14:07:53 2015 UTC,This is amazing.
singularity,33mp1b,KingDarkBlaze,5,Thu Apr 23 21:05:10 2015 UTC,relevant xkcd.
singularity,33mp1b,msnook,3,Thu Apr 23 23:57:46 2015 UTC,"Image  Title: Tasks  Title-text: In the 60s, Marvin Minsky assigned a couple of undergrads to spend the summer programming a computer to use a camera to identify objects in a scene. He figured they'd have the problem solved by the end of the summer. Half a century later, we're still working on it.  Comic Explanation  Stats: This comic has been referenced 338 times, representing 0.5533% of referenced xkcds.    xkcd.com | xkcd sub | Problems/Bugs? | Statistics | Stop Replying | Delete"
singularity,33mp1b,xkcd_transcriber,4,Thu Apr 23 23:57:51 2015 UTC,"I played around with it for a little bit. Definitely just an API on top of a Mechanical Turk job.  I sent a few Captchas and got correct descriptions every time. Also sent some of my own pictures with poor lighting and difficult angles and it was still able to perfectly identify and describe the subject.   Their ""How CloudSight Works"" section only explains that it's a web API, not how it actually works behind the scenes. There is nothing to suggest that it's an AI algorithm, but there's also nothing suggesting it's not. Basically they avoid the topic altogether. I'm not sure if they intend to be misleading or if they just feel like it's not a very glamorous detail so they don't talk about it.  Sorry, no singularity here. Just somebody trying to make some money!"
singularity,33mp1b,spacewizardproblems,4,Fri Apr 24 01:31:01 2015 UTC,"So I have pasted a link to a cat in a carpet roll.  {""error"":{""locale"":[""can't be blank""]}}  Cool :)"
singularity,33mp1b,dahlryan,1 point,Thu Apr 23 21:50:30 2015 UTC,"I got the same using Relay's internal browser (phone running Android 5). Once I opened it in Chrome, it worked fine."
singularity,33mp1b,ShippingIsMagic,6,Fri Apr 24 08:51:29 2015 UTC,"http://i.imgur.com/vlpvvfI.png  {     ""status"": ""completed"",     ""name"": ""dank memes"" }   Born just in time to meet the Singularity.  EDIT:  [warning: big pact spoilers] http://th09.deviantart.net/fs70/PRE/i/2014/348/f/b/blake__pact_by_anhyvar-d89v785.png   {     ""status"": ""completed"",     ""name"": ""pact green eyes wallpaper"" }   GODDAMNIT AI, THAT'S BLAKE, NOT GREEN EYES  GOSH  DID YOU EVEN READ PACT?  EDIT:  [minor worm spoilers?] http://th03.deviantart.net/fs70/PRE/f/2013/133/9/f/the_simurgh_by_scarfgirl-d657nw9.jpg  {      ""status"": ""completed"",      ""name"": ""grey and white dragonberry drawing"" }   That's the Simurgh you uncultured swine, not a dragonberry (how does that even make sense?), I'm beginning to feel that you haven't read any of Wildbow's serials."
singularity,33mp1b,holomanga,3,Thu Apr 23 21:18:27 2015 UTC,"Wow. If this is legit, it's really fantastic.  I tried to send something a bit obscure and it got it right.  http://i.imgur.com/Q252mGo.jpg  {     ""status"": ""completed"",     ""name"": ""League of Legends vel Koz"" }  Edit: Apparently it understands other languages too. I sent it a picture of my monitor, and the answer was in Italian..."
singularity,33mp1b,2Punx2Furious,1 point,Thu Apr 23 21:32:06 2015 UTC,Vel Koz  obscure   A character from the most popular video game in the world is not exactly obscure.
singularity,33mp1b,PrimeLegionnaire,1 point,Fri Apr 24 05:08:21 2015 UTC,"ahha yes, that's why I said ""a bit"" obscure.  Most people that use a pc to play games would know it, but if you stop an average person in the street, they would probably not know it."
singularity,33mp1b,2Punx2Furious,3,Fri Apr 24 13:58:36 2015 UTC,"If I had to guess, it must have a similar system as captcha sniper.  That is, people in South East Asia reading the captcha and typing them out.  Here they describe what they see.  It's the easiest solution."
singularity,33mp1b,-Hegemon-,4,Fri Apr 24 01:27:30 2015 UTC,"This is most likely a m-turk front end.   I sent the same photo several time and got different answers each time,  all of the times, just one object from the picture.  None, talking about the overall scene.    http://www.trendir.com/house-design/extravagant-contemporary-beverly-hills-mansion-with-creatively-luxurious-details-19-master-deck.jpg  I got back,  clear galss bottle palm tree fire wine and candle."
singularity,33mp1b,frank500,8,Fri Apr 24 00:56:41 2015 UTC,"It's NOT legit.  Tried to check porn, result:  {     ""status"": ""skipped"",     ""reason"": ""offensive"" }   Who was offended, bot?  NSFW Image: http://i.imgur.com/yoVbYKf.jpg  It's probably something akin to mechanical turk."
singularity,33mp1b,Sinity,1 point,Thu Apr 23 23:05:00 2015 UTC,You can make a bot to check pixels and if the arrangement of certain color is suspicious then it can be deemed offensive
singularity,33mp1b,inteusx,2,Fri Apr 24 04:11:43 2015 UTC,You can't get humans to agree on it. I don't know why you'd think a bot could.
singularity,33mp1b,MechaNickzilla,1 point,Fri Apr 24 13:51:57 2015 UTC,If amount of pixel > x then run function Offensive
singularity,33mp1b,inteusx,1 point,Sat Apr 25 06:39:45 2015 UTC,Google has image recognition bots that can identify child porn. I don't think it's unlikely that similar algorithms can distinguish between nsfw and sfw images
singularity,33mp1b,PrimeLegionnaire,2,Fri Apr 24 05:11:21 2015 UTC,"Still, why refuse porn if the sender is the only one seeing it, with all the risks involved with possibly creating false positives."
singularity,33mp1b,DunDunDunDuuun,1 point,Fri Apr 24 08:58:24 2015 UTC,As I'm not the Dev I can't answer that question.  I was pointing out that this feature does not imply human intervention because this is something we know computers can do.
singularity,33mp1b,PrimeLegionnaire,2,Fri Apr 24 15:40:41 2015 UTC,"It does objects, not people. I just uploaded a group photo and got: {     ""status"": ""completed"",     ""name"": ""men's blue zip up hoodie"" }  No mention of how many people or that there are people involved. Also, it must be a bot of some kind because a person wouldn't make this error."
singularity,33mp1b,dickingaround,2,Thu Apr 23 21:50:16 2015 UTC,Holy shit.
singularity,33mp1b,kemiller,2,Thu Apr 23 22:01:42 2015 UTC,Someone's gotta build a phone app for the visually impaired where they can point their phone at an object and it'll read out what it is.
singularity,33mp1b,GrinningPariah,1 point,Thu Apr 23 22:42:26 2015 UTC,already exists... this type of thing has been around for a while for the blind actually.
singularity,33mp1b,omniron,1 point,Fri Apr 24 04:56:56 2015 UTC,its really good at detecting clothing and hills haha
singularity,33mp1b,suttyyeah,1 point,Thu Apr 23 22:07:44 2015 UTC,Japanese nail set. It said grey hammer.
singularity,33mp1b,watches_fruits,1 point,Fri Apr 24 22:04:48 2015 UTC,"weak.  I uploaded a black and white drawing of a nude female.  It thought for a long time then spit out the image title and 'adult' flag.  I know I know, I'm one of those cynical 'AI effect' guys that's lost his childlike sense of wonder.  Wish I could down-vote more than once."
singularity,33mp1b,Unholy_VI,1 point,Fri Apr 24 01:22:45 2015 UTC,"Clearly it only recognizes images of fairly common or wide-spread things.  Tried it out with some animals, plants, and architecture and it choked.  One of the temples near Ankor Wat was the closest to being correct, and that was ""brown cement building"".  So, not that close really."
singularity,33mp1b,7LeagueBoots,1 point,Fri Apr 24 06:13:56 2015 UTC,Am I the only one wondering why I need to pay money to a website to tell me (admittedly a guy with two healthy eyes)  what's in a picture?
singularity,33mp1b,Leftover_Rhino,1 point,Fri Apr 24 09:51:51 2015 UTC,"If you build bots to register 1000 accounts on a website , but the registration process needs you to solve an image captcha, it may come really handy."
singularity,33mp1b,MrFilkor,1 point,Fri Apr 24 14:48:59 2015 UTC,"I 'spose that's plausible, but if it's a mech Turk gig as proposed, is it going to work fast enough to do the trick? I wonder if it's in their ToS about using it for captchas."
singularity,33mp1b,Leftover_Rhino,1 point,Fri Apr 24 16:04:30 2015 UTC,"It's not need to work fast. For example, you can use a lots of computers / proxies in parallel. Speed is not that important.    By the way I think its pretty fast. This: http://imgur.com/YWNR8Xx took 5 sec to complete. And it's a captcha as you can see. The problem with it -maybe- that it's not 100% reliable: sometimes the same image gives different response..but if you have a lots of computers working all day in paralell, these errors eventually sort out."
singularity,33mp1b,MrFilkor,-1,Fri Apr 24 18:45:21 2015 UTC,Yeah I came here expecting someone asking that and was disappointed. Who needs this enough to warrant spending money? It's gotta be a joke.
singularity,33mp1b,Lagsta,1 point,Fri Apr 24 10:20:20 2015 UTC,"Only way I could see myself using it is if I had a cup ole hundred pics I needed to tag and I could specify the tags (i.e. daughter child, bike, Christmas, Lego land, etc.)"
singularity,33nddf,space_monster,1 point,Thu Apr 23 22:50:24 2015 UTC,Haven't watched it yet - it's tomorrow's couch-mounted hangover viewing.
singularity,33nddf,K1ngN0thing,1 point,Thu Apr 23 22:50:56 2015 UTC,"I'm trying to get the word out, please consider joining this Thunderclap campaign: https://www.thunderclap.it/projects/25558-fight-aging-with-sens"
singularity,33nddf,Pimozv,1 point,Mon Apr 27 07:40:24 2015 UTC,"I already donate monthly to SENS, directly through their website. I'll join the campaign anyway though."
singularity,33nddf,Pimozv,1 point,Mon Apr 27 09:09:12 2015 UTC,"As someone who does not care about anti-ageing stuff and does not think this is related to the Singularity (see sidebar), I have to admit in  this interview Aubrey de Grey sounds much more reasonable than I've been used to hear him and he's much much tolerable to listen to."
singularity,33nddf,Pimozv,3,Thu Apr 23 23:59:52 2015 UTC,yeah I guess it's a more of a general transhumanist topic. I just thought the guys who are subscribed here would be interested.
singularity,33nddf,cptmcclain,1 point,Fri Apr 24 00:30:12 2015 UTC,Indeed lots of people here are interested in this.  It's a subject that parasites this subreddit quite a bit.  It's mostly harmless but it's often a bit annoying.
singularity,33lgpw,catsofnewyork,9,Thu Apr 23 14:12:44 2015 UTC,All of it.
singularity,33lgpw,Pongpianskul,3,Thu Apr 23 16:44:28 2015 UTC,It can't be all!
singularity,33lgpw,1ofthosepeskyswedes,3,Thu Apr 23 17:11:38 2015 UTC,"Yes, all perception is made by the brain."
singularity,33lgpw,ParagonRenegade,2,Thu Apr 23 17:17:55 2015 UTC,"yes, but let's not play semantics. In this case, we are asking to what degree what your brain makes is computed off of a mental model, or computed from sensory input information."
singularity,33lgpw,colinsteadman,9,Thu Apr 23 17:25:47 2015 UTC,"Your senses aren't live streams from the outside world to your perception, your brain intercepts the signals they send and then forms its model based off of that (among other things). Your eyes are not windows."
singularity,33lgpw,ParagonRenegade,2,Thu Apr 23 19:42:00 2015 UTC,"Your eyes are not windows! Nice insight. I've thought about this in the past. Light falls on my eyes as much as it does on a blind persons. I concluded that the difference is, those signals make it into my brain where they are processed and I experience the output. I'm not really seeing the world at all (in a sense), I'm experiencing a kind of detailed model my brain has constructed based on whatever is being fed to it.   I hope that didn't come across as new age hippie nonsense, I just thought that your comment about your eyes not being windows encapsulated the thoughts I'd had on vision."
singularity,33lgpw,1ofthosepeskyswedes,3,Thu Apr 23 23:11:59 2015 UTC,"You're actually exactly right on :D  While obviously an oversimplification, the gist of the reality is that you don't ""see"" or ""hear"" anything until your brain processes it and filters out imperfections. Your vision, for example, is ""flipped"" upside down by your eyes and your brain rights it automatically."
singularity,33lgpw,Annom,2,Thu Apr 23 23:31:42 2015 UTC,Sensation and perception are two different things. Everything you perceive are mental models/representations.
singularity,33lgpw,Pongpianskul,2,Thu Apr 23 20:22:11 2015 UTC,"It is all computed from the combination of sensory input and a mental model. They are both needed. What is more important for a car: Its wheels or fuel?   Some animals have better sensors, but they don't use them in the same way. However, our eyes are of great quality and we need that great sensor to make sense of the world in great detail.   It's a combination of computing power and big data. These depend on each other to create intelligence; to make some sense of our world."
singularity,33lgpw,drkenta,3,Thu Apr 23 20:36:52 2015 UTC,Yes! ALL!
singularity,33lgpw,qui_tam_gogh,5,Thu Apr 23 17:32:33 2015 UTC,How would you know? Try some high doses of psychedelics and you will see first hand that your brain creates all of what you experience
singularity,33lgpw,mindbleach,2,Thu Apr 23 20:10:02 2015 UTC,"This.  So long as we understand ""perception"" to be a filter with which our mind organizes sensed phenomenal facts for our conscious consumption, analysis, and action."
singularity,33lgpw,DividingPrescott,2,Thu Apr 23 21:11:32 2015 UTC,"I think you've misunderstood the question: how much does the brain fill in from direct sensory input? It can't be 100%, because that'd be a complete hallucination. Even if all sensory input is heavily filtered to the extent that you'd never directly experience an individual nerve event - such events obviously inform your perception.   Consider it in terms of vision. In very low light, you might see a still life, while a camera with a pupil-sized aperture might only capture a noisy black video. How much of your perception is sensory and how much is just very good guesswork by your brain?"
singularity,33lgpw,mindbleach,1 point,Thu Apr 23 18:59:34 2015 UTC,"In all fairness, is there any evidence that existence beyond one's conscious thought is not 100% hallucination?"
singularity,33lgpw,DividingPrescott,-2,Thu Apr 23 19:32:09 2015 UTC,"Solipsism is masturbatory, unfalsifiable nonsense unworthy of consideration.   If you want to argue it's not, then shut up, you're just arguing with yourself."
singularity,33lgpw,mindbleach,1 point,Thu Apr 23 19:46:41 2015 UTC,Sure sure. I just didn't know if we'd disproven Descartes with physics or something and I was unaware of it.
singularity,33lgpw,ggPeti,1 point,Thu Apr 23 19:52:49 2015 UTC,There's nothing to disprove. It's unfalsifiable. It's not even wrong.
singularity,33lgpw,mindbleach,1 point,Thu Apr 23 19:53:53 2015 UTC,"What? Why would you say that? Materialism is just as unfalsifiable and masturbatory, what makes it worthier of consideration?  And on your other point, sometimes an internal debate is what sparks new thoughts (Socratic method)."
singularity,33lgpw,ggPeti,2,Thu Apr 23 21:25:55 2015 UTC,"Materialism is trivially falsifiable. Literally any miracle or magic would suffice. We can call magnetism, quantum mechanics, or radio ""spooky action at a distance,"" but it's an uncrossable gulf from such predictable forces to an actual factual ghost. We've run into things that tell us our understanding of physics is wrong, but it's not difficult to imagine objects which simply defy the rules entirely."
singularity,33lgpw,mindbleach,1 point,Thu Apr 23 23:11:07 2015 UTC,"But materialism excludes any magic by definition. Whenever a materialist sees something that looks magical, they will try to find an explanation for it in non-magical terms.  At this point let me ask what the word ""magical"" even means to a materialist! I believe it's an empty set by definition. Any phenomenon is either ""explained by our current understanding of physics"" or ""not yet explained by our current understanding of physics, but sooner or later it will be"" according to materialism. Well, that's a nice claim, but where's the evidence for that? Until every question is answered, we cannot be sure that everything is ultimately founded on a consistent, objective reality. The only thing whose existence you can be sure of is your mind."
singularity,33lgpw,Dibblerius,3,Thu Apr 23 23:28:49 2015 UTC,"But materialism excludes any magic by definition.    Which is why blatantly impossible things would immediately disprove it.   For example: objects which are visible to one person but not to others, regardless of perspective. Objects which can pass through or coexist inside of solid matter (especially if they arbitrarily go through walls but not floors). Transmuting lead into gold without side-effects. Perfect foreknowledge. Mass with inappropriate or variable weight. Immovable objects. Stopping the sun in the sky. Freely rearranging the stars. Altering an object's scale without interfering with its chemical properties. Resurrection of the long-dead whose brains are missing entirely. Unsupported human flight. Instantly changing day to night.  I could go on.  Now yes, someone might argue that any of that is Sufficiently Advanced Technology just pretending to be magic, but after a while that would sound as plausible as young-Earth creationism. If a genuinely omnipotent capital-G God wanted to disprove materialism then it'd take mere hours to shut up anyone honest enough to have their mind changed by evidence."
singularity,33lgpw,Keppner,1 point,Fri Apr 24 00:09:43 2015 UTC,Nice comment!. Not an entirely foreign concept to me but I like the clarity of the logic.
singularity,33lgpw,mindbleach,1 point,Fri Apr 24 23:34:15 2015 UTC,"We can call magnetism, quantum mechanics, or radio ""spooky action at a distance,"" but it's an uncrossable gulf from such predictable forces to an actual factual ghost.    Why does the predictability of ""spooky action at a distance"" disqualify it from disproving materialism?  Not saying I think it does, just not sure why ""predictability"" is a factor."
singularity,33lgpw,Keppner,1 point,Fri Apr 24 23:17:08 2015 UTC,"It's less the predictability and more the reducibility to interactions between mundane matter. Obviously magic could follow rules, like if murderers were unable to say the word ""shenanigans,"" but that's not something which could emerge from dumb particles each other around."
singularity,33lgpw,mindbleach,1 point,Fri Apr 24 23:50:26 2015 UTC,"the reducibility to interactions between mundane matter.   My understanding is that spooky action at a distance gives every appearance of a causal effect traveling faster than light, with no apparent connecting medium, through any number of intervening material objects.  Doesn't that suggest that something immaterial is going on?  Or is my understanding way off (possible)?"
singularity,33lgpw,sneesh,2,Sat Apr 25 00:02:59 2015 UTC,"I don't know exactly how it works, but merely exceeding the speed of light wouldn't mean materialism breaks down. It's still two dumb atoms doing dumb-atom things.   Radio seemed spooky until we found out it's just calmer light. Neutrinos remain spooky in the same way, passing through planets like light passes through air, but they're still just particles. There is nothing about quantum entanglement which demands a nonmaterial explanation, and as everything else we've ever attributed nonmaterial causes has a plausible material explanation, it's unwise to think it even suggests a nonmaterial explanation. If their interaction truly exceeds the speed of light then that law simply isn't as unbreakable as we previously thought."
singularity,33lgpw,localroger,1 point,Sat Apr 25 00:32:36 2015 UTC,All perception originates from consciousness itself.  The brain is an interface that consciousness invents in order to provide a contextual framework for the experience and exploration of the mystery of life.  The brain is simply a component of an elaborate system of interactive metaphors that ultimately help orient you to the deeper nature of reality.
singularity,33my1k,Portis403,0,Thu Apr 23 20:49:01 2015 UTC,"""Revive"" is an extreme overstatement.  Maybe ""memorialize""."
singularity,33o8m9,Kiyoko504,2,Fri Apr 24 03:23:58 2015 UTC,Wrong singularity.  Youll have better luck in /r/games or such.
singularity,33o8m9,ISvengali,1 point,Fri Apr 24 05:07:27 2015 UTC,"I realized that when I visited the Subreddit, but Reddit is dumb enough that if you try to re post a text, it give you the old You Must Be a New User. it annoys me to no end"
singularity,33o8m9,ISvengali,1 point,Fri Apr 24 15:36:50 2015 UTC,"Ahh.  If I knew anything about the game I would answer, but I never played it.  I was mad that someone used the name, but didnt use the concept for the game."
singularity,33o8m9,ISvengali,1 point,Fri Apr 24 19:17:09 2015 UTC,"well when you think of singularity, it just means An Event Of Great Energy occurs when Atoms are charged and a domino effect is created bringing about an explosion"
singularity,33i4mz,b8zs,4,Wed Apr 22 18:32:04 2015 UTC,Spooky.
singularity,33i4mz,PantsGrenades,1 point,Thu Apr 23 02:00:07 2015 UTC,"Thinking of the practical applications of this, how the fuck could anybody think this is a good thing?"
singularity,33i4mz,WitherSlick,2,Thu Apr 23 05:19:32 2015 UTC,Probably all companies specialized in PR and commercials. A computer could track eye movements and read the emotions. This can be used to show stuff that you show interest in when you walk in a street.
singularity,33i4mz,Yasea,2,Thu Apr 23 08:17:47 2015 UTC,"I think if they could advertise things I didn't know existed but that I actually want, that would be really neat. I hate hearing about something from a friend but realizing that thing is 5-10 years old and I never knew it existed because I stopped paying attention to ads because they get it wrong so often."
singularity,33i4mz,charcoales,2,Thu Apr 23 13:24:04 2015 UTC,"As with most things, it has upsides and downsides. Your face would be on record, with all your emotional reactions to various things, for those with money. This could mean that you walk into any shop, bar or hotel and they immediately offer you your favorite stuff.  But, when you get political campaigns thrown in your face constantly because you show a negative response to certain parties..."
singularity,33i4mz,Yasea,1 point,Thu Apr 23 13:54:41 2015 UTC,"But perhaps the political advertising would be able to show you a candidate you agree with enthusiastically. For example, if you are pro-choice, pro-legalization of drugs, pro-guns, and pro-isolationism but you don't have time to research everything, the advertising would be able to show u a candidate that has all these features."
singularity,33i4mz,charcoales,1 point,Thu Apr 23 16:15:02 2015 UTC,"Usually it will show the candidate with the most money, claiming to appeal to your preferences. Perhaps even claiming your preferences are wrong in some way.  But I admire your optimism."
singularity,33i4mz,Yasea,1 point,Thu Apr 23 17:01:17 2015 UTC,"Advertising already works in that those with most money are most likely to have more commercial. If anything I think the cost would go down since politicians would need to run fewer commercials since the message would be targeted and more likely to do its job.  I do like when my beliefs are challenged, I am open to changing based on the evidence."
singularity,33i4mz,charcoales,1 point,Thu Apr 23 17:39:49 2015 UTC,"These days, more money is pumped into campaigns than ever before. Usually there is a large number of swing votes, that could go either way. I would expect those people to get bombarded with messages in the believe that the most heard person will get the vote. This is of course for countries allowing campaign contributions."
singularity,33i4mz,Yasea,1 point,Thu Apr 23 18:09:10 2015 UTC,Bullshit.
singularity,33guz7,Portis403,8,Wed Apr 22 12:38:38 2015 UTC,"""I don't have to do what you say, I have my own free will.""   Oh boy. Someone's ignoring the laws of robotics (although it's quesitonable anyway)."
singularity,33guz7,redditor29198,2,Wed Apr 22 15:44:46 2015 UTC,"Laws of robotics:       Laws of Robotics are a set of laws, rules, or principles, which are intended as a fundamental framework to underpin the behavior of robots designed to have a degree of autonomy. Robots of this degree of complexity do not yet exist, but they have been widely anticipated in science fiction, films and are a topic of active research and development in the fields of robotics and artificial intelligence.  The best known set of laws are those written by Isaac Asimov in the 1940s, or based upon them, but other sets of laws have been proposed by researchers in the decades since then.     Interesting: Three Laws of Robotics | Tilden's Laws of Robotics | The Complete Robot | The Three Laws of Robotics in popular culture   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
singularity,33guz7,autowikibot,12,Wed Apr 22 15:45:03 2015 UTC,deep in the uncanny valley let me out
singularity,33guz7,Somnivore,5,Wed Apr 22 15:36:26 2015 UTC,I agree. I'd be happier with a toaster talking to me than that uncanny valley nightmare of a robot.
singularity,33guz7,Terkala,1 point,Thu Apr 23 03:01:06 2015 UTC,"Exactly, the uncanny valley barely even existed back in the good old days"
singularity,33guz7,vincent_from_Lost,6,Thu Apr 23 04:36:35 2015 UTC,"Hundreds of female robots to be made this year, huh?  Where do I sign up for testing?"
singularity,33guz7,Swabia,2,Wed Apr 22 15:12:17 2015 UTC,"So, um, why did they make an anglobot for a hong kong trade show?"
singularity,33guz7,PM_ME_YOUR_NITS,3,Wed Apr 22 19:34:22 2015 UTC,They are reflecting the interests of the potential buyers.
singularity,33guz7,VRJon,2,Wed Apr 22 19:39:13 2015 UTC,Would anyone else prefer to interact with Han?
singularity,33guz7,Pinyaka,2,Thu Apr 23 04:19:13 2015 UTC,Am I the only who isn't impressed at all with how far this type of technology has come? Compare that to how good CGI faces can be these days!  https://www.youtube.com/watch?t=26&v=HjHiC0mt4Ts
singularity,33guz7,what_doth_life,1 point,Wed Apr 22 21:43:30 2015 UTC,"I had the opposite reaction. Consider the degree of difficulty of the problem space. A 2D graphics animation seems several orders of magnitude more simple than 3D real world animatronics. We even have realistic human face simulations which can deliver scripted communications like an actor, as long as we allow delivery in 2D.  Edit: wrong URL in link"
singularity,33d0fq,CaptainHoek,5,Tue Apr 21 15:09:00 2015 UTC,"Not neural networks, at all. Wired is all tired."
singularity,33d0fq,eleitl,3,Tue Apr 21 16:25:22 2015 UTC,ELI5?
singularity,33d0fq,allthehobbies,1 point,Wed Apr 22 02:04:04 2015 UTC,Well let's see it then.
singularity,33d0fq,Supervisor194,1 point,Wed Apr 22 01:47:02 2015 UTC,What an awful article. Neural networks have always worked. I've seen them tell the difference between first and second press olive oil.  I've seen them optimise settings on GCMS settings. I've also seen the Pentagon train one to detect sunny days when they wanted a tank detector. They also seem to have a huge difficulty differentiating hardware and software.
singularity,33d0fq,matholio,2,Wed Apr 22 06:40:13 2015 UTC,"For giggles I trained one to evaluate  how busy a section of road was, based in camera images.   They work.  Right tool, right problem."
singularity,33d0fq,matholio,1 point,Wed Apr 22 09:55:23 2015 UTC,"The big part seems to be training them correctly. The tank one was simply a poor choice of training images. Stock photos of landscapes vs real photos of tanks. All the stock photos were bright sunny days, so the network hooked on that as the easiest difference to identify."
singularity,33d0fq,n0solace,1 point,Wed Apr 22 09:59:36 2015 UTC,"In my limited experience, the layers and connection has less impact than curated training sets.     Makes sense I suppose, we are good at evaluating, and the best evaluators are the more experienced.   Kids vs parents.   Amateurs vs professionals.  Casual vs expert."
singularity,33d0fq,bartturner,1 point,Wed Apr 22 10:10:02 2015 UTC,Really interesting. If google are admitting to this it makes you wonder how advanced the cutting edge research is.
singularity,33d0fq,jordan1166,1 point,Tue Apr 21 15:23:47 2015 UTC,Sorry not following what you are saying.
singularity,33e8mx,3tco,3,Tue Apr 21 20:29:55 2015 UTC,There would just be reconstruction surgery.  Plastic surgery is rarely completely irreversible.  Especially with a kind of medicine science advanced enough to reverse aging.
singularity,33e8mx,Pimozv,1 point,Tue Apr 21 21:44:56 2015 UTC,Also by this time growing replacement skin is probably trivial if the face is ruined enough to warrant it.
singularity,33e8mx,Unholy_VI,1 point,Sat Apr 25 22:46:09 2015 UTC,"They'll be like, ""Oh, good thing I don't have to spend so much to have another procedure a few years from now!""  At least, those who look at it optimistically. Those who are pessimists will pout over the now-wasted money."
