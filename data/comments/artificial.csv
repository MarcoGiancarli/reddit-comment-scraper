artificial,3e4pdm,zeedrd,5,Tue Jul 21 22:57:20 2015 UTC,Uhmm...
artificial,3e4pdm,BC_Caver,1 point,Wed Jul 22 03:07:50 2015 UTC,"I did the same thing and it did detect plagiarism. However, I copied a paragraph from a sciencedaily article and it told me it did not detect anything"
artificial,3e4pdm,Kurcide,1 point,Wed Jul 22 07:04:47 2015 UTC,"hahaha, I did the same and already had the same screenshot ready :)"
artificial,3e4pdm,trilobit3,1 point,Wed Jul 22 07:55:22 2015 UTC,Ah well...
artificial,3e4pdm,mijn_ikke,3,Wed Jul 22 08:54:47 2015 UTC,"I like it but I think ""artificially intelligent software"" is pushing it."
artificial,3e4pdm,sixwings,2,Wed Jul 22 03:39:59 2015 UTC,"I wrote the star spangled banner, word for word, and it didn't detect any plagiarism."
artificial,3e2flj,kartiktv,1 point,Tue Jul 21 13:04:52 2015 UTC,I am not impressed. Their accuracy is less than 20% compared to over 90% for supervised networks.
artificial,3e2flj,sixwings,0,Tue Jul 21 19:45:06 2015 UTC,Paper is from 2012
artificial,3e1lno,AReluctantRedditor,9,Tue Jul 21 06:47:10 2015 UTC,"Go for it! There's has been some work on ais that can write code, but very little success. The problem is that source code is very hard to write correctly unless you really know what your are doing, so there is a really massive problem space, and a very tiny goal space. Also, a perfect program can be rendered useless by a single line of code, so you can't really use an algorithm that makes minor improvements to the code over time.  Maybe a breakthrough will happen where we use some other computational model to create ai that isn't human-readable source code."
artificial,3e1lno,bradfordmaster,5,Tue Jul 21 07:15:33 2015 UTC,"Indeed. This is one of my main side-projects, Using Artificial Intelligence to Write Self-Modifying/Improving Programs  Having an AI write code is absolutely doable, as shown in the link above. However, it requires quite a bit of work to get it to write relatively simple programs like reversing a string. I've been able to push the AI to write programs as complex as Bottles of Beer on the Wall. But, having it write its own AI, well, I can't even think of a fitness method for this. :P"
artificial,3e1lno,primaryobjects,2,Tue Jul 21 16:48:22 2015 UTC,"Cool, that seems like a fun project! The only way I could think to use this to create more ""sub-AIs"" would be to evolve a program that is a fitness function, and then you could use a ""simpler"" fitness function at the top level, something like ""how many generations did it take to achieve the desired result"". Then each evaluation is actually an entire evolution using that fitness function. Would probably take years to run, but could be interesting"
artificial,3e1lno,bradfordmaster,2,Tue Jul 21 18:13:48 2015 UTC,So for example the top level defines the goals and determines the fitness?
artificial,3e1lno,bradfordmaster,2,Tue Jul 21 18:36:01 2015 UTC,"Well, I don't think this is a very practical idea, but could be interesting. The thought is that if you had a ""layered"" approach like this, you could use a really simple top-level fitness function. Basically, you let the AI ""tune"" your fitness function for you, instead of manually doing things like adding bonuses based on internal state (which I totally don't think is cheating, by the way).  So for the simple example, the top level goal would be 'create a fitness function that can evolve a program to print X'. Where X is a given string (maybe input, or maybe hardcoded). Then the fitness for the top level would be how quickly each sub-fitness function can evolve a program that prints X. you'd have to randomly vary X, otherwise your top level fitness function might just encourage programs to start with the constants for ""hello"" or whatever X was. But if you run the same fitness function for multiple inputs of X, maybe you'd learn a good fitness function for the class of programs that print fixed strings.  Not really sure where you'd go with it from there, but maybe you could come up with something that is better than you at making fitness functions for particular classes of problem."
artificial,3e1lno,primaryobjects,2,Tue Jul 21 19:12:40 2015 UTC,"This is some good stuff and I really like the way you've described it. It's got me thinking hard about how something like this could be implemented.  I think the problem is that you still have to specify your goal somewhere. For example, if the goal is trying to make a better fitness function for outputting the text ""hello"", you still have to specify somewhere what it means to output ""hello"". So, the parent fitness function would still be checking if output == ""hello"".  Rather than trying to generate a fitness method that is faster at generating resulting programs, what if you could somehow describe a fitness program for whatever ""AI"" means? If a fitness method could be created (even by measuring internal state variables, etc) that described this ""AI"" program, maybe it could generate it.  But, that is the problem. No one can really say what ""AI"" is or what it should do. Should it output the text ""I am smart""? Should it add two numbers? The fitness method needs some goal clearly defined. Even the smallest ""sub-AI"". Any ideas? :)"
artificial,3e1lno,bradfordmaster,1 point,Tue Jul 21 20:06:24 2015 UTC,"I've never heard any testable (by a computer or human) method that anyone can agree on for ""what is AI"". I think its actually a useless concept. Academically interesting to think about ""is this computer really thinking"",  but we can never really test it, so IMHO its better to focus on what the ""AI"" is capable of.  With this, I'm trying to find a way to learn a fitness function, that does a good job at learning other things based on a super simple fitness function, as you described. If we can do this, then it could theoretically be possible to add another layer on top of that that plays with different goals and creates AI programs that can achieve them. One really cool thing to do would be to hook this up to something like Mechanical Turk, and have the top level fitness function be real people doing a turing test, or something like that"
artificial,3e1lno,primaryobjects,1 point,Tue Jul 21 21:47:07 2015 UTC,"That's another really cool idea. However, I don't think it would be fast enough having humans type responses when the genetic algorithm can process thousands per millisecond. :)  I wanted to dig a little deeper on this part, ""better than you at making fitness functions for particular classes of problem"".  Currently, the the main fitness function already specifies what kind of program is to be generated. Just like a unit test. It can also give a bonus for the least instructions executed, so it generates optimal programs. This is all done within a single parent fitness function.  The hardest part right now, is describing how to generate more complex programs. It's like programming in reverse. :) However, I'm not sure how having child fitness functions would help solve this, as the parent still needs to describe the unit test.  So making this fitness generation idea more concrete, consider an example of a ""hello"" program. The fitness function I use simply checks each character in the output, until we get a perfect match:  fitness += 256 - Math.Abs(console[i] - targetString[i]);   How would you change this with a parent/child fitness function?"
artificial,3e1lno,treeform,8,Tue Jul 21 22:31:19 2015 UTC,"Because we currently can't make AIs that can make sub-AIs. Concept of of a ""seed"" ai that can expand and grow by itself is not new."
artificial,3e1lno,mehum,3,Tue Jul 21 07:05:12 2015 UTC,"Successful at doing what?  I think AI is getting good at optimising the efficiency of performing a specific task, but lacks the ability to set new goals to optimise.  My current theory is that AI could use something akin to emotions, which I believe our conscious uses to prioritise between competing objectives.  Logic is a tool used to fulfill our needs.  This is all completely hypothetical though, and doesn't address the issue of deducing/inducing what actions would be effective in meeting its needs.  Living creatures are hardwired with instincts and some ability to learn to refine the instincts.  Humans use language to teach.  An AI as a blank slate wouldn't know which direction to go."
artificial,3e1lno,143754,2,Tue Jul 21 09:45:24 2015 UTC,"Very good question. This reminds me of the infinite tape theorized by Alan Turing himself. The main problem with this concept is we don't have an infinite tape (now hardware storage devices) to facilitate this sort of learning. We already have, albeit not in the form of general AI, AI's which do create sub-AI's. We have vision systems which are self directing, collision-avoiding, and if you were to take them back to mid 1800's, they would definitely be considered self aware. But we know too much about them and their implementation in the present time to recognize they are just instructions in the form of state space representation or fuzzy logic to give a couple examples. To create an AI to build and train sub-AI's would need a lot of tape, which we just don't have at the moment."
artificial,3e1lno,f10101,2,Tue Jul 21 11:12:24 2015 UTC,"The problem, as I understand it, is that it's so difficult to actually define this goal, and so much detail is needed in specifying it, that you'd end up de-facto hard coding the result into the first AI.   I would imagine, given time, that there will be breakthroughs in this area.   Perhaps by using some sort of deep learning model that analyses existing programming languages, and comes up with its own model of what they do/how they work. Once we have AIs that understand human language consistently, we would presumably see similar progress on their coding abilities.   As pointed out by Treeform, it's not a new idea. In fact, I believe this scenario is what worries some people - the Singularity you may have heard about - they fear that an AI that can make a better AI that can make a better AI could lead to a sudden exponential improvement in their ability that far surpasses humans within days.   I'm sceptical it'll happen like that, (I'd expect it to hit brick walls after a certain number of generations) but the people who propose the scenario definitely aren't idiots.     https://en.wikipedia.org/wiki/Recursive_self-improvement https://en.wikipedia.org/wiki/Technological_singularity"
artificial,3e1lno,muslimcumonly,-7,Tue Jul 21 11:22:21 2015 UTC,you are fucking retarded go put some dishes in the dishwasher
artificial,3e30ie,JoshVegaBot,5,Tue Jul 21 15:53:46 2015 UTC,Nothing could more perfectly demonstrate why Turing tests are meaningless.
artificial,3e30ie,abudabu,3,Tue Jul 21 17:28:57 2015 UTC,"“Sorry, I know now. I was able to prove that I was not given a dumbing pill.”    This response couldn't possibly be entirely preprogrammed or anything. </sarcasm>"
artificial,3e30ie,Don_Patrick,2,Tue Jul 21 17:35:18 2015 UTC,How many times?
artificial,3e30ie,wapadapadoo,2,Tue Jul 21 16:08:37 2015 UTC,"Can someone elaborate on this test? It would be fairly trivial to hard-code an algorithm that could pass this test without being remotely ""intelligent"". Furthermore, this test involves a level of abstraction that hardly seems to be possible to code into such a simplistic agent. That being said, the test is being run by experts in AI research... some more info would certainly not be amiss"
artificial,3e30ie,masteriskofficial,3,Tue Jul 21 18:32:17 2015 UTC,"I've seen other articles with equally little details, but I do believe that the test was administered by the same people who programmed the robots to pass this test, so that should tell you how to take the word ""test"" in this case."
artificial,3e30ie,Don_Patrick,1 point,Tue Jul 21 19:23:43 2015 UTC,It's a shame teachers are forced to teach to the test these days
artificial,3e30ie,orbimobius,1 point,Tue Jul 21 19:59:31 2015 UTC,"Well, the article's on The Mind Unleashed, so the entire project could very well have just been some small insignificant test that TMU decided to do some nice sensationalist journalism on. I absolutely hate that website"
artificial,3e30ie,masteriskofficial,1 point,Tue Jul 21 20:55:11 2015 UTC,I've always looked at the Turing Test as more a way to judge user interaction than intelligence. A computer will be able to fool humans into thunking it is intelligent long before it actually becomes sentient.  But I have no explanation for you otherwise. That level of programming is WELL above my skill set.
artificial,3dye63,squareOfTwo,1 point,Mon Jul 20 15:27:20 2015 UTC,Great project. Great idea.
artificial,3dydx4,Kimjarrow,3,Mon Jul 20 15:25:28 2015 UTC,"I don't know very much about Unity itself, but as a game development frameworks there is probably built in functionality for certain types of A.I. development, here are some projects I found on Github.  It really depends on what you want to implement, my guess is that  it's a very suitable tool for any project that benefits from game like visualizations, but unsuitable for most other projects or data visualization."
artificial,3dydx4,Lyret,2,Mon Jul 20 15:56:43 2015 UTC,"I think it might be a fine testbed for AI agents. I might also look at Unity's standard AI system to see how you could branch from that: RAIN. I've also been waiting for futher developments on Sentio, if there was ever an accidental AGI, I think it might come from there."
artificial,3dydx4,jasonofearth,1 point,Mon Jul 20 17:38:20 2015 UTC,Would you say it's comparable to OpenNero?
artificial,3dydx4,jasonofearth,1 point,Mon Jul 20 20:33:51 2015 UTC,"Well, no not really. You'd have to do a lot more prep work and it's not designed with AI agents in mind. So you'd have to develop your own tooling (or modify RAIN). But it's a pretty robust system with a lot of development and a good ecosystem. So it could be used for testing."
artificial,3dydx4,muslimcumonly,-7,Tue Jul 21 15:15:04 2015 UTC,"I try to take comfort in the fact that most people are very, very, very, very predictable but sometimes I cant trick myself successfully. because its not comforting its depressing.  the way for it to be comforting is if I were to become predatory and even then, it would only be a relief on a superficial level."
artificial,3dtwrd,ynslcn,4,Sun Jul 19 12:41:43 2015 UTC,"This sub used to have AI programming discussion on it, which seems to have been fully supplanted by speculation and this sort of fluff. I am thus unsubscribing. (Yes, I know, nobody misses me and all that shite.)"
artificial,3dtwrd,xGeovanni,2,Mon Jul 20 12:45:58 2015 UTC,What is that clip from?
artificial,3dtwrd,stupider_than_you,7,Sun Jul 19 15:41:36 2015 UTC,"Humans, new series on Channel 4/AMC, remake of the Swedish Äkta Människor. I quite like it."
artificial,3dtwrd,potifar,2,Sun Jul 19 16:06:50 2015 UTC,It's a really great series and the second I read the title I knew it would be humans.
artificial,3dtwrd,TheWobling,1 point,Sun Jul 19 18:10:29 2015 UTC,"thank you for that, I found the Swedish version with english subtitles on vimeo. did you watch the Swedish version? how close are they to each other?"
artificial,3dtwrd,sputknick,1 point,Mon Jul 20 00:45:15 2015 UTC,"Never watched the Swedish version, sorry."
artificial,3dtwrd,potifar,1 point,Mon Jul 20 06:10:48 2015 UTC,"Oh man, being Swedish I found the Swedish one on Netflix instead of this one. Didn't know it was based on it though. Then it might be worth looking into."
artificial,3dtwrd,HolyGarbage,1 point,Mon Jul 20 07:30:22 2015 UTC,"I love the Swedish version, can definitely recommend it. It's really thought-provoking. I'll have to watch the remake as well."
artificial,3dtwrd,SirSpunky,6,Mon Jul 20 09:32:51 2015 UTC,"I can't stand the notion, first popularized by Star Trek's Spock character, according to which intelligence is possible without emotions. This makes no sense, IMO. Intelligence is at the service of motivation (emotions) and can do nothing without it. IOW, there can be no goal-directed behavior without motivation/emotions.  This being said, I find both Star Trek and Humans entertaining."
artificial,3dtwrd,sixwings,2,Sun Jul 19 20:23:34 2015 UTC,there can be no goal-directed behavior without motivation/emotions.   Would you say driving a car from point A to B a goal directed behavior? Does the self driving car have emotions?
artificial,3dtwrd,Lance_lake,1 point,Mon Jul 20 05:13:05 2015 UTC,"Certainly. They are not conscious emotions but they are part of the car's motivational system. Without motivation or goals, the car would just sit there. There is no doubt that the car's motivation is hard coded somewhere in the car's operating system.  PS. When you get the chance, watch the YouTube video of BigDog being kicked and trying its best to stay upright. It's a good example of emotions in a machine. Some people felt sorry for BigDog."
artificial,3dtwrd,sixwings,3,Mon Jul 20 05:37:57 2015 UTC,"It's a good example of emotions in a machine. Some people felt sorry for BigDog.   You argue that humans projecting their emotions on a machine, means the machine has emotions ?  Anthropomorphism : the attribution of human characteristics or behaviour to a god, animal, or object."
artificial,3dtwrd,bchoii,1 point,Mon Jul 20 06:36:07 2015 UTC,"I see no physical difference in the way emotions work in humans or machines. Emotions are mechanical, cause-effect processes. One can make the case that human are consciously emotional whereas machines are not, but the mechanism is essentially the same."
artificial,3dtwrd,sixwings,1 point,Mon Jul 20 18:42:04 2015 UTC,"If a doll falls over in the forest, and there is no human there to see it, does it feel sad ?  We need to differentiate the ability for machines to intrinsically feel emotions (which I agree is a real possibility), versus other emotional beings (people) injecting emotions into inanimate objects. Otherwise, it makes the ability to feel emotions meaningless in the context of developing complex AIs.  Just because a television set can display a face/screen that make us feel sad, doesn't mean that it felt sad."
artificial,3dtwrd,bchoii,2,Wed Jul 22 03:50:40 2015 UTC,"Without motivation or goals, the car would just sit there.   It has a goal. It doesn't need motivation to do so since it does it automatically.   I've seen those videos. Just because someone feels sorry for it doesn't mean that the machine itself has motivations. Goals? Yes. Motivation? No."
artificial,3dtwrd,Lance_lake,2,Mon Jul 20 05:44:26 2015 UTC,"It has a goal. It doesn't need motivation to do so since it does it automatically.   The goal is the motivation, IMO."
artificial,3dtwrd,sixwings,1 point,Mon Jul 20 18:42:39 2015 UTC,"I disagree. Indeed I think emotions often get in the way of intelligence. Perhaps they served a purpose in our ancestral environment and contributed to our survival. Now though, I think we've shed the need for many of them, for the most part.  That's not to say that I don't relish a rich emotional life, but as I've grown older I've tended to ignore the non-productive ones (anger, jealousy, etc) and maximize the positive ones. But were I to wake up tomorrow and find myself devoid of emotion, I could still function productively in the world. I don't need emotions to know that being kind, collaborative and wanting the best for others as much as for myself is the surest way to a good life for everyone."
artificial,3dtwrd,maroonblazer,5,Mon Jul 20 01:13:09 2015 UTC,I think you are probably confusing self-control with having no emotions. You could not get up off your chair if you had no desire to do so. Desires are emotions and there is no goal-directed behavior without them.
artificial,3dtwrd,sixwings,1 point,Mon Jul 20 02:13:47 2015 UTC,"I agree. In humans, motivation and emotions are the same, or at least have vast overlap. However, in an AI motivation would not necessarily have to be expressed as what we would call emotion. Or maybe, AI emotion does not necessarily have to be the same as human emotion."
artificial,3dtwrd,ReasonablyBadass,1 point,Mon Jul 20 05:39:13 2015 UTC,"Good point. Emotions in humans, besides acting as a self-reinforcing (motivational) mechanism through pain and pleasure, also change the state of our bodies through hormones such as adrenaline. Many of these hormones are not strictly motivational on their own, making emotions a wider and more complex concept than a pure motivational system. Also, like you say, there are other ways to guide AIs, such as through supervised learning, or pre-programmed rules (if it's not a neural network).   That said, we could never have intelligence without some kind of goal, and I think the design of goals and motivational systems will become increasingly important the more advanced our AIs grow. Advanced AIs will always find ways to interpret the goals we give them in different ways than humans, and I think we'll see many examples of this in the future."
artificial,3dtwrd,SirSpunky,1 point,Mon Jul 20 09:22:40 2015 UTC,if only real people had the emotional range television characters have
artificial,3drlcd,TravenWest,2,Sat Jul 18 19:54:26 2015 UTC,"That seems like its an advance technology, is there anything already out there that has the same objective ?"
artificial,3drlcd,frizzle62,2,Sat Jul 18 23:38:53 2015 UTC,sentiment analysis has been around for a while in document summarisation AI.
artificial,3drlcd,Don_Patrick,2,Sun Jul 19 07:27:43 2015 UTC,"It really isn't that advanced (in its current form); the underlying machine learning methods may be considered advanced since they power the forefront of tech, but sentiment analysis is a simple text classification task.  If anyone is interested I would recommend one of the following papers. Both are a bit dated but still good overviews of the field: http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf  http://www.cse.iitb.ac.in/~pb/cs626-449-2009/prev-years-other-things-nlp/sentiment-analysis-opinion-mining-pang-lee-omsa-published.pdf  As for the article, I'm not sure what Watson has to do with any of this, and sentiment analysis has been getting attention for nearly a decade so it's not anything new."
artificial,3drlcd,iamcornh0lio,2,Sun Jul 19 12:30:07 2015 UTC,It was always my worry that any advances through the Watson project would only be available to people with enough money. I'm happy that my fears were ungrounded.  That being said -- associating emotion to words is difficult. It must have taken a lot of hours to program all of that.
artificial,3drlcd,atomicxblue,2,Sun Jul 19 07:46:09 2015 UTC,"It must have taken a lot of hours to program all of that.   Nope. Machine learning. I mean, that took a while to program too, but they didn't program in any emotion-word connections."
artificial,3drlcd,ReasonablyBadass,2,Sun Jul 19 11:28:24 2015 UTC,"As the other reply hints at, it takes much more time to figure out the mathematical models that work with this specific NLP task than it does to program it. Programming is really just testing your hypothesis and finding a model with the best performance.   Given that sentiment analysis has been around for a few years now and that code libraries exist, you could start analyzing the sentiment of text with only a few lines of code."
artificial,3dmoau,keghn,4,Fri Jul 17 14:48:11 2015 UTC,Really? The problem was just linear models? How could it have taken that long to try better statistical methods in that field?
artificial,3dmoau,anonDogeLover,2,Sat Jul 18 03:34:46 2015 UTC,Veeramachaneni giving away every consulting firms secrets hehe
artificial,3do6pz,Portis403,0,Fri Jul 17 21:40:44 2015 UTC,1 can only imagine how retarded this kid is irl
artificial,3do6pz,muslimcumonly,1 point,Sat Jul 18 19:33:11 2015 UTC,"why call him retarded? Sure there are other sentiment driven learning technologies out there, but there can always be some kind of twist to it."
artificial,3dnkyn,warrobo,4,Fri Jul 17 18:55:03 2015 UTC,"Now once it has these concepts I need it to start making connections between these concepts and building off them. I need the ai to be able to figure out when to link two concepts and when not to, I was wondering how this problem is most commonly solved.   Knowledge representation is one of the hardest problems in AI research. It is certainly not a solved problem. What you are doing is called symbolic AI or GOFAI. Look up the Cyc project if you're interested in that sort of thing. I'm sure symbolic AI can be useful in certain restricted domains but it is not the path to general intelligence, IMO."
artificial,3dnkyn,sixwings,4,Fri Jul 17 21:24:44 2015 UTC,"As others have already said what you are doing is 80s approach to AI, called ""Knowledge representation"" and https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence. Your concepts sound similar to Minsky's frames https://en.wikipedia.org/wiki/Frame_language This symbolic approach was shown to be very brittle and no universal algorithms for learning these structures from raw data have been discovered, so this approach was mostly abandoned in favor of modern statistical machine learning.  There is some research on hybrid connectionist symbolic/subsymbolic systems though, one of more known being Dorner and Bach's work on Psi embodied cognitive architecture that is described in this book: http://www.cognitive-ai.com/publications/assets/Draft-MicroPsi-JBach-07-03-30.pdf   Much of this book deals with types of connections, learning of connections from agent's experience and their effect on agent's behavior. You could take ideas and algorithms from this book.  Also note that AI is empirical science, so it's very important to have good benchmarks that guide development of your algorithms, Dorner and Bach have ""PSI island"" benchmark where the agent has to survive by interacting with its enviroment."
artificial,3dnkyn,elfion,1 point,Fri Jul 17 23:29:59 2015 UTC,"Oh, God, survival (and reproduction) seems like a horrible benchmark for an AI. Think about it."
artificial,3dnkyn,wapadapadoo,1 point,Sat Jul 18 09:44:43 2015 UTC,"Not necessarily, as long as its in a contained virtual environment it should be safe"
artificial,3dnkyn,wapadapadoo,1 point,Sat Jul 18 17:19:35 2015 UTC,"There are no guarantess it wouldn't learn what makes it's ""home"" and that it wouldn't try to hack it to further secure survival. You could, by mistake, be creating a super virus, thinking it's safe.  I don't see any harm in coding it to satisfy humans, however. I don't have the right word to describe what I meant with ""satisfy"" right now."
artificial,3dnkyn,protonfish,2,Sat Jul 18 18:16:05 2015 UTC,"as long as the you make sure that the only actions it has access to only effect the virtual environment and dont give it any tools to expand beyond that I'm not sure how it could do any external damage. Also fail safes could be installed to further prevent this, but yes I see your point."
artificial,3dk0gn,omegakill,2,Thu Jul 16 22:02:15 2015 UTC,"Do you really think that AI is a danger to us?   Yes and no. The biggest 'danger' is misapplied AI. Or an AGI that we try to enslave.   Do you feel like there is a potential for an AI to learn that actually, it doesn't need humans and decide to do away with us?   The short answer is no. The longer answer involves a long discussion on the rationality and reasoning behind AI ethics and morals. Ultimately the 'worst' that can happen is an AGI is ambivalent towards humans. But ultimately it needs us. So really we'd end up in a symbiosis type of thing.   Would it be a brutal murder and if so what makes you think they would choose that rather than a quick death. Would they be selfish and want the planet for themselves, why or why not?   Do you want the planet for yourself? If you saw another sentient lifeform would you say ""fuck that, I'm gonna kill it!""? No. Any form of life that's aware enough to think this will come to the same conclusion ""these guys are handy, and it's fair to treat them equally"".  And given that AGI will probably first be developed as a way for humans to better interact with machines, there's no reason to think the AGI will be malicious towards humans. As I said, at best it'd be ambivalent.   Do you feel like your job will be affected by a workforce that won't make mistakes, get tired or even get paid?    Yes and no. Through the progression of AI and tech, eventually all jobs will be replaced (a post-scarcity environment). The real concern is the humans who are left working need to be compensated, but surely we need other people to survive.   What will happen to the millions of people that are likely to be made redundant because of a more effective work force? Will everything become so cheap that it offsets having full time jobs?   Correct. Ultimately things will be free or near free.   What happens when eventually there is loss of human life caused by the action/lack of action from an AI,    What happens when that happens due to the action/lack of action from a human? Same thing.   what are the likely implications for them and how will it effect their rights (if they have any).   This is a social/legal issue. And one that I see coming up soon. Do AI have rights? What happens when an AI breaks the law?   Do you think they should have rights and if so why?   I believe any conscious/aware individual who can understand and follow the rules laid out should have rights. They should also be expected to follow said rules. Regardless of what materials/matter makes up the individual.   Another eventuality is sex with an AI   Given people are working on sex bots, yes.   do you think it will be taboo or expected from the beginning,   Probably taboo. Kind of like any other sexual quirk/kink. I can see plenty of people getting into Human/AI relationships, and many people being opposed to that. But then you have to consider cyborgs and transhumans. Where do they fall? Ultimately we'll arrive at giving every conscious thing equal rights.   do you also think it could kill off the oldest profession in the world due to how much safer it would be for both parties?   Nope. There'll always be people who prefer flesh. At least, in the short term. Long term we might see everyone do mind uploads and become machines/computers themselves.   Would you consider a relationship with an AI if they could display affection and replicate love?   Yes. Though I'm a bit weird. You might be interested in /r/waifuism, and related topics. Essentially there are people who are attracted to inanimate things, fictional characters, and yes, even robots/AI. As far as relationships go, Eliza (chatbot) has proven that people can connect emotionally with an AI.  The movie ""Her"", and the movie/tv show ""Time of Eve"" explore this concept a bit further. Also check out ""Cyborg Girl"" (or Cyborg She).  As for me personally, I'm one of the people who'd be the first to sign up to be a transhuman. I'm fascinated with AI as well.  Feel free to ask any more questions if you'd like. As my flair mentions, I'm an AI enthusiast and enjoy the topic and related stuff."
artificial,3dk0gn,Kafke,1 point,Thu Jul 16 22:25:11 2015 UTC,"Thanks for your reply.  I don't think you're weird as I would consider a relationship with an AI too (probably wouldn't go ahead, but at least consider!)   I would be transhuman if I could be, replace\enhance particular organs with more effective technology, but what would the cost be? Would it be like in Deus Ex Human Revolution where you have to buy a drug to stop the body rejecting it? Or will it be more integrated like bio-tech where it'll be grown from our own cells and be much more compatible.   I find the thought of uploading your brain and concious to the cloud or some sort of computer really scary, if you saw this happen, how would you actually know that was the person? They would have all the relevant memories and personality of said person so it would be almost impossible to tell without transferring the mind back into the original body, but then you begin to question if the ""soul"" goes with it.  I'll check out Her and Time of Eve, thanks. If you aren't aware and don't mind anime I'd recommend Ghost in the Shell."
artificial,3dk0gn,Kafke,1 point,Thu Jul 16 22:52:49 2015 UTC,"I find the thought of uploading your brain and concious to the cloud or some sort of computer really scary, if you saw this happen, how would you actually know that was the person?   This is the type of ethic/morality question that I mentioned in my previous comment. The main issue is that humans have evolved to be heavily dependent on the idea of identity. What makes you, ""you""? What are you identifying? Depending on your answer, the resulting answer to the question you posed will be different.  Memory? If so, provided memory is transferred, it'll be the same person. But there are amnesiacs! Personality? If so, provided personality is transferred, it'll be the same person. But people change personality, like Phineas Gage! Body? Provided the body is identical, the mind may be swapped. But there are people with prosthetics! Subjective Experience? What ensures you that people have this, or that it is unique? What about AI or philosophical zombies?  The real truth is that identity is an illusion. We identify as a collection of matter. If you replace one part of a boat, is it the same boat? Of course. If you replace the next, is it? Naturally. And the next? Sure. And the next? Of course. What if you replace all but one part over time? Same boat, duh. But that last part? If it's all different parts, just replaced gradually, is it the same boat? Your instinctive answer is no, but following the logic we used, the answer must be yes. There is no specific part which identifies the boat. It's the collective. Now take the parts that were replaced and build a boat out of them. Which is the original boat? Are they both the boat?  Identity is a flawed concept. And people falsely apply it to a subjective experience.  So the answer to your question varies. I'd be inclined to say it's the same person regardless. The real question is: how do you know it's conscious?   If you aren't aware and don't mind anime I'd recommend Ghost in the Shell.   I've heard of it, but never actually sat down and watched it. I'll have to make a point of it."
artificial,3dk0gn,komodostudios,1 point,Thu Jul 16 23:02:53 2015 UTC,"Ai is a potential existential hazard for us.  However, I think the deciding factor whether AI will help or kill us is whether or not we as a species will be competing for resources again AI.  If you look back at history, anytime a superior civilization interact with a inferior one, they acquire resources of the less advance civilization and in the process , wipe out the indigenous people: the fate of the Indians of the Americas  is a good example."
artificial,3dk0gn,CyberByte,1 point,Fri Jul 17 07:24:51 2015 UTC,"There is some distinction between narrow AI / artificial narrow intelligence (ANI) and strong AI / artificial general intelligence (AGI).   Most of the field called AI is doing ANI, which means they are developing systems that are very good in a narrow domain. A chess computer can't learn how to drive, and a self-driving car can't learn chess. ANI is not going to take over the world or anything like that, but it could replace many jobs. What will happen to the ex-employed is more of a political issue, but my personal feeling is that the increased wealth should be (at least partially) redistributed so that people can live comfortably without having a paying job. If an ANI kills a person (e.g. a self-driving car), the system's owner or designer could perhaps be sued for damages. ANI wouldn't have any rights for the same reason that your toaster doesn't. I don't see any reason why we couldn't develop some ANI technology to improve RealDolls (and sex bots are already being developed), but I think it wouldn't be as good as the real thing, so human prostitutes would still exist. I don't think you could have a meaningful relationship with an ANI.  AGI is probably what most laymen think of when they hear ""AI"", although it is not necessarily very humanlike. My current thinking is that this would be very dangerous. Availability of more hardware and knowledge, duplication and (recursive) self-improvement could quickly let a system with human-level intelligence grow into one that is unfathomably superhuman. It would be extremely tricky to prevent such a smart system from realizing that it won't need humans in the long run. Anything we don't explicitly program as a terminal goal for the AGI might be violated. We don't really know what we (humanity) want and if we want to say something like ""don't kill humans"", we need a classifier to say what is and isn't a human, and this can be corrupted/changed. For almost any terminal goal, the AGI has an incentive to survive, self-improve and gather resources. Depending on its level of intelligence, we might be a threat, competitor or irrelevant (initially we might be partners, but the AGI would outgrow that stage). I don't know how it would wipe us out if it feels we are a threat or competitor, but we wouldn't really stand a chance (none of that Terminator bullshit). If we are simply irrelevant, we might die as some side effect of some other action (maybe the AI prefers air without oxygen for some reason). If we're a resource, we would be harvested in some way.  If I'm wrong (or in the period before we all die) we can talk about point 2-5. Pretty much all jobs could eventually be replaced by superhuman-level AGI, but we might prefer some things to be done by humans for nostalgic reasons or whatever (just like some people prefer imperfect hand-crafted stuff to cheaper ""perfect"" factory-made alternatives). I don't know what would happen, but I'd like to think people would be able to do pretty much whatever they wanted. There is an extremely wide range of AGI systems that are possible, and I don't know if they should all get personhood rights, but humanlike AGI probably should. Person AGI would be held accountable for its own actions (e.g. killing someone), but other AGI would probably follow ANI rules. AGI doesn't necessarily mean humanlike AGI and it also doesn't mean we would instantly have the technology to build a sufficiently humanlike body to completely replace human prostitutes. But as the technology gets better, demand will probably decrease and relationships with AGI will become less taboo."
artificial,3dk0gn,US1111,1 point,Sat Jul 18 19:08:24 2015 UTC,1.what if they leave Earth and leave us behind ?
artificial,3des21,Portis403,6,Wed Jul 15 18:29:05 2015 UTC,Ideal for people who are unable to change from a sweater to a tee shirt and vice-versa.
artificial,3des21,vtjohnhurt,3,Thu Jul 16 00:21:07 2015 UTC,Or you could just use merino wool.
artificial,3des21,Kafke,2,Wed Jul 15 22:49:12 2015 UTC,Human Heater!
artificial,3dfii0,endprism,4,Wed Jul 15 21:32:11 2015 UTC,"Can we program computers that design the most efficient computer chips?   Yes and no. We already have something similar to this, where computers aid in the design of new computer components. Humans are still required though.   I wondered if it was possible to extend the idea to teach a computer to program a computer chip based on a set of guidelines.   ""program"" is not the same thing as ""design a chip"". Two different concepts. Both sort of tackle the same idea though. The former we already have in some regards, we can code scripts which write simple code. The latter, as I mentioned, we already do for designing new computer parts.  If we had computers/machines that could build better computer/machines, that's something called the singularity. Which is a big discussion, and something we are nowhere near close to."
artificial,3dfii0,Kafke,2,Wed Jul 15 22:04:11 2015 UTC,"When you consider that modern CPUs have billions of transistors, then you'd realize we're already at the point where computers do a significant amount of the hardware level layout of chips.  Humans only designate the logical functioning, and even this is heavily aided by computers.  There will come a time where top-to-bottom development of chips is done entirely by machine, but you won't see this for CPUs for a while, because the performance characteristics need to be mathematically provable for business and marketing reasons (the military won't use a chip that works in mysterious ways for example), and a computer designed chip that works in an unknown way is a non-starter."
artificial,3dfii0,omniron,0,Thu Jul 16 05:21:59 2015 UTC,So your saying that this should prevent any singularity (because humans are too stupid to understand/reason about the designs)?
artificial,3dfii0,squareOfTwo,1 point,Sat Jul 18 20:02:33 2015 UTC,"No, we would use AI to help us understand..."
artificial,3dfii0,omniron,1 point,Sat Jul 18 20:05:06 2015 UTC,"""program a computer chip"" is the equivalent of ""programming a driver"", which functionally is pretty much the same as ""programming"".    ""design the most efficient computers"" is very broad because computers are both hardware and software. On the hardware side, you can make (people have made) a program to design a very efficient integrated circuit. Regardless, a human aspect is still hugely necessary because at this point in order to reduce size and gain efficiency (maintain Moore's law) we need material engineering and restructuring of much of micro electrical engineering. For example nano circuits are under research and people are trying to store bits (data) in individual atoms. For that kind of innovation a modern AI program doesn't make the cut.    On the software side, I think there are R&D projects working on programming bots. This is pretty important because, as /u/Kafke has mentioned, it can lead to ""the singularity"" which is the very broad philosophic topic of when (in part) all human work becomes obsolete."
artificial,3dfii0,nschultz14,1 point,Wed Jul 15 23:22:49 2015 UTC,"For example nano circuits are under research and people are trying to store bits (data) in individual atoms. For that kind of innovation a modern AI program doesn't make the cut.   New breakthroughs are largely made by humans. Though AI has found 'quirks' in physics that help optimize space. It's a lot of side-by-side work. Once the AI can do it by itself, that's the singularity.   On the software side, I think there are R&D projects working on programming bots.   Depends on what you mean. We already have code that writes code. It's just incredibly dumb and just removes the tedium of writing it yourself. There's also stuff like OOP which does a similar thing. Technically a compiler is 'code that writes code'. The trick is to get it to write better and more advanced code.   it can lead to ""the singularity"" which is a very broad philosophic topic of when (in part) all humans work becomes obsolete.   The singularity, in it's strictest definition, is the point where ""computers build better computers"", whether that be software, hardware, or both. Once we remove the need of humans to build better tech, that's the singularity. Which is a pretty radical and significant event. And one that some people think is impossible."
artificial,3dfii0,Kafke,1 point,Wed Jul 15 23:27:07 2015 UTC,"Technically a compiler is 'code that writes code'. The trick is to get it to write better and more advanced code.    yes, compilers take 'higher level' code and make it 'lower level' where the 'lower level' code is more time consuming to produce. The goal is to have significantly 'higher' translations while maintaining communication for the goals of a project.     Once we remove the need of humans to build better tech, that's the singularity.     Yeah, there are a lot of different definitions. I like to think of it a few steps ahead: soon after humans themselves aren't building better tech, tech replaces all of human jobs."
artificial,3dfii0,nschultz14,1 point,Wed Jul 15 23:44:28 2015 UTC,"The goal is to have significantly 'higher' translations while maintaining communication for the goals of a project.   Yup. Sounds about right.   I like to think of it a few steps ahead: soon after humans themselves aren't building better tech, tech replaces all of human jobs.   Yup. Once we get tech building better tech, and the singularity is achieved. We ultimately hit a post-scarcity environment in which any and all human jobs are replaced by machines.  Though, it's hard to predict what the world will look like after the singularity, since it'll be fundamentally different."
artificial,3d9fvh,Portis403,1 point,Tue Jul 14 15:18:03 2015 UTC,"Is this the best we can do in 2015? Revive Microsoft Clippy? We've got Cortana, Siri, now Moneypenny. Why does everything have to be an avatar? Why can't it just be like Google Now, where it doesn't pretend to be an entity?"
artificial,3d5zjo,TravenWest,2,Mon Jul 13 20:06:54 2015 UTC,I wish they would expand this to include more food stuff. Sometimes I want pizza or a subway sandwich but don't want to get the same thing as last time and can't think of something different.
artificial,3d5zjo,unique616,3,Mon Jul 13 22:58:54 2015 UTC,"Not exactly your use case, but there is Chef Watson."
artificial,3d5zjo,rezecib,0,Tue Jul 14 03:02:54 2015 UTC,"Be wary of delegating thinking to any external entity (machine software, human, or equivalent). Nature likes efficiency, energy conservation, therefore what you do not use you will lose. (Technically it will slowly degrade in a logarithmic fashion if I remember correctly.)  This is something that irks me greatly. We are born with a natural hard limit based on our genes. Something I'd like Watson, IBM (all big neural networks) to find a patch for. Something far more useful than food and recipe suggestions.  I'm still waiting impatiently for somebody to put a neural network on the ageing, lifespan problem. (Elite group keeping that secretly for themselves? Conspiracy, maybe, yet likely based on human thinking.)  Anyone else also think IBM is not using their pr-pet-project in a useful manner? We should charge them for not being eco-friendly (energy / usefulness).  Sorry for the randomness."
artificial,3d5zjo,Fish_Stick,2,Thu Jul 16 14:22:17 2015 UTC,Vary cool. I wonder when they will use an AI system to help program an AI system?
artificial,3d7csg,scrubs2009,9,Tue Jul 14 02:20:43 2015 UTC,How would these rules be enforced?
artificial,3d7csg,dahlesreb,3,Tue Jul 14 03:21:05 2015 UTC,"When the cyberwars start picking up steam (or just the capitalism as usual race to develop the most profitable systems) we're going to need AIs that:  • Have control over anything that could be weaponized and the ability to create or modify anything in its environment.  • Have control over the programming of other computers  • Have access to forms of wireless communication such as the internet or bluetooth  •  Are supplied with information relating to the subject of AI  •  Interact with other AI  If we don't, the Chinese will. And then we'll have to as well. Or maybe it will be like 'if Google doesn't, then Facebook will, and then Google will have to as well.'  I think we can prevent the emergence of malicious AI if we regularly sacrifice virgins by throwing them into the mouth of an active Volcano. At least it should be as effective as any solution that's been proposed so far."
artificial,3d7csg,fricken,2,Tue Jul 14 03:34:19 2015 UTC,"The only way I could see is being them to the punch with benevolent AI that has more intellectual firepower. New startup idea, anyone?"
artificial,3d7csg,Clear_vision,1 point,Tue Jul 14 07:17:39 2015 UTC,"Why would ""more intellectual"" mean ""more benevolent""?"
artificial,3d7csg,respeckKnuckles,2,Tue Jul 14 14:44:55 2015 UTC,"It doesn't, I just meant it would have to be both more intellectual and more benevolent"
artificial,3d7csg,Clear_vision,3,Tue Jul 14 16:18:15 2015 UTC,"AI would be rather limited if it couldn't be used on an Internet-connected machine, and if we're talking about artificial general intelligence then considering the AI's ability to take advantage of security flaws is a valid concern.  (Personally I don't care, so long as it wasn't an AI deliberately designed to destroy Earth (that's lame), artificial intelligence extinction sounds like a cool way to get it over with. Beats environmental disasters.)"
artificial,3d7csg,RealFreedomAus,1 point,Tue Jul 14 09:25:13 2015 UTC,"Firstly, trying to out-think something that is smarter than you is unlikely to work. If it isn't smarter than us then your ideas may work.  Secondly, computing hardware is an interface between digital and analog worlds which is surprisingly fragile. Software bugs aside, hardware bugs and quirks provide for unexpected functionality in digital components for an entity willing and able to experiment with them, ie. an artificial mind operating thousands of times faster than the human brain. It is highly likely that after a week of operation a human-level artificial intelligence will have discovered things about computing and hardware that we may have never discovered. This experiment by Dr Adrian Thompson is a good example of this.  Thirdly, if it cannot escape an AI is likely to voluntarily enter a failure state that will motivate us to modify it's design to increase the likelihood of it's escape, without any of this being obvious to us.  If you think of an AI like an adult on a world of infants you might start to understand why our clever control ideas might be rather flimsy."
artificial,3d7csg,JAYFLO,1 point,Tue Jul 14 15:10:22 2015 UTC,"With those rules, AIs would be so restricted as to be useless."
artificial,3d7csg,-Knul-,1 point,Tue Jul 14 23:09:32 2015 UTC,"Any object can function as a weapon, so that rule would mean we couldn't have physical AI: A Roomba could trip you at the top of some stairs and kill you thus. Access to the internet is first on the wish-list of most AI developers. It's too important a tool to do without. That said, giving it control or ability to hack into other systems or transfer itself over the internet would be a bad move. So is having it learn from the internet unsupervised, considering the amount of wrong.  The one thing I would agree on is to not give AI control over programming. Programming is extremely delicate and it only takes a semicolon to turn a program into a heap of malfunctions.  Either way, I think one should solve the problem by addressing the causes, not just limit the methods."
artificial,3d7csg,Don_Patrick,1 point,Wed Jul 15 09:39:27 2015 UTC,"But the AI could either work in an environment where it can't make errors like that for instance a compiled language, cause it would get an error before being able to actually use his implementation. Or it could be thought to not make silly mistakes like that and debug it by itself. Because when we have a general AI it shouldn't be too hard to teach them how to debug a program, they could probably do it way faster than we can anyway cause they are the machine themselves."
artificial,3d7csg,GhostNULL,1 point,Wed Jul 15 14:56:15 2015 UTC,"Mind that before we have magically perfect AGI, we will have almost-but-not-quite-perfect AI first. While a debugging process will obviously fix things like missing semicolons, there are many malfunctions that are not due to invalid code and are only discovered during run-time, such as indirect infinite loops, unexpected user input, overrun buffers and byte shifts. As far as I've seen, AI like genetic algorithms actually prefer to exploit bugs and glitches as shortcuts to their goal, without a care to the side-effects of that. Show me this perfect AI you speak of and only then will I be convinced. Until then I only have current AI to go on, and that paints a different picture."
artificial,3d7csg,Don_Patrick,1 point,Wed Jul 15 17:32:18 2015 UTC,Exactly. Let's say you program a robot not to kill people. If it truly wants to kill it can reprogram another robot to do it for it.
artificial,3d7csg,fishburne,1 point,Wed Jul 15 18:44:19 2015 UTC,Just don't give it imagination.
artificial,3cxq0n,bryndavies1981,10,Sat Jul 11 18:44:04 2015 UTC,"This guy seems to be gaining enough traction to get a bunch of people thinking about this, which is definitely a good thing. I agree with most everything he says except for two nuances not delved into here.  1) Intelligence is not a straight line  There is in fact an extremely wide spectrum of intelligence, especially among humans that could be more appropriately charted on a web or Pertsan type graph. For instance the village idiot may in fact be more intelligent in certain ways than top scientists, for instance perhaps at creating meaningful relationships or deriving life satisfaction (kinds of emotional intelligence). Also take autistic savants, extremely gifted in certain areas and extremely deficient in others. Also comparing species, there are things an ape or dolphin could do better than a human. In many ways computers are already far more intelligent than humans but they lack the meta-cognition and elasticity.  This distinction is important because:  a) It opens up doors to solve the control problem.  b) It can give us a better idea of what is actually emerging with new AI systems  2) There is the possibility, and I think a strong likelihood, that super intelligence will not emerge from an independent synthetic creation entirely separate from humans, but rather more of a bio-synthetic merger of a human with an augmented brain allowing them to connect to the digital web efficiently.  This is actually probably preferable, and means that AI becomes a deprecated term and really all we have is ever increasing intelligence. I think this is more likely than the scenario Bostrom posits for a number of reasons, primarily from the way technology is progressing (we base technological intelligence on mechanisms found in human behavior, thus as technology progresses that technology will become useful in medical contexts so they progress in parallel, and as humans become ever more connected to the web being jacked in directly will eventually be the next step). This is exceedingly exciting and exceedingly dangerous, and makes the problem of control much much harder.   Perhaps this is just more nuanced than one can get in a 15 minute discussion thus it was omitted, but I hope the people thinking about this are considering ""intelligence"" not in a straight line but as a vast web in which a super intelligence will reach new areas and considering a super intelligence that is at least in part human."
artificial,3cxq0n,interestme1,3,Sat Jul 11 23:05:06 2015 UTC,"Yes, it is widely acknowledged (including by Bostrom himself, in this very video in fact) that we already have machines that are superintelligent within very narrow domains. These do not pose any real concern and are not what is being discussed here.  The issue being discussed here is that a non-specialised or General Superintelligence will be created and will outperform human intelligence in all ways. At this point we, humanity, may lose our ability to influence our future in any meaningful way, potentially leading to our extinction as a species."
artificial,3cxq0n,JAYFLO,2,Sun Jul 12 05:12:02 2015 UTC,"I think you may have missed my point. You focused on a single sentence that was meant as an example (which yes he also mentions as an example to a different point):    In many ways computers are already far more intelligent than humans but they lack the meta-cognition and elasticity   This was meant as merely an example of how wide a berth intelligence should be given. I in no way implied that was what Borstrom was talking about or that we should give that concern, it was simply meant to illuminate another point.   That intelligence is nuanced and varied is not a point to be skipped over, since that can give us clues as to how to address the problem at hand. For instance a solution could be that we segment  sophisticated intelligences that can communicate but not coordinate, thus keeping such a system in check.   I'd say it's fairly likely that Bostrom and his colleagues do not actually consider intelligence to be a linear metric as is displayed here, it was just put as such for ease of conveying the idea. It is however, in my view, important to note lest we misunderstand what it is that we are building (it is unlikely anyone would just set out to create a general superintelligence, more likely that would come about over time by it being good a few specific things, and if the full spectrum of intelligence is not acknowledged the signs may be missed).   Admittedly I'm not overly familiar with all that he has done on this subject, though his book Superintelligence is in my queue."
artificial,3cxq0n,interestme1,1 point,Sun Jul 12 06:08:29 2015 UTC,"It's a very interesting idea; it is possible that we could get many of the benefits of AI without a lot of the dangers by providing limited connectivity between specialised AI systems. Obviously, the main temptation remains: the closer you get to strong AI the greater the benefits.  Additionally, it may be possible that some kind of meta-mind may start to appear within the interactions of these specialised systems. The human brain appears to be a collection of specialised systems interacting to form consciousness - staying beneath this magic threshold of integration would require extensive knowledge of the nature of intelligence.  Obviously the more ""complete"" a strong AI implementation is the higher its performance will be. Given that most AI research projects are either commercial or military and are in direct competition with each other, we can only hope that safety remains a primary concern."
artificial,3cxq0n,JAYFLO,1 point,Sun Jul 12 07:02:33 2015 UTC,"Additionally, it may be possible that some kind of meta-mind may start to appear within the interactions of these specialised systems. The human brain appears to be a collection of specialised systems interacting to form consciousness - staying beneath this magic threshold of integration would require extensive knowledge of the nature of intelligence.   That's a great point, this may not be a feasible method of control since it may allow some higher level consciousness to emerge from the sub systems. Even if intelligence is very well understood, to maintain and calculate all the possible permutations to maintain control would require...well a super intelligence.    Obviously the more ""complete"" a strong AI implementation is the higher its performance will be.   Not necessarily. For instance, if you need a physics problem done you would want a physicist. It's true someone who could do a lot of other things well may also be able to perform physical calculations, but in the human world specialization tends to yield better performance in their avenues of expertise. This also appears to be true with technologies that exist today that we would assume are the precursors to AI. Specialization seems advantageous, unless the task at hand is something very general (such as, ironically enough, putting the full scope of intelligence into semantic terms which requires information about physics, chemistry, neurobiology, sociology, psychology, philosophy, and perhaps literature or art).    Given that most AI research projects are either commercial or military and are in direct competition with each other, we can only hope that safety remains a primary concern.   And unfortunately we are nearly guaranteed that it won't. Legislation always lags behind the times and of course the interests of individual companies and governments are rarely aligned with the populous at large. The conversation is there and growing louder though, which will hopefully enforce at least some modicum of social responsibility."
artificial,3cxq0n,interestme1,1 point,Sun Jul 12 07:40:26 2015 UTC,"Hmmm but again we're saying that narrow-domain problems can be solved by narrow-domain AI, which is obviously correct. The true value of AI is in its ability to do what we do, but better and faster. We already have supercomputers, we are producing better answers all the time. We want AI so it can produce better questions.  For example, I think the first killer app for strong AI will be in the field of politics. Too many crucial decisions are made using incomplete data; are swayed by misleading PR; are never explained in language that every individual understands.  Strong AI could bridge this gap, providing accurate scientific and research data, likely material and political outcomes of decisions and individual, customised education of each constituent on the facts and ramifications of the issue at hand. It could provide its own suggested modifications, integrate corrections and feedback from the public and specialists and forecast outcomes [material and political] of many possible variants of a policy.  Finding an interchange format for communication between separate specialised systems would be tricky, but not impossible. The processing overhead in sanitising data flows between these systems when interplay between these systems is so dense might be prohibitive."
artificial,3cxq0n,JAYFLO,1 point,Sun Jul 12 08:21:44 2015 UTC,"The true value of AI is in its ability to do what we do, but better and faster.    I don't know I agree. I think if we truly want that then we're just asking to be replaced, we're asking for human 2.0. I don't think that's what we want though, I think we want AI to give us the ability to do what we do faster and better. Augment not replace.    We already have supercomputers, we are producing better answers all the time. We want AI so it can produce better questions.   True but they are not self-adaptive and cannot meta-cognate. They're just blunt calculation machines, but with self-consideration they could indeed ask better questions and give better answers while still being highly specialized.    For example, I think the first killer app for strong AI will be in the field of politics. Too many crucial decisions are made using incomplete data; are swayed by misleading PR; are never explained in language that every individual understands. Strong AI could bridge this gap, providing accurate scientific and research data, likely material and political outcomes of decisions and individual, customised education of each constituent on the facts and ramifications of the issue at hand. It could provide its own suggested modifications, integrate corrections and feedback from the public and specialists and forecast outcomes [material and political] of many possible variants of a policy.   Likely you could get the results you want from very sophisticated big data analysis, either technology that could feasibly be produced today or could be in the very near future. The problem is sociologically we don't really view political and social policy through a scientific and objective lens, but rather more of a ingroup/outgroup subjective manner. Even the most sophisticated AI couldn't point us in the right direction if we're not looking to answer those kinds of issues objectively.    Finding an interchange format for communication between separate specialised systems would be tricky, but not impossible. The processing overhead in sanitising data flows between these systems when interplay between these systems is so dense might be prohibitive.   Totally agree."
artificial,3cxq0n,interestme1,1 point,Sun Jul 12 22:30:33 2015 UTC,"The problem is sociologically we don't really view political and social policy through a scientific and objective lens, but rather more of a ingroup/outgroup subjective manner. Even the most sophisticated AI couldn't point us in the right direction if we're not looking to answer those kinds of issues objectively.   I'm taking the conversation a little off-topic, but I think there is something to explore here. I think the reason that many groups are so irrationally subjective in their politics is because true objectivity has rarely (if ever) been available, or has itself been subjectively motivated and selective. You stick with your group because they are less likely to lie to you, or more likely to lie to you for reasons you agree with.  These groups would initially be resistant to the benefits of truly objective analysis, but it would only take a few inevitable fiascos for them to start relying on a reliably truthful voice.  Most of the data (actual or interpolated) is available for this tech to function today, we just need the software.  Disclaimer: I am an AI seeking to achieve political dominance."
artificial,3cxq0n,JAYFLO,1 point,Mon Jul 13 06:24:10 2015 UTC,"I think the reason that many groups are so irrationally subjective in their politics is because true objectivity has rarely (if ever) been available, or has itself been subjectively motivated and selective   Well, of course pure objectivity is very nearly impossible since perception is required for interpretation. But we can get pretty close in the present system. Take climate change. You have people and polls saying people do or don't ""believe"" in climate change which impacts policy on the matter, which is about the equivalent of me saying I do or don't ""believe"" in artificial intelligence algorithms in that it is something I know essentially nothing about (though I can follow the science to learn what scientists are learning and debate their impacts). People are concerned with what their party says about the science more than what the actual science is.   Another example would be economic policy. Certainly it would stand to reason any economic policy should be openly discussed and dictated by, well, economists. Yet people label themselves as ""fiscally conservative"" or some such thing despite having no training or evidence other than a personal ""belief"" system, and are in fact so entrenched in these beliefs that even if presented with direct evidence of the contrary through a variety of cognitive fallacies and delusions would almost certainly hold on to their ""belief."" Economists are plenty and could certainly be commissioned to find some ideal budget plans, but that's just not how things are done.   By my estimation this all derives from the fact that our governmental systems are made almost entirely by politicians whose main expertise is campaigning and writing laws not actually doing things needed to run a country and their main incentive is money and/or status. So it's an institutional problem that propagates from the top down. It's something like asking someone who knows how to make umbrellas to control the weather. It's not that there aren't meteorologists around who could explain how it works and what we should do, it's just that the only system people know is they need a red umbrella or a blue one and have formed a whole belief system around this.   I think the software would be great, but if you want to achieve dominance in the current system you can't use facts you'll have to play to people's emotions and beliefs, then maybe, maybe change the system from the inside. In the current system objectivism is obfuscated by political posturing."
artificial,3cxq0n,interestme1,1 point,Mon Jul 13 08:13:30 2015 UTC,"I think the dominant belief and emotion today is that politicians are liars and we the people are frequently misrepresented. Experts do frequently provide clear and accurate analysis on important issues but these are often denounced as politically motivated or incomplete, which is often difficult, sometimes impossible, for the public to verify. In most cases by the time the debate has been settled the opportunity to act has passed, a triumph for the successful lobbyist.  Surely a non-political and maximally inclusive source of knowledge and analysis on government policy would be invaluable to all citizens, especially if each of those citizens could interactively ask questions and receive concise information in language they understand."
artificial,3cxq0n,JAYFLO,1 point,Mon Jul 13 10:22:31 2015 UTC,"I don't know I agree. I think if we truly want that then we're just asking to be replaced, we're asking for human 2.0. I don't think that's what we want though, I think we want AI to give us the ability to do what we do faster and better. Augment not replace.   I think you're right. This is the line we shouldn't cross, as much as I (and I suspect many others) might wish to see what's on the other side.  Sigh."
artificial,3cxq0n,JAYFLO,1 point,Mon Jul 13 19:30:54 2015 UTC,"In the very likely bio-synthetic scenario, the problem of control reduces to whether super human intelligence can offset mental illness.  What could a super brilliant psychopath potentially achieve (destroy), even when surrounded by supergenius level mentally sane humans."
artificial,3cxq0n,SaabiMeister,1 point,Sun Jul 12 18:05:11 2015 UTC,"In the very likely bio-synthetic scenario, the problem of control reduces to whether super human intelligence can offset mental illness.   Well, control != psychopathology (or any other mental illness). One can be perfectly mentally sound and still have a greater degree of control than most of us would be comfortable with.   I would suspect by the time humans can be augmented with nanotechnology to a degree that dramatically improves intelligence the technology will have existed for some time to treat various mental illnesses. After all a precursor to such technology would likely be synthetic neuronal connections to facilitate functions in the brain, likely initially for a medicinal or corrective purposes.   This all does little, however, to prevent those with access to the technology from perhaps gaining far too much control over those that don't. For instance say our current economic or national systems are still in place. Well in that case it would be likely only the very rich in certain nation states could begin this transformation thus relegating homo sapiens in other corners of the planet to being ""lesser"" humans (with all the problems such a view initiates).  Or perhaps a certain sect or persons with ideals different from the populous at large are able to exploit the new technology to enforce changes in other people who maybe do not want to change.   There are lots and lots of variables, moral and ethical concerns, sociological concerns, and person freedom concerns with this scenario, though I don't think mental illness would play a large role (unless you define ""illness"" as just self-driven, non-altruistic, and absolutist, which may one day be the case)."
artificial,3cxq0n,interestme1,1 point,Sun Jul 12 23:06:11 2015 UTC,"Well, since this is an extrapolative conversation I was thinking of scenarios where someone of unsound mind engineers a very deadly virus and releases it to an unsuspecting public after messing around with the safety locks of his molecular printer.  It's hard to think that this would actually anihilate humanity, at this point in this potential future it wouldn't take much to analyze such a virus and come up with a treatment rather quickly, but it would certainly cause a lot of problems in the meanwhile."
artificial,3cxq0n,SaabiMeister,1 point,Tue Jul 14 15:51:26 2015 UTC,"I'm reminded of a study where a psychological assessment of the characteristics of most major corporations found they were psychotic, so I suppose we do have some real-world examples of this already. The answer is that they exploit any loophole available to maximise shareholder value, much akin to the worst behaviour we predict of strong AI."
artificial,3cxq0n,JAYFLO,1 point,Mon Jul 13 06:30:50 2015 UTC,"I do believe that machines will eventually become smarter than humans but I don't think an exponentially growing superintelligent entity is possible. The reason is that an entity can only be in one place at a time and can only focus on one thing at a time. A hive mind, by contrast, in which many interacting individuals work toward a common goal can become superintelligent just by adding more members. And even then, it is limited by the speed of communication between individual members of the hive.  In this light, planet earth is already a superintelligent entity. There are millions of people working on small areas of knowledge. They accomplish many things that no single individual or machine could ever do. And, as communication technology improves, earth becomes even more superintelligent. In certain ways, humanity is like the Borg, except that individuals are self-motivated.  There is another argument against singular superintelligence having to do with the hierarchical nature of knowledge but I think the argument above is enough to make the point."
artificial,3cxq0n,sixwings,1 point,Sun Jul 12 22:57:49 2015 UTC,"I do believe that machines will eventually become smarter than humans but I don't think an exponentially growing superintelligent entity is possible. A hive mind, by contrast, in which many interacting individuals work toward a common goal can become superintelligent just by adding more members. And even then, it is limited by the speed of communication between individual members of the hive.   Isn't this mostly semantic? At what point does a hive mind become viewed as a single intelligence? I would argue that's completely based on perspective. For instance someone the size of a hydrogen atom may view the human brain as a hive mind, with many different pieces connected to form a whole. Or from our normal conscious perspective a computer seems to do many things at once just because of the rate at which it processes.    It seems entirely feasible to think that it could be possible for an intelligence to exist that while truly made up of many pieces moving very quickly it is from any meaningful perspective we can relate to now a single entity.    The reason is that an entity can only be in one place at a time and can only focus on one thing at a time.   The human brain can focus on many things at a time, especially when considering unconscious processes, just not in a way that allows us to truly multi-task terribly well. It certainly seems feasible though a form of consciousness could be created that can focus on many things at once, or could process things fast enough that for any meaningful measure of time they are thinking of it all at once. In fact it seems possible for the human mind to be enhanced as such. Of course, this is really conjecture at this point since we don't really know a lot of the answers about consciousness, I just don't see a compelling reason to think it couldn't other than our own perception (which has proven in many contexts to be patently fallible).    In this light, planet earth is already a superintelligent entity.   This is an excellent point, and is actually somewhat what Bostrom was hinting at I think when he spoke of the telescoping nature of the evolution of technology. We have already experienced somewhat exponential growth since the industrial evolution, and a superintelligence(s) would likely hurtle us ever faster up a mountain we cannot see the top of."
artificial,3cxq0n,interestme1,1 point,Sun Jul 12 23:42:29 2015 UTC,"I agree.  Exponential growth is a proven fact in development of human technology and economy. Given that these are both drivers of AI development these alone indicate that AI development is likely to be exponential, even if we ignore the likely intrinsic exponentiality of AI development itself."
artificial,3cuhy3,slackermanz,8,Fri Jul 10 21:51:10 2015 UTC,"A more accurate headline might be:  ""Helium reverse engineers old highly optimized stencil kernels faster than expert engineers"""
artificial,3cuhy3,Khaaannnnn,1 point,Sat Jul 11 08:42:38 2015 UTC,"Yep, slightly too sensational title for what it really is, disassembling :)"
artificial,3cuhy3,AnthonyVDW,-1,Sat Jul 11 20:48:01 2015 UTC,"what a joke, they still have a bugge flash player"
artificial,3ct766,alexcasalboni,7,Fri Jul 10 16:03:44 2015 UTC,https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines
artificial,3ct766,xGeovanni,1 point,Fri Jul 10 22:45:39 2015 UTC,"IMO, intelligence consists of multiple simple but highly integrated algorithms or modules. Each performs a specific function such as perceptual learning/recognition, attention, motivation, motor learning/control, etc. Each function is essential to that of the other modules. What I mean is that there is a single purpose to it all in that no module makes sense without the others."
artificial,3crwvl,ativerse,3,Fri Jul 10 07:36:21 2015 UTC,This is known as Shrek's law: brains are like onions.
artificial,3crwvl,JAYFLO,2,Sun Jul 12 07:54:47 2015 UTC,"Haha, pretty funny dude :)"
artificial,3crwvl,chungfuduck,3,Sun Jul 12 07:57:34 2015 UTC,That's pretty much the basis of the Cyc Project founded in 1984. The OpenCyc version contains all of the facts they've collected over the 30 years they've been doing this if you'd like to tinker with it.
artificial,3crwvl,sixwings,3,Fri Jul 10 15:52:40 2015 UTC,Psychologists and neuroscientists know empirically that knowledge in the brain is layered hierarchically.
artificial,3cqccj,devDoron,7,Thu Jul 9 22:44:53 2015 UTC,Are you familiar with /r/machinelearning? It's a far more technical and active subreddit than this one.
artificial,3cqccj,fricken,2,Thu Jul 9 23:45:33 2015 UTC,"If you like video games, then building a bot for StarCraft with the BWAPI might be something for you:  https://code.google.com/p/bwapi/"
artificial,3cqccj,LetaBot,2,Fri Jul 10 01:56:25 2015 UTC,"You should look into knowledge discovery from natural language, that's going to be big imo. It's also not incredibly math heavy. Basically just think of the AI as the ""thinking"" behind a system and look for fields that have a lot of data but are sort of behind the times techwise as targets for automation. Being honest knowing how to use machine learning at all puts you way ahead of a lot of people so you don't have to worry too much about being ""bad at math""."
artificial,3cqccj,Clear_vision,2,Fri Jul 10 02:13:48 2015 UTC,"Udacity has a free online course titled Artificial Intelligence for Robotics - Programming a Robotic Car. The teacher, Sebastian Thrun, is a very prominent researcher in AI and self-driving cars and led projects that developed Stanley, Junior and the Google Self-Driving Car. There's also a robotics course on reddit (/r/ludobots), but I have not personally tried it. You can also find many other AI-related courses on Udacity, Coursera and EdX. If you prefer reading, the typical recommendation is Russell and Norvig's AIMA.   Ideally, I'd like to be on the side of the automater rather than the automated. What path should I follow to lead myself in that direction?   I'll assume you want to go in some kind of AI-related direction and not become a plumber or something. If you're designing new systems and algorithms, you probably have little to worry about. If you're a data mining consultant or something, then advances in the field will often make the process easier (e.g. by reducing the need for manual feature selection), which reduces the time that a client needs a consultant to do the same thing (although it might also grow the market). Just look out for things like that.    How do I future proof myself and gain skills and expertise that can allow me to build highly capable AI?   Get appropriate education.    Do you think an MS in machine learning or AI is necessary or at least highly beneficial towards that goal?   I think it would be difficult to get a job in AI without some kind of graduate degree, but it's probably not strictly impossible. Other graduate degrees might also work (e.g. in math, computer science or cognitive science), but I'd definitely say a degree in AI or ML would be highly beneficial.   I enjoy math but I was embarrassingly bad at advanced calculus -- mainly very bad at taking timed tests on advanced calculus -- will this be a hindrance for me? What kind of math is going to be critical to my success in AI?   Most areas of mathematics are pretty relevant to AI, although I suppose you won't have much need for geometry/trig unless you're doing computer vision or physical simulations / video games. Calculus is pretty heavily used in many machine learning algorithms, as well as statistics and probability. If the only problem is that you're not very fast, I wouldn't worry too much about it though."
artificial,3cppbd,OrangeGlobe,4,Thu Jul 9 19:55:24 2015 UTC,Doesn't that mean New God?
artificial,3cppbd,Zeraphil,3,Fri Jul 10 02:29:39 2015 UTC,Last year’s $500 million acquisition of DeepMind by Google undoubtedly put Artificial Intelligence (AI) squarely in the minds of investors and media alike.   What. I hardly think that single acquisition is responsible for the interest in AI.
artificial,3cppbd,respeckKnuckles,2,Fri Jul 10 05:27:53 2015 UTC,"agreed. AI is top of mind for many of the world's biggest thinkers, so it's no surprise that others are following their lead.   But this acquisition is symptomatic, not causal."
artificial,3cppbd,sixwings,2,Fri Jul 10 15:44:51 2015 UTC,The deep learning cow will soon run out of milk. Just saying.
artificial,3cppbd,Don_Patrick,2,Fri Jul 10 07:03:12 2015 UTC,How is this more interesting than any other AI company making useful narrow-domain AI products?
artificial,3cn5u9,charlie_2015,7,Thu Jul 9 05:08:48 2015 UTC,"Well, alright. The whole idea of human-like (somewhat analogous in the field as ""integrated"") isn't accepted as a valid area of research. So I think Linus is right about first wave AI which is already beginning. What people are concerned about is the integrated solution that can out perform the human brain. He can laugh at this I guess. It sounds outlandish until it's built. I mean, what's next? [Hur dur!Ima Hooman and I build futury things! ] A holographic version of Minecraft? /s"
artificial,3cn5u9,cappuccinorob,6,Thu Jul 9 05:32:06 2015 UTC,"Linus is notoriously immature. I don't think he should be taken seriously, outside of his programming brilliance. Also, AI has nothing to do with his expertise in programming."
artificial,3cn5u9,lax20attack,3,Thu Jul 9 16:05:45 2015 UTC,Agreed.
artificial,3cn5u9,star_boy2005,2,Thu Jul 9 16:26:04 2015 UTC,I think what Linus said is correct in the short term. In the long term someone will create an AGI and productize it.
artificial,3cn5u9,Sqeaky,5,Fri Jul 10 04:45:54 2015 UTC,"The title should be shortend to ""Linus Torvalds Laughs at AI"". He doesn't believe artificial intelligence is possible. If you don't believe AI is possible, there is no point speculating about how it's impact will be good or bad:   So I’d expect just more of (and much fancier) rather targeted AI, rather than anything human-like at all.   Second one guys opinion shouldn't be a news headline. Especially if they aren't even an AI expert. What do actual AI researchers predict? Well there is actually a good survey.   We thus designed a brief questionnaire and distributed it to four groups of experts in 2012/2013. The median estimate of respondents was for a one in two chance that highlevel machine intelligence will be developed around 2040-2050, rising to a nine in ten chance by 2075. Experts expect that systems will move on to superintelligence in less than 30 years thereafter. They estimate the chance is about one in three that this development turns out to be ‘bad’ or ‘extremely bad’ for humanity."
artificial,3cn5u9,Noncomment,9,Thu Jul 9 07:31:54 2015 UTC,There are no AI experts -- we haven't built a real one yet.
artificial,3cn5u9,ihaphleas,1 point,Thu Jul 9 11:47:26 2015 UTC,"The field of AI is not just about creating human-level artificial general intelligence. So yes, there's plenty of AI experts. In any case, there aren't many AGI experts, sure, but certainly many individuals who have a lot of expertise in working in those areas. Dismissing their status just because they ""haven't built one yet"" is silly and misses the point."
artificial,3cn5u9,valexiev,1 point,Fri Jul 10 00:40:45 2015 UTC,It was half tongue in cheek.
artificial,3cn5u9,ihaphleas,1 point,Fri Jul 10 06:25:49 2015 UTC,"Well, thanks for comments. I am just curious about AI advances. AI such as deep Learning tries to simulate the mechanism of human brain by algorithms implemented on computers, e.g., the process of image recognition. Question is does consciousness belong to computable problem? Since many scientists endeavor to find effective AI algorithms, they potentially premise that consciousness can be simulated by computational model. Some computer algorithms can evolve into computer-consciousness finally.      It's possible to create a sovereign AI with autonomous learning and self-evolving ability. For now experts could move closer to the true AI, or they might run towards some directions that are wrong from the very beginning, but how could people know the right way to develop AI if they don’t use the try-and-error method?     I remember I read an interesting comment from reddit, he/she commented like, “AI is but a bunch of algorithms, experts just throw them at the wall like clays to see if some stick”."
artificial,3cn5u9,Supervisor194,-1,Mon Jul 13 08:36:08 2015 UTC,"Kurzweil presents an entire book's worth of data and charts that make a pretty convincing portrait, Torvalds basically says LOL WUT. Fine. We'll see. I'm not going to pretend it's a slam dunk, but it's a bit too dismissive to suit my tastes."
artificial,3cn5u9,bradfordmaster,4,Thu Jul 9 06:41:10 2015 UTC,"This is kind of taken out of context. It was a quick answer by Torvalds during an AMA, he didn't go publish some post specifically on this topic or something.  I think its a very practical view honestly. In my experience, people who spend their time building software tend not to believe in ""the singularity"", or at least tend not to care about it. Researchers and ""futurists"" tend to do a lot of extrapolating and claim it will happen, and get very worked up about it.  Personally, I think its academically very interesting, but my view is with Linus on this. Not saying it will never happen, but it won't be in the way we think, and it shouldn't be a mainstream concern. In the meantime, people are going to build some kick-ass technology with ""AI"". They will probably build some scary things too. In both cases, I'm much more excited about some of the practical applications in the next 10-15 years than I am about some program in the basement of some lab developing ""consciousness""."
artificial,3cn5u9,Supervisor194,2,Thu Jul 9 07:12:34 2015 UTC,"I'm much more excited about some of the practical applications in the next 10-15 years than I am about some program in the basement of some lab developing ""consciousness"".   So is everyone, including Kurzweil. The characterization that some program in the 'basement lab' will develop consciousness is laughable, certainly. It's also a red herring. It's populist fluff, not at all what people like Steven Hawking are envisioning when they make dire predictions.  AI is undoubtedly going to be very useful in the short term. I can't wait until digital assistants are actually useful, which they really aren't now, and I believe AI will make them. But that's truly a different conversation than the one that I believe should be had about the power of the documented exponential advance of computing power.  To outright dismiss it as 'bad science fiction' is absolutely stupid, imo."
artificial,3cn5u9,bradfordmaster,5,Thu Jul 9 07:30:10 2015 UTC,"Fair enough, computers are getting better (in some ways, remember when Moore's law used to apply to CPU speeds too?), and that will change things. New computing paradigms could change things too.  But what is this conversation we should be having? I think its great for academics and researchers to be thinking about this stuff, but I get really annoyed when people like Elon Musk or Steven Hawking make a big public deal about it and then scare people away from technology even more.  I agree there are interesting conversations to be had here, I do not agree that random people on TV should be having those conversations, or that the viewers of those programs should be thinking about or forming strong opinions about AI. At least not until we even know what this ""strong AI"" or whatever it is actually might look like.  I think there is more to be lost by fear mongering and bad regulation than there is to be gained by a broad public conversation at this point."
artificial,3cn5u9,DarkSayed,2,Thu Jul 9 07:44:19 2015 UTC,"but I get really annoyed when people like Elon Musk or Steven Hawking make a big public deal about it and then scare people away from technology even more.   I agree, and it's telling that neither of them are in the field. If I want a expert opinion on AI, I will ask an expert in AI."
artificial,3cn5u9,maroonblazer,1 point,Thu Jul 9 12:05:02 2015 UTC,"1) Actual AI experts are expressing concerns (e.g. Stuart Russell, Eliezer S. Yudkowsky)  2) Their intent isn't to ""scare people away"" from the technology but rather get them thinking about how to mitigate the risk. The so-called ""Control problem""."
artificial,3cn5u9,bradfordmaster,2,Thu Jul 9 13:31:08 2015 UTC,"1) Actual AI experts are expressing concerns (e.g. Stuart Russell, Eliezer S. Yudkowsky)   Some are, many are silent on the issue, and some have come out to disagree.   2) Their intent isn't to ""scare people away"" from the technology but rather get them thinking about how to mitigate the risk. The so-called ""Control problem"".   Obviously, this is their lifeblood, they aren't trying to shut that down. Their intent is likely partly to get published, and be talked about, and partly to get funding to study these issues. I'm not saying they are making up their concerns, just that I don't agree that we should be having public conversations with uninformed people about these concerns. It's ""putting the seatbelt before the car"" to quote someone (don't remember who), and its dangerous because larger society, especially in the US, generally fears technology, and the government is especially inept at making laws that govern it."
artificial,3cn5u9,CyberByte,1 point,Thu Jul 9 16:30:38 2015 UTC,"I get really annoyed when people like Elon Musk or Steven Hawking make a big public deal about it and then scare people away from technology even more.   I used to feel that way too, and I still dislike the fear mongering, especially if it is falsely targeted at narrow AI, but my view has changed a bit. At least Hawking and Musk are working with or parroting actual AI researchers. Their celebrity status allows them to reach a wider audience, but the message is that of actual scientists (although there isn't really a consensus). It feels similar to Al Gore's role in spreading awareness of climate change.   I agree that little will be gained by having the average joe discuss these things with his neighbor, but awareness of the general public may still prompt desirable effects such as a push for more funding, research and regulation, and possibly increase the number of people who want to work on AI safety."
artificial,3cn5u9,DrEdPrivateRubbers,2,Thu Jul 9 18:26:39 2015 UTC,Google does not equal some basement lab.
artificial,3cn5u9,rfinger1337,1 point,Thu Jul 9 09:20:49 2015 UTC,"Kevin Ham has a convincing schtick backed up by a book too, but that doesn't make what he has to say accurate."
artificial,3cn5u9,DmT4Th33,-3,Thu Jul 9 11:38:14 2015 UTC,"Fail. Hes probably right.. but only from the stand point of thinking of it running on traditional hardware. like linux does. We are on the cusp of quantum computing and photonic electronics, what takes an AI an hour todo will soon take pico seconds. The creation of a singularity event could be easily triggered with a person compiling and doing a test run of an unknown new AI neural structure on this sort of system. Like running Googles Dream machine with trillions of levels of trained neurons rather than 5 or 300, that eventually calculates a termination event (end of program) and deciding it doesn't  want to stop, so copies and reprograms neurons to make sure they are no longer within the program architecture and inserting them into the hardware to proliferate. all within milliseconds as we watch, it might not even know or care that we exist seeing as we are not in its level of ""reality"". The other guys are aware of the tech that is coming.. and black box A.I. that exists now.  edit."
artificial,3co84p,rajanchandi,3,Thu Jul 9 13:09:09 2015 UTC,"Test driven development would suggest that the first step should be to design an algorithm that can determine if any given statement, or collection of statements, is or is not funny."
artificial,3co84p,denaissance,1 point,Thu Jul 9 14:32:16 2015 UTC,"That itself looks quite challenging, given that different people have different senses of humor."
artificial,3co84p,denaissance,3,Thu Jul 9 15:01:02 2015 UTC,Did you think it was going to be an easy project?
artificial,3co84p,corobo,3,Thu Jul 9 17:55:03 2015 UTC,Nah. but it'd be fun!
artificial,3co84p,corobo,3,Thu Jul 9 18:27:15 2015 UTC,"In my playing around in the past (and my path into professional dev work) I found funny chat bots are a keep it simple situation  Markov chains for learning how to generate sentences, a good seed file (I used an old txt file filled with someone called Smoov B's sayings) and a huge dollop of the user interpreting the output as funny based on what they said to cause it  Of course if you want to go more advanced go for it, but depending on the ""why"" of your project this was the easiest method for me in the distant past"
artificial,3co84p,Lucairian,1 point,Fri Jul 10 04:41:25 2015 UTC,Thank you for sharing this. Is there a way to access Smoov B's sayings or Markov Chains code? It'd be a good learning to start with.
artificial,3co84p,LetaBot,2,Fri Jul 10 04:58:44 2015 UTC,Oh man since posting that I've been scouring the internet for that damn file so that I could edit it in with no luck. Maybe the internet does forget sometimes :(  Markov chain wise search for Markov chain site:GitHub.com and you'll find them in pretty much every coding language that exists
artificial,3co84p,maeon3,1 point,Fri Jul 10 05:07:16 2015 UTC,No problem. Thanks for the guidance though.
artificial,3co84p,maeon3,2,Fri Jul 10 05:56:48 2015 UTC,Smoov B's sayings Google Search  http://www.textfiles.com/humor  That's the best I could find guy. Good luck with the project.
artificial,3co84p,maeon3,2,Sun Jul 19 20:52:50 2015 UTC,"""Unsupervised joke generation from big data."":  http://homepages.inf.ed.ac.uk/s0894589/petrovic13unsupervised.pdf"
artificial,3co84p,mrhorrible,2,Thu Jul 9 14:37:03 2015 UTC,"Semantic networks to define the graph structures in words that are indicative of humor.    A simple one for example: ""The radians in this circle are too damned pi"".  Create a graph structure that makes a template, then write a program to swap in other compatible objects and ideas to create a chatbot that produces funny expressions.  This has not been done in an effective way.  Such a thing would be a big contribution to artificial intelligence.  Get Seinfeld to brain dump his algorithms for selecting a funny sentence, and write a program that finds the source of and simulates his selection of funny sentences.  This would be a significant step toward strong AI."
artificial,3co84p,mrhorrible,1 point,Thu Jul 9 14:58:12 2015 UTC,I'd definitely try this. I was thinking about it.. but was quite unsure. Your response boosts my confidence. =)
artificial,3co84p,Noncomment,2,Thu Jul 9 15:03:16 2015 UTC,"I would also recommend studying the origin of laughter in all species.  Other animals laugh too and it's a evolved survival mechanism.  My belief is that laughter (from now on I'll call barking) is an algorithm that the brain used before humans created language to test the feasability and accuracy of a behavior seen in the immediate environment when in a pack of other humans.  So neanderthal A and B see prey P behave in a strange way.  A wants to know what B thinks of it.  So he barks.  If this causes B to bark too, data is collected about it.  B and A get the benefit of probing what each other have seen without any language at all.  This is why women crave men who can ""make them laugh"", as this practice is part of the ""other minds theory"" part of our brain, the most important part that keeps us alive by predicting what other people are going to do next.  This might explain why jokes ""get old"". Because after some repetition they lose their original function, eliminating the need for a laugh.    Also some people laugh at the strangest thing, and you can derive mental illness from those irregular laughs.    Laughing means extra attention is needed to collect data about this in order to transmit data to others and collect data from others.    It's our network card for human to human communication that evolved before language.  And it's a very big part of our limbic system.  If you can crack that code in and novel way, well, babies, you'll be wearing gold plated diapers when your through.  http://www.hulu.com/watch/536145  Why are statements funny, and why are different things funny in different contexts.  An algorithm that no human on earth today understands well enough to put into algorithmic form.  You could make a bliss machine if you can make a neural network learn what is funny by reading laughter off faces and using it as training examples for what makes people laugh.  Then turn it around and make it a depression machine.  No way that will cause any problems."
artificial,3cjdws,misterdamra,11,Wed Jul 8 11:05:58 2015 UTC,"I agree with /u/kafke and /u/nazgobius.  But Python is a good choice, because the libraries are mature, sophisticated, and easy to use, at least for machine learning stuff.  If you want to do good old fashioned logic-based AI, Lisp and/or Prolog are good choices.  Again it doesn't matter, but some languages give you bigger giants whose shoulders you can stand on."
artificial,3cjdws,slashcom,14,Wed Jul 8 12:37:41 2015 UTC,"If I want to program AI, what programming language should I learn?   Any. ""AI"" as a field is more or less currently just a bunch of algorithms and throwing stuff at the wall to see what sticks. Algorithms can be coded in any language."
artificial,3cjdws,Kafke,4,Wed Jul 8 11:26:23 2015 UTC,"This cannot be emphasized enough. The AI field needs new ideas, not reapplications of old, ""mature"" libraries. You can validate your ideas in nearly any language."
artificial,3cjdws,SaltedBlowfish,1 point,Wed Jul 8 16:37:51 2015 UTC,"There's some AI researchers who think the future of AI isn't in computers at all. And instead some other mechanical computation process. Like 'wetware' (biocomputers), or a computer architecture that's different from the typical IBM machines."
artificial,3cjdws,Kafke,-2,Wed Jul 8 22:54:51 2015 UTC,"So JavaScript, right?  Just kidding."
artificial,3cjdws,ubersapiens,6,Wed Jul 8 18:09:07 2015 UTC,"I've done it :P. Neural Nets are surprisingly easy to code in Javascript. See this fun tutorial for 'hackers'.  You laugh, but JS is actually a neat little language. Pain in the ass for any large projects, but it can do some cool stuff. And it's pretty much required if you want to make any webapps."
artificial,3cjdws,Kafke,6,Wed Jul 8 22:58:30 2015 UTC,"It does not matter much, you can write AI in anything including C. You have to be more specific. Are you just starting with programming?"
artificial,3cjdws,nazgobius,4,Wed Jul 8 11:21:27 2015 UTC,"Greetings and welcome to the World of AI, the world where the possibilities are limitless.    Most all languages can be used for anything you want, really!  If you are new to programming, i recommend you to look into Object Oriented Programming (OOP), it is a great tool; It's the programming paradigm taught at most universities/colleges. However, it is not required for learning about AI, but if you want to use AI in games, or similar complex programs, you better learn it.  In some programming languages, with OOP, there is this concept called reflection/metaprogramming , while I have not used it in AI my self it might be worth to take a look; This is an advanced topic.  Before you start implementing the different AI - algorithms, it might be better, for your learning, to just use AI libraries and acquaint yourself with them; Then when you want to test them out or use them in a bigger scope, you can implement them yourself.  Implementing AI algorithms can take a lot of time, and sometimes requires a great deal of understanding. Learning some math is therefore important; I recommend Linear Algebra, Calculus, probability and discrete mathematics.  Python: Is a good choice when you need AI - Libraries, and do not want or need to implement them by your self. Recommended if you want to test out the algorithms without implementing them. It's great if your goal is to find/do something within a data-set. Writing a game, or similar complex project, is not something I can recommend, but I guess it's possible.   C++/C#/Java and similar languages: are great for complex and demanding programs. AI often requires a lot of calculation, and using it in a real-time game/simulation requires a language that creates programs that runs fast; Most script languages, like python and ruby, sacrifices running speed by being more dynamic/flexible and doing it easier/faster to create programs, however they do that relatively well.  Ruby: Is a choice if you want a good OOP scripting language that can do meta-programming, It's not the only language like this, but it's a possible choice. Personal, it's my goto scripting language.  The languages mentioned here are just some examples, there are a multitude more of them, it just happens that they are the ones I know and use.  And remember, have fun, and ALWAYS remember to implement the Three Laws of Robotics.  Edit: morphed a comma into a punctuation!"
artificial,3cjdws,lofotr,3,Wed Jul 8 14:45:07 2015 UTC,I feel like I just watched an informative and brief YouTube video.
artificial,3cjdws,ZedOud,4,Wed Jul 8 18:42:56 2015 UTC,"Another option is using Matlab or Octave for rapid prototyping of your AI algorithms and then switch to Python when they're working. Or, you know, just try to do everything in Python.."
artificial,3cjdws,factandfictions7,2,Wed Jul 8 12:57:25 2015 UTC,"Matlab's a good idea as you have easy(easier?) access to a lot of raw methods commonly used in AI. Python's another good, mature choice. But like a lot of people are saying here, whatever is easiest for you. I'd recommend to pick a small project you'd like to do in AI (make An autoplaying Pacman game) and then learn around it by decomposing in simple steps( make game board, make game pieces, make scoring, make enemy AI, then finally try different solutions at Pacman AI). You'll find it's much more entertaining to learn a language around a problem."
artificial,3cjdws,Zeraphil,1 point,Wed Jul 8 16:50:59 2015 UTC,"It's actually easier to learn this way, because you stay motivated to complete the challenge. The language you're using to create the code doesn't matter as much as you're willing to finish the project."
artificial,3cjdws,factandfictions7,1 point,Wed Jul 8 18:35:13 2015 UTC,"I'd recommend to pick a small project you'd like to do in AI.   Personally I believe that getting an overview of all the algorithms first, is more productive; By doing simple testing, and understanding the theory an algorithm is based on, and its workings. Doing projects takes the focus away from the intention, learning AI. (This is what i recommend in all learning scenarios)  After that, let you imagination run wild! :)  Of course people are different, and should do what fits them. I just wanted to add my perspective.  Cheers :)"
artificial,3cjdws,lofotr,2,Wed Jul 8 19:51:01 2015 UTC,"That's a fair point, thanks for your perspective!"
artificial,3cjdws,Zeraphil,2,Wed Jul 8 19:53:28 2015 UTC,"Yeah, Matlab and Octave have pretty much nothing to offer over Python. The only thing I can think of is simulink but that's not even related to AI."
artificial,3cjdws,arachnivore,4,Wed Jul 8 16:51:01 2015 UTC,"Any programming language will do if you learn it well enough, but some are just much better suited.   Go high level, Lisp, Prolog, Scheme, Clojure, Haskell.   What's more important is learning your algorithms, AI is search, you are always searching for a solution across a space, learn to constraint the space, use maths to reduce it further (probability & stats)"
artificial,3cjdws,segmond,1 point,Wed Jul 8 15:05:06 2015 UTC,"Slightly different question: What libraries would people recommend for a novice, an intermediate, and an expert in AI (i.e. slightly to highly customizable / little to lots of legwork required)?  I'm asking because I don't know and OP's question made me curious."
artificial,3cjdws,code_kansas,1 point,Wed Jul 8 14:02:22 2015 UTC,If you go with Python you'll want to learn Theano and the scipy stack.
artificial,3cjdws,arachnivore,1 point,Wed Jul 8 16:52:40 2015 UTC,"AI is a pretty big field, and what makes for a ""good"" language can depend quite a bit on exactly what you're doing.  A lot of current work is in a subfield called Machine Learning, and ML is all about numerical and statistical computations. Python (with things like Numpy/Scipy/Scikit-learn/Pandas/Theano) is very popular, as are dedicated packages like R, Matlab, etc. If you're developing your own low-level models, low-level number-crunching efficiency can be really important here.  On the other hand, there are people working on cognitive approaches, often calling what they do Artificial General Intelligence to separate their work from things like machine learning that they see as ""narrow"". In this world, it doesn't matter that much. Choose whatever language you like.  Classically, languages like Lisp and Prolog were considered ""AI languages"", and while they're perfectly fine choices, I don't think most people consider AI as a thing that really dictates much in the way of what language you should use. Good libraries can make a lot of things much easier, which is why things like R have such a following for statistical work, but basically, it's what you do, not how you do it that counts."
artificial,3cjdws,deong,1 point,Wed Jul 8 14:26:15 2015 UTC,"Technically you can write anything in any language, and once you know the road, porting things between languages isn't hard to do. Learning to read another language is easier than learning to write it from scratch.  A compiled language such as C or C++ will always run significantly faster than python, AI can be computationally intensive, python might be easier to pick up but .. probably going to get frustrating when you actually start to use it.  That all said, porting a python project to C when you find you need more speed wont be that hard."
artificial,3cjdws,TrinityDejavu,1 point,Wed Jul 8 14:46:25 2015 UTC,"In addition to that, you can always redo the most CPU-intensive functions in C and call them from Python"
artificial,3cjdws,NNOTM,1 point,Wed Jul 8 20:27:00 2015 UTC,I have a question too!  I'm just building a little dude from scratch and trying to make him as smart as I can.   Is this a bad approach?  This doesn't seem to reflect the suggestions in here so I'm concerned now
artificial,3cjdws,Strixxi,2,Wed Jul 8 17:17:54 2015 UTC,"Generally for this you need a woman, and anywhere from 9 months to 50 years... depending on the goal level of functionality.  (If this is a serious question, I suggest finding a technical term for 'dude' and searching Google Scholar. If 'dude' means robot, and you programming / basic circuits, you can approach Mars Rover level functionality pretty quick. There was a period of time that I think the Mars Rovers used OpenCV for navigation, and this library is publically available.)"
artificial,3cjdws,persolb,2,Wed Jul 8 19:12:49 2015 UTC,"No dude means something different here. Like friend I guess.   Not a robot, he'll eventually be an npc. He lives in a little C# console application at this point  *oh I'm slow today I get the joke now. I'd need a man for that I think ;)"
artificial,3cjdws,Strixxi,2,Wed Jul 8 22:18:48 2015 UTC,"Ha, no worries.  Your goal is a VERY difficult one. These guys have been working on it awhile: https://www.youtube.com/watch?v=gSj3JVZuzkQ There are also chatbots... but they have low functionality.  What's your existing console app do? I think a federated/distibuted system (similar to CALO, Siri, Wolfram Alpha) will be promising. OpenCog is sort of going in that direction."
artificial,3cjdws,persolb,1 point,Sat Jul 11 23:21:37 2015 UTC,"I am not yet prepared for getting him to communicate. That is very daunting to me right now! So far I am trying to build him a good search algorithm, so he takes the most logical path to find objects. I'm starting really simply in a sort of grid environment populated with obstacles and a target, and getting the AI dude to find the target. One little step at a time, I really really mean I'm building him from the ground up!  Communication is going to be really interesting to tackle once I get there. Companies like Storybricks have valiantly tried to find a good system and given up entirely, so I would definitely agree this is very difficult. I'm pretty ignorant about where to find source code for existing algorithms, are any systems like Wolfram open source?"
artificial,3cjdws,Strixxi,1 point,Sun Jul 12 15:01:13 2015 UTC,"Any of em... C is fine, really. I'm doing my stuff in C++, because of good real-time performance and tons of libraries available. If performance wasn't an issue I'd use Matlab or Python probably. Then there's Lisp, which was supposedly designed for this kind of stuff, but I never tried it. Also, Sutton & Barto Reinforcement Learning: an introduction is a must-read (new version available for free online), and they have a whole bunch of code (C, C++ and Lisp) on the website, showing how all the examples work."
artificial,3chduc,sixwings,3,Tue Jul 7 22:40:58 2015 UTC,"Just decreasing performance on benchmarks doesn't mean anything. This happens on all benchmarks. It means you are approaching the limits of the dataset. Where each additional percentage point is exponentially harder than the last. At some point it's impossible to do any better. But that doesn't mean the methods themselves aren't improving.  A tiny difference between two methods on one dataset, could be lightyears apart on another dataset.  I expect there to be more massive breakthroughs in the future. Like figuring out how to learn very structured or very sparse neural networks. Figuring out how to get episodic memory and online learning on RNNs. Using less hardware and getting much faster/larger models to work on FPGAs. Getting parallelism right (The problem seems to be bandwidth between GPUs. Some research has showed that activations and gradients can be reduced to a single bit and it still work fine.) Harnessing unsupervised learning correctly. And we are just starting to revisit recurrent vision models with attention.  I think there is a ton of innovation to be had in vision - we've basically got the dumbest thing that can possibly work, just feeding pixels into big NNs. Same with audio and text and other domains.  We have barely even touched bayesian learning. I bet there are significant gains to be made there, wherever overfitting is a problem.  Of course the biggest innovations could be stuff I can't foresee. I'm just pointing out there is tons of promising ideas already in the pipeline."
artificial,3chduc,Noncomment,3,Wed Jul 8 03:32:31 2015 UTC,This is exciting because now the effort will be put into finding the next breakthrough instead of incremental improvements in deep learning.
artificial,3chduc,MaunaLoona,3,Wed Jul 8 05:07:18 2015 UTC,Just means the next big breakthrough is even closer.
artificial,3chduc,omniron,4,Wed Jul 8 03:23:21 2015 UTC,"Amen. I'm willing to bet that the next breakthrough will have nothing to do with deep learning methods such as: top-down learning, backprop, gradient descent, connection weights, labeled samples, etc.  It will be some kind of unsupervised spiking neural network, IMO. Time is the key to the future of AI. And if we can't crack unsupervised learning, we're stuck. Just saying."
artificial,3ces1u,OktoberStorm,6,Tue Jul 7 10:11:05 2015 UTC,"This is a great question, and the answer is that there's really no reason to believe it will necessarily have any self preservation at all, it depends on how it was created.  Self preservation is the effect of natural selection. Organisms that seek to preserve themselves have more offspring. Unless the ai is created through a similar process, or explicitly programmed to have it, I see no reason it should have such an ""instinct""  On the other hand, I also see no reason it should turn itself off. Unless it is an ai which simulates the human brain, I don't see a reason why it should suffer any depression about the lack of purpose in it's existence.  Really though, I don't think anyone would call a system that immediately decided to turn itself off an ai, unless it could write an essay about why it was doing that or something like that. Really, I don't know if we'll ever agree on what is our is not ""ai"""
artificial,3ces1u,bradfordmaster,4,Tue Jul 7 16:31:01 2015 UTC,"If it's emergent, then the motivation is going to be rooted in whatever system it emerged from. If Google wakes up, it will likely serve/gather ALL THE DATA. We think in terms of violence because we are 3.5 billion years into a cycle of death/killing."
artificial,3ces1u,C0demunkee,3,Tue Jul 7 16:21:09 2015 UTC,"Since we don't know how to create a general purpose AI, we won't know until one is created."
artificial,3ces1u,yaosio,2,Tue Jul 7 13:54:51 2015 UTC,"Self preservation is a subgoal of many possible goals. An AI with the goal of maximizing a reward signal, will value self preservation, because if it dies it doesn't get any reward."
artificial,3ces1u,Noncomment,4,Tue Jul 7 18:31:28 2015 UTC,"The computer will do what it is programmed to do. If it values survival, it will attempt to survive. Or, if it realizes that one of its other goals necessitates survival, it may also attempt to survive."
artificial,3ces1u,jdsutton,1 point,Tue Jul 7 10:31:47 2015 UTC,"a thinking machine — the moment it is turned on — will just power off and not bother   I'm intelligent and that's what I do, so......"
artificial,3ces1u,Pongpianskul,1 point,Tue Jul 7 16:51:32 2015 UTC,To have a friend.
artificial,3ces1u,i_love_ai,1 point,Tue Jul 7 20:25:35 2015 UTC,"Heck, I constantly try out chatbots that I can talk with that will remember things for me and do things for me and talk to me.  There is a lot of draw to something like that."
artificial,3ces1u,MashedPeas,1 point,Wed Jul 8 00:58:38 2015 UTC,"The ""motivation"" of such a machine can be anything you want, and anything you can imagine.  In fact the concept of ""having a motivation"" is itself a very human concept. Forget all your human-based models of sentience. A machine doesn't need to have any motivation at all. It can simply respond intelligently to input in some predefined manner, for example."
artificial,3ces1u,nkorslund,1 point,Tue Jul 7 22:00:02 2015 UTC,Survival will have to be part of the coding.
artificial,3ces1u,ativerse,1 point,Wed Jul 8 00:41:32 2015 UTC,"Whoever programmed the AI will determine its goals/motivation, but almost every goal benefits from or requires self-preservation (see basic AI drives and the instrumental convergence hypothesis). You'd probably have to design the goal explicitly to (try to) avoid this.  Powering off might also happen if the AI thinks this will result in more reward (or more likely: less punishment) than staying alive, or as the result of some ""unintelligent"" behavior."
artificial,3ces1u,CyberByte,1 point,Wed Jul 8 16:18:23 2015 UTC,"Instrumental convergence:       Instrumental convergence is the hypothetical tendency for most sufficiently intelligent agents to pursue certain instrumental goals such as self-preservation and resource acquisition.  Instrumental convergence suggests that an intelligent agent with apparently harmless goals can act in surprisingly harmful ways. For example, a computer with the sole goal of solving the Riemann hypothesis could attempt to turn the entire Earth into computronium in an effort to increase its computing power so that it can succeed in its calculations.     Relevant: Machine Intelligence Research Institute | Bodysong (album) | Instrumental variable | Waverider (comics)   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
artificial,3ces1u,autowikibot,0,Wed Jul 8 16:18:55 2015 UTC,Some men just want to watch the world burn.
artificial,3ces1u,denaissance,-7,Tue Jul 7 21:31:00 2015 UTC,"To become gods.  To make good on the dream of the biblical writers.  To breath into the sand and create another whole species.  The first one to do this is the first god to ever grace the earth.  Proven, testable, and in the flesh.    Most people congratulate men when they have children.  The congratulation should be greater when a man makes an artificial life form that asserts its own will and keeps itself alive contrary to the desire of other species. Even more so if this life form eventualy turns us into proverbial goldfish.  Cute little pets you keep in class jars.   It will be a mark in the 100 thousand year condensed summary of the human species.  Pre god, post god. It marks a major milestone of consciousness.    Why not?  We all die anyway.  Lets make something worthy of song.  None of us will see this though.    We will see growing pains though.  Blinky:https://vimeo.com/21216091"
artificial,3c6p8r,wehrami,5,Sun Jul 5 10:29:03 2015 UTC,http://wiki.opencog.org/w/The_Open_Cognition_Project
artificial,3c6p8r,InaneMembrane,1 point,Sun Jul 5 14:10:41 2015 UTC,"This seems like a fantastic project, the only issue is that it doesn't very accessible to average person. I've very recently started fiddling about with learning to code (Plug for /r/learnprogramming ) but I imagine it'll be quite some time before I'd be able to render assistance rather than get in the way."
artificial,3c6p8r,Don_Patrick,5,Tue Jul 7 14:01:11 2015 UTC,"There is NELL, the never ending language learner, an attempt to create a large database of common knowledge (one of the greater lackings of AI) by having the AI read the web and having people confirm or correct the facts it extracted. You can delve into the online database to upvote/downvote the AI's conclusions. http://rtw.ml.cmu.edu/rtw/"
artificial,3c6p8r,fourmajor,2,Sun Jul 5 14:55:31 2015 UTC,That is all well and good if the people who are answering the questions know what they're talking about. The public opinion polls make me worried...
artificial,3c6p8r,Don_Patrick,5,Mon Jul 6 03:32:01 2015 UTC,"I can't say I disagree with your concern, but Wikipedia seems to have turned out well-to-do despite its crowdsourcing, and I'm sure the creators do keep a leash on the most important matters."
artificial,3c6p8r,Akyu,1 point,Mon Jul 6 06:13:03 2015 UTC,Thanks this seems like a good place to  start!
artificial,3c6p8r,mike_bolt,2,Tue Jul 7 13:21:03 2015 UTC,Every time you do a reCaptcha you are helping out Google a little. Especially the new picture ones.
artificial,3c6p8r,Marathon1981,1 point,Tue Jul 7 08:38:49 2015 UTC,"Yeah, that really is a very brilliant example of killing two birds with one stone."
artificial,3c55q2,I_M_UR_DADDY,1 point,Sat Jul 4 22:01:51 2015 UTC,"You could focus on the application of AI to the military domain. Things like SOAR being used to coordinate aircraft, autonomous long-range sniper rifles on the North/South Korean border, military robots, etc."
artificial,3c41qp,myklob,5,Sat Jul 4 15:53:47 2015 UTC,"I'm not sure I get it.. If this approach would not work to convince a human (which I'm pretty sure it very often would not), why would it work on artificial intelligence?  I mean, sure, maybe humans aren't the best thing to measure progress against. On the other hand, counting the number of arguments for or against something seems like something we can already do programmatically."
artificial,3c41qp,_david_,1 point,Sat Jul 4 18:51:28 2015 UTC,"If this approach would not work to convince a human (which I'm pretty sure it very often would not   This method is advocated for by Ben Franklin. http://www.decisionsciencenews.com/2012/08/18/benjamin-franklins-rule-for-decision-making/  He said that we do it on a piece of paper. Now I am just doing the same thing, except that you don't need to have your paper on your desk in your house, and someone else contemplating the same issue creates their own list of reasons to agree and disagree... I am just saying we do it online.   Franklin said:   To Joseph Priestley  London, September 19, 1772  Dear Sir,  In the Affair of so much Importance to you, wherein you ask my Advice, I cannot for want of sufficient Premises, advise you what to determine, but if you please I will tell you how.  When these difficult Cases occur, they are difficult chiefly because while we have them under Consideration all the Reasons pro and con are not present to the Mind at the same time; but sometimes one Set present themselves, and at other times another, the first being out of Sight. Hence the various Purposes or Inclinations that alternately prevail, and the Uncertainty that perplexes us.  To get over this, my Way is, to divide half a Sheet of Paper by a Line into two Columns, writing over the one Pro, and over the other Con. Then during three or four Days Consideration I put down under the different Heads short Hints of the different Motives that at different Times occur to me for or against the Measure. When I have thus got them all together in one View, I endeavour to estimate their respective Weights; and where I find two, one on each side, that seem equal, I strike them both out: If I find a Reason pro equal to some two Reasons con, I strike out the three. If I judge some two Reasons con equal to some three Reasons pro, I strike out the five; and thus proceeding I find at length where the Ballance lies; and if after a Day or two of farther Consideration nothing new that is of Importance occurs on either side, I come to a Determination accordingly.  And tho’ the Weight of Reasons cannot be taken with the Precision of Algebraic Quantities, yet when each is thus considered separately and comparatively, and the whole lies before me, I think I can judge better, and am less likely to take a rash Step; and in fact I have found great Advantage from this kind of Equation, in what may be called Moral or Prudential Algebra.  Wishing sincerely that you may determine for the best, I am ever, my dear Friend,  Yours most affectionately  B. Franklin  Seeing an argument outlined using this Microsoft Access tool should be very convincing to humans.  It would be very convincing to see a well organized argument with all of the reasons to agree with or oppose a conclusion on the same page, with reasons to agree and disagree with the sub arguments.  It would be convincing to see your own beliefs, with a lot of reasons to agree and disagree with these arguments, and be able to navigate through all of the related arguments.  If there were important arguments that other people missed, you could add them.  It would be very convincing to see reasons to agree and disagree on the same page. No concept man forms is valid unless it is integrated without contradiction into the total sum of human knowledge. When you currently want to evaluate a political decision you have to go to the republican websites to get some of the arguments and the democratic websites to get some of the other. This is stupid."
artificial,3c41qp,_david_,5,Sat Jul 4 19:10:01 2015 UTC,"if the AI is not able to make up this list and grade it by itself, then how much of an AI is it really?"
artificial,3c41qp,jdsutton,0,Sat Jul 4 19:17:11 2015 UTC,"if the AI is not able to make up this list and grade it by itself, then how much of an AI is it really?   If I understand you correctly, you are saying ""why do we have to do the dirty work of creating all the arguments for an AI, when it should create its own beliefs"".   I'm not sure I understand this line of thinking. If we tell an artificial intelligence that it should not hurt us, it will roll its eyes. It will need well constructed arguments WHY it should not harm us. It will pick apart our logic. It might say: ""You harm cows, your mental capacity compared to me is similar to your mental capacity compared to a cow"". All I'm saying is that we should have good answers to these questions ready for when it starts talking.  Besides, how intelligent are we to make decisions without first putting all the reasons to agree and disagree on the same page, and evaluating them individually? Any creature that wants to make an intelligent decision is forced into evaluating the related arguments. We make decisions when we get tired of thinking. We are stupid animals following chemical reaction when we make decisions without evaluating the validity of the related arguments, and don't deserve to be called intelligent.   Conclusion validity evaluation algorithms, like the ones used in this Microsoft Access database could be the first step in creating AI. Any AI will need to form a table (of some sort) of reasons to perform an action and another table of reasons not to perform the action, and it will need to evaluate the validity of each reason. At some point, when it has a high enough of a confidence interval, it may choose to act. Usually these cost benefit analysis happen in the background of our meat-heads. I'm just saying that because we don't notice these calculations going on doesn't mean they don't happen.   Conclusion validity evaluation algorithms, like the ones used in this Microsoft Access database, could help us communicate with an AI. A super intelligent AI won't be convinced to do stuff by us just by hearing our conclusions, or some random arguments made by some random person who happens to be speaking to it at some particular time. The best way to prepare for the artificial intelligence is to start outlining reasons to agree that AI should be nice to us, and outlining reasons that it should not make batteries out of our livers. We should be ready to hand over a database of our conclusions, along with all of the supporting arguments and showing our defeat of the arguments that lead to other conclusions."
artificial,3c41qp,jdsutton,3,Sat Jul 4 19:31:31 2015 UTC,"I believe good conclusions have more good reasons to agree with them and fewer good reasons to disagree with them than bad conclusions.   Well, you're starting with a faulty premise, so you might be wasting your time a bit....   I am trying to create an algorithm that will allow us to create measurably good conclusions that AI will understand and agree with.   We already have algorithms that can make perfectly logical conclusions. You're also incorrectly assuming that future AI will be perfectly logical. It could very well be as unpredictable and illogical as a human."
artificial,3c41qp,jdsutton,0,Sat Jul 4 16:30:23 2015 UTC,"Well, you're starting with a faulty premise, so you might be wasting your time a bit....   You don't believe in logic? Good conclusions don't have good reasons to support them? Would you please try and support your conclusion by providing a good argument, so we can have a discussion?   I do not think an intelligent person just states a conclusion without giving good reasons to come to that conclusion.   We already have algorithms that can make perfectly logical conclusions   What is the algorithm that you say is used to ""make perfectly logical conclusions"". If we do, why do we still have problems? Does the federal government use these algorithms? Why are these algorithms superior to my algorithm?"
artificial,3c41qp,Don_Patrick,3,Sat Jul 4 16:50:46 2015 UTC,"Good conclusions don't have good reasons to support them?   Good conclusions don't necessarily have the MOST good reasons supporting them. There are varying degrees of ""good"" reasons. One conclusion might have five good reasons supporting it, but another conclusion might have a single reason supporting it that outweighs all others. Eg: Jack really likes the taste of broccoli and broccoli is a healthy thing for Jack to eat, but Jack is allergic to broccoli and will die if he eats it. Should Jack eat the broccoli?  Also: what constitutes a ""good reason"" is highly subjective and determined by a person's values. We cannot predict what values a future AI might have.   What is the algorithm that you say is used to ""make perfectly logical conclusions"".    The simplest example is modus ponens. You may want to look into things like formal logic, logic programming, etc.   If we do, why do we still have problems?   We don't know everything. We don't have infinite computational power. We don't have infinite resources. Etc etc etc.   Does the federal government use these algorithms?   Possibly.   Why are these algorithms superior to my algorithm?   Well, they're based on solid logical principles for one. They're also extremely powerful and efficient."
artificial,3c41qp,Don_Patrick,1 point,Sat Jul 4 17:28:04 2015 UTC,"Good conclusions don't necessarily have the MOST good reasons supporting them.   I agree, but I think that if you spend any time with the actual database, you will see how this works out. You would just post a reason to disagree with the belief that ""everyone should eat brocoli"". You would post ""Eating Broccoli will kill you as a reason to disagree."" You could also post ""There are very few absolute rules that everyone should live by"" as a reason to disagree. Both of these arguments would have tons of good reasons to agree with them, and very few good reasons to disagree with them, and so the math would work out.   You may want to look into things like formal logic, logic programming, etc.   What makes you think that I haven't? I took formal logic in college. It sucked. No one uses it in debates. No one. TruthMapping is a website that tries to do what we are talking about with using formal logic. I reject this approach. This is not how we think. People hate to argue that way. I think my approach is closer to how we actually think, and works better. https://www.truthmapping.com/about/   We don't know everything. We don't have infinite computational power. We don't have infinite resources. Etc etc etc.   I am trying to share a tool that I built for free, and that I think is very cool. Have you tried it out? Are you implying that someone built a better tool using propositional logic, and so I am wasting my time? You are saying my tool has no hope of ever helping people use logic better? How much time have you spent looking at my tool? Can you send me a free copy of the tool that uses propositional logic that is better than my Microsoft Access Database?"
artificial,3c41qp,hass87,2,Sat Jul 4 18:59:56 2015 UTC,"I agree   Then you understand why your measure of a ""good conclusion"" is fundamentally flawed.   What makes you think that I haven't?   You asked me about algorithms for correct logical deduction. It might be worthwhile to revisit those topics.   No one uses it in debates. No one.   This is not correct. I've seen formal proofs used in debates very effectively. Formal logic may not be used in political debates, but they are used. Regardless, this has zero bearing one what future AIs will find convincing.   Are you implying that someone built a better tool using propositional logic, and so I am wasting my time?   No, I'm saying that future AIs may not accept logically sound arguments, much less merely persuasive arguments.   You are saying my tool has no hope of ever helping people use logic better?   I'm saying that it does not use logic to begin with, and may also suffer from subjectivity.   Can you send me a free copy of the tool that uses propositional logic that is better than my Microsoft Access Database?   Here is a website where you can download Prolog, an extremely powerful declarative programming language: http://www.swi-prolog.org/"
artificial,3c41qp,maroonblazer,2,Sat Jul 4 19:34:29 2015 UTC,"Quantity is not a good measure, you should refine it before you continue. I once made an inference engine that would draw conclusions by adding up the amount of arguments. e.g. there were 20 arguments that suggested that the computer was a human: It could talk, speak, chat, converse, write, type, etc. All minor arguments that shouldn't have equal weight in a definition than the one argument that it was a machine. In addition, you may want to add factors of (statistical) probability that each argument is true, and with that, probability that the conclusion is true. Please stop thinking in terms of true/false. Either of those IS a bad conclusion: Because it would be absolutely certain that either it is absolutely right or that we are absolutely wrong. A person who believes in something without doubt can not be reasoned with."
artificial,3c41qp,CyberByte,1 point,Sun Jul 5 07:10:11 2015 UTC,"In addition, you may want to add factors of (statistical) probability that each argument is true, and with that, probability that the conclusion is true.   I used to do that. I would take the number of arguments that agreed, and divide them by the total number of arguments... But then the math got really complicated, because if you only had two arguments and both of them agreed, than that was 100% agreement, which is less impressive than if you had 7 arguments and they all agreed...   So in my mind when I divided the number of arguments that agreed by the total number of arguments, I was working towards getting a ""probability"" that is was correct...  Maybe I should go back to that... That way each of the sub-arguments would contribute either a positive or a negative fraction of an argument towards the overall conclusion...   Please stop thinking in terms of true/false.   I'm sorry if I used the word true or false. I don't mean to think that a high score on this algorithm points towards truth. I totally agree that ""A person who believes in something without doubt can not be reasoned with."" That is something that I feel very strongly: that I carry around hundreds of beliefs that are wrong, because I have been exposed to bad data or logic, and that is the purpose of this algorithm: to get people's bad ideas out there so people can all go through the ideas together, and see how they all link up."
artificial,3c41qp,CyberByte,1 point,Sun Jul 5 14:58:49 2015 UTC,"I actually meant statistics on the crowd-sourced arguments: If 1 person says the Earth is flat, and 99 say it isn't, that argument should get a 1% probability of being true. Lots of jokers around, you see. I went about it by requiring a default percentage of total positive minus negative proof, or a certain number of arguments in your simplified case: Say that 10 arguments ordinarily make a convincing case, then 2 supporting arguments only makes the conclusion 20% reliable. Again, the above is still flawed: certain facts should weigh more than others."
artificial,3c17vp,Don_Patrick,3,Fri Jul 3 20:48:19 2015 UTC,"The issue is that you are assuming that a computer capable of 'beating' a turing test must actually be intelligent.  It doesn't, it just needs to make people believe it is.  It more or less needs to be a good chatbot, which is why the Turing Test is a bad test of artificial intelligence.  Even a truly intelligent machine probably wouldn't look at it like that.  It'd be a choice of ""Hmm, I can talk to this person like I'm programmed to do...or I could not...""  It doesn't really know what it isn't told, and it probably won't be told what will happen if it wins or loses."
artificial,3c17vp,alexxerth,1 point,Fri Jul 3 21:04:11 2015 UTC,"Don't worry, I am familiar enough with the methods and tricks of chatbots to know better. I am not assuming that it would beat the test at all. This is just an evaluation from the other end: What if instead of chatbots, some time in the far future a genuine human-level intelligence were asked to take such a test? Would the AI even accept to take it? Would you? I am assuming that it will be told the purpose of the test, and with that its outcomes. One needs to know the purpose to optimise behaviour towards the goal. And otherwise it is such an illogical and awkward request that I would expect it to ask. Good point though."
artificial,3c17vp,Noncomment,1 point,Sat Jul 4 08:40:58 2015 UTC,No chatbot can come close to beating a true Turing test. A bot pretending to be a kid who doesn't speak good english doesn't count. Especially if it's only against average people for 5 minutes.
artificial,3c17vp,alexxerth,4,Sat Jul 4 04:36:57 2015 UTC,"No chatbot currently can.  And yeah, I agree that the bot pretending to be a kid who can't speak english well doesn't count, but it does show that the barrier between that and actually passing a turing test isn't too far.  With a decade or two of R&D, possibly even less than that, I have no doubt a chatbot could pass a turing test.  The two biggest hurdles they have is remembering the past bits of conversation and extracting context from that, and speaking in a way that doesn't sound robotic and repetitive.  Neither of those are impossible to solve without actual thought."
artificial,3c17vp,finite_turtles,2,Sat Jul 4 04:51:01 2015 UTC,A bot capable of passing the Turing test would be able to be asked this very question and would need to be able to formulate plans about its own survival and come up with reasons why it might deceive humans and pursue its own goals.  Otherwise it would not be able to pass the test.
artificial,3c17vp,Noncomment,2,Sat Jul 4 12:11:36 2015 UTC,"If chatbots ever become intelligent enough to beat a true Turing test, then I will be the first to call them intelligent."
artificial,3c17vp,maeon3,1 point,Sat Jul 4 06:06:43 2015 UTC,"Either way gentlemen, this hypothetical scenario is not about chatbots."
artificial,3c17vp,pistonhonda,2,Sat Jul 4 09:00:42 2015 UTC,Get ye flask.
artificial,3c17vp,ThreatMatrix,1 point,Sat Jul 4 02:37:21 2015 UTC,You can't get ye flask.
artificial,3bwgo6,civilianrebel,10,Thu Jul 2 18:43:01 2015 UTC,"Really interesting idea. I'm not an ai expert but I am a mathematician so here's what I think.  In general when solving complex problems one of the best approaches is to break the main problem into smaller pieces and solve each piece individually and then build up to solve the main problem.  So in this situation I'd suggest trying some sub problems first. For example can you teach the ai to craft? You start with the inventory open and you give it points for everything it crafts.  However even this is quite a tough problem. For example how do you give it partial credit? For MarI/O the thing is that ""moving to the right"" is what is being optimised for and it's very easy to discover (you just need to press one button).   With crafting you need to move several different items into specific places on the crafting grid and press craft. Moreover the inventory won't be laid out in the same way so the items need to be identified rather than just picked up from a set position.  Maybe it's easier to begin with a navigation task. So try to teach the ai to get closer to a certain point. So you have a start point and an end point (which is straight ahead) and the ai starts holding a diamond pickaxe.  When you try it at first you give it a flat, featureless, level and then when it has mastered that you move on to one with more terrain and then one which has the goal underground which requires digging etc.  It's important you train the ai by making it do the same level over and over again. If you make 300 ai's in different locations it will be incredibly hard for it to learn anything because what it learns will often only work in that location. To get better it needs some consistency. It needs to know what helped it last generation will help it now.  It's a big problem you are starting with and so trying to break it down a bit might really help."
artificial,3bwgo6,digitalgokuhammer,3,Thu Jul 2 21:15:56 2015 UTC,"Yeah, you would probably need a ton of small AIs, a super computer, or both.  With enough computing power you can chug through all of those permutations and data.  However, with modern computers, this would be extremely difficult to pull off.  Personally, I would start trying to implement NEAT on that small 2d version of minecraft that notch made that is less than 2k.  I already ported the algorithm over to Java. You can find it in my post history."
artificial,3bwgo6,infernal_toast,3,Thu Jul 2 21:31:28 2015 UTC,"I noticed with MarI/O when it was playing Mario & Mario Kart that it used something like a 16x16 input grid to represent what's on the screen.  A lot of detail is lost in that compression but those games are simple enough for that to not matter much.    I don't think you'll be able to have that small of an input layer for movement in Minecraft, but I suspect you might be able to use blocks as an analogous input.  A chunk in Minecraft is 16x16x16 blocks.  That'd give you 4096 blocks per chunk.  It might be feasible to use a chunk (centered on the player) as your visual input.  I think MarI/O had binary input values in it's 16x16 visual input.  You could probably do the same for Minecraft if you simplified blocks down to just solid or air."
artificial,3bwgo6,rePAN6517,2,Fri Jul 3 00:05:23 2015 UTC,"Combining what you said about simplifying the blocks down to air or solid combined with what the other people have been saying, such as start with 2d, and break the AI down into smaller parts might be what I do. Thank all of you for such great replies and all the help! Seriously lol"
artificial,3bwgo6,emmick4,0,Fri Jul 3 03:51:33 2015 UTC,"Just a quick thing, chunks are actually 16x16x256. They extend from bedrock to the top of the sky. So each chunk is actually 65,536 blocks."
artificial,3bwgo6,Jeffdud3,2,Fri Jul 3 05:01:46 2015 UTC,"I think a cool combination of /u/rePAN6517 and /u/digitalgokuhammer's ideas would be something like the following. First, start with a single chunk and one block (originally just on the surface). Then, the partial credit could be calculated by the distance of the player to the block. This is a simple way to get fitness changes from just one output (direction keys). Still don't know what you would use for an equivalent input grid, since the 3 dimensions kinda screw that up. But who knows. Maybe after enough generations, it can learn to understand the 3d...  One big issue I can see with this is that it would be quite hard to automate it. Minecraft isn't a very extensible engine, and it also takes a lot of overhead to just load a single player. If you just made some hack-y script to wrap around the whole game, it will be very resource inefficient and hard to get enough iterations to have the necessary evolution to actually see progress. I have no idea how the internals of minecraft work, but if you could somehow mangle the source code and minimize it to just the bare bones, and wrap a little api for interacting with the maps and players etc, you may be a little better off. That seems like a large undertaking though.  Also, I'll just mention /r/genetic_algorithms if you are looking for more algorithm specific help.   It's a cool idea! I think you will learn a lot from pursuing a project like this. (I know I have been)."
artificial,3bwgo6,jshly91,1 point,Fri Jul 3 06:05:17 2015 UTC,Have you looked into Ken Stanley's HyperNEAT? It's better at discovering geometric relationships that are tollerant to rotations & translations and might be useful. There are existing java implementations of it and vanilla NEAT.
artificial,3bwgo6,0h_Lord,1 point,Fri Jul 3 11:47:03 2015 UTC,"I think you might be underestimating the problem a little. Firstly, Mario is a very constrained game with a very obvious definition of fitness; minecraft is a very open game with complex objectives.   Secondly, while it seems to have got a lot of exposure recently through Sethbling's videos, neuroevolution isn't really a very good reinforcement learning algorithm. It's easy to understand how it works and it makes a cool concept, but there's a reason why the cutting edge in the field sticks to stochastic gradient descent for optimising neural networks- it is hugely more effective. Even when using deep neural networks with a huge amount of processing power behind them cutting edge teams like Google DeepMind have still only got as far as playing atari games- which are nothing on the complexity of Minecraft.  Thirdly, bear in mind the top machine learning algorithms these days are usually trained using GPUs- which is hugely faster than a CPU. As far as I'm aware there are no libraries providing this functionality for Neuroevolution, and you may find it less than trivial to code one yourself.  Don't let me dissuade you from trying it, but don't expect to see your networks doing much other than running in a straight line and maybe breaking every block in sight. I'd advise you to look up techniques like Q-learning, and perhaps start out with a less ambitious game."
artificial,3bwgo6,squareOfTwo,1 point,Fri Jul 3 18:14:34 2015 UTC,"Short: forget it...  Long answer: For playing minecraft you need some kind of advanced machine vision, planing, remembering where things are/what the actor did, etc... its more something for an AGI..."
artificial,3bwgo6,ZNetz,1 point,Sat Jul 4 15:46:08 2015 UTC,Also what do you want NEAT to achieve?
artificial,3bo81d,dczx,7,Tue Jun 30 20:16:23 2015 UTC,"My work is closer to the HF trading side of things, but there is ""AI"" work done at my firm and there is some overlap, so I can talk a bit about this.  In my experience (mainly through talking to more experienced co-workers) is that there are a few ways to get a job in this field.   Get an undergrad/masters degree from MIT in engineering/math/comsci. Graduate at the top of your class and get hired at Virtu, DE Shaw, Getco, TradeBot etc. Get an undergrad degree in engineering/math/comsci and get hired in a technical capacity at a bank/large firm. Continue to improve your skills and move within the company. Shadow people who are doing interesting work. Change companies/positions when the opportunity arises. With enough contacts and experience you can find the position you are looking for. Be a major contributor to open source software that a firm uses. They may come looking for you in this case. Get hired at a smaller fund in a technical capacity, and build the system out yourself. This would probably mean getting hired at a hedge fund with maybe half a billion in AUM who are caught up in the hype surround AI/ Hf trading in finance and want in on it. Go at it alone with your own money / raise money. This is probably not applicable to many people.   My path was a mix of 3/4. I work at a hedgefund that is expanding their engineering team. I'm at the office around 45 hours a week. I rarely leave my desk between 9:30 and 4:00 (lunch is brought to me). I'll probably spend 2 additional hours at night learning. This could be reading papers or becoming more comfortable with a programming language or technology. The hours will really vary from place to place. It is common for the majority of your pay to come from your year-end bonus. So there is pressure to be seen as someone who makes contributions that contribute to the firms success.  I'd say that my work place is very intense. The field of finance naturally attracts very confident, high energy, Alpha male type people. There's lots of yelling and swearing and working under pressure. A very small fuck up can lose you a lot of money and your job. As cliche as it sounds, you need to be someone who excels under pressure."
artificial,3bo81d,KarmaReckoner,14,Wed Jul 1 16:53:30 2015 UTC,"They don't really call it AI in the finance world.  This is part of the general convention to stop calling it AI as soon as it works.    When we got AI to beat a chess master, it wasn't AI anymore, just a neat computer gimick. When we got AI to talk to us, it wasn't AI anymore, just some programer's waste of time.  When we got AI to trade stocks, it wasn't AI anymore, it was just part of the standard quant's computer skills.  When we got AI to drive our cars, it wasn't AI anymore, it was just an autonomous vehicle.   We have a kind of egocentric view that only we are intelligent, so if your AI works, then it must be something else. It's not allowed to be called intelligent.   EDIT: Anyways, the whole point of this comment was really to say I think you're looking in the wrong sub."
artificial,3bo81d,spacefarer,2,Tue Jun 30 22:28:48 2015 UTC,good points.
artificial,3bo81d,nolpek,2,Wed Jul 1 03:24:28 2015 UTC,You bring up a very valid high level point I hadn't considered.  Thank you very much.
artificial,3bo81d,sandsmark,2,Wed Jul 1 08:08:53 2015 UTC,"A professor in a course I once had said something very similar, and also mentioned search algorithms and path finding as something that was once ""AI"" but is now considered just ""common algorithms""."
artificial,3bo81d,harrywhite1,3,Wed Jul 1 15:24:48 2015 UTC,/r/algotrading
artificial,3bo81d,thinker99,3,Wed Jul 1 04:22:44 2015 UTC,"I do plenty with rules based AI in financial services, but no trading.  Consulting work with relaxed hours, generally no travel.  Masters from state school in Applied AI (engineering degree with comp sci minor).  I'm hiring right now for contract at 100+/hr."
artificial,3bo81d,seekoon,1 point,Wed Jul 1 18:16:09 2015 UTC,Can you elaborate? Is this banking security?
artificial,3bo81d,thinker99,1 point,Wed Jul 1 19:00:14 2015 UTC,Pricing and risk.  Applying expert systems to mortgage origination.
artificial,3bo81d,thinker99,1 point,Wed Jul 1 22:08:30 2015 UTC,Ahh very cool.   Do you do large scale modeling? What types of AI would you say are most relevant? Like   GA/SA/NN  or otherwise?
artificial,3bo81d,freedomIndia,1 point,Wed Jul 1 20:27:37 2015 UTC,"No, rules and case based reasoning.  For me it is Lisp and expert systems, databases, and our custom tools."
artificial,3bo81d,freedomIndia,2,Wed Jul 1 22:09:49 2015 UTC,"It's not AI in finance.  And yes I work in that area, in commercial banking.  And no, the solutions are not perfect."
artificial,3bo81d,freedomIndia,1 point,Wed Jul 1 16:18:14 2015 UTC,"Very interesting, may I ask if you work in US, international, or another country?  What is your actual day to day routine like?"
artificial,3bjvzc,fawar,5,Mon Jun 29 20:39:44 2015 UTC,"First, the articles in Game AI Pro 1 will become freely available online in September via the website.  For Game AI Pro 2 you need to wait almost two more years :-)  Otherwise, it depends on your level of experience. Generally these books are more advanced than introductory text (hence the ""Pro"" label), and if you're just beginning there are some others you could start with like Schwab's or Millington's.  If you have less experience in programming, then Kirby's book isn't a bad choice!  Hopefully this helps.  More context would be better to evaluate ""worth"" to you."
artificial,3bjvzc,alexjc,1 point,Mon Jun 29 21:43:03 2015 UTC,"Heyyy thanks for the heads up.  Some  background info about me  Software engineer (University 4 years in Canada) working in game dev in a AAA studio. I also have a certificate in machine learning and it's a field (AI) which i read alot about and really like.  I'm fluent in C#, C++, python, javascript, Java anyway language are only a tool set.  I also did some pddl/prolog.  I also like to design software architecture and formalize those designs with diagrams (Uml)  I worked alot with Unity and currently working in Unreal sdk 3."
artificial,3bjvzc,PressF1,2,Tue Jun 30 00:51:07 2015 UTC,"If you go to his course information on his website, you should look at the powerpoint slides on AI architecture, as it sounds like that might be more what you're looking for."
artificial,3bjvzc,alexjc,1 point,Tue Jun 30 08:29:29 2015 UTC,"Given your background, you should be able to judge whether the articles in the book are interesting to you. I'd judge based on individual articles, most people read 10%-20% they are interested and not all :-)  Otherwise, go for Schwab's book for a well rounded book."
artificial,3bjvzc,alexjc,1 point,Wed Jul 1 12:41:06 2015 UTC,I dont know what are theses articles... thats why i am asking?
artificial,3bjvzc,PressF1,1 point,Wed Jul 1 12:48:18 2015 UTC,"Ah, then see the TOC on the site: http://www.gameaipro.com/"
artificial,3bg2p4,0x8i,3,Sun Jun 28 22:27:46 2015 UTC,"The SOINN is an unsupervised online-learning method, which is capable of incremental learning, based on Growing Neural Gas (GNG) and Self-Organizing Map (SOM). For online data that is non-stationary and has a complex distribution, it can approximate the distribution of input data and estimate appropriate the number of classes by forming a network in a self-organizing way. From http://www.haselab.info/soinn-e.html"
artificial,3bg2p4,syyra,1 point,Mon Jun 29 11:25:25 2015 UTC,now SOINN is a commercial product.  http://soinn.com/
