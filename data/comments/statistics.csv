statistics,3e2ci3,cruise02,30,Tue Jul 21 12:34:04 2015 UTC,"The question vastly overstates the problem, but there's a legitimate point in there.    There are already some good responses on stackexchange, but I want to add that this is part of the reason why it's pretty rare for anybody to take a single scientific paper as 100% definitive proof of anything.  Scientific journalists and the general public love to do that, but scientific knowledge is built on a collective body of work.  People think that science is about proving truths, similar to math, but it's not.  Science is about finding truth as best we can.  Outside of pure mathematical fields it's incredibly difficult (many would argue literally impossible) to prove anything with 100% certainty.  I have a dream that one day we will live in a world where we stop arguing about p-values all the time."
statistics,3e2ci3,kevjohnson,2,Tue Jul 21 13:55:32 2015 UTC,"Science is about finding truth as best we can.   This is such an important point. Although many of us know this it is always great to see it explicitly stated. P-values have become so popular as a measure of ""correctness"" that it is incredibly easy to forgo a natural skepticism about results. P-values can often be as misleading as they are useful and when it comes to modeling uncertainty it's hard to certainly model anything. Aaaaanyway, was just happy to see the above statement in your comment."
statistics,3e2ci3,Iloveyoumarkov,18,Tue Jul 21 21:06:50 2015 UTC,"Look.  At least in my field, urban economics, p-values are supporting evidence but they're not THE EVIDENCE.  Here's what I was taught.  1) Observe the word.  Formulate a testable hypothesis.  2) Hypothesize : An increase in X increases Y.  3) Consider alternative hypotheses.  4) Gather data to test your hypothesis and that tests those alternatives.  5) Run the model.  What did you learn?  6) Check the robustness of your model.  Did you use prices?  Then use a different measure of price.  Did other related papers use other variables?  Be sure to have those.  Do you results generally hold up to a variety of changes in estimation specification?  7) Good, your idea held up.  You might have something.  Now go publish it, and check back in 10 years to see if anyone followed up your work and confirmed your findings using different tests or data you didn't have.  P-Values are not everything.  This is known to most researchers..."
statistics,3e2ci3,Jericho_Hill,6,Tue Jul 21 13:13:17 2015 UTC,"Yep. Also economist here. While certainly not all economists understand the philosophy of statistics, we are generally better than (insert random fields to piss people off here). Starting with an idea that makes some theoretical sense helps prevent solely searching through random noise.   However, as one bad case, take the Nurses Health Study. This huge dataset with 1,000's of variables on diet, exercise, and many health-related outcomes has been randomly data-mined to look for significant correlations. While this study HAS generated a lot of good information, many bad, poorly thought out papers have come from it as well. (Of course, economists have committed some of the same sins with the NLSY, for example!)"
statistics,3e2ci3,BurkeyAcademy,2,Tue Jul 21 13:39:51 2015 UTC,You don't need to observe the word.  Everybody knows that the bird is the word.
statistics,3e2ci3,HughManatee,0,Wed Jul 22 02:50:55 2015 UTC,"Is this the standard? While this is absolutely what we learn in theory, the practice is rarely up to this standard.  Checking various estimation specification is actually a headache, because it is very difficult to know whether adding a term may lead to endogeneity or not. In social science all factors tend to influence one another in a big circle..."
statistics,3e2ci3,selectorate_theory,2,Tue Jul 21 14:54:51 2015 UTC,"To get published in a good journal, yes, this is the process you must go through. Which is an argument for not paying too much attention to papers in obscure journals."
statistics,3e2ci3,standard_error,2,Tue Jul 21 15:35:55 2015 UTC,Unless of course publication criteria in a certain field of research are heavily biased. In my field it is not that uncommon to dig up/find references to papers published years ago in very obscure journals that point out flaws that by now the field is slowly starting to accept exist. The marring conclusion: years ago these views/papers could not be gotten published in the 'main stream'/'good'/'high impact' journals.
statistics,3e2ci3,penthiseleia,2,Tue Jul 21 21:38:35 2015 UTC,"As far as the journals I submit to, and the researchers I interact with, yes."
statistics,3e2ci3,Jericho_Hill,1 point,Tue Jul 21 15:00:19 2015 UTC,"Could you please link me to a typical paper in your field, i.e. not tip top stellar, but not obviously bad either?"
statistics,3e2ci3,selectorate_theory,3,Tue Jul 21 15:16:07 2015 UTC,"http://sfpl.org/pdf/main/glc/glbtsfdemographics.pdf  This is an earlier draft of a paper that is probably near seminal-paper status, though the authors have somewhat come around (based on later work that they may have discounted the role of house prices, this is closer to my work and findings).    Of import here is that, theory comes first with natural, testable implications.  The authors are clear that a direct test isn't possible, but they are able to conduct a reasonable indirect test based on implicit prices of amenity (which builds from past works of others) and that they check their mainline result to several falsification test.  The final version of this paper is behind a paywall and came out in 2009."
statistics,3e2ci3,Jericho_Hill,1 point,Tue Jul 21 15:39:44 2015 UTC,"Thanks for the link -- I do enjoy the paper quite a bit.  You're correct that the paper did a good job of trying multiple measures for the variable of interests. For example, to measure the concentration of gays, they tried both attitude surveys and # of AIDS related death.  That's great, but I think the main reason that this paper is good because they have sexual orientation as an exogenous variation. That is a serious luxury in social science, especially in fields like political economy (mine). Factors like governance and development and corruption are so intertwined that, without exogenous variation, no amount of statistical modeling can rectify. Not to at all diminish the admirable rigor in the gay men paper, but with the exogeneity of sexual orientation, it's a relatively simpler problem to tackle.  Back to the paper, I also have a question. The authors claim that the sociological factor (i.e. gay friendliness of a region) isn't that strong. But this could entirely be due to the measurement error of gay friendliness, compared to the precision in the house price measure.  If so, the alternative explanation stands. It could be the case that liberal regions extract more gays, and also other highly educated residents, which push the house price up. Is this properly addressed by the paper?"
statistics,3e2ci3,selectorate_theory,1 point,Tue Jul 21 19:20:22 2015 UTC,"That's really cool.  Glad you liked it.  Its a pretty important work for my own writings, and yes, finding an exogenous source of variation is very difficult and here they did just that.  I think measurement error is a good point here about gay friendliness. What makes the paper convincing is that several other papers that followed also are finding links between house prices and labor force composition, and these findings survive alternative explanations and questions of endogeneity.  For instance, it is a stylized fact in the urban economics literature that the income elasticity of demand for housing is much less than 1 (.4 to .6 by my read).  This means house prices and their changes have very different effects based on income...and indeed this appears to be the case, human capital (and earnings) increases as house prices rise, both in levels and differences.  This is because house prices act as a filter (they dont cause human capital to rise, they filter it to locations).  In the black paper they say that its the difference in housing consumption rather than any income difference (though I'd argue that while making equal earnings to straight couples, gay couples have more disposable income (no kids)).  In that case, recent work I saw presented at the AEA conference finds a similar effect of this housing preference hypothesis with hispanic workers but not for non-hispanic workers (logic being that hispanic workers, particularly recent migrants, are separated from their family and therefore have less housing demand).  Its a particularly intriguing new test that I feel validates the Black paper.  Back to your concern, perhaps the fact that cities themselves tend to be overwhelming liberal in the first place can allay some concerns.  I wouldn't argue that there are systematic political differences in the average city dweller of Atlanta, Dallas, San Fran, or Chicago.  Maybe in the suburbs but that's a different animal and house prices would be more homogenous out there anyways, its really the urban core where we get alot of differences in house prices."
statistics,3e2ci3,Jericho_Hill,4,Tue Jul 21 19:55:28 2015 UTC,"It's frustrating that he cites the chocolate-fueled weight loss ruse. The point was to show that crap-tier ""journals"" would publish anything, no matter how flawed the methodology, and the popular press would eat it up. A side note is that p-hacking is, of course, terrible, and every journal should be mindful of it.  That doesn't mean that p-values always suck or that people are consciously trying to screw up their own methodology to publish."
statistics,3e2ci3,Hakawatha,-5,Tue Jul 21 17:26:04 2015 UTC,"p-values do always suck anyway, though"
statistics,3e2ci3,faore,3,Tue Jul 21 19:55:11 2015 UTC,"It's not a problem with p-values, it's a problem you'll have to deal-with-it with for any uncertain criterion. Any test could give you a false positive. The p-value just makes that explicit."
statistics,3e2ci3,waterless,5,Tue Jul 21 15:36:00 2015 UTC,"If the tests were done checking for totally random associations it would be true, but that rarely happens.  For example, if I wanted to check if significantly more males with Tumblr accounts own fedoras than males without Tumblr accounts, that is testing a hypothesis with underlying supporting evidence.  If I wanted to test whether Bank of America customers were more likely to own a fedora than customers of other banks, that makes no sense and I'm going to get garbage associations 5% of the time.    For the test to have any meaning, there has to be a reason to assume that a particular association is not random or that there is an effect.  And the reason p-values below .05 are so common in the literature is that there is very often a good reason to suspect that there is some effect or association.  The test is to demonstrate that there is a difference prior to a more in depth analysis."
statistics,3e2ci3,calibos,4,Tue Jul 21 15:42:11 2015 UTC,Is this really true though? I thought a p-value would give a false positive alpha% of the time... Does it really have to do with the underlying probability of the event in question?
statistics,3e2ci3,King_Crab,3,Tue Jul 21 15:47:58 2015 UTC,"I like Fireflite's answer below, but I'll answer without invoking the Bayesian framework.  The p-value directly corresponds to the % of times you would see an effect of the observed size or more extreme only if the null hypothesis is true.    So what is it good for if the null is seldom 100% true?  Generally, the farther from ""true"" the null hypothesis is, the easier it is to reject the null with high confidence.  For example, if I were comparing the effectiveness of 2 drugs and one treated the condition 20% of the time and the other treated it 90% of the time, it would be easy to reject the null hypothesis that the 2 drugs are equally effective.  If, on the other hand, the truth was that one drug treated the condition 20% of the time and the other treated it 21% of the time, that is much harder to reject.  You would need a large sample to show the small effect with significance.  If you only had a small sample, you would probably not be able to reject the null even though the null is, in truth, false.  You know the null is probably false (it is 2 different drugs, after all), but your p-value tells you that the effect is too small to detect with the data you have."
statistics,3e2ci3,calibos,5,Tue Jul 21 16:40:34 2015 UTC,"So this is much clearer from a Bayesian perspective. The p-value is the chance of observing a test statistic as or more extreme than what was observed if the null hypothesis is true.  In the BoA + fedora, we can assign a prior probability of the null hypothesis to 1 and the alternate hypothesis to 0 because we ""know"" that it's a stupid hypothesis.  As a result, using a strict p<=0.05 rule will produce false positives 5% of the time by definition because we know the null hypothesis is true.  In the Tumblr + fedora hypothesis, we assign a non-zero prior weight to the alternate hypothesis. This pushes the false positive rate down because sometimes (in the frequentist universe of infinite alternate realities), the alternate hypothesis is actually true, converting some of those false positives into true positives and some of the true negatives into false negatives. Just how commonly that happens depends on how big the effect is; which is where power calculations come in."
statistics,3e2ci3,Fireflite,3,Tue Jul 21 16:03:48 2015 UTC,"I see, that makes a lot of sense. So then can we say that the false positive rate is 5% in cases where the null hypothesis is in fact true?"
statistics,3e2ci3,King_Crab,4,Tue Jul 21 16:37:12 2015 UTC,Yes. The clue is in the name - a positive can only be false if the null hypothesis is true.
statistics,3e2ci3,red_concrete,1 point,Tue Jul 21 18:00:44 2015 UTC,"pulls out the obligatory links for anyone interested in this topic  The Two Compulsory Academic Papers on this Topic:  False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. By Simmons, Nelson, & Simonsohn (2011): http://pss.sagepub.com/content/22/11/1359.full.pdf  &  Why Most Published Research Findings Are False. By Ioanidis (2005). http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124  Plus: not a research paper, but a very comprehensive journalistic piece on several of the biases that may be in action: The truth wears off:  http://www.newyorker.com/magazine/2010/12/13/the-truth-wears-off"
statistics,3e2ci3,penthiseleia,1 point,Tue Jul 21 21:50:18 2015 UTC,"One assumption that's missing here is that most papers consist of multiple studies on a similar phenomenon. So you need to calculate the odds of getting 5 studies in a row with p less than .05, which is going to be much rarer.  Of course, then you add back in p hacking and things get worse again."
statistics,3e39wp,BigMakondo,7,Tue Jul 21 16:59:24 2015 UTC,Those arrows remind me of PowerPoint 2010.
statistics,3e39wp,mooktank,2,Tue Jul 21 17:54:32 2015 UTC,I've drawn DAGs in PowerPoint that came out almost exactly like this. Ng's have that telltale PowerPoint-y look with everything snapped at a compass rose position and the arrowheads on top of each other.
statistics,3e39wp,normee,5,Wed Jul 22 04:43:07 2015 UTC,"I don't know if it's how Ng did it, but that is definitely something Tikz can do. It's also something matplotlib or a similar plotting library should be able to do, and doing it that way would probably be a lot less painful than using Tikz."
statistics,3e39wp,bluecoffee,2,Tue Jul 21 18:41:07 2015 UTC,Inkscape
statistics,3e39wp,farsass,1 point,Wed Jul 22 03:10:05 2015 UTC,You can use Microsoft Visio or Libreoffice Draw
statistics,3e39wp,SinnyCal,0,Tue Jul 21 18:19:02 2015 UTC,Illustrator.
statistics,3e39wp,homercles337,2,Tue Jul 21 19:11:19 2015 UTC,This was very useful for me. Not as pretty as your example...but still ;)  https://beckmw.wordpress.com/2013/11/14/visualizing-neural-networks-in-r-update/
statistics,3e3s0a,MasterGani,2,Tue Jul 21 19:01:37 2015 UTC,"LDA is a classification algorithm. The idea is that, for each class, you fit a multivariate normal distribution to the data that belong to that class. You can then classify a new data point based on which distribution has the largest value at that point.  If you fit the distributions separately for each class, using only data from each class to calculate the relevant covariance matrix, you will in general get quadratic decision boundaries (QDA). But if you assume there is just one covariance matrix that is common to all the data, then you get linear decision boundaries (LDA).  I'm not sure what you mean when you say ""we find this average variance for each predictor and then use it and another predictor's average variance to get their covariance."" You don't just look at each predictor's variance separately, you have to consider how both predictors vary over a set of observations to calculate the covariance. In QDA you only use observations belonging to the relevant class. In LDA you use all observations. The formula for the covariance matrix in that case can be found on slide 11 here or in a book."
statistics,3e38c7,umib0zu,1 point,Tue Jul 21 16:48:56 2015 UTC,"If you could describe in words or equations what you are doing, more people will help."
statistics,3e38c7,AndersonCoopersDick,1 point,Wed Jul 22 00:32:04 2015 UTC,"I just updated the readme. I always want to know what the chances of my particular favorite English Premier League ending in the top 4 are by around 8 weeks left in the season, and the code is designed for a user to give a current table and the remaining games and compute a specified observable function. It's written entirely in javascript and is mostly a proof of concept, but it's not going in an academic journal."
statistics,3e2tgo,hokie_john,1 point,Tue Jul 21 15:02:09 2015 UTC,FBI Uniform Crime Reporting Series.
statistics,3e2tgo,Jericho_Hill,1 point,Tue Jul 21 15:03:32 2015 UTC,"The UCR doesn't include information about the victim or the motivation for the crime-it's strictly aggregate monthly counts. There might be some special reports out there, but the raw UCR files don't have the info OP is asking for.  NIBRS does include victim characteristics and other data like whether the attack is considered a hate crime, but you have to be super careful using it because there's a lot of missing data and the missingness isn't ignorable.  Source: my dissertation is on crime statistics."
statistics,3e2tgo,statmama,1 point,Wed Jul 22 03:03:55 2015 UTC,If you create an account on Enigma (kinda like a search engine for public datasets) you can access the FBI dataset Jericho mentions as well as a bunch of others.  https://app.enigma.io/search/source/us.gov.fbi.known-offenses
statistics,3e2tgo,eagz2014,1 point,Tue Jul 21 15:37:05 2015 UTC,What about a FOIA request to the FBI or someone? We get them all the time for research projects.
statistics,3e2tgo,harDCore182,1 point,Tue Jul 21 15:41:54 2015 UTC,"The Bureau of Justice Statistics. I think they even do a special report on hate crimes, which includes religiously motivated attacks."
statistics,3dz6qt,twit16,21,Mon Jul 20 18:47:21 2015 UTC,"How good is this field at dividing itself into cliques and clans and tribes, and then vigorously debating who's right ? I'm absolutely amazed by this. First there was the Bayesian and the frequentist, but it seems that's so last century ! now you can be a cool machine learner, or the younger kids on the blocks: the data scientists, or a boring old statistician.  What does this achieve ? From the point of view of the progress of science, probably nothing at best, and at worse we're shooting ourselves in the foot. From the point of view of academia though, it makes life so much easier. Why bother with keeping up with the one huge field of statistics when you can just cast anathema on two thirds of it and ignore it safely from your ivory tower ?  All the best machine learners, data scientist (what a stupid buzzword) and statisticians know it: there is only one field and we are all studying the same thing. The name we put on it doesn't matter"
statistics,3dz6qt,Hairy_Hareng,4,Mon Jul 20 20:22:57 2015 UTC,"I agree. The lines between statistics, data mining and data science are not even really there. I feel that data science took some approaches from IT in terms of fast prototyping and throwing computational power at things (which is great sometimes).  On the other hand I feel most people who get into data science come from IT which makes sense, since lots of the stats/ML algorithms come prepackaged, while you do not get pre-packaged programming."
statistics,3dz6qt,somkoala,3,Mon Jul 20 20:43:15 2015 UTC,"Like the lines between applied math, physics and engineering."
statistics,3dz6qt,Antimoneyyy,2,Mon Jul 20 20:50:07 2015 UTC,It's true. I went to an undergrad institution big on the interdisciplinary learning. So I was lucky enough to learn calc at the same time I was learning physics. I felt it was very enlightening
statistics,3dz6qt,therndoby,2,Mon Jul 20 23:12:58 2015 UTC,"Unless it's ""Physics for Arts Majors,"" any decent program should teach Calc and (Pre-relativity) Physics at the same time."
statistics,3dz6qt,NeuroG,1 point,Tue Jul 21 02:12:09 2015 UTC,I did biochem undergrad at ASU. Our physics class was not calculus based. I think it's really just physicists and engineers that routinely do calc based physics.  Worst part was when I got marked down for using calc in the 'show your work' steps.
statistics,3dz6qt,Sir_Cuitry,1 point,Tue Jul 21 16:29:28 2015 UTC,but not in the same course
statistics,3dz6qt,therndoby,2,Tue Jul 21 02:45:53 2015 UTC,So I was lucky enough to learn calc at the same time I was learning physics   When is this ever not the case?
statistics,3dz6qt,FappingNowAMA,1 point,Tue Jul 21 02:21:31 2015 UTC,Pretty regularly. Considering the calc tends to be a requirement for the physics
statistics,3dz6qt,therndoby,2,Tue Jul 21 02:46:58 2015 UTC,"This is why I like my grad program.  You're exposed to machine learning techniques alongside traditional statistics, and all of these new ""fields"" are treated as statistics."
statistics,3dz6qt,beaverteeth92,1 point,Tue Jul 21 00:28:08 2015 UTC,May I ask what grad program that is? I'm curious.
statistics,3dz6qt,bloomer14,1 point,Tue Jul 21 03:00:18 2015 UTC,Check your messages.
statistics,3dz6qt,beaverteeth92,1 point,Tue Jul 21 03:01:42 2015 UTC,Also wondering.
statistics,3dz6qt,-dont-panic,4,Tue Jul 21 06:14:34 2015 UTC,"The slight decrease in the use of ""statistician"" in the graph would be because some statisticians are calling themselves other things now. But they still know at least the same stuff they did in 2005."
statistics,3dz6qt,efrique,9,Tue Jul 21 04:03:55 2015 UTC,"I'll go opposite of what's posted already in comments and say I think there is a difference between statisticians and data scientists. Data scientists I tend to think are programmers that are experts at manipulating large, complex data and know a little statistics. Statisticians are the opposite...  I think the average person filling a ""data scientist role"" won't be able to tell you the difference between a Wald and Score test, or when one might be preferred over the other. They won't understand how to get variance estimates through the delta method or really ""do statistics"" if it doesn't come in a canned function on the computer.   I think it's scary because it's so easy to apply statistical tests to data and get p-values... But you have no idea what you're doing and those values don't actually make sense.   At the same time, statisticians don't have the expertise in programming or ""data mining."" We know R and SAS, but tell a lot of us to go scrape data from Twitter/Facebook/etc. and we'll give you a blank stare. We can learn, just as data scientists can learn advanced statistics... But we each have our areas of specialization and there is only so much time...  One shouldn't exist without the other. ""Data scientists"" can get the data and statisticians should come up with the complex and proper ways to analyze it!"
statistics,3dz6qt,Distance_Runner,1 point,Mon Jul 20 22:40:13 2015 UTC,"Data scientists I tend to think are programmers that are experts at manipulating large, complex data and know a little statistics. Statisticians are the opposite...   I disagree. A programmer who has on a little idea what they're doing with their ML algorithms and stat tools isn't a data scientist, but they are in the ""danger zone"" of decision making where they now think they know enough. Statisticians who can't manage code to deal with diverse, large, multidimensional/multi-source data are data scientists, they're experts on methods.   Edit: This is the visual I find to be the best representation."
statistics,3dz6qt,HelloMcFly,2,Tue Jul 21 01:36:57 2015 UTC,"Did my first grad degree in economics and statistics and I'm currently doing another in Data Science. Good programs require a very rigorous probability course, as well as other rigorous statistics courses.   I currently work in a data science department and 8 out of 9 people have a graduate degree in statistics, so I don't really agree with the article (this has been my experience at other research institutions in the private sector as well).  All these programs/fields are useful, imo."
statistics,3dz6qt,econometrician,3,Tue Jul 21 03:35:09 2015 UTC,"Economics checking in.    I acknowledge the extreme usefulness of statistics.  I do not acknowledge ""data science"" has superseded economics and statistics.   Any more than ""Neuroscience"" has supersede neurology and psychiatry and psychology.  Or ""Dental science"" has superseded dentistry, would be another.  It's all just kind of ridiculous."
statistics,3dz6qt,bwik,0,Mon Jul 20 22:38:15 2015 UTC,"Just fyi neuroscience is a very real science that is separated from both psychology and biology (of course, we try to investigate the same object in the end). Neurology is the medical field and deals with the practical treatment of the many medical issues that can happen"
statistics,3dz6qt,Hairy_Hareng,0,Tue Jul 21 06:59:20 2015 UTC,"While I am enthusiastic about interdisciplinary fields of study, the danger is always jack of all trades, master of none.  Then there is a trouble that mature fields have a century or more of traditional common core curriculum an presumed expertise.  A local third tier school offers masters degrees in data science.  What is that?  There are no traditional obligations.  It's not a recognized credential.  At best because nobody is familiar with it.  At worst because it is a total scam."
statistics,3dz6qt,bwik,1 point,Tue Jul 21 17:06:23 2015 UTC,"I have a problem with the possibility of statisticians being sidelined in evaluating evidence in science and policy, which the author of the post mentions by way of referring to the NSF and DHHS exclusions but doesn't elaborate on. For a more substantial discussion of the role of statisticians in big data, I refer people to an ASA policy white paper from last year describing important research areas. I think it's clear from these kinds of problems that statisticians will continue to have a place in science. We need to make sure domain experts have adequate training in statistics themselves and are comfortable collaborating with us. If ""big data"" brings them into my office, that's (hopefully) great for both of us.  But other examples the HackerRank post uses like the lack of statisticians at ""big data"" conferences and events, which are mostly going to be commercial oriented, I don't care so much about. (The post also mentions statisticians' lack of interest in business intelligence -- guilty as charged.) It's to businesses' detriments if they pay too much for people with poor statistical training who come up with expensive solutions because they don't realize that a lot of ""big data"" problems in business analytics don't have to be big. I see someone in the comments on that post made the point I was thinking of too:   That the data is 'big' is next to irrelevant: They extract just the data they need and work with that; they don't do much will all the data. Besides, statistics is about making estimates, and usually 50,000 samples are quite sufficient and don't really need 50 trillion."
statistics,3dz6qt,normee,-11,Tue Jul 21 06:41:33 2015 UTC,I hate statistics and data science. It is nearly complete nonsense.
statistics,3dz6qt,lakelandman,3,Mon Jul 20 19:28:15 2015 UTC,How so?
statistics,3dz6qt,Beardamus,-6,Mon Jul 20 19:53:35 2015 UTC,I just hate everything about it. I've worked in the field for a long time (medical research). Just about everything reported in journals in this field is complete BS.
statistics,3dz6qt,lakelandman,9,Mon Jul 20 20:03:28 2015 UTC,you probably hate miss-applied stats more than stats themselves
statistics,3dz6qt,Hairy_Hareng,7,Mon Jul 20 20:17:41 2015 UTC,Med students who don't give a flying fuck about statistics probably
statistics,3dz6qt,JCPenis,5,Mon Jul 20 20:29:26 2015 UTC,"True, I have no problem with theory of stats...just how it gets used in practice in the medical field."
statistics,3dz6qt,lakelandman,3,Mon Jul 20 20:20:00 2015 UTC,"Ya. I'm doing a phd in neuroscience right now. Last year, there was a flurry of paper (in nature neuroscience and science) where the big draw of the paper was ""We did a horribly needlessly complicated analysis to reduce our data to 3 dimensions, in which the neuron activity through time rotates !!!! Give nobel plz"". Those are complete shit but were apparently worthy of our top journals. It does make one really angry about science and stats"
statistics,3dz6qt,Hairy_Hareng,2,Mon Jul 20 20:34:57 2015 UTC,"You should have been more clear on why you hate it.  I dated a girl that was working in the med research field.  They totally lie to their investor and use statistic to lie on how good their product is when it doesn't work at all and if so it wasn't an improvement over existing method. But because they pour so much money and investment in R&D, they needed to recoup the cost...   edit:  She left the field btw. She was disgusted and went back to school for a different field within STEM."
statistics,3e1gtt,DethFromAbove,1 point,Tue Jul 21 05:50:50 2015 UTC,You may find some references here: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1119478/
statistics,3e1gtt,chicken_bridges,1 point,Tue Jul 21 10:09:15 2015 UTC,"If you have access to a university library, the reference librarians there will be able to help you."
statistics,3e0xk8,pushmycar,1 point,Tue Jul 21 02:47:17 2015 UTC,That sounds delicious. You should post this in  r/datasets and maybe r/machinelearning as well
statistics,3e0xk8,SinnyCal,1 point,Tue Jul 21 03:15:20 2015 UTC,I like /r/machinelearning - checking it out now.
statistics,3e0xk8,Sir_Cuitry,1 point,Tue Jul 21 03:22:28 2015 UTC,"If you have the whole comment context, then you could do some topic analysis (LDA?) on the results. That's sort of the standard approach for text data imo."
statistics,3e0mzz,Dyinu,1 point,Tue Jul 21 01:20:04 2015 UTC,"Do you have enough cases just to look at those whose first visit is in 2013? If so, I'd just go with that - unless if there's an issue related to a new treatment that has been put in place in 2014 or later."
statistics,3dzbff,jewami,5,Mon Jul 20 19:20:55 2015 UTC,"The probability of p_H equaling an exact number is infinitesmal   Right. For Bayesian statistics, there are usually two approaches for statistical tests. You can look at a Bayes factor, which will tell you how confident you can be that your model (p_H = 0.5) is more likely than an alternative model (e.g., p_H = 1). An alternative approach (Bayesian estimation) is to establish a ""region of practical equivalence"" (ROPE) around the value of interest and then compare the posterior distribution to that range of values (there's a bit more to it than that, but not much).   Kruschke advocates for Bayesian estimation in his book(s), and even Gelman has advocated against Bayes factors. I haven't run across anything that specifically addresses best practices for establishing a ROPE, however. You might say something like, ""if the coin is within one flip of being fair out of ten tosses, that might as well be a fair coin, so I'm going to establish a ROPE of [0.4, 0.6].""   That's kinda kludgey, but then again, so is p < 0.05."
statistics,3dzbff,Doc_Nag_Idea_Man,2,Mon Jul 20 19:36:41 2015 UTC,Thanks for this great explanation!  So basically you treat P(p_H=0.5) as P(p_H=0.5 +- some reasonable value).  Kludgey indeed...it really scares me sometimes the question of how we know anything given that our methods of hypothesis testing are functions of parameters that can be changed to wildly affect the result.
statistics,3dzbff,Doc_Nag_Idea_Man,2,Mon Jul 20 19:45:57 2015 UTC,"So basically you treat P(p_H=0.5) as P(p_H=0.5 +- some reasonable value).    In Bayesian estimation, you use Bayes' Theorem to derive the distribution of P(p_H) over all values (in this case, the interval (0, 1)), based on your data and your prior. If you see a distribution with a huge peak at p_H = 0.5, well then of course you'll think, ""this coin is probably fair.""   it really scares me sometimes the question of how we know anything    Welcome to science. You'll get more comfortable with it, I promise. :)"
statistics,3dzbff,efrique,2,Mon Jul 20 19:51:11 2015 UTC,"Here's one approach that might make a kind of sense if you really believe p=1/2 has nonzero probability (i.e. there's some chance the coin is truly fair): take as your prior on p a spike-and-slab** or more accurately, perhaps a 'spike-and-hump'; assign some prior probability q to the state p=1/2 and then the remaining 1-q is then given to some continuous distribution on (0,1) (which is what you think about $p$ if it isn't exactly 1/2).   **(Strictly it's generally only called a 'slab' if you make that uniform, but I expect something like a beta distribution more concentrated near the middle would make more sense -- it's probably quite near to fair even if it isn't exactly)   The data can then be used to update the spike-and-""hump"" into a posterior of the same form."
statistics,3dzbff,dlive,1 point,Tue Jul 21 04:09:40 2015 UTC,"Zero chance of it being ""fair"".  If you have infinitesimal belief about something, no event can happen to change your belief.  On the flip side(no pun intended), if you started with a 100% belief that the coin would always come up tails and a single heads comes up, you can't say ANYTHING about what your belief is, its beyond the scope of this course called your life.  However, with no prior knowledge(thus using a flat prior), if you saw 2 heads come up in a row and then were offered a bet to make $0.75 if the coin comes up heads on the next throw and lose $1 if it comes up tails that would be considered a ""fair"" bet to you, if you were risk neutral."
statistics,3dzbff,standard_error,-3,Wed Jul 22 01:18:47 2015 UTC,"I know this is not what you're asking, but a coin  can't be biased  if tossed correctly."
statistics,3dypt6,Racingram,2,Mon Jul 20 16:50:27 2015 UTC,Try a ROC curve. https://en.wikipedia.org/wiki/Receiver_operating_characteristic
statistics,3dypt6,jairgs,2,Mon Jul 20 17:44:35 2015 UTC,"Unless you have a specific point you want to make about model's fit, I wouldn't bother with a GoF.  AIC, BIC, and log-likelihood are essentially meaningless unless you're directly comparing two or more models, I've never once heard anyone ask about the F test for a model, and talking about a pseudo-R2 is opening a can of worms.  I'd be surprised if the reaction of a group of economists to hearing a GoF stat wasn't ""So what?""  Again, that changes if you have a specific point you're making about the model's fit, but if you're not sure what stat to talk about, I'm going to guess that there isn't anything about the fit that you want to say or has anything to do with the point you're making.  A good rule of thumb for presentations is not to put any number on a slide that you don't intend to talk about (except sample size; always include sample size, because if it's not there someone will ask)."
statistics,3dypt6,npgraham1,2,Mon Jul 20 18:35:17 2015 UTC,I would note the pseudo-R2 and keep the rest on an extra slide at the back in case anyone asks.
statistics,3dyjzy,UkyioWeee,2,Mon Jul 20 16:09:37 2015 UTC,"This is actually quite a difficult problem because there is always the possibility that you haven't observed enough samples to learn the ""true"" pattern. For your first example, you can't really say that the probability of a 1 is 100%. The more samples you see the greater your confidence, but you can never be certain.  Most approaches use principles such as ""parsimony"", ""occam's razor"", ""minimum description length"", etc. The idea is that simple patterns are more likely than complex ones, so try to find the simplest pattern that explains the observed data.   Trying to dig up some references.. .I'll post back here if I can track them down."
statistics,3dyjzy,galton1,1 point,Mon Jul 20 16:58:38 2015 UTC,"The (relatively) simple program I'm thinking of is called ""Seek Whence"", described in one of Hofstadter's book. Can't find much info about it online, but might be worth checking out his book ""Fluid Concepts and Creative Analogies"" in a library."
statistics,3dyjzy,galton1,2,Mon Jul 20 17:06:58 2015 UTC,maybe take a look at markov chains?  forecasting is hard for a lot of reasons.
statistics,3dyjzy,ILikeLeptons,1 point,Mon Jul 20 23:26:56 2015 UTC,"You could just make a histogram of the number of zeros in-between ones. That directly estimates a basic probability distribition of how long you'll have to wait following a 1 till the next 1. Or how long you'll have to wait following 1, 2, 3, ... zeros. It's a purely empirical / numerical distribution then.  If you use a probability model you have to make assumptions about the process generating the data, I think. E.g., you could fit a geometric distribution (https://en.wikipedia.org/wiki/Geometric_distribution) but that assumes independence between the observations of the sequence."
statistics,3dyjf7,slipperybutter,3,Mon Jul 20 16:05:30 2015 UTC,"Take a look at ""change point detection"". Also ""local regression"" and ""generalized additive models"", although that's not so much an answer to your question as a suggestion for what you should be using instead."
statistics,3dxkqk,ikarampa,5,Mon Jul 20 10:53:30 2015 UTC,"A correlation matrix needs to be positive-semidefinite, which is what puts bounds on the third correlation.  Here is one way to calculate it: through the partial correlation. This is a conditional correlation; the correlation that remains between Y and Z when you fix X. The partial correlation can always take values between -1 and 1. From the partial correlation of YZ conditional on X (denoted corr(Y,Z|X) ), and corr(X,Y) and corr(X,Z) you can determine the bounds on corr(Y, Z). See here for a formula."
statistics,3dxkqk,TomatoAintAFruit,2,Mon Jul 20 15:19:14 2015 UTC,"Here's a graph on the restrictions for the other correlation based on the minimum eigenvalue of the matrix.  If you had a lower correlation between the two variables there coule be a negative correlation, but not with the example you applied.  I only gave a numeric approximation (0.14) of the minimum correlation.  In general, if you solve for the eigenvalues, you can write an expression for the restrictions on the third correlation.  It becomes more difficult when you get to higher dimensions, since it is more difficult to solve for roots of those."
statistics,3dxkqk,SymphMeta,1 point,Mon Jul 20 15:27:32 2015 UTC,"If you are willing to make assumptions about the marginal distributions, copula theory may help you to put bounds on the range of possible correlation values."
statistics,3dxkqk,ice_wendell,-1,Mon Jul 20 13:23:38 2015 UTC,"The minimum value for corr(Y,Z) is -1 if you assume there can be missing values.  Just a simple example here. X's domain are integers -10..+10 Y's domain are integers -10...+2 Z's domain are integers 0...+10.  You can see for yourself this could break a transitive property in correlations."
statistics,3dxkqk,WinUnlimited,4,Mon Jul 20 12:36:24 2015 UTC,"This is not correct.  There is a restriction on the correlation matrix that it must be positive semi-definite.  i.e., there are no negative eigenvalues.  Here is the calculation on wolframalpha.com:  http://www.wolframalpha.com/input/?i=find+eigenvalues+of+[[1%2C0.7%2C-1]%2C[0.7%2C1%2C0.8]%2C[-1%2C0.8%2C1]]  Additionally, the determinant of the matrix is -2.25, which is less than 0.  Determinants of covariance matrices can be no less than 0, as a property of their semi-definiteness."
statistics,3dvqb6,kevo1001,2,Sun Jul 19 22:49:07 2015 UTC,For fracking?
statistics,3dvqb6,intothelionsden,1 point,Mon Jul 20 03:25:04 2015 UTC,"Not necessarily, however that is a common example of why you do directional drilling."
statistics,3dvqb6,intothelionsden,1 point,Mon Jul 20 13:50:27 2015 UTC,"But it is for oil though right? If so, I certainly could help, but am not going to on principle. Nothing  personal."
statistics,3dvqb6,poliphilo,1 point,Mon Jul 20 16:56:46 2015 UTC,"If you would like to apply this orbital trajectory then by all means be my guest. I'm not using this to profit, just applying a concept."
statistics,3dvqb6,iamanonion,1 point,Mon Jul 20 17:24:42 2015 UTC,"Seems like possibly a good fit for Bayesianism. Much depends on how you model the relationships between the different variables mentioned, and where you have sufficient data to specify their relationships.   One possibility is to set up a Bayesian network. Many examples use mostly categorical variables; you have mostly (or entirely) continuous variables, but the principle is the same.   It's difficult to say much more without understanding the variables and problem in more detail, but I thought I'd also suggest particle filters; there's a small chance they might be useful in this case."
statistics,3du2gq,blackrosethorn,4,Sun Jul 19 13:54:44 2015 UTC,"The expected counts change because you have less cells the counts can fall into. You are taking away cells, not number of observations in your study; they all have to go somewhere.  I'm guessing the 1.96 you threw out there was in order to determine the statistical significance of the chi-square test (right tailed .05 alpha from a normal distribution). However, the test statistic you will use in this case is the chi-square statistic with the corresponding degrees of freedom."
statistics,3du2gq,BigBucksGentleman,2,Sun Jul 19 14:34:22 2015 UTC,"Thanks for you answer !   Ah yes sorry, I read that there is output for the chi - square itself, and in this case it is significant, but I'm not sure that in order to determine where the significance actually lies in relation to the categories, if this is where we then look at the standardized residuals ?"
statistics,3du2gq,BigBucksGentleman,2,Sun Jul 19 14:48:02 2015 UTC,"Ok, so you are looking for post-hoc tests. Check this paper here for some approaches. The one you seem to be after starts at the bottom of page 2 under ""Calculating Residuals""."
statistics,3dry34,qball3438,5,Sat Jul 18 21:47:57 2015 UTC,The bullets in airplanes example is a great example of sampling bias put to good use.
statistics,3dry34,belarius,3,Sat Jul 18 23:56:48 2015 UTC,Doesn't this reflect perfectly the Bayesian attitude? That it's not as important to gather a straight probability as to gather the context for interpreting it. The priors are the important stuff. The context is what gives you an understanding in a larger context; not just what you're looking at but how you look at it. To understand your own view is fundamental to interpreting in an unbiased way.
statistics,3dry34,noman2561,2,Sun Jul 19 02:41:41 2015 UTC,"I don't mean for this to come across the wrong way, I am not advanced in statistics and am here to learn, but when you say ""priors are the important stuff"", I thought that the purpose was that you pick priors because you don't have enough data, but in the end, your posterior will be accurate because the number of samples will make the importance of the prior diminish to 0 in the limit. Doesn't this mean that the prior is not important or is only important until you have enough data?  BTW OP, this article is straight to the point and a good read."
statistics,3dry34,blueblob11,4,Sun Jul 19 03:43:58 2015 UTC,It's important to understand how the distributional choice and parameter value choices you make for priors can impact your inference. Here's a paper by Andrew Gelman showing how much your choice of prior distribution on variance parameters can impact your analysis. The TL;DR is that the choice matters.
statistics,3dry34,trousertitan,3,Sun Jul 19 16:09:18 2015 UTC,"At some point we have to acknowledge that we don't have all the evidence and possibly never will. Seldom can one make a perfect test in practice so the priors you chose can act as a scaffolding for your search for truth. That's why I call it context, because it's how you chose to interpret the data given your own ideas about what it's supposed to look like. I'm no statistics expert either but that's what I got from it."
statistics,3dry34,noman2561,2,Sun Jul 19 10:55:27 2015 UTC,"You are right that (in well behaved cases) the prior stops to matter in the large data limit. However, the point of using a prior is exactly to cheat and get closer to the large data limit by restricting your search space by using a soft contraint. So in most useful applications the prior matters and matters quite a bit  Imo it s best to think of priors as regularizers that have a slghtly more normative justification."
statistics,3dtn0b,dl_usf,3,Sun Jul 19 09:59:31 2015 UTC,"You need to know what type of variables your IVs and DVs are, categorical or continuous.  Then the stats website a UCLA will tell you what to do. http://www.ats.ucla.edu/stat/mult_pkg/whatstat/"
statistics,3dtn0b,master_innovator,2,Sun Jul 19 14:37:25 2015 UTC,"This sounds clearly like a regression problem to me, but you'll need to know more about your data to understand which sort.   Are you interested in interactions between your variables?"
statistics,3dtn0b,juular,1 point,Sun Jul 19 20:16:49 2015 UTC,"Sorry, to be more clear, there would be 7 independent categorical variables may or may not impact predicted consumer behavior. Also, I would like to see how well these factors predicted behavior compared to an actual measurement. Thanks!"
statistics,3dtn0b,TheFrogTrain,1 point,Sun Jul 19 21:57:44 2015 UTC,"I need more details. What is ""behavior"" in terms of units? Do you want to see if each individual variable impacts behavior, or do you want to see how they do relative to one another (does one do better than another etc), or both? And what are these variable - are they continuous or categorical?  Never hurts to provide too much information for your question."
statistics,3dtn0b,shadowwork,1 point,Sun Jul 19 19:58:38 2015 UTC,You may want to consider a cross-validation regression.  Here is a wiki that describes the general idea.  Of course a deeper look will be needed if this sounds good.
statistics,3dtn0b,KSUpsych,0,Mon Jul 20 01:05:08 2015 UTC,What?
statistics,3ds8z9,spottybod94,1 point,Sat Jul 18 23:30:44 2015 UTC,This was already posted to this sub less than 3 weeks ago with the exact same (not auto-suggested) title.
statistics,3dsmfk,cogsbox,3,Sun Jul 19 01:45:08 2015 UTC,"Multivariable calculus and linear algebra. Know integration and differentiation backwards and forwards. Depending on how in depth your theory courses are, you may want to brush up on Taylor series. Be comfortable with multiplying through matrices (or at least understanding how you would if you had to do it by hand)."
statistics,3dsmfk,Distance_Runner,1 point,Sun Jul 19 02:41:22 2015 UTC,Great! Thanks for the help.
statistics,3ds3wc,Kristian26,3,Sat Jul 18 22:41:36 2015 UTC,Probability of at least one = 1 - probability of none having side effects  1 - probability of none having side effects = 1 - (.65)8 = 0.968  probability of 6/8 having side effects = (8 choose 6) * (.35)6 *(.65)2 =  0.0217
statistics,3ds3wc,toadling,2,Sat Jul 18 23:07:55 2015 UTC,Thanks. I was having trouble setting up the binomial distribution formula.
statistics,3ds3wc,toadling,1 point,Sat Jul 18 23:11:14 2015 UTC,"No problem! Also if you wanted the probability of 6 or more having side effects just sum the probability of having 6, 7 and 8 having side effects using the same binomial probability formula."
statistics,3ds3wc,toadling,1 point,Sat Jul 18 23:18:04 2015 UTC,"One question, where did you get .65 from?"
statistics,3ds3wc,efrique,1 point,Sat Jul 18 23:22:11 2015 UTC,".65 is the probability of a patient taking the medication and not having the side effects: 1- .35 = .65  You can do this since the variable is dichotomous, aka there is only 2 cases- either the patient has side effects or he/she does not."
statistics,3dqsvw,Michaelpr,3,Sat Jul 18 15:43:18 2015 UTC,"Huh? One model is fit on increasing data, the other on decreasing. Do you really even need to get a p-value? Regardless, see here for comparing the slopes by hand."
statistics,3dqsvw,BigBucksGentleman,2,Sat Jul 18 16:14:31 2015 UTC,"Can you describe the experiment in more detail? You have two groups and appear to be getting counts on days 4, 10, 17 (and 30 for group 1 only). What are these counts of? Are there multiple samples in each group? What do your raw data look like?"
statistics,3dqsm9,piscoster,2,Sat Jul 18 15:40:47 2015 UTC,"It looks right. The null in the KPSS is stationarity. If your test statistic is greater that the critical value, then you reject the null."
statistics,3dr6hy,thumbandthebrain,3,Sat Jul 18 17:47:53 2015 UTC,"It sounds like when you hit a key, it takes 5-10 seconds for it to appear on the screen. I take that it isn't doing the same with other programs.   Windows 8 has a cool task manager (ctrl + shift + esc). You can ensure your processor and memory are not maxed out during the session.  Also, they have changed the gui stuff around in the newer builds. What build are you on?"
statistics,3dr6hy,Corruptionss,2,Sat Jul 18 21:11:41 2015 UTC,"Do you find that when you open up rstudio it is immediately laggy or is it after having used the program for an extended period of time?  Edit: If you find that the lag doesn't occur until later in use, then the program may have memory leaks and be accumulating memory over time, causing the program to slow down. Perhaps more likely is that you are storing large datasets in memory? The computer that you are using may not have enough RAM to support the types/scale of computation that you are doing. I've never personally used R or RStudio so I can't be sure, just speaking from experience in software engineering."
statistics,3dpp94,Poonsie,2,Sat Jul 18 06:21:47 2015 UTC,"You don't have your experiment results by group (a,b,c,d), only overall results?"
statistics,3dpp94,case9,1 point,Sat Jul 18 12:58:56 2015 UTC,"I do, I'm just not sure if I have to do individual tests or if the overall one is enough!"
statistics,3dpp94,master_innovator,2,Sat Jul 18 17:26:29 2015 UTC,"Use G*power, a free program from a German university to tell you if your sample size is enough. I'm sure you'll do a ttest and your sample is huge.  You're probably okay if your groups are split evenly."
statistics,3dpp94,m_dave80,1 point,Sat Jul 18 13:31:00 2015 UTC,"Thanks appreciate it, I'll check it out!"
statistics,3dn2l8,majortom721,14,Fri Jul 17 16:38:36 2015 UTC,"The (adjusted) r2 is not a good measure of predictive power of a model. Very generally: For prediction models, one usually splits the dataset into a training and a test data set. The model is fit on the training dataset and then tested how well it predicts the test dataset. This allows one to estimate how well the model will do ""in the real world"".  So it is not entirely clear what you mean by ""making the model better"". Improving the r2 doesn't necessarily mean that the model improves in predictive power. You may simply overfit your data. To select variables on the p-value or t-statistic is not a good strategy to improve the model. Here is a short list by Frank Harrell that addresses the problem using this approach.  Here are some suggestions:   You might consider looking at other models than multiple linear regression: regularized regression (lasso/ridge regression), machine learning algorithms (random forest, support vector machines, boosting etc.)  Very applied and good book on the topic: Applied Predictive Modeling. Free textbook on time series prediction by Rob Hyndman. As your data are time series, this might prove useful. It has a chapter on multiple regression. Consider posting your question on Cross Validated.  To improve the probability of a good answer, I'd include some graphics of regression diagnostics (residuals etc.)."
statistics,3dn2l8,COOLSerdash,2,Fri Jul 17 19:40:26 2015 UTC,"Upvote for Rob Hyndman's book. Also, Rob's ""forecast"" package in R sounds like it could easily handle this problem. I'd suggest investigating that, it's incredibly simple to use and Rob has tons of examples floating out there."
statistics,3dn2l8,wannaBePeterCampbell,1 point,Sat Jul 18 14:23:29 2015 UTC,The forecast package is simply fantastic. I must admit that I never tried forecasting with predictors before. But as you said: the documentation is great.
statistics,3dn2l8,COOLSerdash,8,Mon Jul 20 06:54:20 2015 UTC,"Honestly if I were working for a grocery store, a correlation matrix across all products is where I would start.  Then map items most frequently bought together to questions like ""what would predict these sales patterns.""  Going beyond correlation, you could do a principal components analysis or exploratory factor analysis to find how your products group in terms of sales.  You can then create index variables with these factors and use them as DVs in your regression.  Your IVs should be mostly independent of one another (you can factor analyze these too).  The big question is what events (IVs) predict increased sales in your product groups (DVs).  An Multivariate Multiple Regression would allow you to test all product groups simultaneously.    Similar to regression, a factorial ANOVA would help you show which product sales by group or category are different from one another.  The only difference is ANOVA provides an easier view to see differences in means.    It seems you don't really know why you are predicting things, or what your DV should be.  In the end you have to make a business decision... Statistics will model reality, but you need a reason to use the model in the first place.  If you are forecasting sales then I agree with the guy who said time series or ARIMA modeling.  This is basically a regression over time.  Does the average sales over the days of the week change based on different events... Best of luck!  I've worked with many companies and they don't even understand correlation well.  A regression may be fun to do, but useless to the business because they don't understand it.  How do you expect to convince them when you don't understand it completely either?  Not trying to be mean, but correlation and factor analysis may help frame a real question to attack."
statistics,3dn2l8,master_innovator,1 point,Fri Jul 17 21:32:21 2015 UTC,"I really appreciate your response, yes my dv is daily store sales"
statistics,3dn2l8,injectilio,1 point,Fri Jul 17 23:35:40 2015 UTC,This is the approach I would use. In addition to the very good ideas here you may want to check out a priori algorithms to form actionable rules to increase rev.
statistics,3dn2l8,AllezCannes,2,Sat Jul 18 02:48:50 2015 UTC,"a reasonable holiday bonus variable for outliers (an accurate pre-prediciton of whether we will be 10-20k (1), 20-30k (2), or 30-40k (3) variance for the day, and a holiday 'hangover' variable which is the (-1)*average(last three days of the holiday variable)   Are these variables created specifically from the amount of sales? If so, you shouldn't include that. You're only making sales data explain itself, which is unhelpful for predictive purposes. Instead, try to include those variables such as whether it's before a holiday, after a holiday, or neither. You also mentioned including the average of sales from prior weeks, and I'm not sure how helpful that really is.  Also, don't get rid of the rainy and payweek data because it doesn't make sense or is not a significant contributor. They could be helpful nonetheless.  It's difficult to provide help over a post, but I'd suggest reading this free book, specifically chapters 3, 5, and 6 to learn more on how to run this type of analysis with R."
statistics,3dn2l8,Adamworks,1 point,Fri Jul 17 18:17:29 2015 UTC,"Yes, however it's more of a learning variable for next year. And only applies to about 7 observations. And the hangover variable relies on the three past days, so that's not unrealistic. thanks for your input, I will look into your recommendations!"
statistics,3dn2l8,m1sta,2,Fri Jul 17 21:07:13 2015 UTC,"It looks like you are working with time series data. If you haven't done so already, you should look into how to transform your time series data to control for autocorrelation. It will inflate your r2 artificially. A traditional regression will be misleading without it."
statistics,3dn2l8,mrcodewizard,4,Mon Jul 20 16:50:39 2015 UTC,"guy/gal who just downvoted, do you have advice on what I did wrong in my post to have earned the downvote? I'd be happy for literally any advice in this situation."
statistics,3dn2l8,eoliveri,7,Fri Jul 17 17:10:08 2015 UTC,Some people are just jerks.
statistics,3dn2l8,bickbastardly,1 point,Fri Jul 17 20:34:42 2015 UTC,on Reddit? noooooo!
statistics,3dn2l8,msdrahcir,1 point,Sat Jul 18 08:56:36 2015 UTC,"You could consider breaking the sales numbers down into categories, like durable goods, non-durable goods, alcoholic beverages, etc., and predicting each one seperately.  I don't see why predicting gross sales would be very helpful."
statistics,3dn2l8,manofthewild07,1 point,Fri Jul 17 19:54:53 2015 UTC,"That's really in phase two, I have access to department sales but store leadership and customer service rely on store total sales."
statistics,3dn2l8,And_Ends_Battle,1 point,Fri Jul 17 21:08:23 2015 UTC,"So is your dependent variable daily sales for one store?  I think having daily dummies is a good idea. Game days, major holidays and seasonal stuff also. You should keep track of promotions and ads your store runs some how. You should also interact everything that might possibly make sense.  I'd be worried, however, that there could be auto correlation over time. There are test you can do and ways to deal with it, but it'll be a bit harder. Id recommend diebold's book on forecasting, it's not to long.   Sounds like you have a nice data set. You should probably get some good results."
statistics,3dn2l8,oryx85,1 point,Fri Jul 17 20:31:47 2015 UTC,"Thank you, and yes my y is daily store sales"
statistics,3dn2l8,And_Ends_Battle,1 point,Fri Jul 17 21:09:07 2015 UTC,"And I spoke with marketing but they had no meaningful activity or data for me, which was suprising"
statistics,3dn2l8,oryx85,1 point,Fri Jul 17 21:09:56 2015 UTC,"ARIMAX, and split your data into training/test set. Also, what is the value in predicting overall store sales based on variables out of your control? Is the aim to guide staffing?"
statistics,3dn2l8,Fogrocket,1 point,Fri Jul 17 22:13:47 2015 UTC,Will look into that thanks. Yeah labor goals and forecasting are determinants of leadership success
statistics,3dn2l8,oryx85,1 point,Fri Jul 17 22:31:32 2015 UTC,"Regarding your aside, you should be able to get hourly rain data for nearby stations from NOAA."
statistics,3dn2l8,Fogrocket,1 point,Sat Jul 18 02:26:00 2015 UTC,"Couple of things, one I think this is multivariate regression not multiple regression. The latter is when you have more than one outcomes you are trying to model together[edit: nope I am completely wrong, apologies]. Secondly you can't include some dummy variables from a category and not others. It doesn't matter if only a couple of days were significant, you put all the days into the model if the category as a whole is to be part of the model.   This is what I was taught recently when doing regression training in a medical context. I'm not an expert myself but these points were made very clear."
statistics,3dn2l8,lma21,2,Sat Jul 18 07:10:00 2015 UTC,Multivariate regression is when you have more than one outcome you are trying to predict. Multiple regression is when you have more than one predictor for one outcome.
statistics,3di8lv,mrestko,27,Thu Jul 16 14:10:00 2015 UTC,Look into discovering statistics with r by andy field. It's pretty stylized but it covers a lot of material.
statistics,3di8lv,buckhenderson,6,Thu Jul 16 14:45:27 2015 UTC,It's also hilarious.
statistics,3di8lv,ExtrasolarEarth,5,Thu Jul 16 14:52:23 2015 UTC,Worth every penny.
statistics,3di8lv,srkiboy83,3,Thu Jul 16 15:42:37 2015 UTC,"Does it do a good job of explaining the stats behind the tests too? I'm taking an ecological stats class this fall and I need to brush up.  The syllabus says to ""be familiar with basic hypothesis testing using t-test, anova, and simple linear regression"".  You think this will help?"
statistics,3di8lv,manna_tee,3,Thu Jul 16 17:23:44 2015 UTC,Yes it will.
statistics,3di8lv,eavc,3,Thu Jul 16 18:54:38 2015 UTC,"discovering statistics with r    As a bonus, Jeremy Miles, the second author, is active on Reddit :)"
statistics,3di8lv,engelthefallen,2,Thu Jul 16 21:02:38 2015 UTC,I think this might be the winner. Thank you for the recommendation.
statistics,3di8lv,GoldFisherman,2,Thu Jul 16 20:12:26 2015 UTC,Andy Field is amazing. You won't regret reading him.
statistics,3di8lv,MyDayJobSucks,2,Fri Jul 17 01:48:29 2015 UTC,Just purchased!
statistics,3di8lv,guynamedgrandma,2,Fri Jul 17 01:54:51 2015 UTC,I've used his SPSS book more than any other stats book.  He's  unique.
statistics,3di8lv,COOLSerdash,8,Fri Jul 17 01:57:12 2015 UTC,"I posted these links in other posts but maybe it's still helpful:   OpenIntro is a free basic statistics book including R labs on their website. Norm Matloff has a free book on statistics here. Even though it's intended for computer scientists, it's contents are really universal and basic (and it uses R too). See this post or this one for a compilation of freely available textbooks on statistics.   Here are some resources for learning statistics using R:   http://www.r-statistics.com/2009/10/free-statistics-e-books-for-download/ http://blog.revolutionanalytics.com/2011/11/three-free-books-on-r-for-statistics.html http://www.r-project.org/doc/bib/R-books.html   Then, these websites provide very valuable resources for doing statistics with R:   http://zoonek2.free.fr/UNIX/48_R/all.html http://www.cookbook-r.com/ http://www.statmethods.net/"
statistics,3di8lv,ctphoenix,3,Thu Jul 16 15:19:00 2015 UTC,An Introduction to Statistical Learning with Applications in R is awesome.
statistics,3di8lv,DontBendYourVita,2,Thu Jul 16 16:26:12 2015 UTC,"Looks cool but I'm not going to be doing statistical learning, just normal biomedical research comparing treatments and such."
statistics,3di8lv,DontBendYourVita,3,Thu Jul 16 20:11:17 2015 UTC,"I'm not sure how your university/hospital is set up, but I'm a biostatistician at a large research hospital. We do all of the stats for the research projects residents and med students do.   At the  universities, often the stats and biostats departments also have free consulting.   I'm mean the following in the nicest way possible: please consult a biostatistician/statistician with your project. The simple truth is you're busy and you won't have all the right domain knowledge. Sure you may learn how to do a 2x3 way ANOVA but what if that isn't the correct model given your data? The reality I've found is medical professionals learn the student's ttest and then think it can be applied to anything.   Investigate resources and use them. If they don't exist, come back here and give us a detailed summary and let us help you decide. Nothings worse than misapplying methods.    And it wouldn't be your fault. You're busy learning how to save lives. We were busy learning ways to crunch numbers. That's our thing.   Of course, still learn stats--read dalgaard, and then ISLR (intro to statistical learning using R)and many many more (I would recommend getting the basic understanding of survival analysis down so you understand why you can't use t-tests on such data. You say that you won't be using learning methods. The fact is the methods in there aren't the mystical machine learning methods you hear about Netflix using per se. There are many methods covered that really should be getting used in basic medical research.   I've rambled long enough."
statistics,3di8lv,truncatedusern,1 point,Fri Jul 17 02:20:55 2015 UTC,"I agree with everything you wrote, but it's not up to me.  I'd like to take you up on the offer of using this subreddit for more advice once I have concrete data collected."
statistics,3di8lv,MBaggott,1 point,Fri Jul 17 02:40:02 2015 UTC,"Please do. You can also message me directly, if that works for you."
statistics,3di8lv,ColorsMayInTimeFade,1 point,Fri Jul 17 02:55:11 2015 UTC,"This is a great book, but probably a little more advanced than what OP is requesting."
statistics,3di8lv,MBaggott,3,Sat Jul 18 18:54:06 2015 UTC,"Forgot to mention, I came across Introductory Statistics with R and was wondering if anyone here had experience using it."
statistics,3di8lv,ColorsMayInTimeFade,3,Thu Jul 16 14:31:24 2015 UTC,I like it and recommended it above. Dalgaard is a core R developer and is the guy naming each R release
statistics,3di8lv,MBaggott,2,Thu Jul 16 15:33:46 2015 UTC,Dalgaard is the guy who comes up with the release names?! They are always a big topic of discussion among my friends.
statistics,3di8lv,truncatedusern,1 point,Thu Jul 16 21:52:12 2015 UTC,"Yep, he's the guy responsible for the seasonal Peanuts references."
statistics,3di8lv,MBaggott,1 point,Fri Jul 17 04:27:38 2015 UTC,The names are peanut references? This just gets crazier.
statistics,3di8lv,pippo9,1 point,Fri Jul 17 12:00:50 2015 UTC,The comic not the food.
statistics,3di8lv,afatsumcha,1 point,Fri Jul 17 15:03:32 2015 UTC,"I highly recommend this book.  Along with Data Manipulation with R, Introductory Statistics with R is the book that I used to learn R.  It does a pretty good job of covering introductory statistics, but it might be helpful if you've had at least a little prior stats exposure (like an introductory undergrad course), but you should still find it accessible if you have no stats background.."
statistics,3di8lv,VermillionAzure,2,Sat Jul 18 19:00:07 2015 UTC,Peter Dalgaard's Introductory Statistics with R is a good book if you have no background. The resources online at UCLA are excellent for walking you through more advanced modeling.
statistics,3di8lv,belarius,1 point,Thu Jul 16 15:28:32 2015 UTC,"Beginning R by Mark Gardener is the best introductory book to R + Statistics, especially for those who have limited time."
statistics,3di8lv,FappingNowAMA,1 point,Thu Jul 16 18:31:46 2015 UTC,"I've been looking through R in Action. If you already know the stats tests that you want to run, this book might be helpful."
statistics,3dkimq,burnt_banana,2,Fri Jul 17 00:30:30 2015 UTC,"First, 'randomness' just means uncertainty in knowledge. If we knew all the causes and effects, very few things would be random. In your situations, though, you don't know any of the causes and effects in any of them, so they are all equally 'random'.   In A, you are in effect drawing numbers. In C, you are also drawing numbers. You could draw numbers explicitly, or you could draw names in some order. It doesn't matter, though, since each name/golfer/number corresponds exactly to a particular place in the draft order. All you're doing is adding some flair to it. You could write fleem, gobbledy, flurble, and tribbity-doo on pieces of paper and draw those out and order everyone by the number of syllables or something. In the end, it's all the same probability (and the same 'randomness') because the methods don't bias to any one person, because they are still simple drawing without replacement in a one to one mapping of order.  In B, it's the same thing. You (and the drawing process) have no effect on the outcome of the PGA. What does change is your perception, and probably the fun of watching the PGA. We can do a quick example to demonstrate this. Let's say there are four of you, so you draw the names of the top N PGA players.  Each PGA player has probability P_n (n = 1 to N) of winning. You have probability 1/N of picking any one of those players. Therefore, your expected value of winning is:  E[going first in draft] = Sum (n = 1 to N) P_n * 1/N   Since Sum (n = 1 to N) P_n = 1 (someone has to be first in the group of N players!), your expectation of going first in the draft is just 1/N, which is the same chance you have for situations A and C."
statistics,3dkimq,PhaethonPrime,3,Fri Jul 17 02:46:13 2015 UTC,Thank you very much for a simplified way of thinking about it for me :)
statistics,3dkimq,PhaethonPrime,1 point,Fri Jul 17 02:52:30 2015 UTC,No problem!
statistics,3dmehn,t_rex_tullis,2,Fri Jul 17 13:23:20 2015 UTC,"Another thing that is undervalued is not having a Ph.D. in statistics or biostatistics.    Should this be ""... that is overvalued is having..."" in reality? I don't feel like you can undervalue not having something unless having it is detrimental to the desired skill set. There are arguments that getting a PhD detracts from certain skills (a form of indoctrination, to some), but I don't think that's what the intent was here.  Perhaps a better terminology would be that we are undervaluing candidates that lack a PhD."
statistics,3dmehn,alwaysonesmaller,0,Fri Jul 17 13:43:13 2015 UTC,Another thing that is undervalued is not having a Ph.D. in statistics or biostatistics.    Perhaps a better terminology would be that we are undervaluing candidates that lack a PhD.   Or perhaps the author is pointing out that candidates with PhDs in other fields are being undervalued.
statistics,3dmehn,carmichael561,1 point,Fri Jul 17 14:46:32 2015 UTC,"From my perspective, I couldn't agree more, but I'm going to wait until after I get tenure before I start focusing on things other than JASA/AOS/JRSSB/Biometrika papers."
statistics,3dmehn,NOTWorthless,1 point,Fri Jul 17 13:40:51 2015 UTC,Don't forget Biometrics or Biostatistics :p ... I'd even throw Stats in Med in the mix if you're in medicine!
statistics,3dmehn,Distance_Runner,0,Sat Jul 18 04:45:13 2015 UTC,"Another thing that is undervalued is not having a Ph.D. in statistics or biostatistics.    He means that departments (University) weight too heavily the Ph.D; because they are focused on the wrong thing, rankings. Thus they are missing out on talent (non-Ph.D'd folks). IOW, they are missing a ""moneyball"" measurement.  Well Duh. That's nothing new. Universities are ""show ponies."" They don't produce anything. Even the value of their output (BA/S, MA/S, Ph.D) is questionable. And research? Please. Most of it is esoteric, nonsense.  Large organizations look for anything to make them feel safe in their decisions. Degrees do that (and publication), even more so in Academics.   Can you imagine a college drop-out heading up Harvard? But he can head up Microsoft."
statistics,3djkuw,toadling,3,Thu Jul 16 20:06:59 2015 UTC,"You should search for keywords when you look for jobs. Try searching ""simulation"" and ""Monte Carlo."""
statistics,3djkuw,webbed_feets,2,Fri Jul 17 03:58:16 2015 UTC,"Yea I have been trying ""simulation"", adding ""Monte Carlo"" is a great idea. Thanks!"
statistics,3djwe8,qball3438,3,Thu Jul 16 21:32:05 2015 UTC,Andrew and Bob are the feuding dudes. Three cases:   Andrew is one of the 3 men on the committee. How many ways can you pick 2 women of the 5 and 2 men of the 5 remaining (5 because you already have Andrew and can't choose Bob)? Bob is one of the 3 men on the committee. How many ways can you pick 2 women of the 5 and 2 men of the 5 remaining (5 because you already have Bob and can't choose Andrew)? Observe symmetry here with the previous case. Neither Andrew nor Bob are on the committee. How many ways can you pick 2 women of the 5 and 3 men of the 5 remaining (5 because you don't have Andrew or Bob)?
statistics,3dii23,hrb1979,7,Thu Jul 16 15:25:15 2015 UTC,"I'm always insulted by the cavalier attitude aspiring data scientists take towards learning statistics. Like in this article: ""Learn statistics: 57 hours of work."" OK, pack everything up. You're an expert now.  Statistics is a serious discipline. People spend their lives studying it. Even worse is that unless your well versed in statistics you'll make mistakes and have no idea. This isn't engineering where your bridge will collapse or your power grid will explode. You'll just give people nonsense results and be none-the-wiser.  I'll get off my soapbox now."
statistics,3dii23,webbed_feets,4,Fri Jul 17 04:05:48 2015 UTC,"I spent well over 57 hours per week learning about statistics my first couple years of grad school, and that was just coursework, not research."
statistics,3dii23,normee,5,Fri Jul 17 04:24:17 2015 UTC,And you didn't drop out after 2 weeks? You were practically an expert by then.
statistics,3dii23,webbed_feets,1 point,Fri Jul 17 04:38:54 2015 UTC,"The data science field is swamped with this attitude,which is why I don't take it seriously. It's like how 8 year olds all want to play for the Yankees. But now, as this analogy goes, everyone can download their own free copy of a stadium, put on a jersey, and call themselves a pro player. And then they start writing blogs about what p-values really mean."
statistics,3djj6r,bakersbark,1 point,Thu Jul 16 19:54:48 2015 UTC,"The student version of HLM software is free, and comes with examples and a manual:  http://www.ssicentral.com/hlm/student.html"
statistics,3dj505,riders994,3,Thu Jul 16 18:12:26 2015 UTC,"For the method you're describing, you would typically use either Fisher's exact test or a z-test. Fisher's exact test uses the binomial pmf and is more appropriate for small samples, while the z-test relies on the fact that the binomial distribution is well approximated by the normal distribution for large samples.  The beta distribution is commonly used in Bayesian inference when working with binomial data because of a couple useful properties - specifically, it's a continuous distribution defined on [0,1] and the conjugate prior of the binomial distribution. To grossly oversimplify the difference between frequentist and Bayesian inference, the frequentist approach relies on relatively lax assumptions but gives a result (specifically, the probability that you would observe the data that you did if A and B were equally good) that's only tangentially related to what you actually want to know, while the Bayesian approach relies on much stronger assumptions (the prior and its parameters) but gives you the figure that you care about (the probability that A is better than B) given those assumptions. Each approach is useful in its own right.  Edited to add: all the above aside, just based on what you're studying, you should also make sure that treating the data as binomial is appropriate. If you're comparing the proportions of people who purchased a product based on which ad they saw, for example, you can't use this methodology to account for people who purchased multiple or differently-priced products. Or if you're looking at click-through rates, it will ignore which people actually made purchases, or visited multiple times. Just make sure the only data you have and/or care about is actually binomial in nature."
statistics,3dj505,tf113,1 point,Thu Jul 16 19:57:40 2015 UTC,"The data that matters for the test (for client purposes) is binomial, so that's something that I definitely have taken into account in the past. Thank you so much for your reply!"
statistics,3dj505,efrique,3,Thu Jul 16 20:45:20 2015 UTC,"""events"" don't have the property ""statistical significance""; this is a category-error.   How are your A/B tests set up? What is the response (e.g. is it clicked/did-not-click? If so you would be looking at a discrete distribution like a binomial rather than a continuous one like a beta"
statistics,3dfumv,shnarfshnarfshnarf,1 point,Wed Jul 15 23:04:28 2015 UTC,You might want to repost this over in   https://www.reddit.com/r/publichealth  You will probably have better luck.
statistics,3dfumv,Case_Control,1 point,Thu Jul 16 18:38:57 2015 UTC,"I'm currently majoring in math and stats and doing some unpaid work for an NGO this summer. There's a fair amount of statistics work being done by places like the UN that you could look into. I would recommend browsing various NGOs on the internet and contacting them to see if they have positions open for someone with your skillset. Just for future career advice, you may want to consider pursuing a graduate degree in public health or biostatistics if you are interested in making a career out of global health or development. From what I understand it's a lot easier to get jobs in the field if you have a graduate degree."
statistics,3dh124,efrique,2,Thu Jul 16 05:14:13 2015 UTC,"What's the normative standard underlying ""should"" there? (""Should"" according to whom?)"
statistics,3dh124,efrique,2,Thu Jul 16 05:31:42 2015 UTC,i guess how should the course be designed so the students are prepared to succeed in the rest of their coursework and in their careers? i basically want to know what is typically in a standard MBA stats quarter long class.
statistics,3dh124,m_dave80,1 point,Thu Jul 16 05:39:53 2015 UTC,"typical for where? ""standard"" by which standards?  I could discuss a class I taught in the 1990s in one country ... but that might not be at all typical for what you really want to know about"
statistics,3dglh4,Brighteye,3,Thu Jul 16 02:46:11 2015 UTC,"How does the q-q plot for the studentized residuals look? As long as the residuals are normally distributed, OLS should be fine (in all honesty this can be violated to some degree as well). You could also apply a box-cox transformation to make it a little nicer. It might also be helpful to remove some outliers using Mahalanobis distance or something similar."
statistics,3dglh4,BigBucksGentleman,3,Thu Jul 16 03:24:23 2015 UTC,"What are we, mind readers? What is that a plot of? If it's a residual plot of some kind, what kind, and what was the model?"
statistics,3dglh4,efrique,1 point,Thu Jul 16 05:39:12 2015 UTC,You could also try bootstrapping if you are unsure of the right distribution to use.
statistics,3dfuwz,eugene447,2,Wed Jul 15 23:06:33 2015 UTC,"edit: clarification: I am conserving the same frequency of 0s and 1s in my columns, it's just the order that is changing   Just to be clear, are you changing the order for both columns at the same time (and in the same way) when you do this? It won't work if you change the order for one column but not the other since then you've actually changed your data.  Also, with the info you put here, I would re-consider doing this particular test. Your measurements are before/after cholesterol levels, which are continuous values on the usual mg/dL scale probably? By binarizing these numbers into too much vs. not too much cholesterol, you're throwing away a lot of information (e.g. magnitudes of changes, how far above or below the normal line they each started). I don't know what you're trying to show exactly but I wouldn't want to ignore all that information without a good reason."
statistics,3dfuwz,normee,1 point,Wed Jul 15 23:35:20 2015 UTC,"Hi.   I changed the order for 1 (i only wanted to know if it changes anything to the resutls), but I am just wondering why the P-value is even changing. Doesnt Fisher's exact test only test for frequency (number)? If the number of 0s and 1s in each column doesn't change, why would the p-value change?  As for your 2nd point, hyperlipidemia is an end point of the experiment. We have a cutoff and we're just looking at values above and below that. We are also looking at a bunch of other parameters but this one (with Fisher) is giving me a big headache."
statistics,3dfuwz,normee,1 point,Wed Jul 15 23:47:19 2015 UTC,"Fisher's exact test tests the assumption of independence of your rows and columns in a 2x2 table. You fill in cells in that 2x2 table by looking at your data and seeing how many observations were (col1, col2) = (0,0), (0,1), (1,0), and (1, 1), and so it's not just the column totals that matter, but the total of each combination. The p-value tells you how unlikely that configuration of the 2x2 table was under the assumption that there's no relationship between your dichotomized before measurements and after measurements. If you change one column and not the other, you have a fundamentally different 2x2 table that may be more or less likely to be consistent with the null independence assumption, giving you a different p-value. Permuting one column destroys the link between each person's before and after values by mixing around all the before values while leaving the after ones untouched. So you should definitely not do this, or expect to get the same p-value from doing so."
statistics,3dfuwz,normee,1 point,Thu Jul 16 00:00:47 2015 UTC,"thank you very much! This is exactly what I've been looking for for hours!  So, in your expert opinion, would you go with the SPSS results or online calculator results? I get a significance in one but not the other. I don't want to rig the results. The online calculator does not take into account the relation between the 2 columns."
statistics,3dfuwz,normee,1 point,Thu Jul 16 00:05:31 2015 UTC,"If they are giving different results on the exact same data, that is concerning, though I'd trust SPSS more than Joeblow Stat's Questionable Java eFisher Applet. I wouldn't be bothered by a difference like 0.167 vs. 0.168, but I would by 0.067 vs 0.0168. If that persists, first, I would make sure the data are correctly input for both programs. I would also make sure you are doing the test in the same way: there would be a big difference for one-sided vs. two-sided, and SPSS and the online calculator may use different defaults. If after checking these things they still differ by a lot, then I would try other online calculators or the fisher.exact() test in R. If everything is working correctly, these should all give you about the same p-values, but one differing a lot from the rest suggests that you should not trust that software. (Hopefully that is not an indictment of SPSS because that would screw a lot of researchers.)  edit: Just saw that you wrote:   The online calculator does not take into account the relation between the 2 columns.   What does it have you input? If it's entries you get by summarizing your data as a 2x2 table that's okay, but if it only requires row and column totals, then I don't see how it can possibly be working."
statistics,3dfd3c,jewami,2,Wed Jul 15 20:54:10 2015 UTC,You already know that they have the ace of spades because you asked and they said yes. So you need to find the probability of one of the remaining three aces being among their remaining 12 cards.
statistics,3dfd3c,Doc_Nag_Idea_Man,2,Wed Jul 15 21:36:17 2015 UTC,"Your initial problem can be reworded as ""what is the probability of having one or more aces, if being dealt 12 cards out of a 51 card deck that lacks the ace of spades?""  So pretend you're dealing this 51 card deck to four players, including yourself.  You start off dealing to the person on your left and go clockwise.  When you're finished, they all have 13 cards, you have 12, and there are only 3 aces to go around, right?  Add up the chances of getting each ace and subtract the probability of getting 2 and 3 of them.  The calculation is here."
statistics,3dfd3c,thisguyisadumbass,2,Wed Jul 15 23:34:59 2015 UTC,"This was very intuitive, thanks!"
statistics,3dfd3c,WolframAlpha-Bot,1 point,Mon Jul 20 19:13:34 2015 UTC,Input  3×12/51-(3×12/51×11/50-12/51×11/50×10/49)   Exact result  11686/20825   Decimal approximation  0.5611524609843937575030012004801920768307322929171668...   Repeating decimal  0.5611524609843937575030012004801920768307322929171668... (period 336)   Number line  Image  Egyptian fraction expansion  1/2+1/17+1/430+1/298492+1/267292123700   Percent decrease  (3 12)/51-((3 12 11)/(51 50)-(12 11 10)/(51 50 49)) = 11686/20825 is 20.50%  smaller than (3 12)/51 = 12/17.     Delete (comment author only) | About | Report a Bug | Created and maintained by /u/JakeLane
statistics,3dfd3c,WolframAlpha-Bot,1 point,Wed Jul 15 23:35:34 2015 UTC,Input  3×12/51-(3×12/51×11/50-12/51×11/50×10/49)   Exact result  11686/20825   Decimal approximation  0.5611524609843937575030012004801920768307322929171668...   Repeating decimal  0.5611524609843937575030012004801920768307322929171668... (period 336)   Number line  Image  Egyptian fraction expansion  1/2+1/17+1/430+1/298492+1/267292123700   Percent decrease  (3 12)/51-((3 12 11)/(51 50)-(12 11 10)/(51 50 49)) = 11686/20825 is 20.50%  smaller than (3 12)/51 = 12/17.     Delete (comment author only) | About | Report a Bug | Created and maintained by /u/JakeLane
statistics,3dfd3c,thisguyisadumbass,1 point,Wed Jul 15 23:29:39 2015 UTC,"Weird, when I clicked the link I posted, the answer was wrong, so I deleted and resubmitted... but this bot picked up the right link somehow.  I'm sure I screwed up somewhere but I don't know where.  That's what these [deleted] comments are, if anyone is wondering."
statistics,3dc0iu,LotusEyes92,6,Wed Jul 15 02:24:28 2015 UTC,"Suspect people are downvoting in part because they don't understand what ""do a paper"" means in your kiwi dialect of English. Translated for American audiences, your query would read something like: ""given a choice between taking a course and doing a project on Bayesian methods or taking a course and doing a project on R, which would you recommend?"" My kiwi English is a little rusty and my understanding of the NZ higher education system is a little murky though, so my translation may be a bit off. Now excuse me, I'm off to buy a growler of beer and carry it home in my fanny pack. ;-)"
statistics,3dc0iu,kiwipete,2,Wed Jul 15 17:28:07 2015 UTC,Haha oh right. Gosh I'm so used to this lingo that it didn't even cross my mind. Silly me.
statistics,3dc0iu,kiwipete,2,Wed Jul 15 18:15:11 2015 UTC,"Auckland?  I'd suggest R. Honestly, Bayesian methods will are great, but you'll need to code no matter what statistical epistemology you follow."
statistics,3dc0iu,BadSoles,1 point,Wed Jul 15 04:40:01 2015 UTC,"Yupp! True, thanks for your advice :)"
statistics,3dc0iu,normee,2,Wed Jul 15 05:19:45 2015 UTC,As an American with Kiwi friends I just thought I'd check in to sympathize with the confusion/potentially angry posts you'll get from the dialect thing :)
statistics,3dc0iu,beaverteeth92,1 point,Fri Jul 17 23:03:46 2015 UTC,"EDIT: Can someone tell me why exactly this post is being downvoted? Am I in the wrong sub?   Probably because your question doesn't make sense and doesn't provide enough information for someone to begin helping you. Are you picking an existing research article and doing something with it, or writing your own paper on one of these topics? What does it mean to do a paper on R ""taught by the person who developed R""? What are the requirements and constraints? How long do you have to do this? etc."
statistics,3dax0r,Hellkyte,1 point,Tue Jul 14 21:22:28 2015 UTC,"I suppose you never heard of a star design?   Both the first four and the seven points do not do interactions. However the seven points do effect such as x12 x22 and x32  I am not sure you can say trials are orthogonal the way you try to do. Vectors can be orthogonal, points not."
statistics,3dax0r,wiekvoet,1 point,Wed Jul 15 18:30:55 2015 UTC,"Central composite design right?  Yeah I'm familiar with those.  This would be like a CCD but without the cube, just with the axial points.  My understanding is that the first 4 points would show interaction effects x1*x2, but the 7 wouldn't.  Vice versa for x2 x3.  ED:  crap I'm thinking 4 points for 2 factors.  For 3 factors I know what you mean.  Neither would show interactions.    And yeah I guess the way I'm using orthoganal here is confusin since it is more related to aliasing/confounding in DOE.  Seems like you know what I'm talking about regardless."
statistics,3da8v7,milkstake,1 point,Tue Jul 14 18:38:33 2015 UTC,"You should code each condition 0 (absent) or 1 (present) for each subject so you have a binary matrix of subject X condition.  Once you have this there are a number of ways to progress. I don't think k-means/medioids is recommended with binary data.  This post suggests any of heirarchical clustering, two-step clustering or 'transforming' the data to continuous by first running a PCA and then clustering the continuous components.  Alternatively from a data-mining POV, this looks like a nice way to do it."
statistics,3d7dto,ScienceandVodka,7,Tue Jul 14 02:28:59 2015 UTC,Because the exponentiated value is a multiplicative term related to the outcome. See here for more info.
statistics,3d7dto,BigBucksGentleman,1 point,Tue Jul 14 03:06:25 2015 UTC,"Ah great, thanks. So does a value ""1"" for heartperf mean ""1 above the lowest possible value?"" I ask because if not, and if it actually means ""1"" then that seems a bit strange, since 1 is the worst heart performance you can possibly have yet per the formula it would still decrease the intercept (11 irregular heartbeats) value by 25%. (level of ""2"" would decrease it by ~45%)"
statistics,3d7dto,BigBucksGentleman,1 point,Wed Jul 15 09:09:47 2015 UTC,"If you post your regression output (all coefficients), we can give you a much better interpretation of the model."
statistics,3d7dto,BigBucksGentleman,2,Wed Jul 15 11:49:05 2015 UTC,"Estimate     (Intercept)   2.3992       Heartperf    -0.2994      Heartfail1    0.3657    I'm guessing it can be thought of as follows (let me know if I'm wrong!): The intercept is exp ^ 2.4 = 11 irregular heart beats, which is meaningless since the lowest possible value for heartperf is ""1"".   So someone with ""1"" for heartperf is expected to have: exp(2.4 + -.299 * 1) = ~8 irregular heartbeats (makes sense, ~25% less than 11)  Someone with a ""2"" for heartperf has ~6, again a 25% reduction.  Then there's the heartfail variable which either has no change (heartfail=0) or increases the response by 44% (heartfail=1) since exp ^ .3657 = 1.44"
statistics,3d7dto,Bayesbayer,1 point,Wed Jul 15 20:11:31 2015 UTC,Sounds like you got it!
statistics,3d7dto,anzhili,1 point,Wed Jul 15 21:04:12 2015 UTC,May I also ask how exactly I could do model diagnostics for this? It's actually a mixed model by the way since some patients were measured up to 3 times - so I have heartperf and heartfail as fixed effects and the actual patient as the random effect.
statistics,3d7dto,Bayesbayer,3,Fri Jul 17 05:22:34 2015 UTC,"the model is multiplicative, not additive, since you are using exp as the response function:  E(response|covariate) = exp( intercept +  covariate * coefficient) = exp(intercept) * exp(covariate*coefficient)  So in your example, for every increase of ""irregular hearbeats"" by one unit, the expected value of ""heart performance"" decreases by 25 % (i.e, by a factor of .075)."
statistics,3d7dto,FullSharkAlligator,1 point,Tue Jul 14 09:56:46 2015 UTC,This is a good answer.
statistics,3d7dto,dasonk,1 point,Tue Jul 14 17:09:33 2015 UTC,"Indeed! But does a value ""1"" for heartperf mean ""1 above the lowest possible value?"" I ask because if not, and if it actually means ""1"" then that seems a bit strange, since 1 is the worst heart performance you can possibly have yet per the formula it would still decrease the intercept value by 25%. (level of ""2"" would decrease it by ~45%)"
statistics,3d7dto,efrique,1 point,Wed Jul 15 08:48:57 2015 UTC,"My response variable is ""irregular heartbeats"" - did you mean for it to be the other way around? Heartperf is a predictor variable which has an intercept of -.3.   Also - does a value ""1"" for heartperf mean ""1 above the lowest possible value?"" I ask because if not, and if it actually means ""1"" then that seems a bit strange, since 1 is the worst heart performance you can possibly have yet per the formula it would still decrease the intercept value by 25%.  (level of ""2"" would decrease it by ~45%)"
statistics,3d7dto,efrique,2,Wed Jul 15 08:36:44 2015 UTC,"ok, sorry for switching up the two quantitites. first, let's clear up some terminology. predictor variable don't have an ""intercept"", they are associated with a regression coefficient. You don't need really need to know the value of the global intercept in order to interpret a regression coefficient.  so your model is irreg_heart| heartperf ~ Poisson(lambda) where      lambda = E( irreg_heart| heartperf )           = exp(intercept + coefficient * heartperf)           = exp(intercept * -.3*heartperf)           = exp(intercept) * exp(-.3 * heartperf)   so lets say you have two patients, one with heartperf=x and one with heartperf= x +1.   Your model can quantify what the relative difference in their expected irreg_heart is:    E( irreg_heart | heartperf = x +1 )/E( irreg_heart | heartperf = x )  =  exp(intercept) * exp(-.3 * (x + 1)) / (exp(intercept) * exp(-.3 * x)) =  exp(intercept) * exp(-.3 * x)  *  exp(-.3 * 1) / (exp(intercept) * exp(-.3 * x))) =  = exp(-.3)  = .75    So, the interpretation is: ""In a model with a log link function, a coefficient value of -.3 means that an increase of the predictor variable by one unit is associated with a decrease of the expected response by a factor of exp(-.3) = .75""    Also - does a value ""1"" for heartperf mean ""1 above the lowest possible value?"" I ask because if not, and if it actually means ""1"" then that seems a bit strange, since 1 is the worst heart performance you can possibly have yet per the formula it would still decrease the intercept value by 25%. (level of ""2"" would decrease it by ~45%)   No , it means a change by one unit as in the derivation above.  You are always implicitly comparing two hypothetical patients and the coefficient tells you about the expected change between the two: As you see in the formula above, the coefficient tells you about the effect of changing your covariate by one unit -- you model assumes that going from 0 to 1 will result in the same change in the response as going from 1 to 2 or from 7 to 8. But if at least one of the values you are comparing is outside of the range of observed/observable data, that of course is an interpretation that is at least dangerous or even nonsensical. Going from 1 to 2 on your irreg_heart scale induces a change by a factor of .75. Going from 0 to 1 is not possible because 0 is not defined on your scale. Going from 1 to 3 on your scale will induce a change in expected heartperf by a factor of .75 * .75, about .56."
statistics,3d7dto,efrique,1 point,Wed Jul 15 09:42:27 2015 UTC,The intercept defines the baseline.  Negative coefficients reduce the predicted response outcome from the baseline.
statistics,3d7dto,efrique,0,Tue Jul 14 05:27:24 2015 UTC,"e-# doesn't reduce anything because it always comes out positive, that's the point of the OP"
statistics,3d6be8,imherejusttodownvote,3,Mon Jul 13 21:30:18 2015 UTC,"You could consider using a sandwich estimator to address the heteroscedasticity or Kendall's tau here. Both have distributions of p-values that appear to be closer to uniform in your example than the one from standard regression, though not perfect (either tends to reject null not often enough vs. standard tends to reject too much).  library(sandwich) do_one <- function() {     x <- rnorm(100)     y <- x*c(-1,1)     mod <- lm(y~x)     p_lm <- summary(mod)$coef[2,4]     sand_SE <- sqrt(vcovHC(mod, type=""HC0"")[""x"",""x""])     z_sand <- coef(mod)[""x""]/sand_SE     p_sand <- pchisq(z_sand^2, 1, lower.tail=FALSE)     kendall <- cor.test(x, y, method = ""kendall"")$p.value     return(c(""lm""=p_lm, ""sand""=p_sand, ""kendall""=kendall)) } p_comp <- replicate(10000, do_one()) plot(ecdf(p_comp[""lm"",]), main=""Empirical CDFs"") lines(ecdf(p_comp[""sand.x"",]), col=""red"") lines(ecdf(p_comp[""kendall"",]), col=""blue"") abline(a=0,b=1, lty=2) # reference line for uniform CDF legend(""bottomright"",c(""lm"",""sand"",""kendall""),col=c(""black"",""red"",""blue""), lty=1)   (full disclosure: having weird bug with plotting the Kendall's tau p-value CDF where it always shows up as a thick line on the plot even if I change the lty/lwd but don't know why, might not be problem for you)"
statistics,3d6be8,normee,2,Tue Jul 14 00:58:56 2015 UTC,"This is great.  I think the sandwich estimator is what I'm looking for.  Thank you so much, really appreciate it!"
statistics,3d6be8,CassanovaFrankenstei,2,Tue Jul 14 03:52:33 2015 UTC,"trying to be helpful  There are a few things odd about the way you word your question. 1. Uncorrelated dependent-independent variables don't make sense-- do you mean your Gauss-Markov assumption uncorrelated independent variables?  2. Why are you worrying about type one errors? Statistics are set up to limit type 2 errors.  In fact H0: x=y=0 IS set up to minimize a type 2 error.  3. If your eq is correct,  with a probability of .5 then your model won't work because you are measuring randomness. If something is just as likely to be -1 as +1, then the model is unbiased but random.  Does this make sense? Is like flipping a fair coin."
statistics,3d6be8,shaggorama,2,Mon Jul 13 23:43:34 2015 UTC,"Thanks for your response.   I'm not exactly clear on why uncorrelated X and Y don't make sense.  In my example, I was trying to give something where E(Y|X)=E(Y)=0.  Also, the Xi are iid. I'm worried about type I errors because I want to show that P(reject|null)=alpha.  The reason for this is that it will demonstrate that approach is valid in that sense (the type II error will be far more difficult to demonstrate)   Yes, the way the example is set up is exactly to measure randomness.  I want to show that with random data, we only reject the null with probability alpha.  When there is some true effect present, we hope to measure the correlation between X and Y."
statistics,3d6be8,Jimmy_Goose,2,Tue Jul 14 02:28:20 2015 UTC,Great username there
statistics,3d6be8,normee,1 point,Tue Jul 14 12:55:02 2015 UTC,"The n*R2 only works if you are sampling from a linear model (you aren't).  If you want to measure association between two random variables, use correlation."
statistics,3d6be8,Jimmy_Goose,2,Tue Jul 14 02:56:02 2015 UTC,"Testing Pearson correlation = 0 is equivalent to testing slope = 0 in a univariate regression model, which is what OP was doing. You get the same p-value."
statistics,3d6be8,normee,1 point,Tue Jul 14 03:03:43 2015 UTC,There are more correlations than just pearson...
statistics,3d6be8,Jimmy_Goose,1 point,Tue Jul 14 03:35:06 2015 UTC,"Certainly, and I had actually suggested using Kendall's tau coefficient. Spearman's rho is another possibility. I was responding to your point because when people say ""correlation"" without any qualifiers, Pearson is usually what they mean, but that option won't work here."
statistics,3d28ip,jlongr,47,Sun Jul 12 22:56:49 2015 UTC,"TOOLS  Learn Excel very well.  A lot of people may disagree, and I can see where they are coming from.  However, like it or not, Excel is the most ubiquitous data analysis tool and you will always have to use it.  So learn it well.  Learn programming in either R or SAS (or both).  SAS costs a lot of money but has been around for a very long time so a lot of businesses use it.  R is the new hotness and a lot of companies are switching or using it concurrently. Pick one to learn first (probably R since it's much easier to get your hands on).  There are too many visualization and report tools.  Instead of trying to master one, learn the general concepts of how to create good data visualizations. Pick up Stephen Few's Show Me the Numbers to learn how to create visualizations.  Read it cover to cover.  Take notes.  This man knows what he's talking about.  Later, you may want to dig into ""Big Data"" technologies, but for now, this should cover most of what you need.  Caveat: you will get people here talking about Python, which is also an excellent programming language to learn.  It would not be a waste of time to learn python as well.  CONCEPTS  99% of your job will be (1) data wrangling and (2) data quality.  The 1% of your job is actual analysis, modeling, and reporting.  For the data wrangling, that's where your tools come in.  Excel, SQL, R, *nix tools, whatever.  If you're faster and better with your tools, this part goes faster.  For the data quality, you need to learn to be skeptical.  Don't just accept the answers that your model spits out.  Are these results intuitive?  If they're not, why are they not?  Where is there bias?  Can you look at this a different way?  Does this align with previous results?  Do other sources say the same thing?  If you don't answer these questions, I guarantee you the business will.  Also, don't be afraid to look at your data.  Graph out a quick histogram.  Trend out your results over time.  Look for outliers.  Hell, there are times where I printed out the tables/logs/data and spread them out in a conference room to hunt down something.  Don't be afraid to get your hands dirty.  Oh yeah, and sometimes you'll do math, stats, and shit."
statistics,3d28ip,flipstables,10,Mon Jul 13 00:11:04 2015 UTC,"Just want to mention that SAS University edition can be downloaded, installed, and used for free. It's just Base SAS but might be good for learning basics."
statistics,3d28ip,MJGSimple,9,Mon Jul 13 00:24:54 2015 UTC,"For the data quality, you need to learn to be skeptical   As a manager of an analytics team, I must say this is the trait in my analysts I value above almost everything else, and also find hardest to cultivate. Skepticism is a skill and a habit of thought and takes serious practice to master. All is for naught as an analyst if you don't align yourself with this idea"
statistics,3d28ip,nerdsarepeopletoo,2,Mon Jul 13 03:11:22 2015 UTC,"This is a really great answer!  I'd like to throw Tableau into tools also...  It can do some beautiful things, latest version has a surprisingly powerful set of functions. I have been using it on a couple of data sets with >30 Mill rows comfortably (Excel's limit is 1M)."
statistics,3d28ip,trip_2020,2,Mon Jul 13 13:18:02 2015 UTC,"+1 for excel.  Pretty much everyone uses it.  One advantage for R (or python) over SAS is it's free and you won't have to write a business case to convince your boss to let you use it.  (Unless of course your organization already has SAS, in which case learn to use the tools they provide)  /edit: this is a super thorough answer!  Nice work @flipstables!"
statistics,3d28ip,pandemik,1 point,Mon Jul 13 00:43:58 2015 UTC,"Fantastic response! Thank you for that book suggestion, I will definitely check it out. I recently got Data Smart by John Foreman, which primarily uses Excel to show you how to implement classic methods like clustering and naive Bayes, and later using R to do the same, so I agree with your point about not unfairly dismissing it as an analytics tool.   Furthermore, in my short time at work I've already encountered the continuous battle of data cleaning, so its good to hear that confirmed as part of the job :). Something I've been working on is building a data mart in preparation for some analysis, and I can already tell it is going to be an invaluable tool in taming messy data."
statistics,3d28ip,f4gb4lls,-2,Mon Jul 13 02:30:30 2015 UTC,"Looking at the data does not obey the likelihood principle, and thus is not something that rational statisticians should do."
statistics,3d28ip,addfunr3,7,Mon Jul 13 03:41:23 2015 UTC,R (actually RStudio). It's quickly become the most important program that I use at work as it can do a whole range of things with a few well made commands. It can be a bit difficult at first but is well worth learning.
statistics,3d28ip,tigereyes69,2,Mon Jul 13 00:02:34 2015 UTC,Suggestion for favorite packages?
statistics,3d28ip,addfunr3,6,Mon Jul 13 00:54:23 2015 UTC,"Shiny - build easy web apps. Coding is a bit finnicky but results are so worth it. GGplot2 - Fantastic for making graphs with endless customisation features, you can either control every aspect of the graph or let it do the work. SQLDF - Allows you to perform SQL style commands on dataframes,  sometimes simpler than learning slabs of R code.  Last one is not for statistics at all but one that I use every time I open R: beepr. Set it on the last line of code to make a sound when it's run and done. I tend to run code and switch to other work so it's great to have something aleart me when it's done."
statistics,3d28ip,tigereyes69,1 point,Mon Jul 13 01:27:41 2015 UTC,"I'm decently experienced with ggplot and I keep promising myself I will learn Shiny.   beeper though, that's an excellent suggestion!"
statistics,3d28ip,kevjohnson,4,Mon Jul 13 01:29:50 2015 UTC,"All of Hadley Wickham's packages are a must (ggplot2, plyr, dplyr, lubridate, reshape2, scales, stringr).  The forecast package is really nice and full of features for all levels.  The igraph package for networks.    The caret package is essential to me.  It provides a ton of tools for streamlining the model building process.  It's great for people who don't just want a function that takes in data and spits out a model.  The train() function is highly generalized and useful in so many situations.  Others include kernlab, randomForest, glmnet, and tree."
statistics,3d28ip,tf113,1 point,Mon Jul 13 04:18:38 2015 UTC,"On top of the other (excellent) ones other people have mentioned:   magrittr for the pipe operator (x %>% f(...) is equivalent to f(x, ...), and x %<>% f(...) is equivalent to x <- f(x, ...)) XLConnect, which lets you modify and import .xlsx files within R RODBC, which is pretty much exactly what it sounds like"
statistics,3d28ip,addfunr3,1 point,Mon Jul 13 04:32:50 2015 UTC,"How do you stage the data before feeding it into R? For example, in a CSV, by integrating with SQL, etc."
statistics,3d28ip,shaggorama,2,Mon Jul 13 02:54:51 2015 UTC,"I receive all my data from other sources, so generally whatever they give it to me as. I typically work with CSV (read.csv), but sometimes get Excel files (read_xlsx). Unfortunately, work won't support Java packages (some bs updating problem) so everything I export has to be CSV."
statistics,3d28ip,HughManatee,8,Mon Jul 13 04:34:12 2015 UTC,"SQL.  I used to work as a data analyst and I spent every day neck deep in SQL. Now I'm a data scientist (with an MS in stats) and it's amazing how poor my colleagues SQL skills are. Pretty much across the board. I get recruited all the time to help them with fairly simple queries.   It's a fairly simple skill, easy to learn, and it is widely applicable to basically any job where you will touch data.  Additionally, excel pivot tables were also bread and butter during my data analyst days.   If you have the time to learn R and/or python they are also immensely powerful and beneficial but as a data analyst way less necessary than SQL and excel skills."
statistics,3d28ip,shaggorama,2,Mon Jul 13 14:01:00 2015 UTC,"Those are all useful, but joins can create sneaky issues, so be cognizant of those.  If you're planning on working with big data, tyou might want to learn about explain plans, pass-through queries, and correlated subqueries.  Syntax for SQL within a DBMS can become important as well."
statistics,3d28ip,case9,1 point,Mon Jul 13 17:59:27 2015 UTC,There really isn't a lot of syntax to learn with SQL. Really what you need to learn is:   How different kinds of joins work and can be combined How to ask fairly complex questions from the database How different query design decisions will affect the query's performance   I recommend finding some kind of demo database somewhere and just play with it. Start with simple queries and try to come up with increasingly complex questions to ask the database.
statistics,3d28ip,trip_2020,1 point,Tue Jul 14 02:31:38 2015 UTC,Thanks!
statistics,3d28ip,failuretomisfire,4,Tue Jul 14 03:16:35 2015 UTC,"In my experience there are 3 major things which distinguish great data analysts from mediocre.   Analyst Intuition Experience Production Habits (need a better name for this)   Not a lot I can give you on 1. or 2. here, so I'll try and talk about what I believe is often underrated 3.  When working in a commercial environment (and most others), being a great analyst is not just about producing great analysis, but also in turning it around swiftly.  A lot of the tasks involved with of Data analysis lend themselves very well to scaling - investing more time early in the process to be returned with significant interest later.  Some examples of this are:    Writing well structured, re-usable code (and commenting it well) Coding for the general case, rather than specific. If you are using Excel, don't pull out static data points - structure the data properly in a table and run pivots from that table. If using SQL - use variables where possible   There are hundreds of these examples - but all based around the same concept of investing early to set up efficient systems that can be re-used and scaled later.  When you have these structures in place, your work will be:   A hell of a lot faster Less prone to errors Higher quality insight - as you are spending your time analysing rather than manipulating.   Now, unfortunately this isn't a simple case of being able to say ""Always do this"" - because sometimes building a scalable data structure is a HUGE waste of time if you are never going to use it again.  But at least being cognisant of the concept and thinking about where/when it should be applied will help you become a great analyst."
statistics,3d28ip,decimated_napkin,1 point,Tue Jul 14 03:34:27 2015 UTC,"This sort of big picture approach to coding is something I must teach myself. I've never had a software engineering course so it is a bit challenging, but I'm learning how to implement a data warehouse and use SQL in a way similar to Excel's pivot tables, which is a start.  Thanks for the advice!"
statistics,3d28ip,mythstified,1 point,Mon Jul 13 13:11:38 2015 UTC,"If you are using Excel, don't pull out static data points - structure the data properly in a table and run pivots from that table.   Sorry, I'm a little new to the game here, but can you elaborate on what you mean by this?"
statistics,3d28ip,not_rico_suave,7,Tue Jul 14 01:15:01 2015 UTC,"Excel, excel, excel. Some of the better gigs won't use it, but a whole lot of them do. Plus it's just super useful to know no matter what your job is."
statistics,3d28ip,MrGoodGlow,3,Mon Jul 20 17:25:29 2015 UTC,"I recently learned about VLOOKUP/HLOOKUP, and I have to say it has given me a whole new perspective on the power of Excel."
statistics,3d28ip,mythstified,11,Mon Jul 13 00:06:20 2015 UTC,Ok good. Now forget those and learn index and match. You'll thank me!!!!
statistics,3d28ip,decimated_napkin,2,Mon Jul 13 02:56:30 2015 UTC,I know index and match but I exclusively use Iferror(vlookup()). What's the benefit of using index and match over vlookup?
statistics,3d28ip,Pfohlol,1 point,Mon Jul 13 05:00:14 2015 UTC,"It requires less memory, and you can drag across the rows to do multiple by locking the lookup value with ""$"".  Something that is hard to do with vlookup"
statistics,3d28ip,decimated_napkin,1 point,Mon Jul 13 16:58:42 2015 UTC,"Never have to worry about counting the distance between the lookup.  Also works 2 dimensionally index (where, x, y)"
statistics,3d28ip,thenumber0,1 point,Mon Jul 13 21:04:14 2015 UTC,"I use vlookup constantly, though now I'm planning to switch over to Index/Match for all lookups to speed up my larger excel workbooks, since vlookup recalculates far too often."
statistics,3d28ip,decimated_napkin,2,Mon Jul 13 23:20:35 2015 UTC,"To me, it seems like anything I can do in Excel can be more easily done from a programming environment like R. Would you disagree with that?"
statistics,3d28ip,thenumber0,1 point,Mon Jul 13 15:19:23 2015 UTC,"There are many simple analyses that can be done much more quickly in excel than R, but for the more complicated stuff I agree wholeheartedly that R is probably better. The big thing here is that most companies still use excel far more often than excel, so you will have to learn excel anyways. That being said if you are a specialist in R then you may be able to get a top offer, that is a very valuable skill that I still need to learn."
statistics,3d28ip,decimated_napkin,1 point,Mon Jul 13 12:17:24 2015 UTC,Where would you recommend learning excel (beyond basic use)? Any particular sites or books?
statistics,3d28ip,HughManatee,2,Mon Jul 13 15:08:21 2015 UTC,"Google is your best friend, and is how I did most all of my learning. Just set out to accomplish some task and then google your way towards an answer. For instance, figure out how to generate a list of every possible combination of three sets of numbers, each set consisting of the numbers 1 through 15. Doing tasks like this will get your brain into excel mode in no time, and google literally has the answer to everything you may need in excel."
statistics,3d28ip,kierisi,1 point,Mon Jul 13 09:49:05 2015 UTC,"Google is your best friend, and is how I did most all of my learning.   I think I know how to learn, though perhaps not what to learn. When an employer asks for proficiency / expertise in Excel, what does that mean I ought to be able to do? Is there a resource that would cover the appropriate areas, either with learning material or suitable questions?"
statistics,3d28ip,ILikeChillyNights,2,Mon Jul 13 15:06:08 2015 UTC,"this is a good place to start: http://chandoo.org/wp/2013/01/16/top-10-formulas-for-aspiring-analysts/  what you need to learn is how to manipulate large sets of data quickly and accurately. sumifs, countifs, sums, vlookups, if functions, etc. If you can master nested if functions you will own excel in no time. Then you will just have to add to your knowledge base of more obscure functions, like datedif, randbetween, choose, right, mid, left, etc. Once you get the basics and then have a large toolbox of less common functions, excel will start to actually get pretty fun, and you will have no problem with any proficiency tests. Also know how to do pivot tables and be able to format graphs and charts very quickly."
statistics,3d28ip,t_rex_tullis,1 point,Mon Jul 13 15:13:38 2015 UTC,"VLOOKUP is such a tremendously useful function.  I have saved people so many man-hours by showing them how to ""join"" their data using VLOOKUP."
statistics,3d28ip,Kurac_80,3,Mon Jul 13 15:24:36 2015 UTC,"work through john forman's ""data smart"" to become fluent in excel now learn R learn to ask questions and find insight. i manage a data science team, and the most valuable thing someone can do is to see the story in the data.   for example, almost anyone can run an analysis and come back to me and say that there are 75% more women than men in our targeted age group.  so what?  75% more seems significant - is it? looking at the data, why do you think this is? what additional data would clarify your analysis?"
statistics,3d4yk4,Blue_Faced,1 point,Mon Jul 13 15:46:31 2015 UTC,Assuming your data represents an RV or series of RVs there are both parametric and non parametric ways to do histogram consistency tests. I'll s nod you a couple resources shortly. But the general idea is to show that the data from your old query is distributed identically or nearly identically to the data from your new query.
statistics,3d4yk4,timshoaf,1 point,Mon Jul 13 16:30:56 2015 UTC,That makes a lot of sense! I'll stand by for the resources. Thanks!
statistics,3d4yk4,timshoaf,1 point,Mon Jul 13 16:57:49 2015 UTC,http://arxiv.org/pdf/1009.5604v1.pdf
statistics,3d6rim,Facelamp,1 point,Mon Jul 13 23:31:37 2015 UTC,Calculate the z scores for the respective outer range and then you can look up the score on a table and get percentages for each value then subtract them.
statistics,3d6rim,akay47smalls,1 point,Mon Jul 13 23:45:02 2015 UTC,"it's an exam question and that is all I am given with 4 answers :S  a) approx. 34%  b) approx. 15%  c) approx. 68%  d) None of the above are within 5% of the correct answer  another question I have:   What can we infer from “Obtained F (3, 96) = 7.94, p < 0.001”?   a) The between groups df were 3 and the within groups df were 96, so there were 3 conditions and 100 participants  b) The between groups df were 3 and the within groups df were 96, so there were 4 conditions and 100 participants  c) The between groups df were 3 and the within groups df were 96, so there were 4 Factors and 100 participants  d) The between groups df were 3 and the within groups df were 96, so there were 3 Factors and 100 participants  Mind explaining that if you'd know? thakns"
statistics,3d6rim,MrTimnTheGreat,2,Mon Jul 13 23:46:20 2015 UTC,"Check the rules of the sub please. You may be more interested in /r/homeworkhelp  Edit: This was just posted over there, disregard"
statistics,3d6rim,efrique,1 point,Mon Jul 13 23:51:36 2015 UTC,An IQ Test with a mean of 100 and a standard deviation of 15 is administered to a normally distributed population.    Whoever wrote that question appears to have a fundamental misunderstanding.
statistics,3d1iz8,Radiohead_dot_gov,3,Sun Jul 12 19:30:04 2015 UTC,Is this a homework question?  It might be best to post it to /r/homeworkhelp.
statistics,3d1iz8,slowgradstudent,1 point,Sun Jul 12 20:11:31 2015 UTC,This is a genuine work-related question for deciding how much to take from a mixture so that I can confidently say that I've included nearly all unique items.
statistics,3d1iz8,slowgradstudent,1 point,Sun Jul 12 20:16:22 2015 UTC,"Oops, sorry about that.    Homeworkhelp may still be able to help point you towards some resources, it looks like they get questions about sampling a fair bit."
statistics,3d1iz8,Smartless,1 point,Sun Jul 12 20:19:59 2015 UTC,Good call. I'll look through what those folks have discussed related to this.
statistics,3d1iz8,gwern,3,Sun Jul 12 20:24:18 2015 UTC,Also try /r/AskStatistics
statistics,3d0fvi,blackrosethorn,5,Sun Jul 12 13:28:47 2015 UTC,"Hi there! Current psych grad student here (so I feel your pain!). I'll try and answer your questions in this post, but here are two good (and decently intuitive) resources for you:  http://www.theanalysisfactor.com/effect-size/  http://stats.stackexchange.com/questions/15958/how-to-interpret-and-report-eta-squared-partial-eta-squared-in-statistically  Also, it's a good idea to keep take notes of things you learn in stats. You will NEVER be able to remember it all, and when you need to know something, you will then have it--and in your own words. I have over 20 Wordpad documents for this and they have been ESSENTIAL.  Anyway,   Eta squared is easy to understand. Remember that your DV, by nature of being a variable, is defined by variation, and it is this variation that we want to explain using our experimental factors. Eta squared is defined as the proportion of the total variation in the DV that is explained by your IV (i.e., factor). Therefore, if eta squared is .45, then the intuitive meaning is that 45% of all the variance in your DV is explained by this factor.   Now, partial eta squared is very similar and is relevant in situations when there is more than one factor (i.e., a factorial ANOVA design).  For instance, when we have two factors, we have a more complicated situation: Two IVs that explain the variation in the DV. In this situation, one can simply compute a regular eta squared for each IV, and it's interpretation will be the same as above. However, when this is the case, we also know that some of the variation that was not explained by Factor 1 was explained by Factor 2. The problem is that computing regular eta-squared for Factor 1 does not account for the fact that a significant amount of variance might be accounted for by Factor 2 and lumps this into the total variance that Factor 1 is trying to explain. Thus, in some ways, a better metric of the importance of Factor 1 would be one that excludes the variance that is explained by Factor 2 in the computation. The interpretation of this effect size would be: The proportion of variance accounted for by a factor after what's left after taking out the other factor(s). This is partial eta squared, as it ""partials"" out the systematic variance attributable to other factors and does not require the factor of interest to explain it.   Here's an excerpt from a paper we wrote (currently under review, so I'll ""blank"" out the variable name): ""A two-way repeated-measures ANOVA on _ resulted in a main effect of _, F(1, 23) = 9.31, p = .006, ηp2  = .29."" You should also communicate the marginal means somewhere. For specific mean differences, Cohen's d is the more intuitive effect size. Pearson's r is for two continuous variables, and by definition, an ANOVA has a continuous DV and categorical IVs.   Hope this helps!"
statistics,3d0fvi,andypandy14,1 point,Sun Jul 12 14:44:44 2015 UTC,"Thank you so much for explaining this in plain English !! Oh I've taken loads of notes during the course, but for some reason other than a brief overview of what effect sizes are no further detail was given. Me being pedantic as well I like to try and understand every little detail.   Thanks again !!"
statistics,3d0fvi,leonardicus,1 point,Mon Jul 13 09:43:10 2015 UTC,Try asking in /r/askstatistics
statistics,3czheu,efavdb,1 point,Sun Jul 12 04:47:52 2015 UTC,"Re Option 1, note that (1) you only need to compute around n log n integrals/comparisons to rank n items... but this assumes that (2) this P(X>Y)<1/2 relation is a total ordering on beta distributions with positive integer parameters.  It isn't, at least not without some minor housekeeping.  Also, there are other natural options for Bayesian-ish rankings; see here for details, including some of the ""holes"" in a total ordering."
statistics,3cwque,avinassh,3,Sat Jul 11 13:06:44 2015 UTC,BURN IT BEFORE IT'S TOO LATE  j/k. sounds cool. what sort of use do you see for this?
statistics,3cwque,upreal,10,Sat Jul 11 16:31:05 2015 UTC,preservation and statistical analysis of dank memes
statistics,3cwque,Utrolig,3,Sun Jul 12 02:05:46 2015 UTC,Someone could make a chatbot. Or make a program like Siri more entertaining.
statistics,3cwque,Iskandar11,1 point,Sun Jul 12 01:30:59 2015 UTC,Pretty good sampling frame for a reddit survey.
statistics,3cwque,Adamworks,2,Sat Jul 11 21:29:24 2015 UTC,Cool
statistics,3cwque,Smartare,1 point,Sat Jul 11 16:03:40 2015 UTC,/r/datasets
statistics,3cwque,shaggorama,1 point,Sun Jul 12 05:09:55 2015 UTC,Yea that's what it links to.
statistics,3cwque,Iskandar11,0,Sun Jul 12 05:26:40 2015 UTC,We generally delete dataset posts and direct people to that sub.
statistics,3cx6xr,ty54,12,Sat Jul 11 15:56:10 2015 UTC,"So when I think of using ""SQL"" in this context, I would say it means an applicant has experience with one or more SQL dialects (T-SQL, MSSQL, Oracle...etc) on datasets of all sizes but not necessarily huge. SQL is flexible and the amount of data you can handle really depends on the infrastructure and use case. An enterprise setup with powerful server hardware on the back end, well indexed tables, and sensible structure can support interactive querying for tables in the hundreds of millions of rows, if not the low billions. ""Large"" really depends on the context. But unless they mention Hadoop/NoSQL specifically, they probably aren't talking about ""big data"" in that way.   I think one thing that might help is to remember that once you are fluent enough in SQL, you will see that the same principles and abstractions apply to any data manipulation situation in any language. There is a reason it's called the Standard Query Language. Sometimes I find myself thinking through a problem in SQL-like steps and then mentally translating that to Python/R code that does essentially the same thing. Employers might be less interested in your actual SQL knowledge and more in how well you can think through data manipulation in general contexts. Many other domain specific querying languages look a lot like SQL, and if you can write decent SQL you can already write queries in things like Hive/BigQuery with little additional training. So in a way learning SQL trains you for these situations at the same time. Keep it on your resume but be honest about your experience. De-emphasize your actual technical SQL knowledge and emphasize your ability to transform and manipulate data in a variety of situations and with different tools.   If you are interested in getting a more technical SQL background, read up on query execution plans, indexing, database design (know the difference between a relational data warehouse and a transactional ODS). Intermediate/advanced techniques like temp tables/ctes, dynamic sql, stored procedures, explicit transactions (begin, commit, rollback).  Situations that you might actually encounter in a production environment. IMO SQLzoo is fine for the basics, but there is a big difference between sorting cities by population and writing a complex stored procedure that will combine multiple data sources and scale up when the data is 10x or 100x that volume."
statistics,3cx6xr,thunderdome,2,Sat Jul 11 19:20:41 2015 UTC,"I work in an environment that uses Teradata SQL and SAS proc sql, with databases up to 15,000 elements and 54 billion rows, and I believe this is really good advice so I'm not going to re-type it all myself.   If you have follow-up questions, I'd be happy to try and build on this response from my own experience."
statistics,3cx6xr,Ecopath,2,Sun Jul 12 13:03:22 2015 UTC,Holy shit. What kind of information does 15 000 elements provide?
statistics,3cx6xr,The_Amp_Walrus,2,Sun Jul 12 14:02:12 2015 UTC,"Medicare system.  Billing, claims, medical information, beneficiary information, all kinds of stuff.  Also several sets of legacy variables because government bureaucracy."
statistics,3cx6xr,Ecopath,2,Sun Jul 12 18:38:40 2015 UTC,"Any particular book or resource recommendations for learning? How did you ""break in"" to this type of work?"
statistics,3cx6xr,fvpokemon,1 point,Mon Jul 13 15:47:31 2015 UTC,"I broke into data work fairly sideways, learning to use R and SAS for my research in grad school.  When it came time to get a job, I sort of punted on my original research path and marketed my data skills instead.  In terms of recommendations, R is easiest to break into because of its open source nature and thriving community.  I would look into any of the many books or free resources for R, and pay special attention to the sqldf package.   Find some pet projects or datasets and find some questions to answer. Each time you get through something or learn new packages or methods, market those. Focus on them instead of individual languages. If you can learn R, you can learn any of the other stats programming languages.  My only other bit of advice is that healthcare is booming right now, and it seems like the easiest industry to build a career as a statistician or analyst in.  It's what worked for me, anyway."
statistics,3cx6xr,Ecopath,3,Mon Jul 13 17:29:51 2015 UTC,"You could download all of the Census data from 2000 at all geographic levels, and then construct a relational database out of it- relating blocks to block groups and tracts, blocks to ZCTAs, tracts to counties, counties to states, etc.  While this isn't a truly massive database, at around 3 GB compressed it is a decent size.  Here is a link to their FTP site."
statistics,3cx6xr,BurkeyAcademy,3,Sat Jul 11 17:49:11 2015 UTC,"https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/  53 million Reddit comments along with all associated meta data... that's pretty massive.  You could parse it into a giant table and then use SQL for normalization, etc."
statistics,3cx6xr,UntrustedProcess,2,Sat Jul 11 18:44:08 2015 UTC,"Employers probably know that a lot of candidates don't have exposure to big data and query efficiency, but knowing the basic concepts will give you an edge over other applicants.  Knowing syntax, join types, subqueries, and explain plans is a pretty good start.  My job deals with big data a lot, but I came into it pretty green as well, for what it's worth."
statistics,3cx6xr,HughManatee,1 point,Sat Jul 11 22:28:29 2015 UTC,"Don't actually try to work with large datasets, it's gonna bankrupt you (you may be able to run serious jobs on Amazon EMR for 10-50 USD, but you'll prob. need to run it a several times).  That doesn't mean you can't get any experience. If I were you I'll play around with Hadoop/Spark APIs on Amazon with a small dataset. You could also play with Redshift, but unlike Hadoop/Spark its interface is SQL, so what you want to learn is how MPP DBs work and how optimisations work.  PS Millions of rows is actually quite small, you'll most commonly be working in billions."
statistics,3cx6xr,dict_disc,0,Sun Jul 12 08:05:23 2015 UTC,"In practice, with SQL Server, the size of the data you're using doesn't really matter: You write your query, and the optimizer does the rest.  Just learn join, group by, union, except, etc, and know that indexes are super important.  That said, it's a good idea to screw around with sqlite, where you get severely punished for doing dumb stuff (e.g., not using indexes, using text columns when you could be using ints), and learn how to interpret ""explain query plan"".  For practice, find a kaggle competition where they give you multiple tables, and write some queries.  Here's a good example:  https://www.kaggle.com/c/avito-context-ad-clicks/data"
statistics,3cx6xr,hahdawg,-1,Sun Jul 12 02:04:34 2015 UTC,Kaggle
statistics,3cy24x,acinomismonica,2,Sat Jul 11 20:29:42 2015 UTC,"There's a nice book by Andrew Gelman about teaching statistics.  I don't remember if he specifically mentions this in the book, but I know he advocates collecting example data from students that reflect the students' own interests and identities.  (Sort of obvious, but I guess the idea is that people like to learn about themselves and compare themselves to peers.)    So maybe stuff like tastes in music, amount of time spent listening to music, watching TV, studying, playing sports.  Number of friends on facebook.  I'm guessing AP students are pretty motivated, so maybe also stuff like intended college major, number of times they plan to take the SAT, number of colleges they plan to apply to.  One thing to watch out for is that a seemingly innocuous data set can still be used to test some pretty non-pc hypotheses.  Like if you have gender and gpa data, someone is bound to ask about sex differences in academic performance."
statistics,3cy24x,_dog_welder,2,Sun Jul 12 04:01:41 2015 UTC,"Thanks for that book suggestion! I think I want things that can have boundaries, and that can easily be switched back and forth between the two types of data so I can use it throughout the year for discussions and raw data. That's a good idea though for discussion on gpa and college majors."
statistics,3cy24x,slowgradstudent,2,Sun Jul 12 06:03:18 2015 UTC,"I'm not a statistician, but it may be helpful to check out Andy Field's texts on statistics (he has one based on SPSS and one based on R).    If using existing datasets (which are available on his website) could be an option, I think the examples he uses are are really engaging."
statistics,3cy24x,addfunr3,1 point,Sun Jul 12 19:51:26 2015 UTC,This. His examples aren't your usual datasets and are actually interesting to go through. His stuff got me interested in stats in the first place.
statistics,3cy24x,tpn86,-1,Mon Jul 13 04:36:53 2015 UTC,"Gender, sexual partners, height, parent height, how attractive they consider themselves (1-10, 5 being average). Their current gpa, their expected gpa at graduation.  Sounds fun!"
statistics,3cy24x,GoldFisherman,3,Sat Jul 11 22:46:27 2015 UTC,Are you serious?????
statistics,3cy24x,tpn86,1 point,Sun Jul 12 01:25:46 2015 UTC,"Well to be fair I am not american, but those might be the sort of things young people consider interesting.  As long as no one can be identified and are free to not answer I think its fine. No doubt some jokers will make up numbers so you can teach data cleaning as well.  When I taught first year stats at uni we did formal testing to compare male vs female iq scores."
statistics,3cy24x,GoldFisherman,1 point,Sun Jul 12 08:23:38 2015 UTC,OP would get in big trouble if a question about sexual partners appeared on a student survey.
statistics,3cy24x,tpn86,1 point,Sun Jul 12 13:04:53 2015 UTC,"Fair enough, but too bad, could be interesting data.  Do the height and gpa thing instead then :)"
statistics,3cw5pa,blueberry_crepe,4,Sat Jul 11 07:32:11 2015 UTC,"Kruschke's book has a practical focus that makes it well suited to self-study. It has chapters on Bayesian equivalents of ANOVA, t-tests, etc. that are easy to grasp for people with a background in traditional statistical methods. I would recommend it for anyone who wants to quickly get up to speed with applied Bayesian stats.  BDA3 by Gelman et al. is more like a college-level text. In fact, it is used as the main textbook for a graduate-level course in Bayesian methods at my college. It would be beneficial to form a reading group of people that you can discuss the chapters with as you work through them.  I would also recommend the book on Bayesian regression models by Gelman & Hill (2006), as well as ""Bayesian essentials with R"" by Marin & Robert (2014)."
statistics,3cw5pa,mmoores,3,Sat Jul 11 10:48:03 2015 UTC,"Kruschke, the guy, is an absolute maniac and a really great teacher. His approach seems to spill over into the book. When I took his course, I found it to be extremely applied, with very little emphasis on the theory of Bayesian inference outside of the requisite ""Bayes' rule is clever and revolutionary, here is how it's derived"" chapter. I don't know how well the book would stand alone from the course, mainly from a statistical programming perspective. The code is pretty 'canned', which means adapting it to your data can be pretty frustrating if you don't have the opportunity to get questions answered by Kruschke himself. The code is honestly not all that well commented."
statistics,3cw5pa,kusetsu,1 point,Sat Jul 11 14:16:45 2015 UTC,"Kruschke, the guy, is an absolute maniac   That sounds interesting. How so?"
statistics,3cw5pa,Bromskloss,4,Sat Jul 11 20:39:34 2015 UTC,"By that I mean he's incredibly energetic, very passionate about stats, especially Bayes, and he always comes into the classroom like he just drank about 12 espresso shots. Just a very wired and excited lecturer."
statistics,3cw5pa,kusetsu,3,Sun Jul 12 17:12:43 2015 UTC,"Kruschke's book looks deceptively simple, but it does an amazing job at building intuition and helping you recognize where Bayesian statistics is useful.  It'll give you a much better idea on how to actually apply Bayesian statistics in your work.  Gelman's book is a lot more theoretical and technical.  There's some theory in Kruschke's book, but only enough to get you working.  He explains MCMC algorithms in detail but once he teaches a theoretical concept, you won't see it again.  Gelman's book is almost all theory, but there's a section in the back on how to use STAN."
statistics,3cw5pa,beaverteeth92,2,Sun Jul 12 17:19:28 2015 UTC,"From what I have seen from Kruschke's book it is more applied rather than more basic. BDA has more theory. But since I read BDA ed2 and various others before Kruschke's was on the market, I never delved into Doing Bayesian Statistics."
statistics,3cw5pa,wiekvoet,2,Sat Jul 11 08:06:25 2015 UTC,Doing Bayesian Data Analysis  Bayesian Data Analysis  These are the links on google books if you want to browse through a selection of pages and compare.
statistics,3cw5pa,maxwell_smart_jr,2,Mon Jul 13 15:16:23 2015 UTC,"2nd edition, Krushke 2014"
statistics,3cw5pa,mmoores,1 point,Mon Jul 13 21:37:46 2015 UTC,"Haven't read Kruschke, but I love Gelman. BDA promotes a really pragmatic philosophy for data analysis, that, while not a recipe, gives you a broad rationally laid out structure for approaching any analysis. though I wouldn't call it an easy read, it's definitely written with great clarity. I think having R code to go along with the text sounds really good, however."
statistics,3cw5pa,TheStorer,1 point,Sat Jul 11 20:50:55 2015 UTC,"Don't you mean ""Doing Bayesian Data Analysis - A Tutorial with R and BUGS"" by J. Kruschke?  Or did he write another book?"
statistics,3cw5pa,eoliveri,4,Sat Jul 11 22:00:31 2015 UTC,"The 2nd ed. is subtitled ""A Tutorial with R, JAGS, and Stan""  https://sites.google.com/site/doingbayesiandataanalysis/what-s-new-in-2nd-ed  Both JAGS & Stan use variants of the BUGS language, but they are faster and more stable than OpenBUGS."
statistics,3cuezm,Nanonaut,3,Fri Jul 10 21:30:43 2015 UTC,"There is a brief discussion about this on StackExchange  Essentially, p-values should not be really used for variable selection, and a better option is to use the LASSO method."
statistics,3cuezm,yeezypeasy,1 point,Fri Jul 10 22:01:33 2015 UTC,Why couldn't I just do a likelihood ratio test for comparing the two models?
statistics,3cuezm,efrique,2,Fri Jul 10 22:54:02 2015 UTC,"The effects are the same as I just listed in my other comment. You can, but if you're doing it for model selection, p-values in your final model are biased down, estimates of non-zero coefficients are biased away from 0 and standard errors are too small (it doesn't matter whether you use AICs or p-values, you're still doing model selection and then ignoring the impact of doing it)."
statistics,3cuezm,BigBucksGentleman,1 point,Sat Jul 11 02:31:39 2015 UTC,"I'm lost here - ""the effects""....of what? And on what? This is for model selection (for fitting some data, not doing predictions). Are you saying stepwiseAIC is just a horrible idea in general?"
statistics,3cuezm,BigBucksGentleman,4,Sat Jul 11 02:54:11 2015 UTC,"I believe he is referring to LRTs. Also model fit and prediction go hand in hand.  As others have mentioned, the preferred methodology in the field of statistics right now (for model selection) is to enter all the variables in a model, and shrink them down using regularization. Not only does the glmnet package in R fit logistic LASSO regressions (using the binomial family as a response option), it also performs k-fold cross validation for you as well (to select a lambda one standard error from the minimum). It is also written in part by Rob Tibshirani who developed much of the theory on the LASSO regression.  If you have any questions about the using the LASSO for subset selection, feel free to ask!"
statistics,3cuezm,BigBucksGentleman,1 point,Sat Jul 11 14:22:57 2015 UTC,"Hi there, I absolutely do! First off - how do I make two way interactions a possibility for the lasso algorithm? (I'm pretty sure I need at least one in my model). And - what exactly do I do with the plots to determine the # of variables I should use in the model? Here's my code thus far:  library(glmnet)  x.matrix=as.matrix(cbind(1,mydata.A[,c(2:13)]))  fitlasso = glmnet(x.matrix, y=as.factor(y), family = ""binomial"")  plot(fitlasso,xvar=""lambda"")  grid()  coef(fitlasso)[,11]  -#Corresponds to 3 variables"
statistics,3cuezm,efrique,3,Tue Jul 14 00:26:22 2015 UTC,"For your interactions, just add them to your data frame before you run the LASSO. See here for quickly adding all pairwise interactions. Make sure in your glmnet call you specify ""alpha=1"". That specifies LASSO (as opposed to ridge or elastic net). It might be redundant, but better to be explicit. Use cv.glmnet to perform cross validation to select the best lambda. Once that is run you can use $lambda.1se as the lambda parameter for the LASSO. You can then use call the coef() function. Any non-zero coefficients have been selected by the LASSO."
statistics,3cuezm,yeezypeasy,1 point,Tue Jul 14 02:29:37 2015 UTC,"Hmm, do you know how I can use the 2 command used in your link in conjunction with what I have already (as.matrix(cbind...)? Or will I have to redo that and type out the variables?"
statistics,3cuezm,efrique,2,Wed Jul 15 01:51:04 2015 UTC,"Try this:  lassomatrix <- model.matrix(~.2, x.matrix)  And just pass that to the glmnet call."
statistics,3cu6gf,PilgrimAnimation,4,Fri Jul 10 20:23:27 2015 UTC,"To many other variables for you to make a statistical correlation. Things like tax code, regulations, consumer confidence, etc. Always fun to look at that stuff but don't try to read to much into your findings:)"
statistics,3cu6gf,BitcoinPatriot,0,Fri Jul 10 20:36:05 2015 UTC,"Yeah, it's just for fun.   And maybe for some propaganda.  :)"
statistics,3csbjq,smoochie100,4,Fri Jul 10 11:07:42 2015 UTC,"The median is the 50th percentile and defined as the value such that:   At least 50% of your observations are less than or equal to the median At least 50% of your observations are greater than or equal to your observation   In your set {1,2,2,3,4}, the value 2.5 has 60% less, but only 40% more, hence does not meet the definition for the median. The value 2 has 60% less or equal and 80% greater or equal, which meets the definition of the median.  If you had an even number of observations, like {1,2,2,3,4,4}, in principle any number between 2 and 3 could be considered the median. The halfway point, 2.5, is the accepted convention."
statistics,3csbjq,grozzy,2,Fri Jul 10 16:01:45 2015 UTC,"It comes down to convention and simplicity. The use of fractional ranks has the nice property of keeping the rank sum independent of the number of ties and thus avoiding bias, but that is pretty irrelevant to the median. Median isn't even usually defined in terms of rank.   Using your proposed method would increase the complexity of the calculation, make it harder to understand and explain, and lose the property that median minimises the sum of the absolute residuals. What is gained in return?"
statistics,3csbjq,ararelitus,1 point,Fri Jul 10 13:06:19 2015 UTC,Thank you for your answer. How is it then defined?
statistics,3csbjq,dmlane,2,Fri Jul 10 13:39:02 2015 UTC,You can define it as the 50th percentile. The complication is that there are multiple ways to define percentile see this site.
statistics,3csbjq,wiekvoet,2,Fri Jul 10 13:54:41 2015 UTC,"I think you are confusing two concepts. Lets multiply your data by 10 and reshuffle, so it is clear what are ranks and what is data. We have 10 30 20 40 20. In another dataset we have 10 30 21 40 20.  If you rank #1, they become 1 4 2.5 5 2.5 .   If you rank #2, they become 1 4 3 5 2.  These ranks have the property that sum of the ranks is the same. Also only 20 and 21 are influenced by the difference of the data. We only need this property because it allows us to do interesting stuff with the ranks rather than with the original data.  If you look for the median, you just look for the median. The middle number. In #1 that is 20. In #2 that is 21. You are not ranking.  In #1 it is 20 because reordering gives 10 20 20 30 40. Take the middle: the median is 20. It does not matter which of the values 20 is used, it is 20.  In #2 it is 21 because reordering gives 10 20 21 30 40. The median is 21."
statistics,3csbjq,isarl,1 point,Sat Jul 11 08:24:50 2015 UTC,"Thanks, very helpful and understandable explanation"
statistics,3cr924,SantyClause,1 point,Fri Jul 10 03:19:52 2015 UTC,"replace * in your equations with \* so that markdown doesn't interpret the ""*""s as ""turn this text into italics"""
statistics,3cr61x,orangecamo,3,Fri Jul 10 02:53:54 2015 UTC,"Why would there be a category for slavery in 1950? Slavery was abolished in 1865 in the US. Of course, with human trafficking and modern slavery that occurs everywhere perhaps a code would be interesting, but I doubt that these people would be sampled in any kind of survey, no matter how ambitious."
statistics,3cr61x,BurkeyAcademy,2,Fri Jul 10 14:14:22 2015 UTC,"IPUMS applied the 1950 census' schedule of occupations to the other censuses to form a unified occupation variable across all of the US censuses. This included all of the census between 1790 and 1860, during which there were slaves, so IPUMS assigned occupation codes to everyone in the census, including slaves. However, there was no occupational code for slaves in 1950, so the only variable listing people's occupation does not include an indication of slave status. I am trying to find out if there is another slave status variable for the data sets that I have not been able to find yet."
statistics,3cr61x,BurkeyAcademy,2,Fri Jul 10 17:13:44 2015 UTC,"Ah, thanks for the clarification.  I will try to contact some of my friends in the BLS and see if anyone has anything to say."
statistics,3cnq0a,com2mentator,7,Thu Jul 9 09:27:37 2015 UTC,"No clue why.  Looking at ""Ordinary least squares"" it looks like there is some exam induced seasonality :D https://tools.wmflabs.org/popularpages/graph.php?title=Ordinary+least+squares&start=Jul09&end=Jun15"
statistics,3cnq0a,tpn86,2,Thu Jul 9 13:18:57 2015 UTC,"That may explain it, if it was a newly introduction question (no rise previous years)."
statistics,3cnq0a,m1sta,4,Thu Jul 9 13:28:35 2015 UTC,MOOC
statistics,3cnq0a,TDaltonC,3,Thu Jul 9 14:51:24 2015 UTC,Maybe a high traffic wiki just started linking to it?
statistics,3cnq0a,dresdnhope,1 point,Thu Jul 9 14:32:49 2015 UTC,"Its possible, will see what happens next month."
statistics,3cnq0a,Hairy_Hareng,3,Thu Jul 9 14:59:30 2015 UTC,"600,000 hits a day on 6/2/2015 and 6/3/2015.  http://stats.grok.se/en/latest60/Quasi-maximum_likelihood  I'm guessing a faulty bot or internal testing."
statistics,3cnq0a,epostma,2,Fri Jul 10 03:14:04 2015 UTC,"Thanks, that make more sense now. Thanks for the link to show daily hits. : -)"
statistics,3cnq0a,jonthawk,2,Fri Jul 10 14:59:28 2015 UTC,This is not a shitpost because ?  Am I the only one who has no idea of the context here ?
statistics,3cnq0a,jonthawk,5,Thu Jul 9 12:55:19 2015 UTC,"I thought someone here might know the reason 'Quasi-maximum likelihood' was suddenly looked up by many people.  Also, someone might have a suggestion on how to improve the article on Wikipedia."
statistics,3cnq0a,epostma,1 point,Thu Jul 9 13:02:56 2015 UTC,"Slightly off-topic: it's interesting to me that ""about 1000x more hits"" is necessarily a meaningful change, whereas ""more than 1000x more hits"" is not -- the latter can mean 1 hit instead of 0, which generally means nothing."
statistics,3cppzg,Northstat,1 point,Thu Jul 9 19:59:31 2015 UTC,"If you are looking for an analytical strategy: multiple correspondence analysis.   Otherwise, what exactly are you looking for?"
statistics,3cppzg,dearsomething,1 point,Fri Jul 10 01:12:31 2015 UTC,"To be honest, I'm just not that familiar with how to deal with non-numerical data so I don't know what the possibilities are. My data is user website activity and I'm trying to predict if that user becomes a customer or not. The followup is to help marketing understand where to focus their efforts."
statistics,3cppzg,addfunr3,1 point,Fri Jul 10 14:27:27 2015 UTC,"Bit late here, but is factor analysis or dimension reduction an option?"
statistics,3codve,slammaster,1 point,Thu Jul 9 14:00:59 2015 UTC,"I just finished a course in Experimental Design using this text by Gary Oehlert at the U of Mn. There was an great amount of R-based materials in the class as well, uncertain how you might access that."
statistics,3codve,shoneone,1 point,Thu Jul 9 14:13:00 2015 UTC,You could consider the book from Kim Seefeld: http://cran.r-project.org/doc/contrib/Seefeld_StatsRBio.pdf  and perhaps take a look at the MOSAIC books: https://github.com/ProjectMOSAIC/LittleBooks
statistics,3cpnss,Leadboy,2,Thu Jul 9 19:45:52 2015 UTC,"I think you're looking for an empirical distribution function with smoothing.  http://www.mathworks.com/help/stats/examples/nonparametric-estimates-of-cumulative-distribution-functions-and-their-inverses.html  Full disclosure: I didn't read the above article, but I think it's probably what you are looking for."
statistics,3cpnss,joshsaa,1 point,Thu Jul 9 21:50:06 2015 UTC,"Sounds like you probably want an empirical CDF. In R:  > x <- c(1.02, 2.33, 2.42, 4.55, 4.56, +        6.79, 9.22, 10.01, 12.11, 12.67) > ecdf(x)(10.01) [1] 0.8   Now if you're trying to estimate a specific percentile (e.g. 50th, 75th, 90th) there's about 10 different ways to do that. See for example type in the R quantile function, or Calculating Percentiles for PROC UNIVARIATE in SAS. Type 9 in R should give fairly unbiased estimates if you know that the distribution is normal.  You can compare results as follows:  > res <- matrix(as.numeric(NA), 9, 5) > for(type in 1:9) res[type, ] <- y <- quantile(x, type = type) > dimnames(res) <- list(1:9, names(y)) > print(res)     0%      25%   50%      75%  100% 1 1.02 2.420000 4.560 10.01000 12.67 2 1.02 2.420000 5.675 10.01000 12.67 3 1.02 2.330000 4.560 10.01000 12.67 4 1.02 2.375000 4.560  9.61500 12.67 5 1.02 2.420000 5.675 10.01000 12.67 6 1.02 2.397500 5.675 10.53500 12.67 7 1.02 2.952500 5.675  9.81250 12.67 8 1.02 2.412500 5.675 10.18500 12.67 9 1.02 2.414375 5.675 10.14125 12.67"
statistics,3cpnss,zip117,1 point,Fri Jul 10 20:32:54 2015 UTC,"I ended up treating the values as a sample from a population (they are) and calculating z-scores, then from those z-scores, percentiles. Is that reasonable in your opinion? The data is normal and so I figure parametric options are okay?"
statistics,3cmjf9,ScienceandVodka,9,Thu Jul 9 01:51:09 2015 UTC,"Alright, I feel you want me to skip all the statistics and math jargon and get straight to the ELI5 version... So here goes:  The unadjusted, untransformed parameter estimates aren't easily interpretable. To make sense of them, you want to put them in terms of odds ratios. This isn't hard actually! Let b= beta, then you simply exponentiate the bX... i.e e^(bx)  Binary variables are easiest. Let's say your outcome is ""develop mad cow disease"" and your predictor of interest is gender.  Let X=1 if gender is male, and X=0 if gender is female. Your logistic regression model would look something like Y= b0 + b1X. For males, b1X = b1 (because X=1) and for females b1X = 0 (because X=0). You can then interpret eb1 as the odds ratio of males developing mad cow disease compared to females. Let's say b1= 0.5, then eb1 = e0.5 = 1.65. That is interpreted to mean than males have 1.65 times the odds of catching mad cow disease than females, or alternatively males have a 65% increase in odds of catching mad cow disease.   If you have a categorical predictor, they are treated the same as the binary, but your reference group is the same across all other levels of that variable. For example, if you're looking at races: white, black, Hispanic, and Asian... Then you set up 3 binary predictors around a reference category. Let's say we want white to be the reference, then we have 3 binary variables indicator black vs white, Hispanic vs white, and Asian vs white. You'll have 3 binary indicators X1, X2 and X3, each taking on a value of 0 of the person is white, and X1=1 if black, X2=1 if Hispanic and X3=1 if Asian. Then model then looks like, Y=b0 + b1X1 + b2X2 + b3X3. In this case, eb1 gives you the odds of developing mad cow disease in blacks compared to whites, eb2 gives the odds of developing mad cow in Hispanics compared to whites, and eb3 gives the odds in Asians compared to whites.   A cool characteristic about logistic regression is that no matter how many variables are in the model, as long as you're not fitting interactions, the betas are always interpreted this way!   Continuous predictors are a little trickier. Instead of comparing to a reference group, you're now comparing the increased or decreased odds for a unit increase in the continuous predictor X. Let's assume X is now age, and b1=0.2, then e0.2 = 1.22. We would then say for each each year increase in age, the odds of developing mad cow disease increases 1.22 times that of the previous year, or in other words, increases by 22%. Let's say you want to compare two specific ages (or two specific points for of a continuous variable). Define X1 and X2 to be the points along the continuum you want to compare. For example, you want to know the odds of mad cow disease at X1 relative to X2... Then you would simply look at eb1X1/eb1X2, which gives you those odds. Let's say X1 is age of 20, X2 is age of 17, and b1=0.2. Then e0.20*20 = 54.6, and e0.20*17 = 30, taking the quotient is then 54.6/30 = 1.82, which is the odds ratio of developing mad cow disease at 20 years old compared to 17. In other words, a 20 year old is at 1.82 times the odds of developing mad cow disease than a 17 year old (i.e a 20 year old has 82% greater odds of developing mad cow disease than a 17 year old).  I hope that helps. There's quite a bit of math and statistics to support how and why all of this works, but I've skipped that for you."
statistics,3cmjf9,Distance_Runner,1 point,Thu Jul 9 03:46:05 2015 UTC,"Ahhhh lovely! Thanks very much. Quick follow up questions - if I mean center my data first, will that change how to interpret the parameter estimates again in some way?   Also - my model has several interaction effects (where the variables alone are not significant but their interaction effect is) - how the heck does one interpret the estimate for an interaction? Or is that way too complicated?"
statistics,3cmjf9,Distance_Runner,6,Thu Jul 9 06:54:59 2015 UTC,"Interactions are tricky and a bit harder for some people to understand. If one of the two variables in the interaction is categorical, then interpretation is a bit easier than if both variables are continuous. Essentially what it's saying is if an interaction is significant, then the relationship between one predictor and the outcome depends on the value of another predictor. Let's jump back to the mad cow disease example...   Pretend we have predictor X1 which is age, X2 which is gender, and we model an interaction X1X2. This gives the model, Y= b0+ b1X1 + b2X2 + b3X1X2, and the model indicates b3 to be significant... Then the most straightforward way of interpreting this would be the change in odds in one variable, within a group of the other variable. These are the same results you would get in a stratified analyses, like if you were looking at the effect of age within genders, you would get the effect of age among men, and then among women in two separate models. If you want to know the change in odds between men and women (holding age constant), this is given by eb2+b3. Let's pretend eb2+b3*X1X2 = 1.4, then we would interpret this as, ""men have a 1.4 times the odds, or 40% increased odds of mad cow disease at age X1 than women"". The odds ratio is valid only at specific age X1, and because X1 interacts X2, this relationship changes depending on both age and gender. Conversely, you could also set it up to see the stratified effect of age within men or women, i.e the increased odds of developing mad cow disease per one year increase within men or within women. To do this, we hold gender constant, giving eb1+b3. Lets pretend eb1+b3 = 1.1, and eb1 = 1.3, then if females are used as reference, this means in males there is a 1.1 times increase in odds of developing mad cow per year aged, and in females there is a 1.3 times increase in odds of developing mad cow per year aged.   Results are the same if you have two categorical variables interacting, albeit even easier to interpret. With interacting categorical variables, you're essentially getting the same results as if you were to set up multiple contingency tables, stratified by the levels of one factor within the two way interaction.   edit: To address mean centering, if you fit the model around centered variables, the direction and significance of effects will remain the same! That is, a significant increase in odds of one group to another is invariant to a linear transformation of that variable."
statistics,3cmjf9,Distance_Runner,1 point,Thu Jul 9 10:50:39 2015 UTC,"This is why I think I'm failing out of my program, nothing makes sense to me AT ALL. I generated a model using stepwiseAIC. In the end the model is the usual: fit1=glm(y~variables+interactions,family=binomial)  If I mean center the data I get estimates between 0 and 1, which I decided not to do - but now this gives me a bunch of crazy estimates like b1 = -11 and b4 = 5 (and std error of 2). But e-11 is an impossibly tiny number and e5 is massive. The response variable is ""are you satisfied with the economy"" and x1 is ""how satisfied are you with life, on a scale of 1-4?"" - I know for sure that x1 should be the most important variable. How do I interpret these crazy estimates? x4=""doesn't have children"" - so what, these people are e5 x more likely to be satisfied with the economy?"
statistics,3cmjf9,drunkensamuraids,1 point,Sat Jul 11 03:29:50 2015 UTC,"I think you probably have an issue in either your coding or in the data set you're using.  Your comment tells me you're coding in R. If you feel comfortable with doing so, you can send me a PM with your R code copy and pasted, and I'll look it over to see if I find an issue with it. If I don't, it's probably the data set"
statistics,3cmjf9,dinkum_thinkum,4,Sat Jul 11 03:42:37 2015 UTC,"It gets a little confusing, but increasing your variable by one unit changes your log odds by the parameter estimate.    Go get the free pdf of the ISLR text for an adequate, simplistic explanation."
statistics,3cmjf9,TraptInaCommentFctry,5,Thu Jul 9 02:13:19 2015 UTC,"To expand for OP's example: your log odds ratio for one additional hour of work per week is .08. Getting rid of the log, the odds ratio for one additional hour of work is exp(.08)=1.083.  Odds ratios aren't terribly intuitive, but let's try. If something happens with probability p, then it's odds is p/(1-p). For example, p=.5 gives an odds of 1, p=.666... gives an odds of 2, p=.9 gives an odds of 9, etc. Odds ratios are the ratio between these odds for two different conditions.  Back to your example, let's say at x hours per week of work there's a 90% chance of you outcome (odds=9). Your logistic regression then says that the ratio between the odds at x+1 hours per week and the odds at x hours (=9) is 1.083, implying the odds at x+1 hours is 9.747, or a probability of 90.7% (reversing the calculation, .907/1-.907=9.747 odds, and ratio of the odds 9.747/9=1.083).  Since this is ratio-based, the effect of each additional hour is multiplicative on the odds, and produces a sigmoidal effect on the probabilities as described by Trapt: At x+2 hours: odds=10.56, prob=.9135 At x+3 hours: odds=11.43, prob=.9196 At x+4 hours: odds=12.38, prob=.9253 At x+5 hours: odds=13.41, prob=.9306    To get your actual values for a given x, plug your regression results into the logistic function as described by /u/TraptInaCommentFctry."
statistics,3cmjf9,autowikibot,2,Thu Jul 9 03:54:10 2015 UTC,"I don't have time for a full answer but the short answer is that it depends on intercept and the other coefficients.    To understand why, look at the graph of the logistic function.  If you take a step rightward of .08 units, the change in Y value is going to depend on where you were before the step.  If you were at -5, the change in Y is going to be smaller than if you are at 0.    If you just want a sense of the change in probability associated with a increase of x in ""hours of work per week,"" evaluate the logistic function at the intercept, and then evaluate the logistic function at the intercept + x*.08."
statistics,3cmjf9,dmlane,1 point,Thu Jul 9 02:08:43 2015 UTC,"Logistic function:       A logistic function or logistic curve is a common ""S"" shape (sigmoid curve), with equation:     where e = the natural logarithm base (also known as Euler's number),    Image i - Standard logistic sigmoid function     Relevant: Generalised logistic function | Malthusian growth model | Sigmoid function | Logistic regression   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
statistics,3cni8x,Congruence,3,Thu Jul 9 07:35:28 2015 UTC,"It should be a PAIRED test because you are checking if doctors agree in their measurements. Else t-test checks if MEAN age of skeletons in two INDEPENDENT groups are the same. You should use nonparametric method, probably, if you have not tested sets for normality (Or if it at least looks normal visually on qqplot). Wilcoxon signed-rank test should be fine for you. You can also use Kendall rank correlation coefficient for two sets, but it won't tell you if this sets actually have same distribution. Kholmogorov-Smirnov test will provide you answer if sets are equally distributed.   Actually, you can find best criteria yourself there: https://en.wikipedia.org/wiki/Nonparametric_statistics"
statistics,3cni8x,severemand,1 point,Thu Jul 9 11:36:42 2015 UTC,"Nonparametric statistics:       Nonparametric statistics are statistics not based on parameterized families of probability distributions. They include both descriptive and inferential statistics. The typical parameters are the mean, variance, etc. Unlike parametric statistics, nonparametric statistics make no assumptions about the probability distributions of the variables being assessed. The difference between parametric model and non-parametric model is that the former has a fixed number of parameters, while the latter grows the number of parameters with the amount of training data.  Note that the non-parametric model is not none-parametric: parameters are determined by the training data, not the model.     Relevant: Mathematical statistics | Kruskal–Wallis one-way analysis of variance | StatPlus | Philosophy of statistics   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
statistics,3cni8x,autowikibot,1 point,Thu Jul 9 11:38:17 2015 UTC,"Thank you, I started off doing paired, but was convinced otherwise by the argument that they do not directly affect each other, but your explanation makes perfect sense.  Just spoke to one of them, she said they usually plot them linearly against each other and compare the R squareds (there are three raters, and the goal is to find out whether e.g., a & b vs. a & c agree the most). Does that make sense, or would you stick to a t-test?"
statistics,3cni8x,COOLSerdash,3,Thu Jul 9 14:33:23 2015 UTC,"Have a look at the Bland-Altman plot. This plot is often used to visualize inter-rater-agreement. I don't see Cohen's kappa as particularly useful here because it concerns categorical data whereas you have continuous data (age). You could categorize the ages into groups by percentiles, for example. Have a look at these posts: first, second."
statistics,3cni8x,efrique,3,Thu Jul 9 12:10:31 2015 UTC,"Any pointers on a test which would tell me what I need to know?   What exactly do you need to know, and why do you think a test is the tool you need?  If you want to investigate the way the two relate on paired assessments (the values are paired on skeleton, naturally), a Tukey sum-difference plot (which is sometimes misnamed a Bland-Altman plot) would be the first thing I'd look at."
statistics,3cni8x,efrique,1 point,Thu Jul 9 12:59:14 2015 UTC,"Basically, it's an attempt to look at whether the differences in methodology result in a difference in assessments. I'll have a look at the plot, thank you."
statistics,3cni8x,efrique,2,Thu Jul 9 13:53:02 2015 UTC,"Are you looking for a change in location (say mean or something like that), or more general kinds of difference?"
statistics,3ckw9q,FlyingCarsArePlanes,7,Wed Jul 8 18:30:27 2015 UTC,Intro to Statistical Learning is a good place to start for learning about building predictive models and holding your hand through data analysis with R. There's a link on that page for a PDF copy if you don't want to buy the physical text.
statistics,3ckw9q,normee,5,Wed Jul 8 19:09:57 2015 UTC,"If you're literally just getting started, judging by those two interests, The Signal and the Noise and Moneyball would be good reads.  Plus a Statistics 101 textbook."
statistics,3ckw9q,likestoreadreddit,3,Wed Jul 8 18:59:33 2015 UTC,Any recommendations on a Statistics 101 textbook?
statistics,3ckw9q,Valgor,3,Wed Jul 8 19:33:09 2015 UTC,"Some might be better than others, but really any cheap book off Amazon will probably do.  Search for ""introduction to statistics"", find a good popular book, then buy an earlier edition.   edit: words"
statistics,3ckw9q,Katdai,1 point,Wed Jul 8 20:31:27 2015 UTC,Statistics by Freedman.  I like how he insists you learn the correct reasoning behind the formulas.
statistics,3ckw9q,michaericalribo,2,Wed Jul 8 23:28:04 2015 UTC,"It depends on your goals. If you just want to learn enough to build some predictive models, hack away at some data analysis on the weekends, etc, I'd recommend a combination of learning Python+data analysis packages (NumPy+pandas etc), or R; and using an introductory statistical learning / 'machine learning' / data mining text---Intro to Statistical Learning is a good one. The idea here is to get your hands dirty with programming and dealing with real datasets quickly---that's what a lot of statistics and data science is---and augment technical skills with some useful techniques and algorithms.  If you're interested in starting to study statistics as a broader field, you'd do well to start with an intro stats textbook. That'll give you very basic information on probability, statistical inference, and some exploratory data analysis. Next steps might be a regression / ANOVA course; you could accomplish this using an intro stats textbook, Khan Academy, open courses from MIT and Stanford, etc  Neither of these will give you a very deep or sophisticated knowledge of statistics---you'll be learning a lot of skills to take at face value without understanding, and it'll be just the very top layer. This is enough for a lot of people: you might want to learn to program and use data to predict interesting things in your spare time.  Anything more in depth than that requires fluency in (multivariate) calculus and linear algebra. A basic course in mathematical statistics depends entirely on the student's ability to wrestle with all sorts of strange multivariate calculus tricks. And anything beyond a crude study of regression / applied statistics is built entirely on linear algebra. There are, of course, degrees of depth to these topics; but serious study of statistics is very akin to a math degree."
statistics,3ckw9q,Falling__Up,1 point,Thu Jul 9 02:11:40 2015 UTC,"I was going to say something very similar. If you want to actually have a career in data science, it is almost required to have a formal education. I'd recommend checking these resources out and seeing how you like it with your own projects.   Additionally, I'd recommend immersing yourself in data science beyond fivethirtyeight and Upshot. Once you get a basic understanding of statistics, try and read journal articles from any discipline that interests you. Try and evaluate these articles using your statistical knowledge. What is the experimental design? How do you know if it's significant or not?   Finally, I'd recommend listening to podcasts as well, if that's your thing. Data stories and Data Skeptic Podcast are two that I listen too (fivethirtyeight also has a new podcast that they just started)."
statistics,3ckw9q,michaericalribo,1 point,Thu Jul 9 02:38:27 2015 UTC,My goal is just to build some predictive models.  I already downloaded ISL.  And I'm probably just going to screw around with Excel and Applescript.  I've become relatively proficient in both.
statistics,3ckw9q,eltoro,1 point,Thu Jul 9 11:52:48 2015 UTC,"That's a good way to start: the apocryphal story has it that Nate Silver started out with massive color coded Excel sheets.  But, Excel is going to have limitations; don't be discouraged if you run into these pretty quickly. Excel is just less of a programming language than you'll often need in statistics, but the beauty of the ISL book is it will walk you through some R code, too. The benefit being that if you want to go deeper or further, it's really only incremental.  For what you're looking for, I'd also recommend against an introductory stats book. That's like going to architecture school (or maybe taking a technical drawing course in your spare time) when all you want to do is build some cool Lego towers. There's a lot of interesting things you can do without trying to learn all of statistics from the ground up, and ISL / predictive models will be much more of an instant gratification.  Good luck! And most importantly, enjoy yourself. You're in for a treat.  Edit to note that in fact, Excel macros and VBscript are 'legit' programming languages, and can probably do everything R or Python can...eg, MCMC has been done, etc. But my experience is that very quickly you end up fighting against Excel to do tasks that are basic or trivial in R or Python, and the visual style of workbooks gets cluttered fast. Is it impossible to do 'real statistics' in Excel? Of course not, just like you can probably find a way to hammer a nail with a paperback book. It may be more comfortable and easier in the long run to use a hammer, but if all you have is a paperback for now, you should try it out to make sure you actually enjoy hammering nails and buying a hammer is worth your money."
statistics,3ckw9q,mynameismarkcarlos,1 point,Thu Jul 9 14:47:21 2015 UTC,"Super Crunchers by Ian Ayres gives a good overview of successful prediction models.   Black Swan by Nassim Taleb is essential for developing a healthy skepticism of overprecise models, especially in forecasting. If you like Black Swan, Anti-Fragile is even better (although less focused on statistics and modeling)."
statistics,3ckw9q,mynameismarkcarlos,1 point,Thu Jul 9 01:51:47 2015 UTC,"What's your mathematical background? If you don't have any background in Multivariable Calculus, or Linear Algebra, I recommend you develop that. There are a ton of online resources for that, from Khan Academy to MIT OCW.  In the meantime, Statistics Done Wrong is a fun and good book to read through. It's also for free!"
statistics,3ckw9q,normee,1 point,Wed Jul 8 21:28:08 2015 UTC,"I always did well in high school math, but I majored in Religion, where (obviously) math wasn't a priority.  I passed a college algebra course, but it was a college algebra course.  Why would multivariable calculus or linear algebra be applicable?"
statistics,3cikg9,blueberry_crepe,8,Wed Jul 8 04:42:18 2015 UTC,Bayesian Data Analysis 3 (BDA 3)  It does a great exposition on why Bayesian analysis is useful in terms of theory as well as practice. Very fun to read and includes TONS of examples.
statistics,3cikg9,quattro,7,Wed Jul 8 06:09:42 2015 UTC,Introduction to Statistical Learning by Tibshirani and Hastie.  Lots of R code examples.
statistics,3cikg9,EvanstonNU,3,Wed Jul 8 07:00:31 2015 UTC,"Kruschke's book on Bayesian stats might also be useful for you, and it's a little bit of an easier read than BDA3. Lots of examples and code, as well."
statistics,3cikg9,srkiboy83,1 point,Wed Jul 8 10:00:09 2015 UTC,"Kruschke looks like a better read imo, but since these books are very expensive, I would like a second opinion. It seems like BDA3 is to intro Bayesian what Agresti is to intro Categorical Data Analysis, which is to say they are seminal. Anyone can confirm that Kruschke will give me a good Bayesian foundation as well?  Also, why are people directing me to Bayesian? Is that the next step after taking a course in both Regressions and Categorical Data Analysis?  edit: looking around, it also seems like Kruschke is really introductory. Like, I could start reading it with no stats background. Is that correct? I'd prefer to build off what I already know (even if it isn't that much. Maybe I've overestimating how much stat I know already)."
statistics,3cikg9,FappingNowAMA,1 point,Wed Jul 8 12:43:18 2015 UTC,"Also, why are people directing me to Bayesian?   I was wondering the same thing..."
statistics,3cikg9,_dog_welder,2,Wed Jul 8 23:18:32 2015 UTC,"BDA 3 is great, if you want to get into Bayesian stuff.    Agresti has a new book out, Foundations of Linear and Generalized Linear Models.  Some overlap with Categorical Data Analysis, but probably a good place to go if you like his style (I do) and want a solid background in regression.  Kutner et al. Applied Linear Statistical Models is a classic, but I don't think the examples are R-based.  Very thick book that can do double duty as a door stop, melee weapon, etc.  Since you're into Epi, check out the upcoming causal inference book by Robins and Hernan.  It's going to be a classic.  Draft chapters are still available for free, just google their names."
statistics,3cikg9,_dog_welder,1 point,Wed Jul 8 07:08:36 2015 UTC,Will the new Agresti be useful if I've already read Chatterjee?
statistics,3cikg9,StatNoodle,1 point,Wed Jul 8 12:32:08 2015 UTC,"Yeah, Chatterjee just scratches the surface.  But it really depends on the kinds of problems you're working on."
statistics,3cikg9,det-mittens,2,Wed Jul 8 13:43:10 2015 UTC,"In All Likelihood: Statistical Modelling and Inference Using Likelihood by Yudi Pawitan  Move to likelihood.  It is an even-handed presentation of the single most important part of statistics theory: the likelihood function.  Whether you are Bayesian, Neyman-Pearson Frequentist, NHST frequentist, or relative likelihoodist, you have to start here...not with Bayes.  This book is the best that I have seen.  It is very tractable.  And is now available in an very affordable paperback format ($60 new).  Most of the other suggestions here mystify me."
statistics,3cikg9,homercles337,1 point,Wed Jul 8 16:27:03 2015 UTC,"Hello,   I just purchased ""Statistics (the Easier Way) with R"" from Nicole Radziwill.   It has pretty good reviews on Amazon and is new, maybe you could try it."
statistics,3ckio9,legends444,2,Wed Jul 8 16:57:39 2015 UTC,"What kind of path analysis are you using? Testing for mediation has changed quite a lot in the last decade, and the most common method (Barron and Kenny) has been ""kicked to the curb"" for lack of a better way to describe it - instead there are a bunch of new methods, including the Sobel test, bootstrapping, and Preacher & Hayes methods. Usually, the directionality of the outcome isn't what is important - it's the significance of the effect. With a full mediation the input variable should stop being significant, with partial it should drop in significance. In other words, the explanatory power of the model doesn't rely on the directionality of the effect per se, but instead on whether there is an effect or not..."
statistics,3ckio9,guitarelf,2,Wed Jul 8 17:11:44 2015 UTC,I wouldn't say kicked to the curb...  Baron and Kenny just claim the x -> y path should become non-sig when that is not true.  a*b just has to be significant.  They have too many checks.  I agree with you about sobel test and bootstrapping.  Bootstrapping is crazy easy with Hayes's macro for SPSS.
statistics,3ckio9,master_innovator,1 point,Thu Jul 9 03:48:45 2015 UTC,Sorry - I should have clarified - in most journals in my field it is no longer acceptable as a test for mediation
statistics,3ckio9,guitarelf,1 point,Thu Jul 9 03:49:52 2015 UTC,MY field views that as extremely outdated too.
statistics,3ckio9,Case_Control,1 point,Thu Jul 9 16:40:13 2015 UTC,"Whoops I'm sorry for not being more detailed! I'm working off summary data (AKA meta-analytic path-analysis). This way, I've built a correlation matrix based on the work of other meta-analyses to fill in the empty cells that I couldn't fill because my current meta-analysis could only fill one of the rows for the variable I meta-analyzed that no one else has."
statistics,3ckio9,AllezCannes,1 point,Thu Jul 9 16:42:01 2015 UTC,"My thoughts will assume you know what directed acyclic graphs are. If you do not it won't make a lick of sense, and I would strongly suggest you do some reading before proceeding with any sort of mediation analysis (plenty of stuff on google).   You need to think very carefully about the causal assumptions you are making in the mediation model. One alternate explanation, would be that there is a confounder between  X and Z (U1) and another confounder between Z and Y (U2). This wouldnt normally confound the the path from X to Y, because the backdoor path from X to Y through U1 and U2 is blocked at the collider at node Z. Conditioning on Z opens up the pathway inducing  a confounding pathway from X to U1 to Z to U2 to Y, this is called M-bias (draw it out and you'll see why). You would need to also include either U1 or U2 (or both) in the model to fix this."
statistics,3ckio9,AllezCannes,1 point,Wed Jul 8 20:42:17 2015 UTC,"Out of curiosity, is the correlation between Z and Y stronger than the correlation between X and Y? And is there a strong level of correlation between X and Z?"
statistics,3ckio9,AllezCannes,1 point,Fri Jul 10 03:27:26 2015 UTC,X - Z is .69 Z - Y is .40 X - Y is .19
statistics,3ckio9,AllezCannes,1 point,Fri Jul 10 03:41:07 2015 UTC,That's what I was thinking. The negative coefficient is the result of multicollinearity between the 2 IVs (X and Z).
statistics,3cki2d,DaneKast,1 point,Wed Jul 8 16:53:16 2015 UTC,"I don't have any recommendations but could you perhaps give some extra information on the content of the course? Intro to probability could mean a ""non rigorous"" class on basic definitions etc. that are used throughout statistics. Or it could be a course based on measure theory. It seems like your course is of the first type but I ain't certain. And if it's the first type there's still some kind of continuum of how rigorous the course is.   tl;dr: I suggest you also post the name of the book that you're using/give the link of the course homepage."
statistics,3cki2d,Noshgul,1 point,Wed Jul 8 18:02:03 2015 UTC,"Thanks for the advice. I edited the post with the course page, which I would think has all the info you recommended I add, probably worded better than I would if I tried to paraphrase."
statistics,3cki2d,howdidiget,1 point,Wed Jul 8 18:30:11 2015 UTC,"One of my favorite texts is linked in a previous comment; specifically ""Probability, RVs etc"".    That being said, what's not clicking?"
statistics,3cki2d,thrownaway42long,1 point,Fri Jul 10 11:14:54 2015 UTC,"Thanks for the recommendation. I'll check it out.  That said...I'm not sure. When we go over stuff in class, everything seems to make perfect sense. I don't have a history of test anxiety, so I can't pin it on that...the only thing I can really pinpoint is that two days before the day of the last test, we had a massive heat wave coupled with our A/C going out, and my wife developing a horrid cough, which means I was very sleep-deprived. However, I view that as an excuse for poor performance, and not a reason, so I just want to really hit things hard and make sure I'm cementing my knowledge. I've always found the best way to do that is through practice."
statistics,3cki2d,homercles337,1 point,Fri Jul 10 16:32:04 2015 UTC,I took an intro to probability class earlier this year at UC Davis and the text we used was this . The problems at the end of each section should be good practice and the answers are on the very next page so it's easy to check if you got them right.
statistics,3ciisd,blueberry_crepe,2,Wed Jul 8 04:25:21 2015 UTC,"1) Yes, it can be, depending on what you want to do for a career. My experience is that people in industry generally want to see someone with strong subject area knowledge first and then have Math/Stat be a supplement to that. Data Science may be a bit different, but it still does stress the subject area knowledge.  2) I would think that you can get in somewhere at least with your background. I'm not sure how much credit you're going to get for your stat classes though if they were taught in the social sciences. There's a little bit of difference between taking Stat in say, an Econ or Education department than a Math or Stat department.   If you were planning on seriously thinking about it, take a look at the required reading for the intro MS level Mathematical Stat classes at a few different tiers of schools. You'll almost certainly have to take 2 of those classes and have them be the focus of half your comprehensive exams. Regression, and Multivariate Statistics may make up a good chunk too while the other Stat courses like Design of Experiments, Non Parametric, Quality Control, Data Mining, etc. are probably going to be electives.   Again, focus mostly on the reading lists for the Mathematical Stat classes. The experience is going to be totally different from a Top school and a bottom ranked one. With a background in calculus and some exposure to stat, you should be able to jump right in."
statistics,3ciisd,fortyninerbruin,1 point,Wed Jul 8 06:44:41 2015 UTC,"I took all my statistics courses in the statistics department. Also, I am mainly interested in social policy, which I'm told is very quantitative. Obviously this career goal would make a choice between an MPP vs a MS Statistics natural. The reason I'm leaning to stat is that the quantitative aspect of social policy seems more interesting, and frankly I want to make myself more marketable and have more options (e.g. with a stats degree, I could shift into finance or something if I find out social policy isn't a field I want to work in).   What type of math pq do stat programs generally desire? I'm surprised that you are telling me my coursework background is sufficient. Are there certain programs that cater more to those with weaker backgrounds? Are any of them top ranked (and how important is degree prestige in statistics programs and the community)?"
statistics,3ciisd,fortyninerbruin,1 point,Wed Jul 8 12:49:52 2015 UTC,"Depends on what tier you want to go into.   Your Stanfords, Berkeleys, etc. will probably want to see lower division calculus, Linear Algebra, some programming, Real Analysis. Regional, less known schools will probably say that Calculus and Linear Algebra will be sufficient.   Prestige necessary depends on what you want to do and how high up you want to go. If your goal is industry (doesn't really sound like it is), then a degree is a degree. The alumni connections will matter more than the strength of the school. I can't say for government and more public positions though.   Although, I was placed at a top research hospital in a post-doc like position. I've been told that if you didn't go to Harvard or MIT, you'd be treated as a second class citizen. I don't recall if the Stanford or Johns Hopkins people were included or not."
statistics,3ciisd,EvanstonNU,1 point,Wed Jul 8 21:59:05 2015 UTC,Hmm. Well it seems like the T10 Stat will be out of my reach due to my low math coursework. Perhaps a quantitative MPP might be best if I want to remain at a elite university (not that there is anything wrong with regional universities to those who go to one)
statistics,3ciisd,tree_man,2,Thu Jul 9 01:00:19 2015 UTC,"I was in a similar situation (wanted a MS Stats, but I didn't have the math background).  I decided to get a MS Analytics.  The degree was very focused on applications and programming.  When math was needed, the instructors would fill in with some simple tutorials (matrix operations mostly). The degree doubled my salary."
statistics,3ciisd,sulandra,1 point,Wed Jul 8 14:53:47 2015 UTC,May I inquire which program you attended? I'm currently looking at analytics programs as well.
statistics,3ciisd,EvanstonNU,1 point,Wed Jul 8 17:56:22 2015 UTC,"With a name like EvanstonNU, I would presume Northwestern."
statistics,3ciisd,beaverteeth92,1 point,Thu Jul 9 03:33:32 2015 UTC,"Northwestern University (full time, on campus). NCSU and USF are also good universities with good programs."
statistics,3ciisd,xkq227,1 point,Thu Jul 9 14:53:23 2015 UTC,"You could probably get in somewhere, but you'd need linear algebra, probability, and mathematical statistics to do well (I'm assuming you have regression already).  Look into Carnegie Mellon's Statistics and Public Policy program too.  It seems exactly what you're looking for."
statistics,3cgkci,chem4u,2,Tue Jul 7 19:12:23 2015 UTC,Glass door?
statistics,3cgkci,I_am_a_lion,1 point,Tue Jul 7 22:07:28 2015 UTC,Based on your username are you organic or inorganic?  Im guessing organic.  I worked with tons of organic chemists.
statistics,3cgkci,homercles337,1 point,Wed Jul 8 02:41:33 2015 UTC,what is your line of work?
statistics,3cgkci,homercles337,1 point,Wed Jul 8 20:37:13 2015 UTC,Computational scientist.
statistics,3cgkci,bigmansam45,1 point,Thu Jul 9 15:56:31 2015 UTC,"If one exists, I very much doubt it would be any where near accurate.    There was one in the UK; it did if you get X degree you will probably in one of these careers on Y salary after Z experience.  It was very inaccurate."
statistics,3cf3v0,IRunFast24,3,Tue Jul 7 12:36:20 2015 UTC,"I'm going to assume that the 4 indicators are a subset of the full variance-covariance matrix.  So you have other latent factors, and other indicators of those factors.  First, calculate the simple correlation matrix of all observed indicators.  What you will likely see is that 3 of the 4 indicators of the latent factor under question are moderately intercorrelated, but the fourth has only weak correlation with the other three.  That is the root cause of why one indicator is not a significant correlate of the latent factor.  There is something about that indicator that is ""weak"".  Maybe it really isn't a measure of that latent factor.  Maybe it has a lot of error variance.  Or maybe it is correlated with other things such that it does have some ""signal"" but is correlated with other sources of noise such that it simply does not correlate with the other three, and hence is not a good indicator of the latent factor that you are letting it load on.  Stepping back a moment, the latent factor can be thought of as a metric that is a composite of the shared variance of it's observed indicators.  If one indicator proves to be defective, you can usually drop it from the model, but it is always good to note that in doing so you have revised your measurement theory, and you should have an explanation as to why that revision is needed.  Not just ""it didn't correlate well"" but ""it didn't correlate well, and my explanation is that perhaps..."".   Sometimes the explanation is ""I really don't know"" and that's OK.  Just don't be in the habit of revising your measurement theory and measurement model based on the data without reflecting on the implications for your theory."
statistics,3cf3v0,wil_dogg,1 point,Tue Jul 7 15:13:41 2015 UTC,Your explanation is beyond helpful. Thank you so much!
statistics,3cfhpt,InsanelyHotUncle,4,Tue Jul 7 14:40:52 2015 UTC,"The 'full information' in Full Information Maximum Likelihood refers to using a system of equations to estimate a parameter rather than a single one as in Limited Information Maximum Likelihood (LIML).  Both LIML and FIML are instrumental variable(s) methods generally used for causal inference, and have nothing to do with missing data.  If you just want to predict something and have a lot of missingness, you can either use imputation (Google it), or for each variable make a missing indicator and set the missing values equal to 0.  Example:      age age_missing adjusted_age  1:  31           0           31  2:  47           0           47  3:  19           0           19  4:  NA           1            0  5:  25           0           25  6:  26           0           26  7:  41           0           41  8:  20           0           20  9:  29           0           29 10:  NA           1            0"
statistics,3cfhpt,npgraham1,1 point,Tue Jul 7 19:20:28 2015 UTC,You can also use bayesian statistics/mcmc to impute missing data.   http://www.bias-project.org.uk/Missing2012/Lectures.pdf http://cran.r-project.org/web/packages/mice/mice.pdf
statistics,3cfhpt,troutmn_q,-1,Thu Jul 9 23:46:33 2015 UTC,"Someone with a deeper background in stats may give you a better answer that's more aligned with what you're already doing, so take this with a grain of salt. As I understand it, a Bayesian analysis doesn't require anything special to impute missing values. You can run your analysis without being too concerned about the missing values--the consequence of too many missing values is that your estimates will reflect more of the prior in the posterior.  If you're already familiar with MLE, I'd suggest grabbing Kruscke's ""Doing Bayesian Data Analysis, second edition."" He does a good job of helping bridge the gap between frequentist and Bayesian methods, for a given research design.  EDIT: downvoter care to elaborate why?"
statistics,3cfhpt,kiwipete,1 point,Tue Jul 7 15:26:28 2015 UTC,"Didn't downvote you but can elaborate:  Missing values are not numeric so they cannot be included in the design matrix. What does it mean to sum the vector {1,2,1,4,NA,5}? So observations with missing values must either be dropped or imputed (one way or another - multiple imputation, zero imputation, mean imputation, etc...). There are assumptions that must be made either way. Imputation (generally) assumes that your missing values are Missing At Random. That said, dropping cases does too..."
statistics,3cg6f4,keeplosingpasswords,1 point,Tue Jul 7 17:36:53 2015 UTC,"What do you mean ""good fit""? We can't tell you what to do with your life. Especially because you gave us no information about you likes and interests.  Do you like statistics? Do you like analyzing data? Then as Stats masters is good for you.   I'm sure you could get into a masters somewhere. You're right that those C's are problematic. You should consider taking real analysis as a transient student. If you do well, it will help offset those lower grades."
statistics,3cg5y4,manthew,5,Tue Jul 7 17:33:40 2015 UTC,I'm not entirely sure why you are asking this.  If you want to do it then do it.  Any difficulty that arises in math or other areas can be over come if you truly want to pursue a new degree.
statistics,3cg5y4,Valgor,1 point,Tue Jul 7 19:35:23 2015 UTC,The math required to get into a stats PhD isn't as much as you're expecting. If you were prepared for an econ PhD then you're fine for stats.
statistics,3cg5y4,webbed_feets,0,Wed Jul 8 03:12:13 2015 UTC,This is what postdocs are for.
statistics,3cg3wq,AllezCannes,3,Tue Jul 7 17:19:42 2015 UTC,"The size of the coefficients are dependent on the scale of the IV, so comparing coefficients is useful only if the scale of the IV's are held constant.  In your situation of using product ""ratings"" that may be reasonable.  One method I use is to sum the Wald chi-squares for all IV's, and then divide each Wald chi-square by the sum of all Wald chi-squares.  That is a rough approximation to the weight of each variable in the final equation, where the weights will add to 100% for each equation.  You could average those weights across equations.  Average the weights, not the chi-squares."
statistics,3cg3wq,wil_dogg,1 point,Tue Jul 7 17:39:36 2015 UTC,Thanks!
statistics,3cdgu5,dorik,2,Tue Jul 7 01:35:20 2015 UTC,In general this is a bad idea - see Jaeger (2008) for more details on why binomially distributed data violate ANOVA's assumptions and can result in false positives. Logistic regression is a better alternative & the paper I linked works through an example using R.
statistics,3cdgu5,raising_is_control,5,Tue Jul 7 05:08:25 2015 UTC,"There are two levels to answering this question: That of the significance test, and that of a predictive model.  Regarding the first level: Given OP's description, significance testing using ANOVA and logistic regression will likely yield similar results. ANOVA does break down when used on proportions, and does so more readily when N is small and/or the rates are close to 0.0 or 1.0. However, for middle-of-the-road values, normal approximations will work fairly well, so the bias should be small. Of course, at that point, one may as well use a Kruskal-Wallis test and bypass the assumption of normality altogether. In that case, you're at least not violating the assumptions of the test (however mildly).  (Of course, given OP's more detailed description, the data appear to have both within- and between-subject effects. Dealing with those optimally requires a multilevel model, which complicates just about everything)  Regarding the second level: Having run an ANOVA (or equivalent test), one still hasn't really determined what happened. The biggest advantage of biting the bullet and using logistic regression instead is that, if you have built the analysis correctly (e.g. if you have nested your predictors correctly), you will be rewarded with a p-value and a predictive model. In almost all cases, this will yield a much more fruitful understanding of the experiment's implications, and will better motivate follow-up experiments.  TL/DR: An ANOVA is literally the least you could do."
statistics,3cdgu5,belarius,0,Tue Jul 7 08:42:27 2015 UTC,"I see where you would think OP's DV is categorical. But, it's not. DV can be any number from 0 to 1 and the the sum of the two group avgs does not equal 1."
statistics,3cdgu5,KSUpsych,1 point,Tue Jul 7 05:41:30 2015 UTC,"An error is categorical - either a 0 or a 1. Logistic regression can easily take into account different numbers of observations across conditions, one of its biggest benefits."
statistics,3cdgu5,raising_is_control,1 point,Tue Jul 7 05:43:25 2015 UTC,"Yes , but that is not what OP means. OP's DV is (number of task errors correctly identified/number of total errors in the task)."
statistics,3cdgu5,KSUpsych,3,Tue Jul 7 05:46:03 2015 UTC,"Oh, I see. I totally misunderstood! That's still categorical still though, right? Subjects either identified the error or they didn't. Maybe I just don't know enough about the design, though..."
statistics,3cdgu5,raising_is_control,1 point,Tue Jul 7 05:48:29 2015 UTC,"No, this is a continuous variable with limits. For example, if the task was for participants to identify grammatical errors in a book chapter and the researcher knew there are 100 errors. If I identified 23 errors, my score would be .23 = (23/100)."
statistics,3cdgu5,KSUpsych,1 point,Tue Jul 7 05:52:41 2015 UTC,But the outcome for each error in the trial is categorical:  Error 1: identified or not  Error 2: identified or not   ...   Error n: identified or not  Repeat for each trial type. And the differing number of errors in different trials would be handled fine in logistic regression but not in ANOVA - see the example study in the article I linked above.
statistics,3cdgu5,raising_is_control,1 point,Tue Jul 7 05:58:22 2015 UTC,Each error is not a trial here. OP is doing group analysis. Not item analysis.
statistics,3cdgu5,KSUpsych,2,Tue Jul 7 06:00:55 2015 UTC,"No, I meant ""trial"" in the sense of some chunk of text or whatever where the participants identify errors. Each of these chunks has a certain number of errors.   Here's a very relevant portion of the article I originally linked:   Categorical outcomes are analyzed using subject and item ANOVAs (F1 and F2) over proportions or percentages. The approach is seemingly intuitive and, by now, so widespread that it is hard to imagine that there is any problem with it. Unfortunately, that is not the case. ANOVAs over proportions can lead to hard-to-interpret results because confidence intervals can extend beyond the interpretable values between 0 and 1. For the above example, a 95% confidence interval would range from 0.52 to 1.08 (= 0.8 +/− 0.275), rendering an interpretation of the outcome variable as a proportion of correct answers impossible (proportions above 1 are not defined). One way to think about the problem of interpretability is that ANOVAs attribute probability mass to events that can never occur, thereby likely underestimating the probability mass over events that actually can occur. This intuition points at the most crucial problem with ANOVAs over proportions of categorical outcomes. ANOVA over proportions easily leads to spurious results."
statistics,3cdgu5,raising_is_control,1 point,Tue Jul 7 06:05:48 2015 UTC,"I'll have to think about this a bit. First blush, you article refers to proportion like proportion of each eye color in a sample, which as you suggest should not be analyzed with an ANOVA. Also, each observation is a trial. However, here each observation/participant only has one score. Groups of scores are compared. ANOVA seems fine. Frankly in this case, if variance surpasses bounds greatly, it's a sign of a methodological problem rather than statistical.  Regardless, I have used this analysis in the past and so has Dr. Timothy Rickard (2008, strategy probes). If you know him, I would love to know what he thinks."
statistics,3cdgu5,KSUpsych,1 point,Tue Jul 7 06:17:23 2015 UTC,"That does not preclude use of logistic regression with binomial outcomes (successes, failures) instead of binary outcomes, such as in section 5.2 here."
statistics,3cdgu5,normee,0,Tue Jul 7 06:08:32 2015 UTC,I never wrote that you couldn't use logistic regression?
statistics,3cdgu5,KSUpsych,1 point,Tue Jul 7 06:21:31 2015 UTC,"With a small number of items, though, it may be more along the lines of a small discrete distribution and approximate something more like an ordinal variable."
statistics,3cdgu5,dearsomething,1 point,Tue Jul 7 17:20:27 2015 UTC,It can't be any number between 0 and 1 though because it is a proportion. Given the value of n that it was computed with there are n+1 possible values for the outcome. This is discrete. I would need to know more about the study but you can use logistic regression with binomial outcomes (which this seems to be).
statistics,3cdgu5,dasonk,1 point,Tue Jul 7 11:40:34 2015 UTC,"Interesting that I can not think of one real data set that is continuous. Also, as I wrote elsewhere, yes, you certainly can do a logit regression. But, that wasn't OP's question."
statistics,3cdgu5,KSUpsych,1 point,Tue Jul 7 13:03:53 2015 UTC,Why not between subject t-test? Do you have a within factor?
statistics,3cdgu5,KSUpsych,1 point,Tue Jul 7 05:30:28 2015 UTC,"Nope, there isn't a within subjects factor. Each participant performs two different tasks (so that we could get enough participants for each task), but each participant does not complete all tasks.  From what I understand, a one-way ANOVA and a t-test are equivalent, right? So I just decided to go with ANOVA."
statistics,3cdgu5,KSUpsych,1 point,Tue Jul 7 06:53:37 2015 UTC,"If each person does two different tasks, that is a within subject factor.  So, you should do a 2 (group manipulation) X 2 (task) mixed-factor ANOVA. Also, if half of the participants did Task A first and half Task B first, then a 2 X 2 X 2 mixed ANOVA is the omnibus test."
statistics,3cdgu5,KSUpsych,2,Tue Jul 7 07:04:31 2015 UTC,"Ah, then I misunderstood what within-subjects factor meant. I think this may be more complicated than I originally thought. Let me give some more details.  There are four tasks in total, and each task has two versions (a high and low error version). The tasks are assigned to participants in a counterbalanced way so that each version of each task is seen the same number of times. Each participant completed two different tasks (one high and one low error version).  There are two experimental groups: primed and unprimed. In the priming group we give them a short exercise beforehand to see if it improved their performance (i.e., higher rate of errors caught).  The questions I'm interested in are:   Is there a difference in errors found between the the primed and unprimed groups. Is there a difference in errors found between low and high-error versions. Is there a difference in errors found between the tasks themselves.   We're also interested in plotting what percent of errors will be found given X people, for example, maybe 3 people can collectively find 75% of all errors, but that's not related to hypothesis testing.  The approach I had in mind was to run 3 separate one-way ANOVAs to test these questions. Now I'm almost certain I'm doing something egregious here, and I'm still in the process of reviewing my old stats notes, but could you let me know if I'm on the right track?"
statistics,3cdgu5,KSUpsych,2,Tue Jul 7 07:20:49 2015 UTC,"I would definitely run 2 X 2 X 2 mixed factor (factor = IV) ANOVA. Each factor corresponds to each of your questions. Plus, you will see interactions, have an honest Type I error rate without need for correction, and will have more power than 3 one-way ANOVAs.  Edit: Your second paragraph suggests a poor research design--You have empty cells."
statistics,3cdgu5,dmlane,1 point,Tue Jul 7 07:30:42 2015 UTC,"You're right, it isn't the best design for this type of analysis. It was designed to find out what are the actual error rates for doing this type of task, since there isn't anything in the literature. The only difference we were interested in was between the two experimental groups. Now that we have the data, I figured it might be interesting to see if we could compare differences for versions and tasks, but that was an afterthought and there are lots of missing cells.  If I could add on another question... Suppose I only use ANOVA to address the first question. Since each participant completed two tasks, my data looks something like:  P1 group1 task1 high-error 0.32  P1 group1 task2 low-error 0.16  P2 group2 ....  Each participant has two entries. Is it still okay to run the ANOVA if each participant exists twice in the dataset?"
statistics,3ca71n,Hairy_Hareng,3,Mon Jul 6 08:47:06 2015 UTC,Nonnegative Matrix and Tensor Factorizations  Chapter 2: Similarity Measures and Generalized Divergences
statistics,3ca71n,farsass,1 point,Mon Jul 6 13:19:41 2015 UTC,Thank you for the ref Ill have to see if i can find it at the library: i cant find a pdf
statistics,3ca71n,farsass,4,Mon Jul 6 14:43:20 2015 UTC,http://gen.lib.rus.ec/search.php?req=nonnegative+matrix+factorizations&lg_topic=libgen&open=0&view=simple&phrase=1&column=def
statistics,3ca71n,Bromskloss,1 point,Mon Jul 6 14:54:32 2015 UTC,Huh? What kind of sorcery is this?
statistics,3ca71n,homercles337,1 point,Tue Jul 7 00:55:49 2015 UTC,"I wrote a paper that used EMD and did the exhaustive reading/citing on distances.  I also implemented all of them, but that was in Matlab and many years ago.  I can share the code if you want though."
statistics,3ca71n,isarl,1 point,Mon Jul 6 15:58:35 2015 UTC,"I'm not currently planning to include any code in there, but thank you very much for the offer.  If you have some good references on various distances, that would be absolutely great ;-)"
statistics,3ca71n,Jimmy_Goose,1 point,Wed Jul 8 15:05:11 2015 UTC,Bhattacharyya and Hellinger distances are pretty similar; a comparison of those would be nice.
statistics,3ca71n,SinnyCal,1 point,Mon Jul 6 16:30:22 2015 UTC,"kolmogorov distance (is that the proper name for the max distance between cumulative density functions ?)   I think the proper name is the infinity norm, or Linfinity norm. I might be wrong though..."
statistics,3ca71n,StatNoodle,1 point,Mon Jul 6 17:42:01 2015 UTC,Check out Renyi divergence or the alpha family of divergences
statistics,3c7c2v,donnemartin,3,Sun Jul 5 15:46:36 2015 UTC,gold.  thank you
statistics,3c7c2v,stabbinfresh,1 point,Mon Jul 6 01:52:24 2015 UTC,No problem :)
statistics,3c7c2v,DrLionelRaymond,1 point,Mon Jul 6 12:40:12 2015 UTC,I've seen your repo posted at least a dozen times in various subs and it never fails to get tons of upvotes. Really quality stuff. Thanks!
statistics,3cbsdw,pstavro,6,Mon Jul 6 18:06:58 2015 UTC,"Guideline #3: ""Just because it has a statistic in it doesn't make it statistics."""
statistics,3cbsdw,normee,1 point,Mon Jul 6 18:36:50 2015 UTC,"Crap, sorry, didn't see that. Thanks for the reminder."
statistics,3c7huo,asymptotics,1 point,Sun Jul 5 16:43:22 2015 UTC,What's your current level of statistical training? What are you looking for... new areas of application? theory?
statistics,3c7huo,Distance_Runner,1 point,Sun Jul 5 18:58:42 2015 UTC,"Good foundation in classical mathematical statistics, some background in Bayesian methods."
statistics,3c7huo,luiggi_oasis,2,Sun Jul 5 19:34:16 2015 UTC,"Casella & berger, and gelman's BDA are all you need."
statistics,3c7huo,FappingNowAMA,1 point,Sun Jul 5 19:37:44 2015 UTC,"Neyman from Life - a biography on Neyman. The author spent a fair amount of time speaking directly with Neyman, and sheds a lot of light on the beef he had with Pearson. I came away with a very different view on Pearson as a person  (granted, from hearing one side of the story)"
statistics,3c7huo,statmama,1 point,Mon Jul 6 02:01:54 2015 UTC,Surprisingly solid advice from someone who's currently fapping.
statistics,3c7huo,kick_muncher,0,Mon Jul 6 02:10:19 2015 UTC,Excel
statistics,3c83kz,rmartini,1 point,Sun Jul 5 20:00:36 2015 UTC,Do you know the definition of power? Or can you state the definition but are not clear about some aspect of what it means?
statistics,3c83kz,efrique,1 point,Sun Jul 5 23:41:39 2015 UTC,Unfortunately its not really a concept a five year old would be able to understand.  Here are some resources to get you started...   Wikipedia - Statistical Power Statistics Done Wrong - Statistical Power   A book I often recommend for people not too concerned with the mathematics involved in statistics but rather concerned with understanding the underlying principles and interpretation of analyses is Intuitive Biostatistics by Harvey Motulsky
statistics,3c83kz,enilkcals,1 point,Mon Jul 6 09:00:42 2015 UTC,"Statistical power:       The power or sensitivity of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis (H0) when the alternative hypothesis (H1) is true. It can be equivalently thought of as the probability of correctly accepting the alternative hypothesis (H1) when it is true – that is, the ability of a test to detect an effect, if the effect actually exists. That is,     The power of a test sometimes, less formally, refers to the probability of rejecting the null when it is not correct. Though this is not the formal definition stated above. The power is in general a function of the possible distributions, often determined by a parameter, under the alternative hypothesis. As the power increases, there are decreasing chances of a Type II error (false negative), which are also referred to as the false negative rate (β) since the power is equal to 1−β, again, under the alternative hypothesis. A similar concept is Type I error, also referred to as the ""false positive rate"" or the level of a test under the null hypothesis.     Relevant: Parametric statistics | Noncentral t-distribution | Sample size determination   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
statistics,3c7inb,tellmeyourstoryman,2,Sun Jul 5 16:51:14 2015 UTC,"Well, first let me refer you to r/homeworkhelp for additional questions, as r/statistics isn't really meant to walk you through homework questions like this.   With that said, it looks like you made a basic algebra mistake, where you're calculating MSwithin by multiplying F by MSbetween.  In fact, MSwithin is equal to MSbetween (20) divided by F (4).   Next, it looks like you've calculated the SSwithin improperly, so go back and recalculate that, based on your new MSwithin value. It'll be 5 x 38 instead of 80 x 38."
statistics,3c4j4s,onan_pulled_out,18,Sat Jul 4 18:33:24 2015 UTC,"1) What size difference would be ""statistically significant"" is largely a function of sample size.  The larger the sample size, the smaller the ""detectable effect size"" is.  Another thing that makes it more difficult to detect small differences is multicollinearity (correlations between being female and other variables, such as experience, height, etc.).  Now, you seem to be confusing the idea of ""practically significant""-- meaning, at what point should we stop caring? That is your personal choice.  2) If we look at enough variables, we could certainly find something arbitrary that could create a similar gap.  3) The huge problem with all studies like this is getting ALL the data one would need to accurately determine whether there is a difference in pay for equal work.  Measuring all of the things that make an employee valuable -- willingness and ability to travel, work weekends, likelihood of having someone else go pick up the sick kid from school, aggressiveness, collegiality, IQ, education, dedication/loyalty, 100 dimensions of job performance...  It is just impossible.  Any variable that you leave out could be the one that really explains the difference.  That said, the better studies with more and better variables tend to have smaller and smaller measured effects.  This tends to say to me that while there might be some ""discrimination"", it is probably either small, or perhaps represents risk that females are more likely to temporarily or permanently leave your firm, which can be a big cost to many firms in many industries.  4) When can we conclude the pay gap no longer exists?  Well, it will probably always exist due to field choices, career choices, job tenure differences, more part timers, etc.  The better question would be when does the pay gap only represent justifiable, acceptable factors?  There will always be differences of opinion on what is acceptable and what is not. ☺  edit: typos..."
statistics,3c4j4s,BurkeyAcademy,2,Sat Jul 4 19:58:54 2015 UTC,Great answer thank you
statistics,3c4j4s,SuperRuub,0,Sun Jul 5 13:47:54 2015 UTC,to be completely fair you can test what variables are important in order to decide which ones to take along in your final regression. You can also do for example PCA to reduce dimensions while sort of taking all the variables along.
statistics,3c4j4s,BurkeyAcademy,2,Sun Jul 5 15:45:44 2015 UTC,"""Testing which variables are important "" is something that isn't really possible, but depends on your definition of important. At least, I do not believe in the mindless criteria (maximizing adjusted R2 , AIC, BIC, or what have you). There are going to be many variables that are important to the model that mindless procedures will leave out, largely due to multicollinearity.  There are also lots of other model uncertainties, such as proper functional form, censoring, including various interaction terms...  It really makes this kind of exercise, where one searches for the ""perfect"" model that captures everything without bias, futile.  That doesn't mean we shouldn't try, but it means we should take results with a grain of salt.  Using factor analysis or PCA is not a panacea, in my opinion.  You are replacing one set of problems for another set.  Omitted variable bias is a big one... sort of including the right variables isn't one of the Gauss-Markov assumptions.  Also our goal is to understand/test why females are earning less, one whole constellation of correlated variables are bundled up with femininity itself.  Check out the hundreds of books on ""female leadership"" and ""female management styles"".  Of course, all of the books say that whatever females are doing, it is better... I will remain agnostic since I doubt I will ever have time to read one of these books, but they apparently list dozens of characteristics that most women have that most men don't. If we construct some variable that somehow captures all of the many dimensions of this management style thing (doubtful), that is seemingly important and correlated with being female...  There goes our precision.  My comments apply equally to most other problems regression tries to solve: do guns cause crime, did abortion lower crime, how much does a college degree increase earnings...  I love regression and use it all the time, but there is a limit to what it can accurately tell us in such difficult data measurement/acquisition and modeling situations."
statistics,3c3klp,bifido,13,Sat Jul 4 12:44:57 2015 UTC,"it's a common misconception that gelman is advocating for multiple comparison corrections.  the point is that p-values are often the right answer to  the wrong question.   he tends to advocate effect estimation with partial pooling. the shrinkage effect due to partial pooling inherently compensates if your base rate of true positives is small - this will appear in the fit of the prior distribution to the data.  however, the catch is that there's no right answer to how much shrinkage is the right amount of shrinkage, and that will usually pretty sensitive to top-level priors on variance parameters.   uncertainty in the top level priors propagates to uncertainty in conclusions and so perhaps the inconvenient truth is that a lot more is unknown than we'd like to believe. Unfortunately, convenient untruths are more amenable to careers."
statistics,3c3klp,klaxion,1 point,Sat Jul 4 13:24:32 2015 UTC,"Thanks for the reply.  I am not superfamiliar with the methodology so if you have time elaboration of how the shrinkage effect compensates would be appreciated.  More importantly and careers aside, is this uncertainty quantifiable? Can you get an idea about if you need more samples or if you need to collect a new type of information? The whole argument seems to boil down to the fact that you cannot really know if a p-value is valid, but can you positively say that the bayesian approach is?"
statistics,3c3klp,doompie,0,Sun Jul 5 22:13:23 2015 UTC,I regret I have but one upvote to give ;(
statistics,3c3klp,dimview,6,Sun Jul 5 06:01:31 2015 UTC,"First, Bonferroni correction is not counting every analysis you could have made, just every analysis you have made.  But more importantly, garden of forking paths is not the only problem. There is also type M errors (overestimating the magnitude because of p-value filter) and other related problems.  There are many ways deal with it, from preregistration (publicly disclosing before the study what you're going to measure and how) to advanced Bayesian techniques."
statistics,3c3klp,anonemouse2010,5,Sat Jul 4 13:43:17 2015 UTC,"The bonferonni correction is a conservative correction and does not necessarily give you the 'appropriate' p-value, that depends on the dependence of the tests.  As for Gelmans comments, it's only a problem when people are fishing for positive results, and that has nothing to do with p-values specifically anyways. If you a-priori specify the tests you want then no, you don't need to correct for all (possible i.e., infinite number of) tests as is suggested.  This is ultimately the problem here, people not pre-specifying their tests or running multiple stage experiments."
statistics,3c3klp,anonemouse2010,10,Sat Jul 4 13:47:28 2015 UTC,"Your comment is hyperbole.  Separating experiments into exploratory and confirmatory parts or having multi stage designs are valid solutions to the stated problem.  THIS HAS NOTHING TO DO WITH P-VALUES.  This is a problem with people who are going on fishing expeditions with questionable data claiming things their designs and exploration can't support.  Regardless of what a pure Bayesian would tell you, this can happen with ANY type of inferential method."
statistics,3c3klp,Coffee2theorems,0,Sat Jul 4 15:04:46 2015 UTC,"Separating experiments into exploratory and confirmatory parts   This is analogous to the machine learning approach of using a training set and a test set, which is standard practice. Downright ubiquitous, in fact! I find it somewhat ironic (or plain amusing) that something in ML is done more rigorously as a matter of course than in applied statistics (1), when it's statistics where people usually tend to be sticklers..  (1) Yes, I called scientific use of statistics ""applied statistics"" with a straight face. Benefit of the doubt and all that. Scientific research is supposed to be the gold standard, right? ... Sigh. Right now I can't help having traitorous thoughts of replacing scientists with computer programs that can reliably be forced to separate training and test sets and otherwise following proper procedures.."
statistics,3c3klp,westmeister,8,Sat Jul 4 15:28:16 2015 UTC,"exploratory and confirmatory != train and test. I'm pretty sure what anonemouse2010 meant is that any analyses the scientists want to be taken as serious evaluations of specific hypotheses (confirmatory analyses) should be pre-specified so that no p-value corrections are needed. Additional analyses can be conducted as well for the purpose of exploratory analysis/hypothesis generation (in response to klaxion's concerns), and for these analyses p-values/Bayes factors/whatever need not be exactly correct since they are only meant as an indication of the possibility of a phenomenon, not scientific proof.   The idea of setting aside some part of your data for testing as you suggested is reasonable  as well, especially for exploratory analyses, but it isn't what anonemouse2010 meant."
statistics,3c3klp,filetoffresh,2,Sat Jul 4 19:32:25 2015 UTC,"A couple things. As far as the garden of forking paths thing goes, its not an infinite set (not at least in the example he gave). It's a combinatorial one. If I have 3 variables and I'm fitting some regression model, sure I could fit a 50th order polynomial to the fata but I'm pretty sure Gelman would be the first one to call over fitting. Realistically, you have a much smaller set of available models.  Next, even with an infinite set of variables we want to test (not even just pairwise comparisons), Scheffe's methods allows us to test all contrasts simultaneously and get a sensible p value back. Lastly, I just wanted to address something Gelman said about p-hacking by choosing your model after you look at the data. That is not an issue with hypothesis testing, not even close. I could estimate means and get the wrong answer if I chose my method after I looked at the data. The real solution to that problem is to cut you datasets up into pieces, lock one piece in fort knox and then have a hacking party with the other piece. Test sets are useful, we should use them."
statistics,3c3klp,filetoffresh,3,Sat Jul 4 21:29:40 2015 UTC,"I'm not so sure that this is the case. As far as efficiency goes, ""statistical efficiency"" means variance and exactly the goal of estimating parameters through cross validation (or test/train) is to reduce the variance of your error metric over different datasets. I'd love to be proven wrong there though. Did you mean computationally inefficient?  As far as power goes, I can imagine choosing my test through cross validation on a training set and validating that test on a test set. Any gains in power from choosing and evaluating my hypothesis test on the same set amount to the same type of overfitting we use train/tests sets to avoid.  Imagine we tried 10 tests  for a difference in means and it turned out the t-test was ""the most significant"". If we then took the t-test and applied it to a new set of the same data and found that ""nothing was significant"" we would call that over fitting, not a decrease in power.  Lastly, training/testing sets are not something used mainly in machine learning. They're just another tool used to try and estimate the true error of a predictor."
statistics,3c5c9a,PijnlijkOlifant,3,Sat Jul 4 23:08:59 2015 UTC,"A standard deviation can be thought of as a ""common distance that individual outcomes of this process will be from the mean"".  Lets suppose that the true mean ( p ) is 3 in 10 or .3 or 30%. Your standard deviation is around 1.45.  Loosely speaking, this means that when you run this process 10 times, most of the time the outcome will be 3 +/- 1.45.  This is a little hard to interpret/fishy with discrete variables, because what does it mean to be within 1.45?  Well, ""commonly"" you will see answers of 2, 3, or 4.  A little easier to think about would be to scale this up to 100,000 repetitions, with mean = 30,000 and s.d = 145.  Now, more straightforwardly, it will be common for you to see results within +/- 145, or between 29,855 and 30,145.  With so many occurrences the central limit theorem has helped to give this distribution a normal-ish shape, so we will find occurrences in this region around 68.26% of the time.  The difference with what you have (""raw data"") is that your 3 in 10 is an estimate, a ""statistic"", not the parameter of a model.  So, you have an estimate of p , normally pbar or ""sample proportion"".  However, using these estimates you can get a similar, though less confident picture of what you might expect in future repetitions."
statistics,3c5c9a,BurkeyAcademy,1 point,Sun Jul 5 01:29:59 2015 UTC,"Thank you, that helped!  But there is still one thing that still confuses me: I didn't do x experiments with y tries but I did 1 experiment with z tries (z being around 10000 I think).  So are you saying that if I do the experiment again (with the same number of tries) a few times I should get a distribution that looks like you described? But I only need the 1 experiment with z tries to be able to estimate what the distribution would look like?"
statistics,3c5c9a,BurkeyAcademy,2,Sun Jul 5 03:46:02 2015 UTC,"Yes, sounds like you have it.  Say we observe a basketball player shoot 10,000 free throws and he hits 30% of them.  With such a large sample, we would have a pretty good idea of 1) His true proportion, p, and 2) what future trials of 10,000 shots might look like.  There is always that small chance that our group of 10,000 shots we observed the first time was way off... but this is a small chance."
statistics,3c5c9a,gschroder,1 point,Sun Jul 5 05:14:06 2015 UTC,"Ok, great, thank you so much! I was really stuck but now I get it!"
statistics,3c3i8r,painperdu,15,Sat Jul 4 12:14:47 2015 UTC,"You would need to know how much bias you are looking for - i.e. how much bias counts as bias. e.g. 1 million coin tosses won't give you much confidence that there isn't bias at the level of 1 excess head/1 million tosses.  This all boils down to a power calculation. So you'll also need to pick values for the alpha (type I rate) and power (1 - type II rate).  Here's a neat calculator that can give you the numbers.*  * Frequentist approach, bayesian approaches available, your mileage may vary."
statistics,3c3i8r,blozenge,9,Sat Jul 4 13:12:18 2015 UTC,"The answer to your question requires answering two other questions: How unfair is the coin in question and how certain do you want to be that it's unfair? For most applications, a event of probability ~2.5% or less is considered reasonable evidence against a given hypothesis, but particle physics requires a 1 in 3.5 million chance that the event occurs randomly before it asserts that they've discovered a new particle.  Let's illustrate this with an example. If you had a completely biased coin that only came up heads and flipped it twice, you would get two heads. A fair coin would do this 25% of the time. What we're measuring here with unfairness is the number of times that heads comes up. In this way we're asking ""What is the likelihood that we get x heads after flipping a coin n times, assuming that heads comes up with a probability of 50%?"" This is our hypothesis.  In terms of statistics we have   p-value = Pr(|x - n/2|; θ = 1/2)   where our p-value is our certainty that the event is unlikely, |x - n/2| is a measure of how unbiased the coin is, and θ = 1/2 is our assumption that the coin is fair. Note that we're measuring the absolute value of |x - n/2|, meaning that it's just as likely for a fair coin to get two heads as two tails.  Jumping back to the example now, our p-value would be 0.5 after two tosses since there's a 25% to get 2 heads (1 more than expected) and a 25% chance to get two tails (1 less than expected). If we toss the coin 4 times our p-value becomes 0.125. To get the illusive 0.05 threshold we're looking for, it would take 6 tosses in total for a p-value of 0.03125. This gives us ""reasonable evidence against the hypothesized value"".  Let's do another example. If our coin came up heads 5 times out of 6 tosses we would have  p-value = Pr(6 heads; θ = 1/2) + Pr(5 heads; θ = 1/2) + Pr(1 heads; θ = 1/2) + Pr(0 heads; θ = 1/2)   or the total probability of that event occurring + the probability of all the events that are more extreme occurring. This comes to  p-value = (1/2)^6 + (6c1)*(1/2)^5*(1/2) + (6c1)*(1/2)^5*(1/2) + (1/2)^6 p-value = 1/64 + 6*1/64 + 6*1/64 + 1/64 p-value = 14 * 1/64 p-value = 0.21875   which isn't enough to say that it's a biased coin.  The general formula can be found in this wolfram alpha link where n is the number of tosses and x is the number of heads. As you can see, 26 heads out of 39 tosses (i.e. a 2:1 heads ratio) gives a p-value of 0.053 which is just outside of the p-value we normally use. Feel free to play around with it and see what you get for different values.  In terms of the numbers you provided in your original question, 5 heads out of 20 tosses (0.25%) is anomalous as is 17 out of 50 (0.34%). Wolfram Alpha didn't like 10000 coin tosses so I stopped there."
statistics,3c3i8r,DesiredUsername,3,Sat Jul 4 14:00:51 2015 UTC,"It depends on what the true probability of a head is, and on the relative cost of making the two kinds of error (false positive - concluding it's unfair when it isn't, and false negative - failing to conclude it's unfair when it is), and - at least if you're a Bayesian - the particular form of the prior on p(Head).  e.g. imagine coin A had p(Head) = 0.3 and coin B had p(Head) = 0.50001 ... coin A will take few tosses to generate a surprisingly low number of heads for a fair coin, but coin B will take a very large number of tosses before you can tell it's not fair."
statistics,3c3i8r,efrique,2,Sat Jul 4 19:03:57 2015 UTC,"Depends on how big of a ""bias"" you want to detect and what confidence level you demand."
statistics,3c5ouu,ziggy24z,2,Sun Jul 5 01:30:35 2015 UTC,"What do you mean by ""compare tables"" exactly? there's any number of things you might compare -- counts, row proportions, column proportions, proportions of table totals, proportions of the (i,j) cell total across tables, odds ratios, etc etc"
statistics,3c5ouu,efrique,1 point,Sun Jul 5 09:27:16 2015 UTC,"I want to know if the proportions of the (i,j) cells across 2 tables are the same or not."
statistics,3c5ouu,efrique,2,Sun Jul 5 16:44:14 2015 UTC,"So if cell (1,1) in table 1 has a count of 15 and cell (1,1) in table 2 has a count of 25 you want to see if 15/(15+25) could plausibly be observed from a binomial(40,0.5)?"
statistics,3c5ouu,efrique,1 point,Sun Jul 5 16:54:12 2015 UTC,"Not exactly, each table has cells (i,j) right, and there are percentages of the count distributed in various cells.  I want to know if the percentages are similar between the two tables. So for example, if cell (1,1) in table 1 has 15, and it's 20% of the total in table 1. Table 2 has  25 in cell (1,1), is this also 20% of the total in table 2? I want to compare the table as a whole like this. Is this possible? Thanks!"
statistics,3c5ouu,Fryhle,2,Sun Jul 5 18:31:31 2015 UTC,"That's the third thing I originally mentioned   proportions of table totals   rather than the fourth.   Yes, it's possible.   Since you're looking at proportion of table totals, rows and columns don't matter; it's a straight multinomial problem that you can just treat as a collection of cells.  So write out each of your tables as one long row:               Cell_1,1  Cell_1,2    ...    Cell_m,n     Total  Table1:      15           9       ...     ...           75  Table2:      25           7       ...     ...          100   You can then test for homogeneity of proportions (which is equivalent to a test of independence) in this 2xk table"
statistics,3c5ouu,kestrel2,1 point,Sun Jul 5 23:40:09 2015 UTC,exactly what do u want to know? are the variables same between two table?
statistics,3c5ouu,autowikibot,1 point,Sun Jul 5 03:12:28 2015 UTC,"so each cell is an integer count, which is also a percentage of the total for each table. I want to know if this specific percentage is the same for corresponding cells (i,j) between the two table? Is my intention unclear or is this possible?Thanks!"
statistics,3c1t3n,27clubhereicome,2,Fri Jul 3 23:45:05 2015 UTC,"I would write this in WinBUGS, OpenBUGS, JAGS, or, maybe, Stan. These are typical software for fitting Bayesian models. You can do so in R as well, but perhaps without the flexibility you require. You'd probably do best collaborating with someone familiar with the methods you're interested in."
statistics,3c1t3n,Frogmarsh,2,Sat Jul 4 01:19:40 2015 UTC,Why not rjags?  Edit: sorry for not reading more closely!
statistics,3c1t3n,jwdink,3,Sat Jul 4 07:59:40 2015 UTC,I said JAGS. RJags is simply the way to access JAGS via R.
statistics,3c1t3n,Frogmarsh,1 point,Sat Jul 4 13:39:07 2015 UTC,"I've been doing everything in R up this point using lme4/nlme and was looking to switch over to blme for this.  The person I work under has an engineering background so this part is kind of up to me, unfortunately.  I'll try to get ahold of one of my professors I guess.  I realize it's difficult to give any advice to me here since I have to be careful not to divulge any details of what it is exactly."
statistics,3c1t3n,Bromskloss,1 point,Sat Jul 4 01:40:14 2015 UTC,"Just a check: Is a prior really what you're looking for, or is it rather a more flexible model that allows for some quantity to vary over the year?"
statistics,3c1t3n,Bromskloss,1 point,Sat Jul 4 09:50:55 2015 UTC,"Could you give some examples for more flexible models just  so I'm sure I understand your suggestion?  I've been trying models so far that do vary across months, but it's been very difficult for me to  capture the shape of the seasonal distribution.  I could just bring it down to a two-level model and combine them into years, but that will kill my current degrees of freedom.  I also wanted to use a prior on this because I've only ever used Jeffrey's priors in my short time of undergraduate research and had a desire to design my own for once since there was enough 'prior' knowledge from studying it during the fall and winter to know these details, but you make a good point.  I think I'll go back to the drawing board and see if there is any less common distribution that can match the shape I'm looking at."
statistics,3c1t3n,hahdawg,2,Sat Jul 4 16:37:07 2015 UTC,"I don't have a clear picture of what the problem you're solving is about, but suppose we're looking to find the rate λ of some events. In the simplest model, the rate is fixed, so λ is our only parameter. If we then want to capture a yearly variation, we might, for example, use the more flexible model λ = λ_0 + λ_1 sin(ω t + φ_0), with the three parameters λ_0, λ_1 and φ_0 (and a constant ω = 2 π / (1 year)).  It's possible that I have misunderstood your question, though."
statistics,3c1t3n,hahdawg,1 point,Sat Jul 4 16:59:14 2015 UTC,Why not just use monthly dummies to capture the seasonal effect?
statistics,3bz8h4,atorch,4,Fri Jul 3 10:13:50 2015 UTC,iOS or web app for those without an Android would be awesome!
statistics,3bz8h4,kkacci,1 point,Fri Jul 3 15:03:04 2015 UTC,"Good idea -- I think I could make a web app without too much effort, given what I already have."
statistics,3bz8h4,DesiredUsername,3,Sat Jul 4 09:14:12 2015 UTC,This is a little probability puzzle app I made for android.  I've posted it here in the past and gotten helpful feedback.  I recently added some graphics to some of the puzzles -- do you think they're helpful?
statistics,3bz8h4,guilhermejardim,1 point,Fri Jul 3 10:15:02 2015 UTC,"I'm really enjoying the app; the layout is good, the responses to getting a question right or wrong are a great touch, and the problems are well written and engaging. My one criticism is that it's hard to find a problem once I've completed it. For example, I really liked the easy problem about the three differently numbered dice that are blue, green, and red. If each problem had a title and I could scroll through a list of them, I would be able to quickly find the problem in question so I could show it to a friend. That and having the answer hidden by default would increase my reuse of the app.  On a different note, are you interested in more problems? I haven't gone through all of them yet, but I'd be interested in sending you some ideas.  Thanks for putting this together!"
statistics,3bz8h4,polished_iconoclast,1 point,Sat Jul 4 14:18:01 2015 UTC,"Thank you for your suggestions, especially about reuse. Feel free to email me -- there should be a contact button on the play store page."
statistics,3bz8h4,TheFrigginArchitect,0,Sat Jul 4 17:43:16 2015 UTC,"Please, send a link"
statistics,3bz8h4,howdidiget,3,Fri Jul 3 13:31:32 2015 UTC,Nice app! Is there any way to see what is the number of a puzzle that I am currently solving? (I am stuck on one of the easy ones...).
statistics,3bz8h4,slacker22,2,Fri Jul 3 17:58:54 2015 UTC,"Not at the moment, but that's a good (and easy) suggestion -- thank you!"
statistics,3bz8h4,ElSuerte,2,Fri Jul 3 20:29:32 2015 UTC,This is fun! I'm working through the easy-peasy's right now
statistics,3bz8h4,bigmansam45,2,Fri Jul 3 14:35:02 2015 UTC,This is pretty cool. How long did this take you?
statistics,3bz6g3,thebigllamaman,3,Fri Jul 3 09:47:27 2015 UTC,"If it doesn't satisfy the triangle inequality it means that your distance measure is not a metric (Euclidean means that the distance uses the L2 norm). If it's not a metric, forget about clustering with it and making any reasonable conclusions based on the output. Choose a better distance measure."
statistics,3bz6g3,vogon_toothbrush,2,Fri Jul 3 14:11:27 2015 UTC,"Depends on the clustering algorithm. Some algorithms (like k-means) need the actual data points. Hierarchical clustering is the only clustering algorithm that comes to mind that uses a distance matrix, in which case, it seems you have an acceptable input."
statistics,3bz6g3,jiggityjanked,1 point,Fri Jul 3 12:11:21 2015 UTC,"Ah, thanks for the answer! unfortunately, it turns out that my distance matrix is not Euclidean, so am working out what would be the best option here."
statistics,3bz6g3,jiggityjanked,1 point,Fri Jul 3 13:27:32 2015 UTC,"Strange, how did you calculate this distance matrix? Or where did you get it?"
statistics,3bz6g3,lieagle,1 point,Fri Jul 3 14:27:15 2015 UTC,"I have some probabilities which represent the probability that any 2 observations in my dataset are not the same due to experimental error. I then did a ln(1/probability) to make the small numbers sane and produced what I thought was a distance matrix. As /u/vogon_toothbrush has pointed out, this variable is not a metric though."
statistics,3bz6g3,Ibarea,2,Fri Jul 3 14:37:21 2015 UTC,I've seen decision trees be used for this kind of data
statistics,3bz1zy,daveWaveV,3,Fri Jul 3 08:45:38 2015 UTC,I can't find the article... you could help with a link or a title...
statistics,3bz1zy,BurkeyAcademy,1 point,Fri Jul 3 15:50:47 2015 UTC,"Here it is. I'm really sorry. I've been so puzzled I  even mixed it up with a previous article. The results are called ""Relative Strength Portfolios"". I've understood now that the T stat tells us how confident we may be about the future. Computing it is still a challenge for me though. Trying to understand the application of the T stat."
statistics,3bxtq0,tdog3456,8,Fri Jul 3 01:21:02 2015 UTC,"You could also use the free and always-will-be-free R! It has the 'manova' function, and there's probably even other libraries in it that use different algorithms for is http://www.statmethods.net/stats/anova.html [PDF warning] http://cran.r-project.org/web/packages/HSAUR/vignettes/Ch_analysis_of_variance.pdf http://www.statmethods.net/stats/anova.html"
statistics,3bxtq0,asoplata,1 point,Fri Jul 3 02:43:24 2015 UTC,"I'll have to look into learning it, thanks man!"
statistics,3bxtq0,wil_dogg,3,Fri Jul 3 12:54:07 2015 UTC,Often the student version has some limitations and I think you found it.  Go to the IBM website and download / install the full version.  It's a big download file so will take a while.  Then you have a fully operational version for 2 weeks.  https://www-01.ibm.com/marketing/iwm/iwm/web/pick.do?source=SWG-STATS-DESKTOP_TRIAL&S_TACT=M161008W&lang=en_US
statistics,3bxtq0,Zorander22,2,Fri Jul 3 01:34:17 2015 UTC,"Great, thank you!"
statistics,3bw6d1,futrawo,4,Thu Jul 2 17:26:37 2015 UTC,"I read a book once called Who's #1 that went into the math behind different sports ranking algorithms, Kendall's Tau, PageRank, and Spearman's Rho were all discussed. It would work out the math with examples in sports. I really enjoyed it and it may help you learn some more about how ranking decisions are made."
statistics,3bw6d1,millsGT49,1 point,Thu Jul 2 18:25:36 2015 UTC,Thanks very much - I have ordered it!
statistics,3bu5yc,john_philip,2,Thu Jul 2 04:28:30 2015 UTC,Can anyone compare this functionality with R? Ease of syntax between the two?
statistics,3bu5yc,FappingNowAMA,1 point,Fri Jul 3 17:58:16 2015 UTC,"These opinions are mostly filtered through me using R/Python for my work, and talking to an economist friend who uses Stata for his work.  Longer explanation below, but here's what I'd recommend: 1. Figure out what your professors/co-workers/your field likes- using the same platform makes inheriting data or learning from their work 100x easier. 2. If your field is split, learn the basics of both so you can work with everyone. 3. Unless you have no time to do so, or your folks all use Stata, or stats are only a small part of your work,  learn R the best- sometime in the future, you'll need to use some obscure package, and knowing R will make that easy.  R is open source and free, Stata costs money. Thanks to the open source nature of R, people are constantly developing new packages for it, so it'll probably have a wider degree of new tools than you'd have in Stata, but if you're not doing anything particularly cutting edge or strange, Stata will work great.  People at my school in quant-y areas generally seem to think that Stata might be slightly easier/faster to learn, thanks to the GUI being better than something like R Studio's.  However, R's learning curve is probably worth it if you have the time- the flexibility of R really means that it's a solid base for nearly any longterm statistical learning.  EDIT: in case you were asking about specifically Bayesian methods, there's much less ambiguity: you almost definitely want to learn R and eventually a more narrow modeling language like STAN, which will work nicely with R or Python, but probably not Stata."
statistics,3bu5yc,BadSoles,1 point,Mon Jul 6 14:24:22 2015 UTC,"Yeah, I know my field and R is in the solid minority, and for good reason. But that's another post.   R is open source and free, Stata costs money.   This doesn't say anything (directly) about the quality of either.   the open source nature of R   It sounds like you may not be aware of the open source nature of Stata, or indeed any other commercial software that still lets people write functions for others to use.    flexibility of R   I have not found R to be flexible. I've had package updates that break earlier code, and I've had errors happen for trivial, annoying reasons that are hard to find and poorly documented. ""Flexible"" is not what I would call R."
statistics,3bwguc,Aarjbar,2,Thu Jul 2 18:44:13 2015 UTC,"Pretty sure it's both, in a sense that you should report average differences of the means in the permutation groups along with their standard deviation. I'm not a statistician, but I 'm under the impression that you take a difference of means in both groups, then you permute multiple times, each time taking a difference of means. Then after you report the average and standard deviation of that. Then if your original data is significantly different than the average differences of means (is 2 or greater standard deviations), then it's a flag that it's significant."
statistics,3bwguc,umib0zu,1 point,Thu Jul 2 20:34:52 2015 UTC,"Yeah, that's how the standard permutation t-test is done. (Except that it's more common to simply look at where your original test statistic falls in the reference distribution instead of looking at the standard deviation since the reference distribution may not turn out to be symmetric.)  I'm wondering what the difference is between this and saving the standardized t-statistics from each permutation instead of simply the difference between the means. Then you would compare the original t-statistic to your reference distribution of t-statistics."
statistics,3bwguc,merkaba8,3,Thu Jul 2 20:41:56 2015 UTC,"Generally the whole point of doing the permutation test is to avoid the distributional assumptions of the t-test. So then why do the permutation and compute a t-statistic?  It makes more more sense to do what you originally said, which is to get a permuted distribution of differences (the null distribution for the difference in means, which under the t-test is assumed to have a particular distribution) and then get a p-value or confidence interval from that empirical null distribution."
statistics,3bwguc,s2s,2,Fri Jul 3 13:58:49 2015 UTC,"When you plot the densities of the two null distributions in the same plot and found them drastic different, if the two curves have similar shapes but different spread-out, it is because the two things (raw mean difference and standardized differences) are on difference scales. If the curves even have different shapes, say the mean difference one is symmetric while the other one is skewed, it could be that your data is not normalized with heavy tails."
statistics,3bwguc,StatNoodle,2,Sat Jul 4 07:32:55 2015 UTC,"Did not notice your question before...so belatedly:  If you compute the standard deviation (standard error of the difference) using a pooled variance and this is a true permutation test, is the standardized-dataset not just the differences-dataset divided by the same number for every permutation?  If so, they should not be discrepant, at all...just rescaled versions of the same empirical distribution.  On the other hand, if you are computing a non-pooled SD/SE then they will be discrepant.  In this case, you have run afoul of the Behrens-Fisher problem in a permutation setting.  The simple Permutation test you've described presumes the variances of the two groups are the same (same population) and that the pooled variance is appropriate.  If you used a t-statistic that uses SDpool*sqrt( nA-1 + nB-1 ) then they will be the same.  If you used a t-statistic that uses sqrt(sA2 /nA + sB2 /nB) then they will not be the same because this form does not assume sA = sB."
statistics,3bwguc,topoloss,2,Wed Jul 8 16:59:48 2015 UTC,"For a direct comparison with a two sample t-test, I would consider the null distribution of the t-statistics. The main advantage is that you can then directly compare the theoretical t distribution under the null with the `actual' null distribution of t-statistics.  This should not affect the results. Any statistic relating to mean differences should give very similar results. In my experience, unexpected results are often explained by mistakes, or occasionally outliers or extreme data values."
statistics,3btvob,JohnHartleyParker,2,Thu Jul 2 02:49:09 2015 UTC,"John Snow's cholera data set.  If you're using R, that guide will show you how to extract the data to a more useful format."
statistics,3btvob,CaffeineExperiment,1 point,Thu Jul 2 08:02:21 2015 UTC,The UCI Machine Learning Repository may be useful. It contains Iris amongst a great many others.
statistics,3btvob,thebigllamaman,1 point,Fri Jul 3 09:31:59 2015 UTC,"Just pick some of the usual macro variables from the FRED database. US GDP, CPI is studied endlessly."
statistics,3btvob,FappingNowAMA,1 point,Fri Jul 3 14:52:55 2015 UTC,Yellowstone eruptions is a pretty classic one and it's bundled with R.
statistics,3bsqfs,FoxyFoxMulder,6,Wed Jul 1 21:04:19 2015 UTC,Guideline #3: Just because it has a statistic in it doesn't make it statistics.
statistics,3bsqfs,normee,0,Thu Jul 2 03:42:20 2015 UTC,"This is majestic and also scary. Here are some other highlights from this infographic:    Each year, about 350 people die from entering a freeway going the wrong way! Around 50 people are killed each year by a vehicle on a private driveway. Speeding only saves the driver 1 minute for every 27 miles. 66% of all fatal accidents in the US are caused by aggressive driving habits such as speeding and reckless driving. *September is the worst month for road rage incidents. Perhaps it's the changing of the seasons?"
statistics,3bsqfs,Corruptionss,0,Wed Jul 1 21:07:58 2015 UTC,"For a second I thought that said 47 hours per week waiting in traffic. I was like, that's complete bull shit. But thinking back, I live in Southern California.  A commute to my work takes about 16 minutes with no traffic. With traffic it has a mean of about 40 minutes and on most days could range to about (30-50 minutes) (extreme days I have seen it take 70 minutes because of construction during rush hour)  That's about a 24 minute difference just going to work every day accounted by traffic.   On my way home, it's usually better because it isn't so much during rush hour. But I'd say about 10 minutes accounted by traffic. (about 26 minute drive home)  That's a half an hour every day, at least, because of traffic. 5 days a week, I'd say about 45 weeks of the year, we are looking about 225*30 = 112.5 hours. I guess Southern California is above the average on how long we spend in traffic.    I'd say a lot of accidents, at least where I live, are accounted by slow drivers too. It's not necessarily fast speed that kills, it's the difference between individual drivers and the average flow of traffic. Often we will have just temporary spaces of traffic because trucks go 45-55 when everyone else is going 65-75.  I've actually seen a pretty dense cloud of cars, with no trucks, maintain constant 65-70 mph no problem.  The problem is when there is a slow moving vehicle, in the minds of people, that lane may as well have closed down. People pile up behind trucks, try getting over, and because the rate of lanes are correlated with each other, this ripples out to the adjacent lane. Person moves left one lane at a rate of 50 mph and increases to 65 in 5-8 seconds and in that time often requires someone to slow down. Repeat this for everyone behind the truck and there becomes a cloud around the truck that has slower rate speeds regardless of what lane they are in.  Another source of increased traffic is platooning, when people pile up behind another person. There really is no point to be directly behind a person and often there is no room for someone to get in. If someone needs to make a lane change, most often they would slow down to do it which would cause small amounts of decrease in traffic.    The last thing people don't understand that intelligence succeeds speed. Most people have this binary way of driving (either press the gas or press the brakes) but don't realize that there are often many maneuvers, safe ones, such as timing lights, that resources and time can be saved. But from my years of driving, people don't normally incorporate that into driving.    Ok very last thing, there are often many other things that I feel significantly attribute to accidents. Large vehicles, even if I'm 5-10 seconds behind a car, a large vehicle can obstruct my view on all of the traffic above them.  The most important thing about safe driving is predictability. If a truck is blocking all my observations, how can I make good inferences based off the single observation of a truck. They could swerve out of the way for all I know and I'd be fucked.  Things like that."
statistics,3bsqfs,FullSharkAlligator,0,Thu Jul 2 00:37:49 2015 UTC,Cool
statistics,3bqsbj,DontBendYourVita,3,Wed Jul 1 11:56:56 2015 UTC,"What parts of the writing style do you dislike from ""The Elements of Statistical Learning"" (ESL) ?  When I first started self-studying for data science, many people mentioned ESL as a sound introduction to the field, but I found it too advanced for me.  I went back and read the undergrad version of that book ""An Introduction to Statistical Learning"", four times (along with some MOOCs and math review) before revisiting ESL months later.    The only caveat is that the style of writing is similar, but if you're having trouble with the content and not the style, then you may want to give the undergrad version a shot."
statistics,3bqsbj,some_data_guy,3,Wed Jul 1 13:10:25 2015 UTC,"It's not the content. It just feels unorganized and sort of rambly. It also doesn't feel like the authors give a good conceptual basis for each model, but instead moves from topic to topic and equation to equation. It feels more like a list of facts put into paragraph form than an actual text book.   It may just be me, but so far I just cannot get into it."
statistics,3bqsbj,giziti,1 point,Wed Jul 1 13:27:42 2015 UTC,"Yeah, it's more of a reference book in many ways, which is good when you want a reference book."
statistics,3bqsbj,shaggorama,1 point,Wed Jul 1 16:05:41 2015 UTC,You might like Kuhn's Applied Predictive Modeling: http://appliedpredictivemodeling.com
statistics,3bqsbj,nazga,1 point,Wed Jul 1 19:24:47 2015 UTC,"It's not a book, but johns hopkins gives very nice (and very free) coursera courses about statistics :  https://www.coursera.org/jhu  For example they lost me a few times with regression analysis when the get into details."
statistics,3bqsbj,FappingNowAMA,1 point,Wed Jul 1 21:04:31 2015 UTC,Kevin Murphy - Machine Learning text
statistics,3bqsbj,FappingNowAMA,1 point,Fri Jul 3 14:55:46 2015 UTC,"I'll look into this one, thanks"
statistics,3bqsbj,FappingNowAMA,1 point,Fri Jul 3 14:59:35 2015 UTC,"Also I wouldn't call the methods you mention ""multivariate models"". I guess you might mean they literally have multiple variables, but as a field ""multivariate stats"" is the proper name for a certain sub-field of stats and has existed for a long time. And as you can see from any multivariate stats text, those methods are not a part of that field. Those methods are part of machine learning. Sorry if this is too pedantic."
statistics,3br7qb,Enginerd,12,Wed Jul 1 14:27:13 2015 UTC,"To add to efrique, if X ~ Normal(0,1), then X2 ~ Chi-squared.  Also, a sum of  k squared normals is distributed chi-squared with k degrees of freedom.  Sometimes you will see a normally distributed test statistic converted into a chi-squared distributed test statistic by squaring it."
statistics,3br7qb,spiranas,7,Wed Jul 1 15:57:10 2015 UTC,"What's the context you're seeing them constructed in?  Is this in a situation where people are using MLE's for example? In that case, Wilks' theorem would be the culprit.  Or is this a situation in which contingency tables are being dealt with?"
statistics,3br7qb,efrique,1 point,Wed Jul 1 14:38:15 2015 UTC,It's because likelihood ratio statistics asymptotically follow chi-squared distributions.
statistics,3br7qb,beaverteeth92,2,Wed Jul 1 20:11:14 2015 UTC,"Actually, it's the Wilks theorem thing that efrique mentioned above. No sampling distribution necessary, you just taylor expand the score statistic around the truth (since we're frequentists for this proof that makes sense) and then use some linear algebra facts on the fisher matrix to get the result. If you're comfortable with some advanced calculus, probability theory and basic linear algebra, a good proof is here: http://www.math.umd.edu/~slud/s701.S14/WilksThm.pdf"
statistics,3br7qb,filetoffresh,1 point,Thu Jul 2 02:21:38 2015 UTC,"The sample variance of a normal distribution follows the chi squared distribution.  A lot of the time you want to see if a random sample is consistent with a hypothesis - for this you assume that deviations from the hypothesis are normally distributed, and then take the variance of these deviations.   This variance will therefore have a chi squared distribution."
statistics,3bqcf0,nazga,2,Wed Jul 1 08:09:20 2015 UTC,"Maybe try the kind of analysis they use to build phylogenic trees (classifying animals by their genes) ?  I don't know this area particularly well, to be fair, but the data structure seems somewhat similar, no ?"
statistics,3bqcf0,Hairy_Hareng,1 point,Wed Jul 1 11:35:44 2015 UTC,"I know a lot is going on right now with biological models applied to archaeological data, mostly in order to study patterns of cultural evolution.   An Approximate Bayesian Computation approach for inferring patterns of cultural evolutionary change Isolation-by-distance, homophily, and “core” vs. “package” cultural evolution models in Neolithic Europe   Those are the most recent/relevent examples I know. Still I don't think that my data is good enough right now to apply those models. Maybe when I will be done with exploratory analysis, and hopefully will have mastered some relevant subsets of data, I could look into this with the best descriptors.  Right now my main concern is how to deal with the data heterogeneity in order to produce meaningful and suitable statistical analysis."
statistics,3bqcf0,Hairy_Hareng,2,Wed Jul 1 20:54:11 2015 UTC,"Well, one way (that is not completely statistically rigorous, but seems sound to me) it to use your expert knowledge to fuse some of these categories together.  For example, I see that you have a bunch of different versions of Apollon in your data. Do these really need to be classified as different (for a first approximation) ? Are there other features can be considered as similar ?  Now, this doesn't seem as principled as doing stats: ""stats is math. I should do math and not trust myself"", but think of it this way: your data is very noisy, and so it gives a very low amount of information. Your expert knowledge probably just towers over how much clustering information is actually in your data. So I think it's completely okay to do what I suggest here"
statistics,3bqcf0,PhaethonPrime,1 point,Thu Jul 2 08:27:10 2015 UTC,"Indeed, I don't expect statistic tools to reveal the ""true meaning of things"". They are only tools that help you express and get the most of what you feed them.  Still I am a bit worried about the noisiness of the data. If we put aside the gods part, which was expended as much as possible but not intended to be used that way in the early stages of the analysis. The moment I start to prune the data for other categories, keeping in mind that will also drop/aggregate null or near-null columns, I fear I will lose most of the informations. My expert knowledge is struggling with the overall quality of my data.  I know there is no miracle and even the best statistic tools cannot go past crappy/noisy data. I am still trying to figure out what would be the best tool to get the most of what little I have, and at this point I am not conviced that phylogenic trees are the best fit."
statistics,3bqcf0,geigercounter120,2,Thu Jul 2 09:23:40 2015 UTC,"I'm wondering if some kind of non-negative matrix factorization (NMF) would work here (a binary version).  That would give you two things. One, it would create 'clusters' (you pick the number) of the descriptors. Second, it creates membership vectors for each site that relates each site to each of the clusters. In the terminology of NMF, you find latent vectors that map between items and attributes. The latent vectors are just weighted combinations of attributes.  You see NMF used for things like ranking sports teams, Netflix movie preference prediction, and lots of other things. There are good Python and R libraries for doing NMF as well. I've used Python's nimfa library before, and you can see an example usage (although without code, sorry) at a blog post I did. I used it to try to find out which Oscar categories win together, and how they cluster.  I think NMF can explicitly answer the ""can we identify specific subsets of descriptors which would distinguish the different archetypes as relevant subgroups of the dataset?"" question. Then again, other kinds of clustering probably can as well."
statistics,3bou2w,BailerIHardlyKnowHer,1 point,Tue Jun 30 22:59:41 2015 UTC,Ooooh this looks to be a lot of fun. We could use some multivariate Gaussian Processes to model through space and time. I haven't seen too many examples actually carried out but that process is what pops into mind.   There is a rough example http://jmlr.org/papers/v22/luttinen12/luttinen12.pdf
statistics,3bou2w,timshoaf,1 point,Wed Jul 1 07:50:10 2015 UTC,"heads up, the link isn't working."
statistics,3bou2w,BadSoles,1 point,Wed Jul 1 17:24:48 2015 UTC,"Thanks for the input, and yes the link doesn't seem to be working.  EDIT: found the working link http://jmlr.org/proceedings/papers/v22/luttinen12/luttinen12.pdf"
statistics,3bou2w,BadSoles,1 point,Fri Jul 3 22:15:57 2015 UTC,"So you probably want to be using the SpatioTemporal package.   If you're not familiar with these methods, the introductory paper I linked is a good starting point: explore the citations after taking a look through the paper itself.  Good Luck!"
statistics,3bopab,farsass,2,Tue Jun 30 22:22:36 2015 UTC,"Like with most questions surrounding ridiculously high-dimensional distributions, the answer is ""stochastically"". Sample a bazillion points from your original PDF, compute their likelihood, build the CDF of their likelihood, figure out the 0.05 quantile threshold and use that. The highest density regions by definition contain the points with the highest likelihood.  Reading over this again, I'm not entirely convinced that your first and second methods are actually that different. What arguments have been given that the second approach is more principled?"
statistics,3bopab,Fireflite,2,Tue Jun 30 23:31:21 2015 UTC,"Agreed - stochastic sampling is the way to go.  Also, if you have a distribution that is hard to sample from (eg Protein structures following a Boltzmann distribution) , then look into 'Monte Carlo' techniques such as Markov Chain Monte Carlo or Importance Sampling."
statistics,3bopab,aprstar,1 point,Tue Jun 30 23:48:40 2015 UTC,"Thanks, I agree that the monte carlo way should work, but maybe I should research more before wasting time on the bazillion samples. Hopefully someone has worked out a closed form solution for p* in the MVN and GMM setting.   The reason I think the second approach would be better is because i don't really have lots of spare held-out data to estimate this CDF and I want to avoid taking a beating from my dissertation advisor and commitee."
statistics,3bopab,mfredrickson,2,Wed Jul 1 00:04:55 2015 UTC,"While it might not be tight enough for you, Chebychev's inequality tells us that at most 1/k2 percent of any distribution's mass is more than k standard deviations from the mean. Therefore a suitable approximation to the 5% cutoff would be 4.5 standard  deviations on either any or all dimensions. The standard deviation estimates could come from your GMM fitting.  A slightly improved method would be to use Mahalanobis scaling to project your data. Any rescaled observation outside of the sphere with radius 4.5 would then be an outlier."
statistics,3bopab,Hairy_Hareng,1 point,Wed Jul 1 01:52:27 2015 UTC,"Chebychev's is not well suited to multi-modal distributions, unless you explicitly model it as a mixture"
statistics,3bopab,Hairy_Hareng,1 point,Wed Jul 1 09:55:44 2015 UTC,"ty, this is an argument for a sort of ""rule of thumb"""
statistics,3bopab,Hairy_Hareng,2,Wed Jul 1 13:27:05 2015 UTC,"You're already doing EM. So why don't you assume that you have another gaussian, with a fixed variance which is wide enough to cover the whole region, which you do not optimize. that way, any point that isn't captured well by a cluster will be assigned to this ""dumpster"" gaussian  As a more general advice, I don't think you should separate model training and outlier identification. If you want to treat some points as outliers, you should apply a model which has explicit generation of ""trash"" observations and invert that  Don't hesitate to ask more questions if that wasn't clear"
statistics,3bnhgq,jinnyjuice,5,Tue Jun 30 17:09:27 2015 UTC,"Trying using the program G*Power. You can download for free online. To get an estimate of the effect size, you need to delve into the literature, or try looking at the smallest effect size that would be interesting to you and in your field, and estimate the needed sample to find that effect size given your analysis of interest."
statistics,3blk38,beaverteeth92,7,Tue Jun 30 05:07:33 2015 UTC,"I saw Gelman at APS this year, and from this title I'm already bored. He seems to have a schtick, and that schtick requires lots of negative-themed drama. I think that leads him, in some ways, away from actually finding out what's going on. In other words, I think his career is now ""flamboyant hyperbolic critic of behavioral research data analysis"" instead of ""person who is actually trying to figure out how to make data analysis better."" The two have partial but not full overlap."
statistics,3blk38,bobbyfiend,5,Tue Jun 30 09:36:42 2015 UTC,"To his credit, his research record is very strong.  In this area, he has been positively contributing.  He is clearly a really smart guy.  I do wish he showed that side in his public writing more."
statistics,3blk38,Floydthechimp,2,Tue Jun 30 13:08:12 2015 UTC,"I agree. He is indeed quite smart, and has a good career. After watching some of the arguments on his blog (which he seemed to resolve using rhetorical techniques that are not on the ""approved rational arguments"" list), and seeing his somewhat theatrical presentation at APS, I have come to think his recent public career (as an entertainer?) is not following the same rigor he shows in his research career."
statistics,3blk38,bobbyfiend,5,Tue Jun 30 18:07:17 2015 UTC,"I agree that I would like to see a transition from ""here's what's wrong"" to ""here's what I suggest.""  In particular, I would like to know when, ideally, a new result should be published.  Gelman seems to be suggesting that you shouldn't publish until you reach some level of confidence that the result is correct.  But what level?  Rather than making the publication process more conservative, I am inclined to go the other way: make the publication process more lightweight, with shorter papers that present relatively small results.  Then use meta-analysis to aggregate results.  In a system like this it would be more obvious (I hope) that a single study with a counter-intuitive result should not be taken too seriously until it gets some support."
statistics,3blk38,AllenDowney,2,Tue Jun 30 13:25:55 2015 UTC,"I lost respect for him when he came out with a ""Blessing of Dimensionality"" presentation. Just a schtick jumping on buzzwords to be contrarian."
statistics,3blk38,FappingNowAMA,2,Wed Jul 1 01:58:11 2015 UTC,"I think he does both.  He still does a ton of methodology stuff and spends a lot of time trying to make STAN better.  With regard to his negativity, he's a political scientist with a Statistics PhD, so I don't blame him for being heavily concerned with how social science research is done and how much money is being wasted on poorly-designed studies.  I don't have an issue with that because he's in a unique and well-needed position to criticize."
statistics,3blk38,bobbyfiend,3,Tue Jun 30 13:07:26 2015 UTC,"Negativity is OK; I think we need the Prophets of Doom coming in from the wilderness and keeping people honest. My concern is that he occasionally transitions into trying to make his points in more theatrical and less rational ways when others seem to disagree, or perhaps when those points have become old hat."
statistics,3blk38,normee,1 point,Tue Jun 30 18:10:46 2015 UTC,"His current contributions in the statistics domain are more ""person who is actually trying to figure out how to make data analysis better"". In contrast to whatever you saw at APS, just searching the Joint Statistical Meetings online program for August, everything he is presenting or a co-author on (from supervising students or postdocs) is constructive: Stan: What Comes Next, Multilevel Regression and Post-Stratification for Survey Weighting, Imputation of Missing Data in the State Inpatient Databases, Evaluating Missing Data Methods for Health Disparities Study Using HCUP State Inpatient Databases, Small-Area Estimates as Covariates: A Measurement Error Approach  I think ""flamboyant hyperbolic critic of behavioral research data analysis"" is what he focuses on for blogging and for outreach to non-statisticians. It's easier to fire off blogpost-length thoughts taking down obviously flawed work than to write about more technical topics, it gets more eyeballs and stimulates more discussion (when he posts research-related content on his blog, there are usually not many comments), and I bet he gets better turnout giving talks with the more negative dramatic ""don't do this"" schtick."
statistics,3blk38,bobbyfiend,1 point,Tue Jun 30 18:52:11 2015 UTC,"That is very heartening. Perhaps I just saw a bit of his blog persona, then. If he's doing constructive stuff, then IMO that vastly outweighs anything that seems fluffy in the public sphere."
statistics,3blk38,Frogmarsh,1 point,Tue Jun 30 21:05:15 2015 UTC,"It seems Gelman assumes there have only been two fake bakers in 20 years. There have only been two who have been caught.  Regardless, the system in which science proceeds isn't meant to guard against fraud. Sure, it'll eventually overcome the consequences of it, but it can be a slow process. The system that does guard science against fraud is the legal system. Fraud is fraud wherever it is perpetuated."
statistics,3bks2z,filologo,3,Tue Jun 30 00:59:32 2015 UTC,"accessible is relative, but I recommend ""Doing Bayesian Data Analysis"" for a fresh, yet deep perspective:  http://www.amazon.com/gp/product/0124058884/ref=pd_lpo_sbs_dp_ss_1?pf_rd_p=1944687462&pf_rd_s=lpo-top-stripe-1&pf_rd_t=201&pf_rd_i=0123814855&pf_rd_m=ATVPDKIKX0DER&pf_rd_r=0828TQETZJXTJV19NKT2  Accessible? It even has puppies on the cover! Don't be fooled though you will need to put in some effort and do some computation."
statistics,3bks2z,klaxion,3,Tue Jun 30 01:45:06 2015 UTC,This is one of the few books that I've actually laughed out loud while reading. I honestly wish there were more statisticians who could write as well as Kruschke.
statistics,3bks2z,mrdevlar,3,Tue Jun 30 06:38:15 2015 UTC,"If you're looking for a more machine learning perspective, Introduction to Statistical Learning is fantastic."
statistics,3bks2z,Fireflite,3,Tue Jun 30 04:25:19 2015 UTC,"I'd recommend Andy Field's Discovering Statistics. Extremely colorful examples, but walks you through basic inferential statistics really well. It's won awards. Also teaches a stat package alongside the theory (can buy an SPSS or R edition).  Check out the reviews: http://www.amazon.com/Discovering-Statistics-Using-Andy-Field/dp/1446200469"
statistics,3bks2z,Brighteye,0,Tue Jun 30 04:35:43 2015 UTC,"It's especially great if you are interested in more than just regression. Not only that it covers ANOVA, MANOVA, etc. but also some very nice scaling techniques such as factor analysis."
statistics,3bks2z,Rylick,1 point,Tue Jun 30 19:41:39 2015 UTC,"I'm a big fan of Serious Stats. It presumes that you've taken some statistics, but that you'd really like to have things explained over again from scratch. It also has the benefit of being quite comprehensive, and compartmentalized enough that you can use it as a reference."
statistics,3bks2z,belarius,1 point,Tue Jun 30 01:54:42 2015 UTC,"If you're looking for a good basic-level, intro book, ""Elementary Statistics, Picturing the World"" by Larson and Farber is good.   Also, check out, ""Introductory to Probability and Statistics"" by Mendenhall.  These might be too basic for you, but I think they are some of the clearest introductory texts."
statistics,3bks2z,norsurfit,1 point,Tue Jun 30 04:37:19 2015 UTC,"Statistics by David Freedman, Robert Pisani and Roger Purves.  This is an excellent beginning book that deals with real issues.  It is the best beginning book that I have seen.  If you need something more advanced, I would recommend something else."
statistics,3bks2z,StatNoodle,2,Tue Jun 30 17:15:05 2015 UTC,"Naked Statistics to first get the concepts, then whatever other people's recommended."
statistics,3bj6at,DrLionelRaymond,7,Mon Jun 29 17:38:38 2015 UTC,"One way to measure your purchasing power different places is to define a basket of goods. You say you don't know what kinds of food you will buy but in reality you probably have some tastes and preferences. Compare prices of the stuff you care about. It doesn't matter if one store is cheaper on average if all of the things you actually end up buying are more expensive than the alternative.  Just obtaining the datasets of items/prices for multiple stores would be difficult, and they change on a weekly basis as well. I have no idea how you would come up with the probabilities for what you'd buy. The store could use data mining techniques to guess what you're likely to buy based on your purchasing history, but only because they have a much larger sample to draw conclusions from."
statistics,3bj6at,Somniferus,0,Mon Jun 29 17:52:54 2015 UTC,"agreed!  you could further sub-divide your basket so that there are the items you know you'll buy every week, every two weeks, and once a month. independent of grocer you could then factor an ""incidentals"" into your budget.  for a fairly straightforward approach i would compare the price of each item in your basket for each store over the course of a month (obviously a full year would be best, if you're feeling ambitious) and then run a stats analysis (i'm thinking t-test) to see if there's a statistically-significant difference.  you wouldn't have to purchase the items from both stores, just check the prices. however you may want to consider adding in an access factor - for example, if both stores are equivalent in distance from the apartment, is one more difficult to access? (construction, hours, several left-hand turns against traffic, etc).  this is also assuming that both stores have the same brands - you could choose to control for brands (only purchasing those that appear in both store) or you could say that the study is brand-independent, and purchase whichever one is the lowest in price."
statistics,3bj6at,kierisi,3,Mon Jun 29 19:24:56 2015 UTC,"This isn't really a statistics problem, more of an optimization problem.  They're related but this is exactly the kind of problem that they teach in optimization courses.  minimize a1x1 + a2x2 + a3x3 + ... subject to x1 + x2 + x3 + ... >= food_requirement            x1, x2, x3, ... >= 0   and so on.    Specifically you're looking for stochastic optimization where the costs follow a probability distribution.  The final model would be a lot more complicated than what I've shown here depending on how much you want to take into account, but this is the basic idea."
statistics,3bj6at,noelsusman,1 point,Mon Jun 29 22:44:36 2015 UTC,thank you! this is exactly the answer I was looking for!
statistics,3bj6at,noelsusman,1 point,Tue Jun 30 02:22:28 2015 UTC,Is there any chance you could point me in the direction of a further explaination? 99% of what I'm running into is the news vendor problem and I'm having a hard time connecting it to my problem above. Thanks!
statistics,3bj6at,Dannysmartful,1 point,Wed Jul 1 02:08:24 2015 UTC,"Let's say we want to decide how much milk, bread, and eggs to buy at each store.  First, we define our variables.  Let xij be the amount of item i bought at store j Let cij be the price of item i at store j Let pij be 1 if item i is bought at store j, 0 otherwise Let ri be the amount of item i required Let M be some large constant   Now we can define our problem.  min p11c11x11 + p21c21x21 + p31c31x31 + p21c21x21 + p22c22x22 + p32c32x32 s.t. x11 + x12 >= r1      x21 + x22 >= r2      x31 + x32 >= r3      x11 <= Mp11      x21 <= Mp21      x31 <= Mp31      x12 <= Mp12      x22 <= Mp22      x32 <= Mp32      x11, x21, x31, x12, x22, x32 int   Ignoring pij, this is a straightforward deterministic optimization problem that can be solved in Excel.  We can treat pij as a random variable that follows a Bernoulli distribution with some probability of success (like you mentioned in your first post).    The world of solving stochastic optimization problems is a rabbit hole you probably don't want to go down (see here).  A naive way to do it is to obtain realizations of the random variable, solve the resulting deterministic optimization problem, and repeat.  So, you roll the dice for each pij, plug the result into the problem above, solve it, and record the results.  Then you can do something like take the most common solution as your final answer.  That's not a very good way to do it, but it's (relatively) easy to implement.  Simple-Average Approximation is a method with similar ideas that is a lot more theoretically sound."
statistics,3bj6at,PhaethonPrime,2,Wed Jul 1 03:20:57 2015 UTC,"To me this sounds like you are looking for a transportation model.  Before making one, create a short grocery list and go to store A in week 1 and B in week 2.  Compare the receipts side-by-side and see the difference.  Obviously if one store is significantly cheaper then that solves the problem regarding where to shop.  If you find there are variable prices from both, e.g. 1/2 the listed items are cheaper at one store, vise verse.  Maybe generate a crass transportation model in Excel using each grocery item.  You could use a if/then regression in Excel (if you download Statools from online )but I don't think you need to go that far.  By doing this in Excel, and possibly carrying it on your phone, then if you run out of certain things you can go to your Excel file put in a 1 or 0 for the items you have listed 1=need, 0=don't need and hopefully it will tell you based on your needs which store to visit minimize your grocery cost. So long as you stick to your grocery list.    Over time you could continue to add different items to your list such as dish soap, laundry detergent, etc. and each time you make these purchases make sure you update the prices in your file and continue doing this as long as possible to collect more data.  After a significant amount of time, maybe a year or more you could do a regression table to see which store has the greatest price increases and you could predict if a store will be out of your price range or not be worth the trip anymore.    That actually sounds like a fun project to do with some free time.  Hmm.  I wonder if there is an app out there that will do this for you?  Perhaps calculate the distance you are from the store and determine if its worth going out of your way to save a few dollars?"
