MachineLearning,3e2cjq,dukwon,4,Tue Jul 21 12:34:29 2015 UTC,"Okay, I hate to be a wet blanket, but these machine learning contests are starting to bug me. Shouldn't CERN pay a grad student or postdoc to do this?"
MachineLearning,3e2cjq,Doc_Nag_Idea_Man,8,Tue Jul 21 17:32:10 2015 UTC,Having worked at CERN and having been one these students I can assure you there are students working on this kind of thing all the time. The difference is that the prize money might be enough to cover a few grad students wages for the summer from which they might see a few solutions at best. These competitions can generate far more ideas and solutions for their money.
MachineLearning,3e2cjq,IBuildBusinesses,1 point,Tue Jul 21 18:42:13 2015 UTC,"These competitions can generate far more ideas and solutions for their money.   Oh, I totally understand that that's the point. I just feel like they're getting away with paying far less than the market value for these skills.  Of course it is in my own interest for these skills to be remain valued, so I realize I'm not impartial here. I had a lot of friends doing high energy stuff at Fermilab back when I was a grad student, and I remember how surprised I was to learn that some of them were basically just doing ML stuff."
MachineLearning,3e2cjq,Doc_Nag_Idea_Man,7,Tue Jul 21 18:57:25 2015 UTC,"Contests like this allow lots of people (99% of whom are nowhere near Geneva/France) to get access to data and experience that they otherwise wouldn't get.  If I was someone who was entering this contest, I don't think it's the money that would be enticing to me. It would be the chance to rub shoulders with the folks at CERN and the chance to get my name out there if I won.  I think empowering more people is better than paying a few months of an already established grad student's stipend. But that's just me."
MachineLearning,3e2cjq,Molag_Balls,1 point,Tue Jul 21 19:26:17 2015 UTC,"Contests like this allow lots of people (99% of whom are nowhere near Geneva/France) to get access to data and experience that they otherwise wouldn't get.   That's why I'm a big advocate of open science. Collect data, publish it, and put it all online so that everyone can see/use it."
MachineLearning,3e2cjq,Doc_Nag_Idea_Man,0,Tue Jul 21 20:18:23 2015 UTC,"I don't really get this mindset. The point of doing science is to further knowledge and understanding - not to pay grad students. That's putting the cart before the horse.  If crowd sourcing the problem finds more and better solutions at a lower price, that's a win for science."
MachineLearning,3e2cjq,nkorslund,0,Tue Jul 21 19:23:05 2015 UTC,"fuck that shit. physics grad students already get boned when they graduate and there are no academic jobs, at least let them have this.  i really hate these academic competitions - with companies, at least if they want to profit off whatever solution, they'll need to pay a developer or engineer to build it into a product.   But with academics, the organizers just write a paper about the competition and grab all the ""profits"" for themselves."
MachineLearning,3e2cjq,sdsfs23fs,0,Tue Jul 21 19:26:38 2015 UTC,I don't really get this mindset. The point of doing science is to further knowledge and understanding - not to pay grad students.    Yeah screw them . let's not even pay them at all. It's about discovering the natural universe /s
MachineLearning,3e2cjq,maxToTheJ,3,Tue Jul 21 19:53:30 2015 UTC,"It's about discovering the natural universe   Well of course it is. What did you think science was about? Scientists are paid because we want them to do science - science doesn't exist because we want to pay scientists. If/when fields of science start to get automated (as is happening and will keep happening in many other places in society right now), then the former practitioners of those fields will be sadly out of a job. That's just reality, regardless of whether you personally approve of it or not."
MachineLearning,3e2cjq,nkorslund,-1,Tue Jul 21 20:02:05 2015 UTC,It was sarcasm hence the /s at the end
MachineLearning,3e2cjq,maxToTheJ,4,Tue Jul 21 20:12:24 2015 UTC,I think that was the problem.
MachineLearning,3e2cjq,fimari,-1,Tue Jul 21 21:35:22 2015 UTC,Who's collecting the data and organizing the contest? A bunch of people who were grad students 10-20 years ago.   I look at moves like this as robbing the near-future for the benefit of the present.
MachineLearning,3e2cjq,Doc_Nag_Idea_Man,0,Tue Jul 21 20:16:39 2015 UTC,"i share the source of your general sentiment, but I do not think that this is a sustainable way of having scientific advancement. researchers and students need to be paid and to know they will be paid, or a decent fraction of them will just do something else, scientific advancement will loose in the long run."
MachineLearning,3e2cjq,Make3,1 point,Tue Jul 21 21:58:57 2015 UTC,"There was a great episode of Niel Degrasse Tyson's show Star Talk about this, with an interview featuring x Prize founder Peter Diamandis.  The episode talks about how a prize can publicize a topic.  even if any given attempt is more costly than the monetary prize, garnering that publicity can still be profitable.  It turns out to be a really effective way to divert money to research.  I believe the historic example they gave was Charles Lindbergh, who claimed a prize for being the first to fly solo across the atlantic.  The ansari x prize and other x prizes would of course be more recent examples."
MachineLearning,3e31m8,improbabble,3,Tue Jul 21 16:01:46 2015 UTC,Code: https://bitbucket.org/twgr/ccf/src/f7831911fe128bbbb6c30ea00fdfd74164c5ed7e/genCCF.m?at=master
MachineLearning,3e3iwe,regularized,9,Tue Jul 21 18:00:08 2015 UTC,IMO no.  Deep learning is super hot right now and there are tons of topics to explore.
MachineLearning,3e3iwe,njchessboy,14,Tue Jul 21 18:16:13 2015 UTC,"That's ridiculous.  Work on the k-means algorithm has appeared at the last two ICMLs [1][2], and there was a paper on the perceptron at NIPS 2010.  If those topics can still be published on, certainly there's more left in Deep Learning.  DL has mostly only been applied to dense, raw signals (images and speech); there are many, many more applications left to tackle.  Similarly, there is little existing work on DL theory.  I'd say the only 'saturated' part of DL is static image classification, but still there are many PhDs left to be done in the area."
MachineLearning,3e3iwe,goblin_got_game,3,Tue Jul 21 18:24:14 2015 UTC,"I agree, especially with the theory part. Right now it works a little like a black box and nobody knows exactly how"
MachineLearning,3e3iwe,unertlstr,9,Tue Jul 21 18:45:07 2015 UTC,"Most likely, a dissertation started now would not be about ""deep learning"", but about ""an X-based method for deep learning of Y"", with the description length of X and Y growing with time.  And, yes, that means that your phd project would be narrow and quite deep. I.e., if the person who would have to supervise you hasn't followed everything in detail and knows good values of X and Y, it may be preferable to jump onto some other bandwagon."
MachineLearning,3e3iwe,wally_fish,5,Tue Jul 21 18:17:03 2015 UTC,"LSTM RNNs, advancements in convnets, further exploration of deep Q-networks and probably many other topics, currently all fall under the banner of ""deep learning"". And more will be added in the years to come.  It's definitely not too late to jump on this bandwagon, if you select your subtopic wisely."
MachineLearning,3e3iwe,nkorslund,3,Tue Jul 21 19:26:30 2015 UTC,"Right now I see a lot of focus on using RNNs with other DL architectures as well as reinforcement learning focusing on a variety of applications. Personally, at this time I'd like to pursue one based on improving RNNs (architecture, learning algo, etc)."
MachineLearning,3e3iwe,chhakhapai,3,Tue Jul 21 18:33:52 2015 UTC,"I agree, it feels like RNNs/recurrence now is where convolutions were in 2013."
MachineLearning,3e3iwe,siblbombs,2,Tue Jul 21 18:53:45 2015 UTC,"Pessimists will be pessimists. We are learning more (about all of ML, not just deep learning) every day. There is plenty of space to do interesting work in deep learning if you are willing to formulate a problem that isn't just ""applying a conv-net/MLP/autoencoder"" to X domain data, and has some interesting conditional modeling components to it which exploit domain knowledge.  If you are an expert in some other niche field dragging in neural networks to solve some function mapping using lots of data, which has been previously solved with handcrafted features can be quite academically profitable, but is a bit ""flash in the pan"" from the ML perspective."
MachineLearning,3e3iwe,kkastner,2,Tue Jul 21 19:08:37 2015 UTC,"Doing a PhD (I'm a 5th year, although not in CS) is a life commitment. Machine learning in industry is moving at a breakneck speed compared to the life and work of a PhD. If you are interested in machine learning in a serious way and want it to be a part of your career, then by all means go for it, but if you're looking to ""jump on a moving train"" then a PhD is incongruous. The world may very well look quite different in 5 years when you're graduating than it does now!"
MachineLearning,3e3iwe,thebiglebowski2,2,Tue Jul 21 19:55:51 2015 UTC,"Quite a few good answers. I'd like to add one more point. PhD is not a race. While the program has a limited time period, your interest in pursuing and persisting with it shouldn't. Choose a problem that you find interesting. Deep nets may or may not be necessary to solve them. Here's a useful question to ask:  Would you work on the problem even if no one paid you or asked you to work on it?   If yes, go do a PhD."
MachineLearning,3e3iwe,sriramcompsci,3,Tue Jul 21 21:32:49 2015 UTC,Already saturated? Isn't ML/DL just now really starting to take off? 3 years ago I wasn't even aware of all of these concepts.   I wish I had the mathematical abilities to delve even deeper into this world. I think you should go for it if it is something you are passionate about.
MachineLearning,3e3iwe,Goldsound,4,Tue Jul 21 18:19:21 2015 UTC,Why are you doing a PhD? If it is to further understanding in an area then choose an area of interest to you!
MachineLearning,3e3iwe,fuzzyballzy,1 point,Tue Jul 21 18:51:12 2015 UTC,"Remember deep learning is just a technique, I would focus more broadly. A lot of PhD's now fall into a few categories but they are broad such as nonlinear optimization theory, linear optimization, probabilistic learning, network science, etc . You can start with deep learning but remember for you to lead it and not the other way around."
MachineLearning,3e3iwe,HamSession,2,Tue Jul 21 18:58:32 2015 UTC,You should not do a phd in deep learning.  You should do a phd in machine learning and apply neural nets to your problems.  Deep learning as deep learning is rapidly becoming commoditized.
MachineLearning,3e3iwe,kjearns,1 point,Tue Jul 21 19:22:04 2015 UTC,"It's been getting a lot attention lately so there's probably high demand for PhD positions in strong programs. But you mean if saturated in the sense that there are few interesting ideas left to be explored, then I'd say that couldn't be farther from the truth. There are so many potential applications and advancements of neural network algorithms remaining to be explored that top-flight researchers in the area are incapable of even agreeing about which directions are most interesting."
MachineLearning,3e3iwe,geothenes,1 point,Tue Jul 21 20:16:11 2015 UTC,"I don't think so.   What is certainly the case is that most of the ""low hanging fruit"" of convnets have been picked.  There is still a lot to do. It just might be more difficult than if you had started your Phd four years ago."
MachineLearning,3e3iwe,sieisteinmodel,1 point,Tue Jul 21 20:36:15 2015 UTC,"I was about to do a PhD on deep learning, in all honesty, i really think this area is the future of artificial inteligence. And i do think there is a lot to understand and discover yet, new and better algorithms must emerge, i think it's only in the beggining"
MachineLearning,3e3iwe,Aerospacio,1 point,Tue Jul 21 21:06:23 2015 UTC,"Don't think so. Deep Learning is still an infant field, with less than a decade of development (if you start counting by Hinton's paper), and machine learning in general is only decades old. We need far more research to understand all its potential.  Look at it this way: there are still many researchers in classical statistics, and that's been around for centuries. We certainly can do much more in deep learning."
MachineLearning,3e3iwe,SMFet,1 point,Tue Jul 21 21:47:21 2015 UTC,"Research one/some of these topics for your phd: ""Many challenges remain, however, in aspects like large-scale (hyper-) parameter optimization, modeling of temporal data with long-term dependencies, generative modeling, efficient Bayesian inference for deep learning, multi-modal data and models, and learning representations for reinforcement learning."" - from ICML 2015 https://sites.google.com/site/deeplearning2015/"
MachineLearning,3e24mm,aayyyy,5,Tue Jul 21 11:04:06 2015 UTC,"The more I read about ""topological"" approaches to data analysis, the more confident I am that it's all just a rebranding of existing techniques (they even mention isomap in the article). What a handful of people call ""topological"", everyone else just calls ""embedding"" or ""nearest neighbor graphs""."
MachineLearning,3e24mm,shaggorama,8,Tue Jul 21 13:06:53 2015 UTC,"TDA (Topological Data Analysis) is not dimensionality reduction. Unlike t-SNE you do not reduce down to, say, 2 dimensions, then plot points as X,Y coordinates. With TDA what is created is called a simplicial complex. This is a networked structure with nodes and edges, the nodes representing clusters, and the edges for shared cluster members. TDA can use any statistical or machine learning algorithm (including t-SNE) to create the clusters. It can use any function (including t-SNE) to color the clusters. It can use any base shape (hypercubes, hyperhexagons) to subdivide the data.  The introduction paper to this Mapper also mentions ISOMap for an illustrative example/previous work. It also mentions its differences with the Mapper method (for instance its inability to produce simplicial complexes and its output being a subset of Euclidean space). [Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition .pdf](www.ayasdi.com/_downloads/Topological_Methods_for_the_Analysis_of_High_Dimensional_Data_Sets_and_3D_Object_Recognition.pdf)  I especially think the compressed graph representation of point cloud data is very exciting. This may aid 3-D convolutional neural nets. If we want robots which can untie knots, then we need to apply the 300 year old maths used in topology and knot theory."
MachineLearning,3e24mm,vodkagoodmeatrotten,8,Tue Jul 21 14:44:10 2015 UTC,"There are some legitimately new techniques though.  Things like clustering based on how many iterations it takes to eliminate a hole in the structure of your data.  If it takes a long time, the hole is a major structural component and should be kept."
MachineLearning,3e24mm,beaverteeth92,3,Tue Jul 21 14:29:02 2015 UTC,"Never heard of this, could you link some lit?"
MachineLearning,3e24mm,shaggorama,4,Tue Jul 21 14:29:38 2015 UTC,"Not sure about papers, but I saw a talk by Larry Wasserman on the topic."
MachineLearning,3e24mm,beaverteeth92,2,Tue Jul 21 16:37:58 2015 UTC,I don't get how it's different from kernel methods/manifold learning/self-expressive decompositions/non-parametrics...
MachineLearning,3e24mm,MeowMeowFuckingMeow,1 point,Tue Jul 21 15:09:49 2015 UTC,"Nonparametrics usually fit an epsilon ball around your data and go from there.  Like fit a curve to a small portion of your data, go to nearby points, and fit another curve to another small portion of your data.  Fair warning that what I'm about to type may be wrong or misinterpreted because it's all from a Larry Wasserman talk I saw a while back.  Topological statistics uses the framework of topology, rather than analysis, to do things like clustering.  You classify the structure of your data using the language of algebraic topology, like Betti numbers.  So if you have two-dimensional data in one connected part with a hole in the middle (a flat donut), it would have b0 = 1 (because one connected component), and b1 = 1 (because it has one hole in it).  This is really nice for unsupervised learning.  Let's say your two-dimensional data has b0=2 and b1=2, meaning two separate connected components and two holes (imagine a donut with a smaller donut in the first donut's hole).  You have 1000 data points.  Start by dividing them into a cluster of 500 blue points and 500 red points while making note of the connectivity and whatever holes are formed.  Make 10 red points blue and 10 blue points red.  Check if the hole is still there.  Do it again.  What happens to the holes?  If one hole closes quickly (i.e. in few iterations), we say it has a shorter ""lifespan"" and isn't that structurally integral to the data.  But if in the process, the other hole takes many iterations to close or doesn't close at all, it's probably important to the structure of the data and should be included in the final clustering.  Real data might have like 20 holes of varying sizes, but there could be one or two giant holes with a few smaller ones in the surrounding ""mesh"".  Algebraic topology is nice for clustering these types of data because rather than guessing the right distance metric, it uses the underlying structural properties of your data and sees what it has to do to get rid of them.  I wish I could draw this or it would be far more clear.  Here's what seems to be a good paper on Betti numbers and computational topology, but I don't have the time to go through it now."
MachineLearning,3dzgb4,ojaved,6,Mon Jul 20 19:56:57 2015 UTC,How does this compare to the stanford course by Andrew Ng on Coursera?
MachineLearning,3dzgb4,iamaquantumcomputer,14,Tue Jul 21 03:05:40 2015 UTC,"The machine learning class by Andrew Ng on Coursera is a watered down version of machine learning with minimal amounts of mathematics. This CMU course covers everything in the Andrew Ng course, but includes mathematical proofs, detailed references, and goes 3x deeper in about every topic."
MachineLearning,3dzgb4,GoldmanBallSachs_,14,Tue Jul 21 05:07:28 2015 UTC,"How does this course compared to Andrew Ng's actual Stanford class which has the videos posted on youtube and the notes, homeworks, and exam posted online?"
MachineLearning,3dzgb4,dashster18,1 point,Tue Jul 21 06:04:10 2015 UTC,"Not that guy, but I currently have his legit lecture notes on my desk and they're pretty nice. His explanations are very much the compliment of a real ML textbook. I mean to say that where normal textbooks lack in intuition and mathematical clarification, he provides it. He also provides review notes on optimization, probability, linear algebra, and HMMs.   I liked his notes so much that I printed it at Stapes for $20 with a nice vinyl binding. It turns out that the lecture notes are half the book - the other half are the prereqs (actually it's probably a 60:40 ratio). There's a ton of material, some dense matrix notation that you would see in a real machine learning book, etc.  I do have some qualms though with his notes. Sometimes he just does some hand-waving and omits a proof for a non-intuitive matrix equation. When he does that, I'm either lost for the next page or I'm spending time on the internet trying to figure it out.   Just to be clear, I'm only speaking of his lecture notes. I hate video lectures in general."
MachineLearning,3dzgb4,Nixonite,1 point,Tue Jul 21 17:13:23 2015 UTC,What book in particular are you talking about?
MachineLearning,3dzgb4,MusicIsLife1995,1 point,Tue Jul 21 21:13:24 2015 UTC,I compiled his lecture notes pdfs and review notes into a 255 page pdf file.
MachineLearning,3dzgb4,Nixonite,2,Tue Jul 21 22:29:51 2015 UTC,It's aight. Video production quality isn't great but good content.
MachineLearning,3dzgb4,classicalhumanbeing,2,Mon Jul 20 21:34:56 2015 UTC,slides http://alex.smola.org/teaching/10-701-2015/website/
MachineLearning,3dzgb4,thegoz,1 point,Tue Jul 21 01:13:21 2015 UTC,"Wow, does it cover Deep Learning?"
MachineLearning,3dzgb4,j_lyf,3,Tue Jul 21 01:33:27 2015 UTC,"Yes, Please take a look at the last two lectures (9.1 and 9.2)"
MachineLearning,3e258f,name_on_water,0,Tue Jul 21 11:12:32 2015 UTC,"I had a quick look at both papers. They seem to be different enough to warrant separate papers:   Mou's work is a convolution over constituency trees Ma's is a convolution over dependency trees   I personally think Mou's work is more innovative, and the results more impressive, but certainly no grounds for plagiarism.   As a deep learning/ML researcher, you should expect significant number of overlapping ideas, given the number of researchers working on it. At every conference I see at least a couple of papers where the authors have done what I was planning to do.  EDIT: I also think Mou should have corresponded privately with the authors instead of (or before) making this public."
MachineLearning,3e258f,20150721,2,Tue Jul 21 14:01:16 2015 UTC,"the similarities in naming and evaluation tasks are pretty damning though.   I would find it very hard to believe Ma's group didn't read Mou's papers, even without the included email:   Dear Lili,  The ACL short paper does not appear to plagiarize your work.   In the submission, the authors cite your work and describe differences.  With best regards,  Michael Strube   There is no excuse for removing Mou from their lit review/references after peer review. That qualifies as plagiarism for me."
MachineLearning,3e258f,j1395010,1 point,Tue Jul 21 14:15:29 2015 UTC,"Agreed on that Ma should have cited Mou. But I don't agree that qualifies as plagiarism.   This seems to be a case of ""not citing existing state of the art results"", which is definitely bad practice, but not grounds for plagiarism."
MachineLearning,3e258f,20150721,4,Tue Jul 21 14:31:46 2015 UTC,"If you come up with your ideas in a vacuum, and don't cite relevant literature, that might just be bad research/writing, not plagiarism.  But when you write a paper with knowledge of other papers, that you clearly know are relevant (evidenced by citing in the submission for peer review), and don't mention those papers, it becomes academic dishonesty."
MachineLearning,3e4iau,cypherx,3,Tue Jul 21 22:04:55 2015 UTC,"There isn't a special name for this, but people definitely do it.  Torch even has a module for it."
MachineLearning,3e4iau,kjearns,1 point,Tue Jul 21 22:15:10 2015 UTC,Do you know what it's called in Torch?
MachineLearning,3e4iau,kjearns,2,Tue Jul 21 22:18:11 2015 UTC,CMulTable followed by a Sum will do it.
MachineLearning,3e4iau,dwf,1 point,Tue Jul 21 22:21:25 2015 UTC,"If both inputs are piped through the same nonlinear transformation, you can see it as a particular kind of siamese network."
MachineLearning,3e1x0z,evc123,2,Tue Jul 21 09:20:09 2015 UTC,"While it's an interesting paper, the statement ""Deepmind can play Pacman now"" does not apply here, as far as I could tell. What they showed with their Ms Pacman example was that they could transfer learning to an agent with a different goal set (specifically to eat a pellet that was not considered by previous agents)."
MachineLearning,3e2zxd,FR_STARMER,3,Tue Jul 21 15:49:30 2015 UTC,"It depends, but I would say it's fair to say most machine learning is done using R or Python. If you are interested in python have a look at the library scikit-learn.  Also have a look kaggle.com. Kaggle hosts competitions for money for other companies, but often people share scripts and there are some tutorials. Once you know the basics, I think it's great place to learn to master machine learning."
MachineLearning,3e2zxd,ma2rten,1 point,Tue Jul 21 16:20:46 2015 UTC,"I think the Andrew Ng coursera course, which I'm assuming you're referring to, is one of the best places to start. Not sure exactly what ""going through.."" entails for you, but make sure to actually take it seriously and do the programming homework assignments. It's one thing to watch the video lectures and think you understand it, and another to actually implement the concepts."
MachineLearning,3e2zxd,JiKuai,1 point,Tue Jul 21 18:39:11 2015 UTC,"I'm having some difficulty implementing the cost functions on the first homework, mainly because there was little demonstration of MATLAB syntax, but I am making sure I understand what is going on. Most of it is intuitive, albeit, higher level stuff."
MachineLearning,3e1lgz,ornstein-uhlenbeck,3,Tue Jul 21 06:45:04 2015 UTC,Thank you thank you thank you (to the OP). Those dang did were getting super annoying super fast.
MachineLearning,3e1lgz,utunga,1 point,Tue Jul 21 14:24:27 2015 UTC,http://www.twitch.tv/317070 is still the best implementation
MachineLearning,3e2mxf,samim23,-1,Tue Jul 21 14:10:09 2015 UTC,"Wow, Medium is really churning out a lot of lackluster ML articles as of late."
MachineLearning,3e2mxf,siblbombs,2,Tue Jul 21 14:21:15 2015 UTC,"As noted, its a summer break limited time post, attempting to bring bit of comedy to the topic. The issue of patents & ML is highly important though, wouldn't you agree?"
MachineLearning,3e2mxf,jkyle1234,1 point,Tue Jul 21 15:35:55 2015 UTC,"I agree, kind of scary if patents like ""classification"" get patented by a large company like google. In effect they could sue every new up and coming ml startup and monopolize the business of machine learning for themselves."
MachineLearning,3e2mxf,dwf,0,Tue Jul 21 16:45:31 2015 UTC,"As a anonymous poster on Redit says:   Well, um, at least you're citing your sources."
MachineLearning,3e2mxf,dwf,1 point,Tue Jul 21 18:22:02 2015 UTC,"hope you understand, that some folks want to be anonymous. I linked to the original reddit post. Thanks for pointing it out anyways."
MachineLearning,3e2mxf,dwf,-1,Tue Jul 21 18:40:11 2015 UTC,"I was mostly stating that ""some guy on the Internet said this"" is not exactly hard-hitting evidence to support your argument."
MachineLearning,3e2mxf,jkyle1234,1 point,Tue Jul 21 19:06:57 2015 UTC,"I find it distracting, having a debate about sources, when the officially announced patents include heavy hitters like classification. Why aren´t more people upset about this?"
MachineLearning,3e02zy,jessiclr,3,Mon Jul 20 22:39:30 2015 UTC,What's a macrobatch?
MachineLearning,3e02zy,feedthecreed,3,Mon Jul 20 23:33:42 2015 UTC,A macrobatch is a set of 3072 images (historically deriving from the flat file format used to store pre-processed images by Alex Krizhevsky in cuda-convnet)
MachineLearning,3e02zy,meepmeepmoopmoop,1 point,Mon Jul 20 23:37:55 2015 UTC,"It might be good to say how long it takes to train on a well known dataset like ImageNet.    Also, I'm super excited about the prospect of a cloud service with multi-GPU machines!"
MachineLearning,3e02zy,alexmlamb,1 point,Tue Jul 21 08:19:24 2015 UTC,"They are one of the few who include this information directly in the readme:  ""3s/macrobatch on AlexNet on Titan X (Full run on 1 GPU ~ 32 hrs)"""
MachineLearning,3e02zy,duschendestroyer,1 point,Tue Jul 21 09:04:12 2015 UTC,Stay tuned :)
MachineLearning,3e02zy,coffeephoenix,1 point,Tue Jul 21 19:06:10 2015 UTC,:/   Am I the only one who actually enjoyed the difficulty of parallelism for DL?  It was nice thinking that an individual could afford a system similar to what funded researchers were using.
MachineLearning,3e02zy,j1395010,1 point,Tue Jul 21 12:28:58 2015 UTC,"You still can: a Quad-TitanX DIY Digits Dev Box with a half TByte SSD is about ~$6,200...  Substitute Core i7-5930K for Core i7-5960K because you really only need 1 CPU core/GPU.  Of course, you have to be willing to build it yourself, but WTF happened to computer science when people with high-fallutin' degrees in it aren't willing to do this?  NVIDIA's stock price went up, that's what!"
MachineLearning,3e02zy,XeonPhitanium,1 point,Tue Jul 21 18:58:22 2015 UTC,"ok, but that $6200 only gets you half of their 8 GPUs. And the comparison is to a single ~$500-1000 GPU, which is what the majority of researcher have been using for the past few years..."
MachineLearning,3dyw9s,efavdb,6,Mon Jul 20 17:35:00 2015 UTC,I'm working on this exact Kaggle data set! Have you tried any other algorithm packages yet like neural net or SVM?
MachineLearning,3dyw9s,theonlyonedancing,1 point,Mon Jul 20 20:40:24 2015 UTC,I also played around with some other models.  SVM with a Gaussian kernel gets pretty slow with the large number of data points.  It will work if you use SVM with a linear kernel at that point but for speed I used Logistic regression for my comparison. I do know that some people with similar rankings were user a neural net.  I have linked to some of their code. (https://www.kaggle.com/amoyakd/sf-crime/in-python/run/17555)
MachineLearning,3dyw9s,zarus,8,Tue Jul 21 00:24:25 2015 UTC,"Hahaha   Machine learning algorithms—when applied to social problems—have a tendency to be racist. Many people can't afford to buy a Prius due to systemic racism. You should be thankful that they have an opportunity to take batteries so they can put food on the table, from privileged people who will simply have their insurance companies reimburse them. It's your job to care for them."
MachineLearning,3dyw9s,dwf,2,Mon Jul 20 21:55:11 2015 UTC,In case you haven't heard of her.
MachineLearning,3dyw9s,zarus,1 point,Tue Jul 21 01:05:03 2015 UTC,I like her.
MachineLearning,3dyw9s,quattro,2,Tue Jul 21 05:51:07 2015 UTC,"You laugh, but this is actually a difficult problem. This excellent blog post by ML researcher Moritz Hardt details why in a fairly convincing way. Defining an algorithm to be ""just"" or ""fair"" in a mathematical way is doable; however, showing that your algorithm behaves in that way is not the norm as of yet."
MachineLearning,3dyw9s,say_wot_again,2,Tue Jul 21 01:03:21 2015 UTC,"Thanks for the read. I figured that biases in the training data would be problematic, but I hadn't really thought about things like lower accuracy due to less data on minorities."
MachineLearning,3dyw9s,quattro,2,Tue Jul 21 04:02:52 2015 UTC,"No problem! Moritz Hardt has his own blog as well that is full of super insightful ideas. There is an entire workshop dedicated to problems in this area now. It's quite interesting to see tools from theoretical CS get applied to ML (differential privacy, etc). Plus it makes for way more fun reads than another boring blog post about some shmuck's implementation of a deep neural net."
MachineLearning,3dyw9s,jrkirby,-7,Tue Jul 21 04:12:55 2015 UTC,It's cause the data scientists writing the algorithms are racist. Have you ever met a data scientist who was a person of color?
MachineLearning,3dyw9s,Coffee2theorems,3,Mon Jul 20 23:34:30 2015 UTC,"Have you ever met a data scientist who was a person of color?   Of course! The rumors that they are all dull and pasty because they hide in their caves sucking the lifeblood out of data all day like vampires instead of socializing on the beach are greatly exaggerated.   the data scientists writing the algorithms are racist.   I assure you Sir, any perceived pro-vampire bias is entirely in your imagination! Everyone knows that the bloodsuckers are all in the legal department. These are not the droids you're looking for.  Incidentally, might I interest you in signing a petition for the government to produce huge ash clouds to globally blot out the Sun? It's an awful health hazard, what with that UV radiation causing skin cancer and all."
MachineLearning,3dyw9s,quattro,3,Tue Jul 21 01:28:15 2015 UTC,"I would imagine you could extend the logistic function to be exponential of a quadratic function, ie, Gaussian. Then you would be fitting a bunch of gaussian distributions dispersed over the region. How many gaussians would need to be trained via CV or in a hierarchical model.  The gist of it would be that crime is dispersed in a 2d normal distribution around ""hot spots"". Obviously is a simplification of reality, but all of these models are. You could probably enforce the 2d normals to be spherical and cut back on parameters and reduce over-fitting."
MachineLearning,3dyw9s,noelsusman,2,Mon Jul 20 22:55:54 2015 UTC,"I did some work for my city's police department last year and this is pretty much what I did.  It's nice because you can come up with a map of hot spots for various types of crime at any given time of the day.  The cops loved it.    The old system just told them to go to a certain intersection between 3pm and 4pm based on historical data, and they hated that.  Now whenever they have downtime they pull up the map and decide where to go based on that.  I learned that predicting homicide locations and times with any practical level of accuracy is extremely difficult."
MachineLearning,3dyw9s,despardesi,1 point,Mon Jul 20 23:46:10 2015 UTC,"That is interesting, it is always good to know that data is getting used in real world scenarios."
MachineLearning,3dyw9s,ihopeudie,2,Tue Jul 21 00:26:52 2015 UTC,"Perhaps I'm being incredibly dense here, but I don't understand the point of predicting the type of crime once the crime has already happened.  What would be interesting is to predict the place and time of the next crime of a particular type. Tell me which are the most likely locations of the next murder at 10pm tonight. Where are the car breakins more likely to happen tonight. etc."
MachineLearning,3e07l4,utapaua,4,Mon Jul 20 23:15:14 2015 UTC,"This is pretty much a direct application of encode-decode / sequence to sequence RNNs. If you understand how the ""data was pre-processed, the wiring of the network, how sequences of different lengths were handled and so on"" for that model, it should map almost directly here. The preprocessing might be a tiny bit different since you don't have to deal with multiple languages, etc. The one thing that is not clear in the paper is what the ""extra tokens"" of turn taking and actor are used for - beyond marking who is saying what (which is already covered by the actor token) the ""turn taking"" piece seems redundant to me unless that is their replacement for EOS.  If you aren't familiar with encode-decode type models, I encourage you to check out this blog by Kyunghyun Cho, followed by a bunch of the papers he cites in the post.   You might also be interested in this model for dialog ""Hierarchical Neural Network Generative Models for Movie Dialogues"", I. Serban, A. Sordoni, Y. Bengio, A. Courville, J. Pineau. They have shared the code here and other people have used it for things like IRC bots.   There are some other interesting methods ""Skip-Thought Vectors"", R. Kiros, Y. Zhu, R. Salakhutdinov, R. Zemel, A. Torralba, R. Urtasun, S. Fidler which seem highly related (thinking of previous and future responses as context), and they have at least shared their encoder here."
MachineLearning,3e07l4,kkastner,2,Tue Jul 21 03:44:24 2015 UTC,"Oh wow thanks, I think I will try this out. Do you know how these methods scale? I'm thinking of applying it to the reddit dataset that was published a few days ago. Probably just a subset of it..."
MachineLearning,3e07l4,quirm,1 point,Tue Jul 21 10:12:40 2015 UTC,"It should be fine for that. You have most of the same problems of dealing with large volumes of text that you have with the translation tasks, with noisier data. In my experience the pain of traversing from anything > RAM sized to multi-TB is mostly a matter of time and patience.   It will probably take some engineering on your part to deal with the data with decompressing (Python can read straight from gzip, but I am not sure about the data format of the dump you mention), and these models take a long time to train. Skip thoughts seem ideal for this since you have comment-reply threads, though I think the dialogue approach of Serban et. al. can work directly as well. You might try on a particular subreddit just to see if it is working (r/machinelearning for some fun meta-analysis :) )"
MachineLearning,3e07l4,kkastner,1 point,Tue Jul 21 13:00:26 2015 UTC,"@_DaveSullivan   2015-07-20 16:12 UTC  Encoder-decoder trained 3 days on 12gb gpu. Dataset is IRC. TBF, this is what IRC is like, but this is no chatbot...   [Attached pic] [Imgur rehost]     This message was created by a bot  [Contact creator][Source code]"
MachineLearning,3e07l4,TweetsInCommentsBot,1 point,Tue Jul 21 03:44:31 2015 UTC,"I was really impressed by the Vinyals and Le (neural conversation) paper. When I was in high-school I to tried to build a chat bot, but I got disappointed by the technology at the time. I've been thinking about this problem ever since for the past 10 years. The reason I was so impressed is the kind of common sense reasoning the model can do. Even with a lot of feature engineering and specialized models, you could not do that right now.  I just skipped over the Pineau et. al. paper you posted, but it seems they use a single layer of GRU, where Vinyals and Le use 4 layers. I am not sure how they generate the words, but Vinyals and Le use a softmax over the entire vocabulary which was previously thought to be impossible. It seems to me that the neural conversation model is more more powerful, but also much harder to implement than previous models."
MachineLearning,3e07l4,ma2rten,1 point,Tue Jul 21 16:41:22 2015 UTC,"Agreed in general - just want to correct some facts here (you really might want to read the Serban et. al. paper and definitely the Kiros et. al. for vocab tricks...):  1) Vinyals, Le use a single layer LSTM with 1024 cells for the IT task with a small 20K vocabulary (start of 5.1)    Given the lack of data for this domain specific and clean dataset, we trained a single layer LSTM with 1024 mem- ory  cells  using  stochastic  gradient  descent  with  gradient clipping.  We built a vocabulary of the most common 20K words, and introduced special tokens indicating turn taking and actor.   The Open subtitle task has 2 layers, but there is not much difference IMO between that and the one layer (or just having deep inputs and outputs). 100K vocabulary like they have on the Open task is big-ish but certainly doable on a single GPU - see the work by Jean et. al. with vocabulary up to 500K. This all seems single GPU level - not trivial, but not a massive engineering effort either.   TBH the original Sutskever seq2seq had way more engineering effort in it, and was way more daunting to me. This model could likely be built in any RNN framework. I say this because I am planning to implement - may eat these words later."
MachineLearning,3e1tv2,ptitz,1 point,Tue Jul 21 08:36:18 2015 UTC,Why not define them in exactly the same way as you do for the PID controller?
MachineLearning,3e1tv2,kjearns,1 point,Tue Jul 21 09:14:19 2015 UTC,"Because then I expect it to have the same problems and limitations that a PID controller would have. Like for example if I just put a potential function over entire domain, attracting my plant towards the setpoint, it could take a sub-optimal route or get stuck in the middle. If my potential function would only be active closer to the goal, then I would have to shape and scale it somehow, and I'm not sure what would be the best way to do that. The whole point of that thing is to have like a dozen identical controller algorithms running in parallel, controlling all sorts of dynamics of a more complex plant. It has to be as general as possible."
MachineLearning,3e1tv2,NasenSpray,1 point,Tue Jul 21 09:29:03 2015 UTC,"Like for example if I just put a potential function over entire domain, attracting my plant towards the setpoint, it could take a sub-optimal route or get stuck in the middle.   Q-Learning should be able to handle that. You can also bootstrap by letting it learn the actions of a PID controller."
MachineLearning,3e1tv2,tabacof,1 point,Tue Jul 21 10:47:48 2015 UTC,"You could try some cost functions from optimal control. Indeed if you just try to minimize the squared error between the setpoint and signal the result is going to be very fragile, so you should use some kind of regularization on the control input, for example minimizing also the squared control signal value."
MachineLearning,3e1tv2,iamwell,2,Tue Jul 21 19:41:20 2015 UTC,Novice ML-er here.  It seems like most examples for reinforcement learning use discrete states and some reward function and transition probability for switching between states.  I don't understand how this can be used with continuous functions.  Hoping you figure out how to do this and let us know!
MachineLearning,3dzouw,cypherx,1 point,Mon Jul 20 20:56:26 2015 UTC,Gating is essential   Aren't iRNNs the current state of the art on a diverse set of non-trivial benchmarks?
MachineLearning,3dzouw,5at,4,Mon Jul 20 22:11:23 2015 UTC,"This presentation was made before iRNNs came out, the graph shown is much less dramatic if iRNNs are used and I wouldn't make the statement anymore - never seen them be equivalent/outperform in the text understanding domain, however."
MachineLearning,3dzouw,alecradford,2,Tue Jul 21 01:13:32 2015 UTC,Noob here: What's iRNN?
MachineLearning,3dzouw,XalosXandrez,2,Tue Jul 21 06:34:31 2015 UTC,http://arxiv.org/abs/1504.00941  Plain RNN + rectifier activation + identity (or scaled identity) initialization.
MachineLearning,3dzouw,alecradford,1 point,Tue Jul 21 07:48:17 2015 UTC,"Not that I know. I have not seen them used on anything besides that flattened MNIST task, though I have heard from many that it seems to work well."
MachineLearning,3dz3fl,spurious_recollectio,2,Mon Jul 20 18:24:31 2015 UTC,I've used keras to build a tagging model using an LSTM (or a GRU). You can also use a CNN to do the same thing I suspect.
MachineLearning,3dz3fl,simonhughes22,1 point,Mon Jul 20 22:24:09 2015 UTC,How did you do this in keras?  Classifying given windows of tokens or something else?
MachineLearning,3dz3fl,StupidQuestionOracle,2,Tue Jul 21 01:30:31 2015 UTC,"It's pretty simple, now they have timedistributed dense: embedding_size = 64 print('Build model...') model = Sequential() model.add(Embedding(max_features, embedding_size))  model.add(GRU(embedding_size, 64, return_sequences=True)) # try using a GRU instead, for fun model.add(TimeDistributedDense(64, 1, activation=""sigmoid""))  model.compile(loss='binary_crossentropy', optimizer='adam', class_mode=""binary"")"
MachineLearning,3dz3fl,simonhughes22,1 point,Tue Jul 21 15:45:16 2015 UTC,"embedding_size = 64 print('Build model...') model = Sequential() model.add(Embedding(max_features, embedding_size)) model.add(GRU(embedding_size, 64, return_sequences=True)) # try using a GRU instead, for fun model.add(TimeDistributedDense(64, 1, activation=""sigmoid"")) model.compile(loss='binary_crossentropy', optimizer='adam', class_mode=""binary"")   Wait, why not categorical? :) Thanks!"
MachineLearning,3dz3fl,StupidQuestionOracle,1 point,Tue Jul 21 18:22:59 2015 UTC,Wow nice. Can we see the code for that? I'd love to try out something other than CRF (via pycrfsuite) on my toy tasks.
MachineLearning,3dz3fl,cryptocerous,1 point,Tue Jul 21 12:50:18 2015 UTC,See reply above
MachineLearning,3dz3fl,simonhughes22,1 point,Tue Jul 21 15:45:21 2015 UTC,Could you specify the architecture and datasets you used?
MachineLearning,3dz3fl,simonhughes22,1 point,Tue Jul 21 13:15:02 2015 UTC,"See reply above. I am training it on a research problem for my PhD. It's not a public dataset. It's a tagging problem, where we are tagging words with concepts according to a model of a domain."
MachineLearning,3dz3fl,newwave2k,3,Tue Jul 21 15:46:07 2015 UTC,"A little bit old but still very good: Natural Language Processing (almost) from Scratch It's a feedforward architecture.   For advanced architectures, this k-max pooling CNN one is my favorite: A Convolutional Neural Network for Modelling Sentences. They use the network for sentiment prediction or question type classification tasks but all of your tasks can be easily converted to a classification problem, and then use their fully-connected layers for that."
MachineLearning,3dz3fl,cryptocerous,1 point,Mon Jul 20 19:34:57 2015 UTC,Thanks I should have mentioned that I know about the SENNA paper (I was actually looking at it before asking the question).  I don't think they do coreference resolution.  I was hoping to get some reference to an RNN architecture that can handle coreference resolution.  The dynamic CNN paper looks very cool.  I'll check it out.
MachineLearning,3dz3fl,Articulated-rage,1 point,Mon Jul 20 19:45:45 2015 UTC,"NER is can easily be done with classifiers, just as a CPU can ""easily"" be constructed just with diodes :)"
MachineLearning,3dz3fl,cryptocerous,1 point,Tue Jul 21 12:57:44 2015 UTC,"It's not terribly painful to write a wrapper around the Stanford NLP software. Pyjinius took fairly little work to get working.  Given you're willing to do this, their NER tagger is state of the art (pretty sure, may be wrong).    The state of the art just uses a linear chain crf. If you find a decent dataset, you could probably roll your own using something like pystruct. There's some other crf's in python too.    Also, if you do come across an rnn based ner tagger, please let me know. But as far as I know, no one has published a new state of the art with dnn sequence models. Or at least, it hasn't made a big enough splash (yet?) for people to talk about it often."
MachineLearning,3dz3fl,cryptocerous,1 point,Tue Jul 21 00:21:13 2015 UTC,"I've tried using stanford core NLP, ollie and its varients and Berkeley NER in python projects and I find it rather annoying to integrate java into a python project.  Moreover, as i said below, I'm pretty unfamiliar with CRFs and I feel this is a task that RNNs should be good.  It would seem to me if you can translate between two languages from scratch you should be able to do entity recognition.  I guess the main hurdle is the latter often requires out-of-band data or data from earlier sentences and hence maybe the raw sequential approach is not the best.  I was thinking that treating it a bit like QA and providing a set of context sentences to allow the RNN to better extract the context of the references might be a nice way to go (see e.g. some of the recent facebook AI question-answering architectures).  But I guess no one has done this yet."
MachineLearning,3dz3fl,cryptocerous,1 point,Tue Jul 21 13:14:29 2015 UTC,"I guess the main hurdle is the latter often requires out-of-band data or data from earlier sentences and hence maybe the raw sequential approach is not the best.   Exactly. That's a huge part of the problem. This is the part that I've been working on for the past few months.  The vast majority of the unique words used in any modern language are some kind of special technical terminology words. Think - medical terminology, product names, scientific terminology, legal terminology, etc. And you can't just assume that these rare words are being used to identify an entity. These rare technical words could also be talking about a part or feature of the entity, or describing some unique attribute of the entity, or describing something the entity does, or all kinds of other non-identifying jargon.  More stats to consider:   There are 600,000 names just for unique species of fungus alone! 200,000,000+ products listed on amazon, including duplicates. Even removing duplicates, it's still in the millions. Estimated 129,864,880 books in existence. The vast overall majority of unique English words are specialized terminology words (can't find a good source on this at the moment, but I guarantee it.)   So there have been a variety of early attempts to better fuse external entity knowledge bases with advance QA systems (which presumably could be used for NER too).  Some early attempts. I know I've seen others, but I'll have to find the links again:  http://arxiv.org/pdf/1506.07285.pdf  http://cs224d.stanford.edu/reports/KapashiDarshan.pdf"
MachineLearning,3dzjw8,AdventureTime25,5,Mon Jul 20 20:21:41 2015 UTC,"I.e. can you train an existing image classifier to recognize a new type of image (like, say, a parrot) by adding a new neuron on the last layer and only adjusting the weights/parameters for that neuron?   Yes. The computer vision literature is now full of people who do this sort of thing routinely.   As a related question, can someone explain what models like googlenet or alexnet do? I downloaded the caffe models for both, but I'm not sure how to train them to recognize specific images.   You should be able to interact with them programmatically and have them give you probabilities for each of the classes they know about for a given image. What people usually do is chop off the entire classifier layer and train a new one. You'll have to look at the Caffe docs for a clearer picture of how to do that in Caffe."
MachineLearning,3e2af0,martijnT,4,Tue Jul 21 12:13:08 2015 UTC,╔═════════════════ ೋღ☃ღೋ ════════════════╗                    This post is spam     ╚═════════════════ ೋღ☃ღೋ ════════════════╝
MachineLearning,3e2af0,Calling_out_spam,2,Tue Jul 21 12:59:38 2015 UTC,"you're doing god's work, son."
MachineLearning,3e2af0,j1395010,1 point,Tue Jul 21 13:10:20 2015 UTC,Why? It seems to have a lot of code for a spam article
MachineLearning,3e2af0,Calling_out_spam,1 point,Tue Jul 21 14:34:14 2015 UTC,"http://i.imgur.com/CquBXo5.png  89% of the domains martijnT submitted are for datacamp.com.  If martijnT don't work with/for them it sure looks like he does.  Reddit's policy regarding self promotion   https://www.reddit.com/wiki/selfpromotion  Feel free to post links to your own content (within reason). But if that's all you ever post, or it always seems to get voted down, take a good hard look in the mirror — you just might be a spammer. A widely used rule of thumb is the 9:1 ratio, i.e. only 1 out of every 10 of your submissions should be your own content.  edited: for clarification"
MachineLearning,3e2af0,cavedave,1 point,Tue Jul 21 15:53:07 2015 UTC,Sorry who do you mean by 'you've submitted'? I haven't submitted any datacamp domains that I am aware of
MachineLearning,3e2af0,Calling_out_spam,1 point,Tue Jul 21 17:34:50 2015 UTC,Check the post history for the poster https://www.reddit.com/user/martijnT
MachineLearning,3e2af0,cavedave,1 point,Tue Jul 21 19:08:49 2015 UTC,If you check out the meaning of pronouns. Hes are being a grammar Nazi here because sentences make no sense if they mix up pronouns wherever we want to.
MachineLearning,3e2af0,despardesi,1 point,Tue Jul 21 19:38:15 2015 UTC,Maybe it's because submitter seems to be a co-founder of DataCamp:   https://www.linkedin.com/pub/martijn-theuwissen/33/9b9/b56
MachineLearning,3e2af0,Calling_out_spam,1 point,Tue Jul 21 19:18:44 2015 UTC,which makes it self promotion
MachineLearning,3dw6pc,jfsantos,7,Mon Jul 20 01:13:41 2015 UTC,"Seems interesting, and from a design perspective is fairly inline with what I have been working on for my own use.   The docs look nice, it has some semblance of testing, and the ease with which you can do multi-gpu training is impressive to say the least. I would be interested to see the Deep Q Learning example especially."
MachineLearning,3dw6pc,kkastner,2,Mon Jul 20 02:39:38 2015 UTC,"It seems nice, there's some advantages (easier to debug and add ops) and shortcomings (not as memory/computation efficient) compared to Theano.  The Deep Q Learning example is available here, but I still did not have a chance to play with it."
MachineLearning,3dw6pc,siblbombs,6,Mon Jul 20 19:57:19 2015 UTC,I'd be very interested to see the speed of recurrent functions on the gpu in Chainer compared to theano/torch.
MachineLearning,3dw6pc,siblbombs,4,Mon Jul 20 13:47:22 2015 UTC,"One of the included Chainer examples is a 2 layer LSTM next word predictor, which I've recreated in blocks. On my machine, using the CPU, Chainer takes around 240 seconds to run 100 forward/backwards passes, and blocks/theano takes around 280 seconds.   I will do the same test on a Titan X and see what happens there."
MachineLearning,3dw6pc,siblbombs,3,Mon Jul 20 19:49:11 2015 UTC,"With my GPU on ubuntu/cuda7/current theano I'm running 100 training batches in about 17 seconds, whereas chainer is running it in about 29 seconds."
MachineLearning,3dw6pc,sieisteinmodel,1 point,Tue Jul 21 13:02:52 2015 UTC,A factor of two is quite a bit... what model are you looking at?
MachineLearning,3dw6pc,siblbombs,1 point,Tue Jul 21 17:57:51 2015 UTC,The 2 layer LSTM example (ptb) in chainer vs the same setup in blocks.
MachineLearning,3dw6pc,rcwll,2,Tue Jul 21 18:17:46 2015 UTC,"For whatever it's worth, I'm seeing about the same thing for my own code on Amazon g2.2xlarge instances.  Speed on the CPU is about the same between Theano and Chainer (ignoring Theano's compile time), while on the GPU I see anywhere from about a 1.5 to 2.5 fold slowdown with Chainer, depending on the size of the model and the length of the sequence.    Still almost worth it not to have to deal with Theano's scan op, though."
MachineLearning,3dw6pc,siblbombs,3,Tue Jul 21 18:39:08 2015 UTC,"Yea, it is pretty impressive already, but if chainer reached/exceeded theano for recurrent steps I'd move to it in a heartbeat. I'm curious how well chainer is handling moving the data to the GPU, in theano all my data was in a shared variable so there is no transfer time during computation, but IDK if chainer does any similar optimization."
MachineLearning,3dw6pc,rcwll,1 point,Tue Jul 21 18:50:51 2015 UTC,"My guess is that that's probably the main slowdown; since Chainer doesn't actually know what all of the data structures it's going to have to backprop are until you're done building the expression, which can be different each time, it can't be as clever about managing data transfers.    Versus Theano, where you compile the model in a sort of declarative fashion, and can't change it from that point forward, so it can optimize in advance how to manage the data.  Which is too bad in a way, since it means that there's probably going to be a lot of cases where Chainer is going to be inherently slower, just as part of the price for the extra flexibility it gives you."
MachineLearning,3dw6pc,rantana,1 point,Tue Jul 21 19:32:52 2015 UTC,Can you try transferring the data in batches for theano just to get a more fair comparison? I think most datasets these days don't fit entirely in GPU memory.
MachineLearning,3dw6pc,siblbombs,1 point,Tue Jul 21 20:51:57 2015 UTC,"So I run for 100 training examples and report the overall time it took to run, excluding any theano/chainer compile time.  The theano/blocks best-case scenario is the data loaded to a shared variable, embedding/linear mapping to 1st LSTM/dropout of 2nd hidden layer/linear softmax mapping/softmax calculation are done outside of the scan.  Times are within .5 seconds, tested across several runs. GPU: Titan X.  Chainer: 30.5  Theano, best case: 16.5  Theano, best case, no shared variable: 16.5  --- if not stated below, was done inside scan ---  Theano, no shared, l2 hidden dropout/softmax calculation outside scan: 22  Theano, no shared, softmax calculation outside scan: 28.5  Theano, no shared, embed/initial linear LSTM calc outside scan: 30  Theano, worst case. Everything done inside scan: 36.5"
MachineLearning,3dw6pc,rantana,1 point,Tue Jul 21 23:24:38 2015 UTC,"Very nice tests.  It sounds like theano/blocks is taking advantage of a lot of the data independence at the outputs and inputs to stay out of scan(). Instead of looping over each one in sequence, theano/blocks is doing one big BLAS call over the entire sequence.  I wonder if chainer is taking advantage of these tricks as well. There's also different variants of LSTMs where you can concatenate the computation of your input, forget, output gate activations into a single BLAS call instead of 3 to 4 separate ones.  Have you run tests on feedforward networks? I think those networks might be the best bet for comparing raw speed between the two because there aren't as many optimization tricks. Recurrent networks can be implemented in drastically different ways (especially LSTMs)."
MachineLearning,3dw6pc,streakycobra,2,Tue Jul 21 23:50:21 2015 UTC,I'll be implementing a 2 layer LSTM in Chainer and have equivalent Keras code. I can post some throughput figures here later.
MachineLearning,3dw6pc,gsmafra,4,Mon Jul 20 19:58:47 2015 UTC,"How does it compare to keras? After looking at the given mnist examples [1,2] of both, keras seems at least more intuitive and equally powerful (and for flexible I don't know).  [1] chainer's mnist  [2] keras' mnist    Edit: I just saw the open issue about comparison with other frameworks on github"
MachineLearning,3dw6pc,xamdam,7,Mon Jul 20 07:28:06 2015 UTC,Not being built on top of Theano is already a big thing
MachineLearning,3dw6pc,gsmafra,2,Mon Jul 20 08:54:48 2015 UTC,"Why? Theano provides important low-level functionality, it makes sense to have stuff built on top of it."
MachineLearning,3dw6pc,streakycobra,2,Mon Jul 20 12:44:04 2015 UTC,"Yes, but one of the important aspects of using open source software is knowing exactly what you're doing and being capable of extending it. Not everybody has the time to learn how to debug graphs of functions and etc to effectively work directly with Theano"
MachineLearning,3dw6pc,Foxtr0t,2,Mon Jul 20 13:48:09 2015 UTC,"Chainer is lower-level but could be used to implement higher-level functionality such as what's available in Keras. I found it easier to debug than Keras, since with Keras you have the Theano black magic obfuscating your code."
MachineLearning,3dw6pc,rmlrn,1 point,Mon Jul 20 19:52:10 2015 UTC,"Ok, thanks for the comparison!"
MachineLearning,3dw6pc,Foxtr0t,-2,Tue Jul 21 07:11:09 2015 UTC,"Frankly, [1] has twice as many lines of code as [2], so I would draw an opposite conclusion."
MachineLearning,3dw6pc,rmlrn,3,Mon Jul 20 12:18:38 2015 UTC,"hmm, are you unable to count or to read?"
MachineLearning,3dw6pc,sieisteinmodel,0,Mon Jul 20 15:31:13 2015 UTC,"A slight miscomprehension, or maybe I was looking to argue."
MachineLearning,3dw6pc,duschendestroyer,3,Mon Jul 20 17:36:18 2015 UTC,"you want room 12A, next door."
MachineLearning,3dw6pc,sieisteinmodel,4,Mon Jul 20 17:39:09 2015 UTC,"Played around with it for about an hour, implementing the variational auto encoder. I have not completely verified the implementation, but I guess I have come to know that framework a bit.  Some comments.  i) It is astonishing how quickly you get up to speed. I got a deep auto encoder on MNIST running in about 20 minutes.   ii) It took me another 5 to implement my first custom Function, which was just sampling from a standard Normal.  (Sidenote: in 4 years of active use of Theano, I have never implemented one there myself. Either because I found it too daunting or since I never really needed it.)  iii) It took me another 5 to adapt my Theano KL divergence code to chainer.   iv) Chainer has autodiff to some extent. I feel that the primitives currently make it necessary to implement some derivatives yourself, e.g. I did not find a function to only sum over a specific axis. This is extremly important for some models. This did not come in handy when I made the variational bound. Also, having output.sum(1) is just very convenient.  v) Debugging is currently not easier than in Theano, imho. When the shapes do not match, the error messages are worse than when you have computation of test values activated for Theano.  vi) Optimisation is hidden somewhere. I am biased (since I authored climin, http://climin.readthedocs.org) and want to be able to inspect things. I don't know if this is possible right now.  vii) Just a gut feeling, but the CPU performance does not seem to be a match for Theano's.  Bottom line: This tool might be for you. Check it out. Given that it is quite young, it might mature to a serious contestant of Theano, also given the enormous amount of effort that seems to already have gone into the project. Also, the code base does to be much cleaner than Theano's."
MachineLearning,3dw6pc,duschendestroyer,3,Mon Jul 20 20:23:14 2015 UTC,I'd be interested to see the code of your variational auto encoder.
MachineLearning,3dw6pc,valexiev,2,Tue Jul 21 08:34:39 2015 UTC,"https://github.com/bayerj/chainer/blob/vae/notebooks/VariationalAutoEncoder.ipynb  That being said, it does not work currently. Still bugs lurking around.  For one, I am using MSE (because I was too lazy to code a proper NLL). Then, I don't know how to initialise parameters properly.   It was enough for me however, to get the gist of chainer, I believe."
MachineLearning,3dw6pc,duschendestroyer,1 point,Tue Jul 21 12:02:00 2015 UTC,"Thanks, this helps a lot. I made it work. I reformulated the loss and added the batch size to the scaling. all relevant changes are in the forward function:  http://nbviewer.ipython.org/gist/duschendestroyer/a41fcab5f7f9ffa45387"
MachineLearning,3dw6pc,j_lyf,4,Tue Jul 21 23:24:44 2015 UTC,Has anyone tried this out? What did you think?
MachineLearning,3dw6pc,respresprespresp,5,Mon Jul 20 01:57:14 2015 UTC,"I played around with it since yesterday. I love the API. It's super easy to define and train custom architectures, especially recurrent nets.  Give this library a year to mature and maybe then we can finally get rid of theano."
MachineLearning,3dw6pc,siblbombs,8,Mon Jul 20 07:27:21 2015 UTC,What's wrong with Theano?
MachineLearning,3dw6pc,siblbombs,1 point,Mon Jul 20 11:17:14 2015 UTC,"Comparing theano to a deep network library isn't a great comparison. Theano allows you to create fairly arbitrary computations including but not limited to deep networks. I've written spiking neural-networks in theano and also data pre-processing pipelines in addition to deep networks.  You have to work to create something like a deep network library from theano, but you're still left with a lot of flexibility. The trade-off is that you have a more complicated backend.  Unless chainer implements auto-diff and most of the mathematical function in theano.tensor, they aren't really replacements for one another."
MachineLearning,3dw6pc,rantana,2,Mon Jul 20 16:54:06 2015 UTC,Looks like chainer does auto-diff.
MachineLearning,3dw6pc,serge_cell,2,Mon Jul 20 17:57:17 2015 UTC,"Chainer depends on implementations of a forward and backward pass for all operations you want to use. It does auto-diff based on these. If you want to use an operation that's not supported, it is possible to add new operations (adding any simple functions that are already supported by Numpy/PyCUDA is trivial)."
MachineLearning,3dytqn,asymptotics,3,Mon Jul 20 17:17:38 2015 UTC,"I've read that spiking neural networks are going to be the ""third-generation"" neural network model   Where'd you read that?  Wikipedia?  I'm not sure who put that part in there, but I doubt it represents any sort of consensus.  As far as we know - spiking NNs do not have any representational advantages over standard synchronous ANNs.  Binary spiking NNs are actually weaker - requiring more neurons/connections to do the same job.  From a low-level point of view, spiking ANNs can be more energy efficient in hardware - thus the neuromorphic chips like IBM's TrueNorth tend to be spiking NNs.  Unfortunately those designs also use binary neurons and low-bit synapses, which are much harder to work with and haven't been that successful yet."
MachineLearning,3dytqn,jcannell,1 point,Mon Jul 20 17:37:59 2015 UTC,I read it from this intro (pdf): https://people.mmci.uni-saarland.de/~jilles/pubs/2002/spiking_neural_networks_an_introduction-vreeken.pdf  Don't spiking NNs have an advantage in representing time series?
MachineLearning,3dytqn,rmlrn,7,Mon Jul 20 17:47:14 2015 UTC,"that is 13 years out of date... if they knew what they were talking about, wouldn't spiking networks have arrived by now?"
MachineLearning,3dytqn,quiteamess,2,Mon Jul 20 18:12:06 2015 UTC,"Spike time dependent plasticity is what you are looking for. However, I don't se why a spiking implementation should be better than a ANN implementation. The only thin that comes to mind are liquid state machines. But echo state networks are roughly equivalent."
MachineLearning,3dytqn,sleeppropagation,1 point,Mon Jul 20 17:45:05 2015 UTC,"They ended up not getting very popular. If you want to play around with them, the main papers are by Bothe and Booij (SpikeProp and MultiSpikeProp, the two main learning algorithms).  You can get up to 50x speedup if you learn delays and neuron thresholds as well. Unfortunately by the time such techniques were explored (around 2012~2013), SNNs had already lost all the tiny hype they had.  I have a bunch of experiments on recurrent spiking neuron networks using izhikevich auto-encoders, but I'm pretty sure they're not worth publishing anymore.  However, they do have representational advantages over ANNs. The fact that they can learn XOR without hidden layers is worth noticing (even though it is a hair-trigger state). Also RSNNs are way more stable than RNNs. I never finished my Spiking LSTM model, so I can't compare them with vanilla LSTMs, but my bet is that they have smoother convergence and are more stable as well."
MachineLearning,3dy3e9,bluedunnock,3,Mon Jul 20 14:04:35 2015 UTC,"If q(y=c|x) is the posterior output for your model trained on an artificially balanced dataset, q(y=c) is the prior probability of a particular class in the balanced set, and p(y=c) is the true class prior, then you can get adjusted estimates by computing p(y=c|x) = (p(y=c)q(y=c|x)/q(y=c)) / sum_c' (p(y=c')q(y=c'|x)/q(y=c'))"
MachineLearning,3dy3e9,dwf,1 point,Mon Jul 20 18:40:31 2015 UTC,"Thanks a lot. What is the theoretical meaning of dividing posterior q(c|x) by q(c), the reasoning behind this scaling"
MachineLearning,3dy3e9,dwf,2,Mon Jul 20 19:43:26 2015 UTC,"He articulates that in the text. The posterior is proportional to the prior, by Bayes' theorem. By dividing by q(c) you scale away the ""false prior"" from your artificially balanced dataset and replace it by the real prior p(c) by multiplying."
MachineLearning,3dzupp,classicalhumanbeing,1 point,Mon Jul 20 21:37:43 2015 UTC,"Have you actually read PT:TLoS? The robot is just an abstract construct, an agent that operates solely using Bayesian inference. He gives no indication that such robot could or should be built and what it'd be for."
MachineLearning,3dzupp,tabacof,1 point,Tue Jul 21 13:14:19 2015 UTC,He does actually give an indication. Larry wrote about it but said the code was an archaic BASIC variant and didn't age gracefully enough to publish.
MachineLearning,3dx61s,docClassifier,2,Mon Jul 20 07:18:44 2015 UTC,Depends on what you are trying to do but an auto encoder is a good way to build an unsupervised representation
MachineLearning,3dx61s,yggdrasilly,0,Mon Jul 20 08:26:16 2015 UTC,thank you good sir!
MachineLearning,3dzlu7,BrassTeacup,1 point,Mon Jul 20 20:35:25 2015 UTC,"Yes, google created a chat-bot recently using IM's and also movie dialog:  http://www.wired.co.uk/news/archive/2015-06/30/google-chatbot-philosophy-morals  They are not the first to try this."
MachineLearning,3dws7q,downtownslim,1 point,Mon Jul 20 04:33:10 2015 UTC,I love to see game play learning. Great video!
MachineLearning,3dxbf6,AlcaDotS,6,Mon Jul 20 08:37:19 2015 UTC,Maybe you are talking about this one
MachineLearning,3dxbf6,sergii_gavrylov,2,Mon Jul 20 08:58:02 2015 UTC,"It's not the one I had in mind, but it looks interesting, thanks!"
MachineLearning,3dxbf6,piesdesparramaos,3,Mon Jul 20 09:28:26 2015 UTC,"It is not what you are looking for, but you also have: http://karpathy.github.io/2015/05/21/rnn-effectiveness/ vs http://nbviewer.ipython.org/gist/yoavg/d76121dfde2618422139"
MachineLearning,3dxbf6,Foxtr0t,2,Mon Jul 20 15:55:05 2015 UTC,https://github.com/IndicoDataSolutions/Passage/tree/master/examples
MachineLearning,3dxbf6,ndronen,2,Mon Jul 20 12:12:47 2015 UTC,"I would look at the papers by Tomas Mikolov from 2011-2013.  (Sorry , no particular reference — responding on my phone.)"
MachineLearning,3dxbf6,woodchuck64,1 point,Mon Jul 20 16:10:27 2015 UTC,Karpathy's Visualizing and Understanding Recurrent Networks? http://arxiv.org/abs/1506.02078
MachineLearning,3dxfr9,mrflipppy,1 point,Mon Jul 20 09:42:14 2015 UTC,"I am not completely sure what kind of inference you want to make with the model (""explain"" can mean many things IMHO). But what you describe seems to be related to multiple-instance learning. Or perhaps you can use a multi-level model to relate the observations in the tensor to Y?  Perhaps this question is easier to answer if you describe where the data came from."
MachineLearning,3dwpn4,stua8992,2,Mon Jul 20 04:07:30 2015 UTC,Caffe project offers a model zoo wiki. You'll find all the major CNNs. https://github.com/BVLC/caffe/wiki/Model-Zoo
MachineLearning,3dwpn4,stafis,1 point,Mon Jul 20 10:41:58 2015 UTC,First 2 or 3 layers are just Gabor-like filters in most of models. So you may try just pretrained imagenet.
MachineLearning,3dwpn4,serge_cell,1 point,Mon Jul 20 07:14:07 2015 UTC,"sklearn-theano has this for general images, and you can take arbitrary layers. Repo here, should be an easy setup if you already have the scientific Python stack installed.  You may also be interested in this tutorial by Daniel Nouri on keypoint detection in faces."
MachineLearning,3dwpn4,kkastner,1 point,Mon Jul 20 12:49:35 2015 UTC,I played around with the GoogleNet model built there. It takes about 30 minutes for my graph to compile whenever I make any change to the theano expression. Is there any way to lower this insanely long compile time?
MachineLearning,3dwpn4,rantana,1 point,Mon Jul 20 15:37:34 2015 UTC,"Was this CPU or GPU? It should only do that the first time you use a particular setup, though I admit we haven't tried to hammer the edge cases too much. And it definitely shouldn't take 30 mins!"
MachineLearning,3dwpn4,kkastner,1 point,Mon Jul 20 15:53:00 2015 UTC,"GPU  I basically grab the theano expressions for the layers I want via caffemodel.parse_caffe_model() and attach my own theano expressions onto the GoogleNet. I also take the gradient because I need to train my theano expression from gradients produced by the GoogleNet.  I assume the long compilation time is due to the size and complexity of the graph of GoogleNet. It's strange that the compilation is not being cached in some way after the first compilation. In my other theano code, it's usually the first compilation that takes the longest. But this takes consistently about 30 minutes everytime."
MachineLearning,3dwpn4,rantana,1 point,Mon Jul 20 16:20:58 2015 UTC,"OK, thanks for the report. We might be able to play some games with joblib to speed that up :) The thing you are doing is something we have wanted to support (basically an actual fit() ), but never actually implemented or tested - glad to hear it is working.  It is pretty surprising that it isn't cached by theano after the first. Might be a bug or something due to our abuse of theano.clone to stitch together the model."
MachineLearning,3dwpn4,kkastner,1 point,Mon Jul 20 17:24:42 2015 UTC,"If you want to try a different implementation, I have a GoogleNet model using Lasagne.   I wonder if it's a problem with your Theano installation though - googlenet is complicated, but compiling the output only takes a couple minutes for me."
MachineLearning,3dwpn4,em0lson,1 point,Mon Jul 20 17:52:37 2015 UTC,"That looks promising, is there documentation on how to load the weights in the pkl file to the model?"
MachineLearning,3dwpn4,rantana,1 point,Mon Jul 20 20:50:04 2015 UTC,"To load the weights would be something like this:  net = build_model() output_layer = net['prob'] values = pickle.load(open('blvc_googlenet.pkl')) lasagne.layers.set_all_param_values(output_layer, values)   I should really finish writing up the example... there's a similar one for VGGnet here: https://github.com/Lasagne/Recipes/blob/master/examples/ImageNet%20Pretrained%20Network%20(VGG_S).ipynb"
MachineLearning,3dtjit,youngeverest,13,Sun Jul 19 08:53:58 2015 UTC,Last year Microsoft Research published an algorithm to train restricted Boltzmann machines exactly on a quantum computer: http://arxiv.org/abs/1412.3489  Now they have found an efficient classical analogue to that algorithm.
MachineLearning,3dtjit,sidsig,3,Sun Jul 19 08:57:02 2015 UTC,Super interesting. Thanks :)
MachineLearning,3dtjit,PM_ME_UR_OBSIDIAN,-2,Sun Jul 19 14:08:15 2015 UTC,developped by Microsoft Research. MSR and MSFT corporate are mostly disjoint entities.
MachineLearning,3dtjit,despardesi,1 point,Mon Jul 20 16:29:02 2015 UTC,"The ""M"" is the same. They're all still part of ""Microsoft""."
MachineLearning,3dt7lf,JiKuai,28,Sun Jul 19 05:41:35 2015 UTC,simple? perhaps.   readable? hell no.
MachineLearning,3dt7lf,devDorito,17,Sun Jul 19 09:25:18 2015 UTC,"Yeah, it is only 15 lines because the lines are bloated."
MachineLearning,3dt7lf,Portadiam,9,Sun Jul 19 09:43:21 2015 UTC,"yup, PEP80 exists for a reason. If your IPython notebook has horizontal scroll bars, you're doing it wrong."
MachineLearning,3dt7lf,rmlrn,2,Sun Jul 19 12:04:14 2015 UTC,"If you look at the actual section again, it's not even very clear on suggesting what the best character-length is.  Eh, I think that guideline could be modernized a bit.  Better if it were by some kind of ""complexity"" measure, instead of just character count.  A strictly character-count limitation encourages over-use of unreadably short names.  That said, OPs lines are ridiculous, by any measure."
MachineLearning,3dt7lf,cryptocerous,1 point,Sun Jul 19 19:15:25 2015 UTC,"it's not even very clear   The first two statements are:   Limit all lines to a maximum of 79 characters.  For flowing long blocks of text with fewer structural restrictions (docstrings or comments), the line length should be limited to 72 characters.   Followed by justification--and a modern one, at that (tabbed/paned editing). And while someone focusing on making the code one line might resort to shortened/unreadable names, the actual intent here is that one would format their code nicely while continuing to use good naming.  Though, more to the point, the real issue with the ""15 lines"" is that they rely way too much on comprehensions. This undermines it as a teaching tool (no names on the different concepts in each line), and as code (maintenance/reading nightmare)."
MachineLearning,3dt7lf,BomTomdabil,1 point,Mon Jul 20 20:57:17 2015 UTC,You stopped reading it too soon.
MachineLearning,3dt7lf,cryptocerous,1 point,Mon Jul 20 21:27:09 2015 UTC,"PEP0008, do you mean?"
MachineLearning,3dt7lf,BomTomdabil,1 point,Mon Jul 20 20:47:14 2015 UTC,"Yeah.. The ""15 lines"" is definitely a gimmick, but it was actually kind of fun trying to make it fit within 15 lines even though most of it's pretty unreadable.  I'll post a slightly longer, more readable version soon.  EDIT: I added the more readable code version (changed list comprehension to normal for loop and such) to the very bottom of the page. Doesn't require horizontal scrolling now. I don't have time to re-work the whole article using this more readable version, however."
MachineLearning,3dt7lf,leonoel,1 point,Sun Jul 19 19:32:06 2015 UTC,"I can see a couple of places where you could even bring it down to less lines.  Whatever is inside the second for could be crunched inside a whole megaline, and since there aren't any :if"", you perhaps even get away with a list comprehension there, bringing the whole thing to about 12 lines.  You could declare your variables in a single line (10 lines)  Anyway, is still unreadable. And I don't really see any merit on bringing code to less lines (unless you are actually making it more efficient)  Also, training NN with GA is interesting, but pretty old, it gets unfeasible with current sizes of NN"
MachineLearning,3dt7lf,manliest_destiny,14,Sun Jul 19 21:26:29 2015 UTC,"I don't understand the ""___________ in XX lines of code"" thing, because most of the time it's unreadable."
MachineLearning,3dt7lf,BomTomdabil,2,Sun Jul 19 14:15:10 2015 UTC,"Yeah, it's weird that they always get touted with a sort of educational air. How about a trend of A well commented and documented example of _______ with downloadable source/env?"
MachineLearning,3dt7lf,INCOMPLETE_USERNAM,2,Mon Jul 20 21:00:04 2015 UTC,"Hey, OP's a web developer. He's used to minifying his code."
MachineLearning,3dt7lf,mikef22,4,Sun Jul 19 17:22:06 2015 UTC,"""Converges in 4 iterations"".  Did anyone investigate further/read the article carefully?  I'm guessing that either the problem is really easy such that random initialisation can solve it alone, or there is a bug."
MachineLearning,3dt7lf,mikef22,1 point,Sun Jul 19 18:51:57 2015 UTC,"This is definitely because the problem is easy (you can solve it in your head without paper). There are MANY possible optimal solutions. Basically any Theta vector where the first element is a big positive number and the other 2 are small or negative will work. Given that the initial population is set at 100, there are almost certainly a couple pretty good solutions within the first generation that quickly get selected for in the subsequent generations.  I probably shouldn't have put an exclamation point when I wrote ""Converges in 4 generations!"" because it's really not surprising at all. I tried setting the initial population to 10 instead of 100 and running it again, it still converges after about 6 generations."
MachineLearning,3dt7lf,MusicIsLife1995,2,Sun Jul 19 19:44:04 2015 UTC,"So the genetic-algorithm aspects of evolving towards a superior solution, i.e. using crossover and mutation, have not been really tested in this experiment, right?  Did you implement crossover and mutation in your solution? If so, maybe you could make the problem slightly harder to test for the above?"
MachineLearning,3dt7lf,wil_dogg,1 point,Sun Jul 19 19:49:18 2015 UTC,"When I first wrote this, I didn't apply it to solving weights for that simple neural net. I simply made a hard-coded target vector, e.g. target = np.array([4.5, 10.1, -0.75]), so the GA has to evolve solutions that closely match that single array. Still not a difficult problem, but definitely harder than a problem with infinite right answers and it does end up utilizing the crossover and mutation (if the init pop is not too big)."
MachineLearning,3dwvxp,docClassifier,1 point,Mon Jul 20 05:11:29 2015 UTC,Is the page number always in the same place? If so just snip it and do digit classification.   PyStruct even has some examples like this.
MachineLearning,3dw6gw,john_philip,2,Mon Jul 20 01:11:35 2015 UTC,Please post arXiv links to the page containing the abstract.  That way I can choose to download the paper only if I want to read the whole thing.  Thanks!
MachineLearning,3dw6gw,ndronen,1 point,Mon Jul 20 02:50:34 2015 UTC,Well noted !
MachineLearning,3dw6gw,arXiv_landing_bot,2,Mon Jul 20 04:02:33 2015 UTC,arXiv landing page: http://arxiv.org/abs/1507.01589v1
MachineLearning,3dw6gw,rmlrn,1 point,Mon Jul 20 14:51:33 2015 UTC,this seems really unimpressive... their example is just distinguishing blue from yellow..
MachineLearning,3dtdq0,kjw0612,1 point,Sun Jul 19 07:11:24 2015 UTC,Suggest adding this and this for speech recognition section of the datasets.
MachineLearning,3dtdq0,r4and0muser9482,1 point,Sun Jul 19 18:46:12 2015 UTC,"Also this: ""Attention-Based Models for Speech Recognition""  Chorowski, Bahdanau, Serdyuk, Cho, Bengio  http://arxiv.org/abs/1506.07503"
MachineLearning,3dtdq0,kkastner,1 point,Sun Jul 19 23:24:24 2015 UTC,Added!
MachineLearning,3dtdq0,panickmonster,1 point,Mon Jul 20 03:11:11 2015 UTC,Thanks for your great suggestions! Added dataset and the paper
MachineLearning,3dtdq0,panickmonster,1 point,Mon Jul 20 03:00:57 2015 UTC,"Nice, I suggest breaking down ""Question Answering"" into visual and textual (and maybe multi-modal) sections. And then add a bunch more of the textual QA papers."
MachineLearning,3dtdq0,cryptocerous,1 point,Sun Jul 19 19:19:19 2015 UTC,"That seems like a good idea, since each part is getting a bit bulky. Currently our survey covers mainly those related to computer vision; will look more into textual QA papers! BTW, if anyone is using RNNs for other applications, we would really appreciate it if you PR on GitHub."
MachineLearning,3dtdq0,panickmonster,1 point,Mon Jul 20 07:12:17 2015 UTC,End-To-End Memory Networks Grid Long Short-Term Memory
MachineLearning,3dtdq0,knighton_,1 point,Mon Jul 20 01:02:35 2015 UTC,Added both
MachineLearning,3dsirg,feedthecreed,3,Sun Jul 19 01:06:54 2015 UTC,Very Hinton-esque. I would also like some more info.
MachineLearning,3dsirg,Zavidovici,3,Sun Jul 19 17:01:14 2015 UTC,Also interested. This was also posted on HN twice:  https://news.ycombinator.com/item?id=9432601 https://news.ycombinator.com/item?id=9427474
MachineLearning,3dsirg,simonhughes22,2,Sun Jul 19 19:24:55 2015 UTC,Asked Hinton if there was a video recorded of this presentation. His response:   Not that I know of  Geoff  Sent from my iPhone
MachineLearning,3dsirg,g4n0n,2,Mon Jul 20 22:02:37 2015 UTC,You should tell him to come visit /r/MachineLearning again to give an update!
MachineLearning,3dv5je,john_dumb_bear,3,Sun Jul 19 19:52:39 2015 UTC,"I think this is data exploration, or exploratory data analysis. You can visualize, compute statistics, like mean, min/max, variance, covariance matrix, most frequent items, run some clustering algorithms.  On the other hand, you probably have some general task that you want to solve: image understanding, text understanding, process optimization, whatever. You can then try to connect the tools with the purposes.  But it's very hard to talk in general, because 'data' is such an empty term. And what kind of ""questions"" are we dealing with here?  It seem a bit like asking how to come up with good hypotheses in science. It's really hard to formalize, because once we understand how to do something sufficiently well, we automate it. It's very much like this in computer science. Once we get a clear picture of some chore (for example version control), we write a program."
MachineLearning,3dv5je,bonoboTP,2,Sun Jul 19 20:13:17 2015 UTC,Read and understand as many examples of different algos being used in different application spaces as you can. You'll pick up some ideas from how other people have done things which may spark some of your own new ideas.
MachineLearning,3dv5je,dire_faol,1 point,Mon Jul 20 12:52:53 2015 UTC,you mean trying to predict something that isn't as obvious> ...??
MachineLearning,3dv5je,watersign,1 point,Mon Jul 20 00:52:12 2015 UTC,"Not trying to put you down, but that's akin to asking ""how do you get better at deciding what to program."" There are a huge array of ML approaches and all can be used to optimize your model for vastly different aspects of the data. The only sensible way to get anything worthwhile done is to start from some kind of requirement, i.e. a ""problem"" as the target for your eventual solution. In industry you get your requirements from your superiors. In research you have some freedom to choose but what you work on tends to have to conform to what your superiors are interested in. If you are a hobbyist, as you seem to be, just look for something you would like to know."
MachineLearning,3dv5je,skgoa,-2,Mon Jul 20 08:03:09 2015 UTC,Questions to ask AI? Huh? This seems like the question some middle manager would ask after reading about machine learning in a trade publication
MachineLearning,3dv5je,shaqed,1 point,Sun Jul 19 23:54:03 2015 UTC,Or OP has abstracted the process of modeling as asking the math a question. Not all problems have obvious formulations.
MachineLearning,3dvszl,fariax,3,Sun Jul 19 23:12:37 2015 UTC,"It should help if the time saved from doing operations in parallel on the GPU outweighs the disadvantages of the GPU: higher overhead and data transfer time.  GPUs are usually 20-30x faster than GPU when training a NN with minibatch sgd and >200 hidden units.    ""When I tried an MLP using Keras, not using a batch training, it took about 15s for training/test, while using Weka, it was about 1s.""  Is this 1s per iteration or 1s total?  If it's the later, then the majority of your time could be overhead (like compiling theano), which isn't that informative."
MachineLearning,3dvszl,alexmlamb,0,Sun Jul 19 23:31:20 2015 UTC,"1s total.  The time comparison was without the compile time, because I have builded the model before, than I measure only the train/test time...  This time on Keras was either using CPU or GPU... It was much worse than simple Weka. I'd expected that Weka should be faster, but not much faster like what I've got..."
MachineLearning,3dvszl,jrkirby,2,Sun Jul 19 23:41:59 2015 UTC,It also depends on you the GPU you're running on.
MachineLearning,3dqxlw,cryptoz,22,Sat Jul 18 16:28:50 2015 UTC,"I guess the code in the article is actually not correct, and is not really suitable as an introduction to machine learning in python. Well, in a sense, this article illustrates one basic principle of the machine learning: you can screw up quite a bit even in the very simple algorithms, like kNN.  See, in kNN.py there is a seemingly easy function:  def getNeighbors(username, k):     client = MongoClient()     distances = []     for user in allUsers(client):             if len(distances) > k:                     break             dist = vectorDistance(username, user['username'])             distances.append((user['username'], dist))     distances.sort(key=operator.itemgetter(1))     return distances   The fun thing about it? It's fundamentally incorrect: it retrieves a (mongodb-produced) ""random"" sample of k users and computes the distances. So, basically, the neighbors produced by this method are not nearest, they are random.  TL;DR: There is a principal error in the code. Don't trust random tutorials on the internet. Write tests for your code"
MachineLearning,3dqxlw,olBaa,7,Sat Jul 18 21:10:44 2015 UTC,"Good catch. 2 other issues I have with this article: (1) merely finding the k nearest neighbours is not actually the whole kNN algorithm (i.e. we're ignoring classification entirely here), and (2) using vector distance on a very sparse bit string doesn't seem like the most efficient method (but then neither does re-requesting the user and recreating the vector every time we want to measure distance...)"
MachineLearning,3dqxlw,kylotan,4,Sat Jul 18 21:55:29 2015 UTC,"Could/Would this work as well?   You create a website that gives users recommendations on what subreddits to follow Users have to log in with their reddit accounts, so you get all their current subreddits You use your article to recommend a list of subreddits they can subscribe to (you add a subscribe button next to it) Then you can see what subreddits they're actually interested in -- because they subscribed to it You have new data to work with"
MachineLearning,3dqxlw,jellevdv,1 point,Sun Jul 19 11:22:41 2015 UTC,"Yes, that's using user feedback. It's a big thing in information retrieval."
MachineLearning,3dqxlw,skgoa,2,Mon Jul 20 08:06:38 2015 UTC,Thought of the same thing a while back. Didn't I see somewhere you can download all 1tb worth of comments ever made on reddit? Why not use that
MachineLearning,3dqxlw,mamaBiskothu,1 point,Sat Jul 18 16:59:26 2015 UTC,"I've downloaded that data. It could theoretically be fed into Karpathy's char-rnn, But it needs a bit of sanitization and setup first before it'd be useful for that purpose...   If someone's written a reader for the data files, I'd love to hear about it. I may have a crack at it later myself. It would be cool to create a ""forum recommender"" based on scraped information just like the Reddit comment Archive."
MachineLearning,3dqxlw,devDorito,1 point,Sat Jul 18 17:09:05 2015 UTC,Where can I find this data?
MachineLearning,3dqxlw,UnderdogIS,2,Sat Jul 18 18:16:56 2015 UTC,"/r/datasets  It's ~1-2 weeks old, so you may have to scroll down a bit."
MachineLearning,3dqxlw,devDorito,4,Sat Jul 18 18:20:42 2015 UTC,https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/
MachineLearning,3dqxlw,kylotan,2,Sat Jul 18 21:40:02 2015 UTC,"You are no longer limited to the last 100 comments for a given user. Someone published a dataset of all public comments posted on reddit through May 2015. Poke around, it's easy to find. I disagree with your choice of distance function. Cosine similarity is much better suited to a problem like this. You should additionally consider preprocessing your data using something like tf-idf, otherwise people are just going to get clustered relative to which defaults they frequent the most. You're really interested in which esoteric subeddits they frequent the most."
MachineLearning,3dqxlw,shaggorama,3,Sun Jul 19 00:05:20 2015 UTC,"I disagree with your choice of distance function. Cosine similarity is much better suited to a problem like this.   Curious, could you elaborate on why?"
MachineLearning,3dqxlw,respeckKnuckles,7,Sun Jul 19 02:45:51 2015 UTC,"Euclidean distance takes the magnitude of the vector into account, cosine similarity does not, making it much more suitable for binary high dimensional sparse datasets. From stackoverflow:   One informal but rather intuitive way to think about this is to consider the 2 components of a vector: direction and magnitude. Direction is the ""preference"" / ""style"" / ""sentiment"" / ""latent variable"" of the vector, while the magnitude is how strong it is towards that direction. When classifying documents we'd like to categorize them by their overall sentiment, so we use the angular distance. Euclidean distance is susceptible to documents being clustered by their L2-norm (magnitude, in the 2 dimensional case) instead of direction. I.e. vectors with quite different directions would be clustered because their distances from origin are similar.   If we think about this in the context of search engine rankings, you may have one very long document and one very short document. If we're using euclidean distance, the long document will have a much larger magnitude, and as a result may match a lot of other long documents, even if they are more relevant to the short document. In short, cosine similarity prevents bias to vectors of similar sparsity."
MachineLearning,3dqxlw,0h_Lord,1 point,Sun Jul 19 12:52:48 2015 UTC,"This was an interesting post, BUT.  It looks like you used Knn regression (or something like it, see parent comment above) to match similar vectors, rather than actual Knn classification. Knn is a form of supervised learning and would have required outputs for each vector in the training set, unless I missed something :).  Have you tried using kmeans to group by unsupervised clustering instead? The result would be similar, where you select other users in the same assigned cluster and recommend subreddits from there. Here's an example of this ""Mirroring Your Twitter Persona with Intelligence""."
MachineLearning,3dqxlw,primaryobjects,1 point,Tue Jul 21 16:22:07 2015 UTC,"Author here. Thanks for all the constructive criticism. This was only my second blog post, so I'll keep all of this in mind in the future. I'm probably going to do another post on this, trying out some new methods, so I'll try all of your guys' suggestions.   I actually did make a version using Cosine Similarity, but I used the most basic one in the post (just because I wanted to keep explaining to a minimum, and it's a very easy extension to go from here to different distance metrics).  I did end up fixing the bug /u/olBaa identified (good catch!). It was  small change I'd made before posting to speed things up, but it was untested and ended up being wrong."
MachineLearning,3dqxlw,logicx24,1 point,Wed Jul 22 00:31:28 2015 UTC,Does anyone else think this is stretching the definition of machine learning a little? Obviously there is some crossover but this kind of basic ranking strikes me as more of a search engine / information retrieval problem.
MachineLearning,3dqxlw,0h_Lord,1 point,Sun Jul 19 12:58:04 2015 UTC,"This application is part of information retrieval, which itself is primarily a subfield of databases, but makes heavy use of machine learning, especially natural language processsing."
MachineLearning,3dqxlw,skgoa,0,Mon Jul 20 08:09:38 2015 UTC,KNN and clustering are typically taught in ML courses.
MachineLearning,3dqxlw,Articulated-rage,2,Sun Jul 19 19:43:26 2015 UTC,"So is calculating variance but I wouldn't call that ML. KNN is firstly a lazy learner, and secondly the concept of voting based on proximity is pretty wide ranging, and I would argue not specific to ML.   What this post is doing is essentially ranking with a vector space model (albeit with a different distance measure) and then voting on the top results. Here's the exact same problem in the Stanford Information Retrieval book.  EDIT: And in this case the algorithm isn't actually providing a ""classification"" as such- it ranks the users according to the query and then analyses them to give a recommendation. So even calling it KNN in the first place is a bit of a stretch."
MachineLearning,3dqxlw,0h_Lord,2,Sun Jul 19 22:17:53 2015 UTC,"That's a fair point.  While there is significant overlap in the methods IR and ML use, making it unfair claim ""this is IR therefore not ML"", I do agree that maybe something so primitive shouldn't be under any learning theory umbrella.  On the other hand, I have heard machine learning as statistical modeling that respects the constraints of software engineering, which I think this could fall into.   Tldr, quibbling over the boundary between what is and isn't seems mostly fruitless.  In any conceptual distinction, there is almost never hard boundaries."
MachineLearning,3dqxlw,Articulated-rage,1 point,Mon Jul 20 16:51:36 2015 UTC,"Yep I very much agree that there isn't a huge amount of point quibbling over boundaries between very related fields.   However, what I took issue with was the way the post asserted itself as an ""Introduction to Machine Learning"" and exclusively mentioned ML throughout while in fact it was more of an introduction to information retrieval (which, along with search engines, isn't mentioned at all) with a few ML ideas thrown in."
MachineLearning,3dqxlw,0h_Lord,2,Mon Jul 20 18:08:42 2015 UTC,"Ahhh ya, I agree.  I personally like the Duda-Hart introduction.  It's a bit out of fashion now, but their first chapter is basically signal detection theory and bayes optimal decision boundaries."
MachineLearning,3dv6uu,just_learning_,6,Sun Jul 19 20:04:16 2015 UTC,I'd try an svm with a quadratic kernel.  The decision boundaries it creates are conic sections which will be connected in the way you want.
MachineLearning,3dv6uu,kjearns,1 point,Sun Jul 19 21:37:38 2015 UTC,+1 I was going to suggest the same thing.
MachineLearning,3dv6uu,simonhughes22,1 point,Sun Jul 19 22:07:28 2015 UTC,Will it still be connected for higher dimensions?
MachineLearning,3dv6uu,kjearns,1 point,Sun Jul 19 22:17:53 2015 UTC,"Yes, this property holds in arbitrarily many dimensions.  Note that you are only guaranteed that one side of the decision boundary will be connected.  Quadratic kernels can create boundaries that are hyperbolas (and their higher dimensional analogues) where one side of the boundary is disconnected.  However, if your data doesn't look like a hyperbola then presumably it won't do this., and even if it does you are guaranteed that the ""other"" side is still connected."
MachineLearning,3dv6uu,kjearns,1 point,Sun Jul 19 23:41:00 2015 UTC,So the disconnected side would extended out infinitely. I think that will be ok for my situation.
MachineLearning,3dv6uu,siblbombs,1 point,Sun Jul 19 23:46:02 2015 UTC,"That is correct.  The hyperbola is the only case where connectedness can go ""wrong"", and if that happens then both regions extend off to infinity.  You will never find a region that is both disconnected and bounded with a quadratic kernel.  The intuition from 2d is very strong here, the only difference in higher dimensions is that the disconnected region can have more ""lobes""."
MachineLearning,3dv6uu,gsmafra,2,Sun Jul 19 23:55:48 2015 UTC,Did you mean to include a link?
MachineLearning,3dv6uu,gsmafra,1 point,Sun Jul 19 20:06:08 2015 UTC,Haha. I thought I did.  Thanks for that
MachineLearning,3dv6uu,gsmafra,2,Sun Jul 19 20:07:59 2015 UTC,"You could try transforming this in a classification problem: where you have a score of 10 you put 10 samples of class 1, when you have a score of -5 you put 5 samples of class 0, etc. Your 0-1 loss function will be exactly the same (the sum of the scores in that region). I'm assuming here your ""scores"" are integers"
MachineLearning,3dv6uu,tariban,1 point,Sun Jul 19 20:26:24 2015 UTC,"Interesting thought I will think about this. And, it could be extended to all real numbers by rounding to fixed precision."
MachineLearning,3dv6uu,kkastner,1 point,Sun Jul 19 20:59:02 2015 UTC,Or you could just do regression and take the region where the predicted value is superior to 0?
MachineLearning,3dv6uu,gsmafra,1 point,Sun Jul 19 21:04:08 2015 UTC,"That is one thing I was thinking of. However, due to the system I am modeling I know there should be one single region. Provided enough data this should occur naturally probably, however I wonder if I would need to connect regions."
MachineLearning,3dsz17,hey_chicago,1 point,Sun Jul 19 04:00:09 2015 UTC,I think Guyon is a good starting point. At the least it'll give you the vocabulary to catch up on the topic: http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf
MachineLearning,3ds1tp,ojaved,2,Sat Jul 18 22:22:51 2015 UTC,This is a perfect video for this sub. Good for systems people looking to contribute instead of compete with the power and achievements of big companies with servers and engineers. Hard to be on the cusp unless you're where you are needed.
MachineLearning,3drsyt,SmArtilect,5,Sat Jul 18 21:02:11 2015 UTC,"I think so. There are two pull requests open on Caffe, each of which provides OpenCL backend (instead of CUDA):  https://github.com/BVLC/caffe/pull/2195  https://github.com/BVLC/caffe/pull/2610  Both are moderately mature in that they build using cmake, pass all the tests and can run the Caffe-provided example models (e.g. MNIST, Cifar10, Alexnet). I think 2610 probably at this point in time might be the better choice for you since it doesn't necessarily require clBLAS; you can use viennaCL for the GEMM and BLAS routines. This is significant because as far as I know clBLAS (https://github.com/clMathLibraries/clBLAS) is not packaged for most Linuxes, and needs to be manually built and installed, though maybe that's not a problem for you. Whereas ViennaCL is packaged for Ubuntu (and probably other distros) and can be easily installed with sudo apt-get install libviennacl-dev (on Ubuntu). AFAIK that's the only dependency other than the usual Caffe dependencies. Also, the author of 2610 reports he has tested on Intel integrated GPU.   A few more details and caveats:  1) My only experience is running these on AMD GPUs; I haven't tried running on an Intel GPU. YMMV.  2) 2610 author reports the double precision tests fail but the float tests pass (on Intel GPU). Probably not an issue since float is the default, unless you absolutely need double precision for some reason.  3) In order to build 2610 in a way that doesn't require clBLAS, you need to set this option in CMakeLists.txt:  caffe_option(USE_CLBLAS ""Build Caffe with clBLAS support (instead of using ViennaClBLAS)"" OFF)  and will probably need these too; not sure if they are the defaults:  caffe_option(USE_GREENTEA ""Build Caffe with OpenCL support"" ON)  caffe_option(USE_CUDNN ""Build Caffe with cuDNN libary support"" OFF)  caffe_option(USE_CUDA ""Build Caffe with CUDA support"" OFF)  4) I don't know if you'll get much benefit from the GPU. HD4000 is weak sauce, and further constrained by DDR3 memory bandwidth which is lower than that of GDDR5 (or HBM) of a discrete GPU. If you try this, I suggest comparing execution on CPU and GPU and see if the GPU is actually even an improvement."
MachineLearning,3drsyt,jyegerlehner,2,Sun Jul 19 06:14:40 2015 UTC,This is  super-helpful! Thanks.
MachineLearning,3drsyt,georgeo,3,Sun Jul 19 17:26:01 2015 UTC,GPU is still useful for forward prop.    Why do you need to use your onboard GPU?  Use AWS GPU instances?
MachineLearning,3drsyt,alexmlamb,3,Sat Jul 18 23:46:39 2015 UTC,"it runs. its ok for development. for running, rent an aws instance."
MachineLearning,3drsyt,hughperkins,6,Sun Jul 19 04:46:28 2015 UTC,"Older Intel onboard GPUs still don't even support OpenCL. Edit: I've been a bit behind the times. As /u/wtallis points out below, the more recent Intel generations do support OpenCL.  If you have one of these older onboard GPUs, then, not easily, and it's probably not as worth the effort as you might hope.  However, depending on the drivers and OS you're using, they probably do at least support GLSL shaders and OpenGL. You actually can write general-purpose-GPU code in GLSL shaders, and that's how some of the first efforts at GPGPU were done. But it's not going to be as straightforward as CUDA or even OpenCL, and you won't be getting anywhere near the throughput you'd get on a dedicated GPU.   You also won't get the benefit of highly-optimized libraries like cuBLAS or cuDNN. However, if you're still rarin' to try this out, you might be interested in the GLM library, a header-only library for doing GPGPU math with GLSL (and in the absence of a more modern, focused GPGPU backend). In principle, you should be able to get this to work on your Intel card with just about as much pain and boilerplate as setting up any OpenGL application.   Also see:  http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter44.html  https://github.com/onitake/glam  both of which provide some insight into how you might do the math required for the convolutional network on a GPU without CUDA support. Of course, doing it this way would likely lead to a lot of compromises and imposed structure that might get in the way of your algorithms and architecture.... probably much more so than with CUDA or OpenCL."
MachineLearning,3drsyt,MethodOverDrive,7,Sat Jul 18 22:07:08 2015 UTC,"Intel onboard GPUs still don't even support OpenCL.    They have for the past two generations (Haswell and Ivy Bridge). Intel provides an OpenCL implementation for Windows but not for Linux. On Linux, OpenCL support is provided by beignet."
MachineLearning,3drsyt,wtallis,5,Sun Jul 19 00:16:17 2015 UTC,"I notice here it looks like Intel provides Linux OpenCL drivers for the integrated GPU:  https://software.intel.com/en-us/articles/opencl-drivers#lingpu  I don't know if it's their own driver, or perhaps they packaged up beignet."
MachineLearning,3drsyt,jyegerlehner,1 point,Sun Jul 19 06:45:12 2015 UTC,"Ah. Didn't even know. My Intel onboard GPU doesn't support OpenCL, heheh. Thanks for the info."
MachineLearning,3drsyt,MethodOverDrive,2,Mon Jul 20 23:40:21 2015 UTC,"Weird, I have a Sandy bridge laptop with intel openCL running on it.    Conclusion: I must be a wizard."
MachineLearning,3drsyt,horsey_jumpy,2,Sun Jul 19 03:00:30 2015 UTC,"You're running OpenCL on the CPU, not the GPU."
MachineLearning,3drsyt,wtallis,1 point,Sun Jul 19 12:17:27 2015 UTC,Pretty sure you're wrong.  Try googling Intel hd 3000 & openCL.
MachineLearning,3drsyt,horsey_jumpy,1 point,Sun Jul 19 21:59:23 2015 UTC,http://www.intel.com/support/graphics/sb/CS-033757.htm shows no OpenCL support prior to HD2500/4000 Ivy Bridge generation.
MachineLearning,3drsyt,wtallis,2,Sun Jul 19 22:11:46 2015 UTC,"I have never used it, but Intel released a library to be used on their devices. Check if it helps."
MachineLearning,3drsyt,muktabh,-1,Sun Jul 19 02:45:18 2015 UTC,No.
MachineLearning,3drsyt,Foxtr0t,-1,Sat Jul 18 22:02:29 2015 UTC,"No. As dumb as it sounds, your CPU actually has more raw FLOP's than the graphics card."
MachineLearning,3dqdqr,w0nk0,3,Sat Jul 18 12:54:15 2015 UTC,"Lasagne can do that, I believe, by allowing the user to pass Theano symbolic variables for the cell and hidden state; see the discussion buried somewhere in here: https://github.com/craffel/nntools/pull/27  Here is an example which uses that functionality: https://github.com/skaae/nntools/blob/pentree_recurrent/examples/pentree.py Note that the recurrent layers aren't merged yet, but they probably will be soon: https://github.com/Lasagne/Lasagne/pull/294"
MachineLearning,3dqdqr,abracaradbra,5,Sat Jul 18 16:14:52 2015 UTC,"Chainer can do what you call 'elegant', if my understanding is correct, feeding input data to an RNN and obtaining its output one-by-one. You can write a one-step feed-forward process as an intuitive python function. You can see the example code of RNNLM here: https://github.com/pfnet/chainer/blob/master/examples/ptb/train_ptb.py"
MachineLearning,3dqdqr,m000pan,2,Sun Jul 19 01:36:01 2015 UTC,"That might be pretty perfect for my job, thanks a lot!"
MachineLearning,3dqdqr,bge0,2,Sun Jul 19 17:44:41 2015 UTC,Just to clarify : you want something like RTRL?
MachineLearning,3dqdqr,hughperkins,1 point,Sat Jul 18 14:02:18 2015 UTC,"Since I'm only just getting back into ML, I'm not 100% sure how RTRL is defined. As I understand it, the focus with RTRL would be on constant learning as more input is processed, as opposed to training the network once with a corpus and then generating text.  Independently from if I understood your question correctly, for now I'd be fine with getting something like Karpathy's blog post done - which I'm sure is kind of a FAQ at the moment. Then I want to expand to generate other time series."
MachineLearning,3dqdqr,olBaa,2,Sat Jul 18 14:18:22 2015 UTC,"i think you need to state specifically and concretely what you mean by ""not elegant""."
MachineLearning,3dqdqr,jfsantos,1 point,Sat Jul 18 14:15:23 2015 UTC,"Good point. I may have missed a point of how keras implements LSTM, so it may be my inexperience with modern NNs - but this is what I faced:  In PyBrain, I could feed characters into the net one-by-one and have the LSTM predict the next one. If I did the same in keras, it would never converge.    Then I found the following comment by the keras creator:   The recurrent layers in Keras have to process every sample from its first time step to the last. These layers are stateless (memory is cleared after every sample).   ..which explained why my approach wouldn't converge. Every character I fed to the net was a batch, and the net would reset activation after each batch.  From my understanding that invalidates the idea of a RNN for my specific approach, since the longer term ""memory"" of the net isn't active - unless I feed everything back into the network for each generated character.  And feeding everything back in like that struck me as very unelegant (and also, really computationally inefficient).  Side note: If I could get my win7 64bits to cooperate with GPU-based Theano, I might have just gone with it anyway, but alas, I can't get it to work yet."
MachineLearning,3dqdqr,jfsantos,3,Sat Jul 18 15:33:02 2015 UTC,"Basically, char-rnn is doing the 'sliding window' also, look at the code.  The 'long-term' memory is not actually that longterm-ish in a human sense, and is far from how the long-term memory in our brains work (though I guess it is an open question)."
MachineLearning,3dqdqr,jfsantos,1 point,Sat Jul 18 19:58:55 2015 UTC,"Yeah, long-term is clearly a very relative expression here - mcahine learning isn't there just yet! ;)  I think I might have actually already done mini-batches in my code just like those. I guess I have put too much time into this already! ;)"
MachineLearning,3dqdqr,siblbombs,2,Sat Jul 18 20:07:27 2015 UTC,"This is really an issue with Keras. What you can do is pass a ""sliding window"" of an arbitrary fixed size through your text to generate an input matrix, and then give the first character after the window as the target. This way you are limiting for how long the network ""remembers"" the input, though, but it also enables mini-batch training to make things faster (as for mini-batch, all inputs need to have the same number of timesteps)."
MachineLearning,3dqdqr,siblbombs,1 point,Sat Jul 18 18:49:34 2015 UTC,"That's very close to what I have done actually, but I really don't like how it limits the scope; it still defeats the purpose of using a RNN for this use case in my opinion. Even though PyBrain uses no theano or optimization whatsoever, it seems to not be much slower in getting results because of that limitation. I'm sure there is a better way, but probably just not in Keras.  I'm not sure how mini-batch training works, and I don't think I have seen it referred to in the docs. How does it work? I'd love to at least try it since I have all the sliding window stuff set up already anyway!"
MachineLearning,3dqdqr,Kogni,3,Sat Jul 18 19:13:08 2015 UTC,"All optimizers do mini-batch training when you pass a batch_size parameter to model.fit. However, you cannot use sequences with different lengths, which is why in the RNN examples they either pad sequences with zeros or window them to a fixed size."
MachineLearning,3dqdqr,Kogni,1 point,Sat Jul 18 20:23:11 2015 UTC,"I see - that's what I've been doing (basically because that's what everyone else seemed to be doing as well). Could the constraint of non-variable batch lengths cause the execution to just exit after emitting ""Epoch 0"" and before fitting the model? Because I've been pulling my hair out over that behaviour quite a bit when I started using keras, and didn't find anything about it in the docs."
MachineLearning,3dqdqr,simonhughes22,1 point,Sat Jul 18 21:21:28 2015 UTC,"I think in that case you might get a weird Theano error message regarding mismatched dimensions in some operation, as I'm not sure if Keras will check the timesteps dimension before starting training."
MachineLearning,3dqdqr,elanmart,2,Sun Jul 19 15:27:21 2015 UTC,"Hmm, then the Epoch0-mystery remains unsolved for now ;). Thanks anyway!"
MachineLearning,3dqdqr,votadini_,2,Sun Jul 19 17:35:51 2015 UTC,"So I've been doing some rnn sequence generation lately, and I've gravitated towards blocks because of the way they handle creating recurrent functions. I know blocks has a generator construct which takes a recurrent transition and some other stuff and produces generated output, but I haven't used it much. Usually what I do is have a function that takes an input and just produces the output for training, then have a second pretty much identical function that just feeds the step prediction back in as the next input."
MachineLearning,3dqdqr,r4and0muser9482,2,Sat Jul 18 14:38:54 2015 UTC,"Thank you! This is the sort of response I was hoping for. I have a bunch of code I can probably reuse, so hopefully the transition to blocks might be not that hard.  When I looked at blocks it seemed very sparsely documented. Was that a problem? How much of a Python pro are you? I'm decent at Python (wrote a couple 1k+ lines of code apps in in), but not a programmer by trade, so understanding code sometimes takes me a minute."
MachineLearning,3dqdqr,rinuboney,2,Sat Jul 18 15:37:30 2015 UTC,"I'd consider myself competent in python, its the only language I do any work in. I learned python before I started doing ML, so the theano ecosystem made sense when I started to get into it.  Blocks is still pretty young, so there's not a ton of stuff out there for it, but the main page has some of the basic stuff and there are also some examples that you can look at. I don't use everything in blocks (mostly ignore the dataset/mainloop stuff), but the @recurrent decorator is a very nice way to make recurrent functions so I've started using that heavily."
MachineLearning,3dqdqr,singularai,2,Sat Jul 18 16:49:43 2015 UTC,"This is the exact situation i also encountered. With the exception that i just went back to the ""Pybrain Stage"" when i found Keras not working.   Very interested in the answers, thanks for making the thread."
MachineLearning,3dravh,Aerospacio,3,Sat Jul 18 18:25:05 2015 UTC,"Mixing Reinforcement learning with global approximators (such as neural networks) can easily lead to convergence problems [Sutton web page]. In fact, the algorithm fitted Q iterantion has no convergence guarantees when is combined with a neural network [Ernst2005], which doesn't mean that divergence is ensured, obviously.  In practice, usually it is required a depth knowledge of the problem. For example, in the Riedmiller's paper, the author uses the hint-to-goal-heuristic and incrementally add transitions to the experience set. Both tricks are necessary to achive the convergence in his experiments. More advices and tricks can be found here."
MachineLearning,3dravh,pabloesm,1 point,Sun Jul 19 11:25:35 2015 UTC,Thank you so much! Ill read it and do some experiences and post some results!
MachineLearning,3dravh,pabloesm,1 point,Sun Jul 19 16:40:39 2015 UTC,"Welcome, good luck!"
MachineLearning,3dn9b4,john_philip,0,Fri Jul 17 17:28:32 2015 UTC,Holy scroll-jacking Batman!
MachineLearning,3dpi97,deep_rabbit,3,Sat Jul 18 04:58:38 2015 UTC,Modifications to line 30 and 31 here: https://github.com/karpathy/char-rnn/blob/master/model/LSTM.lua  will do what you want.   I split that linear unit into a bunch of separate smaller linear units which worked well for me.
MachineLearning,3dpi97,londons_explorer,3,Sat Jul 18 12:16:06 2015 UTC,"Also on top of this and the hashing trick, just make sure to fix the seed to ensure same model is used every time.  I also think making dropout greater than zero will solve your problem. Look at line 56 in the LSTM model file. It applies dropout in between layers of a LSTM (i.e from a hidden unit in layer1 to input of layer2). You can separate this dropout value from the one used between subsequent time steps.  Also best place for learning torch, https://github.com/oxford-cs-ml-2015/. Step by step practicals from Nando De Freitas's group at Oxford."
MachineLearning,3dpi97,dexter89_kp,1 point,Sat Jul 18 17:28:47 2015 UTC,Thanks for this -- I've gone through those Oxford practicals and you're right -- they're the best introduction to Torch I've been able to find. Particularly Practical 5 really helped me to understand how complex net architectures are assembled. Much appreciated.
MachineLearning,3dpi97,londons_explorer,1 point,Sun Jul 19 19:49:51 2015 UTC,"I split that linear unit into a bunch of separate smaller linear units which worked well for me.   Thanks for pointing me to this spot in the code. Could I ask you to elaborate a bit on the architecture you used? Did you split each layer ""horizontally"" into partitions that are densely connected only within their partition? Are the partitions the same between layers, or do you use an offset so that the different partitions can communicate between layers? Would you mind sharing those lines of code, and your high-level impression of how it affected the results, in terms of scalability and improvements in loss? (Don't worry about it if it's a hassle, just thought I'd ask in case you happen to have it handy.)"
MachineLearning,3dpi97,NasenSpray,1 point,Sun Jul 19 19:54:32 2015 UTC,"Summary:  I split each layer into 30 partitions.  Each partition can get inputs from it's partition and the immediately neighbouring partitions.  Loss degraded a little over fully connected models of the same size as expected.  Memory use should have gone down considerably (ie. a factor of 10) but in fact it didn't go down much :-(.   This was partly because the input (the case where L==1) and the output (the Linear() on line 57) were still fully connected due to implementation trickiness.  I also suspect there is some other memory-eating factor, like the fact that my 30 layers make small memory allocations for weight storage which has a lower packing efficiency due to Nvidia's allocator.  Performance was also sub-par, I suspect because there were more GPU round trips because each section is now processed separately and hence has a kernel startup overhead.  In summary, it ""kinda worked"", but wasn't an obvious win, and in reality you want to find a layer type better than nn.Linear()."
MachineLearning,3dpi97,spurious_recollectio,1 point,Sun Jul 19 20:46:22 2015 UTC,Thanks. Sounds like this problem isn't as easy as I'd hoped. I'm not sure the SparseLinear module is right for the task either -- seems that is a standard linear module with a sparse input format. I would have thought that sparsely connected layers would be something the package would support out of the box.  Hmmmm...
MachineLearning,3dpi97,NasenSpray,2,Mon Jul 20 05:50:15 2015 UTC,It might be easier to use something like the hashing trick.
MachineLearning,3dpz0k,InaneMembrane,1 point,Sat Jul 18 08:50:46 2015 UTC,"Awesome, it even has some clojure code in the appendix."
MachineLearning,3dpz0k,martinBrown1984,1 point,Sat Jul 18 15:42:40 2015 UTC,"Anglican*, which is built around Clojure."
MachineLearning,3dnolr,hey_chicago,26,Fri Jul 17 19:21:56 2015 UTC,"The general sentiment among practitioners (even those of us with expertise in deep learning) is that you should use a model that is as complex as necessary to adequately solve the problem (i.e., subscribe to Occam's razor). Highly overparameterized neural networks are kicking ass and taking names at the kinds of AI-complete problems that were traditionally the domains that require massively engineered pipelines that involved some learning, and where the weakness of the feature extractor/learner combo at extracting relevant information was apparently a performance bottleneck. Lots of problems do not have that flavour; many, many problems have low sample size, very noisy observations (sometimes with very little signal in the data to begin with), and are perfectly well-tackled by more traditional methods, where a lower capacity learner will tend to help reduce overfitting.  There is also a question of the relative cost of investment in terms of time and expertise. Methods like SVMs and stump/tree ensembles are very easily deployed even by relative machine learning novices and can usually get you reasonably good results without thinking too hard about the machine learning side of things; you can effectively treat them as a black box. If you need excellent results, or you have other constraints such as the amount of computation or memory that it takes to evaluate your model at test time, then it's time to hire a machine learning expert, who may choose something like a neural network (but also might not).  In some domains, interpretability is quite important. Sparse linear models are very interpretable, and are pretty much the gold standard in fields like bioinformatics. Decision trees are very interpretable. Things like boosted stump ensembles are still somewhat interpretable. Despite the community's efforts, neural networks remain rather difficult to interpret, especially for input domains that don't consist of images. This makes many domain experts hesitant to adopt them, particularly in domains like biology where mechanistic explanations for the input/output mapping are ultimately the goal."
MachineLearning,3dnolr,dwf,17,Fri Jul 17 21:34:16 2015 UTC,"Need a large dataset Because you need a large dataset, training time is usually significant The scale of a net's weights (and of the weight updates) is very important for performance.  When the features are of the same type (pixels, word counts, etc), this is not a problem.  However, when the features are heterogeneous--like in many Kaggle datasets--your weights and updates will all be on different scales (so you need to standardize your inputs in some way). Parameters are hard to interpret--although there is progress being made. Hyperparamter tuning is non-trivial   Of course, many other algorithms suffer from these problems as well, but I think when weighing whether or not you should use a NN, these are the things you should first consider."
MachineLearning,3dnolr,goblin_got_game,1 point,Fri Jul 17 19:58:56 2015 UTC,"Thank you for the reply.  Just to clarify or expand a little more then -  is there any rough guidance on what a large dataset means when it comes to neural nets?  Or is it more of a gut feel/try and see?  I understand the training time component as a drawback.  However, if you want something to be accurate you would likely be willing to sacrifice a day or whatever it might be while the model runs/trains.  Your 3rd bullet - you are saying that neural nets are slower when their are mixed datatypes?  Such as a mix between numbers and categorical?  I guess this makes sense to me but is this really a meaningful influencer when it comes to running time?  Unaware of this.  I will read through the paper you sent over the next day or two thank you.  But can't the parameters be tuned over however much time you have?    Especially when you have plenty of time to complete tasks (Kaggle competitions with 60-90 days), why aren't neural networks even more prevalent?  I imagine after running through multiple iterations you would at least converge on somewhat good parameters that would be better than random forests or GBM?  So in theory aside from perhaps finding the right number of hidden layers or learning rate or what not, and the training time - neural networks are better in most cases?"
MachineLearning,3dnolr,Refefer,7,Fri Jul 17 20:28:17 2015 UTC,"It really depends on the modality.  In the case for us, we usually start at around 10k per label for classification tasks and as many millions as we can cram for reconstructions.  A couple of other points I'd like to highlight: 1. Knowledge base.  Neural networks are still alien to many if not most data scientists - it takes time to ramp up on them and get a feel for them.   Inference time speed.  For non trivial problems, you generally need a very large network which can be extraordinarily time intensive to evaluate at inference time.  This makes them good for papers but expensive for production uses. Tooling.  At our company, we've ended up rolling many of our own tools because the open source solutions don't cut it out of the box.  In terms of rubber to the road, it's much easier to find COTS using traditional methods than those using new methods. Need.  A lot of problems don't need the fire power of a deep neural network.  If there's one thing I've learned from working heavily with neural networks is just how damned good some of the 'classical' methods are.  I'm still in awe of SVMs. Open questions.  We have a good understanding of how to model multiple modalities with traditional tools.  That's still a heavily active field of research in deep learning land."
MachineLearning,3dnolr,dwf,2,Fri Jul 17 21:24:26 2015 UTC,"Inference time speed. For non trivial problems, you generally need a very large network which can be extraordinarily time intensive to evaluate at inference time.    This can be a very significant issue with a lot of more ""traditional"" methods too. Large ensembles are often expensive to make predictions with, for example, and depending how many support vectors you need to keep around, SVMs can be no fun in that regard either."
MachineLearning,3dnolr,elsatan666,1 point,Fri Jul 17 21:38:12 2015 UTC,"This reply really captures concisely the feeling of non-academia, and should really be a 'sticky post' in this sub to highlight the caveats of deep nn. Good post lad!"
MachineLearning,3dnolr,skgoa,1 point,Sat Jul 18 01:30:53 2015 UTC,"Many people in academia feel the same. CNNs are great for beating certain competition scores, and there is a lot of hype right now, but for a lot of people they aren't all that great for their particular are of interest. That includes many subfields of computer vision that aren't image classification."
MachineLearning,3dnolr,pilooch,1 point,Mon Jul 20 08:21:21 2015 UTC,Let me add that highly unbalanced datasets in supervised settings are not handled well without some rebalancing.
MachineLearning,3dnolr,radikal_noise,4,Sat Jul 18 08:45:55 2015 UTC,"One issue is time to market. Fire up XGboost, throw into hyperopt with your wrapping fitness function, come back with something not awful.  I've basically never been able to get a working NN solution for some of the small problems I deal with just because there isn't time to dedicate and while in theory I guess less prep work is needed to ""throw it at a deep net"", I've never had this work out yet, though libraries like Keras etc are certainly closing the gap.   Speed of prediction is a bit of a bottleneck. This has, I KNOW, solutions. Hashing nets. Using larger net to train smaller net. But even doing these things, it's hard to hit the single digit microsecond per prediction target, and well, these things again are nontrivial to do. (Whereas you can do 100 GBTs in 500 nanos)"
MachineLearning,3dnolr,iidealized,4,Fri Jul 17 23:29:46 2015 UTC,"Your title (and many of the posts here) implies deep learning is implicitly considered the best model.  Can anyone here provide even one example where standard fully-connected feed-forward net is the best method on any classical  tabular data (i.e. where the different features actually have meaning as different real-world variables as opposed to say pixels in an image)?  I personally have never seen this myself (but have also not tried on too many datasets); I feel like all successful DL applications (for fixed-size data, i.e. not sequences used for RNNs) are of ConvNets, a reflection of the fact that they are basically the only model with built-in convolution filters which are clearly necessary for things like speech/image/video data.  However, this provides little evidence why a standard NN should be considered ""superior"" in constrast to methods like Adaboost or Random Forest, which have been found to produce stellar results across a huge diversity of datasets."
MachineLearning,3dnolr,y05f,0,Sat Jul 18 19:05:50 2015 UTC,"Have you ever heard of ""No free lunch theorem""?"
MachineLearning,3dnolr,iidealized,2,Sun Jul 19 11:38:30 2015 UTC,"Of course, the entire point of my above post is to argue against the widely held misconception that ""deep learning"" is the best."
MachineLearning,3dnolr,mljoe,3,Sun Jul 19 19:53:16 2015 UTC,"I view deep learning as a method of last resort, a technique to use when other machine learning models fail. It's still an area of extremely active research, especially if you stray too far from object detection. So the tooling is not quite mature. One exception is object detection/classification. There I'd try neural networks (convnets) first."
MachineLearning,3dnolr,padelas14,1 point,Sat Jul 18 04:58:11 2015 UTC,"I am no expert in aNNs but my intuition is that they are too complex and this is usually not good.  There is overfitting as you say which I don't think you can totally avoid it by limiting training iterations without also missing existing relations in the data. The is also the problem of local minima with the training algorithms, which means that even though the aNN can model any function the training algorithm may not manage to fit it from the data.  I am studying ML in time series and I have experimented with 3 different kinds of datasets. The neural network has outperformed the other algorithms only in one. The particular characteristics of that dataset is that there is a very clear and strong pattern. In the other datasets that are more noisy the aNN performs really poorly (even worse than simple linear regression in one)."
MachineLearning,3dnolr,watersign,0,Sat Jul 18 20:14:00 2015 UTC,takes lots of time to train and stuff
MachineLearning,3dnolr,y05f,-1,Sat Jul 18 01:28:02 2015 UTC,"Neural Networks are used to approach problems that cannot be solved with statistical methods such us Big Data problems and complex problems like pattern and speech recognition.  In fact, neural networks are the most popular these years in image processing with ConvNets and other Deep architectures. Same thing for recurrent networks in speech recognition field.  Taking in count that till now the ANN community has not fully understand yet the process of how human brains work to model it, which means we had just started!!!"
MachineLearning,3dnolr,__AndrewB__,-5,Sun Jul 19 02:56:55 2015 UTC,"Neural Nets are simply not powerfull enough to replace other methods. They're fine to use in an ensamble, but in real-world competitions they will in general perform very similarly to other models (like gradient boosted trees etc.).  And most definatelly neural nets cannot beat ensembles of models, unless one's dealing with vision I guess."
MachineLearning,3dlon1,ornstein-uhlenbeck,20,Fri Jul 17 07:38:24 2015 UTC,Look into topological statistics.  It's brand new (like only around since around 2009) and has a ton of potential.  Think algebraic topology applied to unsupervised learning.
MachineLearning,3dlon1,beaverteeth92,1 point,Fri Jul 17 10:37:48 2015 UTC,"Thanks for the response and sorry for the slow reply. This looks really interesting, I'll be digging into it in a lot more detail to try decipher a topic. Thanks again."
MachineLearning,3dlon1,beaverteeth92,1 point,Sat Jul 18 15:58:05 2015 UTC,"This is extremely advanced in the context you provided it in. It's extremely worth it but it's basically analyzing the relevance of neurons in the case of machine learning and that is something I tried to theorize/design on my own the other month before coming across this emerging field and realizing I needed to read up.  If you want a challenge both computationally and otherwise, do topological statistics. Whole other weighting mechanism for nodes and it really does just add another dimension like you would think. I cannot wait to see what this part of the field brings to our understanding of relationships most of all, i.e. recursive relevance instead of just normalization.  https://en.wikipedia.org/wiki/Minimum_spanning_tree  This is a fantastic way to ground yourself in what the goal is as well, from a machine learning standpoint."
MachineLearning,3dlon1,Rbrogrammer,1 point,Fri Jul 17 21:07:08 2015 UTC,"It's also really good for unsupervised learning.  You can do clustering based on how many iterations it takes for a topological ""hole"" to disappear.  If the hole has a long ""lifespan"", then there's more underlying structure and that clustering pattern is better."
MachineLearning,3dlon1,spongebob,1 point,Fri Jul 17 22:08:05 2015 UTC,"That's awesome. Unsupervised learning is obviously much more difficult to get a practical grasp on for most people but stuff like that sounds incredibly obvious to me and so I hope it is for others who want to take this on. RNNs are a mess in comparison to the way this works once it's understood and a person can think in 3D all the time, mathematically.   Finally seeing data in a 3-D web will finally ""show"" us what we're missing like in your example. It could be the beginning to understanding neurological plasticity too."
MachineLearning,3dlon1,alexmlamb,6,Fri Jul 17 22:13:06 2015 UTC,"Non-parametric time series change point detection in parallel and in real time. Basically the question is ""how can I identify a regime change in a (large, potentially multivariate) time series as soon after it happens as possible without calling a bunch of false positives?""  With the dawn of streaming databases (microbatching) a good solution to this problem would be both useful and feasible to impliment.   Also, there have recently been some cool data sets released that could be used for testing/ benchmarking. The Yahoo! set comes to mind immediately, but I swear I've seen a couple others on /r/datasets recently."
MachineLearning,3dlon1,piesdesparramaos,1 point,Fri Jul 17 11:20:16 2015 UTC,"Non-parametric time series change point detection in parallel and in real time.   Out of interest, what techniques would you suggest are appropriate for achieving this?  I'm using hidden Markov models at the moment but I'm not sure if there are other avenues I should be considering. I'm looking at multivariate physiological data from an intensive care  unit."
MachineLearning,3dlon1,iidealized,1 point,Fri Jul 17 18:00:48 2015 UTC,"Hi Rbrogrammer, this sounds really cool and is more closely related to my MSc research topic than other suggestions; my research was on calibration of stochastic process models for which I looked into various algorithms and even neural networks. I'll dig deeper into this and see what opportunities there are. Thanks!"
MachineLearning,3dlon1,anon1253,7,Sat Jul 18 15:59:52 2015 UTC,"Maybe a natural starting point would be looking at the papers from a conference called COLT, which has more theoretical ML papers.    You could also consider looking at an area like deep learning which has a few simple yet influential theorems (universal approximation, n-bit parity), but doesn't attract many CS theory experts.  In terms of theory, I think that it would be interesting to have more work on universal approximation theorems for different types of deep, narrow networks."
MachineLearning,3dlon1,serge_cell,2,Fri Jul 17 10:23:43 2015 UTC,"IMHO an interesting area that requires a good amount of math is optimization. In deep learning all of us are using algorithms such as Adagrad, rmsprop, etc. to optimize our models, but I think most of us do not fully grasp why/how these are working, and they certainly play a key role in Neural Networks. Example of this kind of research: http://arxiv.org/abs/1412.6980 . Also recent and related with this topic: http://arxiv.org/abs/1507.00210"
MachineLearning,3dlon1,ThePurpleRhinoceros,4,Sat Jul 18 22:55:23 2015 UTC,"You can really do anything with that background (which is where I come from as well), since the line between CS and Stat is an artificial construct these days.  My main interests are in developing new methods (particularly for non-standard types of data) and establishing basic theoretical properties (e.g. consistency/concentration, sparsistency, minimax, etc.) where possible.  You could even jump into the systems/databases side of things if you desire, just look at the direction of Mike Jordan's research in recent years."
MachineLearning,3djot1,rhiever,12,Thu Jul 16 20:35:41 2015 UTC,AI so hot right now. Nature had a similar special insert a few weeks back.
MachineLearning,3djot1,PM_ME_YOUR_INVENTION,6,Fri Jul 17 00:21:14 2015 UTC,"""That Hansel is so hot right now"" -Mugatu"
MachineLearning,3djot1,Tang1000000,5,Fri Jul 17 04:54:01 2015 UTC,exactly. my comment was meant to be read in that voice.
MachineLearning,3djot1,PM_ME_YOUR_INVENTION,1 point,Fri Jul 17 13:25:54 2015 UTC,How do you read the articles? Asks for login passwd.
MachineLearning,3djot1,classicalhumanbeing,1 point,Sat Jul 18 23:34:48 2015 UTC,Access to a internet connection from the university network should do the trick.
MachineLearning,3djot1,ComplexIt,2,Sun Jul 19 21:18:15 2015 UTC,Ok thanks for the heads up
MachineLearning,3djot1,classicalhumanbeing,1 point,Sun Jul 19 21:39:00 2015 UTC,Does anyone have a pdf version to share?
MachineLearning,3djot1,userdei,-8,Mon Jul 20 09:34:32 2015 UTC,"Personal Opinion: If Moore's Law continues to hold true - that the transistor size continues to fall - then Machine Learning will continue to grow at an astounding pace eating up other disciplines along the way. But this is contingent on Moore's law, and the field will plateau rapidly if it falters."
MachineLearning,3djot1,jostmey,8,Fri Jul 17 00:05:07 2015 UTC,"Moore's Law is holding in a literal sense, but transistor connectivity and other limitations mean that actual growth in processing power per dollar has been subexponential for many years now. Moore's Law already has long been expired in practice."
MachineLearning,3djot1,ascendingPig,1 point,Fri Jul 17 04:35:59 2015 UTC,"Really? Processing power per dollar has slowed down? Everything I'm seeing is saying otherwise:  http://aiimpacts.org/trends-in-the-cost-of-computing/  And then consider that not only cost per gate is coming down, but new instructions are constantly being implemented in accelerated ASIC for common useful tasks, with plenty of room left to grow for that, and I'd say we still have tons of breathing room left."
MachineLearning,3djot1,cryptocerous,3,Sat Jul 18 20:55:18 2015 UTC,"I'd say there's currently no definite relationship visible. Even If you take RNNs that require large computational resources, our current approach to train them is very brute force like. There may be other training methods that we haven't discovered yet that can significantly reduce the weight search problem. This will make RNNs intelligence not depend so much on the Moore's law."
MachineLearning,3djot1,Hexorg,3,Fri Jul 17 00:51:39 2015 UTC,"Moore's law failed in 2013. The international roadmap for semiconductors now predicts a doubling every 36 months, and Intel and co seem to be having trouble keeping up with even that."
MachineLearning,3djot1,bluecoffee,1 point,Fri Jul 17 06:47:21 2015 UTC,"I think he's using ""Moore's Law"" as a metonym for general improvements in affordability and power efficiency of computation. The progress of parallel computers certainly hasn't slowed."
MachineLearning,3djot1,VelveteenAmbush,6,Fri Jul 17 18:53:07 2015 UTC,"Eating up which other disciplines? From my vantage point as a statistician who does some consulting and collaborates with researchers in other fields, lack of computational power is not what limits them, nor has that been the case for probably decades if ever. Collecting the right data, designing experiments, and dealing with selection bias is hard enough in much of the social sciences, and the ability to fit deep neural networks is not going to change that."
MachineLearning,3djot1,normee,1 point,Fri Jul 17 04:17:02 2015 UTC,he's talking about general ai being able to be better than humans at said other disciplines I suppose
MachineLearning,3djot1,Make3,7,Fri Jul 17 19:09:51 2015 UTC,"Sheer processing power is not enough. We don't have all the right algorithms yet.  By the way, why say ""personal opinion""? If you can back up your ideas with reasons then why the disclaimer?"
MachineLearning,3djot1,bonoboTP,2,Fri Jul 17 01:45:36 2015 UTC,"Sheer processing power is not enough. We don't have all the right algorithms yet.   I don't think that's accurate. Today's computers can approximate AIXI, just not very well. The approximation is currently not good enough to create general intelligence. At some threshold of computing power, however, that approximation is guaranteed to start acting like a real AGI. The question is what that threshold is. And the answer is, we don't know.  It may be that we'll find other architectures that look nothing like AIXI from which general intelligence emerges at a lower threshold of computing power than an architecture modeled on AIXI. If that is true, then the first general intelligence probably won't be AIXI. But AIXI will get us there if nothing else does."
MachineLearning,3djot1,VelveteenAmbush,1 point,Fri Jul 17 18:48:39 2015 UTC,"Well, I'm skeptical about the AIXI stuff. It's basically just a prior based on algorithmic complexity (relative to a certain universal Turing machine), and it is uncomputable. You just shift the question to how to pick the Turing machine. And also, the world may not work according to a short computer program. There is no reason to think that the universe is a computer, or at least it seems like a strong assumption."
MachineLearning,3djot1,bonoboTP,1 point,Fri Jul 17 22:01:15 2015 UTC,"and it is uncomputable   Thus the use of ""approximate"" in my post. AIXI-tl (an AIXI approximation that has actually been implemented) is not uncomputable.   You just shift the question to how to pick the Turing machine.   No you don't; a Turing machine is not one of the inputs to AIXI.   And also, the world may not work according to a short computer program.   It doesn't need to; all that is necessary is that Occam's razor is broadly accurate, which is more or less the same assumption that underlies the scientific method. Rejecting that assumption would be extremely radical; it would basically mean that there is no rhyme or reason to the universe whatsoever."
MachineLearning,3djot1,VelveteenAmbush,1 point,Fri Jul 17 22:18:59 2015 UTC,"See the Wikipedia page for AIXI saying ""where U denotes a monotone universal Turing machine"" under the main formula. AIXI is based on a particular Turing machine. I'm aware that there are bounds for changing Turing machines, but still, the complexities can be very different once you choose exotic (or maybe even 'pathological') Turing machines.  Algorithmic complexity is just one way of formalizing Occam's razor, not the only one. Rejecting algorithmic complexity as the prior for our universe doesn't mean rejecting the scientific method."
MachineLearning,3djot1,bonoboTP,1 point,Sat Jul 18 00:19:59 2015 UTC,"So your theory is that AIXI may not work on a purely theoretical level because there's a true basic prior to the universe that is consistent with Occam's razor but inconsistent with Kolmogorov complexity (given any plausibly chosen universal turing machine), and that the difference between the true basic prior and the chosen variant of Kolmogorov complexity would be sufficient to render AIXI incapable of general intelligence, and that even though there's only a constant additive difference in Kolmogorov complexity between any two universal turing machines, universal turing machines that are close enough to the true UTM to overcome the additive difference even given arbitrary advances in computing power will be so rare in the space of possible universal turing machines that we'll be unable to find them? I mean, I guess it's formally rigorous, in the same way as Penrose's theory that consciousness uncomputably squirts out of quantum microtubules in our neurons -- can't disprove it, but only in the sense that you can't disprove leprechauns either."
MachineLearning,3djot1,VelveteenAmbush,0,Sat Jul 18 01:54:48 2015 UTC,"It's not all about sheer processing power.  Memristors is where it's at. Those devices have local memory and processing power in the same device. With standard technologies, we are limited by the transaction speed of non-local data (memory on a bus, away from the CPU), along with the energy requirements and heat that produces.  And 2 memristors is already a single neuron that can make decisions and do processing... ?!"
MachineLearning,3djot1,clow_reed,2,Fri Jul 17 14:49:54 2015 UTC,"That's still ""just"" making things faster. You can't ever brute force intelligence. We still haven't found the absolute right approach. But it's true that faster computers enable you to experiment with more ideas in less time, so it can contribute to more insight into the algorithms.  I think there is some similarity with cracking passwords. If you know a weakness in the cryptography, then you can have huge success, but just brute forcing it will take too long.  Having great hardware is great, but you also need great software that you can run on it and software improves a lot less quickly, steadily and predictably than hardware.  TL;DR We need human insight, not just faster computers."
MachineLearning,3djot1,bonoboTP,-2,Fri Jul 17 16:19:02 2015 UTC,"That's part of what I'm saying. The full gist is that I do not think the Von Neumann architecture is going to be adequate for working with learning algorithms and statistical computing.  Memristor tech makes computing and storage local. Therefore any costs of sending/receiving data energy wise is 0. Right now, the cost is non-negligable. If we are to emulate neurons in the brain, it would take an estimated 100GW using our current tech... yet the human body only consumes 100W.  Now, is there room for computation locally (memristors) and remotely(CPU+ram)? Absolutely. Part of the work is also in algorithms as well as hardware."
MachineLearning,3djot1,clow_reed,2,Fri Jul 17 16:40:32 2015 UTC,"If we are to emulate neurons in the brain, it would take an estimated 100GW using our current tech   But current tech will continue to improve (in compute power and efficiency) even without memristors."
MachineLearning,3djot1,VelveteenAmbush,-2,Fri Jul 17 18:51:26 2015 UTC,"It will, undoubtedly. However, all technologies follow an advancement S curve. We're at the long tail of our current combination of technologies.   The memristor (which is on sale here ) is the beginning of the next S curve of advancement. And I'm thinking that this will enable a much greater area of heuristic and neural modeling in the realm of AI."
MachineLearning,3djot1,clow_reed,2,Fri Jul 17 20:30:19 2015 UTC,"We're at the long tail of our current combination of technologies.   No we're not. GPUs are still improving exponentially, year over year."
MachineLearning,3djot1,VelveteenAmbush,2,Fri Jul 17 20:49:29 2015 UTC,Finding the correct algorithm is a completely hardware-independent problem. Finding the algorithm is what you should do first. Then you can start thinking about specialized hardware.
MachineLearning,3djot1,nkorslund,5,Fri Jul 17 21:42:30 2015 UTC,"I think the growth of ML is more closely linked with the rapid accumulation of digital data (which is accelerating far faster than Moore's law).  One  can argue that the increased amount of collected  information is due in part to having faster computers, but it is not directly contingent on this fact.  Recall that before ""Deep Learning"" was mentioned in every ""Changing the world"" article released by tech media, it was all about ""Big Data"".  We all know fitting a model with 13043242304230 parameters is futile without a large dataset."
MachineLearning,3djot1,iidealized,0,Fri Jul 17 01:53:10 2015 UTC,Why? Isn't the solution in that case to scale out?
MachineLearning,3djot1,eliquy,-1,Fri Jul 17 00:15:51 2015 UTC,"Machine Learning will continue to grow at an astounding pace eating up other disciplines along the way   FTFY.  IMO: Moore's law did great. What it's done already is more than enough. Architecture designs, and algorithms always trail behind what the underlying technology can offer. There is a wide open architecture area called neuromorphic computing that can give tremendous boost to ML performance without needing any increase in transistor density."
MachineLearning,3dq2m5,thai_tong,4,Sat Jul 18 09:53:50 2015 UTC,"I'm not trying to be a dick and I only feel confident saying this because other people already have:  Please stop sharing deep dream articles. Oddly, it's one of the most pseudo science things to come from Google. It's real, don't get me wrong, but they completely messed up it's implementation.   Why not write an article about the statistics on useless dog faces and maybe how it correlates to what it was fed.   This also brings up how little we know about why the results are so awful and useless from this deep learning program.   This was shown to the world so people could write that line about electric sheep. We really don't need more of it and of course everyone knows and has an opinion about it. It's just not useful or impressive. Especially given karpathy's char-nn."
MachineLearning,3dq2m5,bonoboTP,3,Sat Jul 18 10:56:26 2015 UTC,"You know, I feel your frustration, but if little fun things from machine learning and computer vision catch the interests of the masses that's a good thing. The point is, even if it's overly speculative and over-interprets things, it still helps research. There is however a danger of a next AI winter if we tell people we can do more than we can actually do.  Inspiration and curious fun stuff are often the motivation for studying the thing seriously.  And this is not unique to ML. Take for example all the articles on the LHC, Higgs boson (""God particle"") etc. But at least people learn that there is this sort of thing and it's interesting."
MachineLearning,3dq2m5,bonoboTP,1 point,Sat Jul 18 12:21:01 2015 UTC,"Yes and I thought of the mainstream/masses thing (""This was shown to the world so people could write that line about electric sheep."" was my metaphor for an understanding) but this is /r/machinelearning.  Data is about appropriation and this has been in every sub that it can worm its way into and in this one way too many times. You know what this is? It's overfitting. We don't need more of this thing we already know about and can infer better things without (I'll come back when I need everything to look like a cockatoo).  I don't know how someone says it's not weird that this is getting reposted like 20x in a subreddit of 42K people. I don't think there is a good argument and there are no ""masses"" or ""mainstream"" people in here.  And saying there are or acting like that's what this repost is doing is a circlejerk, plain and simple. We are not enlightening anyone but ourselves with this useless crap and I only call it that because literally everyone in here hopefully knows better than to think this is a valid study or example of practical machine learning. Just saying."
MachineLearning,3dlgwr,iori42,2,Fri Jul 17 05:59:35 2015 UTC,"Correction: it is ""General Reinforcement Learning Architecture"".  The paper is here: http://arxiv.org/abs/1507.04296"
MachineLearning,3dlgwr,despardesi,1 point,Fri Jul 17 16:58:33 2015 UTC,This is very exciting. I'm looking forward to see the applications of this technology (and maybe come up with some myself) :)
MachineLearning,3dnptf,rrenaud,1 point,Fri Jul 17 19:30:42 2015 UTC,What's a model transfer trick? I've never heard of anything like that.
MachineLearning,3dnptf,jrkirby,2,Fri Jul 17 21:57:24 2015 UTC,"He basically explains in his message: you convert one type of model to another, in this case an RNNLM into an N-gram LM."
MachineLearning,3dnptf,pelemanov,1 point,Fri Jul 17 22:15:52 2015 UTC,"I am not aware of any paper doing this on an LSTM (in fact, I have seen surprisingly little LSTM LM papers), but here's an overview for RNNLM:  http://www.cis.lmu.de/~heike/papers/Interspeech2014_01.pdf"
MachineLearning,3dnptf,pelemanov,1 point,Fri Jul 17 22:03:52 2015 UTC,"How fuzzy of a match are you looking for?  Maybe Grammar as a Foreign Language?   They generated a corpus using an LSTM LM, and then trained an N-gram model on it.   These guys generated a corpus using the Berkeley parser, and then trained an LSTM model on it."
MachineLearning,3dnm1i,You_Have_Nice_Hair,3,Fri Jul 17 19:03:02 2015 UTC,"It might be worth your time to look into the 500p+ book ""Neural Networks: A Systematic Introduction"" by Raúl Rojas from 1996[1].  From all I know it tries not only to derive the math etc. but also to build up an intuition about the concept of neural networks.  [1] http://page.mi.fu-berlin.de/rojas/neural/neuron.pdf"
MachineLearning,3dnm1i,walachey,1 point,Sat Jul 18 10:39:51 2015 UTC,"So much has been discovered since 96, are you sure the book is still relevant? I'm looking for a good introduction too so this is not a rhetorical question, don't feel attacked! LOL"
MachineLearning,3dnm1i,SmArtilect,1 point,Sat Jul 18 17:58:15 2015 UTC,"That's actually a good question and I am afraid I cannot answer that to your satisfaction. That book is still on my to-read-list, too.  But I'd hope that the motivation and intuition behind the main principles still hold for any kind of network and any modification that has been invented since '96 :)"
MachineLearning,3dnm1i,walachey,2,Sat Jul 18 19:54:23 2015 UTC,"It's lunatic to recommend a book from thirty years ago, by a never-heard-of author, that you haven't read."
MachineLearning,3dnm1i,Foxtr0t,1 point,Sat Jul 18 21:59:56 2015 UTC,He said to look into it.
MachineLearning,3dnm1i,kamekai,1 point,Sun Jul 19 05:03:23 2015 UTC,"Well, I know the author and I know that he is somewhat talented in giving good intuitions for problems. That's why the book is on my to-read list.  And from what I can see, the only other finished book about neural networks that does not explicitely focus on f.e. deep learning, which was recommended in this thread, is from the very same year: 1996. Twenty years ago.   And as the book I recommended is available online as well, I don't really think it will hurt anyone to know of its existence. It would obviously be a different story if I said ""go buy the book now!"" or something."
MachineLearning,3dnm1i,walachey,3,Sun Jul 19 15:12:16 2015 UTC,Yoshua Bengio is co-author on a book in progress that is freely available online.
MachineLearning,3dnm1i,Eurchus,2,Fri Jul 17 19:28:58 2015 UTC,Will definitely read through it this weekend. Thanks
MachineLearning,3dnm1i,felipemoraes,2,Fri Jul 17 19:39:30 2015 UTC,I have found Yoshua Bengio book's excellent. However he focused on deep learning. Another book and online is this one: http://neuralnetworksanddeeplearning.com/.
MachineLearning,3dnm1i,jeongyoonlee,1 point,Fri Jul 17 19:31:47 2015 UTC,"The classic (before deep learning era) bibles on NNs are the ""Bishop books"": 1. Neural Networks for Pattern Recognition published in 1996 (http://www.amazon.com/Networks-Pattern-Recognition-Advanced-Econometrics/dp/0198538642) 2. Pattern Recognition and Machine Learning published in 2006 (http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=asap_bc?ie=UTF8)  Both are highly recommended."
MachineLearning,3dnm1i,rasbt,1 point,Fri Jul 17 20:31:44 2015 UTC,"Adding to the already very excellent suggestions, there are also some interesting titles about to be published by O'Reilly that may be worthwhile having on your radar:  Deep Learning: A Practitioner's Approach by Adam Gibson and Josh Patterson (~ Nov 25, 2015)  Fundamentals of Deep Learning: Designing Next-Generation Artificial Intelligence Algorithms by Nikhil Buduma (~ Nov 25, 2015)  Another really good one is ""Neural Networks and Deep Learning"" by Michael Nielsen. It's a free ""online book"" and I recommend checking it out if you want a gentle intro to Nnets and deep learning accompanied by Python implementation examples:  http://neuralnetworksanddeeplearning.com  I am also currently writing on a more intro level book (published in August) that starts with perceptrons and adaptive linear neurons, continues with logistic regression and SVMs, discusses the essential best practices (data preprocessing, hyperparameter tuning techniques, model evaluation), and concludes with multilayer feedforward neural networks. When this is done, I am probably planning to follow up with another one about different Nnet architectures."
MachineLearning,3dnm1i,mmeruz,1 point,Sat Jul 18 17:03:50 2015 UTC,"To this nice list of suggestions I would also add ""Neural networks and learning machines"" by Haykin, which I personally used with ""Pattern Recognition and Machine Learning"", finding them complementary and exhaustive (as far as my interests are concerned) altogether!"
MachineLearning,3doc0g,spudzee111,2,Fri Jul 17 22:23:57 2015 UTC,"You don't have nearly enough data for a model that big.  Try like, one hidden layer with 10 units."
MachineLearning,3doc0g,kjearns,1 point,Fri Jul 17 22:44:49 2015 UTC,Thanks for the advice. I'm a new to machine learning. Could you explain why the model has to correlate with the size of the network? And how does one determine how large your neural network has to be based on the model?
MachineLearning,3doc0g,kjearns,1 point,Fri Jul 17 23:14:32 2015 UTC,"The short answer is ""bias-variance tradeoff"".  This site seems like a decent explanation: http://scott.fortmann-roe.com/docs/BiasVariance.html"
MachineLearning,3doc0g,sdsfs23fs,2,Fri Jul 17 23:51:37 2015 UTC,pybrain is fucking ancient. use something better.
MachineLearning,3doc0g,sdsfs23fs,1 point,Fri Jul 17 22:45:51 2015 UTC,do you have any specific suggestions?
MachineLearning,3doc0g,simonhughes22,2,Fri Jul 17 23:13:08 2015 UTC,"passage, blocks, keras"
MachineLearning,3dmyob,bluedunnock,4,Fri Jul 17 16:08:41 2015 UTC,"Did you normalize the features? It's never a bad idea for any classification task (AFAIK, I'd love to hear counterexamples), but IME, SVM will benefit more from it than, e.g., logistic regression."
MachineLearning,3dmyob,Doc_Nag_Idea_Man,1 point,Fri Jul 17 16:29:30 2015 UTC,Tried both normalization and scaling. No luck
MachineLearning,3dmyob,sriramcompsci,2,Fri Jul 17 17:54:20 2015 UTC,"Have you looked at the training error / test error? Do you observe overfitting? Have you looked at the number of support vectors you get? I'm assuming that you have tuned your regularization parameter using cross-validation. If you do want to perform feature selection, you can add an L1 penalty to the SVM formulation. This is fairly straightforward to do in CVX. Alternatively, you could add the L1 penalty to the online SVM (pegasos algorithm) and observe the resultant weight vector. (The gradient will have an additional term (piece-wise) ). P.S - Data normalization is a pre-requisite before applying any learning algorithm. I'm assuming you've done this."
MachineLearning,3dmyob,sriramcompsci,1 point,Fri Jul 17 17:36:09 2015 UTC,Sounds interesting. Yes I have done normalization. Yes I used CV to tune L1 strength. Do you know a good implementation of pegasos where I can add L1 penalty.
MachineLearning,3dmyob,penguinElephant,1 point,Fri Jul 17 17:57:58 2015 UTC,I have used this in the past.
MachineLearning,3dmyob,giancds,2,Fri Jul 17 18:27:43 2015 UTC,"The answer is not simple unfortunately, but this is how you do it:  https://www.math.uwaterloo.ca/~tfcolema/articles/fspaper.pdf"
MachineLearning,3dmyob,farsass,2,Sat Jul 18 04:13:26 2015 UTC,"You can also use the SVM's decision function weights, fitted using all features to perform the selection. I have used it in the past and always got good results. The reference paper is the following (and endorsed by V. Vapnik, the ""father"" of SVMs):   ""Gene Selection for Cancer Classification using Support Vector Machines"", Guyon et al., 2002.  http://link.springer.com/content/pdf/10.1023/A:1012487302797.pdf  Hope you find it useful."
MachineLearning,3dmyob,mynameisvinn,1 point,Sat Jul 18 09:09:32 2015 UTC,Looks promising. Will give it a try thanks
MachineLearning,3dmyob,You_Have_Nice_Hair,1 point,Mon Jul 20 17:58:42 2015 UTC,"You can use an ARD kernel, but this is not supported in any SVM libraries I know of. You probably will have to search for papers and implement it yourself..."
MachineLearning,3dmyob,ComplexIt,1 point,Fri Jul 17 18:20:56 2015 UTC,what about reducing dimensionality (eg pca) prior to fitting?
MachineLearning,3dmwkp,philly7891,1 point,Fri Jul 17 15:52:22 2015 UTC,"You can include external libraries and call them from your code within Spark, so you wouldn't have to do anything ""outside."""
MachineLearning,3dmwkp,HybridSystem,1 point,Fri Jul 17 17:39:42 2015 UTC,"Have a look at mllib. An image is just data.  Spark might not have the functionality to display an image,  but you can definitely do pattern recognition on the data."
MachineLearning,3dla6g,Letitgo123456,1 point,Fri Jul 17 04:43:07 2015 UTC,"I can definitely imagine scenarios on arbitrary models where the posterior is quite easy to sample from in a fashion that mixes well,  where sampling from the joint is downright hopeless. It's late and I can't think of a concrete example, but it seems quite plausible that the reduction in degrees of freedom could buy a lot for a sampler, or allow you to use a simpler one. For example, Gibbs sampling on a (lower-dimensional) posterior might work just fine whereas Gibbs on the joint might have great difficulty moving between modes due to the local nature of the updates."
MachineLearning,3dla6g,dwf,1 point,Fri Jul 17 06:44:00 2015 UTC,"You wouldn't be sampling from the joint though from what I understand -- the y are observed so once you've sampled z, you can just compute p(y|z) and doing that a lot of times should give you an estimate for the sum. You don't have to sample y."
MachineLearning,3dhybh,Kiudee,4,Thu Jul 16 12:34:41 2015 UTC,"the Reddit formula should probably take into account the percent of votes that are positive, rather than just taking the difference between positive and negative   So if a story A has 1000 up votes and 500 down votes, and story B has 10000 up votes and 9500 down votes, the current formula gives them both the same value (if same age) and the new formula would give story A more value.   Story B may be one of those highly polarizing topics. Everyone has an opinion. Story A more people like in general, but it hasn't gotten anywhere near as many votes in the same time as story B."
MachineLearning,3dhybh,pohatu,6,Thu Jul 16 15:15:29 2015 UTC,"But as soon as the proportion of dislikes goes above 50% the current formula will kill it anyway.  So polarizing topics that the majority of people don't like go away.  This could be seen as good or bad.  Many people may still have liked it, but once that threshold is crossed it drops."
MachineLearning,3dhybh,SuperImprobable,3,Thu Jul 16 16:57:30 2015 UTC,That's an important omission. Thanks
MachineLearning,3dhybh,pohatu,2,Thu Jul 16 17:14:52 2015 UTC,Wouldn't it be a good idea to show every user a different set of random posts that don't have many votes because else they will never be seen?
MachineLearning,3dhybh,Jeave,1 point,Fri Jul 17 11:53:15 2015 UTC,"What you are hinting at is basically the idea of Thompson sampling where instead of using the mean:  (U+1)/(U+D+2)   we sample random realizations from the distribution of possible post qualities (which for up/downvotes is the Beta distribution):  score ~ Beta(U+1, D+1)   I do not want to go into the details, but the advantage of this approach is, that we are exploring more links/comments until we are certain that we found the best ones. The disadvantage for a site like reddit could be that the users experience too many random results and will not visit the site again. That is why I would not use this as a default sort option."
MachineLearning,3dhybh,utunga,1 point,Fri Jul 17 14:42:24 2015 UTC,This needs to be on the front page as it provides a lot of freaking utility (aka it's awesome). Well done OP
MachineLearning,3dkhuz,StupidQuestionOracle,6,Fri Jul 17 00:23:43 2015 UTC,See this http://arxiv.org/abs/1312.4400   and this https://groups.google.com/forum/#!topic/torch7/Phj3dliBBA0
MachineLearning,3dkhuz,linus_rules,1 point,Fri Jul 17 01:17:55 2015 UTC,"Googlenet Inception, if I remember correctly only stacks very simple layers like convolution (of different sizes) and max pooling.   The other paper looks great, but do the authors provide any method to avoid the subnetworks form learning the same things ? The improvement in accuracy does not seem too high compared to other methods like dropout."
MachineLearning,3dkhuz,dexter89_kp,7,Fri Jul 17 05:40:54 2015 UTC,Pretty sure it would be the equivalent of just making the net deeper.
MachineLearning,3dkhuz,siblbombs,9,Fri Jul 17 01:54:19 2015 UTC,Plus a weird block-sparse connectivity pattern.
MachineLearning,3dkhuz,dwf,1 point,Fri Jul 17 03:11:28 2015 UTC,True.
MachineLearning,3dkhuz,siblbombs,1 point,Fri Jul 17 13:26:04 2015 UTC,Madelbrot nets ? or am I imagining too much.
MachineLearning,3dkhuz,dexter89_kp,2,Fri Jul 17 05:41:43 2015 UTC,Your gradients still disappear.
MachineLearning,3dkhuz,avoidban,3,Fri Jul 17 01:03:08 2015 UTC,Overfitting.
MachineLearning,3dkhuz,over_under_up_down,4,Fri Jul 17 00:36:08 2015 UTC,Would this have anything to do with Geoff Hinton's capsules idea?/Know of any implementations?
MachineLearning,3dkhuz,dexter89_kp,3,Fri Jul 17 00:47:56 2015 UTC,Was going to say this. Prof Hinton feels this is the next step.
MachineLearning,3dkhuz,redditmoose,3,Fri Jul 17 01:38:56 2015 UTC,Pre-program subnets that serve known functionality with the desired fitness.  Exclude them from backprop.  Train the larger neural net to take advantage of those subnets.
MachineLearning,3dkhuz,dexter89_kp,3,Fri Jul 17 03:44:07 2015 UTC,"So backprop in the larger net does not update weight in subnets, but only  updates the relative weighting between the subnets ?"
MachineLearning,3dkhuz,redditmoose,3,Fri Jul 17 05:36:08 2015 UTC,"Yeah.  Like for your brain, you don't update your retina colour recognition NN if you're training yourself to recognize when Suzy is flirting.  You have that already."
MachineLearning,3dkhuz,NicolasGuacamole,1 point,Fri Jul 17 15:38:22 2015 UTC,Personally I don't think so. My intuition tells me that the capsules idea is in grouping neurons together to enforce learning distinct / diverse subsets of the feature space - though this is just conjecture.
MachineLearning,3dkhuz,padelas14,1 point,Mon Jul 20 01:14:33 2015 UTC,You end up with a very big neural network :) (which usually isn't good)
MachineLearning,3dkhuz,Descates,1 point,Sat Jul 18 20:20:31 2015 UTC,Isn't this unit network now restricted to producing a single scalar output ? What are the benefits of doing this over deeper networks ?
MachineLearning,3dkhuz,cajag,-6,Mon Jul 20 10:02:47 2015 UTC,Cylons
MachineLearning,3dkhuz,k9triz,-1,Fri Jul 17 00:34:52 2015 UTC,Very close to the human neo-cortex where cortical columns form this kind of interconnected network-within-network structure
MachineLearning,3dkhuz,needz,-7,Fri Jul 17 08:12:18 2015 UTC,Skynet
MachineLearning,3dlhfk,badatmagic,1 point,Fri Jul 17 06:06:09 2015 UTC,"https://www.edx.org/course/sabermetrics-101-introduction-baseball-bux-sabr101x-0  this isn't fantasy, but could get you started?"
MachineLearning,3dlhfk,haribo247,1 point,Fri Jul 17 06:52:29 2015 UTC,"Yeah looks informative, thanks!"
MachineLearning,3dlhfk,Capn_Cook,1 point,Fri Jul 17 22:17:37 2015 UTC,I think the biggest issue would be getting ahold of stats for making your dataset. Is there a publically available way to rip them into like a CSV or something?  I'd like to do this for fantasy football in the fall.
MachineLearning,3dlhfk,MusicIsLife1995,1 point,Fri Jul 17 14:34:59 2015 UTC,Just parse NFL web pages with bots.
MachineLearning,3dm6rn,__null__,3,Fri Jul 17 11:58:51 2015 UTC,"what about using sklearn?  In [1]: import numpy as np  In [2]: from sklearn.decomposition import PCA  In [3]: X = np.random.rand(50, 396)  In [4]: pca = PCA(n_components=2)  In [5]: pca.fit(X)  Out[5]: PCA(copy=True, n_components=2, whiten=False)  In [6]:  print(pca.explainedvariance_ratio)  [ 0.03646259  0.03390783]"
MachineLearning,3dm6rn,USER_PVT_DONT_READ,0,Fri Jul 17 12:10:30 2015 UTC,It seems to work... I 'll have to test it more thoroughly though...
MachineLearning,3dm6rn,ThatGeoGuy,1 point,Fri Jul 17 13:00:09 2015 UTC,"Which axis of the data represents the dimensionality? I'll assume the columns represent separate dimensions, so your data is composed of 50 observations with each observation having 396 dimensions. If I have this backwards, ignore the data.T below and just use data.  In [1]: import numpy as np In [2]: data = np.load('featureShape.npy') In [3]: data = data[:,0,:] In [4]: covariance_data = np.cov(data.T) In [5]: evals, evecs = np.linalg.eigh(covariance_data)   Where the Eigenvectors are the principal component vectors of your data (each column is one principal vector), and the Eigenvalues are the variance of each Eigenvector. Principal components are just the Eigenvalues and Eigenvectors of the covariance matrix of the data. The reason you're likely hitting performance issues is either:   The implementation you're using is performing the Singular Value Decomposition directly on the data. They are computing the complete least-squares solution for each Eigenvector, which can take an extremely long time.    The method above is the fastest and most numerically stable way to obtain the principal components. For more information, see ""Least-squares fitting algorithms of the NIST algorithm testing system"" by Craig M. Shakarji, 1998 [1].   [1] C. M. Shakarji, “Least-Squares Fitting Algorithms of the NIST Algorithm Testing System,” J. Res. Natl. Inst. Stand. Technol., vol. 103, no. 6, pp. 633–641, Dec. 1998.  EDIT: Didn't realise the data was reading in as shape (50, 1, 396) so I fixed it such that the data should be shape (50, 396)"
MachineLearning,3djg0u,BPellegrino,3,Thu Jul 16 19:31:51 2015 UTC,"There are many communities making poker bots and supposedly using them in places like PokerStars to earn money and such. You can even buy ""corpora"" of poker matches to teach your bots and there are various competitions. There was a cool page called poker-ai.org with lots of information but I see it's mostly dead now.  I would try global optimization methods like evolutionary algorithms that would take both the winning odds (computed from visible cards - not an easy computational task mind you) and stakes to decide an optimal strategy."
MachineLearning,3djg0u,r4and0muser9482,1 point,Thu Jul 16 20:57:50 2015 UTC,"yea I'm less interested in actually making one and more interested in how machine learning would approach this scenario, poker just happened to be the medium we were discussing since we both spent our last 5-10 years playing it it was an interesting problem neither of us had the solution to. My real interest lies in sports analytics  edit: I should clarify, what you're suggesting is having the program learn via hand equity what was good/bad, I am saying if you have a starting point that you KNOW is good (i.e. as a training set you have 1,000,000 hands from a player who wins $500,000 per year and wins 56.5% of his games).   So you know the sum of his actions leads to winning play/success, but some of the actions may be across the whole spectrum of say VERY GOOD, GOOD, NEUTRAL, BAD, VERY BAD. We assume most lean towards the positive side, but knowing there are bad ones mixed in how does the program learn which is which in this case and which to weight/act on more frequently. Or does it not and it just tries to mimic the frequencies that play had per situation in an overall sense?"
MachineLearning,3djg0u,inspired2apathy,1 point,Thu Jul 16 21:35:22 2015 UTC,"Is it just the combinatorics that make things so expensive? What do people do, just use heuristics?"
MachineLearning,3djg0u,r4and0muser9482,1 point,Sat Jul 18 02:22:32 2015 UTC,"Don't remember off the top of my head, but turns out the problem is more difficult than you might expect. You want to use lookup tables, but the size of a full lookup table is kinda huge for texas hold'em. I think there are some tricks you can do, to make it more managable, but the problem is not as straightforward as some people think. Conversley, Blackjack is so simple, you can write out a full strategy on a small sheet of paper and that is what many people in Las Vegas do. Poker is a bit more demanding."
MachineLearning,3djg0u,MusicIsLife1995,3,Sat Jul 18 07:12:12 2015 UTC,You could make an optimal bot without machine learning and just rely on statistics. But it is more impressive if you could use a learning algorithm to do this.
MachineLearning,3djg0u,MrPapillon,1 point,Thu Jul 16 21:51:17 2015 UTC,"Stats might work for limit, but I have a doubt for no-limit Texas hold'em."
MachineLearning,3djg0u,Dwood15,3,Thu Jul 16 22:20:41 2015 UTC,"This is almost exactly the problems I am faced with when I'm working on my own Mario ai... The trouble with NN's seems to be not so much the implementation of it as it is what a fitness function is. Even a badly designed (cough MarI/O cough) Neural Network can learn to play a game with a decent fitness function...   But that's the problem we have to solve, isn't it?"
MachineLearning,3djg0u,Dwood15,1 point,Thu Jul 16 22:40:21 2015 UTC,"yea I've tried to think about good goals for video game AIs, score works for some games, time for others, but like... in an AI playing ocarina of time or super metroid do we really want to see it rush through without exploring/finding anything? just simply fastest path to the finish? Like when I think about a computer playing those games ""well"" I would envision them finding many/most of the secrets and completing the game, some progress measurement combining the two, but I can't think of what that might be. That said the more practical form of AI is simply to be the singular best, so the best speed runner or the highest points scorer there can be, but it doesn't give us a very exploratory look at a computer playing the game as a human would (not that that's required by any means)  anyways, what specifically are you working on with your mario AI? what languages are you working in and what progress have you made? would love to hear more about it!"
MachineLearning,3dhvhe,clbam8,1 point,Thu Jul 16 12:06:07 2015 UTC,"I thought that adversarial examples required some sort of non-smoothness properties in the model architecture - but he says this occurs for all models, instead of a subset with these properties. Am I misunderstanding something here?"
MachineLearning,3dhvhe,FatSoccerMan,3,Thu Jul 16 22:57:35 2015 UTC,"It absolutely is related to smoothness.   Smooth response functions (such as RBFs) are more resistant to adversarial examples - see Ian's paper for details. You can think of adversarial example mining (via fast sign) as assuming the model is linear and stepping in the direction that is least ""smooth"" (highest gradient) to get an adversarial example. If your model is very smooth locally, for a given point you may not be able to get an adversarial example that results in a miss-classification within a given epsilon.  An interesting issue seems to be that smooth architectures (such as deep RBFs) are difficult to learn but I don't know if deep RBFs even make sense as a family of models worth trying to learn, honestly."
MachineLearning,3dhvhe,alecradford,2,Fri Jul 17 01:26:43 2015 UTC,linear is as smooth as you can get...
MachineLearning,3dhvhe,sdsfs23fs,2,Fri Jul 17 01:49:15 2015 UTC,"Imagine 2D classification where the linear model assigns a weight to one axis 100x the other, moving a tiny amount along that axis will result in a huge change in the models output compared to the other axis. This amplifies for high dimensional spaces.  One way of defining smoothness (if we're dealing with a probabilistic classifier) is as the KL divergence of the output with respect to the input, this will be maximal for a linear model by moving along its ""response"" axes (perpendicular to the decision boundary)."
MachineLearning,3dhvhe,alecradford,1 point,Fri Jul 17 02:08:12 2015 UTC,My wording was ambiguous - I think I was thinking of some sort of bound on change Ala lipschitz continuity
MachineLearning,3dhvhe,FatSoccerMan,1 point,Fri Jul 17 02:37:23 2015 UTC,Maybe not smooth - lipschitz continuous maybe? Some sort of bound on how fast it can change
MachineLearning,3dhvhe,FatSoccerMan,2,Fri Jul 17 03:40:15 2015 UTC,"Agreed, ""some sort of bound"" on how fast it can change is super related. For things like natural images we know a loose prior (greater than 1/255.) on how fast the class distribution can change in input space.   I guess one way of thinking about adversarial examples is as showing very explicitly that ML models dont learn models internally that reflect this prior and we should develop strategies (like adversarial training) to address that."
MachineLearning,3dhvhe,alecradford,1 point,Fri Jul 17 03:32:26 2015 UTC,"what about the papers showing that you can reduce weight resolution of deep models without losing much performance?   If these adversarial images only differ beyond 8 bits, wouldn't that defeat them?"
MachineLearning,3dhvhe,sdfsdkehg,3,Fri Jul 17 04:05:39 2015 UTC,The adversarial images become effective at beyond 8 bits --- presumably you can crank up their size and they will remain effective.
MachineLearning,3dhvhe,genneth,0,Thu Jul 16 14:35:56 2015 UTC,at least then they'd be visible though.. and presumably easier to filter out.
MachineLearning,3dhvhe,sdfsdkehg,1 point,Thu Jul 16 15:25:30 2015 UTC,"The larger the epsilon (constraint on the norm) of the adversarial perturbation you allow the ""stronger"" the adversarial example you can find using the simple gradient techniques. When you do adversarial training the epsilon used for natural images are often much larger, 20/255 for instance, compared to 1/255.  This is due to the underlying linear structure current models are built on, which extrapolate/respond to any value no matter whether or not it is actually near the data manifold, figure 3 in the article is an example of this."
MachineLearning,3dhvhe,alecradford,1 point,Thu Jul 16 15:40:51 2015 UTC,hmm.. looking at that figure it seems like variance of the logits would be a good indicator of adversarial input then.
MachineLearning,3dkmjy,wonkypedia,1 point,Fri Jul 17 01:04:53 2015 UTC,Weighted impurities can be a bit counterintuitive and aren't a penalty like you're describing. Try weighting the other way.  Ie to reduce the number of missed 0's you want to grow trees where the 1 nodes are purer then the 0 nodes. I you want the nodes that are mostly 0's include all of the 1's that are close to those zeros. And you want the nodes that are mostly 1's to never have a 0.  Another approach may be to use the probability each node is 0 vs 1 and set a threshold that gives the results you want.
MachineLearning,3dkmjy,micro_cam,1 point,Fri Jul 17 04:47:12 2015 UTC,That makes things clearer. Thanks!
MachineLearning,3dkmjy,micro_cam,1 point,Fri Jul 17 04:48:24 2015 UTC,Also there are a ton of different weighted criteria that have been used for tree growth and some work one way and others work other ways so you pretty much always need to do a parameter search of some sort.
MachineLearning,3dhsu4,clbam8,3,Thu Jul 16 11:33:35 2015 UTC,They attached the same DQN algorithm to parameter server.  Did anything else change?
MachineLearning,3dhsu4,evc123,1 point,Thu Jul 16 17:43:30 2015 UTC,So they adapted the work form Jeff Dean and others just for this model ?
MachineLearning,3dhsu4,dexter89_kp,3,Fri Jul 17 05:46:14 2015 UTC,"Did anyone notice the reference to Lin 1993? I loled.  Also, I find it interesting they only report on replicating their previous Atari player work. Since they're claiming an effective order-of-magnitude speedup (from >13 days to 1 day), you would think they would be taking advantage of this by moving a step up in complexity to perhaps NES games, or removing some of the hacks like downscaling the image."
MachineLearning,3dhsu4,gwern,-1,Fri Jul 17 01:43:40 2015 UTC,"Massively Parallel Methods for Writing Papers about Deep Reinforcement Learning, amirite?"
MachineLearning,3djlir,bubbachuck,2,Thu Jul 16 20:12:08 2015 UTC,try using KNIME or RapidMinder.....KNIME lets you use all the weka models without the stupid interface that they made for it.   I am not sure if you can interpret results of features like that of a logistic regression type model..but you could use AUC of said variables and go off that.   EDIT: watch this  https://www.youtube.com/watch?v=27RQRUR7Ubc
MachineLearning,3djlir,watersign,1 point,Sat Jul 18 01:45:57 2015 UTC,"I'll look into it, thanks!"
MachineLearning,3djlir,melipone,1 point,Sat Jul 18 04:00:35 2015 UTC,Features weights does not make sense for SVM. It's the instances or the support vectors that matter. The output of LibSVM from Weka tells you the number of support vectors but that's about it AFAIK.
MachineLearning,3djlir,GoldmanBallSachs_,0,Mon Jul 20 02:30:19 2015 UTC,WEKA    You should use Python
MachineLearning,3djj6s,regularized,3,Thu Jul 16 19:54:48 2015 UTC,There's Max Welling's group in Amsterdam.
MachineLearning,3djj6s,treebranchleaf,2,Fri Jul 17 12:52:03 2015 UTC,"Many labs in Europe actually have people who work on deep learning, it's just not always their main focus.  The most obvious Industry/Academic groups are Deep Mind and FAIR.  There are a few very active meetups all around with both academics and industry people (I am the organiser of http://www.meetup.com/Deep-Learning-Paris-Meetup)"
MachineLearning,3djj6s,CptZouglou,4,Thu Jul 16 20:05:21 2015 UTC,"http://www.idsia.ch/idsia_en/institute.html, where Jürgen Schmidhuber's team."
MachineLearning,3djj6s,zhengzhu1,1 point,Thu Jul 16 20:27:12 2015 UTC,"Parietal: https://team.inria.fr/parietal/ They are active in brain imaging analysis, and are also contributors to the ANN parts of Scikit Learn (also from INRIA)  Willow: http://www.di.ens.fr/willow/index.php Jean Ponce is a world class researcher in computer vision, but I'm not sure the group is active right now"
MachineLearning,3dgnr2,medavis6,7,Thu Jul 16 03:06:25 2015 UTC,"It sounds like your goal is to train a binary classifier given some game state, return whether the team pass or run. The problem with knn and having this mass of data is that the number of data points you need to fill the feature space grows exponentially with the number of features. One algorithm that may handle this sparsity a bit better is random forests, which also won't require you transform your inputs in to numerical values or anything like that.  Sounds like an interesting problem. However it will be tough to keep up to date. Each team is intentionally attempting to be difficult to predict. Having 50 2-yard runs is justified if it leads to one 80 yard pass. They are usually actively setting up expectations through the game with the intent of breaking them at critical points on the most important plays. Further, these trends will change as the season progresses, players have injuries or big mismatches, etc. Certainly you could include these factors into your training set, but you will need to continually update the model after each game or after each play."
MachineLearning,3dgnr2,laxatives,2,Thu Jul 16 03:50:33 2015 UTC,Do you think it would be worthwhile to only model the last n games in a sliding window approach?
MachineLearning,3dgnr2,Jonno_FTW,2,Thu Jul 16 08:48:21 2015 UTC,"Yeah that sounds like a good approach. This may be a case where having too large a window could harm your results and you could tune this parameter using cross validation, or the out of bag error if you use random forests."
MachineLearning,3dgnr2,laxatives,1 point,Thu Jul 16 15:50:29 2015 UTC,"I'd think that you would also want to take into account a particular coach or offensive coordinator biases for certain plays. For instance, if team favors a pass on second down that would be valuable information to bring forward as a predictor variable. So you have all your game state variables(clock, yardage,down, etc..) and you also bring in historical biases for similar situations. Also you'd want to be calculating on-going features through the game such as 3rd down conversions,etc..This would capture individual game dynamics such as the offense having an off day or favoring one type of play over another for that game. I imagine there be a decent amount of feature engineering you would want to do before running the random forest."
MachineLearning,3dgnr2,atlanta_gt,1 point,Thu Jul 16 16:06:37 2015 UTC,"I'd think that you would also want to take into account a particular coach or offensive coordinator biases for certain plays.    This is along the lines of my ultimate goal with this project.    e.g., Look back through my play log for ""Paul Chryst"" who called plays at Wisconsin from 06-09 then at Pittsburgh from 10-13 and analyze his trends specifically.    Obviously Art Briles (head coach of Baylor) is going to pass it 90% of the time while Navy is going to run it 90% of the time so the average style won't apply to them as well as other ""Pro-Style"" offenses."
MachineLearning,3dgnr2,ldaugusto,4,Thu Jul 16 16:38:41 2015 UTC,"Seems totally viable do it in R, I see no problem.  Tell the 'accuracy is 65%' alone doesnt tell a lot about your problem. Whats the frequency of the dominant class? If 'run' is 63% of plays, your predictor wouldnt be so good.  I'm curious to see you try DBSCAN clustering and Random Forest."
MachineLearning,3dgnr2,adamb0mb,2,Thu Jul 16 03:47:26 2015 UTC,This is awesome! Where did you get this data? I'd love to play with it too.
MachineLearning,3dgnr2,joe-murray,3,Thu Jul 16 04:06:20 2015 UTC,"There are a few good resources out there you can use for cfb data, you just have to do some googling. Here's an example of one:    http://www.cfbstats.com/2014/national/index.html"
MachineLearning,3dgnr2,airalcorn2,2,Thu Jul 16 05:36:19 2015 UTC,"I got it in /r/CFBanalysis in this thread. Nobody has uploaded 2014 yet, however."
MachineLearning,3dgnr2,MusicIsLife1995,2,Thu Jul 16 12:17:03 2015 UTC,"Is it even possible to do this in R or do I need to learn some programming to turn it into an app/dashboard/website?   Yep! You can tackle basically any data analysis problem with R these days.   I'm pretty sure this isn't like linear regression where I can start writing equations based on output and weight of variables.   Actually, you should probably look into a type of generalized linear model known as logistic regression. It'll do exactly what you want... given some variables, it'll output the probability of a run or pass.  Edit #1: To elaborate, you could train a different classifier for each team where each row of the training set represents a game state and the play that was actually ran is the target. Quarter and down might be different categorical variables, while points for and against, time remaining, and yard line could be continuous variables.  You might also want to take a look at this project I worked on a couple of years ago for ideas. To plagiarize myself:   Football-o-Genetics is an application for ""evolving"" near-optimal offensive play calling strategies. The application incorporates ideas from both artificial intelligence (specifically, genetic algorithms) and advanced statistics (in the form of a Markov model of an offensive drive) to accomplish this end.   Edit #2: I actually kind of wonder what a recurrent neural network would do with this data, but that might be further down the road, OP!"
MachineLearning,3dgnr2,airalcorn2,2,Thu Jul 16 04:12:29 2015 UTC,Logistic regression is a discriminative linear model.
MachineLearning,3dgnr2,unchandosoahi,5,Thu Jul 16 04:19:54 2015 UTC,"Haha, yep! Not sure where you're going with that..."
MachineLearning,3dgnr2,brational,2,Thu Jul 16 04:29:53 2015 UTC,Thanks for the help and link to your own project! Very interesting stuff. I'll likely be doing a follow up Random Forest and logistic regression.
MachineLearning,3dgnr2,joe-murray,2,Thu Jul 16 12:26:46 2015 UTC,"Hey! I like that your algo it's in R. A good tool for your purpose. Regarding the algorithm used to predict the outcome run or pass,  I think you should use other technique like suggested in the other comments.   The problem with K-NN is that it tends to grows as geometric as your data dimensions are. I will use SVM or Random Forest and I'm pretty sure your results accuracy will increase significantly.   One last thing: how are you testing the algorithm results? Ground truth?"
MachineLearning,3dgnr2,srt19170,1 point,Thu Jul 16 04:15:54 2015 UTC,"Yeah, I'm basically comparing what is predicted vs. what is actually called. Here is some fake data that I made up that vaguely represent how I'm ""testing accuracy"" where the x-axis is my model's predictions.      Pass Run    Pass 2315 1295   Run 1095 2363    I will likely be doing a Random Forest and logistic regression in the near future."
MachineLearning,3dhsv0,ocramz,2,Thu Jul 16 11:33:52 2015 UTC,"More in general, if a fitted generative model is at best a biased approximation, and let's say we're interested in the predictive distribution, there is always an information loss associated with the fit.   True. In a Bayesian context, I've seen some analysis of this at least here (section 3). Quoting from there:   To sum up, what Bayesian updating does when the model is false (i.e., in reality, always) is to try to concentrate the posterior on the best attainable approximations to the distribution of the data, ‘best’ being measured by likelihood.   Basically, in a Bayesian setting you have a set of models, each fully specified given a fixed θ, and you use observations to (asymptotically) find a θ that compresses future data in the least number of bits. In that sense, it is a ""best"" θ among the ones available. Depending on how badly the model is misspecified, it may or may not compress the data much worse than the ""true"" model that (we assume to have) generated the data. That difference in compression/prediction efficiency represents the information lost by using a misspecified model instead of the ""true"" model.   all require the measures to have a common support, which cannot be guaranteed in the purely sampled case.   I'm not sure what you mean. If you observe data points that are not in the support of your model, obviously you either have to fix the model or discard those data points. Discarding can work e.g. when your model assumes that a person is either a male or a female and then you get the rare hermaphrodite who sadly ends up being ignored.. I don't see how this would be a problem in the Gaussian setting you used as an example anyway, you just use the entire Euclidean space of appropriate dimension as the support and that's that, it's not like you suddenly observe imaginary coordinates for an airplane you're tracking or something.   in all cases when we don't have a conjugate pair, we can only use the empirical measure.   Um, what empirical measure is that..?  The only issue with non-conjugate models is the lack of a closed form, but that doesn't mean that the posterior predictive probabilities don't exist. They are just more difficult to calculate. In those cases, one either tries Monte Carlo methods to calculate them or resorts to approximations that do have closed forms. I don't see how any of that - or conjugacy in general - is related to supports."
MachineLearning,3dhsv0,Coffee2theorems,1 point,Thu Jul 16 18:21:26 2015 UTC,"Thank you for the reply and for the interesting reference and apologies for the poor phrasing above. What I meant is: let's suppose we need an expectation, but the measure is uncertain. If we were to pick a variational family of distributions, and optimize wrt these parameters for best ""generalization"", what would be the smallest information loss associated with every distribution family?   Yes, the mismatched support is not a real problem if we know e.g. that the domain \Omega is Rn, or some conservative bounds on \Omega etc.  So, existence but unavailable closed form, how do we reconcile these ideas? Every model we pick will have nonzero bias, and the same goes for a finite number of MC samples (exact evaluation of an approximation vs approximate evaluation of the exact dynamics). What criterion lets us choose the least biased approach?"
MachineLearning,3dhsv0,Coffee2theorems,2,Thu Jul 16 19:51:12 2015 UTC,"what would be the smallest information loss associated with every distribution family?   Compute the KLD between each member of the family and the true distribution that generated the data (assuming such a thing exists). Find the member with the lowest KLD. That's your answer. Unfortunately since you don't know the true distribution (or you'd just use it and get no loss..), this is not at all practical.  If you have a generative process so that your distribution is randomly chosen from the family according to some prior distribution, then you get the optimal posterior distribution and posterior predictive distribution using the Bayesian formulae for those.  If you do not have a generative process, then you are out of luck. There is no single correct answer, much like there is no single correct answer to ""what is the best function minimization method?"", because there is no free lunch and all that. You can assume that the generative process is something you like and then go through with the Bayesian approach anyway, and get reasonable answers. I'm sure there are other methods as well, something as simple as histograms or KDE are methods for estimating distributions after all.   So, existence but unavailable closed form, how do we reconcile these ideas?   As I said, Monte Carlo methods such as various forms of MCMC. It's more about what can be efficiently and reliably evaluated than about what is in ""closed form"" (whatever that happens to mean on that particular day - is Γ(x) ""closed form""? Or the Meijer G-function?). If MCMC provided quick results with a guarantee of correctness up to n decimal places, there would be no problem at all with it. As it stands, its efficacy in each case is tested rather than proven and YMMV. It is quite popular despite its lack of theoretical (non-asymptotic) guarantees."
MachineLearning,3dgvvu,kapectas,7,Thu Jul 16 04:20:33 2015 UTC,What's annoying about Ubuntu?
MachineLearning,3dgvvu,delarhi,1 point,Thu Jul 16 06:59:19 2015 UTC,"Presumably all of OPs software and stuff is on Windows and he won't be able to access it while running Ubuntu. That's why he said it would be annoying.  I experimented with Ubuntu on another computer. I didn't really like the user interface, though I guess that is a minor complaint. Mostly it crashes a lot more than windows. Eventually it crashed and wouldn't boot up. It went to the login screen but was impossible to get any further.  The instructions for dual booting it on my laptop were quite intimidating. Mainly the warnings of how I could accidentally brick my computer which would really suck, and I can't afford it.  It really sucks that all deep learning software is designed for Ubuntu though. Torch, theano, caffe, and a great deal of related software, are all impossible to install on Windows or a massive pain in the ass. And it doesn't appear to be due to any inherent limitation in windows, but just because the developers don't bother to support it."
MachineLearning,3dgvvu,Noncomment,2,Fri Jul 17 02:19:12 2015 UTC,Install Ubuntu and run a VM with Windows... Your last paragraph supports why Windows should be the one in a VM.
MachineLearning,3dgvvu,GoldmanBallSachs_,2,Fri Jul 17 05:58:00 2015 UTC,"mostly it crashes more   Wat? The only time Linux crashes for me is when I accidentally eat all the memory because I'm silly and think I can use a ton of huge matrices all at once. How did it crash for you? Did you break it or did it just crash?    all software and stuff is on Windows    Most programming is easier in Linux.  Leave the games in Windows,  it's easier to have a pure development environment and a pure recreational environment"
MachineLearning,3dgvvu,Articulated-rage,3,Fri Jul 17 06:14:57 2015 UTC,You could rent a GPU instance on AWS.
MachineLearning,3dgvvu,iori42,1 point,Thu Jul 16 07:08:18 2015 UTC,How expensive is this? How difficult is it to edit files and stuff through the cloud?
MachineLearning,3dgvvu,Noncomment,2,Thu Jul 16 08:06:17 2015 UTC,"65 cents per hour for a normal instance, about 10 cents per hour for a spot instance that is much less reliable.    Editing files through the cloud is easy.  From a user's perspective, it's just like using a normal workstation."
MachineLearning,3dgvvu,alexmlamb,3,Thu Jul 16 08:39:31 2015 UTC,"Dual boot is the way to go. Anyway, your computer may be ""unusable"" will training the RNN... I installed Torch7 on my Archlinux : it was really easy to do.  What can't you do on Linux ? Nowadays there are good open-source alternatives for almost everything, and some prorietary softwares are also available for Linux."
MachineLearning,3dgvvu,FilippoC,2,Thu Jul 16 08:37:34 2015 UTC,"Copying from my comment above because I have the same problem.  Presumably all of OPs software and stuff is on Windows and he won't be able to access it while running Ubuntu. That's why he said it would be annoying.  I experimented with Ubuntu on another computer. I didn't really like the user interface, though I guess that is a minor complaint. Mostly it crashes a lot more than windows. Eventually it crashed and wouldn't boot up. It went to the login screen but was impossible to get any further.  The instructions for dual booting it on my laptop were quite intimidating. Mainly the warnings of how I could accidentally brick my computer which would really suck, and I can't afford it.  It really sucks that all deep learning software is designed for Ubuntu though. Torch, theano, caffe, and a great deal of related software, are all impossible to install on Windows or a massive pain in the ass. And it doesn't appear to be due to any inherent limitation in Windows, but just because the developers don't bother to support it.   What can't you do on Linux ?   Gaming? But that's not really my interest."
MachineLearning,3dgvvu,Noncomment,1 point,Fri Jul 17 05:15:46 2015 UTC,"Ubuntu is maybe not the best choice for a Linux Distribution. You should give a try to Fedora : https://getfedora.org/fr/workstation/ (or Archlinux if you like adventure)   Torch, theano, caffe, and a great deal of related software, are all impossible to install on Windows or a massive pain in the ass   It may be because Windows is a pain in the ass for some developpers...so they are not interested in taking time to port the softwares and make easy installation stuffs."
MachineLearning,3dgvvu,FilippoC,1 point,Fri Jul 17 15:31:44 2015 UTC,"Anyway, your computer may be ""unusable"" will training the RNN   I've got the same GPU as OP, and this is really not the case. On Ubuntu at least, I'm able to use the desktop, text editor and web browser without any noticeable slowdown while running training on the GPU."
MachineLearning,3dgvvu,BadGoyWithAGun,1 point,Thu Jul 16 10:29:42 2015 UTC,"I'm also able to do that, but I don't know if it's true for all GPUs."
MachineLearning,3dgvvu,FilippoC,1 point,Thu Jul 16 10:36:26 2015 UTC,"These should work well on Windows   Theano (Python; installing using conda works well) + a package that implements LSTM (e.g. Keras) CURRENNT (C++) RNNLib (C++, but no Cuda)"
MachineLearning,3dgvvu,disentangle,1 point,Thu Jul 16 09:39:16 2015 UTC,"I believe there is some work being done one getting torch to build in mingw on windows, eg https://github.com/torch/torch7/pull/287#issuecomment-121153304"
MachineLearning,3dgvvu,hughperkins,1 point,Thu Jul 16 10:53:39 2015 UTC,"You could dual boot to Linux, but then run Win7 in a VM under Linux so you don't have to reboot to run Windows stuff.  Alternatively put your GPU in a 2nd PC running Linux, then telnet/ssh into it from Win7 (using a Win7 terminal emulator program such PuTTY)."
MachineLearning,3dgvvu,harharveryfunny,1 point,Thu Jul 16 12:25:10 2015 UTC,"Theano on windows is viable at this point, I used to dual boot linux to work in Theano but I've switched to just windows now."
MachineLearning,3dgvvu,siblbombs,1 point,Thu Jul 16 12:49:30 2015 UTC,"OK, so I found an installer for Torch in Windows here. Now to figure out how to use it... Not quite that simple, unfortunately. The Lua script I want to run through it requires lfs and nngraph which aren't installed by default, and there's no way of knowing how to install those packages. I found an lfs.dll file and added it to the install's lib folder, but it declares ""error loading module 'lfs' from file: not a valid win32 application""."
MachineLearning,3dgvvu,Noncomment,2,Fri Jul 17 01:17:48 2015 UTC,"NNgraph is in pure lua as far as I can tell: https://github.com/torch/nngraph  You should be able to just copy the folder and put it in a place where it can be required from. No compiling or anything required. You could literally paste the code into your file if you wanted (I would not recommend.)  'lfs' stands for lua file system. Lua file system is not related to torch and IS supported on windows. The best way to download it is to use luarocks. If not you can probably find a download somewhere.  You can also install nngraph with luarocks if you like, but it requires an extra parameter something like luarocks --server=https://raw.githubusercontent.com/torch/rocks/master install nngraph. But you might have to tweak that.  You should also make sure you are using luajit and not standard lua. Binaries are available here. Lua is extremely slow compared to luajit.  Lastly if you have the core of torch working, none of this other stuff is strictly necessary. These are just utilities. NNgraph is a convenience tool for making really weird neural networks, and lua file system lets you see what files are in a directory. You can work around these things, if you have to."
MachineLearning,3dgvvu,Noncomment,1 point,Fri Jul 17 08:47:53 2015 UTC,"When I run Wintorch it looks for a nngraph.dll file, for some reason... I ran the command you listed for installing nngraph and it worked, as in it started installing the dependencies for itself, torch etc. but then it tried to install paths, and I got these errors:  CMake Warning:   Manually-specified variables were not used by the project:      CMAKE_BUILD_TYPE   -- Build files have been written to: C:/Users/USERNAME/AppData/Local/Temp/luarocks_paths-scm-1-9139/paths/build mingw32-make: *** No targets specified and no makefile found.  Stop.  Error: Failed installing dependency:     https://raw.githubusercontent.com/torch/rocks/master/nn-scm-1.rockspec - Failed installing dependency: https://raw.githubusercontent.com/torch/rocks/master/torchscm-1.rockspec - Failed installing dependency: https://raw.githubusercontent.com/torch/rocks/master/paths-scm-1.rockspec - Build error: Failed building."
MachineLearning,3dgvvu,Noncomment,2,Fri Jul 17 14:07:36 2015 UTC,"It looks like it's trying to build torch. You already have torch so you don't need it to do that.  nngraph is in pure lua as far as I can tell, although the rocspec has some weird code for compiling. I have no idea what it's purpose is.  I'd suggest just trying to copy the github files into a folder yourself where you can require it from.  It does require graph and nn. Hopefully you have nn working. Graph is also in pure lua and available here: https://github.com/torch/graph"
MachineLearning,3dgvvu,Noncomment,1 point,Fri Jul 17 21:11:24 2015 UTC,"The installer has graph and nn, yes. The only thing I'm missing is nngraph, and it keeps asking for a .dll whenever I run my lua script with the nngraph requirement. I've put the entire nngraph folder into the folder where the other libs are located but to no avail."
MachineLearning,3dgvvu,Noncomment,2,Fri Jul 17 22:11:23 2015 UTC,"Ok I found the problem. There is no file in that folder called ""nngraph"". Therefore when you require nngraph, it searches for files named ""nngraph.lua"" or ""nngraph.dll"". The file you want to require is init.lua. Maybe rename that.  This is probably what that weird stuff from the rockfile does. Compresses the lua files into a single dll or something.  Some issues you might come across is that lua's require system is weird. It searches for any file with filename.lua contained in any of the folders listed in package.path. You can add the folder to the package.path variable. But a better way is to do require 'nngraph.init'. The dot is treated like a slash to a new file path.  Then on the init.lua file, it seems to use some weird undocumented function torch.include. I don't know what this does, but you might need to replace it with require 'nnngraph.name'. Same with the other files."
MachineLearning,3di2zh,gabriel1983,2,Thu Jul 16 13:21:23 2015 UTC,"My guess is it would form some kind of recurrent neural network where upstream feature detectors start to bias downstream sensors. Effectively, what the network perceives now would be directly influenced by what it saw in the past. Do you think this is biologically plausible?  A cup could morph into a donut if the network saw a lion 2 seconds ago."
MachineLearning,3di2zh,k9triz,2,Thu Jul 16 14:42:17 2015 UTC,"I definitely think it's biologically plausible. The brain makes use of a context when it classifies things and part of that is constructed from what has been perceived in the recent past. Take a look at the psychological effect of ""priming"" for an example of this."
MachineLearning,3di2zh,valexiev,1 point,Thu Jul 16 17:44:44 2015 UTC,"I think it must be the mechanism by which we are conscious of certain metal processes (the conscious mind), and it could give consciousness to artificial neural nets as well.  A mind's eye.  LATER EDIT: Humans also occasionally go into useless loops, thinking over and over about some stupid thing without making any progress. Mirrors sometimes do this."
MachineLearning,3di2zh,k9triz,1 point,Thu Jul 16 15:26:35 2015 UTC,I see what you mean.   But feeding back directly into the sensors would influence what the network is directly seeing. It would be like some form of synesthesia as opposed to a mind's eye. Maybe recurrent connections slightly further up the hierarchy or even within the associative layers themselves could do the job.
MachineLearning,3di2zh,sharp7,1 point,Fri Jul 17 08:01:51 2015 UTC,"Synesthesia, as defined by Wikipedia, is a neurological phenomenon in which stimulation of one sensory or cognitive pathway leads to automatic, involuntary experiences in a second sensory or cognitive pathway.  I fail to see why feeding it back into a sensor specifically and consistently used for this would cause synesthesia."
MachineLearning,3di2zh,sharp7,1 point,Fri Jul 17 11:19:29 2015 UTC,"There was a paper at ISCA 2015 which very loosely used this approach called RUMBA for approximate computing.   Essentially, they would train another ML algo to predict when the neural network would have inaccurate predictions.   This can be extrapolated and is similar to the idea that you present of feeding layer data as input, but in this case to ANOTHER neural network."
MachineLearning,3di2zh,Powlerbare,2,Thu Jul 16 20:19:48 2015 UTC,"As I understand it, RUMBA reads the output of the NN, analogous to the primary motor cortex in humans.  What I am proposing here is reading the central associative layers, like the frontopolar prefrontal cortex."
MachineLearning,3dguqq,Kendama_Llama,2,Thu Jul 16 04:09:57 2015 UTC,I saw this used at a talk a few weeks ago! I think it was called symbolic regression and it built the equation up gradually using a syntax tree. It could also be regularized to make the equations simpler. I unfortunately cannot recall the names of the authors.
MachineLearning,3dguqq,InfinityCoffee,1 point,Thu Jul 16 06:20:22 2015 UTC,I remembered seeing this about 5 years ago using something like a genetic algorithm or parsimony-based optimisation. I think it may have been this one: http://symbolicregression.com/sites/SRDocuments/NonlinearityPreprint.pdf
MachineLearning,3dguqq,fhuszar,2,Thu Jul 16 07:23:14 2015 UTC,"Your problem as I understand it:  Deriving symbolic expression of mathematical function from finite set of examples.   Straight forward solution:  For any finite set of input output pairs there is infinite number of possible functions. Straight forward solution would be to use polynomial interpolation to find polynomial function that goes trough given set of examples. If you have n+1 examples, you can find  polynomial of degree at most n for those points.   Symbolic regression  I think you  want to find very general  simple  solutions that is not necessarily polynomial.  This problem domain is called symbolic regression (finding mathematical expressions to find the model that best fits a given dataset).  Symbolic regression programs:    Eureqa  http://www.nutonian.com/ https://sites.google.com/site/gptips4matlab/ http://cran.r-project.org/web/packages/rgp/index.html   These programs are often  computationally intensive and the underlying algorithm  is often something like   genetic programming (glorified search)."
MachineLearning,3dguqq,nabla9,2,Thu Jul 16 10:51:16 2015 UTC,"There is a Python library, which tries to accomplish this via genetic programming: https://github.com/trevorstephens/gplearn"
MachineLearning,3dguqq,marcuniq,1 point,Thu Jul 16 18:03:00 2015 UTC,"Try Gene Expression Programming, but you'd need much more training data than one pair of input output data."
MachineLearning,3dguqq,snigfargle,1 point,Thu Jul 16 05:00:56 2015 UTC,Do you think it'd be possible to use something like a gp using polynomials as your model hypothesis space to derive a distribution over equations using even just one data point? Usually bayesian approaches are more amenable to small data and have a built in occams razor like mechanism to prefer a balance between parsimonious explanations and felicitous explanations
MachineLearning,3dguqq,Articulated-rage,1 point,Thu Jul 16 06:13:44 2015 UTC,What do you mean one data point? Say you get input x and output y. Wouldn't the simplest and most optimal solution to that be y=x?
MachineLearning,3dguqq,r4and0muser9482,1 point,Thu Jul 16 12:14:14 2015 UTC,"There's actually an infinite amount of answers with a distribution over the coefficients. Though,  I guess there's no sampling noise in this example."
MachineLearning,3dguqq,Articulated-rage,1 point,Thu Jul 16 12:42:46 2015 UTC,"When you restrict your search space to polynomials there is no need for genetic programming because your search space is continuous.  Simplest solution: use [x1, x2, x3 ,.., xn ] as your feature, where n is the highest degree you want to allow and fit a Lasso regression.  Learning anything from one data point doesn't make sense though."
MachineLearning,3dguqq,duschendestroyer,1 point,Thu Jul 16 14:31:56 2015 UTC,"Will respond more later but I wasn't advocating for genetic programming.  I meant gaussian processes.  One datapoint is a question of capability.  What information do you learn from one data point.  In a bayesian framework,  you can answer this question.  Then,  it's an inductive step to N data points.  You're right,  the easiest is the regression and let the coefficients determine which polynomials are active.  Lasso is the one that pushes things to 0 right?   I personally like the grammar idea.  The thought that mathematical functions (polynomials,  trig functions,  etc) are composed and doing grammar induction (fairly well studied) from patterns of i/o pairs would be neat. But it's more of an exercise in elegance."
MachineLearning,3dguqq,Articulated-rage,1 point,Thu Jul 16 18:45:26 2015 UTC,"If you think about a latent function space as generating your data,  and complex functions as being compositions of simpler functions,  and compositions of functions as the mechanism of a grammar,  you could do inference to the most likely explanation given uniform priors on all function atoms and compositions.  This vaguely sounds like something Josh tenenbaum would have a paper on.  Check out his language of thought paper.  I only remember for sure Jacob Feldman being on the paper.  Griffiths and Goodman or maybe Steyvers may have also been on it."
MachineLearning,3dguqq,Articulated-rage,1 point,Thu Jul 16 06:17:30 2015 UTC,"Genetic Programming (which I think is similar to gene expression programming, although i'm not too familiar with that). It involes evolving a program, or a math function in this case. The eureqa tool is very good at this, and free to use for a trial (but very expensive thereafter)."
MachineLearning,3dguqq,simonhughes22,1 point,Thu Jul 16 22:57:24 2015 UTC,linear regression..
MachineLearning,3df6ps,dominosci,4,Wed Jul 15 20:09:23 2015 UTC,That's what LSDnet does. Youtube recording.  The problem with it is it doesn't have the diversity of patterns that appear in deep dream images. However the effect can be really cool. For example an entire screen full of insect parts.  Also the Deep Visualization Toolbox does this.
MachineLearning,3df6ps,Noncomment,3,Thu Jul 16 03:36:21 2015 UTC,"The idea sounds reasonable.  One way to get that effect would be to train on cat faces only  (that might be the easiest way).  But here you're basically trying to make the network forget non-cats.  So conceptually that would mean getting some cat faces from ImageNet and figuring out which top-layer neurons are being highly activated, then disabling all top-layer neurons except those.  Then you have a (sort of) cat-face only network.  Not sure of the code for detecting/disabling features, though."
MachineLearning,3df6ps,woodchuck64,1 point,Wed Jul 15 21:49:15 2015 UTC,"I've done that and put the results online here. Now please find the feature map that results in cats... you see, it's not that easy.   But the top layer has neurons for all kinds of things, right? There's a neuron that turns on for a cat face and one that turns on for german shepherds or whatever.   There are also many units that detect multiple things at once, e.g. humans and bicycle wheels, humans and birds, dogs and whiskey bottles etc."
MachineLearning,3df6ps,NasenSpray,2,Wed Jul 15 21:16:35 2015 UTC,"I was under the impression that these neural networks were able to identify high-level features like a cat, no? I mean, isn't that what this article is all about?  If a neural network can tell us ""there is a german shepherd in this picture"" it must have some neuron that's turning on indicating that. Again, I'm asking out loud to figure out where my reasoning goes wrong.  So, is it that the neural networks capable of recognizing german shepherds are too big to work with or something? Is that some other kind of neural network? Are those networks not available publically?  I completely understand that you can't generate a picture of a german shepherd from scratch. That makes sense. But why can't you take an existing picture of the Taj Mahal, why can't you tweak it with backpropagation to increase it's ""german shepherdness""? Why maximize a whole layer instead?"
MachineLearning,3df6ps,NasenSpray,1 point,Wed Jul 15 21:28:08 2015 UTC,"If a neural network can tell us ""there is a german shepherd in this picture"" it must have some neuron that's turning on indicating that. Again, I'm asking out loud to figure out where my reasoning goes wrong.   ""german shepherd"" only exists at the output. Lower layers don't necessarily have this 1:1 mapping. There are units that detect eyes, dog-like shapes, fur, texture etc. It's this combination of features which encodes that there is a dog in the image.  Edit: this is what the activations look like for a puppy at the last next to last pooling layer"
MachineLearning,3df6ps,NasenSpray,3,Wed Jul 15 22:00:10 2015 UTC,"""german shepherd"" only exists at the output.    Isn't the output read from a layer of neurons? It's called the output layer, right? If so, then just take the actual last layer (not the next to last one) find the neuron that we've trained to detect a german shepherd and maximize that."
MachineLearning,3df6ps,NasenSpray,3,Wed Jul 15 22:49:27 2015 UTC,"That doesn't lead to DeepDream images:  http://i.imgur.com/me86gG0.png 0.835948 Pembroke, Pembroke Welsh corgi  http://i.imgur.com/598Ak0S.png 0.991049 cheeseburger  http://i.imgur.com/m4aG9ao.png 0.990955 goldfish, Carassius auratus    DeepDream really only works using the layers in the middle of the network where there is no easily identifiable ""german shepherd"" neuron."
MachineLearning,3df6ps,FredFnord,2,Wed Jul 15 23:03:24 2015 UTC,AH! Ok. This makes a lot more sense. Using the whole layer gets around the regularization problem. Thanks!
MachineLearning,3df6ps,NasenSpray,3,Wed Jul 15 23:07:39 2015 UTC,"No no, that's not the whole story. Look through the images in here: https://drive.google.com/folderview?id=0B7NQ5tsE8AHbfmd3WnlKbXdrZHNPc1dETTBwSmF3Y0FXbjNMakNHYmZlaFc3dDYxTEF4eDA&usp=sharing Those were all generated by maximizing a single feature map of the respective layers. Compare those in ""inception_4a_output"" with those in ""inception_5b_output"". It works for the lower layers but gets increasingly blurry for the higher ones.  edit: correct link"
MachineLearning,3df6ps,FredFnord,2,Wed Jul 15 23:20:42 2015 UTC,"""Please sign in."""
MachineLearning,3df6ps,NasenSpray,1 point,Wed Jul 15 23:42:41 2015 UTC,This one? https://drive.google.com/folderview?id=0B7NQ5tsE8AHbfmd3WnlKbXdrZHNPc1dETTBwSmF3Y0FXbjNMakNHYmZlaFc3dDYxTEF4eDA&usp=sharing
MachineLearning,3df6ps,NasenSpray,1 point,Thu Jul 16 00:16:51 2015 UTC,"Better, thanks."
MachineLearning,3dczvc,dustintran,1 point,Wed Jul 15 08:33:30 2015 UTC,"Thanks for the overview, hate that I missed it this year. Have there been any discussion during the deep learning talks on the results from ""intriguing properties of neural networks"" by Szeged?"
MachineLearning,3dhbvc,john_philip,1 point,Thu Jul 16 07:26:47 2015 UTC,How did they miss the opportunity for /r/tificial?
MachineLearning,3dhbvc,say_wot_again,2,Thu Jul 16 08:20:24 2015 UTC,Too confusing imho
MachineLearning,3ddal1,pseudopotential,3,Wed Jul 15 11:02:32 2015 UTC,"Is anyone from the lab going to work at IBM full time or part time?    Or is the partnership more about data sharing?  If so, will the work still be publishable?"
MachineLearning,3ddal1,alexmlamb,3,Thu Jul 16 04:39:20 2015 UTC,"I am there (Yorktown Heights / TJ Watson) now as an intern.   Other people are coming later. There is also some data sharing in my own work so far but mostly trying ideas on both regular benchmarks and IBM data.  In general Yoshua promotes open research, open code, and publication in stuff like Arxiv because it ends up better for everyone in the long run.  From what I have seen so far this partnership is no exception."
MachineLearning,3ddjas,compsens,1 point,Wed Jul 15 12:45:14 2015 UTC,"All right. I don't see how those graphs relate to the problem of Reddit at all. Along with that, there's no solid data analysis... It just sounds like showerthoughts of a statistician.."
MachineLearning,3ddjas,devDorito,1 point,Thu Jul 16 06:44:07 2015 UTC,"these phase transitions occur in many matrix factorizations used in ML. The point of the entry was to say that one of these factorization (sparse clustering) is known to have these phase transitions too and that whatever Reddit has to do, they  ought to make sure that the right measure is used to see if they are making progress.   No statistician wrote that blog entry."
MachineLearning,3dfohk,waronxmas,2,Wed Jul 15 22:17:20 2015 UTC,Chi-square analysis of your features is a good start.  http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/
MachineLearning,3dfohk,iamtrask,1 point,Wed Jul 15 23:40:48 2015 UTC,"Hmm an interesting idea.  Though I can't think of a way to do this without clustering my high-dimensional (and probably disparate) data to make similar features appear the same.  Alternatively, I could attempt dimensionality reduction prior to clustering which might help, but again, I am worried that the fact that a large portion of my features are probably useless for classification will result in problems with the reduction.  Maybe I'm not thinking of this correctly.  Can you add some insight about how I could approach this with chi-square analysis?"
MachineLearning,3dfohk,captain_smartypantz,2,Thu Jul 16 20:05:47 2015 UTC,Discriminant analysis? Qda probably over lda
MachineLearning,3dfohk,jetxee,2,Thu Jul 16 00:43:29 2015 UTC,"Generally, overlapping features are not a problem, but it may mean that the classes are not linearly separable.  Consider the XOR problem. Class 0 samples are (0, 0) and (1, 1). Class 1 samples are (1, 0) and (0, 1). All features overlap, but given the right method (a decision tree, an RBF SVM, a two-layer perceptron), this is not a problem. The method should be able to consider combinations of features."
MachineLearning,3dfohk,Nixonite,1 point,Thu Jul 16 08:19:10 2015 UTC,"Curious, how many features and how many records are you working with?"
MachineLearning,3dfohk,Nixonite,1 point,Thu Jul 16 01:02:39 2015 UTC,"At the moment, really not many--a few thousand features randomly sampled from about 100 clips.  I'm just thinking ahead to the bigger problem."
MachineLearning,3des1e,beokabatukaba,1 point,Wed Jul 15 18:28:57 2015 UTC,"this is fun problem...  some observations:  -it'll be easier to look within a given instrument to start.  There will be large differences in frequency spectrum between instruments, however there's probably very small differences between good and bad timbre within instruments.  Removing the between instrument variance first will make it easier to identify within instrument variance attributable to timbre.  -there's some psychological literature looking at how consonance and dissonance is related to the ratio between a tone's funadmental and it's higher formants, sometimes called 'Harmonicity'.  The basic idea being, if the ratio is (near) an integer value it is more harmonic.  A good starting review to try and dig further:  http://www.sciencedirect.com/science/article/pii/S0378595509002366  or some info and possible software here if you can't get past the paywall:  http://www.fon.hum.uva.nl/praat/manual/Harmonicity.html  Extracting these ratios could be helpful to separate good and bad timbre for a given tone.  -if you don't necessarily need to interpret what qualities divide good from bad timbre, neural networks could be useful here.  The display you show students would be the activity several layers into the network that has only two units, you just use the activation of these two units projected onto a 2d space that has the target (i.e. good timbre) region already highlighted."
MachineLearning,3dfnro,feedthecreed,3,Wed Jul 15 22:11:43 2015 UTC,"If you've got a lot of examples coming in, not being able to see an example multiple times doesn't matter. If you don't have a lot of samples, it doesn't matter how many times you train on them anyways. No overfitting and no training time sounds like a bad situation for a neural network. The learning method that springs to mind for those conditions is rote memorization."
MachineLearning,3dfnro,Tesseractyl,1 point,Thu Jul 16 04:00:42 2015 UTC,The distribution tends to shift (sometimes drastically) and at unpredictable times. I would prefer to have the model decide when it should memorize and when it should forget.
MachineLearning,3dfnro,tariban,2,Thu Jul 16 16:42:48 2015 UTC,Try taking a look into data stream mining -- there are tons of models that can be used in situations like this. Neural networks can be used in this way by just using SGD and not worrying about the idea of an epoch.
MachineLearning,3dfnro,nkorslund,2,Thu Jul 16 11:03:35 2015 UTC,"Is this some kind of online learning problem, where you have a constant stream of new samples? If that's the case, you should be fine with just running each sample through the network once (maybe batch them up into small chunks for efficiency) and letting it learn on the go. Given enough data you'll get decent results eventually.  If it's NOT the case that you have a stream of lots of data, then I'm wondering under what other circumstances you would think ""overfitting isn't a concern"". In this case you'll have to provide some more details on the problem."
MachineLearning,3dfnro,iamtrask,1 point,Thu Jul 16 12:10:12 2015 UTC,"Yes, it is an online learning problem. But the rate the distribution of input samples changes isn't constant. I would imagine just running each example through the network with the same learning algorithm the whole time won't adjust appropriately to large shifts vs small shifts."
MachineLearning,3dfnro,khasiv,1 point,Thu Jul 16 16:46:21 2015 UTC,Might be useful to look into Batch Normalization.   http://arxiv.org/abs/1502.03167
MachineLearning,3dfnro,despardesi,1 point,Wed Jul 15 23:47:00 2015 UTC,"You can increase the learning rate. I haven't had a ton of luck manipulating the learning rate but if you're very concerned about new examples needing to change the model substantially then playing around with that would be what I would try first.  Otherwise, if you use neurolab or some other neural net package it can tell you the number of epochs it took before it learned the concept."
MachineLearning,3dfnro,Powlerbare,1 point,Thu Jul 16 16:15:32 2015 UTC,"It's an interesting question; and I'd guess a higher learning rate will help you adjust to the changing distribution.  For others: say, for example, you're trying to ""learn"" a sinusoidal wave. The most recent examples will be the most meaningful; the old ones will become useless after a little bit. So how do you keep forgetting the past as you learn the present to predict the immediate future?"
MachineLearning,3dfnro,alexmlamb,-1,Fri Jul 17 05:07:28 2015 UTC,this is called one shot learning - i dont know much about how practical it is so you may have to do some independent research
MachineLearning,3dfnro,Powlerbare,3,Wed Jul 15 22:39:49 2015 UTC,I think that one-shot learning means that you only have one example of a class.
MachineLearning,3dfnro,MusicIsLife1995,0,Thu Jul 16 02:26:22 2015 UTC,can you give me a source or a paper that claims this - i have never understood it that way
MachineLearning,3dfnro,rcanepa,0,Thu Jul 16 21:37:28 2015 UTC,"You have to overfit in this case. Think about it. If I give you this pattern 1, 2, 4.. what do you think models it?  A line? A quadratic equation?   To be honest both can model that specific amount of data, but you don't really know for sure until you get a larger amount of data to verify your assumptions. i.e. if you see a 6 next, then you know it's more likely to be a line with slope 2.  The problem is that you're not us what type of natural behavior we're trying to model. The more information we have, the better assumptions we can make."
MachineLearning,3defmn,john_philip,1 point,Wed Jul 15 17:03:11 2015 UTC,"I don't think the analogy is very relevant, and I certainly don't think NN are a ""framework for re-examining the brain from a mathematically regimented standpoint"".   No need to put formulas. Connectionnism has nothing to do with that. If we can simulate a brain, we could surely learn a lot about our own behavior. We could place several brains into interaction and see how they act. But I really doubt that having  a connectionnist model in mind and try to simulate it inside your own brain will make you more aware of your own thought processes...  But really, ethics nowadays is still such a black and white field ? (I have no idea)  In my opinion, machine learning can be a very fruitful field if you want to do philosophy, especially if you are into epistemology. What is knowledge? What can we learn from our external stimuli? Is there something truly random? I think these are the most fascinating questions one could ask. I just don't see an interesting parallel with ethics, especially on toy problems as the trolley problem."
MachineLearning,3defmn,bhmoz,1 point,Wed Jul 15 19:53:06 2015 UTC,"I just don't see an interesting parallel with ethics, especially on toy problems as the trolley problem.   I thought it was quite obvious from the article:   Ethics fundamentally concern themselves with the rightness of decisions, so an ethical system can be formalized as some function on the domain of the infinite set of scenarios and decisions that maps each of those (scenario, decision) coordinates to a level of righteousness.  [...] Under this analogy, if we (temporarily) assume all decisions are either right or wrong, (discretizing the z-axis), learning ethics becomes isomorphic to the common task of binary classification from Machine Learning.   I have no clue where you get your ""simulating a brain"" notion..."
MachineLearning,3defmn,pqnelson,1 point,Wed Jul 15 21:53:52 2015 UTC,"I thought it was quite obvious from the article:    should have emphasized interesting. To me, the formalisation is, at best, unnecessarily complicated (see your own quote).    I have no clue where you get your ""simulating a brain"" notion...    There you go:   Machine Learning, and the Neural Network algorithm in particular, provide a framework for re-examining the brain from a mathematically regimented standpoint. [...] Next time you make a decision which causes you to think twice, try to examine the inner workings of your own mind.   This imply that thinking about your own thought process as a NN is useful. So basically simulating a NN in your head is supposed to help you think? I don't think so."
MachineLearning,3defmn,bhmoz,0,Thu Jul 16 07:28:19 2015 UTC,"I agree.  Further, this article has drastically oversimplified ethics. Among many other problems with the piece, there certainly are more than 2 classes of ethicists!"
MachineLearning,3defmn,spyke252,1 point,Wed Jul 15 20:04:23 2015 UTC,"With the caveat below, I think the idea is quite brilliant.   Our behavior has been meticulously refined for our entire lives performing an optimization algorithm machine learning researchers have yet to invent. Given a situation, we perform a series of deeply non-linear transformation of the input we receive from our senses; combinations of contextual cues and instinctual and learned motives and ideas provide an intuition of rightness, the source of our decisions.   I would side with Hume and Pinker that the vast majority  (at least in terms of information quantity) of moral behavior capacity is programmed by evolution in human DNA and expressed in neurons at development in our uniquely social brains.  So the huge difficulty with a neural network algorithm for ethics is pre-programming it as a ""social being"" first.    A blank neural network algorithm subjected to social experiences with the goal of ""maximizing happiness""  can do so, but it won't ever behave like a human since it won't come preweighted with deep, fundamental concerns for (from Haidt's list) Care/harm, Fairness/cheating, Loyalty/betrayal, Authority/subversion, Sanctity/degradation, and Liberty/oppression.    Can a neural network learn human moral intuitions?  I'm not sure here since it clearly took millions of years for evolution to come up with these and it is far from clear whether they are contingent or accidental.  A better strategy seems to be reverse engineering the moral brain in some fashion, which moral psychology tries to do, but maybe there's a better way."
MachineLearning,3dev5e,chaddjohnson,3,Wed Jul 15 18:50:56 2015 UTC,"A quick note: I think the number of representations will also depend on how you choose to encode them. In your example, you have a ""shirt"" classification and a ""striped shirt"" one. If, instead, you had a ""stripeness"" classification, you could combine it with ""shirt"", ""pants"" and even ""horse"" to get classifications for ""striped shirt"", ""striped pants"" and ""zebra"". That is a much more efficient way to represent those concepts and will allow the network to learn more of them, I think."
MachineLearning,3dev5e,valexiev,1 point,Wed Jul 15 20:55:45 2015 UTC,"This sounds like a very a good idea, though I haven't seen any significant examples/discussions of it? The closest I can think of is Hinton's dark knowledge stuff, which is essentially means starting with distinct classes, and letting the network learn the connections between them on its own.  In either case I'd guess you'd end up with output targets that are not strictly one-hot vectors. Maybe combining the methods would be even better (so the system itself could figure out that a zebra is also a 'striped+horse'.)"
MachineLearning,3deeyh,BigMakondo,7,Wed Jul 15 16:58:33 2015 UTC,"Hidden or latent variables are ubiquitous in statistics and machine learning. The physical data from the real world is 'observed'. Most statistical models use additional hidden or latent variables in order to simplify learning useful distributions over the observed data. Many times hidden/latent variables are mathematical necessities. Other times, they might be the quantities we are interested in and introducing them as latent variables allows the use of bayesian and other probabilistic machinery. Hidden variables in neural nets are quantities that are not observed and therefore hidden from the observer.   No idea when it was first used though."
MachineLearning,3deeyh,sidsig,1 point,Wed Jul 15 17:14:45 2015 UTC,Whats the best book on understanding hidden variables and how they work in HMM's.
MachineLearning,3deeyh,j_lyf,4,Thu Jul 16 05:10:52 2015 UTC,Maybe Geoff Hinton?  https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyrzcr
MachineLearning,3deeyh,mathsive,1 point,Wed Jul 15 17:01:37 2015 UTC,"Yup, in one of his neural networks course videos he also said he picked it because he thought it sounded ""mysterious"". The guy has a flair for good brand-name marketing of his concepts.  But the name also makes sense: if you consider an NN to be a ""black box"" model, then you see the inputs (because you provide them), you see the outputs, but you don't see (or need to see) anything in between, thus they are the hidden parts of the black box."
MachineLearning,3deeyh,nkorslund,2,Thu Jul 16 12:39:06 2015 UTC,"In a classic multilayer perceptron, there are three layers: input, hidden and output. Inputs and outputs are visible, the middle layer is ""hidden""."
MachineLearning,3deeyh,Foxtr0t,2,Wed Jul 15 17:15:10 2015 UTC,"My understanding has always been that they were viewed as a kind of a deterministic analogue to ""hidden variables"".  The link to Geoff Hinton's comment seems to suggest that was it."
MachineLearning,3deeyh,dwf,1 point,Wed Jul 15 18:13:37 2015 UTC,"In Hinton's Coursera course, he also made a slight hint that he named it ""hidden"" in early papers because he liked the name... as I recall when he was making the course he apologized for its confusion with statistical models (HMM?). Not sure which video that was though and it's been a while since I watched them."
MachineLearning,3dd0cq,Nixonite,-3,Wed Jul 15 08:40:15 2015 UTC,Please don't host pdf files illegally.
MachineLearning,3dd0cq,procarastinizer,3,Wed Jul 15 17:00:22 2015 UTC,It's just Wikipedia articles in pdf form.
MachineLearning,3dd0cq,procarastinizer,1 point,Wed Jul 15 17:21:32 2015 UTC,Alright then. Never mind. And thanks for hosting.
MachineLearning,3de74a,Stickypatrol,2,Wed Jul 15 16:02:22 2015 UTC,I'm assuming you are considering buying second hand books and in that case http://www.alibris.co.uk/ is one of the best resources I found
MachineLearning,3de74a,mmeruz,1 point,Wed Jul 15 16:28:18 2015 UTC,"second hand is fine, as long as they are in semi-decent shape. Thanks for the link!"
MachineLearning,3de74a,nerdsarepeopletoo,2,Wed Jul 15 16:30:14 2015 UTC,"I've managed to find a few publishers in China that will ship you a new, softcover version of some text books (via amazon for example) for about 1/3rd the cost of the new hardcover in North America. The pages are of the thinnest, most fragile of newsprints and the powdery ink will stain your fingers as you flip through... but the savings...  You really have to put in some internet legwork to find them, though"
MachineLearning,3de74a,farsass,1 point,Wed Jul 15 16:32:05 2015 UTC,"Interesting idea, are any of those websites english? if so, got any hints about where to look ? :P nvm I misread your comment haha"
MachineLearning,3ddmxk,compsens,2,Wed Jul 15 13:20:53 2015 UTC,That would certainly make for a very interesting workshop!
MachineLearning,3db8y3,kjw0612,3,Tue Jul 14 22:49:11 2015 UTC,If you're looking for more theory: http://www.normalesup.org/~scornet/
MachineLearning,3db8y3,kjearns,1 point,Tue Jul 14 23:53:11 2015 UTC,added two papers from it
MachineLearning,3db8y3,piesdesparramaos,3,Wed Jul 15 03:11:20 2015 UTC,to include: https://github.com/glouppe/phd-thesis
MachineLearning,3db8y3,compsens,1 point,Tue Jul 14 23:43:56 2015 UTC,added
MachineLearning,3db8y3,micro_cam,1 point,Wed Jul 15 03:01:01 2015 UTC,great ! I add your page to the highly technical page list: http://nuit-blanche.blogspot.com/p/reference-page.html  also some of my blog entries on random forests can be found under this tag: http://nuit-blanche.blogspot.com/search/label/RandomForest
MachineLearning,3db8y3,micro_cam,2,Wed Jul 15 13:05:03 2015 UTC,"that's awesome! Thanks for your addition. By the way, the page is maintained by Jiwon Kim and Jung Kwon Lee (please add Jiwon Kim :)). Will read your blog entries to learn more about random forest!"
MachineLearning,3db2wb,Velm,2,Tue Jul 14 22:05:13 2015 UTC,"You should definitely study more algorithms in a CS department.   There are many ideas in this space that are super-useful across all of ML that aren't sufficiently covered in a Math or Stats curriculum (and they are generally easier concepts/proofs to learn than advanced math IMO as a math+stats major), such as:   dynamic programming, hashing, graph theory, numerical linear algebra, computational complexity, approximations, geometric computing, combinatorial optimization.  Also, some basic understanding of Systems is definitely good to have (although may not be too useful if your interests are theoretical) including topics like: parallelism, memory, databases, distributed computing.  Finally, information theory is another very useful subject that is probably taught in a CS department rather than Math/Stat.  I view both Statistics and ML as sciences which study data, with the minor distinction that Statisticians emphasize mathematical modeling and provable recovery of the models (important in scientific/financial-analysis) while the ML community is often more algorithmically concerned with ensuring the models are computationally tractable and work well in practice (important in designing predictive/smart apps).  This distinction is merely based on the existing division of the two communities of researchers rather for than any fundamental reason that I can see, and you can find numerous exceptions in both groups."
MachineLearning,3db2wb,iidealized,1 point,Wed Jul 15 00:19:59 2015 UTC,"This is exactly the type of response I was hoping for. Thank you.  It looks like a lot of the topics you mentioned are actually handled by my university's math department in classes like Operations Research, Numerical Analysis, and Numerical Linear Algebra. I'll also look into taking a Systems course from the Computer Science department."
MachineLearning,3db2wb,rawr_777,2,Wed Jul 15 04:46:35 2015 UTC,"This book: https://en.wikipedia.org/wiki/Introduction_to_Algorithms covers pretty much everything a computer science undergrad will learn about algorithms. If you find one second-hand, I'd highly recommend picking it up and reading it (at least parts of it - it is a very large book). With a background in math/stats, and a solid understanding of algorithms, I think you should be set to study machine learning."
MachineLearning,3db2wb,goblin_got_game,1 point,Wed Jul 15 02:21:19 2015 UTC,That looks like a great resource. Thanks.
MachineLearning,3db2wb,TheInfelicitousDandy,1 point,Wed Jul 15 04:51:50 2015 UTC,"Databases (sql and non) and distributed systems so you can work with big data. Algorithms and software engineering so you can write fast, clean, reusable code to share with others and make your research reproducible. Javascript and data visualization so you can make engaging, interactive web apps to market your research to a wider audience."
MachineLearning,3db2wb,TheInfelicitousDandy,1 point,Wed Jul 15 03:55:50 2015 UTC,"Yes its possible to ignore some prereqs.  However,  1) If you are trying to get into a CS department for grad school you're going to need to meet the basic requirements, which as you have seen varies from school to school and may not be ignored depending on the school.  My guess is a lot of ML profs will be more interested in you given your math and stats background than for CS you know but you still need to get past the admins.  I would focus on getting the basic requirements down before focusing on CS courses that will help you in ML and you might be surprised which ML courses help you, for example knowing about operating systems and computer architecture should let you understand why GPU and vector based ML algorithms are popular.    2) Also remember that in a CS department you will need to pass CS grad classes, and depending on department policy they might have some nasty breath requirements that force you to take classes totally unrelated to ML.  Not knowing what is going on in a grad class is very stressful.  Also departments might let you take 4th year class to cover you requirements, so if you miss say databases in undergrad you can pick it up later if you feel you need it.      That being said I have taken so many useless classes (I had 3 majors) so I very much understand your desire to try to focus in on one area."
MachineLearning,3db2wb,MusicIsLife1995,1 point,Wed Jul 15 18:28:37 2015 UTC,"I appreciate your feedback. I'm a bit wary going to programs that are very strict about their prerequisites; if a program is too focused on computer science concepts, I don't think I would want to be there anyway. Ideally, I will be applying to computer science-heavy stats programs and stats-heavy computer science programs.  I think now my plan will be to meet somewhere in the middle by taking the CS classes that I am currently qualified for and ignoring the ones which have prerequisites I have not taken. These would be Theory of Computation, Programming Languages, Algorithms, and Parallel Computing.  Does that sound reasonable?"
MachineLearning,3db2wb,gsmafra,1 point,Wed Jul 15 19:32:52 2015 UTC,"Grad school is not really about the classes.  I have to take 8 over 6 years, which I think is the usual.  Classes should me like 5% of the grad school experience so don't pick a school based on that.  Ideally you pick a school based on it have a prof/profs you can work with whose work interests you.      I'd make sure you have an Intro to programming, data structs, and an algorithms course on your record.  All 3 are needed for ML anyways.  Any others are sugar.  You might like theory given your math background but it can be ignored (you should get a bit from an algorithms class anyways).  Parallel computing you can learn on your own I'd say and Programming languages can be ignored completely."
MachineLearning,3d7z4o,evc123,3,Tue Jul 14 05:39:42 2015 UTC,Sponsored* Looks like it already closed for this year.
MachineLearning,3d7z4o,newhere_,2,Tue Jul 14 12:36:40 2015 UTC,"2nd and 3rd leg are still open right ? You may not win, but would be an interesting exercise."
MachineLearning,3dcr6y,ocropus,1 point,Wed Jul 15 06:36:40 2015 UTC,"I used to work with tesseract and ocropus. Tesseract has more training data and because of this, your recognition performance is better than ocropus.   Ocropus has some good segmentation algorithms, but your char recognition and general performance is very poor."
MachineLearning,3d9vno,jrkirby,2,Tue Jul 14 17:08:25 2015 UTC,"I vaguely remember reading something about this, but I can't remember where unfortunately. I think the general conclusion was that Harr features are great for computational performance (ie. on embedded devices like cameras), but otherwise are significantly worse when compared to fully learned features. Which kinda makes sense, as the learned features could just learn the Harr features as a special case anyway.  Take this with a grain of salt though, someone please correct me if I'm wrong."
MachineLearning,3d9vno,nkorslund,3,Tue Jul 14 22:48:35 2015 UTC,"I cannot speak to the efficacy of deep learning face detection systems, but Viola-Jones (which employed the Harr setup) was the go-to algorithm for a long while, and it was fairly good. It's actually a bit deep itself as it develops a cascade of features, designed to quickly reject non-faces. You could also argue that it learns its features, except from a reduced space of binary masks."
MachineLearning,3d9vno,InfinityCoffee,2,Wed Jul 15 05:52:52 2015 UTC,"Minor correction: they're called Haar-like feature, not Harr, after Haar wavelet, which itself is named after Alfred Haar, a Hungarian mathematician.  Haar-like features exploit the integral image (or summed area table, whatever), and require only 6 to 9 points to calculate most of them (4 points per rectangle). So the basic Haar-like features can be thought of as linear combinations of 6, 8 or 9 values from the integral image.  In the neural network world we use a weights matrix to represent this kind of combination. What we have to enforce to make it similar to Haar-like features:   sparsity, lots of sparsity; there're W×H input variables, we need only 6 to 9 non-zero weights per row; there're too many possible features Haar-like features; Viola-Jones method relies on boosting to select them greedily, neural networks need to have a sparsity constraint of some kind weights values can be only +1, 0, or -1 or weight sum should be zero in every row; that's another constraint... weight matrix should have a very specific structure to match rectangular areas of the original Haar-like features as the features can cover the entire integral image, so the neural network layer has to be fully connected   The major difference between Haar-like features and convolutional networks is locality. Haar-like features are global (integral). They are 'blobs' in the image and their combinations. Convolutional NN features are local (convolutions with a small fixed-size kernel). They are 'edges' in the image and their combinations. But there is a kind of neural networks which are detecting 'blobs': see radial basis function networks.  So if I were to simulate Haar-like features in ANNs, I'd start looking for exotic regularization techniques. And a crazy idea: build an RBF network with a radial basis functions which uses infinity norm (a constant radius set is a square) and train it on integral images..."
MachineLearning,3d9vno,jetxee,1 point,Wed Jul 15 20:29:40 2015 UTC,"Thanks man! I was looking to see if there was any prior art on this with neural nets, which there seems not to be.  What I currently have in mind it just trying to feed an integral image into a neural net and see if it can learn faster (with less layers) than having fed it a regular image. Currently I'm not thinking of trying to use Harr-like Features.  You provide some good insights, like sparsity. I also have an idea about how to train really sparse neural nets, but maybe I shouldn't try too many new things at once.  Having thought about it further, I'm also worried about the precision. Floats have only 23 bits of precision, and worst case the highest numbers will be size of image2 * 255 which is over 23 bits for all images bigger than ~150 pixels. This means in the bottom corner there'll be an (unacceptable?) loss of precision (maybe ~6 bits of precision lost for 1000x1000 images). But maybe this won't matter to the network."
MachineLearning,3d9vno,jetxee,2,Wed Jul 15 21:05:56 2015 UTC,"For integral images you can use integer arithmetic. int32_t should be enough for most images. Keep in mind that Viola-Jones is usually trained on downscaled samples. Down-scaling implies much more precision loss. And I don't think anyone is training detectors on 1000×1000 samples. Anything bigger than 64×64 takes forever to train... (small sample size doesn't prevent from detecting objects of any size in images of any resolution).  Thinking again about it, it is possible to implement an ""Haar layer"". Its parameters matrix will have only 4 columns: x1, y1, x2, y2, which will correspond to top-left and bottom-right angles of the rectangle. Instead of usual matrix-vector multiplication, the layer will calculate I(x1,y1) + I(x2,y2) - I(x1,y2) - I(x2,y1) per every row of its parameters matrix. This is exactly the integral over a rectangular area. On top of this layer we may put a regular fully connected layer which will calculate linear combinations of rectangular integrals + bias (thus a superset of all possible Haar-like features). If Viola-Jones selected features by boosting, the neural network will be able to adjust features and change their weights through backpropagation. It may work.  What this network will lack with respect to Viola-Jones method is early rejection. The network will have to calculate all features per every sample while a cascading classifier may discard almost all negative samples after calculating only few features."
MachineLearning,3d8qdv,Mad-economist,15,Tue Jul 14 11:26:46 2015 UTC,"The community seems to have a lot of problems with Extreme Learning Machines, its authors are accused of disregarding prior art (possibly intentional) and of hyping/bad naming. I find it to be an interesting technology, good to have on your toolbox, but beware involvement with the group behind it (and using ""Extreme"" on anything you develop).  Personally I'm very skeptical about ""Friendly AI"" research and other Singularity themes. I believe that doing such research (actually mostly philosophy) right now is a big waste of time, and I believe my feelings are echoed by a lot of people on the community (hopefully).  Finally, on a general note, if you don't claim to have solved intelligence and you don't say some technology is gonna take over the world (especially if it's yours), you will be fine."
MachineLearning,3d8qdv,tabacof,14,Tue Jul 14 13:24:52 2015 UTC,"I believe that doing such research (actually mostly philosophy) right now is a big waste of time,    I think Andrew Ng put it best on the Talking Machines podcast by saying (paraphrased) ""I worry about evil AI like I worry about overpopulation on Mars"""
MachineLearning,3d8qdv,zmjjmz,1 point,Tue Jul 14 15:10:20 2015 UTC,"The thing is, all indications are that we will have computers that will be able to run neural nets of the approximate dimensions of the human brain within the next 15 years. (Does anyone not think this is true -- i.e. that progress on parallel computation is going to stall out in the next few years?) There is no indication when, if ever, colonizing mars will be technologically feasible or commercially viable. So I think it's a disingenuous comparison -- arguing with scorn instead of with arguments. I think it's disappointing and a little anti-intellectual. I can't tell if he's sincere in the argument or thinks of himself as using misinformation to protect his industry from the perceived risk of scrutiny or another AI winter.  Fair arguments would be, e.g., that (1) even after sufficient hardware is in place, we still won't know what kind of software to run to generate intelligence, or (2) even if general AI really is right around the corner, it's premature to think about how to make it ""safe"" before we have any idea how it is going to work. I think both of those arguments are debatable but at least they're fair.  (For what it's worth, I do think that the philosophical attempts to prepare for strong AI are a waste of time, because I think philosophy is the wrong discipline to tackle a fundamentally technical problem.)"
MachineLearning,3d8qdv,VelveteenAmbush,4,Tue Jul 14 19:20:14 2015 UTC,"computers that will be able to run neural nets of the approximate dimensions of the human brain within the next 15 years   You go on to address my concern, but this argument starts to sound like a ""structure equals function"" argument.  I think there's cause to hedge this argument and couch it in the assumptions that methodologies will become as sophisticated as need be.    arguing with scorn instead of with arguments   I think the reason he uses scorn is because it's distracting us from a very,very real and important discussion.  In that episode of talking machines, he goes on to discuss how AI will affect jobs.  There's also the conversation we should have concerning privacy, capability for desctruction, etc.  While I do agree he's being a bit flippant in his comparison, I don't think it's completely unjustified.   The privacy and job concerns are so much more important and they are talked about a fraction as often as the ""evil ai"" concern.    the philosophical attempts to prepare for strong AI are a waste of time, because I think philosophy is the wrong discipline to tackle a fundamentally technical problem   Some philosophy questions end at ""should we do  it?"".  Given that the answer to this is yes, it does become primarily engineering.  But, I still think there are scientific questions (as opposed to the 'lets just build things really well' engineering questions), such as: what is the right way to limit the freedoms of a machine intelligence?  These concerns are more of those from a design standpoint, and maybe experimental simulations.  Do we want it to just mimic the ethical distribution of humans?  There is some research that looks at computational models of human ethics (I tried to find a person I know's papers, could only find her masters dissertation). Or will we hold it to a higher standard of ethics than we hold ourselves?  I think these questions are fundamentally interesting and outside of the realm of engineering.   p.s. fun fact: I started writing a response to this on my phone, then realized it was you. Given previous interactions, I've switched to computer."
MachineLearning,3d8qdv,Articulated-rage,3,Tue Jul 14 20:44:13 2015 UTC,"I think the reason he uses scorn is because it's distracting us from a very,very real and important discussion.   It's distracting us in the sense that it's actually hindered progress in the field? I doubt that it has; if it's made any difference at all, it's probably increased the amount of attention and funding. I think he just has a vague sense that it's undignified -- that it's offensive to the prestige of his pursuit that people look at his work and start talking about Terminator. And to his credit, most amateur speculation about strong AI is undignified and offensively uninformed -- but it's still unfair to claim that strong AI is as far off as a Mars colony.   But, I still think there are scientific questions (as opposed to the 'lets just build things really well' engineering questions), such as: what is the right way to limit the freedoms of a machine intelligence?   At this stage of human progress, it's not a scientific question, because there's no scientific method involved in addressing it, and it's not an engineering question, because we just aren't far enough along to begin engineering systems to deal with these questions. It's a philosophical question, because the only toolkit that currently exists to address the problem is writing down ideas and debating them (which is what MIRI and similar organizations are doing). And my opinion is that that approach is sufficiently unlikely to advance the ball that funding it now is a misallocation of resources."
MachineLearning,3d8qdv,VelveteenAmbush,2,Tue Jul 14 23:29:49 2015 UTC,"actually hindered progress in the field   well, that's not so much what I meant. Right now, some famous person says something vague about ""evil ai"" and suddenly the public is murmuring about it. You have people fervently warning about a jobless future and many aren't listening. So, it's not so much the field's progress, but the public perception, and thus, the reaction to the field.    my opinion is that that approach is sufficiently unlikely to advance the ball that funding it now is a misallocation of resources.   completely agree. I don't claim it's a viable research programme.  honestly, I think it's a topic best discussed over a beer with hand waiving at all of the hard assumptions.   but there's also a discussion that we should be having about the implications of our algorithms.   e.g. for privacy   as computer vision algorithms approach high performance on semantic tasks and with their already high performance on facial recognition tasks, the ability for their abuse from those who would watch us and assert power over us grows. for example, anyone who has ever posted on gonewild could be deanonymized. as complex network analysis gets better, it becomes possible to de-anonymize people in fairly anonymous networks (it's why netflix never had a second competition).  similar to computer vision downsides, as nlp gets better, the ability to spy on us in all aspects grows.  as we solve the implied meaning problem, we can start to get at what people mean when they leave out explicit details   I honestly have no idea what could/should be done, so I trudge on as a scientist, but doing nothing makes me feel a twinge of guilt.     I might also be reading too many dystopic novels."
MachineLearning,3d8qdv,Articulated-rage,2,Wed Jul 15 00:00:47 2015 UTC,"it's premature to think about how to make it    That's how I took his comments at GTC. From a practical point of view, what is he supposed to do? Sit down at his desk and...do what?  He was also critical of what he sees as a far off concern distracting from the near term concerns of what AI does to employment and inequity."
MachineLearning,3d8qdv,a_countcount,2,Wed Jul 15 18:33:39 2015 UTC,"The game-theoretic and decision theory parts could be quite useful, as well as figuring out value alignment for arbitrary agents.  Like, the people doing FAI research probably over-value it, but it's still far better than philosophy. They're not going to figure out much that adds to optimizing power, so you can easily ignore the field unless the problems seem innately interesting to you."
MachineLearning,3d8qdv,PM_ME_YOUR_PRIORS,1 point,Tue Jul 14 15:13:59 2015 UTC,"Like, the people doing FAI research probably over-value it, but it's still far better than philosophy.   It is philosophy."
MachineLearning,3d8qdv,VelveteenAmbush,1 point,Wed Jul 15 02:05:31 2015 UTC,Extreme Learning Machines   Their papers are also very poorly written.
MachineLearning,3d8qdv,avoidban,1 point,Wed Jul 15 00:00:22 2015 UTC,"It's because it's just a random projection + linear regression. No need to bring neural networks into it, and certainly no reason to call it ""extreme"""
MachineLearning,3d8qdv,avoidban,1 point,Wed Jul 15 01:50:12 2015 UTC,"But a neural network is just a learned projection (and a nonlinearity) plus linear regression. They are the same thing, it's just a question of how you construct your latent feature representation. So it's not a completely unjust comparison IMHO."
MachineLearning,3d8qdv,InfinityCoffee,1 point,Wed Jul 15 12:10:26 2015 UTC,"Indeed, a ""learned"" projection. Although I played with them a bit and think the key part behind calling them a neural method is because they use a bias. If you don't give the random layer a bias it loses almost all performance - but that's not what say, PCA would do."
MachineLearning,3d8qdv,avoidban,1 point,Wed Jul 15 23:50:03 2015 UTC,"Well, without biases all of your sigmoidal patterns are centered at zero, making it  problematic to model functions with variation across their support. But there is likewise nothing keeping ELMs from using random biases. (disclaimer: I am not a proponent of ELM, just playing the devil's advocate)"
MachineLearning,3d8qdv,InfinityCoffee,1 point,Thu Jul 16 05:56:32 2015 UTC,"Before dismissing FAI research as ""mostly philosophy"", as if that were a bad thing, take a look at the actual list of grants awarded by the Future of Life Institute from Elon Musk's $10M donation. Among other things, it contains quite a few concrete, technical questions being pursued by very good people at reputable institutions: Berkeley, Stanford, Cambridge, Harvard, etc.  It's true that there's a longer-term, more philosophical side of FAI research (this is also true of AI research more generally), and newcomers who aspire to be computer scientists as opposed to philosophers are well advised to focus on building up their technical chops on concrete problems. But if you're actually at the point of being able to do meaningful technical research (e.g., background equivalent to a first-year PhD student at a good school in terms of courses taken, papers read, etc.) and you find yourself with an interesting problem related to learning utility functions (aka value alignment), proving behavior guarantees on AI/RL agents, detecting when modelling assumptions are violated, etc., don't shy away from it just because Andrew Ng's hordes of internet followers might write it off as Singularity woo-woo. There are plenty of serious people who think these and related problems are very important for present and future AI systems and will respect you for solving them."
MachineLearning,3d8qdv,davmre,4,Tue Jul 14 17:56:21 2015 UTC,"I didn't mean philosophy as a bad thing, but rather as something that is so distant from the technology we've today that in the end it must be either speculation (making specific assumptions that will most likely be false) or vague (thus not very useful).  I say this as someone with a ""background equivalent to a first-year PhD student at a good school in terms of courses taken, papers read, etc"" like you put it. In this field no one really knows what's gonna be the next big thing in 5 years, and without a clear definition of the AI that could be dangerous I just cannot see how FAI can help the future generation beyond philosophical considerations.  For example, Deep Mind's Atari system is considered to be the best reinforcement learning system today but it cannot even play Pitfall, so a mathematical analysis of the dangers of its utility function seems meaningless to me as the real danger will come from something that is completely unexpected to us today.  I recommend Hinton's lecture on the fog of progress (the last lecture of his Coursera class), it's really important to have this in mind when talking about the future. I also agree with Andrew Ng that there are many pressing concerns today that we can't ignore. And saying I'm a ""horde follower"" for this is not nice, I never said you people were""cult followers"", I only said you were doing philosophy, which is truly not bad in itself."
MachineLearning,3d8qdv,tabacof,3,Tue Jul 14 22:33:11 2015 UTC,"Without a clear definition of the AI that could be dangerous I just cannot see how FAI can help the future generation beyond philosophical considerations.   Obviously we don't know what the first ultimately successful AI will look like, exactly. But barring an utter revolution it will be situated in the basic theoretical framework the field has built up over the past few decades: some notion of maximizing expected utility, with expectations taken under some (implicit or explicit) probability model of the world. Pretty much every successful system ever built, from Deep Blue to Google Translate to Deepmind Atari to Big Dog robots and self-driving cars, fits this description. So advances made in our understanding of this basic framework will likely continue to pay dividends in the long-term future, in addition to be useful for the systems we're building right now.    And saying I'm a ""horde follower"" for this is not nice, I never said you people were""cult followers"", I only said you were doing philosophy, which is truly not bad in itself.   I didn't mean to call you specifically a horde follower, but there are lots of people who use Ng's comments to dismiss all consideration of the future of AI without investing any of their own thought or even consulting other opinions. Stuart Russell is probably the most prominent voice on long-term AI risks at the moment, but many others including Hinton have spoken on the need for research to ensure that AI capabilities are used to benefit mankind. I suspect even Andrew Ng's views are a bit more nuanced than the couple of statements that often get quoted.  For what it's worth, I don't work on FAI research at the moment, but I've seen enough of the arguments to be convinced it's an important direction -- one of many -- that deserves the energy of capable people. The field is still in the process of identifying the correct technical questions to ask; the list of FLI grants provides some candidates, but others will emerge over time as more people with technical knowledge start to consider the problem. Some research directions will have immediate practical applications, e.g., studying robustness of classification boundaries learned by deep neural nets. Others will start at the fundamentals and take longer to pay off. Some may never pay off -- but that possibility is true for every sufficiently ambitious research project.  As a specific example, Stuart (full disclosure: my PhD advisor) is interested in RL agents that don't receive rewards directly but instead have Bayesian priors over their reward function, which they update by observing their environment (for example, a domestic robot might 'learn' about its reward function based on feedback from its human owner). This should lead to exploration-exploitation tradeoffs where the agent acts, at first, to learn more about what its reward function really is (observing its owner's behavior without interference, or even explicitly asking questions), so that it can better optimize it. This is not part of the traditional RL framework, but it's mathematically well defined: you could try to build such an agent, right now, in an appropriate toy environment (gridworld, etc.). What sort of issues come up? What priors are appropriate? What sort of behavior emerges? No one knows because AFAIK no one's ever tried. But it's definitely a technical, not just philosophical, question."
MachineLearning,3d8qdv,davmre,2,Tue Jul 14 23:14:59 2015 UTC,"I'm not entirely convinced that ""value alignment"" is a useful direction of inquiry. At best, you might prove some theorems that start with  ""Suppose agent A can observe actions X of agent B""  however, in the real world, we might be concerned with the actions of a robot, whose observations are video and audio streams that have no intrinsic labels denoting agent B (human or owner) and its actions X, so whatever the theorem might promise goes out the window."
MachineLearning,3d8qdv,5at,1 point,Wed Jul 15 00:48:24 2015 UTC,"Sure, you can add identity uncertainty: you want to optimize Alice's values but you don't know exactly which person Alice is. Presumably in this case the optimal strategy involves first trying to identify Alice so you can learn what she wants. Models of identity uncertainty have been studied, somewhat, and some ability to reason about identity will clearly be crucial for any intelligent agent, friendly or not. That said I'd argue it makes sense to understand the core of the value alignment problem before adding in complications.  If you really want to sidestep the identity problem, you could move directly to the pie-in-the-sky goal of learning a value function for the whole human race, which might not be the value function of any individual human. You could also imagine intermediate cases where a household robot doesn't try to identify the individuals living in the household but just learns some set of values that they share: people may disagree on how to organize the fridge, but everyone wants the trash taken out.  You can also avoid all of this by supposing that values are learned, once, in a lab, and the resulting value function is made available to agent designers, in the same way that you can currently download pretrained ImageNet models. If all robots on Earth are using the same value function, then any results you prove about that value function become universally applicable."
MachineLearning,3d8qdv,davmre,1 point,Wed Jul 15 01:02:01 2015 UTC,"That said I'd argue it makes sense to understand the core of the value alignment problem before adding in complications.   Assuming that the complications are manageable. Saying ""let's think about how we'll cross the bridge after we get to it"" makes little sense if you also happen to know that the bridge has been demolished.   You can also avoid all of this by supposing that values are learned, once, in a lab, and the resulting value function is made available to agent designers   The agents in the real world should have rich sensory inputs, like video and audio, and so their value functions must be formulated, mathematically, as functions of the histories of such rich sensory inputs.  However, anything that the theoretical value alignment research might prove would have to deal with the concepts of the agent being imitated, B, and its actions X (If you disagree, perhaps you could outline the kind of theorem that might come out of this research, especially how it refers to B and X).  So, there is an incompatibility here between the theorems you might prove and the sensory input the agents must have.  One approach here (and I'm not sure that this is what you are proposing) would be to marry a value-aligning agent with proven desirable properties (if this line of research is successful), and some classifier, perhaps developed ""in the lab"" and then frozen, that would interpret the rich sensory observations for the agent. However, you can't prove much, if anything, about the classifier, and therefore about the hybrid system (Who's to say that the agent won't try to deceive the classifier?).   So that's why I'm not convinced that the bridge that the value-alignment research leads to is crossable."
MachineLearning,3d8qdv,5at,1 point,Wed Jul 15 02:02:22 2015 UTC,"I guess I just don't see how identity uncertainty is especially crucial to value alignment, as opposed to other tasks performed by an AI system. If you tell your robot to converse with Alice, it needs to figure out who Alice is. If you tell it to bring Alice some coffee, it needs to figure out who Alice is. And if you tell it to learn Alice's values, it also needs to figure out who Alice is. We study all of these other problems (NLP, robotic motor control, etc.) in isolation without including real-world complexities like identity uncertainty. Why not value learning?  I also don't think the identity uncertainty problem is particularly difficult, at least conceptually. It's pretty straightforward to formulate a Bayesian model of a world containing multiple objects, each with its own properties, so that observing the properties of an object gives you a posterior distribution on which one it is. That's essentially what the paper I linked does. Or you could use a discriminative classifier if you wanted. If you want to be lazy, you just take whatever max-probability output the classifier gives you, or you could be Bayesian and average over the outputs. If the classifier says 50% chance you're facing Alice and 50% chance it's Bob, then whatever values you infer should (partially) affect your estimates of both Alice and Bob's utility functions, at least until you gain further clarity on who it really was.    From a theoretical perspective you can just take whatever guarantees you have about the value-alignment procedure, and combine them with the uncertainty in your identity model. If you can provably learn a value function within epsilon of the target's true value function with probability (1-delta), and your identity model picks out the correct target with probability (1-gamma), then the combined system will learn epsilon-correct values with probability at least (1-delta)*(1-gamma). Or whatever. Even if your identity model has no guarantees whatsoever you could probably at least show that you're likely to learn someone's value function correctly, which is a heck of a lot better than nothing.  It sounds like your ultimate concern is that someone impersonating Alice could cause her robot to learn the wrong values. Sure this could happen, in the same way that if Hitler kidnaps your baby and raises it as his own, it might become a Nazi. This doesn't mean that babies are bad value learners. The goal of value alignment is not to ensure that an agent learns ""humane"" values given arbitrary adversarial inputs, any more than a speech recognition system would work if you trained it on whale song. But just as the best way we know of to specify a speech recognizer (which encodes a messy function that would be impossible to hard-code) is to let a computer experience a large corpus of human speech, it seems reasonable that the best way to specify human values (also messy and difficult to hard-code) will involve letting a computer observe a large corpus of human actions."
MachineLearning,3d8qdv,davmre,2,Wed Jul 15 05:57:20 2015 UTC,"From a theoretical perspective you can just take whatever guarantees you have about the value-alignment procedure, and combine them with the uncertainty in your identity model.    I thought you'd say something like that. However, your identity uncertainty model will be imperfect. In fact, it will probably be easily fooled:  http://arxiv.org/abs/1412.1897  (I'm not necessarily talking about adversarial humans, but rather about things like wireheading by the agent itself)  Speaking generally, when you combine the value-aligning agent with the classifier, even if you have some theoretical results about the former, your knowledge about the latter is entirely empirical (it would be a mistake to assume some nice distribution in its errors), and so as far as I can tell, there is no value in the theoretical results w.r.t. the hybrid system.  To go with Russell's example, if your classifier is prone to confusing the house cat with a picture of that cat, who's to say that the agent (to whom the classifier is just part of the environment, essentially) won't discover this flaw and try to maximize its utility by first microwaving the cat, feeding it to the humans, and then putting a picture of the cat on the wall, so that the classifier says that the cat is alive and well?  P.S. The cat is a bit of a distraction. The essential function of the classifier will be to recognize the agent being imitated and its actions, but my main point is the same: there is not much value in the rigorous theoretical results, if the theorem doesn't apply in practice."
MachineLearning,3d8qdv,5at,1 point,Wed Jul 15 06:37:01 2015 UTC,"That's a fair point, I was being naive about the theory. An optimizing agent that acts in the world (as opposed to a passive approach that learns values from, e.g., reading books and watching movies) has the ability to change its own input distribution, and that does make it harder to reason about.   That said I don't think wireheading is as obvious a problem here as you suggest. An agent that wants to optimize Alice's values can never plan to wirehead, since to do so would not be optimizing Alice's values. In fact, if your final goal is to optimize Alice's values, appropriate instrumental goals would include doing everything you possibly can to ensure you are not being deceived about what those values are. So a value-alignment system ought to act to make its classifier as accurate and robust as possible. Yes, the agent's actions will disrupt the distribution of classifier inputs, but in the opposite way from what you suggest: the agent should act to present its classifier with the best case input distribution from an accuracy standpoint (always making sure to view things with good lighting, looking side-on to distinguish 2D photos from 3D reality, etc.). A classifier attached to a value-aligning agent should perform better, not worse, than its probabilistic guarantees would indicate.   Obviously this is all a bit hand-wavy; I don't expect it to be really persuasive until grounded out in behaviour of an actual agent. This kind of hand-waviness is the traditional flaw in FAI discussions, and it's exactly why it's so important that people start working on these problems technically. Let's actually try specifying a simple value aligning agent and environment where we can actually compute its behavior, and see what happens! That'd help make these discussions more concrete and doesn't seem too hard, plus it'd be really interesting!"
MachineLearning,3d8qdv,davmre,3,Wed Jul 15 07:56:19 2015 UTC,"There are plenty of serious people who think these and related problems are very important for present and future AI systems and will respect you for solving them.   Are any of these people experts in machine learning? How would you recommend that I go about convincing myself that they're not crackpots, at least with respect to the subject matter domain that they purport to address?"
MachineLearning,3d8qdv,VelveteenAmbush,1 point,Tue Jul 14 19:22:15 2015 UTC,"Many of these people (Stuart Russell, David Parkes, Percy Liang, Bart Selman, Daniel Weld, just to name some whose research I'm familiar with) are AI professors at top schools with well-regarded research programs.  If you're trying to judge a more junior grant recipient (grad students, etc.), you could look at their publication record to see what previous work they've published in ML-related conferences."
MachineLearning,3d8qdv,davmre,1 point,Tue Jul 14 19:37:02 2015 UTC,"Many of these people (Stuart Russell, David Parkes, Percy Liang, Bart Selman, Daniel Weld, just to name the ones whose research I'm familiar with) are AI professors at top schools with well-regarded research programs.   Sorry, are you claiming that these people believe in the ongoing philosophical efforts to tackle AGI problems, or are you claiming only that these people would be impressed if you (provably) managed to solve the AGI problems?  If it's the former, I'd be curious to see some references. If it's the latter, it's not very impressive -- e.g. I'm sure the world's physicists would be impressed if you actually invented a perpetual motion machine, but that doesn't mean it's a good idea to try."
MachineLearning,3d8qdv,VelveteenAmbush,1 point,Tue Jul 14 19:48:51 2015 UTC,"Those are people listed as receiving grants from the FLI to work on technical problems having applications to FAI research. They presumably think that these problems are interesting and worth solving, and have ideas for making progress.  Some of them do believe in the long-term existential risks highlighted by Bostrom and others, see e.g. this talk by Stuart Russell (and his other writings on the subject). Many of the others are more guarded, but I suspect they would say that techniques for better reasoning about and controlling the behavior of AI systems are useful to us now even independent of whatever long-term benefit they might provide."
MachineLearning,3d8qdv,davmre,1 point,Tue Jul 14 20:13:28 2015 UTC,Giving 1.5M to Nick Bostrom does not exactly inspire confidence...
MachineLearning,3d8qdv,kjearns,4,Tue Jul 14 19:19:45 2015 UTC,"I don't think this sort of ad hominem elevates the discourse. If you have problems with Bostrom's work, state them. Many people seem to dismiss him as a Kurzweillian-style ideologue without giving any sign that they've actually read his work or engaged the arguments.  Superintelligence is a fairly nuanced book examining many future pathways for AI and the possible effects on humanity. Bostrom has put a lot of thought into these issues and is probably one of the best-qualified people to run a policy research center.   Is he a model to emulate for budding CS researchers? Of course not; he's a philosopher. Not all valuable work is technical. Musk presumably understands this."
MachineLearning,3d8qdv,davmre,9,Tue Jul 14 20:00:04 2015 UTC,"The 'singularity' and anything associated with it / based on it.  Not to say that there aren't some interesting ideas involved, but many of the folks that strongly believe in it treat it like a religion, and that results in very sloppy thinking and bad science.  Oh, I'd also say the Numenta and it's Hierarchical Temporal Memory is a bit suspect as well.  See https://www.reddit.com/r/MachineLearning/comments/393t53/jeff_hawkins_on_intelligence_what_are_the/ for some discussion about that."
MachineLearning,3d8qdv,melvinzzz,4,Tue Jul 14 15:06:54 2015 UTC,"In economics, my home field, you often run into:   Mistaking your model for reality Correlation-Causation errors Self-deception (Sure,  I ran 50 versions of this, but this one shows my results, so it must be right!)   Machine learning, which has an almost magical aura about it, is going to have to deal with each of these."
MachineLearning,3d8qdv,OliverOnTheWeb,5,Tue Jul 14 18:43:47 2015 UTC,"Machine learning (and statistics in general) have loads of tools to deal with these mistakes. Of course, that doesn't mean people don't make them anyway. This is probably amplified by an influx of relatively inexperiences people into the field."
MachineLearning,3d8qdv,nkorslund,0,Tue Jul 14 23:27:23 2015 UTC,"Machine learning (and statistics in general) have loads of tools to deal with these mistakes. Of course, that doesn't mean people don't make them anyway.   Yuuuuuuuuuuuuuup"
MachineLearning,3d8qdv,OliverOnTheWeb,2,Wed Jul 15 02:15:50 2015 UTC,"Other people have covered some areas. I'm going to talk about some features of areas that are also important to avoid.   Becareful of any system building efforts which take forever.  Aka, cyc.   I know people who've moved on from there, so it wasn't a super waste of time for them.. but your career won't head anywhere super fast.   Be careful of research that prioritizes uses a certain method rather than results.  Being flexible in  your methodology is important whether you're doing DNN, kernels, or bayesian stuff.  ML algorithms are tools.  The field ranges from people who use the tools  to people who make them.  I've seen people adhere very strongly to a single hammer and pay for it.    Also on that note, it's happened less in this field, but don't get stuck in a single idea that if it's disproved, will result in your career going to crap.  This results (I'm not going to name researchers) in researchers who stick to their guns and cause the field to slow down because they refuse to accept new evidence.  (also, there's a difference between the fad moving on and a method becoming unfavorable vs mounting evidence that a theory doesn't fit the data because it's made a wrong assumption)."
MachineLearning,3d8qdv,Articulated-rage,2,Tue Jul 14 20:50:19 2015 UTC,"Don't sit around duct-taping other people's systems together into ensembles, just to get the best score on some benchmark task. Similarly, don't go to massive feats of engineering just to run some known method on massive data  to eke out a few extra % of performance, when we already know about the diminishing returns curve of that method.  A more subtle trap is collecting a new data set. Sometimes this is key, if you have the right idea. The Stanford sentiment data is a great example of a data set that allowed new techniques. But often it looks more like, ""we're collecting a named entity corpus for Hindi.""  More subtle again is the extent to which you should work with an existing system, often your supervisor's. This can be great because it can fast-forward you to the current state-of-the-art, making your work more relevant. But the danger is you'll spend too much time studying a particular implementation.  In general I'm talking about the trade-off between activities that will land you good performance numbers, and activities which develop you as a researcher in the long term. More generally, it's important to understand the role that performance numbers really play in the field, and why absolute accuracy figures sometimes are necessary to make some claim, and other times not. I didn't really get this for much of my PhD, although I thought I did."
MachineLearning,3d8qdv,syllogism_,1 point,Thu Jul 16 14:25:01 2015 UTC,The closest thing I could think of: https://en.wikipedia.org/wiki/No_free_lunch_theorem
MachineLearning,3d8qdv,tariban,5,Tue Jul 14 11:38:44 2015 UTC,You want to avoid a theorem about optimisation? This makes no sense to me.
MachineLearning,3d8qdv,egrefen,7,Tue Jul 14 12:42:17 2015 UTC,"It means that it is impossible to find an classifier that works equally well on all possible datasets. Thus, looking for such a classifier would be one of those areas to avoid."
MachineLearning,3d8qdv,iori42,1 point,Tue Jul 14 15:07:23 2015 UTC,"I agree in that sense that better not to pursuit too general directions, and better to choose some real-life domain in which you see a potential for machine learning/AI, and from this perspective get a first hands-on experience. Btw, you mentioned the ""free energy"" impossibility in physics, however I frequently meet biophysical models which make use of it, and they work sometimes quite well (and not only in ""ideal"" world).  In this cases they perform much better than machine learning methods, especially for out-of-boundary predictions."
MachineLearning,3d8qdv,whcosn,3,Tue Jul 14 12:56:28 2015 UTC,"The free energy OP mentions is not the same as the free energy you mean - OP is talking about things that are literally ""energy generators"" and violate all current concepts of physics."
MachineLearning,3d904t,tamagowitch,2,Tue Jul 14 13:08:21 2015 UTC,"I've never trained ReLU RBMs but I had similar issues with Gaussian-Binary RBMs (and lots of other people as well apparently), all I had to do was to decrease the learning rate and it stopped diverging  BTW, does anyone know if there is any work on the stability of RBMs on the point of view of random dynamical systems / control theory or if this would be even plausible?"
MachineLearning,3d904t,gsmafra,1 point,Tue Jul 14 20:18:35 2015 UTC,I have read that people clip the max of the ReLU http://arxiv.org/pdf/1412.5567.pdf
MachineLearning,3d84xs,cast42,3,Tue Jul 14 06:44:11 2015 UTC,Seems like old school feature engineering is still required in practice. Haha.
MachineLearning,3d84xs,j_lyf,4,Tue Jul 14 08:05:24 2015 UTC,Anyone else depressed about the fact that ensembling is so darn good?
MachineLearning,3d84xs,sieisteinmodel,5,Tue Jul 14 12:47:56 2015 UTC,"I think his single best model achieves 69% accuracy, and the ensemble achieves 71%. I don't know if in production systems, one would choose this improvement for slower predictions/training time."
MachineLearning,3d84xs,dexter89_kp,0,Tue Jul 14 15:17:58 2015 UTC,I dislike these competitions where the top 50 get essentially the same score. (although the ones where the dataset is clearly insufficient and the winner chosen by a dice role are much worse).  I'd like to see a lightning comp where the goal is best performance in 6/24/48 hours.
MachineLearning,3d84xs,sdfsdkehg,3,Tue Jul 14 15:44:17 2015 UTC,Because machine learning models and time pressure go hand in hand?
MachineLearning,3d84xs,sieisteinmodel,-1,Tue Jul 14 18:15:24 2015 UTC,"yes, if you're not in academia."
MachineLearning,3d84xs,sdfsdkehg,3,Tue Jul 14 18:22:27 2015 UTC,Like where exactly?
MachineLearning,3d84xs,sieisteinmodel,5,Tue Jul 14 19:05:47 2015 UTC,Of course it is.  Thing is that not every company likes solutions delivered by sleep deprived programmers.
MachineLearning,3d84xs,sieisteinmodel,3,Tue Jul 14 19:19:55 2015 UTC,"Well as many of the kaggle competition hosters are indeed companies, this clearly isn't always the case. Also it's a recipe for getting highly sub-optimal solutions.  It really depends entirely on the problem and use-case scenario. In some cases getting a 5% better solution is worth waiting two extra months, in other cases it's not."
MachineLearning,3d84xs,nkorslund,1 point,Tue Jul 14 19:39:20 2015 UTC,Sorry to hear that.
MachineLearning,3d84xs,sieisteinmodel,1 point,Tue Jul 14 19:45:27 2015 UTC,"I believe that is quite a ""niche"" where good money is paid."
MachineLearning,3d84xs,sieisteinmodel,1 point,Tue Jul 14 22:54:12 2015 UTC,How about a cost function which includes the complexity of the model?
MachineLearning,3d84xs,nova77,2,Tue Jul 14 19:48:00 2015 UTC,So you would almost certainly make a model that matches human intelligence lose because of its (supposedly) large complexity?
MachineLearning,3d84xs,question99,2,Tue Jul 14 19:54:53 2015 UTC,"Fair enough, let's make an exception when there's a kaggle competition about replicating human intelligence."
MachineLearning,3damvg,deathstone,2,Tue Jul 14 20:13:02 2015 UTC,not a bad list but by far not complete. Often Least Squares doesn't give that good results. Remember the Random Forests Regression (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)  Good luck with your interview!
MachineLearning,3damvg,adolokov,2,Tue Jul 14 22:10:57 2015 UTC,"There are lecture notes taken by the students of Ng's MOOC that are available on the internet. For example, here: http://www.holehouse.org/mlclass/ Should be good for reviewing."
MachineLearning,3damvg,stolzen,1 point,Wed Jul 15 08:02:55 2015 UTC,"Thank you for that. I had referred to them when I completed the course the first time around, will be useful again"
MachineLearning,3da50z,csris,4,Tue Jul 14 18:12:09 2015 UTC,"Wall of text inbound - hopefully this is helpful to start your journey.  For extracting features from audio, you will want to look into libROSA. One very common thing to do for speech is to extract mel frequency cepstral coefficients (MFCC, a special type of feature which is known to be good for speech).  If you don't know what MFCCs are yet, start by reading about the Discrete Fourier Transform or DFT (as well as the mathematically equivalent Fast Fourier Transform, or FFT), and how that relates to audio. There was a great guide on introductory signal processing wrapped into this guide to Shazam, the audio recognition system. I highly recommend it as a quick starting point, though there are a crazy amount of resources like The Scientist and Engineer's Guide to Digital Signal Processing, Steven W. Smith and parts of complex2real.com, specifically tutorials 4, 5, and 6. There is also a decent guide to MFCCs here.   The MFCC is basically a DFT, then a special type of filtering process that relates to a crude model of the human ear (mel filterbank), then a log, then a Discrete Cosine Transform (DCT). Hence the name Mel (from the filtering) Frequency Cepstrum (the name for taking a DFT then applying a DCT - note that the name spectrum and cepstrum are almost the same) Coefficient.   What you end up with is a representation that compresses information about both time and frequency into a few numbers - by using these numbers you have an idea of the ""shape"" of the envelope of the speech and roughly what was inside it frequency wise. This happens because the DCT (and its inverse) have a property called compaction, which means the first few numbers hold most of the information about the signal that was transformed. This is useful because we only keep 40 of these coefficients, rather than all of them (usually 256, since the DFT size is typically 256). If they all had the same information we would be throwing out useful stuff.  In the simplest approach, you extract 40 MFCCs per frame of audio, where each frame is something like 20 to 40 ms long with 50% overlap between each frame. Then you normally take each frame, along with 5 neighbors on each side (called the context), flatten that out into a feature vector of 440 (11 * 40) and use that to predict the phoneme target with something like a feedforward neural network or random forest (two classifiers). This is frame wise prediction, since you are only predicting the class of a single frame at a time.  There are way more advanced things, especially when you get into something more than frame by frame prediction and move toward true speech recognition / transcription.  End wall o' text  Unfortunately, you are gonna get way into signal processing - almost no way to avoid it with speech processing. Do you have a background in signal processing / timeseries? What is your general knowledge level? Is this a course project or master's thesis or just something fun?  I am revamping my blog and might be interested in putting together a tutorial of this with something simpler (and free-er) than TIMIT. There are not very many good resources for learning about machine learning for audio.  If you are unfamiliar with machine learning (shameless plug), Andreas Mueller and I recently gave an 8 hour tutorial on machine learning using scikit-learn at SciPy. You can see the course here as well as the course notes here. It probably won't take 8 hours to go through yourself, but the video is there if you get stuck.  EXTRA I forgot to mention that I wrapped a simple recognizer called pocketsphinx if you just want to try it on some wav files. See the repo here."
MachineLearning,3da50z,kkastner,1 point,Tue Jul 14 20:30:29 2015 UTC,"Feeding output probabilities of NNs into a HMM decoder is not really a ""way more advanced thing"" is it?"
MachineLearning,3da50z,gsmafra,2,Tue Jul 14 20:47:14 2015 UTC,"It definitely is if you incorporate a language model and want to go from phonemes -> words or sentence level.   Even without that you would need to understand HMMs / the problems of time-series prediction. So it is somewhat harder than just cramming special data through a classifier, IMO.   Also, that assumes you already have the HMM decoder. If you want to train both and do a hybrid system approach, you get into a weird thing where you have to train the classifier for a while, then train the HMM, then train the classifier, etc. which is super annoying."
MachineLearning,3da50z,kkastner,1 point,Tue Jul 14 20:56:33 2015 UTC,"Second question: if I want to classify large excerpts of audio (e.g. a whole song), would you rather train a classifier for small segments and weigh the classes afterwards (bag of frames approach if I understand well) or extract a descriptor for the whole song?"
MachineLearning,3da50z,gsmafra,1 point,Tue Jul 14 21:10:11 2015 UTC,"Bag of frames can work fine and I would probably prefer it over something that is at the whole song level, since songs can be pretty radically different from beginning to end. See /u/bennane 's work on genre detection here http://benanne.github.io/2014/08/05/spotify-cnns.html . Are you trying to recognize each song specifically, or something more general? What are the classes?"
MachineLearning,3da50z,kkastner,1 point,Tue Jul 14 22:14:40 2015 UTC,"Thanks so much for your help. As to my background, I'm just an undergrad student who just completed his first year. This is for an internship. I've seen MFCC and FFT thrown around a lot in the papers I'm reading, and I still don't quite understand what's going on.   Should I be finding different types of features that are commonly used, and then find a way to implement feature extraction? Or should I be taking some other approach? You mentioned that MFCC is a special type of feature, and also that it's similar to DFT -- what do you mean? I thought DFT was a kind of transformation function -- how is a function a feature?  Either way, I'll definitely check out the resources to posted. Thanks again for your help."
MachineLearning,3da50z,kkastner,1 point,Tue Jul 14 21:03:56 2015 UTC,"Any feature extraction can be considered a transformation - you are finding a new representation for the information that should be easier in some sense for the machine learning part.  This transformation could be lossless (like in the DFT) and invertible. It could also be lossy (like in the MFCC), like only choosing the most useful information (maybe the frequencies from 100Hz to 500Hz are all that really matter for some application - why keep the rest at all?) and feeding to your classifier.  In math speak, feature extraction is just a function f(x) - there is no necessity for this function to be invertible, though sometimes it is. A transformation is fundamentally a function f(x) which has an inverse g(x) = f-1(x) so that g(f(x)) == x So a transformation can be seen as a function with the guarantee of having an inverse.  In general, feature extraction is simply finding a representation for your information that is useful for the machine learning part. DSP people have been doing this forever with filters and things - even the spectrogram is an example. It is usually not very useful to work with the raw timeseries, so DSP people found a better representation using the DFT to transform things. They tended to look at it visually or do really simple ""learning "" (some old person sets a threshold at a very specific level) but the idea is the same.  Not to overload you too much, but you have hit on a key point. Generally what we want is a sparse representation to do good machine learning - something where only a small part of the features are used for each data sample, but the features that turn on are a strong indicator of what the data sample actually is. The DFT actually provides this quite nicely for audio, which is why we see those spikes in the spectrum where the original time series was just a mass of data."
MachineLearning,3da50z,Improbus_42,0,Tue Jul 14 21:08:33 2015 UTC,"Asking for advice about 'how to extract features', it seems that you think of machine learning to be like SIFT, SURF etc.. machine learning is not about hand crafting features. I would recommend reading the FAQ. Or just head over to the coursera machine learning course, in my opinion a good way to get started.. Andrew Ng's teaching style is perfect for beginners.  Edit: I had this very interesting article on my mind: Recommending music on Spotify with deep learning. In traditional machine learning ""hand-crafted"" feature extractors are used. But in Deep Learning, ""trainable"" Feature Extractors are used: yann lecun talk Sorry for the confusion, i am a beginner, too and it feels like all of machine learning is deep learning right now ;)"
MachineLearning,3da50z,kkastner,2,Tue Jul 14 20:09:21 2015 UTC,I disagree - most of the current field of machine learning is about handcrafting features - though I 100% agree on the recommendation for Andrew Ng's course.
MachineLearning,3d7rkf,fariax,3,Tue Jul 14 04:22:54 2015 UTC,How is this C?
MachineLearning,3d7rkf,j_lyf,1 point,Tue Jul 14 08:05:51 2015 UTC,It would be nice to see benchmarks from these implementations vs. GPU based from another frameworks!
MachineLearning,3d7s5y,philly7891,4,Tue Jul 14 04:28:13 2015 UTC,"I don't know any real specifics, but there's a lot of missing OpenCL code in the community. There's plenty of lost opportunity in that department for highly efficient Open CL code. Along that note, I'm sure you could take any given C-based Neural net and port it over to OpenCL for massive performance gains vs CPU.  I'd do that but i'm still trying to learn the basics of C++, haha."
MachineLearning,3d7s5y,devDorito,2,Tue Jul 14 06:00:50 2015 UTC,"Many functions in ML libraries (including Scipy and Sklearn) for Python already run on top of optimized Cython code, which makes use of statically typed structures and run as fast as C."
MachineLearning,3d7s5y,bellari,8,Tue Jul 14 05:48:23 2015 UTC,"Not to mention that linking Numpy/Scipy against Intel MKL or BLAS incredibly speeds up these algorithms and applies implicit parallelism.  OP this is a valiant effort to take; but honestly, leave implementation of core algorithms to the pros. There are so many numerical issues and practical problems to deal with the community would likely not be interested in it. If otoh you are interested in doing this for merely learning then go for it."
MachineLearning,3d7s5y,quattro,1 point,Tue Jul 14 07:35:09 2015 UTC,"There's the gradient descent algorithm, it's used to find the hypothesis or decision function."
MachineLearning,3d7s5y,kabbab,2,Tue Jul 14 04:36:52 2015 UTC,Cough NLOpt Cough
MachineLearning,3d7s5y,quattro,1 point,Tue Jul 14 07:31:44 2015 UTC,https://www.reddit.com/r/MachineLearning/comments/3d7rkf/mlpackmlpack_cc_ml_lib_started_with_neural/
MachineLearning,3d7s5y,fariax,1 point,Tue Jul 14 05:35:21 2015 UTC,DocNADE and replicated softmax  DocNADE paper: http://papers.nips.cc/paper/4613-a-neural-autoregressive-topic-model  RS paper: Modeling Documents with a Deep Boltzmann Machine http://www.cs.toronto.edu/~nitish/uai13.pdf  Python code for both: http://www.dmi.usherb.ca/~larocheh/code/DocNADE.zip
MachineLearning,3d7s5y,Foxtr0t,5,Tue Jul 14 13:35:46 2015 UTC,You know that both c and python are turing complete?
MachineLearning,3d7s5y,keymone,5,Tue Jul 14 04:41:57 2015 UTC,"What you are saying doesn't make sense. Everything you can do in Python, you can also do in C. In the most extreme case I could do it by writing a Python interpreter in C."
MachineLearning,3d7s5y,iori42,2,Tue Jul 14 05:25:12 2015 UTC,"so many things wrong here. there is nothing intrinsically ""dynamic"" about ""typing of ML"". there isn't even such thing as ""typing of ML"". and lastly heterogenous lists are not problem of C, they are problem of programmer that uses them and of course there is nothing in ML that requires heterogenous lists.  bottom line - if you can do something in python, you can do it in C. it might be much more verbose but it is still doable."
MachineLearning,3d7s5y,keymone,2,Tue Jul 14 14:29:05 2015 UTC,"You can use the template classes. That would help a lot. Besides, you can create subclasses that estimate the type of an input."
MachineLearning,3d7s5y,unchandosoahi,5,Tue Jul 14 15:03:41 2015 UTC,C doesn't have templates. C++ does.  I don't see the relation between static typing and the ability to implement an algorithm though. Both languages are turing complete and python is built in C.
MachineLearning,3d7s5y,Tulip-Stefan,1 point,Tue Jul 14 15:37:39 2015 UTC,python is built in C   That point is immaterial since there's layers of abstraction you skip if you jump from python into C.
MachineLearning,3d7s5y,devDorito,1 point,Tue Jul 14 05:03:14 2015 UTC,"C doesn't have templates. C++ does.    Good point. So, why no use C++ then. It feels like C with classes."
MachineLearning,3d7s5y,unchandosoahi,1 point,Tue Jul 14 05:50:04 2015 UTC,"ABI issues, exceptions which are almost impossible to disable, lack of a few features. Most notably VLA's and restrict. But you probably shouldn't use VLA's and restrict is supported by most C++ compilers.  I try to use template heavy C++ whenever i can, but C is still going strong in my industry."
MachineLearning,3d6loz,vikkamath,3,Mon Jul 13 22:46:28 2015 UTC,I'm enjoying the coverage optimization papers mentioned here.
MachineLearning,3d6loz,Rickasaurus,0,Mon Jul 13 23:14:43 2015 UTC,Nice
MachineLearning,3d7dnr,jackbrucesimpson,8,Tue Jul 14 02:27:47 2015 UTC,"Yes, it's definitely good to include a class of negative training examples. There's just as much to learn from what is not what you're looking for as from what is."
MachineLearning,3d7dnr,stupider_than_you,3,Tue Jul 14 02:58:44 2015 UTC,Thanks so much! I'll give it a go!
MachineLearning,3d7dnr,Rickasaurus,3,Tue Jul 14 03:02:20 2015 UTC,Another thing you can do is iteratively look at things near the margin and label those by hand afterward to get better results.
MachineLearning,3d7dnr,isarl,1 point,Tue Jul 14 03:21:00 2015 UTC,"I've been labelling them by hand, the problem is that I can't even tell which group they belong to for a good few of them."
MachineLearning,3d7dnr,r4and0muser9482,2,Wed Jul 15 04:14:59 2015 UTC,"You could try latent training, treating the ambiguous cases as having hidden labels and iteratively training a model using Expectation Maximisation or some other method.  edit: link to paper (PDF)"
MachineLearning,3d7dnr,MeowMeowFuckingMeow,1 point,Tue Jul 14 04:13:21 2015 UTC,"The annoying thing is that most of the ambiguous cases are where my program didn't get a proper view of the tag it's trying to classify, so I don't think there's enough information there to actually attempt a classification."
MachineLearning,3d4mo2,fhuszar,6,Mon Jul 13 14:12:54 2015 UTC,arxiv link: http://arxiv.org/pdf/1502.03509v2.pdf
MachineLearning,3d4mo2,j_lyf,2,Mon Jul 13 14:18:25 2015 UTC,What is the background needed to understand this paper?
MachineLearning,3d4mo2,lstmsallthewaydown,1 point,Tue Jul 14 00:44:08 2015 UTC,"At a glance it looks like familiarity with autoencoders and standard ugrad math (linear algebra, calculus, statistics) but I haven't read it."
MachineLearning,3d4mo2,schmook,1 point,Tue Jul 14 06:08:11 2015 UTC,"I get that this is equivalent to a mixture of autoregressive models in essence, but I can't see where the mixture distribution is hidden."
MachineLearning,3d4mo2,psamba,7,Mon Jul 13 19:33:30 2015 UTC,"I would call it an ensemble rather than a mixture, as it seems the more informative and appropriate analogy.  In this paper -- which presents a clever, efficient implementation of deep orderless NADE -- none of the ""mixture"" components becomes differentially associated with any of the observations at any point in the training process. Differential association of mixture components with subsets of the relevant space is, in some sense, the defining characteristic of a mixture model.  Extending this line of thought, if the sampling distribution used for Monte-carlo approximation of a log-likelihood bound derived via Jensen's inequality does not vary with the input, then analogies to variational inference (or to mixtures) are generally uninformative. I.e. any model that computes a log-likelihood and involves any sort of sampling can be shoe-horned into such an analogy. So, the mere existence of such an analogy only tells us that your model computes a log-likelihood and involves some sort of sampling. Note that I'm talking about sampling ""inside"" the model, not sampling for, e.g., minibatch formation during training."
MachineLearning,3d4mo2,farsass,3,Mon Jul 13 22:33:40 2015 UTC,"At testing/prediction time they form an ensemble by averaging probability prediction from different models, which is essentially an equal mixture. They only train parameters of the component models, but keep the mixture weights fixed at 1/d!, where d is the number of dimensions and hence d! the number of permutations.  Now, if you would train a model like this via maximum likelihood, the likelihood contribution of each datapoint would be log(E(p_i(x))), where p_i(x) are the AR models for different orderings and E is averaging across orderings. Instead, they train something like a stochastic approximation to E(log(p_i(x))). This can be interpreted as a lower bound to the true mixture likelihood via Jensen's inequality."
MachineLearning,3da1bi,joshdotai,2,Tue Jul 14 17:47:35 2015 UTC,"""Now is the time for X"" is very different from ""X is happening now,"" the title of the article."
MachineLearning,3d7n56,rob-on-reddit,2,Tue Jul 14 03:45:21 2015 UTC,"You could just ask me directly ;). Define ""linear memory"" do you mean whether we hold most of the corpus in memory? We stream what we can but I won't claim it's highly optimized as of yet. We are still working on a lot of benchmarks yet.  Re: gpus. We need to implement a batch mode for vocab in order to benefit from GPUs.  We are working on finishing the lasts bits of the spark version as well.  Please come in to gitter next time and deal with me in person, it'd be greatly helpful. We spam gitter on the site for a reason:  https://gitter.im/deeplearning4j/deeplearning4j I promise I don't bite ;)."
MachineLearning,3d7n56,agibsonccc,2,Tue Jul 14 04:48:31 2015 UTC,"Oh cool!  I definitely was not expecting a response from the author.  Sorry, I've been away from ML for a couple years and am just getting back into it.  I'm not familiar with all the forums yet.  I don't even know what gitter is but I will check it out and ask there if I have further questions.  Yeah I meant is most of the corpus in memory.  My corpus is 1TB.  It sounds like dl4j is under development and I'm comparing apples to oranges here.  I'll follow more closely and try to ask questions in the proper forum next time.  Cheers"
MachineLearning,3d7n56,agibsonccc,1 point,Tue Jul 14 05:23:51 2015 UTC,Hah no problem. So far the benchmark corpus for us has been the google news one. Words nearest and other look ups are ~20 ms or so on that (3MM words). We stream the docs and just iterate as neccessary. Unfortunately training time isn't where it needs to be. We are working on integrating a key value store:  http://yahooeng.tumblr.com/post/118860853846/distributed-word2vec-on-top-of-pistachio  Please come on by and I'd be happy to help re acquaint you with the field.  Unfortunately deep learning4j is a bit more than just topic modeling. I'd be happy to answer any general questions about how that interacts with say: spark or gpus etc.
MachineLearning,3d53uo,zionsrogue,3,Mon Jul 13 16:25:00 2015 UTC,"I don't think that this is the right technique for the job. There is some research on image analogies that would probably have better results, without using neural nets."
MachineLearning,3d53uo,jrkirby,1 point,Tue Jul 14 01:22:55 2015 UTC,"Thanks for passing along this paper, definitely looks very interesting."
MachineLearning,3d53uo,fimari,1 point,Tue Jul 14 10:16:09 2015 UTC,"TLDNR  But it looks like they just used the same net than goolge, no training done."
MachineLearning,3d53uo,sieisteinmodel,1 point,Mon Jul 13 21:31:33 2015 UTC,"Well, it tells us that these convnets don't understand a lot about human art."
MachineLearning,3d4vr6,luvmunky,4,Mon Jul 13 15:25:11 2015 UTC,did you notice how blatantly they ripped the site design from kaggle?
MachineLearning,3d4vr6,sdfsdkehg,2,Mon Jul 13 16:25:45 2015 UTC,"You're right, but let's set that aside for now; this is /r/MachineLearning after all. :-)"
MachineLearning,3d4vr6,__mtb__,3,Mon Jul 13 17:14:43 2015 UTC,"ended up in 13th - a bit surprised we dropped 3 slots from the PLB to the private, but sometimes that happens.  As far as the approach / technique: the usual - lots of feature engineering, combined with nets, RF and GBM. Blend over everything ...  I was a bit surprised they extended the comp like they did in the last week. Not really sure why they did that. I didn't think it was that confusing regarding the dates, but maybe we were missing it ..."
MachineLearning,3d4vr6,__mtb__,1 point,Mon Jul 13 19:25:19 2015 UTC,Would you care to tell us more about what techniques you used? What features you derived?
MachineLearning,3d4vr6,watersign,2,Mon Jul 13 20:04:54 2015 UTC,"From what we saw recency was big. Our first model was a just a 1/0 NN with 30 slots, one for each of the days in the course. Nothing fancy, just setting up a benchmark model, i think it came in around .89 or so which at the time was somewhere in the top 20% or so.  We grouped the events into sessions and used this to measure the time the users were spending on the site per day, per course, etc ... For each of the sessions we gave it a relative score based on what type of work was done (i.e. videos/problems and everything else). Besides measure activity, we also did the inverse as well and put a group of features around duration's of inactivity.  Also, it was a really big deal if users had previously dropped other courses, or were concurrently enrolled in other courses. So we put together a group of features in these areas as well.  Spending time on the problems is things students that stayed in the course appeared to be doing, so these type of events had some additional attention.  We spent quite a bit of time parsing / understanding the object hierarchy that described the courses. There was a bunch of junk (i think ...) in this metadata, but I have a feeling we missed some stuff here.  Like I said previously, I am surprised we dropped those 3 slots at the end, I can't wait until I can see how all of our PLB submissions correspond to the private scores.  Did you compete? Care to share your techniques?"
MachineLearning,3d4vr6,__mtb__,1 point,Mon Jul 13 20:22:27 2015 UTC,"I did not submit an entry, because I found out about it too late. I basically spent this past weekend on it; and unfortunately couldn't submit anything as the deadline was in UTC :-((  I had assumed Pacific for some reason. Oh well.  I was able to get around 0.90 AUC on my validation sets, so I guess I could have placed somewhere in the top 200 or so. I used RFs with 2000 trees, nothing special. I wanted to try NNs also, but couldn't.  Like you, I also looked at daily activity. Turns out activity on the last day was a very important feature.   I also took the top modules which seemed important for either 0 (didn't drop out) or the 1 (dropped out) class. These modules turned out to have some signal too.  I built a classifier for each course; since there were only 39, it wasn't too bad.  I didn't look at individual user features, but I wanted to. Though there was a significant number of users in the test set who  were not in the training set, which made me wonder how useful of a signal it would be."
MachineLearning,3d4vr6,watersign,1 point,Mon Jul 13 21:23:36 2015 UTC,"i peaked at this data set and it did look quite complicated due to the hierarchy. How/what did you use to verify/validate its accuracy (ROC, accuracy, etc) ?   was your overall model just a ensemble or did you use one technique..?"
MachineLearning,3d4vr6,wil_dogg,1 point,Tue Jul 14 02:44:30 2015 UTC,"We used AUC for our error metric.  Our best submissions were all ensembles of models. I haven't seen the private to public score mapping yet, so I am not sure how well this worked. But from the PLB side and our xval scores, ensembles were very helpful."
MachineLearning,3d4vr6,wil_dogg,1 point,Tue Jul 14 11:56:52 2015 UTC,cool. im guessing you're using r/python ..the data mining tool i use shows AUC for the all of the variables that you use in the model..  do you combine all of them together in the ROC chart...?
MachineLearning,3d4vr6,ialuronic,3,Tue Jul 14 20:40:06 2015 UTC,"willdogg ranked 231st, about in the top 3rd (and moved up 5 slots on the final public ranking, more on that later), here's my rationale for my work and the approach that I used.    Now before we get into the details, let me make it clear that my comments here are not criticisms of anyone else's work -- this is simply how I approached the competition based on what I knew going in, what my goals were, and what I learned through working with the data and reflecting on my own work and what I saw on the leader board.  I knew I was not going to win the competition, but I also knew my motives and time constraints were not the same as those who were shooting for the cash prizes.  First, I have a real job in this domain and I wanted to use the KDD competition to benchmark my methods.  But in doing so I put several constraints on myself to make the results relevant to my day job.    This means I could not spend a lot of time on the KDD competition, not just because I have a day job, but more important, I'm not really benchmarking my methods if I'm grinding on the data unceasingly to try to beat other analysts with no regard to the structural form of the final solution and the feasibility of its implementation.  In the real world, I'm not in a horse race against other modelers where I know their standing on an hour-by-hour basis, and I do not have 2 months to hammer against the data while constantly comparing my best results to theirs in an iterative Ping-Pong game.  What is more likely in my day job is that a novel modeling project comes to my workbench, and I have 2-3 days to report out a solution framework where I must be 99% sure it will work, and another 2-3 days to get to a working final solution.  For KDD I placed those constraints on myself -- I had to get the job done on the side of my desk, allowing myself only 2 weekends to do mad scientist work (and even there, capping those weekends to a few hours each evening because I don't let work steal my time with my wife and kids).    So I knew I would likely see my ranking fade away given that constraint, but that's OK, I still wanted to enter the competition and also track how the leaders did because I could see early on that some people had some ""special sauce"" that I could learn from.  Second, I always constrain myself to have a transparent and easily interpretable solution that has a high likelihood of working in a real-world application.  This is especially true when I am modeling a new phenomena (this was the first time I ever cracked open clickstream data, and am very grateful that KDD put this data set out in the public domain in that regard).  Most of my clients will not accept a NN solution, and even a TreeNet model is a stretch for their willingness to accept a complex functional form of the model.    This is not just an issue of interpretation, it can also become an issue of operational execution if the code that scores the model becomes so massive that it impedes high-speed scoring in an operational setting.  So when my solution was solid and I then ""juiced"" it by using TreeNet rather than settling for a very simple segmented linear additive model and my C stat increased by only .002 on validation, I knew I was at a stopping point -- there's no way I am going to push harder on the data and generate 32 megabytes of scoring code when my prior solution was not significantly worse than that and could be scored with less than 1 kbyte of code.  Third, I would apply coarse classing to my modeling to improve the robustness of the model to fluctuations in input data.  I knew this would cost me about .01 on the final C stat, but it seemed very important given the potential for future fluctuations in click-stream data.  And I thought that a robust solution might fare better in the final ranking given the tendency for other competitors to possibly Ping-Pong their way to an over-fit solution.  My final KDD submission was using TreeNet which means coarse classing was not imposed, but if I put the solution into operations I would re-do the work by imposing floors and caps on the input variables to make the model more robust to future outliers.  (continued…)"
MachineLearning,3d4vr6,wil_dogg,2,Tue Jul 14 15:07:46 2015 UTC,"Sooooo...I heard about the KDD competition, saw there was plenty of time to work on a solid submission, and even more important I saw that the methods I use and the stuff that I know were well-suited to the problem at hand.  So I downloaded the data and took a quick look and immediately had a model with 3 parameters that put me at what I would call the first plateau of a C stat of about .71.  That model included ENROLLMENT_ID and average course difficulty (3rd parameter is the intercept).  So looking at that I could immediately see two things that made sense:  Course difficulty matters, and early enrollees were less likely to drop out than enrollees who signed up for later courses.  At that point I pretty much discounted any entry that was below a C stat of .69 -- those entries don't even count in my mind with regard to ranking because a cave man can get a C stat of .70 with 20 minutes work.  Next, feature extraction.  I looked at the data, saw that the time-stamped nature of the clickstream data would fit nicely into my standard data management method, and I extracted about 75 features that are very easy to interpret.  Things like the delay between when a course is opened and when an enrollee first enrolled (a measure of procrastination), the total time spent viewing videos, the average time spent on a discussion page, etc.  These are easy easy easy features to extract.  I then formed about 50 ratios that were of the form, ""what percentage of total time spent on the MOOC was spent viewing videos"".  Again, very simple stuff that I expected to pop into the model.  By the time I had that work done I had submitted about 5 models and had consistently creeped up the leader board to the point where I was in the top 10% of about 200 entries, 150 of which were above C = .70.  I felt pretty good about that because I wasn't using the more complex techniques that I was pretty sure would end up in the winner's circle, but I was being true to my real-life constraints, and I also felt like I had a few tricks I could try to climb a bit further.  I saw two segmentation schemes that I wanted to try.  The first, segmenting my total engagement, bumped me a bit higher with regard to the C stat (like from .86 to .87), but at the same time many more entries were coming in so I was actually slipping down the leaderboard.  It made perfect sense that segmenting by total number of minutes spent on the course would improve the results, but the magnitude of the improvement was not large enough to convince me to push on that further, so I stuck with a simple 4 segment scheme that was more or less modeling each of 4 equal-sized engagement quartiles.  Once again, not much rocket science there.    The second segmentation scheme, segment by course, was one I thought would surely give me more juice.  It makes perfect sense that each course has a different configuration of pages, different videos and different problem sets, and thus the relation between my extracted features and drop-out rates would surely vary by course, and by enough to justify course-specific sub-models.  But I was consistently disappointed by the lack of improvement on validation relative to the training sample.  And this was after de-tuning my method to avoid over-fit, particularly on the courses where there were fewer enrollees and a naturally smaller sample size.  So I saw I was actually getting worse results on validation than segmenting by engagement, and I decided that further pursuing course-level segmentation was not warranted -- it made logical sense, but the data did not agree and I needed to move on.  The other innovation that failed to yield gains on validation even though it worked much better on the training data was taking the enrollee's person-level history into account.  When I saw that some enrollees were in both the training and the validation sample, and that so many had multiple course enrollments in the training sample (and the validation sample) I saw that I could use information on their drop-out rates in the training sample to create additional features for those enrollees who were in both the training sample and the validation sample.  When I did that I was seeing significant improvements in my C stat on the training sample, basically clawing my way into the top 5% of entries, but I simply could not get it to work on validation.  And it made no sense to me why it was not validating well, because I was keeping the feature extracting very simple, and I tried several ways of scaling the final model scores but just couldn't get it to work.  At that point I could either call in others on my team to assist (and then require them to enter the KDD cup and join my team, in order to be true to the KDD rules) or simply put my pencil down.  So I cried uncle and put my pencil down.  But just because the pencil was down didn't mean I couldn't reach for TreeNet.  So I ran my engagement segmentation scheme through TreeNet, fitting 4 different treenet models on each engagement segment, and then combining those models with simple math + code.  That model improved my C stat on validation by about .002 -- it moved me up the leader board, but not by much, which was actually cool, because my non-TreeNet model is so simple that it would be very easy to sell to clients, and also a no-brainer with regard to implementation given the simplicity of the code.    I also though I could reach out to Dan Steinberg and get some guidance on using TreeNet and Random Forest (I felt that RF would be particularly useful for fitting models on courses with small enrollment) but I was also at a point where my self-imposed time constraints were there, and not being an expert at RF left me thinking I wouldn't go there in a real-world project, so at that point both pencil and mouse were put down.  So my final model was a set of 4 simple linear additive models, each generated by a genetic algorithm that was capped at using no more than 25 input variables, with no variable interactions allowed within each segment, and with coarse classing imposed on all input variables.    At that point interpretation was pretty simple -- enrollees who spent more time engaged on the course were less likely to drop out, and as engagement increases then factors such as the amount of time spent watching videos and completing problem sets mattered.  Knowing an enrollee's cumulative history across all past and present enrollments in course matters when predicting their performance in the next course they enroll in, but getting that to scale correctly was challenging so that information was ignored in the current solution for the sake of having a model that performed reasonably well.  The model's accuracy could be improved by at least 3 additional actions -- relax the coarse classing constraint, relax the constraint on model complexity (and call in a Random Forest expert), and finally spend more time figuring out how to scale information on prior enrollments.  Of the three, the scaling of information on prior enrollments is what I would put at the highest priority, because it is logically important, clearly not working at this time, and does not involve relaxing constraints that, at this time, I would put on a practical solution for operational implementation.  Respectfully submitted after a day at Grand Canyon with my kid."
MachineLearning,3d3yzs,benjaminwilson,1 point,Mon Jul 13 09:42:57 2015 UTC,The site they've made out of this dataset is amazing.
MachineLearning,3d68lv,askerrore,2,Mon Jul 13 21:10:34 2015 UTC,"I'm am by no means an expert, so take this with a grain of salt. Hopefully my answer can at least give you some terms to google.   The process that you allude to, dividing the parameter space into subspaces and fitting separate models in each one to minimize error, sounds like using a regression tree. One particular example is random forests, where the data is split randomly a number of times, a model is fit to each subset of data, and then a majority vote is used to average the result of the predictions together.   This is of course very different from analyzing the errors by hand, as these methods may not give much intuition for why the data is split as it is."
MachineLearning,3d68lv,thisaintnogame,2,Tue Jul 14 02:42:06 2015 UTC,"So I'm coming at this from more of a statistics side than a machine learning side.   It seems like what you're after is residual analysis.  A lot of models (again, at least on the stats side) make the assumption that errors are randomly distributed and independent, so a situation like you're asking about isn't optimized for.  What you can do is make a series of residual plots.  Plot your residuals against each of the covariates and see if you notice any patterns.  For example, the situation you describe might be indicated by two distinct groups or a linear pattern in the residuals."
MachineLearning,3d68lv,raffled,1 point,Tue Jul 14 03:54:13 2015 UTC,What does error mean in his context though? I doubt it's the natural error (which may in fact be iid) but actually the model's failed attempt to characterize its distribution with relation to the given features.  Perhaps some features are very positively/negatively correlated that the model might not be taking into account.
MachineLearning,3d68lv,MusicIsLife1995,1 point,Tue Jul 14 15:02:12 2015 UTC,"Right.  The residuals might look great in the standard actual vs. fitted value or residuals vs. index plots in a situation like this.  What it sounds like OP was getting at was what if the error is systematic with respect to one or more of the explanatory variables.  By plotting the error vs. each of them, he can check if these relationships are there.  If features are correlated, there's going to be some variance inflation in the standard error estimates (assuming some kind of parametric model) of the coefficients, but this shouldn't negatively impact the predictions themselves."
MachineLearning,3d68lv,raffled,1 point,Tue Jul 14 16:15:09 2015 UTC,"That makes sense, so the OP thinks a specific feature or set of features is causing increased error in his model. Then one could assume either the model isn't taking these specific features into account accurately or that the features are incorrect."
MachineLearning,3d68lv,MusicIsLife1995,1 point,Tue Jul 14 20:23:15 2015 UTC,"That was my understanding of his problem.  A situation like this would probably come from a violated assumption (e.g., OLS assumes a linear relationship, so some explanatory variables might need transformed).    In OP's example, the model underestimates in some subspace of X and overestimates in another.  Creating some derived categorical variable to account for this could work, or simply splitting the data and creating two models is another option like he said."
MachineLearning,3d6rgg,SudoSilman,1 point,Mon Jul 13 23:31:08 2015 UTC,"search the sub's archives and /r/compSci, /r/genetic_algorithms/ , etc, e.g. there's now (at least) 3 books on deep learning (the last is from MS research,   https://www.reddit.com/r/MachineLearning/search?q=deep+learning+textbook&restrict_sr=on  http://research.microsoft.com/apps/pubs/?id=209355"
MachineLearning,3d2i7g,____BOLD____,5,Mon Jul 13 00:20:37 2015 UTC,I wonder if Google is trying to ride out this issue and hope everyone forgets about it.
MachineLearning,3d2i7g,zdk,6,Mon Jul 13 00:27:48 2015 UTC,"Inventions previously disclosed to the public (i.e. in academic papers) should get denied based on prior art, unless these were filed before the publications came out?"
MachineLearning,3d2i7g,ABCDEFandG,1 point,Mon Jul 13 12:49:28 2015 UTC,Are people/companies outside of the US affected by this at all?
MachineLearning,3d2i7g,sandsmark,2,Mon Jul 13 08:26:37 2015 UTC,"if you want to do business related to machine learning in the US, at least."
MachineLearning,3d2i7g,zdk,1 point,Mon Jul 13 14:27:51 2015 UTC,"indirectly, yes. Lets say you're trying to patent your own algorithm. A patent published in the US would be considered prior art for a patent filed in Europe and could be denied on that basis."
MachineLearning,3d2i7g,piesdesparramaos,1 point,Mon Jul 13 19:50:41 2015 UTC,"AFAIK in Europe you cannot patent algorithms. Also, op meant if they can sue a company in Europe for, let's say using word2vec."
MachineLearning,3d2i7g,sanxiyn,1 point,Mon Jul 13 21:30:19 2015 UTC,"In the specific case of word2vec, it is licensed under Apache License 2.0, whose clause 3 is Grant of Patent License."
MachineLearning,3d2i7g,siblbombs,1 point,Tue Jul 14 04:54:22 2015 UTC,Perhaps google is filing patents it knows will be denied to make it even harder for someone else to file something similar?
MachineLearning,3d2i7g,piesdesparramaos,17,Mon Jul 13 00:39:34 2015 UTC,perhaps they are filing patents because they want the patents....
MachineLearning,3d2i7g,sandsmark,7,Mon Jul 13 08:24:26 2015 UTC,"the correct way to do that is to file defensive publications which are added to the database at ip.com, which are used by the patent office when processing new patents.  The Open Invention Network lawyers help people with writing and submitting these for free, so it isn't like Google doesn't know about this as they are a pretty significant member of the OIN already."
MachineLearning,3d2i7g,siblbombs,3,Mon Jul 13 14:27:05 2015 UTC,Good point.
MachineLearning,3d5cvt,thewetness,1 point,Mon Jul 13 17:29:54 2015 UTC,I thought the graph data structure was nested dictionaries. I.e. each key is a node and the values are themselves dictionaries with connected nodes as keys and the type or value of the connection as values.
MachineLearning,3d5cvt,kirwi,1 point,Mon Jul 13 18:16:04 2015 UTC,"You might be interested in graph-based semi-supervised learning: it's about having a partially labeled set of instances, and an undirected weighted graph defined over them, encoding their pairwise ""similarity"" degree. The task mainly consists in ""spreading"" label information across links in the graph. This is usually dealt by doing MAP inference in some kind of Markov random field.  This book chapter by Bengio, Y. et al. is a really nice read: www.iro.umontreal.ca/~lisa/pointeurs/bengio_ssl.pdf"
MachineLearning,3d1piw,vikkamath,7,Sun Jul 12 20:22:31 2015 UTC,Awesome. Is there a video of this somewhere does anyone know?
MachineLearning,3d1piw,simonhughes22,2,Sun Jul 12 21:15:10 2015 UTC,"There was a live stream of the event, but unfortunately archived video doesn't seem to be available."
MachineLearning,3d1piw,piesdesparramaos,1 point,Sun Jul 12 22:36:25 2015 UTC,dammit! the archived video was working a couple of days ago but now is unavailable... I intended to watch it whenever I had some free time..
MachineLearning,3d1piw,evc123,2,Mon Jul 13 00:53:39 2015 UTC,Does Tieleman/Murphy/Schmidhuber's comment during the Q&A at the end mean that unsupervised learning can be solved by using a multi-armed bandit as the reward signal for DQN?
MachineLearning,3d1piw,sieisteinmodel,4,Sun Jul 12 22:28:44 2015 UTC,I think it is more of finding a reward function that is independent of the task and does not require external knowledge to be turned into a numerical reward: things like interestingness or empowerment.  It is somewhat to reinforcement learning what unsupervised learning is to supervised learning.
MachineLearning,3d1piw,Noncomment,3,Mon Jul 13 09:14:03 2015 UTC,"Schmidhuber has a very interesting page on it here: http://people.idsia.ch/~juergen/interest.html   Fundamental Principle of Artificial Curiosity and Creativity: Reward the reward- optimizing controller for actions yielding data that cause improvements of the adaptive predictor or data compressor!  (Formulated in the early 1990s; basis of much of the recent work in Developmental Robotics since 2004)  Variant 1: Reward the controller whenever the predictor errs [1990; refs 1a, 1, 2].  Variant 2: Reward the controller whenever the predictor improves / becomes more reliable [1991; refs 3, 4, 6, 13, 14].  Variant 3: Reward the controller in proportion to the Kullback-Leibler distance between the predictor's subjective probability distributions before and after an observation - the relative entropy between its prior and posterior [1995; ref 6].  Variant 4 (zero sum intrinsic reward games): Two reward- maximizing modules bet on outcomes of potentially surprising experiments they have agreed upon [1997-2002; refs 8, 11, 12].  Variant 5 (progress in data compression): Store entire life, keep trying to compress it, reward controller for actions that yield data causing compressor improvements [1990s - 2008; e.g., refs 14-17].  Both art and science are by-products of the desire to create / discover more data that is compressible in hitherto unknown ways! [Refs 14-21, 35]  Greedy but practical Variant 6 (PowerPlay): Incrementally build a more and more general problem solver as follows. Systematically generate pairs of new (possibly self-invented) tasks and modifications of the current problem solver (where subjectively simple, low-complexity pairs come first), until a more powerful problem solver is found that provably solves all previously learned tasks plus the new one, while the unmodified predecessor does not. New skills may (partially) re-use previously learned skills, that is, tasks and solver modifications that used to be subjectively complex may become subjectively simple. Wow-effects are achieved by continually making previously learned skills more computationally efficient such that they require less time and storage space [2011-; e.g., refs 29-31]."
MachineLearning,3d1piw,Foxtr0t,1 point,Mon Jul 13 07:27:23 2015 UTC,"if it's not a joke, it really sucks."
MachineLearning,3d4tbn,tareumlaneuchie,6,Mon Jul 13 15:06:36 2015 UTC,don't use pybrain. it's fucking ancient.
MachineLearning,3d4tbn,sdfsdkehg,3,Mon Jul 13 18:46:05 2015 UTC,"You could use one library to train and save the weights, then write just the forward pass in C++ and load in the pretrained weights."
MachineLearning,3d4tbn,siblbombs,4,Mon Jul 13 15:27:01 2015 UTC,"Seconded. This is what we do for sklearn-theano, but using Theano instead of C++. The only issue here is to be very sure your math matches what they implemented in the training library. This mostly applies for softmax, softplus, log type operations, though convolutions tend to have a bevy of boundary settings which can give wildly different results."
MachineLearning,3d4tbn,kkastner,-2,Mon Jul 13 16:57:21 2015 UTC,"Exactly, this is what I referred to as 'Numerical Stability'.  Sounds to me like I am about to reinvent the wheel."
MachineLearning,3d4tbn,melvinzzz,3,Mon Jul 13 22:53:06 2015 UTC,"First I highly recommend giving Theano another try.  There are a number of NN wrapper packages (Kearas, lasagne, blocks) that might help make it easier.  However to answer your actual question: Most NNs are very numerically stable, and resilient to lots of types of noise.  Minor floating point implementation difference shouldn't matter, and anyway C++ and Python use the same floating point under the hood, the only real option is float or double.  In fact, in terms of NN's robustness to floating point differences, Nvidia is moving to 16 bit floating point for some of its DNN support libraries..."
MachineLearning,3d4tbn,farsass,6,Mon Jul 13 15:28:38 2015 UTC,I don't see how this is related to numerical stability
MachineLearning,3d46l2,samratkokula,1 point,Mon Jul 13 11:27:34 2015 UTC,"""...It is highly unlikely that all 20 top-features were important in classifying each test set ..."" Than means that your model does not genaralize well..."
MachineLearning,3d46l2,svdalpha,1 point,Mon Jul 13 14:03:03 2015 UTC,"I apologize for not being clear in my question, I meant 'Not always first (out of 20) top feature is more important for each and every sample in the test data, followed by second top feature, followed by third. It may happen that 1st, 2nd, 3rd (first 3 in 20 top features) features played a crucial role in classifying the first sample in the test data. It may also happen that 18th, 19th and 20th (last 3 in 20 top features) played a crucial role in classifying the second sample in the test data. I want to know given the predicted label of a particular sample in the test data, is there any way to find out which features (out of top 20 features) played a role in classifying that particular sample in the test data. Thank you for your response."
MachineLearning,3d46l2,respeckKnuckles,1 point,Mon Jul 13 20:17:23 2015 UTC,"As others have mentioned, this depends on what model you're using to produce the classification. A NN? If so, how many hidden layers? Etc."
MachineLearning,3d46l2,ma2rten,1 point,Mon Jul 13 22:48:41 2015 UTC,"It depends what the model is, if it's a linear model just multiply the features with the weights."
MachineLearning,3d46l2,dive118,1 point,Mon Jul 13 14:27:26 2015 UTC,If it's for a neural network you can use the Lek-profile method to do a sensitivity analysis of each feature. Explanation and R Implementation: https://beckmw.wordpress.com/tag/sensitivity-analysis/
MachineLearning,3d46l2,patrickSwayzeNU,1 point,Mon Jul 13 14:49:19 2015 UTC,"""From the training set, I found the top features (say 20) and using the top features I built the model.""  ""Is there any method to find out which features were influential for a particular test sample?""  You say you have a method of determining the top features of a data set then you go on to ask us how to determine the top features of a data set.  This doesn't make sense to me."
MachineLearning,3d46l2,MusicIsLife1995,1 point,Mon Jul 13 15:15:25 2015 UTC,"I apologize for not being clear in my question, I meant 'Not always first (out of 20) top feature is more important for each and every sample in the test data, followed by second top feature, followed by third. It may happen that 1st, 2nd, 3rd (first 3 in 20 top features) features played a crucial role in classifying the first sample in the test data. It may also happen that 18th, 19th and 20th (last 3 in 20 top features) played a crucial role in classifying the second sample in the test data. I want to know given the predicted label of a particular sample in the test data, is there any way to find out which features (out of top 20 features) played a role in classifying that particular sample in the test data. Thank you for your response."
MachineLearning,3d46l2,despardesi,1 point,Mon Jul 13 20:18:33 2015 UTC,Find which features are highly correlated.
MachineLearning,3d5rti,szech758,9,Mon Jul 13 19:13:38 2015 UTC,I downvote every single deep dream fluff post in this sub
MachineLearning,3d5rti,farsass,4,Mon Jul 13 20:22:38 2015 UTC,"hah fair enough, we're all getting sick of it I guess."
MachineLearning,3d5rti,farsass,6,Mon Jul 13 20:35:25 2015 UTC,"its neat, but if the post is only about visualizations or opinions -- no technical information -- it doesn't belong here IMO."
MachineLearning,3d5rti,skgoa,1 point,Mon Jul 13 20:54:57 2015 UTC,As do I.
MachineLearning,3d3lsi,Punkter,1 point,Mon Jul 13 06:34:53 2015 UTC,"Coresets essentially means replacing a large number of points with a smaller number of points which would behave in a similar way for some algorithm.  At our startup we have recently open-sourced an implementation of coreset to find SVD here. Its from the MIT paper . One more layer of logic needs to implemented on top of it to get it work for K-Nearest Neighbors.  This code is based on implementation of the same algorithm by one of the authors of paper on MIT website, they also had put the matlab code for the logic to be used on top of it for kNN, but unfortunately I dont remember the exact URL. You can find it by a bit of Google search."
MachineLearning,3d3lsi,muktabh,1 point,Mon Jul 13 17:36:38 2015 UTC,thanks for sharing. I will try to make sense of it :)
MachineLearning,3d23sn,Nixonite,6,Sun Jul 12 22:19:02 2015 UTC,this specific method for building an ensemble is called stacking
MachineLearning,3d23sn,farsass,1 point,Sun Jul 12 22:40:03 2015 UTC,"Ah perfect! I didn't know of the specific term for it but you're right  https://en.wikipedia.org/wiki/Ensemble_learning#Stacking  I was wondering how to even google this, but having the proper name for it will help.   Thanks"
MachineLearning,3d23sn,autowikibot,2,Sun Jul 12 22:46:30 2015 UTC,"Section 10. Stacking of article  Ensemble learning:       Stacking (sometimes called stacked generalization) involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although in practice, a single-layer logistic regression model is often used as the combiner.     Relevant: LogitBoost | Bootstrap aggregating | Random forest | Yoav Freund   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
MachineLearning,3d23sn,patrickSwayzeNU,2,Sun Jul 12 22:47:10 2015 UTC,"""I tried it myself, and it did wonders for boosting the classification accuracy. My models improved from ~73% to about ~96% accuracy by tossing in several other model predictions into the feature space. I even ran cross-validation on it to check, and it's definitely 96% across several scores (recall/precision/f1).""  I'd be skeptical of a performance jump this high - make sure you aren't inducing leakage.... i.e. make sure you aren't creating meta-features on data that the lower level models have seen."
MachineLearning,3d23sn,patrickSwayzeNU,1 point,Mon Jul 13 13:31:32 2015 UTC,Can you explain that further?
MachineLearning,3d23sn,normee,2,Mon Jul 13 13:35:03 2015 UTC,"Yes.  Let's say you're using two-fold CV and you're training a Knn model, a boosted tree model and a neural network.  What you don't want to do is train each of these three models on fold1 and then predict again on fold1 to create higher order, meta-features for fold1.    Essentially, make sure you use out-of-fold predictions as meta-features.  *Edit - there are many ways to invalidate CV results.  I've done it myself plenty of times on accident.... which is why I'm always super skeptical of a really large jump like the one you've witnessed."
MachineLearning,3d23sn,patrickSwayzeNU,3,Mon Jul 13 15:04:57 2015 UTC,"I saw an example of invalid CV results from leakage just last week during an /r/science AMA with a researcher who was using genetic information to predict preterm labor in pregnant women. Someone asked her about the CV procedure used. The paper and her response indicates they had done a series of variable selection procedures to find genes that had a strong association with the outcome on the whole data set, threw these genes into a random forest classifier along with some other features, and then used leave-one-out CV to measure prediction accuracy. Never got a comment from the researcher after I pointed out why this was a problem, though granted, I'm sure it wasn't something she wanted to hear.  Rob Tibshirani has an anecdote about this same CV leakage problem in an online lecture here around 7:15. The lesson is you've got to be careful not to let your test labels get involved in any of the previous stages in order to measure your performance correctly."
MachineLearning,3d23sn,patrickSwayzeNU,1 point,Mon Jul 13 16:42:44 2015 UTC,""" Someone asked her about the CV procedure used. The paper and her response indicates they had done a series of variable selection procedures to find genes that had a strong association with the outcome on the whole data set, threw these genes into a random forest classifier along with some other features, and then used leave-one-out CV to measure prediction accuracy.""  Yep.  Good example.  *Edit - the fact that someone down voted your comment is absurd and troubling."
MachineLearning,3d23sn,patrickSwayzeNU,1 point,Mon Jul 13 16:56:56 2015 UTC,"I saw that tibshirani video the next poster linked to, although I am not entirely sure I have the concept solidified in my head.   Can you quickly check if I did it right from my description?   I created 6 models, each trained on a random train/test split (80/20, but randomized!) of dataset X. I then added the results of these 6 models as new features of dataset X, which I will now call Y.  I then created a logistic regression model on dataset Y and it performed at around 96%, whereas the previous models trained on dataset X only performed at around 73%.    That's basically my entire process. Did I mess up? By train/test split, I'm using the scikit-learn implementation seen here.  I want to know more about how cross-validation is messed up, and data leakage in machine learning algorithms, but I'm not sure where to read about it. Do you have any links?  I thought the Kaggle website had a good post - https://www.kaggle.com/wiki/Leakage  But I'm still pretty worried that I'll have some leakage in the future without knowing about it."
MachineLearning,3d23sn,patrickSwayzeNU,1 point,Mon Jul 13 20:38:47 2015 UTC,"""I then added the results of these 6 models as new features of dataset X, which I will now call Y.""  If you only predicted on the test split (20%) then you'd only have new features for 20% of X.  I.E. Y should be only 20% as large as X.  Is this the case in your set up?  "" I then created a logistic regression model on dataset Y and it performed at around 96%""  Where did you test this performance?  Do you have a 3rd data set you aren't mentioning?  ""That's basically my entire process.""  Just to be clear, you haven't described cross-validation anywhere in your description."
MachineLearning,3d23sn,EdwardRaff,1 point,Mon Jul 13 21:18:18 2015 UTC,"To answer this part ""If you only predicted on the test split (20%) then you'd only have new features for 20% of X. I.E. Y should be only 20% as large as X. Is this the case in your set up?""  I tested to see how accurate the models were using a train/test split with a 80/20 ratio, and I also tested to see the accuracy with a 5-fold cross validation check and it came up with similar performance. After seeing if the model was good or not, I then used it to predict the whole of dataset X. In other words, the model predicted all rows of X. Therefore each new feature (model prediction vector) is full, and is predicting on the original X feature set.   After then having these new features, which would be X+new features, I ran the logistic regression model on this new X+new_features dataset ( which I was referring to as Y previously) and the result of 5-fold cross-validation on dataset Y came up as 96%."
MachineLearning,3d23sn,EdwardRaff,1 point,Mon Jul 13 22:08:20 2015 UTC,"""After seeing if the model was good or not, I then used it to predict the whole of dataset X. In other words, the model predicted all rows of X. Therefore each new feature (model prediction vector) is full, and is predicting on the original X feature set. ""  This is almost exactly what I said earlier that you cant do :P.  You can't train on 80% of the data and then predict on all of the data to create a meta-feature... this induces leakage.  Think about it - would you report your performance if you trained AND predicted on the same exact data?  If you want to take this approach then you need to create 5 random folds.    Then build 5 models using 4 folds for each one.  I.e.  Train model one on folds 2-5; model two on folds 1,3,4,5; model three on folds 1,2,4,5; model four on 1,2,3,5 and model five on 1-4.  Then use each model to predict on the appropriate hold out sets.  Now you have 5 sets of predictions that can be used as new features for X because none of the features were generated by models that saw those observations at training time"
MachineLearning,3d1gu1,yyttr3,1 point,Sun Jul 12 19:12:09 2015 UTC,"I second this.  Also, how did you find Perceptrons to read? I'm struggling through it, and there are very few passages which I can really internalize."
MachineLearning,3d1gu1,Eggplay,1 point,Mon Jul 13 00:06:35 2015 UTC,"I'm only 40 pages in, but I enjoy it more than most books I have read of this type. It's a nice conversational style that I enjoy."
MachineLearning,3d1gu1,Eggplay,1 point,Mon Jul 13 18:49:35 2015 UTC,"What type of math background would be necessary (as specific as you know) that would allow for one to really dig in?  His Society of Mind and Emotion Machine drew me in, and now I want to dive into real understanding ... the math.   I'm reminded of Lord Kelvin's quote... ""I often say that when you can measure what you are speaking about, and express it in numbers, you know something about it; but when you cannot express it in numbers, your knowledge is of a meagre and unsatisfactory kind; it may be the beginning of knowledge, but you have scarcely, in your thoughts, advanced to the stage of science, whatever the matter may be."""
MachineLearning,3d1gu1,MusicIsLife1995,2,Mon Jul 13 20:33:03 2015 UTC,UPDATE: Now that I am reading further It would also be helpful to have a good grasp on abstract algebra (Group theory in particular). Which is great for someone in your position since the theory of groups is not dependent on any other fields of mathematics. It is self contained as a foundation for mathematics in and of its self and can be viewed (and is as easy to understand at a basic level) as a sort of introduction to mathematics in general.
MachineLearning,3d1tin,Russian-Assassin,1 point,Sun Jul 12 20:55:38 2015 UTC,"Have a look at NEAT: https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies  That said, I think something based on backpropagation is going to work better."
MachineLearning,3d1tin,ma2rten,1 point,Mon Jul 13 19:03:15 2015 UTC,"Neuroevolution of augmenting topologies:       NeuroEvolution of Augmenting Topologies (NEAT) is a genetic algorithm for the generation of evolving artificial neural networks (a neuroevolution technique) developed by Ken Stanley in 2002 while at The University of Texas at Austin. It alters both the weighting parameters and structures of networks, attempting to find a balance between the fitness of evolved solutions and their diversity. It is based on applying three key techniques: tracking genes with history markers to allow crossover among topologies, applying speciation (the evolution of species) to preserve innovations, and developing topologies incrementally from simple initial structures (""complexifying"").     Relevant: Compositional pattern-producing network | Neuroevolution   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
MachineLearning,3d1it1,samim23,2,Sun Jul 12 19:28:27 2015 UTC,btw. the used parameters have been published along side the video. Making the entire thing easily reproducible. a novel concept in music videos.
MachineLearning,3d1it1,simonhughes22,1 point,Sun Jul 12 22:15:56 2015 UTC,"That's going to give me nightmares! Neat but creepy. It seems obsessed with eyes, probably due to the training data."
MachineLearning,3d1it1,ttt72,1 point,Sun Jul 12 20:51:50 2015 UTC,"I actually did some ""guided dream"" stuff in there to, seeding it with more eyes ;-) Choice of outputs is up to the user. With custom models, guided dreams and optical flow all published, we are bound to see the next generation of this very soon."
MachineLearning,3d1fm7,compsc,3,Sun Jul 12 19:02:09 2015 UTC,"So, my immediate thought is that if you only have a page of text/a few paragraphs per writer you are not going to build a good model. You won't see many ngrams more than a handful of times.  Assuming you had more text, your approach is certainly valid and will give you a simple model. But it won't tell you much that you don't already know, so to speak.   Also, you will be unable to say anything about new words/ngrams. You can try to solve this by developing features. For example, take the character length of the ngram... maybe only Bob uses long words. Or abstract the words into categories... maybe only Jim talks about animals. Then, even if we have never seen the word ""cat"" before, we can predict that it likely to be Jim.  In the end it depends on your context.... what your actual data looks like and what your end goal is."
MachineLearning,3d1fm7,nsfy33,1 point,Sun Jul 12 19:50:25 2015 UTC,"Sorry, a page is the input.  I have millions of paragraphs of training data.  The Jim/Bob example is contrived, but it is binary classification of english ngrams.  I understand your point that it won't tell me much that I don't already know, but in my case, it's not a problem.  I'm more interested in the set of Jim-grams in one document.  So if A, B, C, D, and E are high-probability Jim-grams, then what's valuable to me is knowing that a document has {A, B, C} as opposed to  {B, E}, for example.  So, given a document consisting of ngrams {X0, ... Xn}, I want a subset {Xi, Xj, Xk, ...} which, in expectation, is 90%+ Jim-grams.  Let's say each document is about half and half.  The larger the subset the better, but I'd rather have a set of 5 that I'm 90% sure about than a set of 50 that I'm only 70% sure about.  Obviously these would be tuneable parameters.  So yes, a brute-force set of bigrams could work, but I was hoping for something a bit more flexible."
MachineLearning,3d1fm7,UnknownBinary,1 point,Sun Jul 12 20:40:35 2015 UTC,"""Stylometry"" is the kind of task that you're describing."
MachineLearning,3d1fm7,MusicIsLife1995,1 point,Sun Jul 12 21:19:33 2015 UTC,Edit Try modeling the grammar of the page + bigram model. The bigram model will get you a list of possible candidates and you can use the grammar model to weed out the false positives (things that don't really form complete sentences Jim/Bob would write).
MachineLearning,3cxuqg,Lost4468,29,Sat Jul 11 19:24:13 2015 UTC,"Human drivers make mistakes every 100k miles.   Self-driving system makes decisions 10 Hz.   That's 108 more frequent.   wat  edit: I guess he was talking about autonomous driving where at 10hz decisions need to be made, vs driver assistance where you only need to make a decision much more rarely. He didn't make the point well."
MachineLearning,3cxuqg,Liz_Me,3,Sun Jul 12 07:13:16 2015 UTC,Human drivers make mistakes every 100000 miles (105 miles)-> Intervention for error correction by driver assistance systems is required at that frequency.  10Hz at 50mph is about once every 10-3 miles. That's where the 108 factor obviously comes from. Though I'm also not sure what point exactly he's trying to make with it.
MachineLearning,3cxuqg,sbjf,2,Sun Jul 12 16:54:16 2015 UTC,I think just that the problem of completely automated vs assisted/ADAS is much harder (108 more critical decisions) but worth the effort since it can eliminate nearly all deaths (as opposed to about half with ADAS) and free humans of all the time now spent driving (as opposed to nearly no new time with ADAS).
MachineLearning,3cxuqg,aiworld,4,Sun Jul 12 17:24:43 2015 UTC,Doesn't work in the rain. Doesn't work when pavement is wet or road markings are altered/not visible. Doesn't work if it doesn't already have a detailed 3D scan of the area and images of the pavement. This is not becoming a viable product in the next 20 years.
MachineLearning,3cxuqg,heptode,2,Sun Jul 12 07:11:51 2015 UTC,"Generally I'm a cynic but I am sure you are wrong on timeline. Complete industry takeover by 2031 is my safe prediction. And the transition will be essentially overnight when it happens.  I think 2025 is a 50/50 shot of the takeover scenario. Subsidies, insurance, ride sharing is a triple threat for human driving model."
MachineLearning,3cxuqg,GibbsSamplePlatter,3,Sun Jul 12 14:19:55 2015 UTC,"Though the fact is that when a human person fails to see properly in a rain, he may or may not slow down, but certainly will be taking relatively more risks than before. ""According to the California Highway Patrol, auto accidents increase by over 203% in the rain.""  A robot car might just choose to pull over and wait for the driver to continue or wait till the visibility increases :)."
MachineLearning,3cxuqg,eras,4,Sun Jul 12 12:34:31 2015 UTC,"Of course they do increase. It rains, like, once a year there. It snows for maybe for a week a year in Seattle area (and then not every year), and I bet accidents increase 10x. In Finland? Probably not so much.  The problem is these are unsolved problems that are an order (or several orders) of magnitude harder than anything they have solved so far. It's not a coincidence that Thrun has left Google. It's going to get exponentially harder to solve the remaining issues, and Google is not even a car company, so it won't be making any cars.  Much of what is being touted as a breakthrough was invented in mid-90s by Ernst Dickmanns (and paid for by Daimler Benz). Look him up, he was riding self-driving cars on the Autobahn in, like, 1994.   What we have now is more advanced, sure, but many fundamental problems remain, at least if we want these cars to share common roads with human drivers.  Now if there was a way to segregate the self-driving cars onto separate roadways, I'd say we'd have a viable product (that is, a completely autonomous vehicle) in 10-15 years. As things stand, all we're going to get is advanced driver assist on the highway which doesn't let you let go of the steering wheel and read a newspaper, which kind of defeats the purpose of the whole thing."
MachineLearning,3cxuqg,heptode,1 point,Sun Jul 12 22:54:05 2015 UTC,Don't forget the winter. The mother of all changeable conditions. You really need to feel the road as you're driving on snow and it's never the same.   3D scanning every road isn't a problem though. Google has already done it for Maps (Street View) in America and most of Europe so I don't see why they couldn't use that.   As for the rain - just the fact that you can't drive it in the rain makes this car useless.
MachineLearning,3cxuqg,Beetush,1 point,Sun Jul 12 09:06:04 2015 UTC,"The 3D scan would have to be up to date. All the street view data in my city is a couple of years old, and things have changed since then."
MachineLearning,3cxuqg,tariban,3,Sun Jul 12 11:45:54 2015 UTC,Why can't the car thats there right now update the data?
MachineLearning,3cxuqg,hi117,2,Sun Jul 12 17:31:47 2015 UTC,"Because they are using that data to perform localisation, rather than mapping. To do both simultaneously in unconstrained environments is a pretty unsolved problem"
MachineLearning,3cxuqg,tariban,1 point,Mon Jul 13 00:05:31 2015 UTC,"Because it has to be driven by a human to do so, and it'd have to upload a ton of data. Read up on localization. Cars use a combination of sensors to determine where they are on the pavement. The most coarse of those is GPS, then there's 3d scan, video feed of the pavement, and possibly other things. All of that gets combined to get a more accurate estimate. With one or more of the feeds missing, or lacking reference data, your estimate is going to suffer."
MachineLearning,3cxuqg,heptode,1 point,Sun Jul 12 18:27:17 2015 UTC,The amount of storage and bandwidth would be immense. We're talking Facebook/Youtube in terms of scale which isn't impossible but would no doubt be very expensive and challenging to do right especially the security/authentication aspects of the data.
MachineLearning,3cxuqg,wescotte,1 point,Sun Jul 12 18:04:16 2015 UTC,I can't wait. I want to make some crazy destruction derby with a whole bunch of them vs 1 mad max robotic car controlled by a person.
MachineLearning,3d01e9,Jontte,5,Sun Jul 12 09:42:58 2015 UTC,"If you already have the word embedding fixed (will no be training them during the run) then you can just have the LSTM output a equal dimension vector and minimize the squared loss between output and actual word. During test/generative runs, either feed back directly or use a similarity measurement to find the closest embedding and use that."
MachineLearning,3d01e9,siblbombs,4,Sun Jul 12 12:49:06 2015 UTC,"I don't think this is a good idea. There is a reason word2vec uses cosine similarity instead of a normal distance metric. In my brief experiment with it, two very similar words had a big distance between them. But when I multiplied one word by 0.75, they became much closer to each other.  I don't really understand why, but the scale of the values for each dimension don't actually matter. Just the direction that the vector is pointing.  So I think it would work better if you optimized for cosine similarity between the predicted word vector and the actual word vector."
MachineLearning,3d01e9,Noncomment,1 point,Sun Jul 12 22:00:37 2015 UTC,Fair enough.
MachineLearning,3d01e9,siblbombs,1 point,Sun Jul 12 22:44:55 2015 UTC,"Then, if you want to generate non-deterministically, I would suggest the following:   Take the N closest words and their distances to your computed output Find a probability distribution over these distances - you can apply the softmax function to their inverse distances, for instance Sample from this distribution.   Good luck! Would love to see code + results."
MachineLearning,3d01e9,jayhack,2,Sun Jul 12 16:16:41 2015 UTC,Maybe you could pick from the nearest neighbors of the network's output ?
MachineLearning,3d01e9,pilooch,1 point,Sun Jul 12 09:59:43 2015 UTC,"I considered it, but that way it will only return words that are nearby in the word2vec space = synonyms."
MachineLearning,3d01e9,pilooch,2,Sun Jul 12 10:05:36 2015 UTC,At worst it would give you a possibly semantically close word to the currently predicted word for the window under consideration. Doesn't feel too wrong to me.
MachineLearning,3d01e9,lars_,1 point,Sun Jul 12 10:21:39 2015 UTC,"Not wrong at all, but I'm looking for something more complex. It should be able to choose between terminating the sentence or continuing it with a new clause, for example. And the predicted words don't have to be semantically close to each other as long as they are somehow sensible given the word context."
MachineLearning,3d01e9,iamtrask,2,Sun Jul 12 10:40:13 2015 UTC,"You can still use a softmax as the output layer, regardless of what the input layer is."
MachineLearning,3d01e9,iamtrask,1 point,Sun Jul 12 10:25:24 2015 UTC,I wish to avoid one-hot encoding type of approach (hence word2vec). The database I'm using contains millions of words and learning an explicit probability distribution over all of them doesn't sound feasible to me.
MachineLearning,3d01e9,lars_,1 point,Sun Jul 12 10:54:38 2015 UTC,Theoretically you might be able to use the hierarchical softmax of word2vec to hash to a vocab of words. I'd be surprised if word2vec ever became that precise but if you wanted to reduce the size of your output matrix that might work.
MachineLearning,3d01e9,sergii_gavrylov,1 point,Sun Jul 12 11:13:08 2015 UTC,"Also, word2vec is unordered, so predicting the next word wouldn't likely work as well as an LSTM as in karpathy's work, or maybe an ordered word2vec sucha s http://arxiv.org/abs/1506.02338 but I have doubts there as well. (modeling literally millions of possible outputs is just such a massive probability distribution)"
MachineLearning,3d01e9,lars_,1 point,Sun Jul 12 11:14:37 2015 UTC,"Yeah, a million way soft-max seems infeasible.  The standard approach there is to do a hierarchical softmax, or to just cut down the vocabulary size. There's also this recent paper, which seems to go a long way towards solving this problem: http://arxiv.org/pdf/1412.7091.pdf"
MachineLearning,3cw5xq,modeless,85,Sat Jul 11 07:35:30 2015 UTC,I'm going to use deep learning to create a bot that can create the dankest memes anyone has ever seen
MachineLearning,3cw5xq,mongoosefist,36,Sat Jul 11 11:00:23 2015 UTC,"You don't even need deep learning.  if comment.text.tolower().matches(""ayy+""):     comment.reply(""lmao"")   ...I may have done this before."
MachineLearning,3cw5xq,mr_yogurt,19,Sat Jul 11 14:07:23 2015 UTC,"I'm pretty sure this would rake it in:  if "" or "" in comment.text and comment.text[-1] == ""?"":   comment.reply(""Yes"")"
MachineLearning,3cw5xq,Melchoir,4,Sat Jul 11 19:19:02 2015 UTC,"Void main (void) {          Printf(""hello world!""); }  i just started learning how to code"
MachineLearning,3cw5xq,MasterENGtrainee,6,Sat Jul 11 20:32:32 2015 UTC,"I prefer my trusty ol' Python 2.7  print ""hello world!"""
MachineLearning,3cw5xq,Capn_Cook,7,Sat Jul 11 22:48:58 2015 UTC,"Upgrade to 3, pleb!  print(""hello, world!"")"
MachineLearning,3cw5xq,seekoon,4,Sun Jul 12 00:48:57 2015 UTC,s/Yes/¿Porque no los dos?/
MachineLearning,3cw5xq,Ilyanep,11,Sat Jul 11 21:18:45 2015 UTC,Note that you can also find this data shared on BigQuery - run queries over the whole dataset and in seconds for free (1TB free monthly quota for everyone).  See more at /r/bigquery/comments/3cej2b/17_billion_reddit_comments_loaded_on_bigquery/
MachineLearning,3cw5xq,fhoffa,2,Sat Jul 11 18:10:38 2015 UTC,"Is that really the whole dataset, or only the 1 month dataset?  Edit: I see now it's all there, but in multiple tables."
MachineLearning,3cw5xq,numorate,2,Sat Jul 11 18:51:13 2015 UTC,"I want all the url submissions in a given subreddit, but all I can find in the tables is ""link_id"". How do I map link_ids to urls?"
MachineLearning,3cw5xq,fhoffa,1 point,Sat Jul 11 19:58:01 2015 UTC,I don't have that dataset. /u/Stuck_In_The_Matrix might be able to help :)
MachineLearning,3cw5xq,Stuck_In_the_Matrix,1 point,Mon Jul 13 19:11:23 2015 UTC,Thanks for the alert! :)
MachineLearning,3cw5xq,Stuck_In_the_Matrix,1 point,Mon Jul 13 20:36:47 2015 UTC,You'll want to use the submission objects.  I'm currently organizing that data and hope to have it out shortly.
MachineLearning,3cw5xq,numorate,1 point,Mon Jul 13 20:36:38 2015 UTC,Awesome thanks.
MachineLearning,3cw5xq,maxToTheJ,7,Mon Jul 13 22:08:32 2015 UTC,so awesome is all I have to say.
MachineLearning,3cw5xq,Mr_Supertramp,3,Sat Jul 11 08:32:15 2015 UTC,Its awesome! and overwhelming! Not sure what/where to start!
MachineLearning,3cw5xq,ginger_beer_m,2,Sat Jul 11 20:36:12 2015 UTC,Can anyone suggest the interesting things we can learn/investigate from this dataset?
MachineLearning,3cw5xq,crowhurst,8,Sat Jul 11 15:01:39 2015 UTC,I would like to know which colours get mentioned the most.
MachineLearning,3cw5xq,Wyxi,1 point,Sat Jul 11 16:06:22 2015 UTC,"Investigating the important matters.  On a serious note though, I would love to know answers to even mundane questions like this. Just random interesting facts."
MachineLearning,3cw5xq,fhoffa,3,Sat Jul 11 23:34:32 2015 UTC,/r/dataisbeautiful/comments/3cofju/reddit_cliques_n2_deeper_into_the_subs_oc/csxeswr
MachineLearning,3cw5xq,rickisbored,1 point,Sat Jul 11 18:12:32 2015 UTC,I want to analyze the reading levels of different subreddits.
MachineLearning,3cw5xq,holocauster-ride,1 point,Sat Jul 11 23:20:20 2015 UTC,How many upvotes will a given comment get in the next hour? What is the optimal reply to a given comment?
MachineLearning,3cw5xq,watersign,1 point,Mon Jul 13 17:39:02 2015 UTC,shitlords!!
MachineLearning,3cw5xq,michaelmalak,1 point,Tue Jul 14 02:55:01 2015 UTC,Every comment for a month
MachineLearning,3cw5xq,alexjc,8,Sat Jul 11 13:00:42 2015 UTC,"He put up the whole thing too, scroll down."
MachineLearning,3cxfd8,john_philip,1 point,Sat Jul 11 17:11:11 2015 UTC,"Nice, but I liked the google pics in the first link of the article better. This seems more like abstract art than the ""animals in clouds"" hallucinations. Neural Networks producing pareidolia galore is pretty entertaining. Wonder what they would see in the ""face on Mars"" picture.."
MachineLearning,3d0fak,2Punx2Furious,5,Sun Jul 12 13:21:39 2015 UTC,"It's a good idea -- and needed as Q&A systems scale out to being able to answer lots of kinds of questions.  Historically, Q&A systems like Watson haven't been designed to recognize potential contradictions in the facts or passages they consider returning as answers.  For example, when trying to answer a question like ""What is Barack Obama's religious affiliation?"" from the open Web, a system might retrieve a majority of passages saying he's Muslim (he's not), and a bunch passages saying he's Christian or Episcopalian, etc. While there are lots of ways to assess the goodness of an answer (source metadata, recency, etc.), getting the right answer here ultimately depends on the system considering that there are two potentially contradictory answers (Christian, Muslim) that need to be resolved before being returned to the user.   Systems that can fact check or detect contradiction could really help here."
MachineLearning,3d0fak,andyhickl,1 point,Sun Jul 12 13:52:17 2015 UTC,"If I'm not mistaken Watson can give you multiple answers, each with some probability of accuracy. So in this case it would tell you ""Muslim: x%"" and ""Christian: x%""."
MachineLearning,3d0fak,vincentrevelations,8,Sun Jul 12 13:55:05 2015 UTC,"The percentage reflects belief, not accuracy."
MachineLearning,3d0fak,andyhickl,-1,Sun Jul 12 14:14:09 2015 UTC,Yes.
MachineLearning,3d0fak,squareOfTwo,4,Sun Jul 12 14:15:45 2015 UTC,"Sure, but those are ranked based on likelihood that the answer is relevant to the question -- as in IR. There's not a lot of work that considers whether candidate answers contradict. Or whether they entail one another (e.g. Episcopal |= Christian).  Multiple answers are fine, especially when there's some sorting out for the human to do. I'd want a Q&A system or a fact checker to flag that there were potentially contradictory answers, but not redundant/entailing ones. In those cases, I'd just want the most specific one..."
MachineLearning,3d0fak,hi117,1 point,Sun Jul 12 14:24:43 2015 UTC,"Yeah, even if it's not perfect it could be a useful tool."
MachineLearning,3d0fak,xamdam,3,Sun Jul 12 14:26:52 2015 UTC,"If they would open source Watson, then yes :D"
MachineLearning,3cwzwc,flukeskywalker,4,Sat Jul 11 14:49:11 2015 UTC,"Cool! It would be great if someone could put this in youtube or something, so we can fast forward the video and so on."
MachineLearning,3cwzwc,piesdesparramaos,2,Sat Jul 11 16:00:59 2015 UTC,Any idea where I can find the talk now?
MachineLearning,3cwzwc,logrech,6,Mon Jul 13 03:14:36 2015 UTC,"Did anybody ask the Google guys in the panel about their attempts at patenting everything under the sun that relates to deep learning? On one hand they talk about being open, publishing their research, and on the other hand they are out their to screw everybody else by claiming to invent deep learning algorithms and methods. What hypocrisy!"
MachineLearning,3cwzwc,pseudopotential,4,Sun Jul 12 04:25:10 2015 UTC,I'm beating myself up for not asking this. I hadn't been checking this sub for a couple of weeks so I wasn't aware of what was happening. I even got up and asked the first question (just something I was personally interested in and knew Jürgen could comment on).  Hopefully there will be another opportunity in the near future.
MachineLearning,3cwzwc,FluxSeeds,1 point,Sun Jul 12 09:01:54 2015 UTC,"they've filed the patents. I think if we send enough evidence to the patent office of prior art, they will get invalidated."
MachineLearning,3cwzwc,r-sync,0,Mon Jul 13 06:21:53 2015 UTC,"Such amazing minds, such boring questions..."
MachineLearning,3cwzwc,__AndrewB__,2,Sat Jul 11 21:33:01 2015 UTC,I thought the 2nd question concerning research at big vs small companies was useful.
MachineLearning,3cy7i7,scofface,3,Sat Jul 11 21:16:45 2015 UTC,"This is very similar to what my dissertation was on. I would recommend using an unscented Kalman filter with bicycle model kinematics. For an example, see Havlak and Campbell's paper, 'Discrete and continuous: probabilistic anticipation for autonomous robots in urban environments'. Obviously, you don't need something as complicated as their hybrid Gaussian mixture model, but you should use the same dynamics they present. Edit: link http://arxiv.org/abs/1309.0766"
MachineLearning,3cy7i7,stupider_than_you,1 point,Sun Jul 12 00:50:19 2015 UTC,This sounds great. I will give it a try tomorrow. Thanks for the info.
MachineLearning,3cy7i7,farsass,1 point,Sun Jul 12 01:05:20 2015 UTC,Is this simulated data? Remember a real car has exogenous inputs. It is kinda unreasonable to expect to be able to forecast many steps ahead if you are not measuring this input.
MachineLearning,3cy7i7,farsass,1 point,Sat Jul 11 22:37:57 2015 UTC,It is real data on a toy robot car driving around in an enclosed rectangular space. I get 30 data points per second. I have to predict the final 60 frames/datapoints without seeing the data. The robot has very sporadic motion and will sometimes get stuck for a second or so in a corner of the rectangle.
MachineLearning,3cy7i7,UndergroundOak,2,Sat Jul 11 23:00:16 2015 UTC,"The fact that it is constrained to a box makes things much more complicated, so IMO you should work out the unconstrained case first.  You need a model of the robot car. Assuming it is a linear system, you should probably fit an ARMAX  (1)  (2) model. In order to predict the future states, you will also need to measure the input that is driving the car or another model describing it if it is stochastic."
MachineLearning,3cy7i7,marijnfs,1 point,Sat Jul 11 23:41:39 2015 UTC,I haven't had a chance to try it out but you may look at https://github.com/numenta/nupic.geospatial as a possible solution as well. Interesting to see how it performs against more traditional/simpler methods.
MachineLearning,3cxdze,__AndrewB__,6,Sat Jul 11 16:59:53 2015 UTC,"Consider making 100 rollouts, with 5 actions glances each. Some of these rollouts will lead to good things and some to bad things. Intuitively, you want to look at, say, the top 50 good rollouts and encourage your policy to make more of those kinds of glances, and take the other 50 bad rollouts and discourage those glances in the future.  The gradient of log p(l|...) intuitively tells you how to change the parameters to encourage the specific glances l seen in that rollout. You want to take a small step in this direction if that rollout was good. Whether something is good or bad in the end is in this case measured by p(y|...), since this is the log likelihood assigned by the model to the correct labels. That's why you see this multiplied into the grad log p(l|...) term. In practice, as they mention in the paper it's important to have a good baseline estimator b which gives you some kind of a basic expectation of what p(y|...) is normally like. This way if things go better than expected (i.e. p(y|...) - b) is positive, then the rollout turned out surprisingly well and you want to encourage the glances taken in it. And if the term is negative then the rollout went surprisingly bad and you want to discourage its glances. The multiplication hence does the right thing."
MachineLearning,3cxdze,badmephisto,1 point,Sat Jul 11 18:50:12 2015 UTC,"Thank You so much, this helps me a lot!"
MachineLearning,3cxdze,basedgodel,2,Sat Jul 11 20:01:48 2015 UTC,"Thanks for posting this. I've been pretty interested in DeepMind's original paper on recurrent attention (http://arxiv.org/abs/1406.6247) and it's nice to see they've been extending it and getting some good results. If you look at the paper by Minh et al. they explain the reinforcement learning motivation behind the learning algorithm and also cite some papers which are good references for learning about policy gradients and the REINFORCE algorithm. (http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)  I am a bit confused by some of the notation in this paper though if somebody with more experience with it would like to help me out.When they talk about drawing  lm ∼ p(l_n|I, W)  are they drawing the entire sequence of N glimpses from the distribution or just one glimpse. There's no mention of time steps anywhere in the learning algorithm description but in a previous section they say ""The classification network outputs a prediction for the class label y based on the final feature vector r 1_N of the lower recurrent layer"". If they only make a prediction at the last time step then p(y|lm, I, W) should be a function of the entire sequence and log p(lm |I, W) would reduce to the sum of log probabilities of each glimpse location. I'm assuming this is how it works as this is the same algorithm as Minh's paper, but they don't make this explicit.  The other alternative would be that they try to maximize the cross entropy of the classifier network at each time step. Anyone have any insight into this?"
MachineLearning,3cxaji,sanity,1 point,Sat Jul 11 16:29:09 2015 UTC,Any fixes to the RandomForest discrepancies from last time?
MachineLearning,3cxaji,EdwardRaff,1 point,Sat Jul 11 17:56:40 2015 UTC,"I recall there being some training time comparisons to Weka, but no bugs or errors.  Was there something you had in mind you wanted an answer for?  Btw, would you be open to re-benchmarking on the MNIST data set?  I suspect we'll get more than a 15% speed up from where it was.  Note, the api is a bit different than before (and i don't believe the doc page addresses the difference).  The DecisionTreeBuilder is the class that you'll want to use to build a DecisionTree. TreeBuilder is no longer a class.  Also, like last time, the setter (in DecisionTreeBuilder): ignoreAttributeProbability() should be used to give the probability that an attribute will be ignored (it is not sqrt(n) by default)"
MachineLearning,3cxaji,AlexTHawk,2,Mon Jul 13 04:03:48 2015 UTC,"I tried but I think you have some bugs. Tried to set your code to do something more equivalent to Weka (full depth, splitting to 1, etc) but the decision trees built by QuickML seem to go no deeper than 5, where a tree built all the way out should be reaching depths of 13-17 depending on the branch. Code looked like                  DecisionTreeBuilder<ClassifierInstance> dtb = new DecisionTreeBuilder()                     .ignoreAttributeProbability(1 - Math.sqrt(784) / 784.0)                     .maxDepth(Integer.MAX_VALUE)                     .minSplitFraction(1e-15)                     .numSamplesPerNumericBin(1);             quickml.supervised.tree.decisionTree.DecisionTree tree = dtb.buildPredictiveModel(mnistQML);             forest.add(tree);   You really need to add documentation to this. Its a mess to try and figure out and overly ""factory hell"" to wade through without explanation. I haven't hit a single line of useful javadoc.    Was there something you had in mind you wanted an answer for?   Yea, mostly why there are so many discrepancies with what every other RandomForest package does (random probability of each feature, no bagging, 0.7 probability instead of something to mimic sqrt(D) features, etc)."
MachineLearning,3cxaji,EdwardRaff,1 point,Tue Jul 14 01:32:01 2015 UTC,"Hey Edward,   you are completely correct about the need for documentation.  Will try to add some soon.  On a different note, there is another hyper-parameter: minAttributeValueOccurences, which is causing early termination of your trees. You can set it to 1 like you set the other hyper-parameters in the DecisionTreeBuilder. This hyper-parameter is actually quite important for predictive accuracy when working with sparse data.  Basically, it is the minimum number of samples one needs above And below the split threshold (for numeric attributes), for the split to be considered valid. For categorical attributes, it is the minimum number of samples one must have at a node with a particular attribute value in order for that attribute value to be considered as a candidate for the trueSet. By default it is set to 8, which is generally a significantly better value than 1.  I know...this really should have been javadocced :-(     If you want to eak out another layer or 2 of depth, you can also set numNumericBins to 2, as there is a requirement that each bin have at least 1 sample in it.  Alex"
MachineLearning,3cxaji,AlexTHawk,1 point,Tue Jul 14 22:37:51 2015 UTC,"Doing either (or both) of the 2 hyper-parameters you suggest causes quickml to make the root node a leaf, so depth of 0. This is using your latest 0.9.1"
MachineLearning,3cxaji,EdwardRaff,1 point,Tue Jul 14 23:15:36 2015 UTC,"great catch!  Turns out there was a bug where the 2 numeric bin tree builds failed for no good reason. 0.9.2 has the fix.  If you use more than 2 bins though in 0.9.1, setting minAttributeValueOccurences to 1 should not cause early termination though...at least i don't see that behavior on the data sets used in tests."
MachineLearning,3cxaji,AlexTHawk,2,Wed Jul 15 04:14:16 2015 UTC,"quickml still isn't building trees all the way out, stopping at around depth=5."
MachineLearning,3cxaji,EdwardRaff,1 point,Thu Jul 16 00:22:18 2015 UTC,"interesting.  My guess is that because there is a skip attribute probability (rather than a commitment to consider sqrt(n) attributes) at branching, there are branches builds that are handed 0 attributes to consider 1 every 10 times or so in your data set...and as a consequence, termination occurs.  Can you send me a link to download the mnist data set that you are using? I might play around with it. From a predictive accuracy standpoint, the additional stochasticity of using skip probability seems to be a good thing (for the sake of making trees less correlated) compared to taking a fixed number of attributes at each split...but the early termination that results might be a bigger negative trade off.  Not sure."
MachineLearning,3cxaji,AlexTHawk,2,Thu Jul 16 03:48:42 2015 UTC,"Thats not it, if I set the probability to 0 it only goes to depth=7.  Its MNIST."
MachineLearning,3cxaji,EdwardRaff,1 point,Thu Jul 16 04:00:46 2015 UTC,"Ah, I believe I see now why that is.  So it looks like you should set minAttributeValueOccurences to 0.  The MultiClassAtributeValueIgnoringStrategy that is used by default in branch building requires that for a split to be valid (for a numeric branch) both sides of the split must contain minAttributeValueOccurences training instances FOR EACH classification.  A better AttibuteValueIgnoringStrategy is probably the correct solution for this data set, but by setting to 0, the trees should be  built all the way out.  Btw, do you know if a csv is available for that mnist data set?  I made a brief go at loading it and it  looks like it'd require me to do some research on loading that kind of data before i could get it to work."
MachineLearning,3cxaji,AlexTHawk,1 point,Fri Jul 17 00:13:38 2015 UTC,"ugh...noticed that ignoreAttributeProbability() takes an int instead of a double as it's argument.  QuickML 0.9.0 remedies this.  Please use that for a benchmark, if you are interested in doing that."
MachineLearning,3cxaji,AlexTHawk,-1,Mon Jul 13 17:52:31 2015 UTC,"There have been a lot of improvements, so quite possibly, but I can't say for sure."
MachineLearning,3csz33,bge0,21,Fri Jul 10 15:01:28 2015 UTC,Is this the TMZ of machine learning? :P
MachineLearning,3csz33,XalosXandrez,8,Fri Jul 10 16:00:38 2015 UTC,WorldStarHipHop
MachineLearning,3csz33,evc123,14,Fri Jul 10 17:17:33 2015 UTC,"Haha nice find!  Would've been funny to see LeCun and Hinton there as well, stealing Jurgen's lunchmoney"
MachineLearning,3csz33,death_to_patents,6,Fri Jul 10 15:17:30 2015 UTC,"It seems pretty civil from the picture, I expected some wrestling from the title ;)"
MachineLearning,3csz33,marijnfs,4,Fri Jul 10 15:33:48 2015 UTC,"Hinton isn't around, but there's a panel discussion later today (Saturday) with Schmidhuber, Bengio, LeCun and Hassabis. So we can expect a good amount of 'shots fired'."
MachineLearning,3csz33,FluxSeeds,1 point,Sat Jul 11 08:51:06 2015 UTC,Update plz!
MachineLearning,3csz33,VelveteenAmbush,8,Sun Jul 12 22:02:18 2015 UTC,"I just joined this sub recently, would anyone mind telling me who these two are please?"
MachineLearning,3csz33,MuffinCutter,13,Fri Jul 10 15:27:47 2015 UTC,"For some context, the beef: http://people.idsia.ch/~juergen/deep-learning-conspiracy.html?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=email&utm_source=Artificial_Intelligence_Weekly_8"
MachineLearning,3csz33,KimJongLil,9,Fri Jul 10 15:31:48 2015 UTC,Jurgen Schmidhuber and Yoshua Bengio
MachineLearning,3csz33,marijnfs,14,Fri Jul 10 15:29:06 2015 UTC,"J.S.: ""Dawg, you need to credit them pioneers of the field!""  Y.B.: ""You now I want to, man, but Yann and Geoff won't let me."""
MachineLearning,3csz33,Foxtr0t,23,Fri Jul 10 17:56:18 2015 UTC,citers gonna cite
MachineLearning,3csz33,USER_PVT_DONT_READ,5,Fri Jul 10 18:37:30 2015 UTC,Yoshua.
MachineLearning,3csz33,ogrisel,1 point,Fri Jul 10 17:24:18 2015 UTC,Edited
MachineLearning,3csz33,marijnfs,4,Fri Jul 10 17:25:39 2015 UTC,What if this is the true conspiracy? (and they are simulating war on the outside...)
MachineLearning,3csz33,ciolaamotore,2,Fri Jul 10 16:25:19 2015 UTC,well then this would be a great material for an awesome Michael Lewis book and I'm fine with either state of the world.
MachineLearning,3csz33,sensei_von_bonzai,2,Fri Jul 10 22:40:22 2015 UTC,Deep Boys?
MachineLearning,3csz33,ciolaamotore,12,Sat Jul 11 05:32:52 2015 UTC,In academia the fights are so brutal because they're fighting over so little ;)
MachineLearning,3csz33,GibbsSamplePlatter,1 point,Fri Jul 10 17:52:42 2015 UTC,"consider there are some quite significant industrial interests, e.g. see Google acquiring DeepMind for ~500M US$  also, getting some more funds for your research group can allow your postdocs to pay the rent"
MachineLearning,3csz33,USER_PVT_DONT_READ,4,Sat Jul 11 07:10:34 2015 UTC,It's an old joke.
MachineLearning,3csz33,GibbsSamplePlatter,2,Sat Jul 11 11:17:58 2015 UTC,"This reminds me of the time I was at the 27th Canadian Conference on Artificial Intelligence in Montreal, and Bengio was giving a Tutorial on Deep Learning and at the end was fielding questions.  I went ahead and, being a frequent reader of Less Wrong, decided to ask him what were the moral and ethical implications of Deep Learning A.I.  He replied by asking if I was a journalist.  XD"
MachineLearning,3csz33,JosephLChu,2,Sun Jul 12 09:16:39 2015 UTC,"Yeah, that's kind of like asking a rocket engineer about the ethical implications of the Prime Directive."
MachineLearning,3csayb,tushar1408,27,Fri Jul 10 11:00:15 2015 UTC,The total silence from Google about this is worrisome.
MachineLearning,3csayb,rantana,7,Fri Jul 10 15:44:52 2015 UTC,"Filing patents isn't evil. Enforcing them might be, depending."
MachineLearning,3csayb,dwf,3,Fri Jul 10 19:47:08 2015 UTC,"Their ""Don't be evil"" motto is starting to be ironic and I don't like it.   People have been using that joke for ages."
MachineLearning,3csayb,RushAndAPush,24,Fri Jul 10 20:37:53 2015 UTC,academics will probably ignore it.  it's gonna suck for startups and small companies like indico
MachineLearning,3csayb,j1395010,5,Fri Jul 10 22:10:49 2015 UTC,Set up your startup in Ukraine or China :))
MachineLearning,3csayb,Iskandar11,1 point,Fri Jul 10 15:38:18 2015 UTC,"Or even Europe, most likely, which takes a negative view of both mathematical and software patents."
MachineLearning,3csayb,_delirium,1 point,Sat Jul 11 04:42:44 2015 UTC,That's good to know. Do you know of any European countries that take the most negative views of software patents?
MachineLearning,3csayb,Iskandar11,1 point,Sun Jul 12 23:33:24 2015 UTC,"This is making the business environment in the USA extremely difficult. It would be very easy for me / anyone else with more than half a clue, to move ops to a country with a good tech base, say somewhere in Asia, and set up something there instead."
MachineLearning,3csayb,xaxa_cubed,6,Sun Jul 12 23:37:15 2015 UTC,"I quote a lawyer's opinion on this, which I saw somewhere on internet (I am not sure how does Oracle vs Google recent court case effect this) : ""since CLS Bank v. Alice (SCOTUS. June 2014), it's pretty clear that equations/algorithms, no matter how inventive, implemented on general purpose hardware are not patentable, and existing patents based on such equations/algorithms will not stand up in court."" Also: ""On the other hand, if you are thinking of a particular, relatively specialized, hardware configuration for implementing such equations, such as shown and claimed in the <some> patent, such hardware configurations may be patentable."""
MachineLearning,3csayb,muktabh,2,Mon Jul 13 04:27:01 2015 UTC,"That may be the case, but few startups are going to risk getting taken to court and accumulating the fees requires to litigate the invalidity of a patent. Google's (or Microsoft's) legal team is big enough to ""enforce"" their patents regardless of their validity."
MachineLearning,3csayb,treerex,1 point,Fri Jul 10 23:02:17 2015 UTC,"I agree. If Microsoft or Google's law division contacts a startup for patent violation, they would generally choose to stop functioning than going to court.  I, however, would like to make another point. Outside Deep Learning, other Machine Learning algorithms are patented too. So is map-reduce. However, there are startups which use these. I am however not sure whether they do so because they do not know about the patents, or they take up a risk. For Hadoop patent, when community made a similar outcry 5 years back as it is making now, Google pledged to not go to court for this patent. (As discussed somewhere else, the patent is non-binding).  My point being if one is fearing to start up due to patents, he/she will not be able to use even older Machine Learning methods and not these new ones."
MachineLearning,3csayb,muktabh,0,Sat Jul 11 01:06:59 2015 UTC,But it's clear that machine learning is patentable.
MachineLearning,3csayb,heisenbrau,1 point,Sat Jul 11 07:27:45 2015 UTC,"Not a US citizen, I have no idea how Supreme Court decisions are imposed.   Possible reasons might be that US patent office just does not consider Machine Learning as algorithms. Or another thing might be that Supreme Court revised its decision (so the old ones cease to work like in many other countries).  Also given that this question is about whether researchers will be effected by the patents, and given no researcher has been sued as of now for old Machine Learning patents (like on SVMs or LSA, which are also patented algorithms), I think the patent laws are mostly used/applicable in inter-company disputes. A startup/company using these patented technologies are mostly depending on Google's history of not suing people for using its patented technologies (like map/reduce)."
MachineLearning,3csayb,muktabh,10,Fri Jul 10 23:06:29 2015 UTC,MS has about three times as many ML patents as Google. I don't think it's fair to single out one company for doing something that's standard industry practice.
MachineLearning,3csayb,5at,2,Fri Jul 10 23:17:21 2015 UTC,"They have the patent on map-reduce too, that didn't affect that ecosystem too much."
MachineLearning,3csayb,seanieb,0,Fri Jul 10 20:11:21 2015 UTC,"It will affect a lot.  Google presently has THE best team of deep learning. On one side they have Dr. Geoffrey Hinton and his students, and on the other side you have Deepmind.  Research is done by looking at other people's work, gaining ideas, and mixing those ideas together to form something new of your own.  Without having good papers to read, independent researchers will have a very hard time coming up with their own ideas."
MachineLearning,3csayb,Bhavishya1,17,Sat Jul 11 03:06:55 2015 UTC,"Patents have nothing to do with having good papers to read. They can only patent something that has been published. So if you fear secret research that's unrelated to patents. The patent system was designed as an incentive to go public with ideas instead of keeping them secret.  Your worry is still valid that the greatest ML researchers move to companies instead of public universities, but the worry is unrelated to patents."
MachineLearning,3csayb,bonoboTP,1 point,Fri Jul 10 16:59:31 2015 UTC,"They can only patent something that has been published.    But if it's already in the public domain, how is it patentable? I thought that you basically had the choice: publish or patent."
MachineLearning,3csayb,madmooseman,7,Fri Jul 10 17:48:44 2015 UTC,"Google and Hinton are actually using algorithms from Schmidhuber's group in Switzerland, like LSTM and GPU-based parallelized convolutional neural networks. Key people of Deepmind also were Schmidhuber's students.  As I said before, I hope that at least Google's patent application “system and method for parallelizing convolutional neural networks” https://www.google.com/patents/WO2014105865A1 won't be approved due to prior art.  The earliest relevant paper (16 July 2012) cited by the patent application is this:  D. CIRESAN, U. MEIER, J. SCHIDHUBER: ""Multi-column deep neural networks for image classification"", PROCEEDINGS OF THE 2012 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR'12), 16 June 2012 (2012-06-16), pages 3642-3649, XP032232509, DOI: 10.1109/CVPR.2012.6248110  The patent examiners will probably find that there are even earlier very detailed publications on parallelizing convolutional neural networks by the same group, starting with this IJCAI paper of 2011:  D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, J. Schmidhuber. Flexible, High Performance Convolutional Neural Networks for Image Classification. International Joint Conference on Artificial Intelligence (IJCAI-2011, Barcelona), 2011. http://people.idsia.ch/~juergen/ijcai2011.pdf  Under http://people.idsia.ch/~juergen/deeplearning.html Schmidhuber writes that this decribes their ""special breed of GPU-based max-pooling convolutional networks (GPU-MPCNNs), now widely tested/used by research labs (e.g., Univ. Toronto/Google/Stanford) and companies (e.g., Apple) all over the world.""  Since 2011, ensembles of these parallelized convolutional neural networks also were used to win several competitions summarised under http://people.idsia.ch/~juergen/deeplearning.html  I cannot really see what’s different in the later publication by Alexander KRIZHEVSKY, Ilya SUTSKEVER, Geoffrey E. HINTON (2012). Or is there somebody out there who can point out a difference?  The other applications may have similar issues."
MachineLearning,3csayb,funnypatent,1 point,Sun Jul 12 13:32:42 2015 UTC,dropout is one difference. you also sound awfully close to schmidhuber himself :)
MachineLearning,3csayb,r-sync,1 point,Fri Jul 10 21:46:11 2015 UTC,you will have to give your results to googol
MachineLearning,3csayb,watersign,1 point,Sun Jul 12 15:13:02 2015 UTC,"I don't understand whether Google's patents on already published work will be valid.  The rule is: if it's in the public domain already then it's not patentable.  Even though Hinten et al themselves invented Dropout, the fact that they published it first before patenting it invalidates the patent, AFAIK.  Similarly for Q-learning + neural network - that is been around since at least 1992."
MachineLearning,3csayb,mikef22,1 point,Sat Jul 11 02:00:32 2015 UTC,"In the case of word2vec, the sample source code is under the Apache 2.0 license, which provides a patent grant.  That and researchers build on other patents all the time.  It's commercial deployment that can get... interesting."
MachineLearning,3csayb,happycube,-1,Sat Jul 11 17:13:12 2015 UTC,"Independent researchers will not be affected at all. Patents are about the state granting someone the temporary right to license a novel idea that person came up with out to other people/businesses. In exchange for that the patent holder has to publish their novel idea. If you don't aim to profit monetarily from the patented ideas, it has absolutely no effect on you."
MachineLearning,3csayb,skgoa,2,Sun Jul 12 05:36:47 2015 UTC,"wrong. patent holders have an exclusive right to practice the invention. if google wanted to waste their money, they could get an injunction to force you to stop doing research using dropout."
MachineLearning,3ctnoj,benjaminwilson,0,Fri Jul 10 18:05:59 2015 UTC,"On the 4th page of the linked PDF, Theorem 2 states the following equality:   (1-eps)1DU+eps1DD-1W=(1-eps)1D+eps1W,   which implies that DU=D, where   eps is the dampling factor 1 denotes the vector with all its entries equal to 1 W is an adjacency matrix of a symmetric graph (i.e. W(i,j)=W(j,i)=1, if node i is connected to node j) D is the diagonal matrix with (i,i) element equal to the sum of the i-th row of W (i.e. it contains the degree of the ith node) U is the matrix with all its entries being equal to 1/n (n being the number of nodes in the network).   I could not get so far why the DU=D part holds, especially that D is a diagonal matrix, whereas the product DU is not. Could someone tell me which part do I get wrong?"
MachineLearning,3ctqzr,changingourworld,2,Fri Jul 10 18:29:53 2015 UTC,This is a pretty nice intro for people learning Gradient Descent! Good job.
MachineLearning,3csmv7,mtnchkn,9,Fri Jul 10 13:17:00 2015 UTC,"These slides here are a good overview of the different types of feature engineering.  It's just a catalog of the different types, but you may find it useful."
MachineLearning,3csmv7,0111001101110000,4,Fri Jul 10 16:34:20 2015 UTC,"A lot of feature engineering is finding different representations of the data.  For example, you could reduce your dataset with pca or tsne, then feed the result to your learning algorithm.  Think about variables that might make sense to include as ratios, multiples, or differences.  It's also worth thinking about how to encode data the model can't handle.  For example, lets say you have a bunch of raw text data, and you want to put into a GBM that can't handle categorical variables with more than 128 levels.  It might makes sense to use a tool like vowpal wabbit to fit a linear model directly to the textual data, and then feed the predictions from that model into the GBM."
MachineLearning,3csmv7,pandemik,3,Fri Jul 10 16:58:08 2015 UTC,I don't think t-sne is often used as pre-processing step for supervised learning.  It's more useful to find 2D or 3D representations of the features space for visualization purpose: to visually identify local groups of samples based on the euclidean distance in the original feature space.
MachineLearning,3csmv7,ogrisel,2,Fri Jul 10 17:26:52 2015 UTC,"It can be really useful for something like a random forest or GBM, because trees can exploit the local grouping tsne provides.  It's not always useful, but it's a non-linear dimensionality reduction, so it's always interesting to try out."
MachineLearning,3csmv7,pandemik,2,Fri Jul 10 18:07:06 2015 UTC,"It's hard to answer this without knowing much about where you're at? I mean, there's the advice I'd give to e.g. a new grad student. That advice is really about honing your intuition for how the model works. For more experienced researchers, my main advice is: write fast code, and do good dev ops :). Then you can simply run more experiments."
MachineLearning,3csmv7,syllogism_,2,Sun Jul 12 09:51:47 2015 UTC,"Look at winning competitions, and understand how and why they came to the features (if they used a feature based approach). See http://www.chioka.in/kaggle-competition-solutions/ and also the winner interviews."
MachineLearning,3csmv7,kkastner,1 point,Fri Jul 10 18:33:25 2015 UTC,"i dont do it but i hear many people combine features. when possible..try to append any data you can to geographic type data like zipcodes.. so..if a bunch of data points are in CA and AZ..you can create a feature for the region of the country (southwest, northeast,etc)"
MachineLearning,3csmv7,watersign,1 point,Sat Jul 11 02:23:29 2015 UTC,"I think propositionalization is an under used tool.  Think about it. Most machine learning algorithms work on a single table of data. (attribute value representation) but most data is related to other sets of data. Hence relational databases, so you want to include this information in your model. A common way is to use aggregation. i.e. take the max, mean, sum etc from the other table. So if you have a table of customers, and you are predicting if they will buy a product, you might take the mean value of their past purchases and use that as a feature for example. But propositionalization allows you to have features which are like sql queries which are true or false for an example.  So a feature could be that they bought a particular product in the past, or they bought a product on a Tuesday. Propositionalization is a method to automatically search for these features, with out you having to manually curate them."
MachineLearning,3cx6lw,WorldLoiterer,1 point,Sat Jul 11 15:52:58 2015 UTC,This is really cool I had not seen it - Thanks.  His other work with RNN's is very cool too.
MachineLearning,3cx6lw,h1ghguy,-2,Tue Jul 14 16:25:50 2015 UTC,Please note I am not the author. Credit should go to Andrej Karpathy.
MachineLearning,3csu9d,sunrisetofu,5,Fri Jul 10 14:22:12 2015 UTC,"BPTT is just back propagation; if you're having trouble, you need to better understand back propagation. This article by Andrej Karpathy kind of forces you to grok it: http://karpathy.github.io/neuralnets/. If you go through that, you should be able to derive BPTT for an LSTM network by yourself."
MachineLearning,3csu9d,mathsive,3,Fri Jul 10 15:04:01 2015 UTC,"The deep learning book(page 288) has the backpropagation math for RNNs generally, to apply for LSTM you also need to compute the backprop through the fancy activation but other than that this should be the same."
MachineLearning,3csu9d,kkastner,1 point,Fri Jul 10 18:31:30 2015 UTC,"I would suggest Alex Graves Book http://www.cs.toronto.edu/~graves/preprint.pdf  Step by step equations for verifying your derivations for RNNs, LSTMs and other architectures. Also contains a more holistic overview of the Sequence learning sub-field."
MachineLearning,3cs1a3,bge0,6,Fri Jul 10 08:40:40 2015 UTC,http://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html
MachineLearning,3cs1a3,True-Creek,2,Fri Jul 10 10:13:28 2015 UTC,Thank you!
MachineLearning,3cs1a3,SupportVectorMachine,2,Fri Jul 10 15:24:28 2015 UTC,You can't go wrong with this one. This is the book that hooked me on machine learning as a whole.
MachineLearning,3cs1a3,Articulated-rage,2,Fri Jul 10 16:08:46 2015 UTC,Sutton and Barto are considered the foundational text
MachineLearning,3cs1a3,bushrod,2,Fri Jul 10 11:42:33 2015 UTC,And a brilliantly-written book at that.  They take somewhat complicated mathematics and make it easy to understand.
MachineLearning,3cs1a3,marcuniq,5,Fri Jul 10 15:37:55 2015 UTC,Reinforcement learning class at University College London  slides: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html videos: https://www.youtube.com/playlist?list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa
MachineLearning,3cs1a3,evc123,3,Fri Jul 10 10:22:12 2015 UTC,The original deepmind atari paper is still the best source imo: http://arxiv.org/abs/1312.5602
MachineLearning,3cs1a3,Mallekin,2,Fri Jul 10 17:23:44 2015 UTC,even if u are not into robotics i would recommend this paper: http://www.ias.tu-darmstadt.de/uploads/Publications/Kober_IJRR_2013.pdf  for a much more detailed introduction/state of the art there is: https://www.springer.com/us/book/9783642276446
MachineLearning,3cs1a3,KeinBaum,1 point,Fri Jul 10 09:35:36 2015 UTC,Are you another TU Darmstadt student? I keep being surprised by how many of us are here on reddit.
MachineLearning,3cs1a3,Mallekin,1 point,Fri Jul 10 09:57:20 2015 UTC,nope ;-) but great paper!
MachineLearning,3cs1a3,gwulfs,2,Fri Jul 10 10:18:52 2015 UTC,http://stanford.edu/class/msande338
MachineLearning,3cs1a3,organic-neural-net,2,Fri Jul 10 10:30:08 2015 UTC,"not quite a tutorial/paper but i just re-read the last few chapters of russell and norvig's AI: A Modern Approach and it had a pretty good introduction of reinforcement learning, chapter 21 i believe."
MachineLearning,3cs1a3,ganarajpr,1 point,Fri Jul 10 11:12:14 2015 UTC,You can try these to start with if you havent already seen them : http://cs.stanford.edu/people/karpathy/reinforcejs  https://github.com/karpathy/reinforcejs
MachineLearning,3csbwi,willycs40,1 point,Fri Jul 10 11:12:21 2015 UTC,I'm not familiar with the code. Are these a complete set are are they just a sampling of what you can get from the different layers?
MachineLearning,3csbwi,dominosci,2,Fri Jul 10 15:59:24 2015 UTC,"It's what you get when you set the objective to maximize the L2 norm of a certain layer, and repeatedly backpropagate the error onto the image."
MachineLearning,3csbwi,jirachiex,1 point,Sat Jul 11 06:05:52 2015 UTC,I see. For some reason I thought it was trying to maximize the output of a single neuron (the one that turns on when it sees a dog let's say). Instead we're trying to maximize all the neurons in a single layer. I'll have to study that more to understand exactly how that works.
MachineLearning,3csbwi,dominosci,1 point,Sat Jul 11 06:11:58 2015 UTC,Yup. The backpropagation is also performed at multiple scales to capture and amplify multiple objects in a single image. Here's the code: https://github.com/google/deepdream/blob/master/dream.ipynb
MachineLearning,3csbwi,jirachiex,1 point,Sat Jul 11 06:20:06 2015 UTC,What all feature maps look like: https://drive.google.com/open?id=0B7NQ5tsE8AHbfmd3WnlKbXdrZHNPc1dETTBwSmF3Y0FXbjNMakNHYmZlaFc3dDYxTEF4eDA
MachineLearning,3csbwi,NasenSpray,1 point,Sat Jul 11 00:02:43 2015 UTC,In the images towards the end Google's deep analysis reveals what horrors are truly hidden among the clouds and even in the faces of innocent giraffes. Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn!
MachineLearning,3cvde1,Mr_Muffinman,1 point,Sat Jul 11 02:17:58 2015 UTC,"What is the computer/algorithm supposed to ""learn""? It seems you would already know the rules for eliminating questions, in which case, you can just program that in using simple rules. If not, in the least, you would need a set of answers to the said questionnaire. The number of answered sets required to facilitate learning might be really large, depending on the number of questions you have in the set.  In case you do have a number of recorded answers, I would think a simple correlation between answers will give the filters you would need."
MachineLearning,3cvde1,crying_buddha,1 point,Sat Jul 11 04:03:48 2015 UTC,"You could use ML to learn to predict the probability of someone replying yes or no to a question given his answers to the previous questions and based on the answers of previous users.  However, you would need enough data of full questionnaires before starting to do this, because deciding not to show a question based on previous replies means that you don't know what the person would have replied. For example having 30 people replying yes-yes-no doesn't assure you that nobody is going to reply yes-yes-yes.  For example if ""yes-yes => yes"" would be chosen by 0.1% of people, you would probably not see this case before a while. And if you don't wait long enough to have enough data for this, you will introduce biais in your replies by ignoring unlikely (but possible) answers.  However, you could use ML, or actually just even statistics, to find combinations that happens all the time, and then validate manually if there is a logical reason that explains this. If so, then you can add the rule to hide the question when possible."
MachineLearning,3cvde1,ThomasRobertFr,0,Sat Jul 11 05:21:26 2015 UTC,"If you have some sort of end goal, decision trees might be helpful: https://en.wikipedia.org/wiki/Decision_tree  Otherwise, maybe you can use association rules: https://en.wikipedia.org/wiki/Association_rule_learning"
MachineLearning,3cvde1,sparsecoder,-1,Sat Jul 11 04:27:16 2015 UTC,"As others have suggested, I would also suggest using a decision tree to try to predict the next answer based on previous answers (training it with an existing set of survey answers).  One technique you could use is based on the idea that many people get bored filling out questionnaires, and will frequently stop part-way through.  So, if your decision tree implementation can not just predict what's next, but also give you an indication of certainty (a probability), then you could dynamically re-order subsequent questions starting with the question that the decision tree is least certain about.  Here is a suitable Java decision tree implementation.  Then, if the survey is aborted, you can use the decision tree to guess at the rest of the answers."
MachineLearning,3crxhs,zakou,8,Fri Jul 10 07:45:36 2015 UTC,"People do use the Hutter Prize dataset and have improved on modeling it. The problem is that much of ML is centered about stochastic modeling of data, and stochastic compression is typically not losless. However, the requirements of the hutter prize is to perfectly/losslessly compress/decompress. If the challenge would allow for an error rate (e.g. ""best compression with a maximal error rate of x""), it would be much easier to apply ML techniques. But lossless compression is probably not really feasible right now. For example, a lot of characters (e.g. chinese kanji) appear only 1 time throughout the whole data set. It is absolutely impossible for a neural net to predict such a character, when it has actually never seen it during training.  But people do use this dataset from time to time. For example Alex Graves showed that LSTM does very well on this exact dataset. In 2013 he reached a compression rate of  1.67 bits per characters, and today he is down to 1.42 b/c. However IIRC he doesn't state error rates (and he preprocessed the data to ignore out the most uncommon characters)."
MachineLearning,3crxhs,BeatLeJuce,3,Fri Jul 10 09:30:25 2015 UTC,"Yes, that's the kind of paper I was referring to. The point of actually producing a compressed file is to leave no wiggle room about what is actually modeled, e.g. leaving out the harder-to-compress characters, or not including the model itself (which could be memorizing the data).   I don't think the issue is lossy vs lossless here. Rare characters can always be encoded with an escape code, but of course the compression ratio suffers, as it should. By the way, the model is allowed to see the data (there is no training/testing split) because the model itself is part of the compressed file."
MachineLearning,3crxhs,bhmoz,3,Fri Jul 10 11:01:34 2015 UTC,"I disagree with : ""a lot of characters (e.g. chinese kanji) appear only 1 time throughout the whole data set. It is absolutely impossible for a neural net to predict such a character, when it has actually never seen it during training."" : essentially, this task is about overfitting... You want 0.0% error on your training set. You actually train over the whole data, there is no test set. So everything is seen during ""training"", there are no unseen character.  The more general problem may be that you will have a huge softmax layer as an output:   problem of precision... how many bits necessary? maybe it is related to Hinton's dark knowledge when a net does not learn the ratios of probabilities... problem of complexity (in time? in memory? i'm not familiar with this kind of nets so I don't know) : essentially the same problem that people have had when trying to build word 2 vecs (huge output space)"
MachineLearning,3crxhs,ma2rten,3,Fri Jul 10 13:41:32 2015 UTC,I disagree with your point that the problem is about machine learning being stochastic.   Lossless compression works by assigning a probability distribution over the next character. Using Huffman Codes or similar encodings it assigns the shortest representation to the most likely character. Obviously these algorithms can benefit from better next character predictions.  One of the problems is though that deep learning models tend to be big. You have to factor in the size of the model itself in the overall compression ratio.
MachineLearning,3crxhs,syllogism_,1 point,Sun Jul 12 16:21:27 2015 UTC,"This paper by Knoll and de Freitas use PAQ8 to generate text and LaTeX code that look pretty good already. One could argue about which generated output looks better, but I would rather compare true compression ratios on a large corpus."
MachineLearning,3crxhs,syllogism_,4,Fri Jul 10 11:21:24 2015 UTC,"It's too small. By about, 5 orders of magnitude.  Compressing terabytes of text losslessly is an interesting problem. There you have enough ""head room"" for a big, interesting model. In the Hutter challenge, your model can really only be 10-20mb. You can't really do anything useful with that."
MachineLearning,3crxhs,VelveteenAmbush,1 point,Sun Jul 12 14:01:11 2015 UTC,Thank you! Interesting take. Are you aware of any publications doing that? I am guessing that the common crawl (http://commoncrawl.org) or the recently published reddit comment data (https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/) would be nice playgrounds.
MachineLearning,3crxhs,VelveteenAmbush,1 point,Sun Jul 12 15:51:46 2015 UTC,"Well the adaptation from language modelling to compression is trivial — you need a model P(string), and then you have an entropy encoder. People have been reporting perplexity results for language modelling, for tasks like speech recognition, machine translation etc for the last thirty years or forty years.  For a long time these models were very large, because they'd be a linear computation from ngram counts. The recent neural network models are much denser, so we might see people thinking seriously about whether we can get practical compression systems from these language models.  I'm adding Huffman coding to my NLP library currently, for more efficient serialization. It's a good bang-for-buck compromise, even if there are better compressors."
MachineLearning,3crxhs,VelveteenAmbush,2,Sun Jul 12 16:28:16 2015 UTC,"I bet that modern techniques would blow the old techniques out of the water. Train a sequence prediction neural net. Write a decoder that, after extracting the net's best guesses about content, uses a ""cheat key"" that contains the remaining missing information -- basically a reservoir of everything that the model didn't manage to compress, the ""diffs"" between your lossy compression and a lossless compression.  Then spend a million years shaving yaks... rewrite Torch and all of the relevant libraries to be as small as possible, try a zillion different hyperparameter sets such that the size of the model plus the size of the accompanying cheat key is minimized, obsess over whether to include a zip-like compression utility to further compress the size of the rewritten Torch executable, maybe code your own compression utility that maximizes saved space at the expense of using as much of the processing power as Hutter's rules entitle you to use, etc.  And then watch your result get beaten because someone else is better at the kind of machine-level assembly coding necessary to truly minimize the size of an executable.  I think it's the endless ocean of yaks that need to be shaved, and the difficulty of shaving them well, that discourages researchers from focusing on the task. Why do all of that when your expertise is training a good model? If you've got a serious advance in model quality, just publish it in comparison to some benchmarks that don't require an endless series of chores to do well, and move on to the next thing."
MachineLearning,3crxhs,melvinzzz,1 point,Fri Jul 10 19:04:57 2015 UTC,"You describe the difficulty very eloquently. You are correct about the amount of work and the incentives.  I have trouble reconciling ""modern techniques would blow the old techniques out of the water"" with ""no gain in 6 years"". After all, if the potential gain is so large, a few sloppily-shaved yaks shouldn't be able to destroy it entirely."
MachineLearning,3crxhs,autowikibot,2,Sat Jul 11 07:11:57 2015 UTC,"After all, if the potential gain is so large, a few sloppily-shaved yaks shouldn't be able to destroy it entirely.   I expect that you're right. I bet that with a week or two of work, someone like Alex Graves (or anyone else with significant sequence prediction expertise) could set a new record on this challenge. My theory is that someone like Alex Graves has better things to do than to make his sequence predictor lossless and self-executing and small. If he did do it, and he did set the record in the Hutter data compression challenge, I bet most of his peers in the machine learning intelligentsia would shrug and say ""well, there's no exciting new theory here, but I guess he did a decent job shaving those yaks"" and that would be the extent of his acclaim. Hard to see how the benefits exceed the costs, so it never happens."
MachineLearning,3crxhs,alexmlamb,1 point,Sat Jul 11 17:22:50 2015 UTC,"I am afraid you are right about the perceived cost/benefit analysis, although I do not agree with it. Instead of showing examples of generated sequences (look, the model can close the <xml> tag correctly!) showing improved compression would be proof that the model is capturing more regularities in the data. I was hoping someone had actually tried and could report on it, but this thread answered my question anyway."
MachineLearning,3crxhs,ogrisel,1 point,Sun Jul 12 07:47:45 2015 UTC,"Well, people do report bits per character for sequence predictors' compression potential. I think that combined with the number of the parameters in their model probably gets you most of the objective benchmark value of the Hutter challenge."
MachineLearning,3crxhs,alexmlamb,2,Sun Jul 12 07:54:57 2015 UTC,"Any probabilistic model can be turned into a compressor with almost perfect efficiency (ie, 1.4 bits per char if there are 1.4 bits of entropy in the model), via https://en.wikipedia.org/wiki/Range_encoding.  Thus I suspect the main reasons for no new entries for the Hutter prize based on LSTMs or the like are largely:  1) Limited memory + cpu, with no GPU allowed 2) The 'hidden' cost of including the weights for the network, which count are part of the total compressed size (as does any code) 3) The annoying requirement of very careful management of floating point quantization for real valued data.  However it may just be that no one has bothered, as it's a lot of work, and the prize is pretty minor.  I know in my case, I actually have built a range encoding based compressor and machine learning systems, both at a commercial level for prior projects, but it's still a lot of work to really package everything up, and it may not actually work (see 1 and 2 above), so I haven't given it a go.  If I thought I could get $100k, it might be worth my time to try."
MachineLearning,3crxhs,Powlerbare,2,Sat Jul 11 06:48:59 2015 UTC,Would you really need a GPU for decompression? Good point about quantizing the weights.
MachineLearning,3crxhs,yen223,1 point,Sat Jul 11 07:14:18 2015 UTC,"Range encoding:       Range encoding is an entropy coding method defined by G. Nigel N. Martin in a 1979 paper,  which effectively rediscovered the FIFO arithmetic code first introduced by Richard Clark Pasco in 1976.  Given a stream of symbols and their probabilities, a range coder produces a space efficient stream of bits to represent these symbols and, given the stream and the probabilities, a range decoder reverses the process.  Range coding is very similar to arithmetic encoding, except that encoding is done with digits in any base, instead of with bits, and so it is faster when using larger bases (e.g. a byte) at small cost in compression efficiency.  After the expiration of the first (1978) arithmetic coding patent,  range encoding appeared to clearly be free of patent encumbrances. This particularly drove interest in the technique in the open source community. Since that time, patents on various well-known arithmetic coding techniques have also expired.    Image i     Relevant: Index of information theory articles | Lempel–Ziv–Markov chain algorithm | Multiscale Electrophysiology Format | Arabic script in Unicode   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
MachineLearning,3crxhs,Noncomment,3,Sat Jul 11 06:49:59 2015 UTC,"Unless I'm missing something, text compression doesn't seem like an important task in practice.  Text data is already pretty compact.  A simple compression strategy using template matching (replacing long repeated substrings with special symbols) seems like it would do well on a lot of text datasets and require very little computation."
MachineLearning,3crxhs,MemeBox,5,Fri Jul 10 08:57:39 2015 UTC,"The goal of the Hutter prize is not text compression in itself but to advance artificial intelligence via a large(-ish) scale text compression task as a proxy: compress a small chunk of Wikipedia.  A good machine learning model can be seen as compressing the training set to juice out as much generalizable knowledge as possible while trying to discard as much as possible ""noise"" that would lead to overfitting.  If you can extract high level concepts of redundant knowledge out of Wikipedia you might be able to use that to compress the low level descriptions of those concepts and their common variation more efficiently than traditional compression algorithm that work on limited sized blocks to extract and leverage the redundancy."
MachineLearning,3crxhs,bhmoz,2,Fri Jul 10 09:36:39 2015 UTC,"I agree that it's a challenging task.  My point is that a lot of researchers gravitate towards tasks that are industrially relevant.  A good image classifier is really useful for search, personalization, and recommendations.  Small advances in image classification are worth billions of dollars.  Some researchers intentionally go towards industrially relevant problems, others are swayed in more subtle ways (wanting their students to find jobs in industry, getting invited to industry parties, being paid to consult, etc.)  It's not obvious to me that text compression has such an impact."
MachineLearning,3cu3pp,zZJollyGreenZz,1 point,Fri Jul 10 20:02:55 2015 UTC,"I've looked into binary image/graph segmentation using spectral clustering... pub here: http://machinelearning.wustl.edu/mlpapers/paper_files/nips02-AA35.pdf  there's also markov random fields as a more sophisticated approach,  which I haven't yet explored but is on my agenda: http://www.robotics.stanford.edu/~ang/papers/cvpr05-3dsegmentation.pdf  http://www.inf.u-szeged.hu/ssip/2008/presentations2/Kato_ssip2008.pdf"
MachineLearning,3cs661,Improbus_42,2,Fri Jul 10 09:52:29 2015 UTC,"Natural image prior basically means the edges of the image should be distributed by a hyper laplacian distribution, just like how edges of images of ""natural"" objects or scenes are distributed. L2 prior (Gaussian prior) don't work well, as they don't in deconvolution, super resolution, or other types of image algorithms"
MachineLearning,3cs2j4,john_philip,1 point,Fri Jul 10 08:59:24 2015 UTC,I found an entire Master's thesis on GNGs. Looks like an interesting set of algorithms. thesis link
MachineLearning,3cszk7,tabacof,6,Fri Jul 10 15:05:09 2015 UTC,"I love Michael Nielsen's book, it's one of the best resources for understanding backpropagation and the vanishing gradient problem. You guys should check it out if you haven't already.  However, I found this chapter to be lacking. When I read about the experiment on ferrets on Hawkins' On Intelligence I was really impressed, however when I looked the details of the experiment I wasn't so impressed anymore: The vision the ferrets developed is just the capacity to see simple lights, not the vision they would normally have with regular pathways. It's a very interesting experiment for neuroscience, but for me it's far from proof of a single cortical algorithm.  Nevertheless, my main problem was that he completely disregarded animal intelligence. He admits to being human chauvinistic, which is very surprising to me as animals have shown many different kinds of advanced cognitive skills that ML and AI are nowhere near to achieve. Also, animals can be very intelligent (such as crows) using a different brain architecture (the bird homologue of the neocortex is organized in nuclei instead of layers).  What do you people think?"
MachineLearning,3cszk7,serge_cell,2,Fri Jul 10 15:19:37 2015 UTC,"I agree. The difference in architecture between chimp and human could be relatively small, but chimps are already highly intelligent, and with intensive training apes could show intelligence level of retarded human (gorilla's talking in sign language ""You leaving I sad"")"
MachineLearning,3cszk7,vm_linuz,3,Sun Jul 12 08:42:55 2015 UTC,"If there is one, it's probably simple. Found this interesting: http://www.gatsby.ucl.ac.uk/~pel/papers/ppc-06.pdf"
MachineLearning,3cszk7,zakou,1 point,Fri Jul 10 15:13:04 2015 UTC,Interesting! Has there been any ML based on it? Too lazy to check right now...
MachineLearning,3ct887,claesenm,1 point,Fri Jul 10 16:11:42 2015 UTC,Is there more info on what hyperparameter optimization methods this lib uses?
MachineLearning,3ct887,schmook,1 point,Sat Jul 11 02:03:39 2015 UTC,http://optunity.readthedocs.org/en/latest/user/solvers.html
MachineLearning,3co02o,fhuszar,5,Thu Jul 9 11:44:47 2015 UTC,"So if we are modeling 100x100 images, then all layers in the resulting network will have 100k nodes.   Only if the images are decachromatic."
MachineLearning,3co02o,5at,14,Thu Jul 9 16:48:48 2015 UTC,Not every day you get to whip out decachromatic.
MachineLearning,3co02o,mathsive,7,Thu Jul 9 18:17:30 2015 UTC,Good things happen to those who wait.
MachineLearning,3co02o,5at,1 point,Thu Jul 9 18:53:05 2015 UTC,touché.
MachineLearning,3co02o,VelveteenAmbush,3,Thu Jul 9 22:57:43 2015 UTC,Arxiv link
MachineLearning,3co02o,maxxxpowerful,1 point,Thu Jul 9 22:01:51 2015 UTC,"Is this some sort of a bad cut-n-paste job?  FTA:   Amazingly, this works, and the traninig objective becomes very similar to variational autoencoders. Here is the example reconstruction after the inverse dynamical system is learnt. Here is an explanation. The top images from right to left: we start with a bunch of points drawn from random noise...   What example? What explanation??"
MachineLearning,3co02o,flangeball,2,Thu Jul 9 19:31:18 2015 UTC,"Thanks, I rewrote this. It wasn't bad copy paste, rather sloppy drafting and weak proofreading."
MachineLearning,3co02o,True-Creek,2,Thu Jul 9 23:02:33 2015 UTC,"Scanning through the text, I found the second image rather confusing, with time unexpectedly running from right to left.  Also, the first sentence  ""I spend the week at ICML""  should be one of (depending on your intent):  ""I'm spending the week at ICML"" (present tense)  ""I spent the week at ICML"" (past tense)  ""I'm going to spend the week at ICML"" (future tense)"
MachineLearning,3co02o,iidealized,1 point,Fri Jul 10 11:20:57 2015 UTC,I don't quite get how this works. It turns data points that are arranged in a complex manifold into even an more complex configuration using a Brownian diffusion process? Why?
MachineLearning,3co02o,yaolubrain,1 point,Fri Jul 10 11:19:11 2015 UTC,"The goal is to sample new images from the space of images (aka new datapoints from the manifold), so you need to inject some form of noise into the existing samples.  This is achieved via the diffusion process in this paper.  It's similar to denoising autoencoders, except now the noise is continuously introduced over time rather than fixed amounts of noise being injected in discrete layers."
MachineLearning,3co02o,JustFinishedBSG,1 point,Fri Jul 10 15:03:25 2015 UTC,I wonder if this paper is just another interpretation of http://arxiv.org/abs/1401.4082
MachineLearning,3cs0qa,datatadadata,5,Fri Jul 10 08:31:55 2015 UTC,"at the data science-y events I've been to, 90%+ are using apple products.  you should become very comfortable working through SSH and using remote desktops or AWS instances."
MachineLearning,3cs0qa,j1395010,6,Fri Jul 10 12:25:39 2015 UTC,A Mac book.....
MachineLearning,3cs0qa,simonhughes22,1 point,Fri Jul 10 15:58:09 2015 UTC,"Works great for any data scientific task that would be assigned in an ML class at a university. (excluding classes about mining massive datasets...)   If you come across a task that requires more, i'd recommend getting an account with Domino Data Lab, Yhat or Terminal."
MachineLearning,3cs0qa,jayhack,5,Sun Jul 12 17:10:41 2015 UTC,"I'd actually recommend building a nice desktop, you have much more  control over the elements. If you need mobility, get a cheap laptop and set up VNC/Remote Desktop back to your home computer."
MachineLearning,3cs0qa,siblbombs,3,Fri Jul 10 13:25:58 2015 UTC,"I can't find a decent, recent blog post or article about what's a good laptop for datascience.   Thats because using a laptop to do the needed number crunching is a terrible idea! You get much better hardware (and price!) with standalone PCs. You will run into heat problems if you keep your CPU and GPU 100% busy for a prolonged time on your laptop - and noise levels will generally be quite high. I use my laptop basically as a thin client and run my experiments on decent dedicated hardware in my basement. Also, I can keep my experiments running and still take my laptop with me."
MachineLearning,3cs0qa,quirm,2,Fri Jul 10 15:32:30 2015 UTC,"I have benchmarked and compared 40+ (business line) laptops for a company, which specializes in selling high quality devices to Swiss students and faculty (see http://www.projektneptun.ch/en/products/archive/spring-2015/). I think something like a Lenovo ThinkPad T440p or T450 with a dedicated GPU (Nvidia) is great trade off in reliability, convenience and cost.  Pros:   reliability: 3 year warranty, 5 years availability of all spare parts maintenance: HD, RAM, battery, keyboard replaceable extensibility: M.2 port for additional SSD, ultrabay for additional HD instead of DVD-Drive, docking station, different sizes of batteries T440p with quad-core cpu available very good keyboard many I/O ports good Linux compatibility   Cons:   relatively high cost, if no discount T440p has no real buttons when using the track point (but get used to it) T450 has no docking port if bought with dedicated GPU   I have a T440p, dual booting Windows and Ubuntu. Ubuntu is relatively easy to begin with, has good support for Lenovos and has proprietary graphics drivers from Nvidia.  (EDIT: added pros and cons, cross posting on quora: http://www.quora.com/What-is-the-best-laptop-for-a-data-scientist)"
MachineLearning,3cs0qa,marcuniq,2,Fri Jul 10 09:59:47 2015 UTC,"I've had a thinkpad for my first machine learning job.    Always gets the job done, and is quite sturdy. I've carryied it around all the time for 2 years and still seems new."
MachineLearning,3cs0qa,GibbsSamplePlatter,1 point,Fri Jul 10 12:28:31 2015 UTC,"Any laptop that can run games from 2013 is good enough for you to start with. Also, use Linux."
MachineLearning,3cs0qa,ginger_beer_m,1 point,Fri Jul 10 08:42:02 2015 UTC,"Hi, thanks for your answer. Now let's say I have a 1.5k budget, which is far more that what it takes to buy such a computer. Where should I focus on ? RAM ? CPU ? GPU ? Buying tons of donus because it won't make any difference ?    Cheers !"
MachineLearning,3cs0qa,prassi89,1 point,Fri Jul 10 08:43:47 2015 UTC,"If you're looking at data science, I think more RAM and more cores is what you need to look for. If you want something that works well with Neural Networks, it's better to invest in a nice cuda capable GPU too."
MachineLearning,3cs0qa,simonhughes22,1 point,Fri Jul 10 13:06:20 2015 UTC,"I'd agree with that. Also try and get an SSD, makes disk access much faster. My Mac Book pro has an Nvida GPU (so supports CUDA for DL), 8Gig of RAM and an SSD, and being a Mac is compatible with most unix\linux hardware. When I went to Strata, most people has a mac of some kind I think. It may be more than the 1.5k budget though.  A custom built Linux desktop will be a lot cheaper. Or get a good laptop and use AWS for the machine learning work. If you use spot instances, you can run quite high end machines for next to nothing."
MachineLearning,3cs0qa,michaelmalak,1 point,Sun Jul 12 21:19:06 2015 UTC,"RAM would be nice, but currently there is a 16 GB barrier that won't be broken until September at the earliest. http://www.datascienceassn.org/content/16gb-laptop-barrier-be-broken-september"
MachineLearning,3cs0qa,zayats,1 point,Fri Jul 10 09:03:32 2015 UTC,"I must be misunderstanding something? My laptop has 32Gb RAM, and I have seen it shoot up to 50-100%."
MachineLearning,3cs0qa,michaelmalak,1 point,Fri Jul 10 10:52:31 2015 UTC,Such laptops are not mainstream. What is the make and model? And do you know the CPU model?
MachineLearning,3cs0qa,zayats,1 point,Fri Jul 10 12:50:52 2015 UTC,https://www.reddit.com/r/MachineLearning/comments/3cs0qa/whats_a_good_laptop_for_datascience/csygqqy
MachineLearning,3cs0qa,michaelmalak,1 point,Fri Jul 10 18:53:54 2015 UTC,"Well, in my blog post I did say ""except for exotic gamer laptops"" and I see that the manufacturer, Clevo, specializes in gaming laptops and the primary distributor is even called http://www.xoticpc.com :-)  Edit: It is interesting, though, that the CPU is still considered a mobile CPU, yet the Intel specs say it can handle up to 32GB. So it appears that if one can find an i7 laptop, there is a chance it can go up to 32 GB."
MachineLearning,3cs0qa,zayats,1 point,Fri Jul 10 19:52:17 2015 UTC,"I can see intel lists some cheaper i5's (~225$) that can support 32gigs as well. link.  Looking at some laptops with these lower end chips, they are in the 500-1k price range, however all that I've found list 16gigs as max. I suppose this is due to limitation of the motherboard, too few slots/can't handle high capacity per slot?"
MachineLearning,3cs0qa,michaelmalak,1 point,Sat Jul 11 00:13:16 2015 UTC,Looks like it might be a BIOS limitation. https://www.linkedin.com/grp/post/3078126-274143486
MachineLearning,3cs0qa,walrusesarecool,1 point,Sat Jul 11 00:48:18 2015 UTC,"I have a schenker machine- its a beast. Not very portable, though. So it depends. Do you want a laptop to carry everywhere or a laptop which is normally in one place but is easier to move than a desktop machine.  http://mysn.co.uk/shop/xmg-u705-gaminglaptop.html"
MachineLearning,3cs0qa,zayats,1 point,Fri Jul 10 10:21:43 2015 UTC,"I bought a Clevo P150SM-A for around $1.5k.   i7 4810MQ  32GB low latency RAM  GTX 860M (for CUDA, but never use it, waste of ~300$)  256GB SSD  120GB mSATA SSD (data drive, I plan to insert another to RAID)  I dual boot windows 8.1 with Debian Jessie running MATE. The system runs like lightning, I have to try, hard, to max my CPU/RAM. But, I  also don't work with what people here would consider 'big data'. Got it barebone from rjtech.com.  I bought it because of it's purported reliability, upgradability, and because it was the meatiest laptop I could find that didn't look like a pissed off fighter jet. No logos or anything. There are ""workstation"" laptops with similar specs that look presentable, but they cost nearly twice as much.   RAM usage depends pretty heavily on what language you will be coding in. Python, Perl, and especially R will use a lot more memory than C/C++ and Java. I use R a lot so I upped my RAM.  ** save yourself from the pain, don't even bother putting in a graphics card, unless you explicitly need CUDA. In which case you can SLI dual cards in this laptop if you want.   *** if linux don't go with stock Realtek wifi, it's a driver nightmare."
MachineLearning,3cs0qa,aayyyy,1 point,Fri Jul 10 11:27:24 2015 UTC,"Are you limited to getting a laptop?  If not then I would suggest a cheap used laptop and throw linux on it, and then building a decent desktop. The total cost will be equal to a powerful laptop that still won't be as good as an equivalently priced desktop."
MachineLearning,3cs0qa,serge_cell,1 point,Fri Jul 10 13:21:29 2015 UTC,"Assuming that you do a lot of data processing  -Do I need a lot of RAM? Why ? Is 64GB a lot ?  Yes. You will likely work with memory-mapped databases and  the more memory the better. 32GB is a must, 64GB is better  -CPU   mostly irrelevant. Faster hard-drive and RAM is more important.  -Do I need a GPU ?  Yes. It's your workhorse. Most of modern non-trivial machine learning, statistics etc done by GPU  It good bet to use NVIDIA with latest CUDA compute capability support.  Most of existing libraries use CUDA  If you intend to write form the scratch yourself, or less dependant on 3d party libs Radeon & OpenCL is viable alternative   Is there any reason to use an UNIX OS if I'm used to coding on Windows?   Yes. Most of existing ML, staistics etc libs are poorly supported on Windows, or not at all.  Almost best possible: 2.6k : MSI GT72 Dominator Pro 32GB RAM with gpu NV 980M 8GB  but beware possible ubuntu problems  I'm using ASUS g751, 16GB RAM, gpu - NV 980M 4GB, but I have access to server with Tesla in the office. No considerable problems with ubuntu(some small glitches)"
MachineLearning,3cs0qa,VelveteenAmbush,1 point,Fri Jul 10 18:03:26 2015 UTC,Any laptop that is comfortable to use while you are SSHing into your desktop :)
MachineLearning,3cs0qa,PM_ME_YOUR_PRIORS,1 point,Sat Jul 11 03:37:24 2015 UTC,Literally anything that can SSH into an EC2 or VPS instance.
MachineLearning,3cnhj7,john_philip,3,Thu Jul 9 07:26:08 2015 UTC,"looking at pretty pictures and saying ""ahhh..."""
MachineLearning,3cnhj7,FuckFrankie,3,Thu Jul 9 11:31:01 2015 UTC,This is great work!
MachineLearning,3cnhj7,O_I_O,1 point,Thu Jul 9 14:12:01 2015 UTC,"Cool, thanks!"
MachineLearning,3cnvpd,clbam8,4,Thu Jul 9 10:49:26 2015 UTC,"The ""parameters to points"" idea is a rule of thumb, not set in stone, and for good reason. You really need to know a ton about your data to trust these regularization schemes - arguably more information than is ""in"" the data. Why is a horizontal line through a point better than any other line? No reason at all! But hey look I can invert the matrix. And how the hell are you going to validate this model that's half based on priors pulled out of one's ass? You barely have enough data to fit!  The attitude that this dispels some myth is sort of silly to me, and while these techniques are useful I think they're more the last stab at a dataset, not assumed go-to techniques."
MachineLearning,3cnvpd,TTPrograms,1 point,Fri Jul 10 04:02:42 2015 UTC,"Exactly. This is essentially taking an underdetermined model, and adding more data to it until it becomes determined. I'd therefore say it reinforces the point that you need enough data points to fit your model, just some of those data points aren't of the form (x,y); in this case the extra data looks like this: ""the slope and intercept should be small"".  The article was a very enjoyable and thought-provoking read, however."
MachineLearning,3cnvpd,carthurs,5,Sun Jul 12 14:59:48 2015 UTC,This is very dangerous stuff to tell general researchers in a world where p<.05 means thirty percent of biomedical studies aren't repeatable.
MachineLearning,3cnvpd,ReedMWilliams,3,Fri Jul 10 02:21:00 2015 UTC,Good explanation of basics.
MachineLearning,3cnvpd,nuhuskerjegdetmand,2,Thu Jul 9 13:01:34 2015 UTC,Small stylistic suggestion: italicize less. Other than that it's a fairly good intro to the subject.
MachineLearning,3co8wq,gxy5562,1 point,Thu Jul 9 13:17:01 2015 UTC,Is there code for this?
MachineLearning,3co8wq,philly7891,1 point,Fri Jul 10 02:55:07 2015 UTC,No. It's from Baidu the chinese search giant so I assume they are keeping it inhouse much as Facebook and google do with their current face recognition technology. I assume it can't be long until some grad student makes an open source version of these new techniques.
MachineLearning,3cogo8,Aerospacio,5,Thu Jul 9 14:23:30 2015 UTC,catastrophic forgetting (sometimes called catastrophic inference) is a common thing in neural networks :( my favorite study of it is An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks
MachineLearning,3cogo8,mat_kelcey,1 point,Thu Jul 9 14:34:40 2015 UTC,"oh actually, this might be subtly different. what is the difference between what you call your ""first"" set of samples and the ""new"" samples?"
MachineLearning,3cogo8,mat_kelcey,1 point,Thu Jul 9 14:36:40 2015 UTC,"Maybe make the network deeper and wider, and slow the learning rate?"
MachineLearning,3cogo8,simonhughes22,1 point,Thu Jul 9 15:25:03 2015 UTC,Thank you so much!
MachineLearning,3cogo8,mikef22,1 point,Sat Jul 11 01:50:10 2015 UTC,Rprop with RL sounds dodgy. E.g. rprop is not valid for online learning.  Can you give more detail of how you are balancing batch learning vs online learning
MachineLearning,3cogo8,Noncomment,1 point,Thu Jul 9 21:52:30 2015 UTC,"Hello, i was not using online learning, i was trying to implement part of a experience with a deep auto encoder to reduce dimensionality and using a neural network to do fitted Q iteration, in a riedmiller paper they suggested to re-train the whole Q function from square one for each batch, pretty sad, still working on a solution"
MachineLearning,3cogo8,Bhavishya1,1 point,Sat Jul 11 01:57:39 2015 UTC,"Rprop is not an online learning algorithm. It's a batch learning algorithm. That is, you can't update the weights after looking at only one data point, i.e. one at a time or ""online"". You need to iterate through the entire dataset, or ""batch"". Because Rprop requires an accurate sign of the gradient, and if you only look at part of the data, then the gradients for that part won't be true of all the data.  Try using some variation of stochastic gradient descent."
MachineLearning,3cogo8,MusicIsLife1995,1 point,Fri Jul 10 03:36:58 2015 UTC,"Thank you, when i said ""samples"" i meant set of samples aka batch^"
MachineLearning,3coevb,walrusesarecool,1 point,Thu Jul 9 14:09:21 2015 UTC,Sorry I tried to post this yesterday but the autobot would not let me edit the link when I shared it with out viewing permissions!   Info about ILP 2015 here: http://www.ilp2015.jp/
MachineLearning,3coevb,yetipirate,1 point,Thu Jul 9 14:10:54 2015 UTC,How different is this from constraint based methods? I understand that representing the problem in FOL could be helpful but honestly they sound like similar approaches. Also you probably should try including cell-signalling pathways in your network models as well.
MachineLearning,3coevb,yetipirate,1 point,Thu Jul 9 20:54:41 2015 UTC,"constraint based methods   Constraint methods have similarities but as far as I know using something like constraint handling rules limits you to abductive learning, i.e. learning with out free variables. This means that you could not learn a rule such as on(a),on(X),on(b),link(a,X),link(X,b).   At the moment I am limiting the approach to searching through Reactome Pathways, but I want to expand this in the future to have a broader knowledge base by including further ontologies and rdf graphs. Should be a fairly easy expansion due to the nature of rdf."
MachineLearning,3coevb,yetipirate,1 point,Fri Jul 10 06:50:48 2015 UTC,Looks good man. My only other thought is about handling time course data. Some pathways are inherently slower than others and I haven't seen any approach take this data into account yet.
MachineLearning,3coevb,marijnfs,1 point,Fri Jul 10 16:04:54 2015 UTC,"Yeah- time is something that needs to be handled. The whole thing of looking at chains of reactions etc, should really take into account time. Need to find a nice data set and have a think."
MachineLearning,3ckbfk,vkhuc,17,Wed Jul 8 16:05:51 2015 UTC,"Very cool!  The fact that fundamental tools like this are just now being released shows how young this field really is.  Considering what deep learning is already capable of and how much more attention it's getting, imagine where we'll be in just a few years!"
MachineLearning,3ckbfk,bushrod,8,Wed Jul 8 17:38:56 2015 UTC,"Yeah, still very young field, lots of room for innovation and growth. I think it's neat that we've always understand theoretically this is what the networks were doing, and we've seen snapshots of these internal layers, but this is an awesome visualization and interface for exploring the internals.  I'm certain this tool will lead to a spark of insight for some researcher."
MachineLearning,3ckbfk,omniron,2,Wed Jul 8 18:36:08 2015 UTC,"Yeah, imagine Kerbal Space Program, but for deep learning. :)"
MachineLearning,3ckbfk,AsIAm,1 point,Thu Jul 9 08:50:27 2015 UTC,its very useful indeed. i used it in this blog post for visualising candid content: https://medium.com/@samim/sensual-machines-82858b32a4e5
MachineLearning,3ckbfk,samim23,4,Thu Jul 9 09:10:17 2015 UTC,"Even though I have some understanding of how this works, Deep ConvNets are basically magic to me."
MachineLearning,3ckbfk,TanktopModul,8,Wed Jul 8 21:55:59 2015 UTC,this is dope
MachineLearning,3ckbfk,in_the_fresh,3,Wed Jul 8 16:45:37 2015 UTC,This is awesome.
MachineLearning,3ckbfk,Virakis,3,Wed Jul 8 22:34:54 2015 UTC,the way this communicates insight into the process is amazingly accessible. loved the video!
MachineLearning,3ckbfk,covanga,6,Thu Jul 9 03:28:49 2015 UTC,super cool
MachineLearning,3ckbfk,j1395010,6,Wed Jul 8 16:26:41 2015 UTC,"Wow,thanks I was wondering if you can see how they really worked...! This is a great start...."
MachineLearning,3ckbfk,flexiverse,2,Wed Jul 8 17:08:18 2015 UTC,Suddenly I can understand the 4D tensors they use in these convnets
MachineLearning,3ckbfk,gct,2,Thu Jul 9 04:15:34 2015 UTC,"I still don't understand how you can visualise convolution kernels in a layer that feeds from more than one kernel map, and has several kernel maps itself. How do you collapse a 4-tensor into a single 2d visualisation?"
MachineLearning,3ckbfk,BadGoyWithAGun,2,Thu Jul 9 08:26:31 2015 UTC,Here's a longer talk about the toolbox by Jason: https://archive.org/details/Redwood_Center_2015_05_29_Jason_Yosinski
MachineLearning,3ckbfk,glassackwards,3,Fri Jul 10 15:51:05 2015 UTC,Magic. Got it.
MachineLearning,3ckbfk,bitcloud,1 point,Thu Jul 9 03:12:51 2015 UTC,Awesome. The man in the video looks like it never sleep for days trying to get that Deep Learning toolbox.   I love this topic.
MachineLearning,3ckbfk,unchandosoahi,1 point,Thu Jul 9 03:44:32 2015 UTC,"++ super cool, commenting for later view."
MachineLearning,3cn4wg,MusicIsLife1995,6,Thu Jul 9 04:58:57 2015 UTC,It's a good idea but we tried it once and ran out of steam https://www.reddit.com/r/mlstudy/  1 ch every two weeks is a good rate i think
MachineLearning,3cn4wg,hyphypants,2,Thu Jul 9 05:24:12 2015 UTC,I like the idea of having an own subreddit. Of course one could announce it here.  But perhaps having a subreddit for the book in general might be a good idea. So people can ask questions and discuss without being bound to a specific time frame.  OPs study group could work through it together and initially fill it with their progress and discussions. In this case I would be also very interested in this endeavor.  Edit: A word.
MachineLearning,3cn4wg,ukkoylijumala,1 point,Thu Jul 9 23:29:34 2015 UTC,"maybe a subreddit with a thread for each chapter, and people could post their questions and see previous discussions."
MachineLearning,3cn4wg,hyphypants,4,Fri Jul 10 22:54:21 2015 UTC,"I was thinking about something like this, but for Murphy's ML:PP. I find Bishop's text very messy and hard to read."
MachineLearning,3cn4wg,elanmart,4,Thu Jul 9 11:20:49 2015 UTC,10 pages a day? That's quite a pace! :)  (This is not sarcasm.)
MachineLearning,3cn4wg,sieisteinmodel,2,Thu Jul 9 18:04:33 2015 UTC,"I think it's doable for the first 5 chapters, after that it's going to be pretty hard, yeah"
MachineLearning,3cn4wg,vikkamath,3,Sat Jul 11 12:09:29 2015 UTC,"Good idea. Actually, I also want to read through PRML book."
MachineLearning,3cn4wg,minhpham,3,Thu Jul 9 06:17:07 2015 UTC,"I have read PRML once before, but won't mind re-reading it. It always is nice to brush up some of the math that I take for granted these days."
MachineLearning,3cn4wg,Pillowrath,2,Thu Jul 9 07:49:11 2015 UTC,"Interested! I've been stuck in chapter 2 for some time, really want to continue."
MachineLearning,3cn4wg,kullback-leibler,1 point,Thu Jul 9 09:30:21 2015 UTC,WE GOT YOU!
MachineLearning,3cn4wg,vikkamath,2,Fri Jul 10 00:30:33 2015 UTC,Count me in
MachineLearning,3cn4wg,mathnkann,2,Thu Jul 9 14:56:09 2015 UTC,Yes please ...am interested too
MachineLearning,3cn4wg,srinathsmn,2,Thu Jul 9 16:59:45 2015 UTC,Pls count me in :)
MachineLearning,3cn4wg,Silva9a,2,Fri Jul 10 02:03:27 2015 UTC,I'm interested too .
MachineLearning,3co7o8,mbriner,1 point,Thu Jul 9 13:04:37 2015 UTC,Sounds like you need more exploration. What happens if you do epsilon greedy exploration and very slowly decrease epsilon?
MachineLearning,3co7o8,marijnfs,0,Sun Jul 12 19:34:31 2015 UTC,"this is a good question and a thing I'm facing as well.    side note, that notation is horrible.  do you have sutton and barto's book?   Try just plain Q-learning: at state s, choose a according to Q and your exploration strategy (e-greedy or boltzmann), and then observe state s' and reward R:  Q(s,a) = (1-\alpha) * Q(s,a) + \alpha * (R + \gamma * (max_a'(Q(s',a'))))  and that's it.    I usually do epochs for the exploration strategy.  i.e. 0.9 exploration at first, then 0.7, then .... , down to 0.1 at the last epoch.   so, i'll do 5 epochs, with 10000 episodes per each.  then 50 interaction per episode.    e-greedy selects at random part of the time, argmax other part.  boltzmann does the exp(a_i)/sum_j exp(a_j) and samples using inverse transform sampling  alpha is learning rate, gamma is called discount factor.    learning rate (alpha) is usually 1 / ( offsetconst + episode_num / timescaleconst) where the consts can be set as free parameters.  Depending on the size of your epochs and everything. I do offset = 1 and tiemscale = 1000 or 100 or whatever.   to be honest, I'm still trying to learn the intuition behind setting up parameters.  I'm using the Win or Learn Fast algorithm right now (multi-agent learning, so it actually samples actions off policy and updates according to a delta parameter).   But it's not working now.. I just implemented a plain Q-learner w/ a simulated environment to see if the problem was even learnable.    i'm trying to interpret your algorithm again.   t is the interaction index.  k is episode index.   x is state and a is action.   in next state (why?), you choose price on-policy.  you find the argmax price.  td error is just the reward + discounted argmax nextQ minus the current Q.   in step 4.2, they  never say what alpha(x,a) is.. is that a function?  what's the trace for?  is just a boolean value?  0 if present, 1 if not?  no, because they set it to lambda * trace.. wat. or does <- mean +=?  or is it an =?  oooh. it's the discount factor.  if you don't observe it next time, it gives it the lambda discount.. but i don't see how that compounds because if you observe it again, it goers back to one..   This algorithm is confusing.  I just googled and found a blog post that compares SARSA and Q-learning.  that might be a good place to start for an easier experience.  here's the link"
MachineLearning,3cm4o0,True-Creek,3,Wed Jul 8 23:52:30 2015 UTC,This would be a great place to field some of the newbier questions people like me commonly ask.
MachineLearning,3cp5bm,bourbondog,1 point,Thu Jul 9 17:25:28 2015 UTC,"That depends, deep networks that run on the GPU are more often than not constrained by the RAM available on the graphics card - in which case, your only option is to get another GPU."
MachineLearning,3cp5bm,BadGoyWithAGun,1 point,Thu Jul 9 17:35:58 2015 UTC,"Of course it's possible. On GPUs, you're mainly limited by bandwith and RAM, but running multiple processes there is definitely possible (there will be performance degradation, but as long as everything fits into the RAM you're good to go). The performance degradation depends on how intensive your usage is. I've seen almost no degradation for small/middle sized nets, but I could imagine that it's worse if one process alone is already fully using the GPU (which, mind you, is very seldom the case!)   It's the same on the CPU side: if you have a program that can make 100% perfect use of all cores (and RAM), then running 2 programs at the same time will slow things down considerably. But again, this is very seldom the case. Usually you save time by running multiple things simultaneously."
MachineLearning,3cp5bm,BeatLeJuce,1 point,Thu Jul 9 21:14:38 2015 UTC,"I've seen almost no degradation for small/middle sized nets, but I could imagine that it's worse if one process alone is already fully using the GPU (which, mind you, is very seldom the case!)   Can I ask why this is true? Is it because experienced machine learning researchers/practitioners rarely design a net to be as big as possible within their GPU memory?"
MachineLearning,3cji55,yaolubrain,1 point,Wed Jul 8 11:55:52 2015 UTC,This is pretty cool! Did you also attempt any tensor decompositions or things like LDA?
MachineLearning,3cji55,kkastner,1 point,Thu Jul 9 04:05:17 2015 UTC,Not yet. But I believe improvement and more interesting results can be obtained with more advanced algorithms such as tensor decompositions.
MachineLearning,3cjloi,seabass,7,Wed Jul 8 12:33:32 2015 UTC,How active is research into deep reinforcement learning?  is it a field that has potential?  I've read the Atari paper and it seems interesting but the details were not there.  I feel like if I wanted to attempt to replicate it I wouldn't even know where to start.  Is there a good library out there for reinforcement learning?  One that is actively developed/up with the latest advancements?
MachineLearning,3cjloi,ai_noob,2,Wed Jul 8 14:51:18 2015 UTC,I think there are a lot of references including an online lecture by one of the DeepMind guys (I forget the name) and lots of notes on reinforcement learning.  There are also several libraries that implement deep q-learning...the only name that comes to mind is reinforce.js but there are others.  Also I haven't tried but I remember from reading the paper that there was pretty much enough detail to implement what they did.  The main novelty was coming up with an objective function (and correct definitions of the inputs) and all that is provided in the paper.  Of course they assume you already know how a convnet and a standard feed-forward net run so if you don't that might be a good place to start.
MachineLearning,3cjloi,spurious_recollectio,1 point,Wed Jul 8 15:20:38 2015 UTC,Thanks for the info.  I must be a poor paper reader.
MachineLearning,3cjloi,ai_noob,2,Wed Jul 8 16:52:21 2015 UTC,Deep Reinforcement for Robotics shows huge potential.    Deepmind Atari replication is only a few clicks away...  Soumith's CVPR 2015 workshop has an Amazon EC2 machine image linked (use a GPU instance)  It has torch + itorch + Atari + notebooks (the AMI ID is: ami-b36981d8) all ready to use  and a itorch notebook that trains a deep-Q agent on the atari game pong (with notes for how other game roms are easily added)    Here is the official source code from the Nature Paper - it is well worth reading both papers    Volodymyr Mnih NIPS 2014 talk on the Atari Paper    David Silver NIPS 2014 talk    David Silver's Reinforcement Learning Course videos are very good (based on Sutton & Barto's RL textbook)    This guy has replicated some of the original atari paper results.
MachineLearning,3cjloi,ford_beeblebrox,1 point,Wed Jul 8 18:51:51 2015 UTC,They've published a source code demo.
MachineLearning,3cjloi,dwf,3,Wed Jul 8 15:18:59 2015 UTC,"How much math should I know? I know in the FAQ it states:     ""having at least an undergrad level of Statistics, Linear Algebra and Optimization won't hurt"" (I am currently going in to my third year of undergrad)  I have taken 2 calc courses, which I did meh in. Calc isn't really my thing. Discreet math was an A for me, and it was my favorite math course by far. I also took an Automata class where the proofs were very formal, and I struggled a bit in there. There have been other math courses, but whatever, not that important. I have not taken linear algebra in a course but have done some learning on it, not much though. I have been looking in to neural networks and other topics but sometimes stumble into really mathematically heavy sections of books that I struggle to comprehend. Maybe I should just spend more time thinking on them? If anyone has some books to reccomend or jumping off points from this information, that would be cool. Also algorithms/data structures/ stuff like that, I have taken, so no sweat there..  Pretty much, I want to be able to work my way through alot of the recommended books on here but am not sure if I am prepared mathematically yet."
MachineLearning,3cjloi,Triumphxd,13,Wed Jul 8 14:06:29 2015 UTC,"This is just my opinion but I think linear algebra is the most important math subject for ML.  Fundamentals in probability and statistics are important, but understanding a lot of the math involved usually comes down to how well you understand linear algebra.    In ML we deal with data, and data is almost always delivered in the form of a matrix (whether it's two dimensions or higher).  Linear algebra provides a language for expressing transformations on that data in a succinct way, so understanding the math behind a lot of ML algorithms requires knowledge of that language.  I can't point you to a specific book that I recommend, but I would advise you to focus on that."
MachineLearning,3cjloi,kevjohnson,5,Wed Jul 8 14:54:49 2015 UTC,"understanding a lot of the math involved usually comes down to how well you understand linear algebra   While I agree that linear algebra is an important first step in understanding the mathematics (i.e. the ""how""), it can't explain the ""why"" of the math. I think that for true understanding, statistics and convex optimization are more important in the long run. Most papers in NIPS etc. won't have any linear algebra in them."
MachineLearning,3cjloi,bdol,1 point,Wed Jul 8 16:51:45 2015 UTC,Very well put!
MachineLearning,3cjloi,valexiev,1 point,Wed Jul 8 16:55:35 2015 UTC,"That's what I figured, so I started looking through some overviews and specific texts. I'll continue on. Thanks!"
MachineLearning,3cjloi,Triumphxd,3,Wed Jul 8 15:12:49 2015 UTC,eli25 deep generative models
MachineLearning,3cjloi,evc123,3,Wed Jul 8 19:32:09 2015 UTC,"Suppose I build a model of some kind on a certain training sample, with some percentage of the data used as a holdout. After I am done fitting my model, I check it against the holdout data, and it performs terribly.  What exactly am I supposed to do? It seems wrong to try different things until my performance on the holdout data is ""good enough"" in some way, because it will be difficult to tell whether I am manually overfitting to the holdout sample by adjusting my algorithm."
MachineLearning,3cjloi,Wolog,5,Wed Jul 8 21:49:48 2015 UTC,"What exactly am I supposed to do? It seems wrong to try different things until my performance on the holdout data is ""good enough"" in some way, because it will be difficult to tell whether I am manually overfitting to the holdout sample by adjusting my algorithm.   This is why I'm sick of MNIST papers :)   The truth is you have to be careful and try and judge for yourself. Do cross validation on the training data first and only select a subset for using on the hold-out. But no matter what you do, there are overfitting risks."
MachineLearning,3cjloi,EdwardRaff,1 point,Wed Jul 8 23:16:04 2015 UTC,Are you employing any kind of regularization or overfitting penalty when training? This can help prevent overfitting.
MachineLearning,3cjloi,blowjobtransistor,3,Thu Jul 9 00:53:34 2015 UTC,"Does distribution of training examples over all possible classes have an effect on the accuracy of neural networks? For example, if I'm training a neural net to do binary classification and I have 1 million positive training examples and 1 million negative training examples would the resulting network have better, worse, the same, or an undetermined difference in performance from the same network being trained with 2 million positive training examples and 1 million negative training examples?  Edit: By performance I solely mean accuracy."
MachineLearning,3cjloi,cephalopod1,2,Thu Jul 9 15:24:41 2015 UTC,Are Gaussian Processes useful in practice as compared to other algorithms? Is there anything they are good at that say a neural network couldn't accomplish?
MachineLearning,3cjloi,antiquechrono,2,Thu Jul 9 07:16:00 2015 UTC,"Yes, GPs can handle small-data very well and they also give uncertainty estimates, two things a regular neural network could not do.  For example, in the field of hyper-parameter optimization, where getting new samples is very expensive (in terms of time), GPs are widely used (see Spearmint project).  Vanilla GPs don't scale well (the covariance matrix grows with the square of the number of samples), but there are modifications to handle that.   Also, you can use different covariance matrices for different problems (spatio-temporal models, time-series, etc), so GPs can handle problem specific information well unlike most black-box machine learning algorithms."
MachineLearning,3cjloi,tabacof,1 point,Thu Jul 9 16:04:33 2015 UTC,"I'm not sure if the following is a simple question, but I have the feeling I'm missing something obvious. My experience with ML and applied statistics is limited.  I want to discover the optimal mix in terms of profit given an unknown demand for a large range of products, say a thousand. I have access to a small display on which only ten products fit.   How to proceed?  My best guess is picking ten products at random and measure profit for each product after a period of sales. Then I model the data with polynomial regression and predict the profitability of all untested products; some secondary characteristics are known to distinguish similar products. Then I sort the list and pick the products with the highest estimated profitability and test those. Repeat.   Am I on the right track?"
MachineLearning,3cjloi,luisterluister,1 point,Wed Jul 8 18:54:43 2015 UTC,"There are a number of ways you could approach the task. You could also pull from the multi-armed bandit stuff.   your proposed approach dosn't sound unreasonable, you could also incorporate active-learning into it.   Ultimately, I'm doubtful of the premise that certain products have a specific ""profitability"". Sales are going to depend on a lot of factors (nature of the products, seasonality, foot-traffic ,geographic region, weather at the time, appearance of inferior/superior goods in the same store, relative price differences, the good's status as an inferior/superior good and the current economy, etc) and I'm guessing this would over-simplify the problem."
MachineLearning,3cjloi,EdwardRaff,1 point,Wed Jul 8 23:13:42 2015 UTC,"I think I can capture many of these other factors too. Thanks for the suggesting of active-learning, I'll look into it!"
MachineLearning,3cjloi,luisterluister,1 point,Thu Jul 9 13:48:48 2015 UTC,I think a Generative Learning type of algorithm would work best here. You're going to try to predict the type of features (an N dimensional array of which products to pick) given the amount of profit you want. p(X|y).
MachineLearning,3cjloi,MusicIsLife1995,1 point,Wed Jul 8 19:33:25 2015 UTC,"Interesting suggestion. Maybe complementing that with Genetic Algorithms, it could output very nice results."
MachineLearning,3cjloi,unchandosoahi,1 point,Thu Jul 9 03:56:31 2015 UTC,How does the stochastic update rule of a Boltzmann machine guarantee that its repeated application will put the machine in thermal equilibrium where the probability of a state will be proportional to the exponential of the state's energy?
MachineLearning,3cjloi,question99,1 point,Wed Jul 8 21:18:21 2015 UTC,Is there anyone working to apply machine learning to protein folding? Is that even currently possible?
MachineLearning,3cjloi,2Punx2Furious,2,Wed Jul 8 21:57:50 2015 UTC,"A guest speaker at my university was doing that, so yes."
MachineLearning,3cjloi,EdwardRaff,1 point,Wed Jul 8 23:16:49 2015 UTC,"Wow. Do you have any more info, or do you know what should I search for to learn more?"
MachineLearning,3cjloi,2Punx2Furious,2,Wed Jul 8 23:18:29 2015 UTC,"I'm afraid I don't, it's been a while.   All I remember was they were using boosted neural networks to rank the likeness of different foldings"
MachineLearning,3cjloi,EdwardRaff,1 point,Thu Jul 9 11:48:10 2015 UTC,"A related question to my other:  I have seen it stated repeatedly that one of the problems with stepwise regression algorithms is you cannot trust any p-values or other statistics you see associated with your end model. That is to say, given input variables F and response variable y, if S is a subset of F chosen by some stepwise subset selection algorithm, the p-values R reports for each parameter if I call lm(y ~ S) will be overly optimistic. Furthermore, calculating the actual p-values for the parameters is a ""hard problem""  How hard? Specifically, are there any stepwise subset selection algorithms such that the p-values associated with the parameters of the chosen model can be calculated in a closed form for the general case? Are there any complex special cases for which this can be done? If not, is there any active research in this area?"
MachineLearning,3cjloi,Wolog,1 point,Wed Jul 8 23:57:43 2015 UTC,"I've heard of 'recurrent' NNs, and also 'recursive' NNs which are different.  Also recurrent is considered the opposite of feedforward. But there is a better/well-known word: feedback.  I'm wondering if there is research on feedback NNs, i.e., ones in which the output of the NN is fed as part of the input, and at the same time being used for useful purposes (sent out into the real world).  Also has anyone drawn connections between NNs and feedback and control systems?"
MachineLearning,3cjloi,physixer,2,Thu Jul 9 04:38:18 2015 UTC,"Recurrent is not considered the opposite of feedforward, because recurrent is still feedforward (going from the inputs towards the outputs). Recurrent NN are the deepest of feedforward net if you unfold them, as Schmidhuber puts it.  Nobody says feedback NN because there are already other terms. I think you mean recurrent. Of course, recurrent NN are very useful. Speech / manuscript recognition for example (see LSTM, a special kind of RNN).  As for your question, go there : A Statistical View of Deep Learning (IV): Recurrent Nets and Dynamical Systems, blog post by Shakir Mohamed"
MachineLearning,3cjloi,bhmoz,1 point,Thu Jul 9 08:11:44 2015 UTC,How do ReLUs work? There's no gradient when y <= 0; so what will it learn?
MachineLearning,3cjloi,maxxxpowerful,1 point,Thu Jul 9 19:41:04 2015 UTC,....so how well does machine learning work in the real world...??
MachineLearning,3ckdez,procarastinizer,9,Wed Jul 8 16:20:16 2015 UTC,"Post on arxiv.   The field is moving so fast these days that by the time they get presented at the conference, they're already dated."
MachineLearning,3ckdez,rantana,3,Wed Jul 8 16:26:20 2015 UTC,"Its not posted on arXiv. My adviser believes on the integrity of the double-blind reviews (and so do I for that matter) on these conferenes and therefore disagrees with me when I suggest posting on arXiv. He also considers it a bad idea that top authors do post papers in advance in arXiv and has constantly argued against it in conference committees.   Further from that point, my paper is not something that is in a fast moving field. Its an idea that not many care about and even after three years ""on the bench"", I don't see anyone publishing the kind of method that I am proposing even though its very intuitive. Its in a very specific part of CV literature and there are only a handful of groups that work in this topic.   Even the top groups who work in my topic ( I am not sure if my group is still considered one of the top groups in this topic because we haven't had a paper in it since the decade started ) are now adapting the ideas for deep learning and is still showing substantially lower results."
MachineLearning,3ckdez,XalosXandrez,5,Wed Jul 8 16:42:59 2015 UTC,"I think you could possibly ""mathematize"" your paper by including some small analysis - some kind of guarantees maybe. If you are beating deep learning then it means that you are doing some sort of computation which is otherwise very difficult for a deep network to do. Is there some opportunity there?"
MachineLearning,3ckdez,chcampb,3,Wed Jul 8 17:09:02 2015 UTC,"I am not ""beating"" deep learning per se, there aren't any (at least good) deep learning methods out there yet. A lot of folk (including myself) are attempting deep learning techniques it just doesn't work.   I did ""mathematize"" my paper every time  I resubmitted. First I showed significant results on datasets; second submission I showed  a simple exercise with a two point ""dataset"" and mathematically showed why in that case my method is better than others with the same dataset; third time revision, I showed why my method is computationally better than the others. Now to ICCV 2015, I showed an experiment where I am following the same trend as another SOTA in labeling noise which the SOTA was written for.   I see your point though, I have to think of an argument as to why this is difficult for neural networks to solve, if possible mathematically."
MachineLearning,3ckdez,chcampb,2,Wed Jul 8 17:13:59 2015 UTC,"there aren't any (at least good) deep learning methods out there yet   From a layman's perspective, it seems like there are new techniques and better results every day from deep learning. Why would these not be considered good?"
MachineLearning,3ckdez,quirm,3,Wed Jul 8 19:01:05 2015 UTC,"Think about it this way. Stopping short of using Hinton's capsules, every method is deep learning requires A LOT of data before you can get a good representation. Not just that, the labels have to be quite noiseless as well. The topic I am working on is medical. There aren't datasets that are large enough for deep learning techniques to extract good representations. There are some methods which try use regeneration techniques and add ad-hoc features like guided momentum and unsupervised pre-training but there isn't really anything that is clearly better and published."
MachineLearning,3ckdez,rotit,3,Wed Jul 8 19:05:19 2015 UTC,"OK, that is a pretty good reason to say that they are not acceptable.  Is anything that the Watson team is doing public yet? That's exactly the thing they are working on, is everything still proprietary?"
MachineLearning,3ckdez,alexmlamb,1 point,Wed Jul 8 19:13:31 2015 UTC,"AFAIK, there isn't anything out there yet."
MachineLearning,3ckdez,alexmlamb,1 point,Wed Jul 8 19:15:57 2015 UTC,Unless I make some serious constraining assumptions that seriously limits the scope of my core idea itself.
MachineLearning,3ckdez,penguinElephant,3,Wed Jul 8 19:16:29 2015 UTC,"Unsupervised pre-training is not widely used anymore, thanks to dropout and co. It is definitely possible to learn good representations on smaller datasets with convolutional neural networks, but I would suggest to transfer weights from the first few layers of a bigger task. It does not need to be very related, if you're working with medical images, transferring from imagenet could still be a good idea. I had very good results on 7-way classification with 100 short examples per class and transfer learning from a bigger task of the same domain (audio)."
MachineLearning,3ckdez,penguinElephant,1 point,Thu Jul 9 08:35:07 2015 UTC,I am considering initialization with overfeat weights ( or may be a CNN I train on a larger dataset ) just so that my weights look gabo-rish.
MachineLearning,3ckdez,dwf,1 point,Thu Jul 9 19:13:45 2015 UTC,OP probably meant in their sub field
MachineLearning,3ckdez,goblin_got_game,1 point,Wed Jul 8 19:06:22 2015 UTC,I think that he means that they're not good for his specific mystery task.  One reason for that may be due to a lack of labeled data.
MachineLearning,3ckdez,brainggear,1 point,Wed Jul 8 20:13:14 2015 UTC,"It is. In medical imaging, data is notoriously difficult to obtain. The dataset I have is usually 3000X3000X3 images and I have 400 of them for a 5 class classification task."
MachineLearning,3ckdez,cmaddis,1 point,Wed Jul 8 20:34:03 2015 UTC,"Is your method counting the number of red pixels in the image and classifying images with too many red pixels as cancer?  If so, I can see why reviewers would be critical."
MachineLearning,3ckdez,sieisteinmodel,1 point,Wed Jul 8 22:21:21 2015 UTC,No. Its a multi-instance learning framework with a 1-norm SVM optimizing a density estimate like function which was formulated using some metric space transformations. The transformations are fully learnt as well. I also have some assumptions that lead to a nice sparse formulation.   Like I said the method is not simple. Its not as sound as some ICML papers theoretically - yes. I don't have any proofs showing error bounds - yes. But I haven't had a bad review in all these top conferences on the methodology itself or the novelty. The negatives reviews that I have had in the earlier versions are may be how does this parameter vary with respect to datasize ... or you need to cite these papers .. or you need to compare yourself on this dataset. .. All of which I sufficiently incorporate in either the rebuttals or the next resubmissions.
MachineLearning,3ckdez,kjearns,1 point,Wed Jul 8 23:13:29 2015 UTC,Its never a strong reject as well. Its always two posters and one weak reject or similar.
MachineLearning,3ckdez,kjearns,1 point,Wed Jul 8 23:23:21 2015 UTC,have you considered submitting to journals geared more towards medical imaging? It should be more appreciated there
MachineLearning,3ckdez,farsass,1 point,Thu Jul 9 00:31:16 2015 UTC,"I am on the fence about that. Though my application domain is medical imaging, the contribution is core machine learning."
MachineLearning,3ckdez,farsass,1 point,Thu Jul 9 00:50:05 2015 UTC,"To my understanding, it seems that you are contributing a new machine learning method which you know works on medical imaging datasets really well. The medical imaging journals would be interested in that, even if the focus is on machine learning.  Places like NIPS and ICML tend to focus on the theory. This is because they generally want something that is very different than what was done before, and these very different things need some theoretical backing. Of course, very very different papers usually dont get accepted either, but it seems to me like your problem is that you have developed a new, useful method as opposed to a very different method (hence, why ppl think its too simple)"
MachineLearning,3ckdez,farsass,4,Thu Jul 9 01:48:51 2015 UTC,"Impenetrable double blind reviewing has largely been a fiction for quite some time. The only thing that preserves the blind is the honour code. I do not explicitly seek out the unblinded paper when I review but if I happen across it, I note it in my private comments to the AC as well as the extent (if any) I feel my review was biased by it. It sounds like there'd be a small number of guesses where your paper came from in a double blind review situation, so the blinding is not having that much of an effect to begin with."
MachineLearning,3ckdez,death_to_patents,1 point,Wed Jul 8 22:25:02 2015 UTC,Or post it on your personal website as a Technical Report.  It will be less of an infringement on the review process since way less people will see it.
MachineLearning,3ckdez,Berecursive,8,Wed Jul 8 22:12:32 2015 UTC,"This may not be on time for this particular paper, or you may not like this approach. I'm not defending the reviewers, in fact I frequently argue vigorously in favor of novel but unpolished papers. But when they say it's ""too simple"" they do not mean the technique should be more complicated; they may mean that the paper itself does not bring enough insight and analysis.  Many times, people propose a very very simple technique, but then spend a couple sections proving some interesting things about it. You can prove that a simple iteration converges, and under what conditions; or that another technique reduces to yours after a couple modifications and under some conditions. This math does not complicate your technique, but shows that it is more than an empirical find.  Keeping with the DL theme, dropout as a recipe seems like a simple hack -- but interpreting it as an ensemble of networks is interesting (which they do in the paper). SGD and related algorithms are very simple, but they are usually accompanied by some asymptotic analysis. You don't need to complicate that much, but if you can bring that sort of thing to the table it really helps solidify a paper. And your technique remains simple."
MachineLearning,3ckdez,brainggear,2,Thu Jul 9 00:16:17 2015 UTC,This I totally agree with and understand. My next step is to try and see if I can do some error bounds or some convergence proofs. Thanks.
MachineLearning,3ckdez,brainggear,2,Thu Jul 9 00:49:14 2015 UTC,"It's a little ironic that you picked dropout, because it was rejected initially."
MachineLearning,3ckdez,Berecursive,1 point,Thu Jul 9 14:56:57 2015 UTC,By Science. That is ok.
MachineLearning,3ckdez,j1395010,6,Fri Jul 10 07:55:45 2015 UTC,You should put it on arxiv. All of your reasons for not putting it on arxiv are really stupid. Keeping it to yourself after this much time accomplishes nothing except hurting you.
MachineLearning,3ckdez,classicalhumanbeing,3,Wed Jul 8 18:44:40 2015 UTC,"I am certainly trying to convince my adviser that I need this up on arXiv, but I would like to disagree with you that not putting it on arXiv is stupid. It clearly violates the integrity of a double blind submission. There are terrible papers in top conferences today because people publish their papers in arXiv first. Indeed its a form of bullying the reviewers."
MachineLearning,3ckdez,Berecursive,3,Wed Jul 8 19:01:26 2015 UTC,The integrity of double blind submission only exists if everyone does it. If everyone else plays by different rules then you only hurt yourself by being an idealist.
MachineLearning,3ckdez,j1395010,3,Thu Jul 9 07:53:10 2015 UTC,Then upload it anonymously or under a pseudonym
MachineLearning,3ciprk,john_philip,1 point,Wed Jul 8 05:39:08 2015 UTC,Well guess my days are now full. Many thanks!
MachineLearning,3cfvyf,samim23,14,Tue Jul 7 16:24:53 2015 UTC,;-) ;-) ;-)
MachineLearning,3cfvyf,Make3,3,Tue Jul 7 20:18:29 2015 UTC,How long does it typically take to get all the Caff and FFMPEG dependencies running? I am figuring out if I can do this inside an hour...
MachineLearning,3cfvyf,ContentScript,4,Tue Jul 7 19:37:10 2015 UTC,Took me longer than that for caffe but i had some issues
MachineLearning,3cfvyf,jamesj,2,Tue Jul 7 20:23:13 2015 UTC,A day if you haven't done it before.
MachineLearning,3cfvyf,makeranton,2,Tue Jul 7 22:36:53 2015 UTC,The first example is so scary!
MachineLearning,3cfvyf,phunter_lau,2,Tue Jul 7 22:46:51 2015 UTC,Dafuq did I just watch
MachineLearning,3cfvyf,falcon_shark,1 point,Tue Jul 7 17:26:00 2015 UTC,It would be interesting if with each frame the 'hallucinations' followed the subject in the pictures.
MachineLearning,3cfvyf,devDorito,6,Tue Jul 7 21:37:59 2015 UTC,Not op but that's what I did here: https://www.youtube.com/watch?v=SZHx78vuNho
MachineLearning,3cfvyf,makeranton,1 point,Tue Jul 7 22:37:50 2015 UTC,Now someone run the Dumbo pink elephants on parade sequence through this
MachineLearning,3cfvyf,Ferboten,3,Tue Jul 7 18:58:31 2015 UTC,"Good idea. My GPU capacity is stretched right now, do it!"
MachineLearning,3ch6ch,harrism,1 point,Tue Jul 7 21:46:40 2015 UTC,Didn't see a download after logging in
MachineLearning,3cl6t5,CocoaBotter,1 point,Wed Jul 8 19:42:34 2015 UTC,"Never heard of the game before, just checked it out now, but why not try a more minimal approach - give the network a 3-value pair for each player (x,y coordinates + momentum)? You'd have to train the net separately for each map then, though."
MachineLearning,3cl6t5,neopolitan77,1 point,Wed Jul 8 19:53:30 2015 UTC,"Something like X, Y, VX, and VY could work.  There are a few complications, as you can't tell the position or velocity of players who are off-screen.  That would have made it a lot easier.  One thing that I was wondering about is whether XY coordinates work better or worse than polar coords.  I'd presume that since movement is XY-based (arrow keys) that XY position and velocity would be better than polar.  I would assume that you would also need some kind of input about map structure if you wanted to make a net that works for most/all maps."
MachineLearning,3cl6t5,simonhughes22,1 point,Wed Jul 8 20:14:16 2015 UTC,"Sounds like more of a reinforcement learning problem, Neural Nets are good at that too, check out the deep mind deep q learning algorithm for instance (versions of which are on Github I believe)."
MachineLearning,3ckis9,XalosXandrez,2,Wed Jul 8 16:58:28 2015 UTC,are you missing the giant paragraph about loadcaffe?
MachineLearning,3ckis9,j1395010,1 point,Wed Jul 8 18:25:57 2015 UTC,does it mention fine-tuning?
MachineLearning,3ckis9,j1395010,1 point,Wed Jul 8 18:47:58 2015 UTC,no. it mentions a slew of pre-trained models however.
MachineLearning,3ckis9,r-sync,1 point,Wed Jul 8 18:52:34 2015 UTC,"i'm not sure what you want to know. do you want to fine-tune? If you know how to use torch, fine-tuning is pretty natural, there's nothing you have to do special except initialize with a pre-trained model (available via loadcaffe)"
MachineLearning,3chylb,Noncomment,7,Wed Jul 8 01:28:52 2015 UTC,"The code you saw was probably just loading some data that's natively YUV; YUV or similar is common in image and video compression (e.g. JPEG and MPEG). I don't think YUV by itself would be a big advantage over RGB. However, the reason image/video codecs are YUV is so they can reduce the resolution of the U and V channels while keeping Y at full resolution, because luminance is more important than color. If you can reduce the resolution of U and V in a way that's compatible with convolutional nets, your net should be half the size and therefore twice as fast, without much loss of accuracy.  Another interesting exercise in the same vein would be to train a neural net on raw bayer pattern data from a camera sensor, instead of demosaicing it to RGB first.  I think right now there's too much low hanging fruit elsewhere for this to matter much, but once the field matures, these kinds of efficiency improvements will matter more."
MachineLearning,3chylb,modeless,1 point,Wed Jul 8 06:48:29 2015 UTC,the reason image/video codecs are YUV is so they can reduce the resolution of the U and V channels while keeping Y at full resolution   A bit like the GPU1/GPU2 features in alexnet (figure3)
MachineLearning,3chylb,suki907,5,Fri Jul 10 09:44:49 2015 UTC,"If any of YUV components are specifically more relevant to the problem, it will be better.  Conversely, if e.g. R was extremely important, RGB would be better.  It's more statistically efficient to use features that are more reflective of what you're trying to model.   Why waste data on learning a linear transformation when they could be transformed to a more relevant basis before the fact?   The more data you have, the less difference it will make."
MachineLearning,3chylb,phillypoopskins,1 point,Wed Jul 8 04:49:46 2015 UTC,I mean to say that it doesn't require any additional parameters to convert between them. The weights of only the very first layer would just be slightly different.  A more generalized version of my question: does any linear transformation to the inputs make a difference with neural networks?
MachineLearning,3chylb,benanne,2,Wed Jul 8 05:06:30 2015 UTC,"In theory, no. In practice, the optimisation trajectory is also affected by this, so you may get slightly different results. Especially if the linear functions you're using have coefficients with a wide range of magnitudes. There may also be interactions with regularization: keeping the L2 norm of the weights of the first layer small will have different semantics, for example."
MachineLearning,3chylb,nkorslund,1 point,Wed Jul 8 08:42:12 2015 UTC,"NN optimization is not a perfect algorithm. If you require it to learn the RGB->YUV conversion in addition to all the other learning it is doing, it might take longer, it might increase the chance of hitting poor local minima, and so on. Just because a network CAN learn something, doesn't mean it will.  Although in the end this is an empirical science. The only real answer to your question is to test the same network structure with both RGB and YUV and see which one gets the best results."
MachineLearning,3chylb,vikkamath,3,Wed Jul 8 14:32:44 2015 UTC,A colleague and friend of mine just wrote his Master's thesis on the same topic.
MachineLearning,3chylb,skier_scott,1 point,Wed Jul 8 13:49:41 2015 UTC,"Yes, it's absolutely an advantage. RGB is constrained by device physics and the output is garbage. Other color spaces (HSV, etc) emphasize different things -- subtractive/additive color, pigments, hues, etc. I wrote a blog post on this  https://scottsievert.github.io/blog/2015/04/23/image-sqrt/  The coolest part about this -- I made an interactive widget to test adding in different color spaces!"
MachineLearning,3chylb,sdsfs23fs,2,Wed Jul 8 03:46:41 2015 UTC,"There is nothing wrong with different color spaces for different purposes. My point is that it makes no difference when feeding it into a neural network.  An NN trained on one color space shouldn't have any advantage over one trained on another. They will just have slightly different weights in the very first layer. Everything else could be exactly equivalent.  YUV, specifically, is just a linear function."
MachineLearning,3cgqom,DJKoolKeith,11,Tue Jul 7 19:56:46 2015 UTC,"I have tried various approaches to this, so far with little success.  I tried classifiers on music (by ""band"").   I tried adversarial networks.  I tried RNN's.  I tried both directly on a waveform (wasn't really expecting that to work) and on stft'd chunked audio.  I had most success with an adversarial trained LSTM RNN with a small ""avoidance"" force on the state vector to deter the system from getting ""stuck"" in a stable loop.  Even so, results were by no means ""good"" - there were odd hints at melody, but not much beyond that.  In general, my STFT system didn't have a good way to embed phase information for the network to process.   I ended up just inputting the real and imaginary components as independent inputs, which is obviously suboptimal considering absolute phase compared to the sampling clock isn't relevant in music.  Also, most music isn't absolute in pitch.  Ie. a tune can be shifted up one octave and still be the ""same tune"".   Since the outputs of a fourier transform are in linear frequency space but octaves are only equidistant in log frequency space, the network couldn't learn that relationship.   I couldn't find a good short term log-frequency reversible transform to use."
MachineLearning,3cgqom,londons_explorer,2,Tue Jul 7 20:57:55 2015 UTC,I think one problem is that the network has no training on noise or environmental sounds not considered music to help it pick out what is important in music and avoid non-music sounds.  Consider that all ImageNet images have backgrounds and environmental effects on lighting.  Maybe if you had recordings of a song being played on speakers out in the world in several environments could help it better identify music qualities.
MachineLearning,3cgqom,im_only_a_dolphin,1 point,Wed Jul 8 13:34:01 2015 UTC,"Interesting work with the STFTs - I have been thinking that this is indeed the way to go when it comes to audio.    I ended up just inputting the real and imaginary components as independent inputs, which is obviously suboptimal   Hmm, not sure why you say this - actually, I take it back - there needs to be some way to learn atan(real/imag) to get phase, and the algorithm cannot do this, then its sol - BUT... why not give it two images, one for the magnitude, and one of the phase itself?   Also, most music isn't absolute in pitch. Ie. a tune can be shifted up one octave and still be the ""same tune"". Since the outputs of a fourier transform are in linear frequency space but octaves are only equidistant in log frequency space, the network couldn't learn that relationship.   This is one of the reasons that keeps me deeply (pun?) skeptical about DNNs in general; not to say that they are not useful etc, of course I believe that, but I am not sure that they can learn 'any' relationship so to speak. For example, as you said, learning that the optimal sampling should be logarithmic instead of linear. If we do that for it, then we are accused of 'feature engineering'..."
MachineLearning,3cgqom,Ayakalam,5,Wed Jul 8 15:19:11 2015 UTC,"Well, I've being trying to apply Andrej Karpathy's Character RNN to raw music audio.  I posted this already in another thread, but here's a sample of the kind of stuff it can generate:  https://www.youtube.com/watch?v=eusCZThnQ-U"
MachineLearning,3cgqom,JosephLChu,6,Tue Jul 7 22:00:32 2015 UTC,In the year 2015 the machines have become sentient and have tried to communicate with us by imitating Yoko Ono.
MachineLearning,3cgqom,dauntless26,2,Wed Jul 8 07:21:15 2015 UTC,"I actually trained this using a few songs of a certain Japanese pop rock band as the dataset, so if it learns any language at all, it should be Japanese or bad Engrish."
MachineLearning,3cgqom,JosephLChu,2,Wed Jul 8 21:19:11 2015 UTC,That sounds sort of like actual music. How did you format the audio into char-rnn?
MachineLearning,3cgqom,Noncomment,2,Tue Jul 7 22:29:57 2015 UTC,"Well, first I used Audacity to merge the songs in the dataset into one WAV file and saved it at 8000Hz and 8bit encoding.  Then I used the sndfile library to convert the frames into a tensor that char-rnn could read.  I modified char-rnn to use an exactly 256 ""character"" alphabet, and mapped that to the 8bit values (0-255) that made up the tensor (which I also had to normalize because the smallest that sndfile reads and encodes is in short integers)."
MachineLearning,3cgqom,JosephLChu,3,Tue Jul 7 23:43:38 2015 UTC,Oh so it's actually working on a raw byte level encoding of an audio file? That it works at all is incredible. I can definitely hear human voices and beats in there.
MachineLearning,3cgqom,Noncomment,2,Wed Jul 8 00:48:27 2015 UTC,"Yep.  I wanted to try it on the raw signal because, preprocessing steps like Fourier Transforms have the potential to introduce error and data loss (however small) and aren't exactly natural, not to mention adding another step in the process to complicate things.  I also noted that the raw digital frame encoding already translates quite well into a fixed alphabet length that the RNN can utilize.  When in doubt, it makes sense to me to let the neural net learn the best features to extract."
MachineLearning,3cgqom,JosephLChu,1 point,Wed Jul 8 21:31:43 2015 UTC,"I was so close, only used a single 3 minute song (8-bit at 8khz) trained for 14 epochs with two layers; I got a lot of static sounding noise with only one or two whoosh noises before giving up."
MachineLearning,3cgqom,robertsdionne,2,Tue Jul 7 23:53:32 2015 UTC,That's awesome.  It sounds like Björk playing on a radio station that's too far away.
MachineLearning,3cgqom,robertsdionne,1 point,Tue Jul 7 23:16:09 2015 UTC,"Oh my god, do you have details that you can share?"
MachineLearning,3cgqom,JosephLChu,6,Tue Jul 7 23:28:27 2015 UTC,"I'm still debating how much I should give away...  In terms of hardware, I have a dedicated desktop that's been upgraded with an Nvidia Geforce Titan X.  Compared to the GTX 660 I was using before, the main advantage seems to be the sheer amount of video RAM (12 GBs) this gives me.  Even with this set up, it takes anywhere from 10 to 25 hours to train a reasonable sized net to 50 epochs.  I've actually been experimenting for a few weeks now (generally letting the machine run throughout the day and overnight) with different configurations, mostly regarding hyper-parameters like the number of hidden units, the sequence length, the number of layers, and dropout.  There have actually been some odd surprises.  Sequence length seems to be very important, as at the default length it will actually fail to learn, with the error: ""loss is exploding, aborting"".  I've also seen this happen when dropout is on and there are too many layers.  You would think that a longer sequence length then would be strictly better, but the way the library is set up, doubling the sequence length seems to double the amount of memory the network takes up, which means there's a trade off between the number of neurons possible and the sequence length for a given memory size.  I have a silly theory about how the optimal sequence length may be related to either Shannon's concept of Channel Capacity and Entropy, or the frequency of human auditory perception.  Aside from that, I've yet to attempt to optimize things like learning rate."
MachineLearning,3cgqom,robertsdionne,1 point,Wed Jul 8 00:09:09 2015 UTC,Here's my day's worth of training:  https://soundcloud.com/robertsdionne/sets/char-rnn-ipzel  (use headphones)
MachineLearning,3cgqom,JosephLChu,1 point,Thu Jul 9 01:29:46 2015 UTC,Nice work so far!
MachineLearning,3cgqom,schmook,1 point,Thu Jul 9 22:53:29 2015 UTC,"That's much closer to a good start than I ever got. I can distinctly hear drums, lead guitars and ""voices"" but it still sounds incoherent.   I was thinking if some ""convolutional"" features might help. It seems to me that music have multiple scales of autocorrelation (there's the single notes, with their attack profiles, there's the sequence of notes building up melody, there's the variations on that given theme to compose the whole piece, repetitions, etc) and the RNN won't be able to capture this alone, it seems to focus on the very short term correlation."
MachineLearning,3cgqom,JosephLChu,1 point,Wed Jul 8 11:52:22 2015 UTC,"It's quite possible that having some number of convolutional layers to extract features for the higher level RNN could make it work better.  Though that somewhat complicates the architecture and I wanted to try it first without making too many changes to Karpathy's core code.  Though, this RNN uses LSTM neuron blocks, which have been shown to be able to capture longer term correlations better than an RNN with more generic neurons.  I'm also thinking about attempting to implement the Clockwork RNN that recently was developed and see if that can do a better job."
MachineLearning,3cgqom,unchandosoahi,1 point,Wed Jul 8 21:17:04 2015 UTC,Don't hear this in home at night. Scary and really great work as fack.
MachineLearning,3cgqom,your_aunt_pam,2,Thu Jul 9 04:04:04 2015 UTC,"Speech classification? Classifying short clips, or even single phonemes."
MachineLearning,3cgqom,cybrbeast,2,Tue Jul 7 20:34:29 2015 UTC,"I found this example where machine learning was applied to Rage Against the Machine music and produces something that sounds like music, though still pretty bad though."
MachineLearning,3cgqom,VelveteenAmbush,1 point,Wed Jul 8 04:54:56 2015 UTC,Hard to tell what his method was... sounds like it was working with MIDI files or something rather than raw audio.
MachineLearning,3cgqom,siblbombs,1 point,Fri Jul 10 01:46:10 2015 UTC,It would be challenging because there is no audio equivalent of the image net database. Labeling an image as 'dog' is a lot more concrete than labeling a song as 'rock'.
MachineLearning,3cgqom,gsmafra,1 point,Tue Jul 7 20:05:02 2015 UTC,"There is the Million Song Dataset. The problem IMO is that people in general are not using conv-nets directly on raw audio and still rely on spectrograms and MFCCs, which indicates this is not an efficient solution  Edit: ""The Million Song Dataset is a freely-available collection of audio features"""
MachineLearning,3cgqom,siblbombs,2,Tue Jul 7 20:09:38 2015 UTC,"I think this approach could work to produce individual sounds (french horn, drum, snare, electric guitar riff), but I'm not sure how well it would do at producing full songs."
MachineLearning,3cgqom,kkastner,3,Tue Jul 7 20:21:40 2015 UTC,"I think it has the potential if you leverage /u/bennane 's work on clip classification for genre, using million songs instead of Spotify data coupled with (last.fm maybe?) scores or recommendations. http://benanne.github.io/2014/08/05/spotify-cnns.html  In general I think it could be doable but as you say, you would need to gather the data (even MSD is a pain to get) which could be unpleasant."
MachineLearning,3cgqom,siblbombs,1 point,Wed Jul 8 07:37:26 2015 UTC,"Yea, I've been doing a bunch of character-level CNN/RNN stuff lately so I'm trying to think of good ways to apply inceptionism to text, getting a workable dataset is a similar issue there."
MachineLearning,3cgqom,benanne,1 point,Wed Jul 8 12:51:12 2015 UTC,until fairly recently it was possible to download 30 second preview clips for almost all of the tracks (my colleague and I did this for our work on content-based music recommendation). I believe an API change has made that a lot more challenging though.
MachineLearning,3cgqom,kkastner,1 point,Wed Jul 8 10:11:12 2015 UTC,"I tried to start something like this with Bandcamp's API once upon a time, using the tags bands gave themselves. Unfortunately I think Bandcamp too has changed their API."
MachineLearning,3cgqom,autowikibot,0,Wed Jul 8 16:50:06 2015 UTC,"Spectrogram:       A spectrogram is a visual representation of the spectrum of frequencies in a sound or other signal as they vary with time or some other variable. Spectrograms are sometimes called spectral waterfalls, voiceprints, or voicegrams.  Spectrograms can be used to identify spoken words phonetically, and to analyse the various calls of animals. They are used extensively in the development of the fields of music, sonar, radar, and speech processing,  seismology, etc.  The instrument that generates a spectrogram is called a spectrograph.    Image i - Typical spectrogram of the spoken words ""nineteenth century"". The lower frequencies are more dense because it is a male voice. The legend to the right shows that the color intensity increases with the density.     Relevant: Generalized spectrogram | Spectrum | Scaleogram | Pattern playback   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
MachineLearning,3clcnf,infiveyr,0,Wed Jul 8 20:22:18 2015 UTC,"Artificial Intelligence has significant negative press recently.  With movies like those of the Terminator series it has to be expected.  The unrealistic AI depicted in SciFi movies are all Strong AI capable of fairly human like general reasoning.  In our world they don't exist.  In fact, it has taken decades just to achieve the Narrow (or Weak) AI we have.  Narrow AI is tailored to fairly specific tasks for example, driving a car.  We need to keep developing AI software to unlock the secrets hidden in Big Data to provide a better life for all mankind. There's plenty of time to develop safety protocols for Strong AI - if we can achieve it."
MachineLearning,3cgggk,5at,5,Tue Jul 7 18:45:52 2015 UTC,"The big news seems to be FP16 support across the board. I wonder if their kernels are based on Nervana's or independently implemented, and how the performance compares."
MachineLearning,3cgggk,scott-gray,4,Tue Jul 7 19:01:52 2015 UTC,"fp16 is already supported in cuda and has been for some time.  Really, the only broken thing (at the cuda-c/ptx level) was being able to convert an fp16 to fp32 directly from the high bits of two packed values.  But even that still worked with additional unnecessary instructions.  Maybe they're really just talking about hgemm support.  I'm still in the dark about how much of my code (the Nervana kernels) they'll be using if any.. but for v3 they're only claiming a 1.5x speedup in VGG over v2.  I'm able to get a more than 2x speedup over cudnn v2 (in both fp32 and fp16)."
MachineLearning,3cgggk,scott-gray,5,Tue Jul 7 19:50:51 2015 UTC,"If I remember correctly, cuDNN and your conv kernels use different memory layouts, so it's probably not easy for NVIDIA to use them."
MachineLearning,3cgggk,scott-gray,3,Tue Jul 7 20:40:41 2015 UTC,"fp16 is already supported in cuda and has been for some time. Really, the only broken thing (at the cuda-c/ptx level) was being able to convert an fp16 to fp32 directly from the high bits of two packed values.    The Programming Guide (3.2.11.1.3 v.7.0) says that CUDA C does not support a ""float16"" data type. Perhaps you are talking about the cubin level? So I'm assuming that's changing, plus cuBLAS and cuDNN will support it now (I'm not at ICML. I'm just going by the info we have)   But even that still worked with additional unnecessary instructions. Maybe they're really just talking about hgemm support. I'm still in the dark about how much of my code (the Nervana kernels) they'll be using if any.. but for v3 they're only claiming a 1.5x speedup in VGG over v2. I'm able to get a more than 2x speedup over cudnn v2 (in both fp32 and fp16).   These numbers probably vary depending on the batch size, etc.?"
MachineLearning,3ck8g3,winstonl,2,Wed Jul 8 15:44:14 2015 UTC,"At Facebook, artificial intelligence researchers recently demonstrated a system that can read a summary of The Lord of The Rings, then answer questions about the books.    Has Facebook ever shown this?"
MachineLearning,3ck8g3,rantana,1 point,Wed Jul 8 16:09:32 2015 UTC,https://www.facebook.com/FBAIResearch/posts/362517620591864
MachineLearning,3ck8g3,pretendscholar,1 point,Wed Jul 8 16:53:07 2015 UTC,It's referring to this demo from the FB BABI project:  https://www.facebook.com/video.php?v=10153098860532200
MachineLearning,3ck8g3,cryptocerous,2,Wed Jul 8 16:54:43 2015 UTC,"Ah, so a highly structured non-natural summary. For a second, I thought FB moved past their BABI task."
MachineLearning,3ck8g3,rantana,2,Wed Jul 8 17:05:52 2015 UTC,"The original toy BABI task was very limited. Only a 40 word vocabulary, if I recall.  Subsequent work by other researchers has advanced far beyond that original task now though. Seems there will be significant real-world practical tech that builds upon those concepts before the end of the year."
MachineLearning,3ck8g3,cryptocerous,1 point,Wed Jul 8 17:12:09 2015 UTC,From Facebook?
MachineLearning,3ck8g3,pretendscholar,0,Fri Jul 10 00:24:29 2015 UTC,"I don't know, but do they have to show it to the public in order for this statement to be true?"
MachineLearning,3ck8g3,rantana,7,Wed Jul 8 16:33:18 2015 UTC,"Yes. This is a research community, not a rumor community."
MachineLearning,3ck8g3,pretendscholar,0,Wed Jul 8 16:38:12 2015 UTC,"What I meant to say is that Wired is a decent source for tech news, so I wouldn't say what they are publishing are rumors just because I have not heard about it myself."
MachineLearning,3ck8g3,FuschiaKnight,1 point,Wed Jul 8 16:41:01 2015 UTC,People want to see the project so they can see what advances have been made. Try to separate hype from progress.
MachineLearning,3cfu4l,pcunneen19,9,Tue Jul 7 16:12:05 2015 UTC,"Perhaps any papers that serve as good introductions to the field   Have you read a foundational textbook yet? Standard ones are Machine Learning: A Probabilistic Perpective, Probabilistic Graphical Models, or Pattern Recognition and Machine Learning. Until you've read at least one of these, you likely won't have the context needed to make the most of ML research papers."
MachineLearning,3cfu4l,bluecoffee,1 point,Tue Jul 7 18:02:49 2015 UTC,"Yea, I might go ahead and buy a textbook and start working through it myself. Thanks"
MachineLearning,3cfu4l,bluecoffee,3,Wed Jul 8 09:23:37 2015 UTC,Library Genesis.
MachineLearning,3cfu4l,goblin_got_game,5,Wed Jul 8 10:11:23 2015 UTC,"It's a bit old school, but Leslie Valiant's A Theory of the Learnable is a seminal paper that puts computational intelligence on a foundation.  It was the one that pushed me to study AI/ML."
MachineLearning,3cfu4l,w8803425,3,Tue Jul 7 16:58:55 2015 UTC,The Elements of Statistical Learning  Much recommended as you learn more about ML.(Some finds it's hard to understand for beginner)
MachineLearning,3cfu4l,luvmunky,2,Tue Jul 7 21:45:06 2015 UTC,Especially if you do the exercises diligently.
MachineLearning,3cfu4l,robertsdionne,4,Wed Jul 8 00:48:55 2015 UTC,In-progress: https://docs.google.com/document/d/1e77T9dZymll1dopzs1nbZtfTd6OOE9Qz7iK-xRpdRIw/edit?usp=sharing
MachineLearning,3cfu4l,egrefen,1 point,Wed Jul 8 01:17:59 2015 UTC,Nice work!
MachineLearning,3cfu4l,jdsutton,8,Wed Jul 8 12:36:45 2015 UTC,"I enjoyed these:  Snoek, J., Larochelle, H., and Adams, R., ""Practical Bayesian Optimization of Machine Learning Algorithms""  Mnih, Volodymyr et al. ""Playing Atari with Deep Reinforcement Learning."""
MachineLearning,3cfu4l,MeowMeowFuckingMeow,6,Tue Jul 7 16:51:16 2015 UTC,"""A Unifiying Review of Linear Gaussian Models"" http://mlg.eng.cam.ac.uk/zoubin/papers/lds.pdf  is a great work by Zoubin Gharamani and the (late) Sam Roweis, that gives some idea of the building blocks of machine learning from a bottom-up perspective, rather than ""holy shit bro did u see my 1000 node GPU cluster spit out a 0.09% absolute improvement on this 20 year old dataset, I added a regularizer to my network and it totally worked, yo bro wheres my CVPR paper"" neural network stuff."
MachineLearning,3cfu4l,GoldmanBallSachs_,4,Wed Jul 8 15:24:15 2015 UTC,AlexNet
MachineLearning,3cfu4l,TheGooseRider,2,Tue Jul 7 17:33:25 2015 UTC,"Im in the same situation, only im in engineering. I approached a prof and he is giving me some small research projects to do during school, provided I take Enthoughts Python tutorial and Andrew Ng's coursera course on ML. Would recommend both."
MachineLearning,3cfu4l,Articulated-rage,1 point,Tue Jul 7 18:01:14 2015 UTC,Thanks to everyone who has already responded. Really appreciate the feedback.
MachineLearning,3cfu4l,walrusesarecool,1 point,Wed Jul 8 09:23:01 2015 UTC,I commented once on some of my favorites: https://www.reddit.com/r/MachineLearning/comments/39897w/significant_machine_learning_milestones/cs1aqdk
MachineLearning,3cf2sy,compsens,7,Tue Jul 7 12:25:37 2015 UTC,"I loved this talk, but there were also some comments/questions after the talk that I think are worth mentioning: 1) Yoshua Bengio: In convolutional neural nets the activations take up more memory than the parameters. 2) The state of the art Google system has actually reduced its memory footprint significantly from ~500MB to ~10MB, so compressing the parameters may not be such a critical issue in this context anymore. 3) this also acts as a regulariser, and in fact it may be a pretty good form of regularisation"
MachineLearning,3cf2sy,fhuszar,2,Tue Jul 7 15:31:39 2015 UTC,Third point is great. Where can I find something on the first one ?
MachineLearning,3cf2sy,bhmoz,5,Tue Jul 7 15:49:22 2015 UTC,"blog post adds nothing to arxiv page, which was already published here"
MachineLearning,3cf2sy,Noncomment,2,Tue Jul 7 14:51:12 2015 UTC,"If I understand it correctly, it's random weight sharing. They generate a neural network where each neuron is assigned a random weight from a list, and then they learn those weights. I don't understand why this works. It's just fitting parameters of a totally random function."
MachineLearning,3cf2sy,kokopo,1 point,Tue Jul 7 19:04:02 2015 UTC,"please, correct me if I'm wrong.  Only the hash function is random. As they say, weight sharing has been done already, but in this work, they don't want to store the information of which parameter belongs to which bin. I understand it like this : instead of learning what parameter belong to what bin, they put approximately the same number of parameter in each bin with the help of a random hash on the parameters coordinate in the weight matrices.   Why does it work? Because weight sharing is a good idea, whether you randomly share or smartly share. Why? Because NN weights contain redundant info (see bibliography in the paper)  You want good regularization to avoid carrying more information than what's contained in the data. But in this paper they are not as ambitious as to optimize the binning. If I understand correctly, they choose a random hash (surjective) to approximately put as much weight in every bin. It is suboptimal if you compare to a possibly untractable hierarchical model where you learn the information ""belongs to class z"" (multinomial?) for example?   A more optimal weight sharing approach should be able to determine the number of the bins, the distribution of parameters in bins, etc... if you are interested in this subject: read about minimum description length.  In this paper, rather than study this, they want to save memory, a bit like you would use the hashing trick in NLP."
MachineLearning,3cf2sy,Noncomment,1 point,Tue Jul 7 21:40:57 2015 UTC,"But the neurons it generates are totally random. They aren't compressing anything. It can tune the weights of course. But since they are shared, for every weight that needs to increase, it's tied to another that needs to decrease.  The system is chaotic. Every parameter is randomly connected to different parts of the system. Tuning one weight might change a hundred different things in ways that are suboptimal.  Of course any system with enough parameters should be able to fit any function. And the over complexity might make it underfit and cause some regularization. But otherwise it seems extremely wasteful."
MachineLearning,3cf2sy,kokopo,3,Tue Jul 7 22:27:14 2015 UTC,"""the neurons it generates are totally random.""  only the mapping weight -> bin is random.   ""But since they are shared, for every weight that needs to increase, it's tied to another that needs to decrease.""  You know that, at start, every neuron in one layer are interchangeable. Just imagine that neurons in one bin will be forced to have the same behavior as their bin colleagues. Neuron m from layer n does not have a proper role in a net. Its role is essentially given by its initial value, not by its position in the weight matrix. So don't think of a neuron as something that should be fixed to this or that value in the end of the training. Rather, you need a certain distribution of neuron weights in each layer...  ""Tuning one weight might change a hundred different things in ways that are suboptimal.""  No, because all the gradients of errors are computed and enable us to use standard backprop techniques.   ""And the over complexity might make it underfit""  Do you mean overfit? I feel like you don't really understand how regularization works. Would you say that l1 is wasteful because we encourage weights to be null? Anyway, weight sharing is just one way to regularize."
MachineLearning,3cf2sy,Noncomment,1 point,Wed Jul 8 06:46:02 2015 UTC,"Ok yes the parameters are learnable and can theoretically fit any function. But they don't mean anything. It's just a random function. You are relying entirely on lucky coincidences. Like several neurons just having exactly the right set of shared weights so that they can do a meaningful computation.  Most of the time they won't, and so the vast majority of computations in the network will just be random noise. It's incredibly wasteful and hard to fit. Which is why I was surprised it actually learns anything.  At the very least, the title is incorrect. They aren't compressing anything. They are just creating a different type of model, which quite possibly requires many more parameters to fit the same data. It's like if I created a neural network and defined all the weights as being equal to 1. It's easy to compress, but it doesn't do anything."
MachineLearning,3cf2sy,j1395010,2,Wed Jul 8 08:06:58 2015 UTC,"did you miss the part about how it performs nearly as well with fewer parameters? If you need to criticize something, how about how this is inefficient to compute via the standard matrix multiplication methods."
MachineLearning,3cj5vt,PMMEYOURVCDIMENSION,2,Wed Jul 8 09:13:45 2015 UTC,"I'm going to assume ""top company"" means a large (usually Internet) based company or division.  The only ""cool"" AI roles at top companies are in their research departments, for which you need to be a PhD student or scientist. I'm talking Baidu Deep Learning, Microsoft Research, Google Research and DeepMind, and Facebook AI Research. I've worked in a role at one of these places as well as product side engineering teams. Trust me, the machine learning you do as a software engineer on say Bing Search Relevance, Google Knowledge, and Facebook Core Data Science is nothing close to ""interesting"" AI projects. Those roles use a little math, do some probability distributions, debug low metrics, and that's about it.  To answer your questions:   If you don't go to a top 10 US school or if you don't have publications (yes, as a MS student) then pretty low. Standard recruiting techniques: find alumni from your school or friends who work at said company Position your research ability. To researchers, software engineering is easy  Standard coding questions plus niche vision/NLP/ML/AI coding, design, or reasoning questions."
MachineLearning,3cj5vt,GoldmanBallSachs_,4,Wed Jul 8 20:46:44 2015 UTC,"top 5 what? Also, not everyone might be familiar with ""SWE"" as an abbreviation. I was close to flag your post as spam without even looking at it because the title looked cryptic/spam-y and an all-caps username doesn't increase my confidence, either.  You might want to edit your submission title."
MachineLearning,3cj5vt,BeatLeJuce,-4,Wed Jul 8 09:49:53 2015 UTC,"It doesn't look like I can edit title anymore. Did you read at least TL;DR in the end of the post? I don't see how ""top 5"" might be ambiguous given all these details. SWE is a pretty established abbreviation in IT. Those who don't know it probably don't have any relevant experience either."
MachineLearning,3cj5vt,BeatLeJuce,7,Wed Jul 8 10:06:50 2015 UTC,"I still don't know what you mean by top 5. So from where I'm standing it's definitely ambiguous.  And while SWE might be established in IT, not everyone in ML comes from that direction, so a lot of people might see your post in this subreddit and be confused.   In any case, of course I read your post. But  I don't have any knowledge about the type of question you're asking."
MachineLearning,3cj5vt,rockinghigh,1 point,Wed Jul 8 10:43:58 2015 UTC,I can confirm that SWE is not a common term at all. It may be a title at some companies when followed with a number but everybody says software engineer or simply engineer.
MachineLearning,3cgb1b,ojaved,1 point,Tue Jul 7 18:08:50 2015 UTC,I think allowing users to jump to any point of the videos is important to make your player more friendly. I was watching some videos on computer vision talks and I had to use youtube because sometimes not being a native english speaker or trying to understand technical points requires going back a few seconds.
MachineLearning,3cgb1b,farsass,1 point,Tue Jul 7 18:23:06 2015 UTC,You should be able to jump to any point in the video using ClipMine. Can you please tell me which device and OS are you using (e.g. ipad with IOS or windows 10 with IE)?
MachineLearning,3cgb1b,farsass,2,Tue Jul 7 18:30:48 2015 UTC,I apologize - it works if I turn uBlock Origin off. Much better now :)
MachineLearning,3cbui2,AnonMLResearcher,45,Mon Jul 6 18:22:08 2015 UTC,"Further to (2), about the doubt that a new company based on machine learning has to face:  It's not just from investment. If you're selling into a large enterprise, they won't let you just deny liability if it turns out that your software infringes on third-party IP (both copyright and patents). Typically they want you take out some liability insurance policy to cover that.  If Google's holding patents that cover your technology, no insurance company is going to take you on. It's also possible that we'll get to a point where there are so many unclear patents that just doing due diligence on your tech will exceed the likely value of the deal you're trying to close.  Ultimately I hope that researchers who are at Google or thinking of joining will think twice. Until now the perception has been that there's not really much difference between being a public researcher and being a private one at Google etc. Google have been publishing freely, so the game's been the same.  That really seems different now."
MachineLearning,3cbui2,syllogism_,5,Mon Jul 6 19:28:36 2015 UTC,"Further to this point, this makes building startups with hard technical machine learning component even harder. At least initially a large part of a company's value is the talent and the proprietary intellectual property they develop or can develop. If the big firms build a patent minefield in the area, the early valuations of these companies is less defendable, and they may also have less gunpower in negotiations with potential acquirors (for example Google themselves).  This effect is on top of the competition for talent (it's almost impossible for a startup to compete in this space with Google/FB), and the very aggressive early acquisition/acquihire strategy of the big companies.  Someone pointed out that what we observe today with machine learning may be parallel to what happened to the semiconductor industry, where a few large and established players pretty much monopolised talent and research in the space."
MachineLearning,3cbui2,fhuszar,1 point,Tue Jul 7 15:53:47 2015 UTC,"I agree great point.  However, someone will hopefully make a startup that covers the risk of being sued by Google over IP. And market efficiency will solve this. As long as they stick to the no-sue policy."
MachineLearning,3cbui2,antonosika,93,Tue Jul 7 14:41:39 2015 UTC,"Google is well known for patenting things like this with a public statement like: ""We're just patenting it so no one else does. This protects everyone."" I'm ok with this idea in theory, but point #2 remains valid. The fact that a patent exists does harm to the entire field.    Therefore, we need to call on Google to release these patents into the public domain. I don't mean release open licenses like Creative Commons, any license can be rescinded. Non-commercial doesn't cut it, because these methods are already in commercial use. They must be fully open and free."
MachineLearning,3cbui2,whhaaaat,31,Mon Jul 6 20:09:36 2015 UTC,"Therefore, we need to call on Google to release these patents into the public domain.   What a fabulous point. I will think of this the next time I hear the argument ""we're doing it so no one else does, trust us"". If they're really so grand, release them to public domain :o"
MachineLearning,3cbui2,Zulban,1 point,Tue Jul 7 03:23:17 2015 UTC,All they would need to do is send the contents of their patent application to a pre-print server such as arXiv instead of the patent office. Posting the invention publicly would establish prior art and undercut future IP claims.
MachineLearning,3cbui2,GoFrackOnCobalt,6,Wed Jul 8 02:51:22 2015 UTC,"Non-commercial doesn't cut it, because these methods are already in commercial use.    Forgive my ignorance but isn't the whole point of patents that they must be something novel and that the existence of ""prior art"" that does the same thing pretty much invalidates them? The fact that people already use these methods in commercial contexts - or otherwise - should invalidate the patent claim right? I guess the hard part is relying on patent clerks with no technical knowledge to realise that these are not novel concepts/inventions."
MachineLearning,3cbui2,DokktorRavenstein,10,Tue Jul 7 12:10:16 2015 UTC,"No, because in 2011 the US switched to a first-to-file patent system."
MachineLearning,3cbui2,whhaaaat,5,Tue Jul 7 18:19:35 2015 UTC,"I accidentally downvoted out of pure disgust.  Sorry for that, I gave it back."
MachineLearning,3cbui2,cfrounz,1 point,Wed Jul 8 00:03:11 2015 UTC,"I too am disgusted. However, I have just come up with a patent disclosure to submit called ""A method for oxidising the human bloodstream via mechanical inflation of air bladders and a diaphragm."" Completely novel I swear..."
MachineLearning,3cbui2,DokktorRavenstein,2,Wed Jul 8 10:43:56 2015 UTC,Fool! Now im going to file before you.
MachineLearning,3cbui2,cfrounz,6,Wed Jul 8 11:49:07 2015 UTC,"AFAIK prior art is still a thing - even with first-to-file. Just because someone asserts they have invented something new in a patent application doesn't mean they actually have. Big organizations make overly broad patent claims as a matter of course. But the patent hasn't been granted yet, and if it were, it wouldn't necessarily hold up in court.  That being said it is a major problem that big organizations, because of their unmatchable resources, use the patent system to promote their narrow-minded self-interest at the expense of innovation."
MachineLearning,3cbui2,GoFrackOnCobalt,2,Wed Jul 8 02:45:23 2015 UTC,I think this whole IP business has grown out of control and devolved into an artificial market invented by lawyers for the lawyers. There will come a time when we will need to challenge the notion of IP and put everything into public domain. Then people will have the time to actually work on implementing their ideas instead of worrying about the legal BS.
MachineLearning,3cbui2,ardakaraduman,1 point,Thu Jul 9 02:40:57 2015 UTC,absolutely.  the same issue is coming up in biotech where modified organisms are granted patents. we are just setting ourselves up for more oligarchy
MachineLearning,3cbui2,ba55fr33k,10,Thu Jul 9 16:58:22 2015 UTC,"Actually, Creative Commons licenses can't be revoked. That said, you'd never release a patent under a CC license; that's not what they're for. They're for licensing copyright rights, not patent rights.  Google sometimes does do like what you're suggesting. For example, read about what they did with their patents used in implementing the VP8 video compression format. (I assume you know how to look that up.)"
MachineLearning,3cbui2,troymcc,7,Tue Jul 7 04:09:15 2015 UTC,there are plenty of precedents for defensive patent pools and non-assertion agreement. google is being a bag of dicks.
MachineLearning,3cbui2,sdsfs23fs,2,Mon Jul 6 21:18:06 2015 UTC,"Or, defensive publication to establish prior art. It doesn't even need to be peer-reviewed. They could just post a pre-print on a public archive (eg arXiv) and that would establish prior art."
MachineLearning,3cbui2,GoFrackOnCobalt,1 point,Mon Jul 6 21:27:31 2015 UTC,"If you vilify the good guys, you're fucked. You literally have no way to tell good from bad outside of children's fairy tales."
MachineLearning,3cbui2,cran,10,Wed Jul 8 02:47:46 2015 UTC,"As I said before, I hope that at least Google's patent application “system and method for parallelizing convolutional neural networks” https://www.google.com/patents/WO2014105865A1 won't be approved due to prior art.   The earliest relevant paper (16 July 2012) cited by the patent application is this:  D. CIRESAN, U. MEIER, J. SCHIDHUBER: ""Multi-column deep neural networks for image classification"", PROCEEDINGS OF THE 2012 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR'12), 16 June 2012 (2012-06-16), pages 3642-3649, XP032232509, DOI: 10.1109/CVPR.2012.6248110  The patent examiners will probably find that there are even earlier very detailed publications on parallelizing convolutional neural networks by the same group, starting with this IJCAI paper of 2011:  D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, J. Schmidhuber. Flexible, High Performance Convolutional Neural Networks for Image Classification. International Joint Conference on Artificial Intelligence (IJCAI-2011, Barcelona), 2011. http://people.idsia.ch/~juergen/ijcai2011.pdf  Under http://people.idsia.ch/~juergen/deeplearning.html Schmidhuber writes that this decribes their ""special breed of GPU-based max-pooling convolutional networks (GPU-MPCNNs), now widely tested/used by research labs (e.g., Univ. Toronto/Google/Stanford) and companies (e.g., Apple) all over the world.""   Since 2011, ensembles of these parallelized convolutional neural networks also were used to win several competitions summarised under http://people.idsia.ch/~juergen/deeplearning.html   I cannot really see what’s different in the later publication by Alexander KRIZHEVSKY, Ilya SUTSKEVER, Geoffrey E. HINTON (2012). Or is there somebody out there who can point out a difference?  The other applications may have similar issues."
MachineLearning,3cbui2,funnypatent,0,Tue Jul 7 14:17:39 2015 UTC,"Since 2013, the patent law is first (inventor) to file, not first to invent.."
MachineLearning,3cbui2,lifebuoy,8,Tue Jul 7 09:16:26 2015 UTC,Classification?  Yeah I'm pretty sure logistic regression predates Google by a number of decades.
MachineLearning,3cbui2,beaverteeth92,8,Tue Jul 7 13:45:56 2015 UTC,"Linear discriminant analysis is even older.  And I'm pretty sure methods for ""putting different things into different piles"" have been around more or less forever."
MachineLearning,3cbui2,DavidJayHarris,1 point,Tue Jul 7 03:22:04 2015 UTC,"how much can we put you down for on getting lawyers and court dates? because while that may be true and trivially obvious, it takes a nearly insurmountable amount of time and money to make it a legal reality."
MachineLearning,3cbui2,pyvpx,1 point,Tue Jul 7 03:40:06 2015 UTC,Why?
MachineLearning,3cbui2,cfrounz,7,Tue Jul 7 10:02:55 2015 UTC,"I don't know much about the legalese of these situations, but in (2), one of the claims is   The method of claim 1, wherein the high-dimensional space is a one thousand dimensional space.   so, any method that isn't 1000 dimensions is safe?"
MachineLearning,3cbui2,Articulated-rage,5,Wed Jul 8 00:06:38 2015 UTC,"Technically, but good luck arguing that in court."
MachineLearning,3cbui2,Noncomment,6,Mon Jul 6 21:03:38 2015 UTC,"Patent courts certainly make a lot of mistakes but they aren't complete morons. They understand how claims are structured and know that you aren't violating a claim unless your method reads on every element of the claim.  If your method isn't 1000 dimensions, google would not sue you under claim 2 but they can still sue you under claim 1."
MachineLearning,3cbui2,Jaqqarhan,3,Tue Jul 7 00:28:42 2015 UTC,"It's safe from claim 2 but they can still sue you under claim 1 so it doesn't matter if you violated claim 2. The dependent claims (in this case, claim 2 is dependent claim 1) don't really matter in the independent claim (in this case, claim 1) is ruled valid."
MachineLearning,3cbui2,Jaqqarhan,20,Tue Jul 7 06:19:31 2015 UTC,"Spot on, one could easily elaborate on the negative effects of these actions for hours more. Funny how they timed these revelations just days after deepdream. No media outlet will ever publish the patent issue now."
MachineLearning,3cbui2,samim23,30,Tue Jul 7 06:14:14 2015 UTC,Google is a publicly traded company beholden to shareholder interests. The notion that they're somehow in it for the purity of art or science is incredibly misguided and naive. Their sole objective is to monopolize and provide their shareholders with the highest return possible. It is literally at the heart of what they do. Google and every other private sector company will only and always make the choice it believes will make it the most money. You want to know what their plan is? Just ask yourselves what their shareholders think will make them the most money.
MachineLearning,3cbui2,nested_dreams,25,Mon Jul 6 19:52:09 2015 UTC,"Keep in mind that Larry, Sergey and Eric do still have a controlling interest in the company via their Class B stock, unless that's changed recently. So it's not quite race to the bottom that it otherwise would be."
MachineLearning,3cbui2,dwf,9,Mon Jul 6 21:18:51 2015 UTC,"Yeah, for now. But what happens when they retire or die?"
MachineLearning,3cbui2,ginger_beer_m,1 point,Mon Jul 6 22:00:46 2015 UTC,"Exactly right. Shareholders interests don't matter as much if you own a controlling interest. If some unhappy shareholder says, ""Do what I say or I'm selling my stocks!"" the answer can just be, ""Okay. Go ahead. Goodbye and good riddance."""
MachineLearning,3cbui2,troymcc,3,Tue Jul 7 06:28:08 2015 UTC,"Yeah, if that's what they say. But what if they say ""I'm suing you for shareholder oppression / breaking fiduciary duty""? I've no idea how these would apply to something like google and proving idealistic patent strategies harming the minority but a cursory search reveals that publicly held corporations do have obligations to minority shareholders, discussed here for example, so it's the legal issues that are relevant, not just negotiating power."
MachineLearning,3cbui2,sharkinwolvesclothin,1 point,Tue Jul 7 04:11:57 2015 UTC,"In whoch case I think it goes back to defeating Google in court. Now though, instead of a startup, you are a disgruntled investor."
MachineLearning,3cbui2,cfrounz,5,Tue Jul 7 07:07:27 2015 UTC,"I don't think it was suggested by OP that Google should make everything open source. OP was definitely not questioning the business model of private companies in general. I haven't read the the patents in detail, but I was a bit surprised, at least by the classification one, this seemed to me something which, at least to very large extents, was already available (among other means, in the form of open source implementations). If that's really the case, and I stand corrected, than it's surprising but moreover worrysome that these types of patents are acknowledged."
MachineLearning,3cbui2,brophd,6,Wed Jul 8 00:08:22 2015 UTC,"Actually, Google is commanded uniquely by Larry Page and Sergey Brin. They don't distribute dividends, they don't give a damn about shareholders."
MachineLearning,3cbui2,killugon,7,Mon Jul 6 21:57:09 2015 UTC,Google has a relatively unique governance structure that puts their idealistic founders pretty much solely in charge.
MachineLearning,3cbui2,mrwillis2695,3,Tue Jul 7 01:40:15 2015 UTC,You want to know what their plan is?   That's easy. They described it in their mission statement when they first began their company. To say that they're not influenced by stockholders would be a lie. To say they don't see a greater purpose for their company would be ignoring all of the interviews they gave saying the exact opposite.
MachineLearning,3cbui2,RushAndAPush,0,Tue Jul 7 00:06:26 2015 UTC,Or not believing those interviews.
MachineLearning,3cbui2,cfrounz,5,Mon Jul 6 22:03:37 2015 UTC,Also.. Data augmentation: https://www.google.com/patents/US20140177947
MachineLearning,3cbui2,DrDetection,1 point,Wed Jul 8 00:09:24 2015 UTC,"ffs, color space changes to every pixel? That is any addition of noise! It has gone to these guys' heads"
MachineLearning,3cbui2,marijnfs,3,Tue Jul 7 05:32:57 2015 UTC,"how exactly does the american patent system work? This is only the published application which means it is not granted yet.  For example, in Germany the first version of a patent is usually published before the patent gets approved. And patents should not get approved if there is prior documentation of the invention, which mostly means other patents but does not have to.   So as long as the techniques, exactly as they are described in the first claim, can be found elsewhere, the first claim can not be approved and needs to be altered/expanded.  And even if the patents are approved, if somebody would sue for nullification and presents even one paper/book which was published before the patent was filed the patent will be nullified"
MachineLearning,3cbui2,unertlstr,1 point,Wed Jul 8 07:26:55 2015 UTC,"+1   I think we need to continue to be careful to not be so alarmed by published patent applications. Again, none of these are issued patents, but rather pending applications, which may never reach allowance from the PTO and, if they are allowed, will most likely be left with claims that have been narrowed significantly in scope in light of prior art and publicly known technologies.  There are certainly many issues with the U.S. patent system that need to be addressed, but I think it's counterproductive to not make the distinction between a mere application for a patent and an issued patent. If anything, in debating a broken patent system, our best examples to study should be broad, issued patents being used in litigation. In which case we can more productively debate either the patent's validity and/or what the scope of patentable subject matter under U.S. patent law should be."
MachineLearning,3cbui2,unertlstr,1 point,Tue Jul 7 11:19:04 2015 UTC,"but what I don't get is why do they even try this?  I mean especially the main claim of the classification patent. Off the top of my head I think that with only the original MNIST paper we have each single feature of the claim already present so this should never get approved. I know that you should always start with a too broad claim, but this makes no sense in my opinion"
MachineLearning,3cbui2,jalanb,4,Tue Jul 7 23:24:39 2015 UTC,http://patents.stackexchange.com/questions/13157/requesting-prior-art-on-google-machine-learning-patent
MachineLearning,3cbui2,watersign,4,Wed Jul 8 11:59:51 2015 UTC,so is the C4.5 algorithm gonna be shadow banned from all opensource tools/software languages??
MachineLearning,3cbui2,bluemason,2,Tue Jul 7 16:29:47 2015 UTC,We should crosspost this
MachineLearning,3cbui2,gergi,6,Tue Jul 7 02:14:23 2015 UTC,"To /r/legaladvice to see if we can take these to court and nullify them!  In Europe these patents would not be allowed, because algorithms and mathematical techniques are expressly exempted from patentability.  I wonder what effect this would have abroad when these patents would be granted."
MachineLearning,3cbui2,bluemason,6,Tue Jul 7 03:17:39 2015 UTC,Currently.  But remember TTIP is under way.
MachineLearning,3cbui2,gergi,1 point,Tue Jul 7 06:20:24 2015 UTC,What will this change?
MachineLearning,3cbui2,TotesMessenger,1 point,Tue Jul 7 06:50:03 2015 UTC,"It will harmonize the markets of EU and US.  This includes Regulations, Consumer Protection, and of course Industrial relevant Legislation aka Patent Laws."
MachineLearning,3cbui2,BeatLeJuce,2,Wed Jul 8 06:47:40 2015 UTC,"I'm a bot, bleep, bloop. Someone has linked to this thread from another place on reddit:   [/r/futurology] Why Google's new patent applications are alarming   If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads. (Info / Contact)"
MachineLearning,3cbui2,Nydhal,10,Wed Jul 8 15:23:17 2015 UTC,"I said it in the other thread, I'll say it again here:  People are overreacting. Google did in no way start this arms race. Pretty much every famous ML algorithm is patented. THIS IS NORMAL AND HAPPENS EVERY DAY. Support Vector Machines and many techniques related to them have been patented for ages (see e.g. these), yet it's one of the most popular ML algorithms and people use them in education, research and industry every day and no-one gets sued.  Latent Semantic Analysis is another great example of a popular algorithm that is patented and yet no-one cares.   There are a gazillion other ML patents just like these. Don't make this into something bigger than it is. Software-patents are an unenforceable joke anyhow.  Don't get me wrong, they suck and should be abolished, but this recent witch hunt is really just that: a witch hunt. Google just followed the standard procedure of everyone in the field.   TL;DR lay down your pitchforks"
MachineLearning,3cbui2,sdsfs23fs,7,Tue Jul 7 18:13:35 2015 UTC,I think the problem is this shouldn't be normal. Things should be different and more open. I'm dedicated to ML as a futur career and I didn't know about it before I read this.
MachineLearning,3cbui2,BeatLeJuce,10,Tue Jul 7 06:55:30 2015 UTC,"Your point is still bullshit. Plenty of industries have established patent pools which offer mutual protection from  patent fuckery. Google is in a perfect position to lead such an initiative, but instead they're being a bunch of cocks. Hate the player as well as the game."
MachineLearning,3cbui2,Nydhal,3,Tue Jul 7 08:27:42 2015 UTC,"Yes, that'd be awesome! I don't know too much about patent pools --  are they really feasible in active research areas? I'd expect them to be most common when companies have a common goal (e.g. establish MPEG as default encoding, or broaden the applicability of RFID), whereas there is no such incentive here, as a truly good ML discovery can give a very distinctive advantage in the market."
MachineLearning,3cbui2,Noncomment,4,Tue Jul 7 09:35:21 2015 UTC,"This should be reposted to other subreddits, r/futurology, r/rad_decentralization, r/singularity. The word has to come out otherwise this might be the start of a very tricky and dangerous arms race."
MachineLearning,3cbui2,antonosika,2,Tue Jul 7 14:00:58 2015 UTC,I posted it to /r/Futurology and hacker news. Feel free to post wherever you think it will get attention.
MachineLearning,3cbui2,marijnfs,1 point,Tue Jul 7 05:21:51 2015 UTC,"Could anyone explain how this is bad for academic research - as long as google will not sue open source libraries?  Arguably Google, or anyone in the field, can patent just to be able to actually share their ""trade secrets"" for the better of academics (-one of the few pros of IP according to theory of my theory of science courses)."
MachineLearning,3cbui2,godhaspurpledreads,2,Tue Jul 7 18:12:20 2015 UTC,"Well their researchers claim very standard thing as their own inventions, which is not cool in research. Since they participate in research their actions in patent space means something.   It also hurts research because now everyone working with these big players in ML have to watch out spreading/discussing their ideas, which is normally how research is done."
MachineLearning,3cbui2,enkiv2,1 point,Tue Jul 7 14:34:48 2015 UTC,"could google just be doing this to let people know which techniques they've ""mastered"" thus spurring research and development into other forms of machine learning? cause at the end of the day true AI will prob end up as mix of all these techniques and more we havent discovered yet. so this move may suck now, and slow down research within these techniques/models but will spur more innovation and progress down the line."
MachineLearning,3cbui2,godhaspurpledreads,1 point,Wed Jul 8 07:40:34 2015 UTC,"The way to do this is to file a preliminary patent application and then never file an actual application (thus putting the prior art where all the patent clerks can see it, but not actually getting any patent protection for it).  If they filed actual full applications rather than preliminary applications, then they want patent protection (or at least would like to be able to use this against competitors)."
MachineLearning,3cbui2,enkiv2,1 point,Tue Jul 7 22:25:25 2015 UTC,"ahh wasnt aware of the preliminary patent process. i partially agree, though if they make it public like that, id assume other companies could copy and file a complete patent, just cause, and then sue google for money."
MachineLearning,3cbui2,godhaspurpledreads,2,Wed Jul 8 18:14:47 2015 UTC,"Nope. That's the whole point. The preliminary application counts towards prior art but not towards a genuine application -- the applicant has a year to file a real application, after which the material can no longer be patented by anyone.  This technique is very common with companies who have material that might be patentable, and that they wish to use without worrying about other companies patenting it later. Rather than determining whether or not there's prior art that prevents them from having an application granted (or causes their granted patent to be overturned later), they merely ensure that the material can never be patented. Preliminary applications aren't subject to restrictions on novelty.  Now, part of this is merely purely practical. Historically this was common because although prior art outside the patent system counted against the novelty of new applications, patent clerks rarely had sufficient background to check other places for prior art, and rarely bothered even if they did. But, a preliminary application also counts towards first-to-file, because it's first-to-file an application (not first to have one's application granted), and a preliminary application is a normal application with a request to have the grant process omitted.  (Ostensibly the preliminary application process is for getting a filing date before you do the necessary research to determine whether or not something can be granted -- in other words, if you and your competitor are working on very similar things, you can throw out a bunch of preliminary applications before your competitor does, and then submit an amended filing up to a year later that's more legally bulletproof -- where the claims have been modified to be more novel, for instance.)"
MachineLearning,3cbui2,ArgonJargon,1 point,Wed Jul 8 19:41:24 2015 UTC,ahh i see. then the preliminary application process would have been the right way to go. hopefully google uses this process in later submissions in this and similar fields. thank you for your well put together reply. i learned alot.
MachineLearning,3cbui2,fnielsen,1 point,Thu Jul 9 13:34:53 2015 UTC,"""I’m much more worried about things that are much more in the near term like, things like drones, if you make those smarter and they’re used by the military that’s not good.""  And Hinton also says he doesn't want Google to sell to the military his discoveries, maybe this is a move in the direction of excluding the military from the access to certain ideas.  I want to believe this, can be effectively a reason. I don't think this will be effective."
MachineLearning,3cbui2,braclayrab,1 point,Thu Jul 9 20:40:20 2015 UTC,"Is this a ""Google"" patent: The dropout patent is from 24. dec 2012, while DNNresearch was bought in 2013 AFAIK.  ...though Hinton was already working for Google in the summer of 2012 (as per https://plus.google.com/u/0/102889418997957626067/posts)"
MachineLearning,3cbui2,hhemken,1 point,Wed Jul 8 10:07:33 2015 UTC,Most of these seem to be from 2013...
MachineLearning,3cbui2,xyzwonk,1 point,Fri Jul 10 15:37:48 2015 UTC,"I hate to party-poop this orgy of hand-wringing, but I doubt these applications will go anywhere, at least in the US. Currently, trying to patent software inventions is dead in the water: https://en.wikipedia.org/wiki/Alice_Corp._v._CLS_Bank_International Litigating existing software patents will probably be much more unfavorable for the patent holders rather than for the litigators. It's a whole new ballgame."
MachineLearning,3cbui2,Nydhal,1 point,Fri Jul 10 19:14:41 2015 UTC,"Previous reddit comments have said ""don't hate the player, hate the game"", i.e., we should blame the US patent laws for allowing patents on well-known abstract ideas; that Google might as well exercise its legal rights.   We can't do anything about that. But we can stop using google and google services."
MachineLearning,3cbui2,xyzwonk,3,Mon Jul 13 02:41:35 2015 UTC,In a real democracy you should be able to influence laws.
MachineLearning,3cbui2,Nydhal,2,Mon Jul 6 23:42:49 2015 UTC,None exist then.
MachineLearning,3cbui2,CrossfitFTW,1 point,Tue Jul 7 05:18:28 2015 UTC,I don't know.
MachineLearning,3cbui2,tehgargoth,1 point,Tue Jul 7 06:05:48 2015 UTC,But we can stop using google and google services   Done and done.
MachineLearning,3cbui2,Guildenstern_artist,1 point,Tue Jul 7 07:26:26 2015 UTC,"So my question is, what company would you have rather had file those patents?"
MachineLearning,3cbui2,tehgargoth,3,Fri Jul 10 22:49:58 2015 UTC,"The idea of a patent is, itself, ridiculous."
MachineLearning,3cbui2,Guildenstern_artist,2,Tue Jul 7 12:50:48 2015 UTC,"yeah but google is less likely to go around trying to get $10,000 settlements out of small businesses who need to avoid lawsuits that would run it out of business."
MachineLearning,3chda0,oneAngrySonOfaBitch,1 point,Tue Jul 7 22:36:49 2015 UTC,http://jeremykun.com/2011/07/27/eigenfaces/  https://github.com/j2kun/eigenfaces  Maybe not the exact one you're looking for.
MachineLearning,3chda0,robertsdionne,2,Wed Jul 8 13:41:55 2015 UTC,found it https://vimeo.com/41401331  but the corresponding blog has been taken down.
MachineLearning,3ce8yj,BeatLeJuce,4,Tue Jul 7 05:54:40 2015 UTC,"Grid LSTM seem more general, but convolutional LSTM might be interesting to use for spatiotemporal sequences http://arxiv.org/abs/1506.04214"
MachineLearning,3ce8yj,marcuniq,2,Tue Jul 7 10:14:20 2015 UTC,Section 2.1 contains good and short explanation of a LSTM layer. They use data augmentation for MNIST tests.
MachineLearning,3ce8yj,NovaRom,2,Tue Jul 7 08:29:54 2015 UTC,"Yeah, I was very excited about the MNIST result before I saw the data augmentation. But then again I guess you have to for results like that. People should just let MNIST die out anyway....  The explanation of basic LSTMs was okay, but I felt that the grid could've been explained better. E.g. all their figures contain both green and red arrows, but they never mention the difference between the two.  Nevertheless, this looks like a very interesting and promising architecture.  Sidenote: I wonder why they didn't stick with the ""usual"" notation most everyone else uses when describing LSTMs (they used ""u"" instead of ""i"" for their input gate subscripts)."
MachineLearning,3ce8yj,siblbombs,1 point,Tue Jul 7 08:43:14 2015 UTC,"I appreciate that they included GFRNN for comparison during the character level task, some elements of n-lstm feel similar so it's nice to see a direct comparison listed instead of trying to figure it out."
MachineLearning,3cfzjn,kaj_sotala,2,Tue Jul 7 16:50:17 2015 UTC,"Brilliant, there's at least 10 startup ideas in this one post."
MachineLearning,3cfzjn,woodchuck64,1 point,Tue Jul 7 18:18:49 2015 UTC,/r/deepdream
MachineLearning,3ceu8i,jdsutton,5,Tue Jul 7 10:42:23 2015 UTC,"Yes, check out the UCI repository, which contains many datasets that are often used in ML."
MachineLearning,3ceu8i,a545a,2,Tue Jul 7 10:58:25 2015 UTC,"Awesome, thank you!"
MachineLearning,3ceu8i,changingourworld,1 point,Tue Jul 7 11:45:13 2015 UTC,Are there benchmarks for those datasets?
MachineLearning,3ceu8i,patrickSwayzeNU,2,Tue Jul 7 11:47:29 2015 UTC,Benchmarks for static data become useless very quickly.
MachineLearning,3ceu8i,changingourworld,1 point,Tue Jul 7 13:21:42 2015 UTC,"How come? When testing new algorithms, it'd be nice to compare the error rate to what others have gotten on the dataset"
MachineLearning,3ceu8i,j1395010,3,Tue Jul 7 13:31:25 2015 UTC,http://hunch.net/?p=22
MachineLearning,3ceu8i,changingourworld,1 point,Tue Jul 7 13:36:33 2015 UTC,Can't you avoid this problem by breaking the dataset into train/test and calculating your final error rate on the test set?
MachineLearning,3ceu8i,j1395010,2,Tue Jul 7 13:41:33 2015 UTC,no. look at the last two in particular.
MachineLearning,3ceu8i,patrickSwayzeNU,1 point,Tue Jul 7 13:54:26 2015 UTC,"Yes, it would be nice and that's the reason you see canonical benchmarks/datasets used.  Unfortunately, they don't tell you what we'd like for them to."
MachineLearning,3ceu8i,midnightketoker,3,Tue Jul 7 18:05:37 2015 UTC,Who shaves the barber?
MachineLearning,3ceu8i,midnightketoker,2,Tue Jul 7 16:18:10 2015 UTC,I don't need the dataset of ALL datasets ;D
MachineLearning,3ceu8i,chrico031,1 point,Tue Jul 7 16:47:01 2015 UTC,"Oh good, I bought the last one"
MachineLearning,3ceu8i,patrickSwayzeNU,2,Tue Jul 7 16:50:16 2015 UTC,/r/OpenData and /r/datasets have some good resources.
MachineLearning,3ceu8i,booketor,1 point,Tue Jul 7 15:43:21 2015 UTC,So meta
MachineLearning,3cfblo,cybrbeast,2,Tue Jul 7 13:50:20 2015 UTC,"Wow, this is pretty impressive. It would not surprise me at all if the google VR guys had some involvement in this, adding a walk around mode to google street view would be a killer VR feature."
MachineLearning,3cfblo,siblbombs,2,Tue Jul 7 14:12:28 2015 UTC,"I wonder if it is possible to generate a holistic 3d model of the city this way - it would be a one-time computational investment and could then be used for endlessly smooth exploration.   They could then use each photo from Google Photo to enhance/improve their model, so we could have a GTA Version of the planet."
MachineLearning,3cfblo,TenshiS,1 point,Tue Jul 7 15:01:08 2015 UTC,"The included video is amazing, this would allow smooth travel through Google Streetview, though most likely at huge computational costs."
MachineLearning,3caojo,NovaRom,9,Mon Jul 6 12:42:22 2015 UTC,So Google becomes the Microsoft of the early 2000's and completes the cycle...  If they'd come out and state in a legally absolute way that their patent portfolio was only useable defensively when and only if they themselves were sued for patent then it would be ok.
MachineLearning,3caojo,sifnt,8,Mon Jul 6 17:18:25 2015 UTC,Pecunia non olet
MachineLearning,3caojo,BadGoyWithAGun,7,Mon Jul 6 14:42:55 2015 UTC,"Don't hate the player, hate the game.  We can only hope this latest round of patent fuckery drives more ML researchers to countries that don't recognise or grant software patents."
MachineLearning,3caojo,fhadley,2,Mon Jul 6 14:20:40 2015 UTC,or to burn the fucking state to the fucking ground. woodchippers are relevant here
MachineLearning,3caojo,p_np,4,Tue Jul 7 07:07:06 2015 UTC,"I'm pretty sure Google has a ""don't sue us, we won't sue you"" mentality. I would rather have them have the patent than some other company that would sue over every little thing."
MachineLearning,3caojo,r4and0muser9482,22,Mon Jul 6 15:04:43 2015 UTC,"That may be true if we're talking about the big players, but what guarantee do small startups have they won't be eaten alive by companies like Google?   Also, why is everyone trying so hard to defend Google here? Not that this is anything new in this industry. Anyone's heard of IBM lately?"
MachineLearning,3caojo,raverbashing,2,Mon Jul 6 15:53:27 2015 UTC,but what guarantee do small startups have they won't be eaten alive by companies like Google?   None.   With or without patents
MachineLearning,3caojo,r4and0muser9482,1 point,Mon Jul 6 19:04:45 2015 UTC,How would they be able to sue a startup into bankrupcy without patents? Please explain...
MachineLearning,3caojo,raverbashing,3,Mon Jul 6 20:26:39 2015 UTC,"They don't need to sue anybody to shut them down   Buy the startup (or their competitors) Offer a similar service, in unfavourable terms Poach their talents Sign an exclusive contract for their services   The easiest way to not be picked on by a big company is unfortunately, to be irrelevant"
MachineLearning,3caojo,r4and0muser9482,1 point,Mon Jul 6 21:35:20 2015 UTC,Being picked up is not what I'm talking about. I'm talking about being bankrupted by legal fees. Why would being picked up be a bad thing (for the startup)?
MachineLearning,3caojo,Oxonium,12,Tue Jul 7 05:52:17 2015 UTC,"Let's not delude ourselves here; Google is a massive company and aims primarily to do  what all massive companies aim to do, which is make as much profit as possible.  At the moment it behooves them to play to their 'don't be evil' image, but as soon as they decide they can make more money by being a little bit more evil than they used to be (for example, dodging taxes in Europe), there's little reason why they won't."
MachineLearning,3caojo,syllogism_,3,Mon Jul 6 17:04:21 2015 UTC,"That's cold comfort if you're a start-up trying to get funded. Or trying to sell software to a large enterprise who needs you to take out IP liability insurance, which you won't be able to get."
MachineLearning,3caojo,bloodmoonack,6,Mon Jul 6 19:21:24 2015 UTC,...for now...
MachineLearning,3caojo,blickblocks,1 point,Mon Jul 6 16:57:43 2015 UTC,"I know this was true during the first several years of Android. There were a lot of things they chose not to patent until Apple kept suing every vendor over things that pretty much only had to do with Android, which is Google's baby.  I don't really sympathize with Google, but I do hate how most other corporations are making the field completely uncompetitive."
MachineLearning,3caojo,xcombelle,1 point,Mon Jul 6 17:15:12 2015 UTC,I really don't get what is the fundamental idea. CNN are poorly connectected so we can compute each part in a different node ?
MachineLearning,3caojo,XeonPhitanium,1 point,Mon Jul 6 17:13:47 2015 UTC,"This patent is already obsolete.  Krizhevsky's first GPUs were GTX 580s.  They were awesome GPUs no doubt, but they are 2.5 generations behind the curve (1.6 TFLOPS versus 6.7 TFLOPS) so more than 4x slower than a $1000 TitanX with only 3 GB of memory and they existed at a time when NVIDIA had yet to enable P2P copies between multiple processes.  I'd guess that this workaround was abandoned as of the ""One Weird Trick"" paper last year which I'm sure has been patented as well by now."
MachineLearning,3caojo,xcombelle,1 point,Mon Jul 6 20:43:19 2015 UTC,I don't see how more powerful GPU can obsolete the fundamental idea
MachineLearning,3caojo,XeonPhitanium,1 point,Mon Jul 6 23:13:29 2015 UTC,"For the same reason that NVIDIA enabling 13 GB/s P2P direct copies between PCIE Gen 3 GPUs obsoleted staged multi-part copies through the CPU to another GPU.  That is unless you were cheap and bought a motherboard where P2P copies are blocked by QPI (ergo by Intel because they are jerks), at which point, you probably have a lot of other systemic issues to make a poor decision like that."
MachineLearning,3caojo,xcombelle,1 point,Tue Jul 7 06:00:20 2015 UTC,I mean I don't see how the idea is original
MachineLearning,3caojo,funnypatent,1 point,Tue Jul 7 16:55:58 2015 UTC,"I hope Google's patent application “system and method for parallelizing convolutional neural networks” https://www.google.com/patents/WO2014105865A1 won't be approved due to prior art.   The earliest relevant paper (16 July 2012) cited by the patent application is this:  D. CIRESAN, U. MEIER, J. SCHIDHUBER: ""Multi-column deep neural networks for image classification"", PROCEEDINGS OF THE 2012 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR'12), 16 June 2012 (2012-06-16), pages 3642-3649, XP032232509, DOI: 10.1109/CVPR.2012.6248110  The patent examiners will probably find that there are even earlier publications on parallelizing convolutional neural networks by the same team, starting with this IJCAI paper of 2011:  D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, J. Schmidhuber. Flexible, High Performance Convolutional Neural Networks for Image Classification. International Joint Conference on Artificial Intelligence (IJCAI-2011, Barcelona), 2011. http://people.idsia.ch/~juergen/ijcai2011.pdf  Under http://people.idsia.ch/~juergen/deeplearning.html Schmidhuber writes that this decribes their ""special breed of GPU-based max-pooling convolutional networks (GPU-MPCNNs), now widely tested/used by research labs (e.g., Univ. Toronto/Google/Stanford) and companies (e.g., Apple) all over the world.""   These parallelized convolutional pooling neural networks also were used to win several competitions summarised under http://people.idsia.ch/~juergen/deeplearning.html   I cannot really see what’s different in the later publication by Alexander KRIZHEVSKY, Ilya SUTSKEVER, Geoffrey E. HINTON (2012). Or maybe someone can point out what’s different?"
MachineLearning,3caojo,csmining,0,Tue Jul 7 06:00:58 2015 UTC,"Google/Facebook/Linkedin/Youtube ... a lot of big or small creative companies (or armies) of the United States have served most countries in the world except several countries which are so-called internet-blocked. I think it is no doubt that these huge companies have absolute dominance in technologies. They have a lot of users around the world, they have employed a lot of talents or most brilliant or hard-working PhD or experts, they have huge amount of big data of their users. Of course, they should have a lot of patents. They serve the world, change the world and make money from the world."
MachineLearning,3caojo,marijnfs,-4,Tue Jul 7 09:04:28 2015 UTC,The amount of novelty and innovation of these guys is blowing my mind! http://i.imgur.com/sg017lt.gif
MachineLearning,3cax7t,zionsrogue,10,Mon Jul 6 14:07:52 2015 UTC,Fantastic name for a library.
MachineLearning,3cax7t,Notlambda,5,Mon Jul 6 14:37:36 2015 UTC,Thanks! I was looking at my bookcase for inspiration when I went to name the library. I knew it had to be something trippy/psychedelic. When I landed on Fear and Loathing in Las Vegas I knew instantly -- it had to be bat-country.
MachineLearning,3cax7t,GibbsSamplePlatter,2,Mon Jul 6 16:31:41 2015 UTC,Bat-CouNNtry
MachineLearning,3cax7t,__Zox__,3,Mon Jul 6 19:06:11 2015 UTC,super easy to use...if caffe weren't such an enormous bitch to install
MachineLearning,3cax7t,shaggorama,2,Mon Jul 6 21:26:24 2015 UTC,"Have you tried using a docker instance with Caffe pre-installed? You'll only have access to the CPU (no GPU mode in docker), but you'll be able to get up and running quickly."
MachineLearning,3cax7t,agent229,1 point,Mon Jul 6 21:54:05 2015 UTC,How much slower are we talking for this particular application? Minutes? Hours? Days?
MachineLearning,3cax7t,XalosXandrez,1 point,Tue Jul 7 03:35:38 2015 UTC,"Substantially slower. In the order of minutes or hours depending on how large your input images are. But if you input a 300x300px image and use one of the lower layers in the network, you can process an image every 10-20 seconds."
MachineLearning,3cax7t,potent_rodent,1 point,Tue Jul 7 10:26:06 2015 UTC,"I tried once, failed. Haven't tried again, but really want to get it working"
MachineLearning,3cdxf3,mbriner,2,Tue Jul 7 03:58:00 2015 UTC,"messy and poorly conceived Python code   Fixing this should be your first priority. The process of cleaning up your code can often make bugs magically disappear. Clean code is also much easier to debug, for both yourself and others."
MachineLearning,3cdxf3,jdsutton,1 point,Tue Jul 7 13:02:22 2015 UTC,"Do you have any suggestions for what to clean up? I have no problem going back through it to make it better, but I'm not always sure what is poor practice and what isn't."
MachineLearning,3cdxf3,jdsutton,1 point,Tue Jul 7 15:02:56 2015 UTC,"IMO, you have way too many comments. Each individual line should be readable enough that it is self-explanatory. Comments should be used to explain small blocks of code.  Comments can also reassure you that a line does what you think it does, when really it is doing something else. I would add unit tests for all your functions to make sure they are really behaving the way you think they should behave.  Another thing: You have a lot of ""magic numbers"" in your code.  Also, if you haven't read it already, this is a really good resource for reinforcement learning: http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html"
MachineLearning,3cdxf3,jdsutton,1 point,Tue Jul 7 15:17:44 2015 UTC,"Gotcha, I'll get to work on that. I usually never comment, so I actually only added them because I was posting here. Thanks for the link, I'll check it out."
MachineLearning,3cdxf3,ChrisKennedy,1 point,Tue Jul 7 15:27:23 2015 UTC,"No comments can be just as bad, so keep an eye on that as well."
MachineLearning,3cds4a,okayshokay,1 point,Tue Jul 7 03:09:35 2015 UTC,We can judge the quality of a generative model based on its probability of producing realistic images.  We can estimate its ability to produce real images in general by calculating its probability of producing the set of images in the test set.  Does that answer your question?
MachineLearning,3cds4a,DavidJayHarris,1 point,Tue Jul 7 04:12:50 2015 UTC,"It sort of puts some light on my confusion. But my question now is: While training, when we input the training data, do we care about the labels assigned to the training data? If yes, then what is the use of giving the labels?"
MachineLearning,3cds4a,gsmafra,1 point,Tue Jul 7 13:16:01 2015 UTC,No
MachineLearning,3cds4a,simonhughes22,1 point,Tue Jul 7 14:21:39 2015 UTC,"It's trained in the same way as an auto-encoder - the outputs are the inputs, and it is essentially learning to compress them."
MachineLearning,3cgog6,FuschiaKnight,6,Tue Jul 7 19:41:21 2015 UTC,Is this like... a joke?
MachineLearning,3cgog6,Articulated-rage,2,Tue Jul 7 19:43:57 2015 UTC,Sensationalism.
MachineLearning,3cgog6,USER_PVT_DONT_READ,2,Tue Jul 7 22:31:36 2015 UTC,this is inaccurate on so many levels
MachineLearning,3cgog6,xamdam,1 point,Tue Jul 7 23:46:50 2015 UTC,Here is the paper http://arxiv.org/abs/1506.07285
MachineLearning,3ccdq0,budhdub,0,Mon Jul 6 20:32:34 2015 UTC,title is kinda misleading. seems similar to batch normalization.
MachineLearning,3ccdq0,sdsfs23fs,2,Tue Jul 7 01:36:04 2015 UTC,"this is actually nothing to do with batch normalization. the paper is fundamentally completely different. a classifier is proposed where each class is a point on a sphere whose radius is kept constant.  If you want a higher-level understanding, it replaces Softmax + negative Log Likelihood loss with a new scale-invariant classifier + euclidean distance loss."
MachineLearning,3c8m69,j1395010,76,Sun Jul 5 22:39:52 2015 UTC,"Playing devil's advocate (I know patenting this is ridiculous), hasn't Google historically only used patents for defensive purposes? In other words, they're patenting stuff that shouldn't be patentable so some troll doesn't come along and sue them for using basic machine learning techniques.  Edit: I'd just like to add that in a sense, this could be a good thing in that Google is protecting everyone from potentially being sued by a patent troll regarding the subject of this patent."
MachineLearning,3c8m69,bushrod,8,Mon Jul 6 00:24:44 2015 UTC,"The road to hell is paved with good intentions. Now, everybody will join this race. People will start to apply for a patent first (backed by large companies), then publish their papers. There is also no guarantee against that Google will change its ""good"" behaviour when facing ever increasing competition and decline of profits."
MachineLearning,3c8m69,NovaRom,28,Mon Jul 6 13:41:27 2015 UTC,"I believe your first point is exactly correct. The phrase ""Patent Troll"" is something I am hearing more and more. There are many groups of people out there (professionals in law, not just some punks) that specifically scan through the largely un-pantented tech companies products, internal systems, etc. and rush a vague patent through the system without the slightest idea of what it is. Then they sue, and get a bunch of money. Google is just protecting themselves.    I think you missed one important other reason why they might want to patent this kind of thing. Most big tech companies do their best to patent everything they can. Why not, right? But since software is (in my opinion) kind of in a gray area when it comes to where it lies on the product/infrastructure spectrum, inevitably thing that all companies do get patented by 1 company. This leaves all the other companies at risk of a lawsuit. This has resulted in ALL large companies scrambling to get as many patents as they can (even if they don't use the thing the patent is about) as a different form of protection.  In other words, it becomes a ""If you sue us for all the little patents you have against us, we will sue you for all the little patents we have you."" And patents on the newer technologies are so screwed up, nobody really knows who could or couldn't walk away from an all out legal attack like that.   Its interesting stuff, but it seems to get in the way of progress. Its a real shame if you ask me, but I guess that is just how business needs to be done these days.   On the bright side. I am 99.99% sure that no student studying M.L. will ever get a letter from Google, telling them that their final paper infringed on their patent. That just isn't what this is for."
MachineLearning,3c8m69,edster3194,10,Mon Jul 6 01:59:56 2015 UTC,"Latent semantic analysis is patented as well and, as far as I know, never posed a problem for anyone using the technology... including Google."
MachineLearning,3c8m69,guesswho135,2,Mon Jul 6 03:23:16 2015 UTC,SVMs as well
MachineLearning,3c8m69,BeatLeJuce,2,Mon Jul 6 15:07:46 2015 UTC,RVMs
MachineLearning,3c8m69,farsass,3,Mon Jul 6 18:04:10 2015 UTC,"But someone's gonna pay fees to a rotten agency for nothing, just a waste of time and money. In CV SIFT was patented. Then ppl ended up shifting to HoG."
MachineLearning,3c8m69,grrrgrrr,22,Mon Jul 6 07:18:52 2015 UTC,"Playing devil's advocate (I know patenting this is ridiculous), hasn't Google historically only used patents for defensive purposes? In other words, they're patenting stuff that shouldn't be patentable so some troll doesn't come along and sue them for using basic machine learning techniques.   If you want to prevent someone from patenting something you don't need to get a patent yourself, just publish it so that there is clear prior art.  The only reason to patent something is to prevent others from using the same idea freely."
MachineLearning,3c8m69,sanity,37,Mon Jul 6 02:35:52 2015 UTC,"Bullshit patents still get awarded left and right, and it is typically significantly costlier to go to court to invalidate a patent than it is to just file a deserving patent in the first place."
MachineLearning,3c8m69,dwf,19,Mon Jul 6 02:46:37 2015 UTC,"Exactly.  If the USPTO did their job properly and respected prior art, patenting stuff like this would be unnecessary (and of course impossible)."
MachineLearning,3c8m69,bushrod,6,Mon Jul 6 02:52:27 2015 UTC,I pity the patent office. Doing their job properly is basically impossible because too much is patentable and the nuances of the subject matter requires a certain mastery of the subjects involved in the patent.
MachineLearning,3c8m69,SnOrfys,4,Mon Jul 6 06:04:42 2015 UTC,This right here is the central reason why the patent system has become unworkably flawed. The PO's job is basically logically impossible so they have to make best-guesses.   (On top of that they get paid very little to process patents potentially worth millions. Like a slap in the face.)
MachineLearning,3c8m69,NikoMyshkin,2,Mon Jul 6 12:29:46 2015 UTC,"They should tie up with academia and institute a peer-review system, same as journals."
MachineLearning,3c8m69,ohell,3,Mon Jul 6 12:24:14 2015 UTC,That's not a solution because journals have the same problem.
MachineLearning,3c8m69,SnOrfys,2,Mon Jul 6 14:16:56 2015 UTC,"Ideally, but not always in practice. It's very common for patent trolls to say ""obviously you're going to win, our arguments are laughable, but you can spend $10M and three years proving that in court or we can settle next week for $5M"". Holding the patent, not just the prior art, makes it much more difficult for the bullshit to even start."
MachineLearning,3c8m69,dqkhAgDiMz,8,Mon Jul 6 08:15:00 2015 UTC,The fact that Google might succeed in being granted this patent (indeed that fact that they're even trying) completely invalidates your point.
MachineLearning,3c8m69,bushrod,12,Mon Jul 6 02:54:38 2015 UTC,"It's entertaining to watch arm chair patent experts with their imaginary legal degrees tell us about how easy it is to strike down a bad patent, ""you just need prior art lol""; apparently lawyers grow on trees and work for free."
MachineLearning,3c8m69,dwf,-1,Mon Jul 6 03:58:18 2015 UTC,"""you just need prior art lol"";   Actually, that is all that it takes to shoot down a patent. You certainly don't need to be a lawyer to know that."
MachineLearning,3c8m69,DarkSayed,2,Mon Jul 6 12:59:59 2015 UTC,I think you missed the point. In practice it's prohibitively difficult because you need to pay lawyers and it takes a lot of resources of time and money to win those cases. It's not just any reddit aethist genius going to court and stunning them with their research on prior art.
MachineLearning,3c8m69,classicalhumanbeing,-1,Mon Jul 6 14:37:30 2015 UTC,"Big multinational companies have no trouble affording lawyers, atheist genius."
MachineLearning,3c8m69,DarkSayed,5,Mon Jul 6 15:39:51 2015 UTC,Big multinationals don't get to be big multinationals by doing the less cost effective thing (spending money squashing patents on stuff you invented) rather than the more cost effective thing (filing the patents yourself).
MachineLearning,3c8m69,dwf,-1,Mon Jul 6 17:05:36 2015 UTC,"They actually do both. What big multinationals do as big multinationals is not necessarily the same as what they did to become big multinationals.   Now back to the point: If google wanted to prevent something being patented, they could publish it as comprehensively as a patent but not file a patent. Anyone wanting to file for a patent would then face the prospect of not just one, but many companies challenging it as prior art. There is no shortage of money to do that if it is a useful invention, and it would not stand a chance in court."
MachineLearning,3c8m69,DarkSayed,1 point,Mon Jul 6 18:07:05 2015 UTC,"It is never that simple. Having it is one thing, demonstrating you have it is quite another, that involves quite a few billable hours for your lawyers."
MachineLearning,3c8m69,dwf,2,Mon Jul 6 13:29:31 2015 UTC,"If you want to prevent someone from patenting something you don't need to get a patent yourself, just publish it    Even if you use publicly available knowledge, there is no guarantee you will not be sued for it. And the scrutiny when patents are being awarded is lacking, so there's no guarantee patents would not be awarded despite 'prior art'."
MachineLearning,3c8m69,TenshiS,3,Tue Jul 7 15:10:44 2015 UTC,"You don't get a patent on something just to prevent someone else from getting it. You get a patent so that when someone (Oracle, Microsoft, Apple, etc) sues you for patent infringement, you can sue them right back for infringing your patents too, and force them to settle out of court with a cross licensing agreement."
MachineLearning,3c8m69,modeless,1 point,Mon Jul 6 07:10:54 2015 UTC,"Unfortunately, then someone can patent a trivial addition to your idea, preventing you from making that change to your own work (or costing you a fortune to convince twelve inbreds in East Texas that it was ""obvious""). If you have a patent, their change is useless to them without a license to your original patent, so the two armies of lawyers call a truce and write up a reciprocal agreement."
MachineLearning,3c8m69,wordsnerd,3,Mon Jul 6 07:55:59 2015 UTC,"You don't really believe this, do you? Google is not your friend, despite the common aphorism."
MachineLearning,3c8m69,DarkSayed,5,Mon Jul 6 12:57:44 2015 UTC,"Edit: I'd just like to add that in a sense, this could be a good thing in that Google is protecting everyone from potentially being sued by a patent troll regarding the subject of this patent.   I hope you are on google s payroll and not doing this for free. Google is a publicly traded company hence there are no guarantees about what they will do a few financial quarters from now.  What do people expect to happen if Google's stock takes a dip and shareholders start demanding more aggressive monetization of their patents?"
MachineLearning,3c8m69,maxToTheJ,3,Mon Jul 6 07:38:15 2015 UTC,"or they get sold.  Sun was a ""good"" company but Oracle is now burning and looting the tech industry with their patents."
MachineLearning,3c8m69,Malician,1 point,Mon Jul 6 08:09:36 2015 UTC,"If there is a race to patent machine learning techniques, then yes I would rather have them owned by Google, which has a strong track record of not using its patents offensively, than patent trolls or litigious big-name companies.   Google is a publicly traded company hence there are no guarantees about what they will do a few financial quarters from now.   Of course -- there are no guarantees of anything.  My view reflects the popular opinion that the patent system is broken, so the best case is to have any obvious patents in the hands of companies that have historically used them responsibly.  And no, I'm not on Google's payroll; just a person with an opinion."
MachineLearning,3c8m69,bushrod,1 point,Mon Jul 6 18:20:50 2015 UTC,"y view reflects the popular opinion that the patent system is broken, so the best case is to have any obvious patents in the hands of companies that have historically used them responsibly.   People have given historical examples (sun) that this belief doesn't work since companies and owners of patents change"
MachineLearning,3c8m69,maxToTheJ,1 point,Mon Jul 6 20:25:19 2015 UTC,"I understand, and you are correct that companies change, but that doesn't address my point, which is this:  You know that machine learning patents such as these ultimately will get granted to someone if they the USPTO determines them to be patentable.  So what alternative to you see as preferable to Google owning a chunk of them?"
MachineLearning,3c8m69,bushrod,1 point,Mon Jul 6 21:25:08 2015 UTC,The problem is obviously that patents exist.
MachineLearning,3c8m69,Snjolfur,1 point,Mon Jul 6 22:55:13 2015 UTC,"You are right. I have heard Amazon had patented photos taken in white background, Probably to avoid patent trolls but funny nevertheless."
MachineLearning,3c8m69,xplot,7,Mon Jul 6 04:08:18 2015 UTC,Is this drop out or a variant thereof?
MachineLearning,3c8m69,wilgamesh,6,Mon Jul 6 01:45:55 2015 UTC,looks like dropout.
MachineLearning,3c8m69,devDorito,4,Mon Jul 6 03:28:55 2015 UTC,"Yes, this is exactly DropOut method. The application contains text cuts from the original paper (Srivasta's Master Thesis)."
MachineLearning,3c8m69,NovaRom,16,Mon Jul 6 15:26:42 2015 UTC,The recent comment by Jürgen Schmidhuber looks allot different in this light...
MachineLearning,3c8m69,samim23,8,Mon Jul 6 04:55:34 2015 UTC,Which particular comment?
MachineLearning,3c8m69,CodeReclaimers,3,Mon Jul 6 11:43:47 2015 UTC,https://plus.google.com/100849856540000067209/posts/9BDtGwCDL7D
MachineLearning,3c8m69,simonhughes22,4,Mon Jul 6 17:20:25 2015 UTC,"Wow, that's brutal. Even if that comment only tells half the truth, it's a huge black mark on Hinton and his group."
MachineLearning,3c8m69,vnnc,-1,Mon Jul 6 18:11:38 2015 UTC,http://hyperboleandahalf.blogspot.com/2010/04/alot-is-better-than-you-at-everything.html
MachineLearning,3c8m69,sirseal,24,Mon Jul 6 17:42:47 2015 UTC,"Hate the game, not the player.  If I were Google's CEO I'd file patents for every line of code in production after the stunt Oracle pulled with Java API."
MachineLearning,3c8m69,Inori,-1,Mon Jul 6 07:49:54 2015 UTC,yeah and all the stunt's the open source community pulled on Microsoft amrite?  Bottom line is don't fuck with the GPL :)
MachineLearning,3c8m69,tehgargoth,4,Mon Jul 6 16:29:16 2015 UTC,Isn't there an insane amount of prior art already? It seems like this patent is indefensible.
MachineLearning,3c8m69,shaggorama,12,Mon Jul 6 13:56:56 2015 UTC,"as someone who just finished their undergrad and doesn't have too much industry experience, could someone explain how it is possible to patent what seems to be more of an algorithmic design than a software product? like could someone patent least squares if they wanted to?"
MachineLearning,3c8m69,organic-neural-net,14,Mon Jul 6 01:08:57 2015 UTC,"Most software patents are just algorithms.  Some include design elements, but even those generally include an algorithm/process."
MachineLearning,3c8m69,bluecamel17,6,Mon Jul 6 02:51:04 2015 UTC,"Because software patents, in general, are bullshit. And the system has been abused countless times to patent the most inane software ""inventions"". Patenting a software technique, like this patent is doing, isn't very far from patenting a mathematical formula.  In fact for a long time, software patents had no validity at all in the EU, although from what I hear the situation is more complicated at the moment (court cases have been judged in different directions in different countries.) They are still much harder to get and enforce in Europe than in the US though, and with good reason."
MachineLearning,3c8m69,nkorslund,3,Mon Jul 6 08:27:48 2015 UTC,I think it's weird that people draw such a distinction between patenting a technique involving a particular application and arrangement of highly processed ore-bearing rocks and patenting a technique involving a particular application of highly refined sand.
MachineLearning,3c8m69,thfuran,1 point,Mon Jul 6 12:38:34 2015 UTC,"When you invent a new way to solve a problem, you can patent it (in the US) to ensure that you either have exclusive for a couple of years or everyone who uses your idea has to pay you a fee. Patents are about ideas, not products. A software product isn't patented, though copyright will apply."
MachineLearning,3c8m69,skgoa,0,Mon Jul 6 07:28:30 2015 UTC,I believe this has something to do with the Oracle v Google case being turned down for a hearing by the SCOTUS.
MachineLearning,3c8m69,shortnamed,3,Mon Jul 6 06:40:18 2015 UTC,Sigh. So dies the optimism that we were riding...  Now would be an excellent time for the big players to come out and demonstrate irreversibly why these patents can only ever be used defensively against companies suing them for patent infringement... otherwise their silence will be answer enough.
MachineLearning,3c8m69,sifnt,2,Mon Jul 6 17:23:31 2015 UTC,"Ok, dropout has been patented. Why not patent L2 and L1 regularisation as well?"
MachineLearning,3c8m69,GreenHamster1975,4,Mon Jul 6 08:53:39 2015 UTC,Why stop there? Patent human neural networks and sue every human on earth!
MachineLearning,3c8m69,quirm,1 point,Mon Jul 6 16:00:32 2015 UTC,Let's patent sparsity to boot!
MachineLearning,3c8m69,marijnfs,6,Mon Jul 6 10:54:22 2015 UTC,And let's sue god afterwards!
MachineLearning,3c8m69,sieisteinmodel,2,Mon Jul 6 13:28:55 2015 UTC,"hi~ I have a question: when I used the patented technique, how could they know I use it in my applications? And is there any way to get rid of the patent fee when they know I used it? I heard that many big companies have had a lot of patents and made money from the patent fees. Is it right that there is no need to afford the patent fee if I could modify or improve the algorithm which had been patented?"
MachineLearning,3c8m69,csmining,5,Mon Jul 6 09:53:56 2015 UTC,"if they have good reason to suspect you're using it (like you're making state of the art CNNs), and you're making enough money to bother with, then they file a lawsuit for patent infringement.   During discovery they then get to subpoena your email, code, etc. as well as interrogate you and your employees to see if you're using dropout."
MachineLearning,3c8m69,sdsfs23fs,6,Mon Jul 6 11:06:50 2015 UTC,Seems like a loophole way to expose all the juicy trade secrets.
MachineLearning,3c8m69,log_2,1 point,Mon Jul 6 12:53:16 2015 UTC,Thank you for replying!
MachineLearning,3c8m69,csmining,2,Mon Jul 6 13:06:20 2015 UTC,"At the risk of beating a dead horse, I hope that at least Google's patent application “system and method for parallelizing convolutional neural networks” https://www.google.com/patents/WO2014105865A1 cannot be approved due to non-patented prior art.   The earliest relevant paper (16 July 2012) cited by the patent application is this:  D. CIRESAN, U. MEIER, J. SCHIDHUBER: ""Multi-column deep neural networks for image classification"", PROCEEDINGS OF THE 2012 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR'12), 16 June 2012 (2012-06-16), pages 3642-3649, XP032232509, DOI: 10.1109/CVPR.2012.6248110  But the patent examiners will probably find that there are even earlier very detailed non-patented publications on parallelizing convolutional neural networks by the same group, starting with this IJCAI paper of 2011:  D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, J. Schmidhuber. Flexible, High Performance Convolutional Neural Networks for Image Classification. International Joint Conference on Artificial Intelligence (IJCAI-2011, Barcelona), 2011. http://people.idsia.ch/~juergen/ijcai2011.pdf  Under http://people.idsia.ch/~juergen/deeplearning.html Schmidhuber writes that this decribes their ""special breed of GPU-based max-pooling convolutional networks (GPU-MPCNNs), now widely tested/used by research labs ... all over the world.""   Since 2011, ensembles of these parallelized convolutional neural networks also were used to win high-profile competitions summarised under http://people.idsia.ch/~juergen/deeplearning.html   I cannot really see what’s different in the later publication by Alexander KRIZHEVSKY, Ilya SUTSKEVER, Geoffrey E. HINTON (2012). Or is there somebody out there who can point out a difference?  The other patent applications may have similar issues."
MachineLearning,3c8m69,funnypatent,2,Tue Jul 7 09:27:15 2015 UTC,"You are aware that all of those patents are applications, not granted, right?"
MachineLearning,3c8m69,siblbombs,32,Sun Jul 5 23:17:58 2015 UTC,Like there's a difference with the USPTO
MachineLearning,3c8m69,xyzwonk,2,Sun Jul 5 23:38:17 2015 UTC,"Im curious, roughly what percentage of software patent applications do you think the US patent office accepts?  I don't know much about them."
MachineLearning,3c8m69,the_peanut_gallery,8,Mon Jul 6 01:52:24 2015 UTC,"Even if you know the percentage it's useless because patent lawyers learn what gets accepted, which raises the percentage. If patent lawyers all do their jobs they could conceivably near 100%.   Data in this case, were it available, would be useless.  But here you go: http://arstechnica.com/tech-policy/2013/04/study-suggests-patent-office-lowered-standards-to-cope-with-backlog/  They're all pretty high."
MachineLearning,3c8m69,xyzwonk,6,Mon Jul 6 01:56:46 2015 UTC,they should still be ashamed. and when have you last heard of a software patent being denied?
MachineLearning,3c8m69,sdsfs23fs,11,Sun Jul 5 23:47:24 2015 UTC,"Do you know why they're patenting them? Because patent trolls have been patenting things like this and then suing them. If someone is going to patent it, I'd rather have Google patent it than some psychpath patent troll. The US government (my government) has shown very little progress in the way of stopping patent trolls. Imagine you're running a company, you don't patent stuff like this just in case to prevent someone using it against you?"
MachineLearning,3c8m69,kidpost,-2,Mon Jul 6 00:31:26 2015 UTC,You don't have to patent something to prevent someone else from patenting it - just publish the idea to show clear prior art.
MachineLearning,3c8m69,sanity,3,Mon Jul 6 02:36:41 2015 UTC,"You have a much stronger legal leg to stand on if you have a patent for what you're defending. You have to remember that anyone can sue you at any time for anything, and that can be costly. What's the easiest way you can think of to prevent some patent troll from just randomly suing you? Having a patent. That way the patent troll won't be able to bring up any evidence of their prior art or whatever bullshit legal tricks they decide to pull."
MachineLearning,3c8m69,kidpost,2,Mon Jul 6 09:02:01 2015 UTC,"You SHOULDN'T, yes. But Samsung didn't buy a hockey rink for the city in Texas where 1/4 of all American patent trials are held for no reason. They did it because here people read things on the internet, patent massively broad definitions of them, and then attempt to sue software companies for ""abusing their work,"" and ask for a settlement fee that's lower than the cost of going to court to leave. Instead, Google fights fire with fire."
MachineLearning,3c8m69,xakh,0,Mon Jul 6 08:37:23 2015 UTC,Samsung is a major employer in Texas and stands to benefit from brand marketing with US consumers. It seems unlikely that Samsung intends to corrupt the court system by building a sporting arena.
MachineLearning,3c8m69,cbann,0,Mon Jul 6 16:25:28 2015 UTC,"I'm not going to explain the entirety of patent trolls and defensive patents to you, so I'm going to leave this here."
MachineLearning,3c8m69,xakh,0,Mon Jul 6 17:15:10 2015 UTC,There's no need to be patronizing; the only explanation I'm looking for is how you made the leap of logic from skating rink to judicial tampering when there are such obvious alternative motivations.
MachineLearning,3c8m69,cbann,1 point,Mon Jul 6 18:46:53 2015 UTC,Watch the video then.
MachineLearning,3c8m69,xakh,2,Mon Jul 6 18:56:37 2015 UTC,"I don't see the problem here.  The patent belonged to the authors long before Google registered a copy of it.   What do you think universities make their grad students and professors sign upon enrollment/employment? Permission to make derivative works/publications/bindings using their published papers and copy of all IP while a part of the university. It's usually for the purpose of re-publishing and archiving (i.e. future grad students don't get sued for their research building on prior work).   The papers on deep learning were published and made publicly-available before the patent was registered. This registered patent is meaningless, except that the authors are now assets to any new company that retains/hires them.   You want to avoid a lawsuit against your small company? Buy a copy of the relevant papers from their respective publishers and show it to the judge or instigating lawyer. That's what R&D departments do to keep themselves blame free. That and patents for their own works."
MachineLearning,3c8m69,hugrate,9,Mon Jul 6 02:42:36 2015 UTC,"sadly you're wrong. you have 1 year from the date of public disclosure to file a patent.   this patent is bullshit, but not for that reason."
MachineLearning,3c8m69,sdsfs23fs,4,Mon Jul 6 03:17:15 2015 UTC,"The core claims of the patent application were described in a paper from July 2012, and the application wasn't filed until December 2013."
MachineLearning,3c8m69,teraflop,7,Mon Jul 6 03:48:45 2015 UTC,"Yep. Google employs such idiot patent attorneys that they go to the trouble of filing meaningless patents. Sounds legit.  And as for universities owning the IP... well:   “This is a wonderful opportunity for Geoff, and a great opportunity for the department,” said University of Toronto computer science chair Sven Dickinson in a statement. “In recent years, we have been expanding our industrial relations, and this acquisition represents a wonderful opportunity to strengthen our existing ties with Google, one of the world’s most innovative IT companies.”   Yes, they got a cut, and Google got IP transfer."
MachineLearning,3c8m69,dwf,1 point,Mon Jul 6 04:03:25 2015 UTC,"Sure, but this says nothing about ""rescaling"" them to an arbitrary value that improves performance overall, now does it?  Cue someone submitting exactly this patent."
MachineLearning,3c8m69,XeonPhitanium,1 point,Mon Jul 6 17:55:50 2015 UTC,such eloquence.
MachineLearning,3c8m69,Hougaiidesu,1 point,Mon Jul 6 19:59:28 2015 UTC,"It's happening... The Republic of Google... First, I hear that Google grants MURI to academics with a budget to rival DARPA, then this patent business which used to be the bread-and-butter of government researchers. The reason that a patent is worth more than a paper for government researchers is that (1) it's supposed to encourage businesses who will license the patent to set up shop and (2) to prevent patent trolling in cases of national interest."
MachineLearning,3c8m69,melipone,0,Tue Jul 7 02:39:54 2015 UTC,"Jeez, manner up please."
MachineLearning,3c8m69,sieisteinmodel,1 point,Mon Jul 6 07:31:51 2015 UTC,"The authors of this patent are slapping their dicks in the face of the ML community, they deserve nothing but contempt and ostracism."
MachineLearning,3c8m69,sdsfs23fs,0,Mon Jul 6 11:08:02 2015 UTC,oy vey!!
MachineLearning,3c8m69,watersign,-56,Tue Jul 7 02:16:47 2015 UTC,"Entitled little shit!  You've gotten so used to these guys sharing their life's work for free on arXiv, that when they collect a patent its all ""fuck you"". They never had to share anything with us!"
MachineLearning,3c8m69,Graydyn,38,Sun Jul 5 23:15:47 2015 UTC,"They are patenting regularisation by noise, something that has been around for decades."
MachineLearning,3c8m69,xyzwonk,0,Sun Jul 5 23:39:08 2015 UTC,"It won't be approved. There is something called prior art (https://en.wikipedia.org/wiki/Prior_art) basically if a patent is not an original or new idea, it will be denied on the basis of prior art."
MachineLearning,3c8m69,caedin8,7,Mon Jul 6 01:39:41 2015 UTC,"It might still be approved because the patent office often approved stuff despite prior art. It would be defeated in court, but it would probably take a lot of money. However Google doesn't have a history of suing over this kind of stuff. It's probably a defensive patent."
MachineLearning,3c8m69,tehbored,29,Mon Jul 6 01:53:13 2015 UTC,"There are very good reasons why you cannot patent pure software or mathematics in my country, and in most countries of the world. In case you weren't aware, pure software patents are extremely controversial and considered so actively dangerous in most developed countries that governments have had to specifically promise that they are off the table, despite extremely lucrative industry lobbying.  'Pure software' in this case means any invention that is only a computer program or technique and does not relate to it as a part of solving a 'physical' (outside-the-computer) problem within the limits of the proposed patent.   They never had to share anything with us!   Knowledge shared enriches us all. Are we not all on the same journey to understand the universe, progress as societies and survive as the human race? Or are we all in it to make money and use every advantage to get one over on others?   If TCP/IP or HTTP were patented, we would, most likely, not be able to have this conversation right now - the Internet itself would not have been able to grow into the monstrosity it is and a lot of us would still be using proprietary systems like Compuserve.   The world is founded not on capitalism or selfishness, but on altruism and those who share their inventions, time or resources to provide the foundations on which governments and capitalism depend. Be it open source, charity, volunteering, community projects, someone somewhere - at the bottom of it all - is leaving money on the table so we can all benefit. Stop being such an ass."
MachineLearning,3c8m69,TheMemo,-19,Sun Jul 5 23:47:42 2015 UTC,"Again I'll remind you that you can't patent something and keep it secret at the same time.  Which is the point of the patent system, to prevent people from hoarding technology.  If TCP/IP was patented, I like to think we would still be using the internet, but the inventors would now own very large yachts.  Of course I can't prove that point though.     The world is founded not on capitalism or selfishness, but on altruism     What planet are you from?  It sounds lovely.  I am seriously interested in hearing more about patent law in your country.  How do computer/data scientists get paid there?  What motivates a company to put resources towards research if they aren't going to get a patent out of it?"
MachineLearning,3c8m69,Graydyn,17,Mon Jul 6 00:29:43 2015 UTC,"If TCP/IP was patented, I like to think we would still be using the internet, but the inventors would now own very large yachts. Of course I can't prove that point though.    A quick Google search would demonstrate that the inventors of said technologies have answered this point before. TL;DR: they don't agree with you.  Although the patent system shares a patent with all, it makes it unlikely (as with the case of TCP/IP) that an entire industry would grow around a given patent of a similar type, creating opportunities for everyone far beyond the initial creators of the technology.   What planet are you from? It sounds lovely.   This one. Free state schooling began as an altruistic endeavour (Unions) that was so politically dangerous that it forced free state schooling, for example. Same goes for nationalised health (originally charity 'free' hospitals), the modern fire service, the internet itself (as previously mentioned), a lot of vaccines, early public sanitation works, and on and on. Almost every public good on which we depend began with altruism and involved people putting others before money or time.     How do computer/data scientists get paid there?   The same as anywhere, they are either academics and do research or they work for large firms and work with their systems or build products and services.   What motivates a company to put resources towards research if they aren't going to get a patent out of it?   Well, a lot of research is done in universities which then funnel research through to companies that they either create, invest in or have partnerships with in order to create products or services which they sell.  I mean, the software industry has been going a long time without software patents, you know, and there are plenty of UK and European companies that are doing quite well without having had to patent aspects of their software."
MachineLearning,3c8m69,TheMemo,4,Mon Jul 6 00:47:14 2015 UTC,"I am seriously interested in hearing more about patent law in your country.  How do computer/data scientists get paid there?  What motivates a company to put resources towards research if they aren't going to get a patent out of it?   Isn't it obvious? Having first hand cutting edge research at your disposal allows you to have a competitive edge and make money out of it. Even if it gets published, even if it's not patented, you get to be the first to use it, test it, implement a new product with it and profit.   Even if people catch up with this particular piece of technology later, for you it's already in the past. You're now implementing the next breakthrough while everyone is trying to catch up with what you did last year by reading you papers. You're still making money and having a competitive edge.   And with Software and machine learning, having the papers on what algorithm Google used to solve a problem is often 10% of the job done. Now you have to find out how to optimize hyperparameters, how to get it to work with adequate performance, where to get a dataset big enough to train the algorithm, etc. Good luck reproducing what they did without a big ass pile of money, people and data.   So, even if they shared with you their ""industrial secrets"", they still have months of competitive edge on you. And when you catch up, they're already into something else.   That's why companies do research. Not some patents. Patents don't protect you nearly as much as readiness, know how and constant pushing of boundaries."
MachineLearning,3c8m69,schmook,11,Mon Jul 6 07:47:03 2015 UTC,funded by government grants to do research  doesn't have to share results of research with public   ??
MachineLearning,3c8m69,doompie,21,Mon Jul 6 02:07:07 2015 UTC,"This is math, you aren't supposed to be able to patent universal truths. These things are not invented, they are discovered. It's like patenting the ""design"" of a hydrogen atom."
MachineLearning,3c8m69,antisyzygy,21,Mon Jul 6 01:00:23 2015 UTC,That doesn't make it right.   Do you think Jonas Salk shouldn't have refused to patent the polio vaccine and profited off it instead of making the formulation widely known?   Anyone that supports patenting scientific progress is delusional or making a lot of money.
MachineLearning,3c8m69,bkh,-14,Sun Jul 5 23:25:30 2015 UTC,"Oh man, regularizing neural networks compared to curing fucking polio. I've heard everything now."
MachineLearning,3c8m69,dwf,-8,Sun Jul 5 23:29:43 2015 UTC,"I know how are these comparable? clearly life without our advanced ""gorilla"" detectors is much worse than living with polio..."
MachineLearning,3c8m69,iidealized,-14,Sun Jul 5 23:59:39 2015 UTC,"Patenting a process does make it widely known, that's the whole point. The patenter has to make the work publicly available, but in return is paid for its usage. To clarify the difference here, the Polio vaccine needed to reach the extremely poor in order to be truly useful.  The same can not be said about deep learning techniques. There are a lot of companies making bank using ML research, and its about time they started chipping in."
MachineLearning,3c8m69,Graydyn,0,Sun Jul 5 23:44:40 2015 UTC,its about time they started chipping in.   ...to Google...
MachineLearning,3c8m69,doompie,-6,Mon Jul 6 05:49:31 2015 UTC,"You've made a mistake, next time don't voice a different opinion it's not welcome here"
MachineLearning,3c8m69,eos_styx,11,Mon Jul 6 03:56:14 2015 UTC,"Yet again, the reactions may be a little more welcome if you don't start with ""Entitled little shit!"" when you are voicing a different opinion..."
MachineLearning,3c8m69,coumineol,2,Mon Jul 6 06:13:12 2015 UTC,You sir are correct! :)
MachineLearning,3cby9u,nat47,2,Mon Jul 6 18:47:56 2015 UTC,"Look at the excellent keras python package in github: https://github.com/fchollet/keras You can build a tagging model using word embeddings from that. Your model would look something like this:  X_train = sequence.pad_sequences(X_train, maxlen=MAX_LEN)  X_test  = sequence.pad_sequences(X_test,  maxlen=MAX_LEN)  y_train = sequence.pad_sequences(y_train, maxlen=MAX_LEN)  y_test  = sequence.pad_sequences(y_test,  maxlen=MAX_LEN)  print('Build model...')  model = Sequential()  model.add(Embedding(max_features, embedding_size))  model.add(GRU(embedding_size, 64, return_sequences=True))  model.add(TimeDistributedDense(64, 1, activation=""sigmoid""))  model.compile(loss='binary_crossentropy', optimizer='adam', class_mode=""binary"")  Note your output labels need to be 3D, i.e. each row is a list of n dimensional lists, where n = the number of POS tags you have in a one hot encoding. That model uses a recurrent neural network and should do very well given enough training data. If you can use an Nvidia GPU and configure theano to use it, it'll be pretty fast to train also.  You can also exactly emulate the Weston and Collobert architecture referenced in that paper, i'd done that myself."
MachineLearning,3cby9u,simonhughes22,1 point,Tue Jul 7 03:00:02 2015 UTC,Have you looked at this architecture yet?
MachineLearning,3cby9u,dwf,1 point,Mon Jul 6 19:23:30 2015 UTC,I had skimmed through it... Their parameters and word representation seems very vague
MachineLearning,3c81hg,maxToTheJ,8,Sun Jul 5 19:41:26 2015 UTC,"I would really like someone closer to these patents to comment about this issue. There some fairly well known names from the community on these patents and it would be useful to see their view on it. Right now, the lack of dialogue from Google about this is disturbing, especially when they attempt to patent classification.  I would like to know how this would affect both commercial and non-commercial uses if these patents are approved."
MachineLearning,3c81hg,rantana,3,Sun Jul 5 20:59:26 2015 UTC,There some fairly well known names from the community on these patents and it would be useful to see their view on it.    Anyone who has interacts with very smart people on a daily basis knows how intelligence allows one to self rationalize with much more ease when there the possibility of personal benefit.
MachineLearning,3c81hg,skgoa,1 point,Sun Jul 5 21:15:57 2015 UTC,"I would like to know how this would affect both commercial and non-commercial uses if these patents are approved.   Patents are only applicable to commercial use, so non-commercial is unaffacted. For commercial use:   if you use the exact ideas described in the patent for the duration of the patent (20 years at most), you will have to get a license or risk being sued. Google currently has a policy of not enforcing patents unless you sue them for patent infringement. So in practice very little will change but it does impose a certain risk on startups.  if you come up with some novel idea based on of the ideas described in the patent, you are fine."
MachineLearning,3c81hg,muktabh,7,Tue Jul 7 05:41:46 2015 UTC,"They have already patented mapreduce/hadoop , classification , almost any user profiling using machine learning on a phone , Almost any type of Word Embeddings .   Unless Google is just protecting itself from patent trolls, (which I hope it is given how broad these patents are, they patented Word Embeddings because Word2Vec was invented on their premises ), I think it would have a total monopoly in Data Science in countries following US patents . Dropout patent request is similarly strange, how can Google get a patent on something which was invented in a university, and multiple people contributed to it ? Maybe it is protecting itself from organizations who are trying to patent other similar stuff .  Edit: Just discovered Google has pledged not to press some of the patents it has acquired (which includes the Hadoop patent above), the status of others is still not known."
MachineLearning,3c81hg,autowikibot,15,Sun Jul 5 21:01:40 2015 UTC,"Edit: Just discovered Google has pledged not to press some of the patents it has acquired (which includes the Hadoop patent above), the status of others is still not known.   The problem is that pledges are just pinky promises at best. There is no legal weight behind them and with the dynamics of a publicly owned company no serious expectation for it to last above X quarters.  What do people expect to happen if Google's stock takes a dip and shareholders start demanding more aggressive monetization of their patents?"
MachineLearning,3c81hg,blueblob11,8,Sun Jul 5 21:23:03 2015 UTC,"how can Google get a patent on something which was invented in a university, and multiple people contributed to it ?    That is another interesting question. Publicly funded work through DoE, NSF, and DoD grants being privatized by an aqui-hire."
MachineLearning,3c81hg,blueblob11,3,Sun Jul 5 21:17:53 2015 UTC,"Bayh–Dole Act:       The Bayh–Dole Act or Patent and Trademark Law Amendments Act (Pub. L. 96-517, December 12, 1980) is United States legislation dealing with intellectual property arising from federal government-funded research. Sponsored by two senators, Birch Bayh of Indiana and Bob Dole of Kansas, the Act was adopted in 1980, is codified at 94 Stat. 3015, and in 35 U.S.C. § 200-212,  and is implemented by 37 C.F.R. 401.   The key change made by Bayh–Dole was in ownership of inventions made with federal funding. Before the Bayh–Dole Act, federal research funding contracts and grants obligated inventors (where ever they worked) to assign inventions they made using federal funding to the federal government.  Bayh–Dole permits a university, small business, or non-profit institution to elect to pursue ownership of an invention in preference to the government.     Image i     Relevant: Association of University Technology Managers | Birch Bayh | Technology transfer | Stanford University v. Roche Molecular Systems, Inc.   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
MachineLearning,3c81hg,skgoa,3,Sun Jul 5 22:00:25 2015 UTC,"Maybe I just don't understand because I am not a lawyer but it seems the word embeddings should not stand because the idea was not new for word2vec, similarly with classification. Assuming the patents stand, marching cubes could be treated as an analogue where people very quickly came up with marching tetrahedrons to circumvent it. It seems to be doing a disservice to the overall research and citations that the researchers will receive to be patenting it."
MachineLearning,3c81hg,muktabh,3,Sun Jul 5 22:01:20 2015 UTC,"Maybe I just don't understand because I am not a lawyer but it seems the word embeddings should not stand because the idea was not new for word2vec, similarly with classification.    That is partially the goal for me here. I am aware there is a process to dispute a patent while it is in review and I think it would be naive to assume the process will throw out the patents unless people put in the effort now."
MachineLearning,3c81hg,skgoa,1 point,Sun Jul 5 21:09:23 2015 UTC,"it would be naive to assume the process will throw out the patents unless people put in the effort now.    I agree, even lawyers disagree about the meaning of this stuff sometimes. But if there exists prior art, that's something that could then be used to invalidate the patent even after it's issued. As far as the classification patent, the naive bayes classifier existed since the 50s according to wikipedia so that should constitute prior art I would guess? While the patent should not pass the novelty stage, it's a common issue with software patents that the reviewers do not know the area well enough."
MachineLearning,3c81hg,BeatLeJuce,1 point,Sun Jul 5 21:20:08 2015 UTC,"Dropout patent request is similarly strange, how can Google get a patent on something which was invented in a university, and multiple people contributed to it ?    Didn't Hinton invent that? Isn't he working for Google right now? Didn't his university put out a statement saying how proud they are to have that filed patent?"
MachineLearning,3c81hg,fhadley,2,Mon Jul 6 01:55:50 2015 UTC,"I am not a Canadian so do not know how their law works, but logically speaking, wasn't the research funded by public money. How can a private enterprise patent it?"
MachineLearning,3c81hg,skgoa,1 point,Tue Jul 7 05:45:02 2015 UTC,"The state wants the economy to grow. One way to foster that is to help businesses get an advantage. Patents are the legal basis for that. I don't know how it's in North America but here in Europe there is a big push for transfering scientific results to businesses. The university/research institute will typically get a share of the licencing fees paid for a patent, which can be a big part of a research group's budget. Also, having a healthy output of papers and patents raises your profile, which in turn will attract phd students, industry partners etc. to you. Everyone involved benefits, including society."
MachineLearning,3c81hg,fhadley,4,Tue Jul 7 09:18:28 2015 UTC,"People are overreacting. Pretty much every famous ML algorithm is patented. Support Vector Machines have been patented for ages (source), yet it's one of the most popular ML algorithms and people use them in education, research and industry every day and no-one gets sued.  Latent Semantic Analysis is another great example of a popular algorithm that is patented and yet no-one cares.  I'm pretty sure there are a gazillion other ML patents just like these, I just can't be bothered to look up every algorithm on the planet. Software-patents are an unenforceable joke anyhow.  Don't get me wrong, they suck and should be abolished, but this recent which hunt just because Google follows the standard procedure of everyone in the field is weird.  TL;DR lay down your pitchforks"
MachineLearning,3c81hg,skgoa,1 point,Tue Jul 7 11:37:09 2015 UTC,Then why patent them in the first place?
MachineLearning,3c81hg,fhadley,1 point,Mon Jul 6 17:07:51 2015 UTC,To ensure that the core technology in some of your products isn't patented by one of your competitors?
MachineLearning,3c9iah,MusicIsLife1995,3,Mon Jul 6 03:40:33 2015 UTC,http://projects.iq.harvard.edu/stat110
MachineLearning,3c9iah,kullback-leibler,1 point,Mon Jul 6 10:00:37 2015 UTC,Thanks!
MachineLearning,3c9iah,barmaley_exe,2,Mon Jul 6 18:25:14 2015 UTC,Try these: https://www.youtube.com/user/mathematicalmonk/playlists
MachineLearning,3c9iah,WallyMetropolis,1 point,Mon Jul 6 13:57:09 2015 UTC,Thanks!
MachineLearning,3c9iah,stolzen,1 point,Mon Jul 6 18:25:16 2015 UTC,I think this is a great resource: https://www.khanacademy.org/math/probability/statistics-inferential
MachineLearning,3caite,iamtrask,3,Mon Jul 6 11:37:47 2015 UTC,The abstract of the paper is way better than this crappy press release (spoilers: the huge neural network is a skip gram model). It looks like they combined word embeddings and character embeddings to take advantage of morphological characteristics of the words. It could be useful for practical applications where words are frequently misspelled...  EDIT: Discussion for the paper here
MachineLearning,3caite,zmjjmz,1 point,Mon Jul 6 12:39:22 2015 UTC,"I think the most novel part about it was the partitionings. Aka,  they had two skip gram models: one to predict the previous word and one to predict the next word."
MachineLearning,3caite,Articulated-rage,2,Mon Jul 6 15:30:46 2015 UTC,"Difficult to believe that this got past the reviewers at ICML.  Their model is a hack for solving syntactic analogies. Let me clarify. Consider a very simple model where you simply sum up one-hot representations of characters and use that as your word embedding. This will do extremely well on syntactic analogies (e.g. ""staying - stays"" will be exactly equal to ""playing - plays""), but it doesn't mean that the model is any good.  Their character model is simply a variation of the above.  I am becoming tired of these we-built-the-deepest-and-largest-neural-network-that-solves-all-AI type papers. I guess if you are writing for the press and not your peer researchers, you write different kinds of papers. Deep learning is immensely useful and here to stay, but researchers need to tone down their papers. It's unsurprising to note that industry papers are the greatest offenders. Deep learning papers from academia tend to be, thankfully, more measured."
MachineLearning,3caite,death_to_patents,2,Mon Jul 6 14:58:28 2015 UTC,There should be a metric for performance vs complexity for dnns. If a new model comes out and it fits an extrapolated pattern--it performs exactly as well as we thought it would when you double the number of parameters--then it shouldn't be considered as novel.
MachineLearning,3caite,Articulated-rage,2,Mon Jul 6 15:33:13 2015 UTC,Difficult to believe   Not so difficult. I've read plenty of pretty iffy ICML papers.
MachineLearning,3c7ulr,5at,3,Sun Jul 5 18:40:47 2015 UTC,"Is it just me and my sleep-deprivation, or was the paper harder to read than it needed to be? It has some confusing forward references to future sections, and some unmotivated parts. E.g. , Figure 2 takes up a lot of space but has zilch to do with the topic at hand.  Figure 4 pretty much screams ""handpicked"", and is thus fairly useless as well. Also, I think Figure 1b and 1a are switched around: shouldn't  a longer arclength give fewer but longer arcs, contrary to what is shown there?  With that said, their hypothesis is an interesting one: Can we reproduce the results of dropout by simply producing new training samples that make the nets behave like dropout? Apparently they can, although they never show that their newly-produced datapoints result in dropout-like activations/gradients/weights, but they do achieve dropout-like performance.   The abstract also says that ""[the authors] furthermore propose an explanation for the increased sparsity levels that can be observed in networks trained with dropout and show how this is related to data augmentation"". I find no explanation in the paper, just an (unproved and hand-wavy) hypothesis that sparseness is a sign that the net learns to associate datapoints with different, non-overlapping subspaces ('tiles') of the input-space.  I find their conclusion ""If learned features “tile” the input space, one would expect that the information whether a hidden unit is on or off is more important than the actual magnitude of the hidden response."" questionable, and their experiments related to it as well: so they train what is essentially a net with a sigmoid activation, and say that ""because this works, a network obviously learns to rely on binary information"". Well yes, that's what a sigmoid activation does. The fact that they've binarized the activations doesnt tell us much (in all likelihood the sigmoids were mostly saturated anyhow).  Also, I don't see how any of that relates sparsity to data-augmentation.  All in all, I think the hypothesis is interesting, but they need more experiments to convince me that the viewpoint of dropout=data augmentation is viable."
MachineLearning,3c7ulr,BeatLeJuce,-10,Mon Jul 6 14:29:14 2015 UTC,Dropout is on the way to being patented. People should avoid it. What is the point of investing time in something which can be taken from under the people who invested time by a third party?
MachineLearning,3c7ulr,maxToTheJ,2,Sun Jul 5 19:39:06 2015 UTC,"Not everyone lives in countries that recognise software patents. Thankfully, I don't."
MachineLearning,3c7ulr,BadGoyWithAGun,1 point,Mon Jul 6 08:05:20 2015 UTC,I might have to move to China soon
MachineLearning,3c7ulr,marijnfs,-3,Mon Jul 6 16:37:50 2015 UTC,"Do you mean this patent application?  It is such a simple idea that removing it from your implementation takes just a few minutes. Right now, it is not patented."
MachineLearning,3c7ulr,clrokr,2,Sun Jul 5 21:53:47 2015 UTC,"It is such a simple idea that removing it from your implementation takes just a few minutes. Right now, it is not patented.   The point isnt how easy it is to remove but to not build dependencies on stuff that will become deprecated soon which only serves to benefit the people patenting."
MachineLearning,3carfb,faridthefirst,3,Mon Jul 6 13:12:31 2015 UTC,"It feels like you tried to make your own things based on your intuition which is a great way of thinking, so congrats on that.   It feels like cheating because your cost function encode all the information that you want to ""learn"". Basically, your ""net"" has not learned anything.   Ideally, the cost function is some sort of distance or divergence between predicted outputs to gold standard output, and an algorithm has to lower this distance without knowing how to do it.   A common way to do it is to use the gradient of the error and to ""backpropagate"". You should have knowledge about what is a function and a gradient to understand this.  Siblbombs suggestion is good. If you can, try to make a multi layer perceptron. I tried and it sort of works with 2 inputs, 3 layers with 128 units, 1 outputs with tanh activation function. With ReLU it could be more efficient (no saturation). I feel like binary classification is a better starting point when you are a beginner in machine learning."
MachineLearning,3carfb,bhmoz,1 point,Mon Jul 6 14:37:01 2015 UTC,"Well it doesn't sound like a traditional neural network, since there is no backprop. It also sounds like you could accomplish the same thing with a single layer, I don't think depth is really accomplishing anything here.  Consider the following extension: taking two numbers as input and multiplying them together. A neural net would be able to solve that problem (with some caveats), but I don't see how your system could do it."
MachineLearning,3carfb,siblbombs,1 point,Mon Jul 6 13:36:37 2015 UTC,So how many neurons would it need to do that kind of thing? How much depth is needed?
MachineLearning,3carfb,siblbombs,1 point,Mon Jul 6 13:55:57 2015 UTC,"To do the 2 number multiply? The problem is with your learning algorithm, it's hard-coded to solve for *2, the net can't be trained to multiply by 2 and some other number. If you train it to perfectly multiply by 2, then start trying to multiply by 3, once you have solved *3 *2 will no longer work."
MachineLearning,3carfb,siblbombs,1 point,Mon Jul 6 14:04:38 2015 UTC,So how do you teach a neural network to do stuff without any precise goal? Would it be something like making it try a few times and then shifting gradients towards what got the best results? Like rank the best results and do some sort of weighted average of the gradients?
MachineLearning,3carfb,autowikibot,0,Mon Jul 6 14:34:03 2015 UTC,Backprop
MachineLearning,3carfb,Tofutiger,1 point,Mon Jul 6 14:41:32 2015 UTC,"Backpropagation:       Backpropagation, an abbreviation for ""backward propagation of errors"", is a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of a loss function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the loss function.  Backpropagation requires a known, desired output for each input value in order to calculate the loss function gradient. It is therefore usually considered to be a supervised learning method, although it is also used in some unsupervised networks such as autoencoders. It is a generalization of the delta rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. Backpropagation requires that the activation function used by the artificial neurons (or ""nodes"") be differentiable.    Image i     Relevant: Almeida–Pineda recurrent backpropagation | Neural backpropagation | Backpropagation through structure | Rprop   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
MachineLearning,3carfb,siblbombs,1 point,Mon Jul 6 14:42:12 2015 UTC,"I'm not sure I understood that fully. So I'd have to run my neural network with an input normally. Then I run it backwards? What does that mean? I take the value(s) I wanted to get in the end and run them backwards, so that I can see what weights need altering?  The only thing I don't seem to understand is how to get the deltas. Once you have them, you simply have to scale the ratios accordingly (like if the output is 2x as high as predicted, divide the weight by 2).  Another question is, it seems the article uses weight as a singular. Does that mean each ""neuron"" has a single weight value towards every other neuron? Because I was assuming each link had a weight."
MachineLearning,3carfb,Nyld,2,Mon Jul 6 16:09:17 2015 UTC,"Hi, I'm also just starting to learn machine learning so I might be able to help you out.  1)Like others have said, you don't need a multi-layered network to do *2. You can just have a single input and a single output and let the weight of the connection be 2 and the bias be 0.   *a = wx* where *a* is the output, *w* is the weight and *x* is the input   You also don't need an activation function for this task. This makes the task very trivial.  2)Your ""correction"" function is also tailored to the task at hand and cannot be generalized to other tasks. Suppose we have 2 nodes at the input layer and we want to multiply the numbers by each other and return that as the output. Suppose we try to train the network with the data set:  ([1,1], 1) ([2,3], 6) ([5,7], 35)   For simplicity, suppose we have 2 input nodes, 1 hidden layer with 1 node, and 1 output node and the weights of the input to hidden layer are randomized to 0.3 and 0.5, and the weight from the hidden to the output layer is randomized at 0.7.  Activation of the hidden node for the first input data is   1*0.3+1*0.5 = 0.8,    and activation of the output is   0.8*0.7 = 0.56.   We can now correct by 1/0.56 = 1.8, changing the weight of the hidden node to the output from 0.7 to 0.7*1.8 = 1.25.  This makes the error 0. But when we try to do this to the second data input, we get 1.47 as the output, and if we do the correction for this data input, then we will obviously not be able to get the right output for the first data input. Thus the correction function does not work.  Another major problem with this method is that the correction function can only work when you have an expected activation at a node. For example, we have been using this correction function for the connection between the hidden node and the output node because we know what the output should be, but we cannot do this for the connection between the input and the hidden layer because we don't have expected values for the hidden layers. Thus, learning only occurs at the last layer.   The way that backpropagation works is by slowly changing the weights and the biases of the equation z = wx + b by following the slope of the cost function towards a local minima. You can learn about it from this useful website"
MachineLearning,3carfb,Nyld,1 point,Tue Jul 7 01:28:56 2015 UTC,"In the backwards phase you have to calculate some sort of singular 'error' term, and find the derivatives with respect to that error. Once you have the derivatives, you can adjust all the weights to reduce the error term. A simple error term for this approach would be (expected_output - calculated_output) **2.  A neuron has a weight for each neuron before it. If you have 1 input and two 10 neuron layers, the number of weights is 1 * 10 + 10 * 10  I'd recommend watching Neural Networks demystified parts 1-4 on youtube to get a basic understanding of what is happening."
MachineLearning,3carfb,xyzwonk,1 point,Mon Jul 6 18:08:36 2015 UTC,Less than a handful but I'd recommend reading up on backpropagation first.  Generally you'll be better off by using a general training algorithm based on the gradients (w.r.t. the weights) of a cost function. What you want to do is optimize the weights in such a way that the errors get smaller no matter what the problem is you're trying to solve.
MachineLearning,3carfb,mareram,1 point,Mon Jul 6 14:23:38 2015 UTC,"Another question is, it seems the article uses weight as a singular. Does that mean each ""neuron"" has a single weight value towards every other neuron? Because I was assuming each link had a weight.   Each weight belongs to a link between neurons of consecutive layers.   I'm not sure I understood that fully. So I'd have to run my neural network with an input normally. Then I run it backwards? What does that mean? I take the value(s) I wanted to get in the end and run them backwards, so that I can see what weights need altering?   The forward propagation step is necessary to compute the actual output of the network given the weights and some example for the input.  A cost function like the (mean) squared error between desired output and actual output can then be calculated (for a given combination of weights, input and desired output).  You can also compute the partial derrivative (gradient) of this cost function with regards to all the network weights.   If you imagine some sort of height map then the gradient points towards the direction of the greatest rate of increase of the cost function at any given point (to know where that point / the network output is you need the forward propagation step).  The backpropagation step essentially just walks a certain distance through the space of the network weights in the opposite direction of the greatest increase of the cost function. Thats why it's gradient descent.  Computing the partial derrivative is just the application of the chain rule and treating the network input like a constant. If you want to try that with pen and paper you really should pick something tiny with maybe 2 layers and 2 neurons each. Otherwise you'll may need a few sheets of paper. Alternatively take a look at theano which can compute the derivatives for you and  is probably the easiest way of being exposed to the math without actually computing derrivatives and still getting a running implementation."
MachineLearning,3c9j0j,doompie,1 point,Mon Jul 6 03:47:36 2015 UTC,Actually in this regime a smooth objective would suffice
MachineLearning,3c9j0j,ocramz,1 point,Mon Jul 6 03:58:07 2015 UTC,"if the as are fixed beforehand, I agree"
MachineLearning,3c9j0j,ocramz,1 point,Mon Jul 6 07:34:58 2015 UTC,"I meant that the data I'm fitting is unlikely to want an a or sigma far from 0 or 1, so the penalty a(1-a) could stand in for min(a, 1-a), since they're approximately equal near 1 and 0."
MachineLearning,3c9j0j,ocramz,1 point,Mon Jul 6 15:15:24 2015 UTC,"so this would be a concave and smooth penalization on the a{ij}. How large is the problem, and how sparse does A have to be?"
MachineLearning,3c9j0j,ocramz,1 point,Mon Jul 6 19:20:48 2015 UTC,what's the a_{ij} vs a{ij} notation for? Or was it a typo? So you have a matrix of as and objective sums over both indices?
MachineLearning,3c9j0j,ocramz,1 point,Mon Jul 6 07:31:26 2015 UTC,"Just a typo. The objective is, sadly, nonconvex, so I suppose I'm looking for annealing or swarm-type constrained optimizers..."
MachineLearning,3c87b3,jiminiminimini,4,Sun Jul 5 20:31:53 2015 UTC,"Most music information retrieval (MIR) researchers use mid-level representations such as STFTs rather than raw audio signals. I did some preliminary work comparing both approaches in combination with convnets ( https://dl.dropboxusercontent.com/u/19706734/paper_pt.pdf ). While larger-scale experiments could end up favouring a raw audio approach, for now I think using an STFT representation as input is the way to go. It's just easier to deal with. If you work with raw audio signals your RNN would need an insanely long memory.  As you said, the STFT is complex, unless you use the magnitude and throw away the phase information. The latter is often done in MIR, but if you want to generate music, throwing away the phase might be a bad idea. So sticking to complex-valued data is probably best. (Although - ignoring phase altogether and using some technique to reconstruct audio signals from magnitude spectrograms is also an option.)  Complex-valued data isn't an issue by the way: say you have a complex-valued vector of size 1024 representing an STFT frame, you can convert that into a real-valued vector simply by separating the real and imaginary parts and concatenating them into a vector with 2048 values in total. Or maybe splitting the complex numbers into modulus and argument and concatenating those instead would be better - this neatly separates the phase information. Try both, I guess :)"
MachineLearning,3c87b3,benanne,1 point,Sun Jul 5 21:45:49 2015 UTC,"Thank you very much. You have been really helpful. I guess I will use ""concatenated modulus/argument"" approach."
MachineLearning,3c87b3,JosephLChu,2,Sun Jul 5 22:11:49 2015 UTC,"I've been attempting this as well.  Since I wanted to try to make my network operate as close as possible to the way actual auditory perception works, I decided to use the signal directly as input...  Here's an example of a resulting sample:  https://www.youtube.com/watch?v=eusCZThnQ-U"
MachineLearning,3c87b3,JosephLChu,1 point,Tue Jul 7 15:59:25 2015 UTC,"this is great. how large was your training data? how many epochs did you train the network, how many hidden layers, etc.   what were your hyperparameters? it would really help me :)"
MachineLearning,3c87b3,robertsdionne,2,Tue Jul 7 16:10:48 2015 UTC,"My training data was about half an hour long, with a sampling rate of 8000Hz, and encoded at 8 bits (basically an ""alphabet"" of 256).  This sample was taken from a checkpoint around the 25th epoch.  This particular network used three layers, with 512 LSTM neurons in each layer.  Other than that, the only hyperparameter that was different from the defaults of Andrej Karpathy's char-rnn (https://github.com/karpathy/char-rnn), was the sequence length, which was increased to 250.  Hope that helps!"
MachineLearning,3c87b3,r4and0muser9482,1 point,Tue Jul 7 16:38:19 2015 UTC,yes it helps :)  thank you very much!
MachineLearning,3c87b3,sdsfs23fs,1 point,Tue Jul 7 17:08:21 2015 UTC,I love you.
MachineLearning,3c5f5j,vkhuc,24,Sat Jul 4 23:39:48 2015 UTC,"""and apparatus"", aka a regular computer.  I hate patents."
MachineLearning,3c5f5j,permalurk,22,Sun Jul 5 10:16:49 2015 UTC,"I don't think patenting methods with mathematical basis makes sense. Say, hypothetically, someone proves that deep Q-learning is mathematically equivalent to Bayesian latent asymptotic hyper-parametrisation (BLAH, I made this up as an example), is BLAH, a separate invention, now also patented?"
MachineLearning,3c5f5j,Ninjakannon,16,Sun Jul 5 14:07:16 2015 UTC,Seems they also applied for a patent on dropout
MachineLearning,3c5f5j,utonesm,10,Sun Jul 5 10:30:39 2015 UTC,"These patents are messed up. They didn't invent (iterated) q learning, they didn't invent deep learning, and the combination has been the result of countless papers by others. Using a deeper network as a policy function should not be a patent!"
MachineLearning,3c5f5j,marijnfs,8,Sun Jul 5 18:38:23 2015 UTC,Surprisingly readable.
MachineLearning,3c5f5j,gwern,33,Sat Jul 4 23:55:50 2015 UTC,"No, Google has applied for a patent on Deep Q-learning. The patent office still has to review and decide on it."
MachineLearning,3c5f5j,siblbombs,22,Sun Jul 5 02:03:28 2015 UTC,"When you asked reputable former academicians and now Google and Facebook employees, they claim that their prospect is to push science forward by the support of these great companies. However, it is clear that this not true and only  cheap words. Nothing should be patented if it is for science. If patented, then it is not science, this is business."
MachineLearning,3c5f5j,erogol,3,Sun Jul 5 12:35:36 2015 UTC,There is no way all those claims get accepted. Deep Q-learning has been published many times and is well known.
MachineLearning,3c5f5j,stupider_than_you,7,Sun Jul 5 14:35:29 2015 UTC,Could someone explain what this means?  Is it forbidden to use Deep Q-Learning in closed source projects only in the US now?
MachineLearning,3c5f5j,Reyny,5,Sun Jul 5 00:26:29 2015 UTC,"If they get a US patent, the chances are they will get a European one as well."
MachineLearning,3c5f5j,DarkSayed,1 point,Sun Jul 5 12:18:18 2015 UTC,"Source availability doesn't matter. You can't use it in commercial products.   Nothing too weird here, though. Calling this an algorithm is a bit far fetched, it's a whole toolkit. There's tons of other ways to do what you could call deep Q-learning."
MachineLearning,3c5f5j,vincentrevelations,10,Sun Jul 5 01:29:09 2015 UTC,Commercial doesn't matter. Patents confer an exclusive right to practice a certain invention in the US.
MachineLearning,3c5f5j,revrigel,-5,Sun Jul 5 03:35:15 2015 UTC,"They only check commercial stuff, though. Not even trolls would ask you to take down your github repo."
MachineLearning,3c5f5j,vincentrevelations,8,Sun Jul 5 13:50:54 2015 UTC,"really, I guess this didn't happen then.."
MachineLearning,3c5f5j,sdsfs23fs,-5,Sun Jul 5 14:09:36 2015 UTC,"Well, it didn't. That's a threat, not litigation. I can send you a S&D for the comment you just posted."
MachineLearning,3c5f5j,vincentrevelations,7,Sun Jul 5 14:15:52 2015 UTC,What you said:   Not even trolls would ask you to take down your github repo.
MachineLearning,3c5f5j,sdsfs23fs,-1,Sun Jul 5 15:46:02 2015 UTC,"Yeah, sorry, I kinda relied on the intelligence of the reader."
MachineLearning,3c5f5j,vincentrevelations,2,Sun Jul 5 18:45:59 2015 UTC,C&D?
MachineLearning,3c5f5j,skelterjohn,1 point,Sun Jul 5 15:26:03 2015 UTC,Woops yes.
MachineLearning,3c5f5j,vincentrevelations,2,Sun Jul 5 18:27:30 2015 UTC,Are those other ways allowed? Or does the parent cover them?
MachineLearning,3c5f5j,iforgot120,2,Sun Jul 5 03:43:20 2015 UTC,"If it's different enough, you can use it. Q-learning is an old technique, and ""deep"" is just a hype word they used to give their toolkit a name."
MachineLearning,3c5f5j,vincentrevelations,3,Sun Jul 5 13:56:21 2015 UTC,How do US patents work? Is infringing on a single claim enough to be held liable? Claim 1 reads like a text book Q-Learning description...
MachineLearning,3c5f5j,NasenSpray,1 point,Sun Jul 5 11:00:25 2015 UTC,"Yes, but if I have understood it correctly one or more claims could be ruled invalid in court"
MachineLearning,3c5f5j,johnlu,1 point,Sun Jul 5 13:29:59 2015 UTC,"If you knew about the methods before the patent was filed, those specific pieces will get excluded due to being prior art. When did this stuff first get published on?"
MachineLearning,3c5f5j,dire_faol,1 point,Sun Jul 5 14:29:19 2015 UTC,"Has anyone successfully sued using a patent on a machine learning algorithm?  Until that happens, I'm not too worried about this."
MachineLearning,3c5f5j,alexmlamb,5,Sun Jul 5 21:34:52 2015 UTC,Has anyone successfully sued using a patent on a machine learning algorithm?   Yes.
MachineLearning,3c5f5j,rantana,-24,Sun Jul 5 23:31:09 2015 UTC,Guys why cant you accept that they patent things like these just to protect everyone from patent trolls
MachineLearning,3c5f5j,010011000111,26,Sun Jul 5 00:48:21 2015 UTC,Ah yes. Good Google protecting everybody.
MachineLearning,3c5f5j,heptode,21,Sun Jul 5 01:59:46 2015 UTC,"I heard this lie in early '00, from Microsoft when they were just starting to build their now massive patent portfolio. Now, Microsoft is by no means a patent troll, but this doesn't mean they will turn a blind eye to you using their IP, especially if you're a competitor, or refuse a few million dollars in licensing fees. That's exactly how it's going to play out with Google as well."
MachineLearning,3c5f5j,gwern,10,Sun Jul 5 03:18:38 2015 UTC,"I doubt that. Which patent pool or license, exactly, will they be releasing this under...?"
MachineLearning,3c5f5j,Xeran_,2,Sun Jul 5 02:50:43 2015 UTC,Like Google themselves as one of the biggest patent trolls patenting common already greatly used and known concepts?
MachineLearning,3c4tvs,maccam912,15,Sat Jul 4 20:11:01 2015 UTC,Good idea for a startup
MachineLearning,3c4tvs,drum_code,4,Sat Jul 4 20:17:12 2015 UTC,"Indeed!  With a colleague, we were thinking of setting up something using BOINC: https://boinc.berkeley.edu/"
MachineLearning,3c4tvs,USER_PVT_DONT_READ,2,Sun Jul 5 17:57:28 2015 UTC,It seems like a large bottleneck would be transferring the huge amounts of data needed for a lot of ML problems to all of the nodes involved in processing.
MachineLearning,3c4tvs,Notlambda,3,Sun Jul 5 18:26:34 2015 UTC,"If you're at Berkeley you can join the campus cluster (~$25k buy-in), which is along the lines of what you're talking about: http://research-it.berkeley.edu/services/high-performance-computing  Some other institutions have these as well.  Otherwise I think AWS or other cloud solutions are the way to go."
MachineLearning,3c4tvs,ChrisKennedy,1 point,Sat Jul 4 21:37:44 2015 UTC,"University of Minnesota had something similar, where purchased computer power went to upkeep and new hardware occasionally.  Unfortunately I'm not at Berkeley though. It's too bad there isn't something more public than this."
MachineLearning,3c4tvs,siblbombs,2,Sun Jul 5 02:31:35 2015 UTC,"Nothing that I'm aware of, for a lot of deep learning models it's hard to split that across multiple nodes."
MachineLearning,3c4tvs,alexmlamb,3,Sat Jul 4 20:15:40 2015 UTC,"Well, it can still be useful to have a cluster because it lets one train many models in parallel for hyper parameter tuning.    Some people have also had success with data parallel sgd."
MachineLearning,3c4tvs,USER_PVT_DONT_READ,1 point,Sun Jul 5 07:13:13 2015 UTC,A really useful pointer: http://research.google.com/archive/large_deep_networks_nips2012.html  An open-source implementation of some of the concepts discussed in that paper: http://deepdist.com/
MachineLearning,3c4tvs,spurious_recollectio,1 point,Sun Jul 5 17:59:50 2015 UTC,"I'm thinking about maybe something that only supports a spark cluster (or mapreduce in a hadoop cluster). I'm not that familiar with either, but from what I know as long as you know how to write a mapper and a reducer, that work could be split across nodes."
MachineLearning,3c4tvs,sdsfs23fs,2,Sun Jul 5 02:36:08 2015 UTC,Well you could set something like this up.  An idea would be to just have the nodes be virtual machines which (mostly) avoids privacy issues and also allows easy resource limiting (fixed # of cores and ram) and a standardized environment.  They can all be connected via a VPN and someone can run a grid manager to distribute jobs as on a normal cluster.   Even if you did want to put in the energy to do this though I think there are several important limitations (some of which have been mentioned).     Distributing ML data across a large heterogeneous network seems difficult (though maybe a clever use of torrents might work). I don't know an open source virtual machine implementation that works well with the GPU which is what you would need for at least some ML applications.   In any case I think this is something that might make sense for a small group of people that are actively communicating but having a very large public service like this seems like it would require a much more significant infrastructure.
MachineLearning,3c4tvs,sdsfs23fs,1 point,Sun Jul 5 21:27:52 2015 UTC,"I'm thinking a little on it. I'll be back if I get anywhere, but I'm focusing my attention on kaggle at least for now. I'm trying to learn vagrant, and maybe combining it with docker can get something together.  Commenting here also to keep these tips in mind."
MachineLearning,3c4tvs,singularai,2,Wed Jul 8 00:56:13 2015 UTC,"if you can afford to pay for a 24/7 AWS instance, just spin up your cluster there when you need it."
MachineLearning,3c4tvs,zardeh,3,Sat Jul 4 20:40:46 2015 UTC,"The idea came from the fact that I have a desktop with decent hardware, which I don't have to pay for hourly. As far as I'm concerned, every hour it's just sitting idle is an hour of lost potential computation. If I let another person use that hour of computing time now, and they let me use both my computer AND theirs later on, I'd be trading an hour on my computer now for an hour on their computer later. We'd each be able to use two computers when we need them, and only have to pay for one each. Scale this to 100 people, and if each person spends 1% of their time using the power of 100 computers, they only need to provide 1 computer themselves."
MachineLearning,3c4tvs,bckygldstn,7,Sat Jul 4 21:03:30 2015 UTC,"yeah, but this doesn't scale because of the manpower needed to coordinate sharing.  it works for AWS because it's a homogeneous host, and it works for @Home because it's a homogeneous client. You've got neither."
MachineLearning,3c4tvs,DJ_InsideVoice,1 point,Sat Jul 4 21:07:53 2015 UTC,"IMO, that nagging feeling of wastefulness when your machines aren't running is actually a good thing. Gives you a little bit of extra motivation to keep doing research."
MachineLearning,3c4tvs,pug_subterfuge,2,Sat Jul 4 21:33:46 2015 UTC,"My problem is only having it half trained, coming up with a new idea I want to try (found a new feature, etc.), and having to decide whether I let it finish, or stop it and start it over with my new idea :)"
MachineLearning,3c6btz,mrTang5544,2,Sun Jul 5 06:30:23 2015 UTC,"Check out python projects scrapy and portia, or scrapeR on CRAN. I believe most big name sites have APIs as well. Google ""amazon price API"" and look at the first result. Also check out pcpartpicker.com if you are comparing pc part prices. There are a lot of tools out there you could use for this, and i dont think ML is the right tool for the job. If you really want to use ML you could use NLP or something, but you are going to have to start by scraping html and then you can do your ML on the html tags and content. Or if this project is just an excuse to learn ML, maybe find a different problem that is more suited to ML."
MachineLearning,3c6btz,rsynnest,1 point,Sun Jul 5 18:29:23 2015 UTC,"you'd have more luck in a different subreddit, but unfortunately r/scraping is pretty much dead.  I'm not familiar with any methods for scraping that reliably works on arbitrary websites. I could imagine just extracting all the strings from a page and using nlp techniques on the result though   if you want to scrape a small number of sites, is definitely just use the html tags"
MachineLearning,3c6btz,dreugeworst,1 point,Sun Jul 5 11:31:24 2015 UTC,"One of the techniques we have tried to extract entities has the following steps (in Python).    Get a page using urlllib2 or some other similar library Use BeautifulSoup (now bs4) to extract text. Use NLTK to break the text into sentences and extract parts of speech Extract noun phrases   (These 4 steps provide a set of names which can be a mix of product names, company names, cities and different kinds of entities)   We manually picked a few products from the list and created a training set.  We use the training set to get more product names.    (5 and 6 are done using some ML techniques).  This is still an exploratory process and we have some reasonable results but we still have a long way to go. We are iterating through a bunch of improvements for steps 5 and 6.   Hope this helps."
MachineLearning,3c6btz,dorait,1 point,Wed Jul 8 00:33:28 2015 UTC,"We did it for extracting skills from job descriptions (and not products), btw."
MachineLearning,3c6btz,dorait,1 point,Wed Jul 8 00:34:55 2015 UTC,"I have no trouble extracting the html and cleaning up the markup tags. One problem I noticed is with using TextBlob library to correctly tag the tokens of a product name as either NNP, NNS, JJ, etc. For example, I have a product named ""Elliot's Big cabinet Set"". When we tokenize this product name, the word ""set"" is being tagged as a ""verb"" and this obviously incorrect. How did you solve this problem, or did you manually tag the POS manually?"
MachineLearning,3c6btz,dreugeworst,-1,Wed Jul 8 01:04:46 2015 UTC,"First of all. Parsing data from a webpage is not related to machine learning any more than making a logo for a website is. You don't have anything that encompasses machine learning. You have a goal to scrape a website. It's a simple goal that bots everywhere achieve entirely without machine learning.  Relying on html tags is too ""brittle""?  Search for the div (""for"" loop) with the price in it (<div id=""price"">). Parse data from every instance of that div. Make a 2D array.  You can't just scrape a website for data using MySQL or something. HTML is also not too ""brittle"" for parsing information. Obviously web browsers do it very well billions of times a day. Reverse engineer it.   If you think ""HTML is brittle"" then I'm not sure how on Earth you planned on dealing with error compensation with a NeuralNet.  Sounds to me like you have reading to do."
MachineLearning,3c6btz,dreugeworst,3,Sun Jul 5 11:08:27 2015 UTC,"brittle is the wrong word, but I see his point - you have to make a new scraper for each website you add"
MachineLearning,3c6btz,dreugeworst,1 point,Sun Jul 5 11:23:57 2015 UTC,"So I guess use complicated if statements based on what you observe. They can't all be that different. How many sites is he price quoting?  Search document for ""price"", is there a number with a currency sign after ""price""? Then parse.  http://i.imgur.com/fsNnrXU.png  Honestly, I don't know what sounds harder and more time consuming. Programming the <span> or other tags for separate shopping websites or making a convoluted machine learning program to do such a simple task. One sounds like standard functionality and the other sounds like over-engineering to me."
MachineLearning,3c6k4s,ddofer,4,Sun Jul 5 08:53:14 2015 UTC,"to be slightly more helpful, why not try building an extremely simple NN that's equivalent to logistic regression. play with optimization until you can reproduce sklearn logreg results. then start adding more complexity to your NN."
MachineLearning,3c6k4s,sdsfs23fs,1 point,Sun Jul 5 11:29:03 2015 UTC,you have too many features and not enough samples.
MachineLearning,3c1lra,iori42,2,Fri Jul 3 22:40:30 2015 UTC,"I'm used to modeling uncertainty in neural networks by assuming some form for the distribution p(y | x) and then estimating the parameters of that distribution using a NN optimized for maximum likelihood.  It seems like this should recover the true distribution p(y | x) given enough training data.  In what sense does it not capture the ""model uncertainty""?    I suppose sampling from p(y | x) isn't the same as sampling from p(y | w,x)."
MachineLearning,3c1lra,alexmlamb,5,Sat Jul 4 08:40:33 2015 UTC,"Let's assume y is binary for sake of argument. In a Bayesian model, there are many different ways p(y|x)=0.5 can happen. For example:   the model is 100% certain that for this given x, the label y is essentially the result of a coinflip. The model knows that it's impossible to predict the label better for this input. In this case, for all parameters w that occur with high probability under the posterior p(w|data), p(y|w,x)=0.5 will hold. Therefore, on average p(y|x)=0.5 also holds. the model has no idea whether the label is 0 or 1, or a coinflip. As far as it's concerned p(y|x,w) can be anything. It does not know. In this case, there might be a w_1 so that p(y|x,w_1)=1 a w_2 so that p(y|x,w_2)=0 and w_3 so that p(y|x,w_3)=0.5, each with equal probability under the posterior p(w|data).   So just representing  uncertainty in p(y|x) doesn't give you enough information to know whether the model is certain that the label is uncertain OR if the model is uncertain. It may not be a big deal in just vanilla supervised learning, but this distinction is of crucial importance in reinforcement learning and active learning."
MachineLearning,3c1lra,fhuszar,3,Sat Jul 4 19:38:48 2015 UTC,"If your model says that P(y=0 | x) = P(y=1 | x) = 0.5, how will you know how sure your model is that this is the correct probability?"
MachineLearning,3c1lra,dylanbyte,3,Sat Jul 4 12:29:09 2015 UTC,"I've attempted this approach several times (it's basically mixture density networks, but your distribution doesn't necessarily have to be a mixture of course) but I've never really had any luck with it. If I assume a Gaussian, the variance just tends to collapse. If I then put a Gamma prior on the variance to prevent that from happening, the variances it learns just aren't meaningful.  EDIT: also, this blog post is wonderful."
MachineLearning,3c1lra,benanne,1 point,Sat Jul 4 13:11:45 2015 UTC,"So you have e.g. p(y | x) = N(m(x), v(x)), where m(x) is a NN giving mean and v(x) is a NN giving variance for a normal distribution at a particular x, and you use maximum likelihood to fit the NN?  In that case, consider for simplicity's sake an utterly trivial ""neural network"": m(x) = a and v(x) = b for some constants a and b you've optimized. This is simple maximum likelihood fitting of a normal distribution. It says nothing about the uncertainty in the estimates a and b, which for small number of samples can be considerable. A Bayesian approach to this problem would yield distributions for a and b, quantifying that uncertainty. To sample outputs y, you'd first sample a and b from their distributions, and then sample y from N(a, b). In your case, you'd only sample from N(a, b), because a and b are fixed ML estimates without uncertainty in them. It is the uncertainty in a and b that is not captured. If you have ""enough training data"" then this indeed won't matter because that uncertainty is negligible, but oftentimes you don't.  One non-Bayesian way to look at that uncertainty is to take a bootstrap sample of your data set and fit the NN to that, then rinse and repeat; by looking at the various outputs of the NN's so obtained you get some idea of how unstable (= uncertain) the result of your maximum likelihood procedure is. Note that this procedure produces a set of models, each of which is a reasonable fit to the data, and you're uncertain which of these models is the best one. That's why it's called ""model uncertainty""."
MachineLearning,3c1lra,Coffee2theorems,2,Sat Jul 4 20:49:56 2015 UTC,epic demos bro
MachineLearning,3c1lra,martinBrown1984,1 point,Sat Jul 4 12:00:01 2015 UTC,I didn't know about the connection between dropout neural networks and variational inference on Gaussian processes. This is interesting stuff.
MachineLearning,3c1lra,normee,1 point,Sat Jul 4 18:11:36 2015 UTC,"Well, this is just Bayesian neural networks, the connection to GPs is rather weak here, the entire papers/blog post could be rewritten without mentioning GPs. A more interesting well-known fact is that an infinitely wide BNN has been shown to converge to a GP in the limit (Neal 1994).  What still confuses me is the use of two delta spikes as the posterior approximation? Why: 1. one spike is at zero, the other is at non-zero? It seems like this is just way to make the variational objective looks like the training objective in dropout.  2. related to 1, the use of mixture as variational distribution is well known to be difficult since an additional bound on the entropy is often damaging. One can use HMC to get the posterior of the parameters in the BNNs, I guess this surely doesn't look like a two spiky thing. What insight does this bring?"
MachineLearning,3c5aud,flexiverse,1 point,Sat Jul 4 22:54:45 2015 UTC,that's 189 pages of images. plus every category below that (all dogs) have their own images.
MachineLearning,3c5aud,sdsfs23fs,4,Sat Jul 4 23:20:59 2015 UTC,"I think its even more than that, I think its 189 subcategories of dogs. If you look at lapdog, it has no subcategories, but 25 pages of images."
MachineLearning,3c42eo,brouwjon,5,Sat Jul 4 16:00:26 2015 UTC,"I recently chatted to an engineer at a major Formula 1 motorsport team who said they increasingly use machine learning and statistical techniques to analyse engine performance, optimise time spent in the wind tunnel and model tyre behaviour.  Apparently since last year they use it as part of simulations to model and identify their optimal race strategy.  He was pretty vague about it all. The basic summary was they pull back huge amounts of data from the car and in the past they just binned most of it, now they want to make sense of it."
MachineLearning,3c42eo,randamtumble,3,Sun Jul 5 14:51:04 2015 UTC,"There are severall companies, small and big, startups and public corporates. Examples:   Magic Pony Technology apply machine learning to video compression, enhancement and upscaling. Enlitic apply it to medical diagnosis. Atomwise use ML in drug discovery. Several companies, like Mobileye use research in the automotive space, obviously there is a lot of work these days on autonomous driving. There are ~9 companies who announced their self-driving car projects. Intuitive Surgical develop surgical robots. Rethink Robotics develop robots for manufacturing."
MachineLearning,3c42eo,fhuszar,1 point,Sun Jul 5 07:26:51 2015 UTC,"Companies don't come to mind (except google/facebook), but some applications people are definitely working on:   Computer vision (surveillance, mapping) Self driving cars Building efficiency    However, part of the reason for this I think is that a lot of physical problems require dependable solutions, and already have a background of physical modelling (PDEs etc). So there is less need/use for machine learning techniques (or at least historically)."
MachineLearning,3c4xsk,theirfReddit,1 point,Sat Jul 4 20:47:27 2015 UTC,c.f. https://www.reddit.com/r/MachineLearning/comments/3bk8vh/help_with_training_recurrent_neural_networks/
MachineLearning,3c4xsk,mat_kelcey,1 point,Sat Jul 4 22:33:21 2015 UTC,"Hey! This was my earlier post. From that I learned I have to split the lstm and classifier— from your link— but even after reading the code, I still can't figure out how to train this. When I try to train it as one network, I get undefined in every output layer ( https://www.reddit.com/r/MachineLearning/comments/3c3pyz/deep_learning_with_synapticjs/ )  And if I were to train them Individally this would mean I activate the lstm, average the outputs and activate the classified with the average. But how do I train the lstm if I don't know what it's ideal output should be and how do I train the classifier if I don't know what it's input will be?"
MachineLearning,3c4xsk,mat_kelcey,1 point,Sat Jul 4 23:09:22 2015 UTC,"Ah, I don't know synaptic.js at all sorry. Like you say the usual approach is to jointly train both the LSTM and the classifier on top of it at the same time. Sounds like your problem is with the framework more than the model. If you really had to train them separately you might be able to use an approach like the old school unsupervised pretraining (eg some kind of language model maybe) but i think this is certainly not ideal."
MachineLearning,3c4xsk,mat_kelcey,1 point,Sat Jul 4 23:42:37 2015 UTC,It's okay. It is pretty new.  What i've just done is separate them and have the activations of the lstm averaged and activate the classifier with that. It works fine when they're separate. But that's without training. I don't know how to train them.
MachineLearning,3c4xsk,jfsantos,1 point,Sun Jul 5 00:10:32 2015 UTC,"you could try a language model first, that's pretty simple to train ( eg this simple RNN lm i wrote ) but definitely try to get the entire thing training jointly."
MachineLearning,3c4xsk,mat_kelcey,1 point,Sun Jul 5 00:23:38 2015 UTC,Thanks!
MachineLearning,3c4xsk,jfsantos,1 point,Sun Jul 5 00:40:20 2015 UTC,"I don't know a thing about synaptic.js, but there's a really clear example available with Keras: https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py"
MachineLearning,3c4xsk,jfsantos,1 point,Sun Jul 5 00:28:51 2015 UTC,"I can't seem to figure out how this is training it. How does it solve my issue? If you don't mind, could you explain it to me? Please."
MachineLearning,3c4xsk,alexmlamb,1 point,Sun Jul 5 00:41:48 2015 UTC,this model is effectively the same as the one from the other thread just in another framework.
MachineLearning,3c0amr,elenaward,9,Fri Jul 3 16:33:28 2015 UTC,"Amazon ML does not allow you to see the actual algorithm that's being used, you can only specify the type of model.   The model types are in Amazon terms  Regression - This is numeric-based values. My thought is they would be using more than just a straight regression, but you have no idea given they do not actually define or illustrate any algorithm that's being used.  Binary - True/False, On/Off, 1/0  Multi-class - Classification-based models  These are all supervised algorithms so if you are looking for any unsupervised work, you will not find it here.  In addition to not knowing what algos are being used, Amazon explicitly states you cannot import OR export your models. So if you need to move your work or import work based on PMML you are also out of luck. This would also be helpful in understanding what algos are being used by the AmazonML.  Reading through the docs, I think Amazon is trying to abstract the complexity for programmers. In doing so they are also hiding what their engine is actually capable of performing. I could be wrong, but based on what I am reading and what I am NOT reading the engine seems primitive. Every other ML package out there can walk you through the math and algos they are using to build models...except this one. AmazonML is a complete blackbox.  This goes back to a saying I heard years ago, if I can't explain - I can't use it. I wouldn't put a great deal of stock in the results of AmazonML until they were much more open in how they are arriving at them."
MachineLearning,3c0amr,hangtime79,1 point,Sat Jul 4 00:06:47 2015 UTC,"Yeah, not being able to export models is a non-starter for me."
MachineLearning,3c0amr,libertarien,3,Sat Jul 4 13:50:55 2015 UTC,But what model does it actually use? I did see it explained in the documentation?
MachineLearning,3c0amr,nick_ok,1 point,Fri Jul 3 21:22:07 2015 UTC,"Here's one for Azure, which imo, is better. http://www.microsoftvirtualacademy.com/training-courses/getting-started-with-microsoft-azure-machine-learning"
MachineLearning,3c3pyz,theirfReddit,1 point,Sat Jul 4 13:54:32 2015 UTC,"Any of these a divide by zero?  Line #s: 792, 960, 1145"
MachineLearning,3c3pyz,Mr-Yellow,1 point,Sat Jul 4 20:06:27 2015 UTC,"Thank you for replying! :-)  They shouldn't be. Why would they be when I connect two networks together? But not two separate networks?  I think my issue is that it gets confused with the 2 hidden layers-- the memoryBlock in the lstm and hidden-layers in the classifier. Because they way the network is defined is: network.set({ input: nameOfInputLayer, hidden: arrayOfHiddenLayers, output: nameOfOutputLayer }); How could I define two hidden segments or even 3-- the output of the lstm, the input of the classifier, and the hiddenLayers of the classifier."
MachineLearning,3c3pyz,Mr-Yellow,1 point,Sat Jul 4 20:43:49 2015 UTC,"Why would they be when I connect two networks together? But not two separate networks?   No idea, just a code-monkey running my code eyes over it. ;-)  Given they're all undefined and not just an index here or there.... Which might point to it not being any of the math per-se but some assignment going wrong somewhere."
MachineLearning,3c3pyz,mikeDepies,1 point,Sat Jul 4 21:08:10 2015 UTC,"Yeah, well thank you for reading through the code!"
MachineLearning,3c3pyz,mikeDepies,1 point,Sat Jul 4 23:10:09 2015 UTC,I suspect you are getting nan's or divide by 0's. There is some sort of numeric corruption going on in your calculations. That's just the intuition I'm feeling after my own experience of writing a NN framework from scratch and debugging various bugs. I'll try to remember to take a look at the code when I get off of work.
MachineLearning,3c11cr,no_potion,2,Fri Jul 3 19:57:09 2015 UTC,"Thanks for the link, I think I will go trough the course."
MachineLearning,3c3n5z,spurious_recollectio,3,Sat Jul 4 13:20:21 2015 UTC,Thanks all but just to be clear I'm not asking between the TI and the titan X...my question is about different vendors for the 980 ti.  The titan is a bit too much for me now (12 GB would be great but I'd rather wait a year and shell out then for the next architecture).
MachineLearning,3c3n5z,benanne,3,Sat Jul 4 16:52:54 2015 UTC,"Some vendors like to clock the GPU a little higher, and they also tend to use their own cooling solutions. So those would be the main things I'd look at (and price of course). If you're going to do deep learning the appearance of the card and which games are included won't matter too much I suppose :)  Personally price would be my main consideration, so I'd go for the cheapest option (unless it gets terrible reviews or something)."
MachineLearning,3c3n5z,lotbees,1 point,Sat Jul 4 18:33:47 2015 UTC,"Getting the cheapest card plus a GPU cooler might be the best way to go, since typically one of the main things you're paying for between cards is their cooling solution. For example, you could pick up an Arctic Cooler Accelero card, like the Xtreme III or IV. It will eat up a lot of space in your case however, but you could overclock the core and memory clocks a bit using one. Just an idea."
MachineLearning,3c3n5z,benanne,1 point,Sat Jul 4 20:14:42 2015 UTC,"If you're going to put it in a machine under your desk that's a good idea.  Otherwise, the main trade-off with cooling is how much noise it produces, and that doesn't really matter if you're going to put the machine somewhere out of hearing range. So then just getting the cheapest card might be sufficient :)"
MachineLearning,3c3n5z,lotbees,1 point,Sat Jul 4 20:18:09 2015 UTC,"Hmmm, true, but it looks like those can cool the GPUs quite a bit. If you can overclock a 1GHz GPU to ~1.5GHz, that's quite a speedup, especially when models can take many hours if not days (as long as the model is actually bound on the computation and not overhead from things like memory shuffling, etc). But you are right, a lot of coolers are primarily focused on reducing noise. I personally don't have a GPU cooler, so those may not work as well as I'd hoped for overclocking."
MachineLearning,3c3n5z,timdettmers,1 point,Sat Jul 4 20:55:46 2015 UTC,So as I said below...should I really care about overclocking?  From what I've read (see e.g. Tim Dettmers blog) the main bottle-neck will be bandwidth not clockspeed.  So do vendors actually improve bandwidth at all (can they even do this?).  I suspect that for DL vendor choice will be irrelevant but I was just hoping to get some confirmation.  Also is no one using a 980 TI for DL?  I've heard surprisingly little about it here (unlike when the original 980 came out).
MachineLearning,3c3n5z,sdsfs23fs,1 point,Sun Jul 5 10:27:29 2015 UTC,"Overclocking the core does not increase deep learning performance noticeably since every computation is memory-bandwidth bound. Because GPUs rarely ship with overclocked memory the vendor does not matter for deep learning — pick that which is cheapest.  EDIT: As benanne mentions the cooling is the prime differentiator between vendors. So do not pick what is cheapest, pick that which has the best cooling solution without adding too much to the price."
MachineLearning,3c3n5z,siblbombs,1 point,Mon Jul 6 18:14:00 2015 UTC,Thanks that's more or less what I was expecting.
MachineLearning,3c3n5z,dwf,2,Mon Jul 6 20:11:41 2015 UTC,"unless you really need 12GB memory, the speed boost of the Titan X is not worth the price."
MachineLearning,3c3n5z,siblbombs,1 point,Sat Jul 4 14:15:28 2015 UTC,"Titan X if you think you can use the extra 6 gb of memory, either for caching training data or larger models. You can get slightly better performance with cards that have higher core/memory clocks, however all the maxwell cards overclock pretty well so you could just overclock a card yourself for the same effect."
MachineLearning,3c3n5z,dwf,1 point,Sat Jul 4 14:20:41 2015 UTC,You could also potentially run multiple larger models on the same card simultaneously.
MachineLearning,3c3n5z,sdsfs23fs,1 point,Sat Jul 4 17:42:48 2015 UTC,"Yea, I don't have too much experience with that in theano tho."
MachineLearning,3c3n5z,dwf,1 point,Sat Jul 4 20:14:47 2015 UTC,"By multiple models, I mean in multiple separate processes."
MachineLearning,3c3n5z,5at,1 point,Sat Jul 4 21:48:55 2015 UTC,what the point of this? Is there ever a nontrivial situation where you're not saturating either the transfer bandwidth or the compute capability?
MachineLearning,3c3n5z,dwf,1 point,Sat Jul 4 22:34:46 2015 UTC,"If you're experimenting with lots of different hyperparameter settings on a dataset where each minibatch isn't too big, transfer bandwidth shouldn't be an issue, especially if you only have one card and it is operating in 16x mode. If it's convolutional, even the highly optimized cuDNN kernels only achieve about 40% peak on average on recent cards on a fairly representative setup, see Table 3 in the CuDNN paper.  Not sure how much this has changed with the Titan X. If these numbers hold up you could probably run at least two training jobs on a single card with negligible impact on performance, or run some training while also evaluating a previously trained model on a large validation set. You might have an easier time saturating compute capability if everything's simple GEMMs, though. Definitely if you're screwing around with small prototypes, being able to train more than one thing at a time (without needing to buy/power more than one card) is handy, and the roomier the card the better."
MachineLearning,3c3n5z,5at,1 point,Sat Jul 4 23:02:03 2015 UTC,"CUDA doesn't have multiprocess parallelism on the same GPU (unless things changed with 7.0, which I doubt)"
MachineLearning,3c3n5z,dwf,1 point,Sat Jul 4 22:44:36 2015 UTC,"Not multiprocess as in you fork a single CUDA process; multiprocess as in you run two different CUDA processes. That's perfectly okay and has been for a long time. Not as useful as I thought, since they don't truly run in parallel, but potentially still useful."
MachineLearning,3c3n5z,5at,2,Sat Jul 4 22:46:55 2015 UTC,They won't run in parallel:  http://stackoverflow.com/questions/14895034/multiple-processes-launching-cuda-kernels-in-parallel
MachineLearning,3c3n5z,Bhavishya1,2,Sat Jul 4 23:18:35 2015 UTC,"Well how about that. I haven't actually done this very much. Still, I wonder if the memory buffers on the GPU get pushed to the host; my observation of GPU memory usage stats suggests they don't. In that case, it might still be useful to have multiple sets of parameters on card, be training one and lightly experimenting with another."
MachineLearning,3c3n5z,weez09,1 point,Sun Jul 5 02:29:09 2015 UTC,I have done this on my 970.  It is definitely supported (I run two separate processes both using cudarray).  OTOH even for models that only use 40-60% of the GPUs computational capacity I find a significant slow down in using two processes on the same GPU (even if each seem to average ~50% utilization).  My guess was that this was somehow bandwidth constrained but I didn't investigate enough.
MachineLearning,3c3n5z,astonish,1 point,Sun Jul 5 10:22:41 2015 UTC,http://blog.golang.org/concurrency-is-not-parallelism
MachineLearning,3c3n5z,XeonPhitanium,1 point,Sun Jul 5 17:38:09 2015 UTC,You should post which models you want us to compare/
MachineLearning,3c3n5z,jibjaba2,1 point,Sat Jul 4 19:41:36 2015 UTC,There are many and my first inclination would be to go with price.  My working assumption is that the overclocking and other vendor-specific improvements are relevant for gamers but would less relevant for DL because its memory rather than computationally limited.  This was really the core of the question.
MachineLearning,3c3n5z,snarik,1 point,Sun Jul 5 10:24:57 2015 UTC,"Since the underlying hardware is the same, I think what you're asking for is which 980 ti can I milk the most performance out of while taking into account prices. You may get better answers on /r/buildapc. I've generally heard that the G1 is a beast of a card right now. If you'd like the cheapest, just go with any of the reference ones around 650."
MachineLearning,3c02hz,vikkamath,1 point,Fri Jul 3 15:29:20 2015 UTC,"This is apparently from a meetup, anyone have a recording?"
MachineLearning,3c02hz,Barbas,2,Fri Jul 3 19:57:41 2015 UTC,See https://news.ycombinator.com/item?id=9814091
MachineLearning,3c02hz,gwern,1 point,Fri Jul 3 21:47:54 2015 UTC,Pretty old:  http://jmlr.org/proceedings/papers/v28/coates13.pdf
MachineLearning,3bxb6q,rasbt,60,Thu Jul 2 22:37:34 2015 UTC,"The specifics of the post aside, I think that Schmidhuber doesn't get enough credit for his work, which is quite important.    His lab produced the first work on the vanishing gradient problem, came up with LSTM (which is a big improvement over vanilla RNN), and played a big role in the recent RNN boom (handwriting recognition and synthesis).  His student, Alex Graves, has done a lot of the cool RNN work at DeepMind.    Regarding why he isn't more widely recognized, my guess is that it's mostly a result of him being in an obscure European institute, rather than a major North American industrial lab or university.  A professor at a small school in Europe and a professor at Stanford could publish the same idea tomorrow, and the professor from Stanford would almost certainly get most of the attention and credit."
MachineLearning,3bxb6q,alexmlamb,14,Fri Jul 3 04:10:24 2015 UTC,"Case in point: I posted a link to Schmidhuber's Google+ version of this article two days ago. which garnered 47 internet funpoints and 7 comments.  This post, which was posted two days later, does not directly link to the subsequent discussion between Schmidhuber and LeCun, but contains the proper names in the title, has garnered twice as many funpoints and twice as many comments."
MachineLearning,3bxb6q,votadini_,3,Fri Jul 3 09:44:16 2015 UTC,"To be fair, yours doesnt mention Schmidhuber either.  And it contains the word 'conspiracy', which was instantly a turn-off for me personally.  I didnt click through your link.  I clicked through this one.  Mainly because it mentions Schmidhuber, to be honest."
MachineLearning,3bxb6q,hughperkins,1 point,Fri Jul 3 23:46:44 2015 UTC,"I literally copied and pasted Schmidhuber's own title, and you're confirming that it put you off from reading the article."
MachineLearning,3bxb6q,votadini_,3,Sat Jul 4 06:24:31 2015 UTC,"he copied Hinton-LeCun-Bengio who call themselves the ""deep learning conspiracy"""
MachineLearning,3bxb6q,corebiz,5,Sat Jul 4 07:46:30 2015 UTC,"Regarding why he isn't more widely recognized, my guess is that it's mostly a result of him being in an obscure European institute, rather than a major North American industrial lab or university.   He gives one possible reason in that there is a self sustaining group forming a closed citation group. It would be interesting to do a networks analysis of citation patterns in ML."
MachineLearning,3bxb6q,maxToTheJ,2,Fri Jul 3 14:59:34 2015 UTC,"Not sure if there's been work on the citations network (it's very noisy), but the NIPS paper co-authorship data set is a very popular one for network analysis [1]. In the late 90's for instance, there's a noticeable cluster around Geoff Hinton, Zoubin Ghahramani, and Michael Jordan. I believe this was when Zoubin moved from his PhD with Mike to his postdoc with Geoff.  [1] http://arxiv.org/abs/0912.5410"
MachineLearning,3bxb6q,dustintran,3,Fri Jul 3 18:54:56 2015 UTC,"With consideration to the current discussion, it might be prudent to cite the original paper that showed this cluster [1] instead of the survey paper that reproduced it.  [1] http://papers.nips.cc/paper/2879-dynamic-social-network-analysis-using-latent-space-models.pdf"
MachineLearning,3bxb6q,votadini_,2,Sat Jul 4 06:30:01 2015 UTC,"I'm new to this whole literature, but my first impression is that a lot of the papers he's listing as seminal work are pretty light on evidence. I think it's not enough to be right — you also have to be convincing. Maybe the standards of the time were different, though. I don't know how these papers were received at the time."
MachineLearning,3bxb6q,syllogism_,1 point,Fri Jul 3 18:47:14 2015 UTC,"the thing that makes sense in your post is when you write ""I'm new to this whole literature""   first study then post"
MachineLearning,3bxb6q,corebiz,2,Sat Jul 4 07:35:55 2015 UTC,You're right — I regret writing that.
MachineLearning,3bxb6q,syllogism_,-1,Sun Jul 5 03:13:45 2015 UTC,"cool answer, syllogism!"
MachineLearning,3bxb6q,corebiz,14,Sun Jul 5 19:31:12 2015 UTC,"LeCun's response to Schmidhuber (""How should we attribute credit?"") is a little ironic considering he came down hard on the Yahoo researchers who claimed something unique about their face-detection algorithm using ConvNets: https://www.facebook.com/ylecun/posts/363032813903834"
MachineLearning,3bxb6q,despardesi,8,Fri Jul 3 20:36:46 2015 UTC,This conversation started by Schmidhuber on the connectionists mailing list is worth reading: http://mailman.srv.cs.cmu.edu/pipermail/connectionists/2015-March/027851.html  It has a variety of viewpoints.
MachineLearning,3bxb6q,tjpalmer,10,Fri Jul 3 12:33:42 2015 UTC,"I can kind of understand where he's coming from. There's a certain breed of people who build their (often amazing) careers on indiscriminately taking credit for other people's work. That's not to say their own work is not great. It very much is, in this particular case. That is to say that one should think twice before signing their names to a self-aggrandizing puff piece, especially if one is planning to selectively omit the names of the people who directly or indirectly contributed to their stratospheric rise. Proper, careful attribution is the least you can do in academia.  Kudos to Jurgen for not really buying into the whole political correctness dogma, and doing this important work for the authors of the original article."
MachineLearning,3bxb6q,heptode,30,Fri Jul 3 17:45:49 2015 UTC,Yann LeCun responds in the comments of JS' G+ post.
MachineLearning,3bxb6q,dwf,9,Fri Jul 3 02:38:36 2015 UTC,"Krizhevski, Sutskever and Hinton get a lot of credit for their work, and it's well deserved. They used many of my ideas (and added a few), but you don't see me complain about it. That's how science and technology make progress.﻿"
MachineLearning,3bxb6q,maxToTheJ,19,Fri Jul 3 14:55:47 2015 UTC,Of course he does not complain: they cite him.
MachineLearning,3bxb6q,sieisteinmodel,2,Fri Jul 3 18:59:42 2015 UTC,wow
MachineLearning,3bxb6q,bshako,5,Fri Jul 3 15:38:38 2015 UTC,"around 1992 he also had working unsupervised pre-training for deep networks many years before Hinton, and also attention-learning networks. And  they made recurrent networks that reinforcement-learn directly from large scale vision input long before deepmind whose research is driven by his former students http://people.idsia.ch/~juergen/naturedeepmind.html"
MachineLearning,3bxb6q,corebiz,9,Sat Jul 4 07:24:00 2015 UTC,I think he is completely right.
MachineLearning,3bxb6q,piesdesparramaos,12,Fri Jul 3 11:04:53 2015 UTC,"Looks like we got ourselves a good old fashioned deep learning showdown here, folks."
MachineLearning,3bxb6q,cwolveswithitchynuts,-22,Fri Jul 3 05:35:35 2015 UTC,This guy seems bitter...
MachineLearning,3bxb6q,MyMomSaysImHot,22,Fri Jul 3 00:08:15 2015 UTC,i don't think he's bitter. he's a professor with a long and successful career.  but you don't have to be bitter or jealous to be offended by people getting fame and fortune by claiming to invent other people's work.
MachineLearning,3bxb6q,dfarber,-6,Fri Jul 3 01:16:04 2015 UTC,"Except LeCun, Bengio, and Hinton have not claimed to invent anyone else's work. Schmidhuber is throwing a fit because their work was built by combining pieces that other people who came before them put together. In some cases, like when he says more of the credit should go to Gauss and Euler, he's basically saying they don't deserve credit because they didn't invent the mathematics they use to do their work."
MachineLearning,3bxb6q,PM_ME_UR_OBSIDIAN,21,Fri Jul 3 02:42:28 2015 UTC,"Are you in academia?  Attribution is an enormously important part of research. When it's done well, you can read a paper and have the impression that the author didn't actually come up with anything worth mentioning. That's because the work is properly contextualized; ""you could have come up with it"" is the hallmark of well-reported research."
MachineLearning,3bxb6q,sieisteinmodel,11,Fri Jul 3 05:43:29 2015 UTC,"‘You have to realize that deep learning — I hope you will forgive me for saying this — is really a conspiracy between Geoff Hinton and myself and Yoshua Bengio, from the University of Montreal’   — Yann LeCun  http://www.wired.com/2013/12/facebook-yann-lecun-qa/"
MachineLearning,3bxb6q,dwf,1 point,Fri Jul 3 09:52:11 2015 UTC,"As far as the rebranding of such algorithms as ""deep learning"", the reinvigoration and redirection of the community towards this kind of inquiry (notably trying to convince the wider machine learning, as well as speech & vision, audiences that such algorithms were worth another look) he's absolutely correct."
MachineLearning,3bxb6q,sieisteinmodel,3,Sat Jul 4 02:20:21 2015 UTC,"Granted, but that is not what he is saying. And it's certainly not what people are reading--which is what matters in the end."
MachineLearning,3bxb6q,sanity,20,Sat Jul 4 16:55:01 2015 UTC,Maybe he has a right to be bitter.
MachineLearning,3bxb6q,JanneJM,9,Fri Jul 3 00:24:49 2015 UTC,"I think he has some legit points there, and scientists should do science for the sake of science; making discoveries, being humble and honest. This is not a business."
MachineLearning,3bxb6q,quiteamess,27,Fri Jul 3 01:16:14 2015 UTC,"Except it is a business. A research lab is effectively a small company run by its owner/founder the PI. It needs to produce a competitive product (research papers) in order to earn income from its customers (the funding agencies). To do that they need to attract and retain good workers, keep an eye on competing labs/companies and so on. You have all the same issues with budgeting, personnel management and so on as a real company.  And it's not a joke for those involved. If you and the lab doesn't get credit for a piece of work you did, you've lost an important product. And since funding is very tight in many fields (10-15% rate in neuroscience) that really can make the difference between getting a grant or not.  And for the people working on the project that is the difference between having a salary to pay for a place to live and food for you and your family; or not having it and having to move to a different city or country, or give up and leaving research.   Science is not a vocation. It is not a calling. For the people doing it it is a career, a livelihood, a way to make ends meet. Please never forget that."
MachineLearning,3bxb6q,JanneJM,-2,Fri Jul 3 01:35:03 2015 UTC,"Except it is a business. A research lab is effectively a ...   This is a circular premise. If you apply the mental model of business  to science then science is business. But this is tautological. If people like you get into positions for granting research money then it indeed becomes a business. But it is science and not business, that's why you make a distinction.   So where does the money come from? It comes from rich people. If you want rich people to dictate the rules and their logic to science then this is your choice. But there are other choices. Science could be a 'winner-take-all' business. Or it could be a proper credit distribution, like Juergen is arguing for. If you just say that science is business you are neglecting this distinction in the first place."
MachineLearning,3bxb6q,quiteamess,6,Fri Jul 3 10:36:23 2015 UTC,"This is a circular premise. If you apply the mental model of business to science then science is business.   I understand what you're saying. But - and I was woefully unclear about this in my comment - that was not what I was trying to say.  I'm not saying science is a business. I'm trying (and failing) to say that running a research lab is a business. You have employees, you create a product, and the product determines if you get any income to continue running your venture. Fail to create stuff of sufficient quality and demand and your lab will fail as well. And since things such as citations critically determine the (perceived) quality of your output, attribution is not a small matter.  The ones with the power to change this dynamic are of course the people with the money. But even if that does change - and I hope it will, somehow - the business bit still stands. No matter what else it is, science is also the primary income for most researchers, and very few of us could afford to do science without getting paid to do so.   When the rubber hits the road (the beaker hits the bench?), you are still dealing with a typical employer/employee relationship, driven by things like income, career prospects, employee evaluations and all the other things that money brings with it."
MachineLearning,3bxb6q,mare_apertum,-1,Fri Jul 3 12:31:34 2015 UTC,"I agree that you have to think economically to run a lab. And I also think that this a good thing at the end. Competition may be a factor which drives scientific progress. But scientific communities should be able to have their own value system, independent of the rest of society. This is what impact factors try to measure. At least in theory these two measures should be kept separate, although they influence each other in reality."
MachineLearning,3bxb6q,heptode,7,Fri Jul 3 12:59:48 2015 UTC,"Welcome to capitalism. Here, everything is business. You could have made just the same comment about sports, music or fine arts..."
MachineLearning,3bxb6q,throwaway0x459,8,Fri Jul 3 05:49:53 2015 UTC,"It's a vicious cycle. People dismiss Schmidhuber because he is too bitter to take seriously, and Schmidhuber becomes more bitter because people dismiss him. He'd be much more influential if he wasn't so antagonistic."
MachineLearning,3bxb6q,sieisteinmodel,3,Fri Jul 3 02:44:21 2015 UTC,"I don't think anyone familiar with Schmidhuber's work on RNNs and LSTM would be put off by him stating the facts or fixing other people's attribution problems. He's done a lot of seminal, groundbreaking work; you can't help but take him seriously."
MachineLearning,3bxb6q,throwaway0x459,3,Fri Jul 3 21:59:37 2015 UTC,"LeCun et al. also have released a lot more source code and such for reproducing their work, while Schmidhuber's group tends to dole them out a lot less willingly (and less completely).  Schmidhuber bet the wrong way on this sort of thing, and lost out."
MachineLearning,3bxb6q,eaturbrainz,5,Fri Jul 3 03:47:32 2015 UTC,"That's just not true. Back in the days Schmidhuber's group was one of the first to embrace open source. The library PyBrain (which is horribly outdated and not maintained anymore) was the place were most of the reinforcement learning work done in his groups was published. Even years later, I don't know of any open source project that comes close to the amount of RL algorithms that is in that package.  Additionally, rnnlib (by Alex Graves) was the first openly available RNN software, along with PyBrain!  Then, some is not openly available. But that's the case for code from Lecun's lab as well. Even worse, Facebook is publishing code with a super evil backdoor license, which is even worse than not publishing it.  (Disclaimer: I was part of the PyBrain team 5 years ago.)"
MachineLearning,3bxb6q,piesdesparramaos,3,Fri Jul 3 18:54:00 2015 UTC,"That may have been true earlier, but several of the classifiers they used to win image labeling contests have not been published in a reproducible way.  Perhaps they've since corrected this, but not that I've noticed.  I should have another look, to verify."
MachineLearning,3bxb6q,throwaway0x459,3,Fri Jul 3 22:18:21 2015 UTC,"Yeah, it would be nice if he actually released the source code to his contest-winning neural network algorithms."
MachineLearning,3bxb6q,piesdesparramaos,3,Fri Jul 3 15:52:47 2015 UTC,"Releasing code and such is of course great, but that does not mean that the guy who had the idea should not receive credit for it. As Schmidhuber says in his article, popularizing the use of something and coming up with the idea are different things."
MachineLearning,3bxb6q,corebiz,2,Fri Jul 3 17:11:20 2015 UTC,"Sure, but if you're doing ML (or CS in general), and don't release source code, you have no one to blame but yourself when another group that does similar work and releases code gets more attention.  And that's exactly how it should be.  If you want to maximize your payoff from being secretive, go into business, not academia.  And, although it's correct that discovery and popularization are different things, the popularizers, in this case, are enabling a lot more follow-on research than the discoverer, so deserve more credit, IMO.  Schmidhuber has no one to blame but himself.  If his group released code as freely as the ones that he's railing against, he'd be getting as much attention as them.  At this point, it's probably too late."
MachineLearning,3c1nvf,TheCatelier,4,Fri Jul 3 22:59:03 2015 UTC,"You need cross validation or a hold out set to estimate what the error of your model on new data would be.  Many models will fit the training set perfectly and, indeed if your model is able to predict the error then it should be able to predict the actual value."
MachineLearning,3c1nvf,micro_cam,1 point,Fri Jul 3 23:09:20 2015 UTC,"Additionally/alternatively, if you use the closed-form solution rather than gradient-based optimzition methods, you could calculate/estimate the degree of uncertainty"
MachineLearning,3c1nvf,rasbt,1 point,Fri Jul 3 23:59:16 2015 UTC,"What problem are you trying to solve?  What do you mean by the ""reliability"" of the model, in a formal sense?"
MachineLearning,3c1nvf,alexmlamb,1 point,Sat Jul 4 07:50:34 2015 UTC,"A regression model maps features X to response variable Y. In my case, I would like more than just a response variable Y. Ideally, I would have a pdf conditional on the features.   If we make the assumption that the residuals are normally distributed, then I could model the pdf as a normal distribution with mean Ŷ and use my estimate of the variance (conditional on the features) as well."
MachineLearning,3c1nvf,iidealized,1 point,Sat Jul 4 15:23:27 2015 UTC,"This is called ""boosting"" if you are trying to improve your predictions via such a strategy.  No idea why you think training the model on the residuals (error) would give you reliability estimates."
MachineLearning,3c1nvf,georgeo,1 point,Sat Jul 4 12:17:59 2015 UTC,See my response above where I clarify my goal. I'm not trying to improve predictions.
MachineLearning,3c1nvf,svantana,1 point,Sat Jul 4 15:24:07 2015 UTC,ARCH models
MachineLearning,3bygn5,robertsdionne,2,Fri Jul 3 04:50:03 2015 UTC,abstract http://arxiv.org/abs/1506.08473
MachineLearning,3c0exb,AWKWARD_HANDS_GUY,10,Fri Jul 3 17:06:53 2015 UTC,"We spend a lot of time thinking, talking, coffee-ing. White-boards everywhere. We devote time to reading up on latest research and methodologies. We even end up issuing contract work for researchers that really interest us. We break the problem into smaller components, and different solutions. Then we structure and clearly define each of the competing ""possible solutions"". We look for common components to each solution.  We also agree on a universal set of features compatible with each solution and create a rig that can help us extract and format said features.    We pick a solution and continue.  Everyone does their part. The manager,generally more experienced in technical matters, knows that some need more motivation than others to complete the project so the manager ends up doing all the bitch-work(boring boiler plate coding) in a way that enables us to get work done.    Needing to produce billable projects is a recipe for disaster.  Our job is to do cool things with data and sell confidence to management and the rest of our firm. We end up creating pre-production level prototypes in the span of 1-2 months. If our results are positive, then we push for integration in the firm and spend time creating necessary modules for that to happen.    There is no magic structure. Our team shrinks and grows all the time. If we find that researcher xyx writes interesting papers, we sometimes get away with contracting them for the sole purpose of having prolonged discussions about his/her work.    I'm never very sold on six-sigma terminology."
MachineLearning,3c0exb,liz_lemma,2,Fri Jul 3 17:58:01 2015 UTC,"Your situation sounds quite ideal, but also like the exception to the rule. You seem to have a great deal of freedom to explore novel ideas. How  big is your team currently and what sort of hardware resources do you have at your disposal? Are you in tech or finance?  Also, FUCK SIX SIGMA. The only brilliant thing about it is that a bunch of shills make money selling that garbage to mindless MBAs."
MachineLearning,3c0exb,nested_dreams,1 point,Sat Jul 4 20:39:09 2015 UTC,your team sounds alpha. how many PhDs do I need to get an interview? :[
MachineLearning,3c03dq,shubphotons,4,Fri Jul 3 15:36:16 2015 UTC,"What are your goals with getting involved in machine learning? Also. ML is not a design pattern or best coding practice.  Its the synthesis of mathematical,statistical and algorithmic thought. If you're weak there then you may not understand how to approach a problem on your own in the future.      You have to understand the underlying math. Understand calculus,  Learn linear algebra, differential equations, stats, signal processing,possibly control theory or signal processing theory, convex optimization, stochastic processes, etc.    You can't skip more than 6 posts on this subreddit at any given time before you run into something that teaches  you ML concepts. So you might be in trouble if you're not willing to do things on your own.     Oh look http://nixonite.github.io/open-source-machine-learning-degree/ Literally 4 posts down."
MachineLearning,3c03dq,123A321,4,Fri Jul 3 16:14:36 2015 UTC,Is it not more like   from sklearn import model  import pandas as pd   data = pd.read_csv('data.csv')  model.fit(data)  ... I'm obviously not 100% serious but I have to say I've run models before without understanding the whole science behind it. It does help though!   Edit: gotta get my pseudo code right...
MachineLearning,3c03dq,der_luke,1 point,Fri Jul 3 20:07:04 2015 UTC,"Actually I am a Physics Undergrad so I am okay with Math, not a genius but yeah given little time I am capable to understand the concepts. My goal is simple, have fun while learning something, maybe make a few movie/song/book predictor and such toy projects. If need be I can have this as a professional skill"
MachineLearning,3c03dq,Sorcerysoft,1 point,Mon Jul 6 16:37:17 2015 UTC,Looking for a more Hands On/Project Driven rather than theory based. I mean sure I would love to read theory along but I do not wish to get lost in it
MachineLearning,3c03dq,ThrowawayTartan,3,Mon Jul 6 16:42:12 2015 UTC,"scikit-learn is a great Python ML library with excellent documentation and examples.  http://scikit-learn.org  Intro to scikit-learn (I), SciPy2013 Tutorial, Part 1 of 3 https://www.youtube.com/watch?v=r4bRUvvlaBw  Intro to scikit-learn (I), SciPy2013 Tutorial, Part 2 of 3 https://www.youtube.com/watch?v=hlaMiXCRxB0  Intro to scikit-learn (I), SciPy2013 Tutorial, Part 3 of 3 https://www.youtube.com/watch?v=XS4TIGe7MaU  Jake VanderPlas - Machine Learning with Scikit-Learn (I) - PyCon 2015 https://www.youtube.com/watch?v=L7R4HUQ-eQ0  Search youtube for scikit learn and you will see many more tutorials."
MachineLearning,3c03dq,der_luke,2,Sun Jul 5 23:31:00 2015 UTC,"Did you try googling or reading the sidebar ? Because there's like stuff everywhere.   Also I'd say that ml toy projects aren't hard to make up. Make up a dataset generator in Python, then just run analysis on it."
MachineLearning,3c03dq,watersign,1 point,Fri Jul 3 15:58:48 2015 UTC,If you want some incentive I'd suggest giving kaggle a go.
MachineLearning,3c03dq,tod315,1 point,Fri Jul 3 20:09:51 2015 UTC,download weka or knime and run data through some algorithms just to see how they work. alot of people say oh go code this and that but why not just try classifying some toy data sets and seeing how it all works?
MachineLearning,3byk8a,robertsdionne,1 point,Fri Jul 3 05:25:07 2015 UTC,"Another related work of interest comes from David Blei's group, which proposes it for stochastic variational inference [1]. It takes an active learning approach in order to learn the sampling distribution. There are lots of exciting ideas on subsampling schemes for stochastic gradient descent in general.  [1] https://drive.google.com/file/d/0BwY-r_90KHY4SDVBRUZUQ3Rkb2ZQVl9STzcyb0NRTnJPMW9R/view"
MachineLearning,3bxfha,Nixonite,4,Thu Jul 2 23:13:22 2015 UTC,Hopefully someone might like it as much as I liked the data science masters page.
MachineLearning,3bxfha,bridgebywaterfall,3,Thu Jul 2 23:20:26 2015 UTC,"FYI -- the book by Grinstead ""Introduction to Probability"" was coauthored by Laurie Snell. Might be nice to include both authors.   Also, Lloyd Trefethen wrote a book (with David Bau) on numerical linear algebra which was recommended to me. That might make a nice addition to the page as well if you're looking to add more books on numerical linear algebra."
MachineLearning,3bxfha,rbxpecp,3,Fri Jul 3 01:51:16 2015 UTC,"I tried REALLY hard to find a numerical linear algebra book on google but I listed the only one that I could find. My google-fu is not strong enough, perhaps you can give it a shot and let me know if you find a link?   I'm trying to keep it all free, and avoiding torrent links for legal reasons.   I will fix that author issue, thanks for the note.   Seriously though, free pdfs of numerical linear algebra books are surprisingly rare on the internet."
MachineLearning,3byvco,jonej,3,Fri Jul 3 07:24:20 2015 UTC,"""Looking three to five years out, we expect to see far higher levels of artificial intelligence, as well as the development of distributed autonomous corporations.""  This seems much farther away than 3-5 years..."
MachineLearning,3byvco,jlas_,3,Fri Jul 3 12:51:03 2015 UTC,how are they going to sell consulting services to people unless they make it sound imminent?  :)
MachineLearning,3byvco,lozj,3,Fri Jul 3 23:36:57 2015 UTC,"""C-level executives will best exploit machine learning if they see it as a tool to craft and implement a strategic vision.""  Sorry, there is too much McKinsey-speak in that article for me to take."
MachineLearning,3byvco,inarrears,2,Fri Jul 3 10:00:23 2015 UTC,"lol..i think its gonna be a solid 10 years before ML is making big waves in traditional industries. management is stuck in their old ways at alot of these big corps and they dont want to take risks since they know shits hitting the fan economically. the only companies that are really truly doing anything with machine learning are tech firms like google, fb and amazon...and maybe some leading P&C insurance companies like Progressive."
MachineLearning,3bu55q,_bskaggs,44,Thu Jul 2 04:20:18 2015 UTC,I made this with Google's recently published Inceptionism code.  It's very easy to use - these pictures were made with zero tweaking.   Edit: http://imgur.com/gallery/lQLf3eS
MachineLearning,3bu55q,twonickman,10,Thu Jul 2 04:21:21 2015 UTC,Awesome! I've been aching to torture people with images processed using details of their own face since I saw the images for the first time. I wonder how that'll go.
MachineLearning,3bu55q,Jumpy89,2,Thu Jul 2 09:48:06 2015 UTC,Just downloaded and installed everything and this is how I've been occupying my day...
MachineLearning,3bu55q,AdventureTime25,7,Sat Jul 4 01:38:21 2015 UTC,That is so cool. Thanks for the link.
MachineLearning,3bu55q,AdventureTime25,7,Thu Jul 2 04:55:41 2015 UTC,It looks like people have started using #deepdream on twitter for these.
MachineLearning,3bu55q,TDaltonC,4,Thu Jul 2 05:00:14 2015 UTC,/r/deepdream also worth looking into
MachineLearning,3bu55q,flexiverse,1 point,Thu Jul 2 15:33:03 2015 UTC,Ran it on actual advice mallard.
MachineLearning,3bu55q,NasenSpray,6,Sat Jul 4 18:11:22 2015 UTC,"Do I have to use the ""dog face generator"" settings or could I use the settings that build these lower level feature extractors too?"
MachineLearning,3bu55q,flexiverse,3,Thu Jul 2 14:06:58 2015 UTC,"I would like to know why them dogface set is the default, surely you can feed it your own images to learn from ?"
MachineLearning,3bu55q,NasenSpray,2,Thu Jul 2 19:48:34 2015 UTC,You can select different layers. Here's an overview of the different effects: http://imgur.com/a/0chFD
MachineLearning,3bu55q,Lost4468,1 point,Fri Jul 3 12:09:22 2015 UTC,How do you choose the different effects ? A -flag?
MachineLearning,3bu55q,nickl,1 point,Fri Jul 3 13:17:45 2015 UTC,You can set the name of the layer as a function parameter. There's an example for this in the code.
MachineLearning,3bu55q,agent229,1 point,Fri Jul 3 13:35:43 2015 UTC,The further down I scroll the more I feel like I'm losing my mind.
MachineLearning,3bu55q,personalityson,1 point,Fri Jul 3 17:28:08 2015 UTC,Yes. I output what each different layer is doing here: https://goo.gl/photos/NcbUuCjQo1LNrMeA6
MachineLearning,3bu55q,twonickman,3,Fri Jul 3 10:08:55 2015 UTC,"Thanks I didn't realize they made it available. Can't wait to try it :) how long does it take on a decent personal computer, assuming you use the pretrained network?"
MachineLearning,3bu55q,personalityson,2,Thu Jul 2 12:07:20 2015 UTC,Can you apply inceptionism on top of already processed images?
MachineLearning,3bu55q,kjmitch,11,Thu Jul 2 09:02:39 2015 UTC,"That is pretty much how you get the end result, so..."
MachineLearning,3bu55q,twonickman,0,Thu Jul 2 09:48:39 2015 UTC,I don't see the problem. It can't be applied twice?
MachineLearning,3bu55q,kjmitch,17,Thu Jul 2 10:30:59 2015 UTC,"These are created with a feedback loop in the recognition algorithms. It's already been applied a number of times to each image. You can apply it more times, hundreds or thousands if you like, but it's just going to recognize the faces/animals/edges/buildings/whatever that it already has and bring them out even more.   ""Applying it twice"" is literally how these images are modified and end up this way."
MachineLearning,3bu55q,twonickman,1 point,Thu Jul 2 10:35:18 2015 UTC,"Well, probably not literally twice, but many times.  I don't know how strong is each iteration, but according to Google you have to do it lots of times before it has a strong enough effect to actually see something. Doing it too many times would just add eyes to the eyes, and eventually you would not be able to see the cat you originally saw, only eyes that see you using eyes of their own.  So actually yes, you can. It has been applied more than twice to these images already but you can still go deeper. Should I try it? I'm trying to install it already. Edit: added the first line break"
MachineLearning,3bu55q,kjmitch,4,Thu Jul 2 11:09:55 2015 UTC,"Well, probably not literally twice, but many times.   That's exactly what I said; saying ""'Applying it twice' is literally how [...]"" was just my manner of using what /u/personalityson said to emphasize the point that reiteration is at the center of the algorithm. And being applied again is literally how it works, while ""twice and only twice"" obviously is not, so that's why I said the first thing and not the second. Hopefully that actually is clear in my comment, but if it's not then I'd like to know so I can improve.  As far as applying it too many times, I enthusiastically encourage you to do so. I hypothesize that, rather than more and more newly-recognized dog faces on top of dog faces (or whatever the algorithm is looking for), you'll see the first 'generation' of those generated become more pronounced and that is what will drown out the features of the original image, until the whole image looks like a Magic Eye rendering made out of distorted versions of whatever the dreaming algorithm was told to look for in the picture.   Actually, perhaps that will lead to a soup of static and noise that can then end up producing what you guessed, with fractals of generated features on generated features on generated features. Be sure to post what you get, and take samples at regular intervals; I'll see if I can do so as well."
MachineLearning,3bu55q,twonickman,-1,Thu Jul 2 11:30:43 2015 UTC,"...I was sorta kidding actually... You said ""literally"" when it wasn't literally twice, but metaphorically twice, so your use of the word was annoyingly incorrect, in the sense that you are literally killing me when you use it like that."
MachineLearning,3bu55q,twonickman,4,Thu Jul 2 11:54:53 2015 UTC,"Well, again, I didn't say it was literally any specific number that the effect was applied, but rather that the effect being applied repeatedly is ""literally how these images are modified"". That's why ""Applying it twice"" was in quotes like that: scare quotes to indicate the term was being used ironically rather than genuinely. They were also supposed to quote /u/personalityson's question to emphasize the connection to how he had asked the question.   I'm glad to know that my original meaning was clear, but it's too bad that the proximity of an ironically-used term to the word 'literally' on the page happened to trigger a response concerning such an irrelevant point. I guess the best thing here is just to spread the message of the Cautionary Ghost."
MachineLearning,3bu55q,personalityson,-1,Thu Jul 2 12:32:38 2015 UTC,You're right :(  In other news I'm having trouble compiling Caffe...
MachineLearning,3bu55q,g4n0n,1 point,Thu Jul 2 13:03:58 2015 UTC,"Oh, I got distracted. I'm still installing the dependencies. I don't even know if it has a GUI or if it's hard to use so I'm pretty much going blind into this."
MachineLearning,3bu55q,cafedude,1 point,Thu Jul 2 12:22:29 2015 UTC,Make a video:)
MachineLearning,3bu55q,Saivo,1 point,Thu Jul 2 11:33:09 2015 UTC,"To use the places pre-trained network did you have to adjust any of the other parameters? (eg. the dataset mean, etc)"
MachineLearning,3bu55q,asherp,1 point,Thu Jul 2 11:57:45 2015 UTC,Could you post some instructions for doing this?
MachineLearning,3bu55q,NasenSpray,1 point,Thu Jul 2 16:11:09 2015 UTC,"Great pics, and a great link! Didn't realize Google had published the guide."
MachineLearning,3bu55q,Lost4468,1 point,Thu Jul 2 17:50:57 2015 UTC,How long would it take to run a short film sequence through this? That would be truly terrifying.
MachineLearning,3bu55q,NasenSpray,2,Thu Jul 2 18:04:57 2015 UTC,I have ~0.5 fps with a GTX 970 on 480p images... aka it takes a while :-\
MachineLearning,3bu55q,omniron,2,Fri Jul 3 12:03:31 2015 UTC,"That's fast, from the original blogpost everyone made it sound like it takes ages on a supercomputer."
MachineLearning,3bu55q,kikkomane,1 point,Fri Jul 3 17:26:47 2015 UTC,I guess most people run it on their CPU because that's the default. You need at least 4GB VRAM on the GPU to be able to run decently sized images. 4GB isn't even enough for 1080p...
MachineLearning,3bu55q,Noncomment,1 point,Fri Jul 3 18:31:11 2015 UTC,"your definition of easy to use is different than mine... having to setup caffe, then a full scipy/numpy environment, then ipython notebook is not going well for me.  This is possibly exacerbated by the fact that i'm on the el capitan beta but no aspect of any tutorial i can find is working without issues.  I'll try on my linux box later, but it's things like this that really make me hate OSS sometimes..."
MachineLearning,3bu55q,ford_beeblebrox,17,Fri Jul 3 05:59:08 2015 UTC,All I see are dogs.
MachineLearning,3bu55q,PJvG,1 point,Thu Jul 2 07:00:30 2015 UTC,Like half of imagenet is just different breeds of dogs.
MachineLearning,3bu55q,nicecleatswannaruck,15,Sat Jul 4 00:22:40 2015 UTC,Applied to half a donut
MachineLearning,3bu55q,seventythree,5,Thu Jul 2 09:34:30 2015 UTC,That's some sick-looking alien caterpiller you got there
MachineLearning,3bu55q,normee,4,Thu Jul 2 10:02:16 2015 UTC,"That's terrifying. It's like a nightmare, or something out of Hitchhikers Guide."
MachineLearning,3bu55q,haddock420,9,Thu Jul 2 15:52:55 2015 UTC,"Number 8 is my favorite. There is washington looking stony-faced, and there is a cute dog sticking out of his head like it's perfectly natural."
MachineLearning,3bu55q,jiminiminimini,0,Thu Jul 2 09:03:46 2015 UTC,JK Rowling anticipated this
MachineLearning,3bu55q,asherp,10,Thu Jul 2 17:54:03 2015 UTC,"I wonder how it generated this one (Windows XP hill background).  The original image is just a grassy hill, but the algorithm has added things that almost look like people to it."
MachineLearning,3bu55q,jiminiminimini,20,Thu Jul 2 11:28:55 2015 UTC,"Definitely my favorite implication - the places dataset the model I used was trained on must have frequently had people in beautiful grassy fields like that one, so it just started hallucinating them."
MachineLearning,3bu55q,houdoken,9,Thu Jul 2 11:40:46 2015 UTC,"I love how people are saying that the software is ""hallucinating"" :)"
MachineLearning,3bu55q,dominosci,3,Thu Jul 2 14:58:39 2015 UTC,Isn't that an apt description?
MachineLearning,3bu55q,misch_mash,5,Thu Jul 2 15:22:21 2015 UTC,"it really is. and it makes me smile to see that the ai technology is so advanced, saying it is hallucinating is appropriate."
MachineLearning,3bu55q,twonickman,2,Thu Jul 2 16:03:11 2015 UTC,pareidolia needs a verb form.
MachineLearning,3bu55q,XalosXandrez,3,Thu Jul 2 16:09:33 2015 UTC,"Lets try ""to pareidoliate""."
MachineLearning,3bu55q,flexiverse,3,Thu Jul 2 19:07:50 2015 UTC,"Hmm. Pareidolatry. Not sure if it's sacrilege, but I like it."
MachineLearning,3bu55q,tehyosh,1 point,Thu Jul 2 20:03:09 2015 UTC,"Context isn't taken into account so much. They could have been originally walking over spaghetti, they would still have been highlighted like that."
MachineLearning,3bu55q,adzm,3,Thu Jul 2 14:50:49 2015 UTC,"This one's my favorite! It shows that the network has learn the concept of a ground plane - all the wheels and tents are hallucinated so that they properly ""sit"" on top of the grass."
MachineLearning,3bu55q,Daughter_of_Darkness,2,Thu Jul 2 12:05:20 2015 UTC,"Don't wonder it depends what images it was trained from. If you read a little about these neural nets, it's entirely dependant on what images and how long they are trained."
MachineLearning,3bu55q,nameBrandon,8,Thu Jul 2 19:46:37 2015 UTC,why are there eyes everywhere? o_o
MachineLearning,3bu55q,Daughter_of_Darkness,39,Thu Jul 2 12:06:30 2015 UTC,It gives the neural network a competitive advantage by being able to spot predators (and prey) quickly. False positives are less important than not noticing a ravenous bandersnatch before it is too late.
MachineLearning,3bu55q,DavidJayHarris,2,Thu Jul 2 12:32:59 2015 UTC,"These neural networks are not created genetically. Yes I get the joke, but there's a whole class of algorithms that are in a sense 'bred' and this is totally different."
MachineLearning,3bu55q,tehyosh,6,Thu Jul 2 18:37:07 2015 UTC,"Wrong. This is exactly what happens when we go renegade and start splicing multi-layer perceptrons with restricted boltzman machines all willy-nill with no regard for well defined hyperplanes.. we get doggly eyes. evil, evil doggly eyes.  When will we learn people.. WHEN?! You don't mess with nature.  or doggly eyes."
MachineLearning,3bu55q,vintermann,5,Thu Jul 2 18:46:02 2015 UTC,"The way these algorithms work is that they feed a large dataset of images (e.g. your photos library) into a neural network and the algorithm extracts high-res 'features' or similar components of each image. That's not anything specific to this code.  What this code does is take an image and take every small blurry pixelated region and 'guess' what that blurry region could actually be. Since the code has been tuned to substitute small pixelated regions for large features, it turns out that it 'guesses' that there are a lot of eyes everywhere."
MachineLearning,3bu55q,Freiling,3,Thu Jul 2 18:35:56 2015 UTC,"Nice answer.  Also worth noting: Assuming these networks were all trained on ImageNet, they were trained on these 1000 object classes.  A huge portion (half?) of the images would have featured some kind of vertebrate.  The network apparently learned that eyes are especially important for finding vertebrates and distinguishing among them."
MachineLearning,3bu55q,wizzahd,1 point,Thu Jul 2 20:14:31 2015 UTC,thank you!
MachineLearning,3bu55q,thirdegree,2,Thu Jul 2 18:49:40 2015 UTC,"If you look at the list that model is trained on, you'll see. It's taught to distinguish between tons of labels - but a lot of those labels are merely different dog breeds.  So in effect, the network has to become a connoiseur of dog features, all the little things that distinguish a Scottish terrier from a mastiff from a pug. It's extra sensitive to these features, which means that they get more amplified by the ""dreaming"" process."
MachineLearning,3bu55q,organizesubredditscl,11,Fri Jul 3 11:20:33 2015 UTC,"Doggy?   No, it's not a doggy, try a-   Doggy!   Okay, draw a doggy."
MachineLearning,3bu55q,chasevasic,9,Thu Jul 2 13:25:19 2015 UTC,I feel like this is what the world must look like after taking too much acid.
MachineLearning,3bu55q,s505,12,Thu Jul 2 07:40:11 2015 UTC,"Disturbingly similar, actually. #9 in particular reminds me of a nice, mild trip."
MachineLearning,3bu55q,andreasblixt,5,Thu Jul 2 08:32:53 2015 UTC,"LSD shuts down the reception of some serotonin, which is usually received to deaden consciousness during REM cycles where dreaming occurs, while receptions that usually only happen during dreaming continue. So, the analogy that these images are equivalent to Google's dreaming is apt."
MachineLearning,3bu55q,NasenSpray,1 point,Thu Jul 2 13:02:30 2015 UTC,It's not too far off from what psychedelics can do visually .   Or so I hear
MachineLearning,3bu55q,dogedickguy,7,Thu Jul 2 17:46:47 2015 UTC,I love how the Salvador Dalí was probably the least affected out of all of them... Dalí's got the surrealism already figured out
MachineLearning,3bu55q,elev57,5,Thu Jul 2 15:22:58 2015 UTC,Very nice! How long does it take to run on a single image?
MachineLearning,3bu55q,personalityson,2,Thu Jul 2 14:08:00 2015 UTC,480p image takes ~2s on a GTX 970
MachineLearning,3bu55q,Daughter_of_Darkness,9,Fri Jul 3 12:05:21 2015 UTC,"I love the dali surrealism one (forgot original name), it's like double surreal with inceptionism applied"
MachineLearning,3bu55q,PJvG,3,Thu Jul 2 06:27:48 2015 UTC,"""Persistence of Memory"", I think."
MachineLearning,3bu55q,adzm,5,Thu Jul 2 13:12:38 2015 UTC,"As a side note, google is using Caffe?"
MachineLearning,3bu55q,bitcloud,3,Thu Jul 2 10:31:44 2015 UTC,"Probably the only way to open source this code. In general, no they have their own crazy highly tuned stuff."
MachineLearning,3bu55q,nicecleatswannaruck,3,Thu Jul 2 18:31:07 2015 UTC,"The eyes, they are everywhere!"
MachineLearning,3bu55q,autowikibot,5,Thu Jul 2 10:04:10 2015 UTC,The neural network must have pareidolia like us!
MachineLearning,3bu55q,dafragsta,2,Thu Jul 2 12:30:32 2015 UTC,Now i just think this is a glorified photo mosaic.
MachineLearning,3bu55q,CaptainHoek,2,Thu Jul 2 09:16:47 2015 UTC,"Faces, eyeballs, spiny shell fish, and dogs. These are terrifying."
MachineLearning,3bu55q,plbogen,1 point,Thu Jul 2 15:27:37 2015 UTC,"Salvia divinorum:       Salvia divinorum (also known as Diviner's Sage,  Ska María Pastora,  Seer's Sage,  and informally by just its genus name, Salvia) is a psychoactive plant which can induce ""visions"" and other hallucinatory experiences. Its native habitat is in cloud forest in the isolated Sierra Mazateca of Oaxaca, Mexico, where it grows in shady and moist locations.   The plant grows to over a meter high,  has hollow square stems, large leaves, and occasional white flowers with violet calyxes. Botanists have not determined whether Salvia divinorum is a cultigen or a hybrid; native plants reproduce vegetatively, rarely producing viable seed.      Image i     Relevant: Legal status of Salvia divinorum | Daniel Siebert | Salvinorin A | Oneirogen   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
MachineLearning,3bu55q,flexiverse,1 point,Thu Jul 2 11:03:01 2015 UTC,"I've been thinking ""Use it on Scream and Starry Night"" for the past few weeks. They have such long smeared strokes that it's perfect for this kind of thing."
MachineLearning,3bu55q,EPYJAO,1 point,Thu Jul 2 11:03:36 2015 UTC,Dogs. Dogs everywhere.
MachineLearning,3bu55q,flexiverse,1 point,Thu Jul 2 13:27:32 2015 UTC,"Huh, we're calling it ""Inceptionism"", ok.  One of my teammates fed his Kayaking photos through it.   Truly disturbing."
MachineLearning,3bu55q,-Jason-,1 point,Thu Jul 2 13:47:13 2015 UTC,"Pissed off really, no way this will install on my osx 10.7 macbook, think it needs CUDA the graphics card can't handle it."
MachineLearning,3bu55q,hellosilly,2,Thu Jul 2 16:57:36 2015 UTC,"You can compile Caffe to be CPU only, the instructions are on the install page. Hopefully that helps you."
MachineLearning,3bu55q,omniron,1 point,Thu Jul 2 19:44:45 2015 UTC,"I did notice that, still have to upgrade from 10.7.     Real pain really just need to get a decent new laptop."
MachineLearning,3bu55q,flexiverse,1 point,Thu Jul 2 21:19:32 2015 UTC,"Everything looks inspired by Alex Grey, the artist for some of Tool's album covers / visuals. All those doge eyes."
MachineLearning,3bu55q,TDaltonC,1 point,Thu Jul 2 21:55:42 2015 UTC,There's something hideously creepy about #11. Such a disturbing image.
MachineLearning,3bu55q,MrPapillon,1 point,Thu Jul 2 20:55:46 2015 UTC,"Their NN seems to love eyes and faces, kinda weird. I wonder if it is just overfitting on eyes, or did they pull these visualizations from an eye-heavy layer?"
MachineLearning,3bu55q,verandaguy,1 point,Thu Jul 2 21:04:34 2015 UTC,"Since I can't install this just yet, can anyone explain why it's using the dog recognition data? Is this data freely available or something ?"
MachineLearning,3bu55q,CreativePunch,1 point,Thu Jul 2 21:56:16 2015 UTC,"Hey /u/_bskaggs quick question. I'm trying to get this up and running but I get this error on the second cell: ""RuntimeError: Could not open file ../caffe/models/bvlc_alexnet/bvlc_alexnet.caffemodel""  And when I check the folder, the file that it's trying to load isn't there. Any idea what I'm doing wrong?"
MachineLearning,3bu55q,JonnyLatte,1 point,Thu Jul 2 23:41:32 2015 UTC,The Singularity will be painful...
MachineLearning,3bw1xr,downtownslim,3,Thu Jul 2 16:52:49 2015 UTC,Wow they did not mess around with the maths in that paper. Can anyone give a slightly more intuitive explanation?  EDIT: Things that clarified the paper for me:   Refresher on the specifics of whitening Refresher on the specifics of batch normalisation
MachineLearning,3bw1xr,0h_Lord,4,Thu Jul 2 20:02:46 2015 UTC,Honestly it makes me feel depressed that after putting in a reasonable amount of effort many of this kind of stuff is still over my head.  Do machine learning researchers/professionals understand math in papers like this at the first read?
MachineLearning,3bw1xr,question99,4,Thu Jul 2 21:04:02 2015 UTC,"Most of the math can at least be understood (though I don't claim to be a master of it yet) if you are familiar with the research area/prior work. The authors on this one usually publish fairly math dense stuff, but it is strong math that is really useful!. The high level description is right there in the abstract:   """" ... speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix.  In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. """"  Basically this is an alternative (much more theoretically grounded) approach to something like batch normalization, but it seems to be even faster and better. Results on autoencoders and CIFAR10 are really promising, and ImageNet results while preliminary seem to extend upon that promise. I bet we will see more about this soon.  It is fairly math heavy, but the topics are all related to ""hot topics"" in deep network optimization. Like all else in math, you just get used to it after a while and relate it to other concepts."
MachineLearning,3bw1xr,kkastner,1 point,Thu Jul 2 21:53:44 2015 UTC,"To add: they provide in their references some good overviews.   There is also a very good roadmap provided by one of the researchers on the forefront these methods (natural gradient).   https://www.metacademy.org/roadmaps/rgrosse/dgml/version/6  Although, as other people point out, don't expect to learn this stuff overnight. A lot of the references they point to require a certain level of mathematical maturity."
MachineLearning,3bw1xr,iamkx,2,Fri Jul 3 03:42:28 2015 UTC,"If you don't have graduate-level experience reading/comprehending papers, you're probably going to need to 'train' a little bit. Print the paper out, get out a pen and start marking it up. By the end you should have a list of further questions/clarifications and a better understanding of what's going on. For an intuitive look, focus on the abstract/conclusion to get a high level sense of what's going on."
MachineLearning,3bw1xr,caughtinthought,1 point,Thu Jul 2 22:10:12 2015 UTC,"It's not math, as in it's not just being able to follow the equations. I can bet an expert mathematician wouldn't be able to follow this math, if, (s)he has not studied information theory.  I think a very good criteria for being able to follow this is if you've been through Cover-Thomas and understood at least half of it."
MachineLearning,3bw1xr,physixer,0,Thu Jul 2 22:57:24 2015 UTC,"The underlying idea is quite simple once you cut through the math. Anyone who's done machine learning knows that you should normalize your data before doing any kind of modeling (remove mean, and pair-wise correlations). This algorithm essentially does the same thing for all the hidden layers of the MLP, but in a way which doesn't interfere with gradient descent."
MachineLearning,3bw1xr,potatomasher,2,Fri Jul 3 09:14:32 2015 UTC,Here's another method of speeding up convergence for SGD with SGD:  Accelerating Stochastic Gradient Descent via Online Learning to Sample http://arxiv.org/abs/1506.09016  Here's guaranteed generalization error bounds with polynomial training of two-layer neural networks with a tensor method:  Generalization Bounds for Neural Networks through Tensor Factorization http://newport.eecs.uci.edu/anandkumar/pubs/NN_GeneralizationBound.pdf
MachineLearning,3bw1xr,robertsdionne,1 point,Fri Jul 3 04:48:38 2015 UTC,Access denied.  Do I need to be on a university network?
MachineLearning,3bw1xr,UnluckyPenguin,1 point,Thu Jul 2 20:40:33 2015 UTC,"Not having that problem, here's a direct link to the pdf"
MachineLearning,3bw1xr,siblbombs,3,Thu Jul 2 20:45:57 2015 UTC,"It could be my company's firewall. I am able to access it on the university network just fine.  Here's what I saw earlier:    Access Denied  Sadly, you do not currently appear to have permission to access http://arxiv.org/pdf/1507.00210v1.pdf  If you are using the PDF Plug-in, it has many bugs and is forbidden here due to problems it causes at the server end. You must confirm that you have disabled it before access can be restored.  In Netscape try Edit -> Preferences -> Navigator -> Applications, look for Portable Document Format and uncheck the plug-in box. Or delete the pdf plugin dll file from the Program Files/Netscape/Navigator/Program/plugins directory and restart browser. Or for Acroread4/Explorer5 users, go into Acroread's File : Preferences : General : Web_Browser_Integration and make sure the little box is unchecked.  Note to MacOSX users: There is a bug in the Acrobat reader which causes it to make endless streams of requests after having successfully downloaded the full pdf. Note that it is not necessary to use Acrobat at all, since pdf's from here render as well or better in the default Preview.app on MacOSX. If for some reason you think you need to use Acrobat, go to Acrobat Preferences -> Internet and turn off the ""Allow speculative downloading in the background"" option, which comes (incorrectly) turned on by default, and whose behavior is quite broken.  If you believe this determination to be in error, see http://arxiv.org/denied.html for additional information."
MachineLearning,3bw1xr,UnluckyPenguin,1 point,Thu Jul 2 22:02:37 2015 UTC,"That is bizarre, Arxiv is the one blocking you so I don't think it is a firewall issue. For some reason Arxiv doesn't like the IP address you are coming from."
MachineLearning,3bw1xr,siblbombs,1 point,Thu Jul 2 22:06:44 2015 UTC,Read the text in the error. They want you to turn off the PDF plugin.
MachineLearning,3bw1xr,cafedude,1 point,Fri Jul 3 00:28:10 2015 UTC,"Doesn't necessarily have anything to do with that. I don't have any PDF plugin (my Firefox just downloads PDFs), but I get that error message when I happen to be connecting over Tor."
MachineLearning,3bw1xr,gwern,1 point,Fri Jul 3 02:26:01 2015 UTC,"I got exactly the same error using Firefox, worked fine with Chrome."
MachineLearning,3bz4dj,spurious_recollectio,1 point,Fri Jul 3 09:18:20 2015 UTC,"There's ""Zero-Shot Learning Through Cross-Modal Transfer"" of course, which I assume you know about. That's going the other way though.  I had an idea Baidu had published something around captioning unknown images, but I can't find it now.  Edit: The Baidu paper is ""Learning like a Child:Fast Novel Visual Concept Learning from Sentence Descriptions of Images"" http://arxiv.org/pdf/1504.06692v1.pdf"
MachineLearning,3bz4dj,nickl,1 point,Fri Jul 3 10:30:02 2015 UTC,I didn't know about either of these papers and they look quite helpful so thanks!
MachineLearning,3bz4dj,nickl,1 point,Fri Jul 3 13:44:42 2015 UTC,I think the state of the art in NLP is more around what Facebook is doing with memory networks though. Eg: https://research.facebook.com/publications/1600512920197684/end-to-end-memory-networks/ and https://research.facebook.com/publications/1454559778197013/learning-longer-memory-in-recurrent-neural-networks/
MachineLearning,3bz0no,stua8992,2,Fri Jul 3 08:29:08 2015 UTC,"for SVM you should normalize each dimension separately, so it doesn't matter whether you concatenate first or not."
MachineLearning,3bz0no,sdsfs23fs,1 point,Fri Jul 3 13:04:36 2015 UTC,"Right, that makes a lot more sense. Just for future reference does this still work well if your histograms come from different sized images in which case an identical but upscaled image could have each histogram bin multiplied by some constant (very roughly speaking)? Presumably you would still go by feature but would also normalise each histogram as a whole first if that were the case?"
MachineLearning,3bvr3n,personalityson,1 point,Thu Jul 2 15:28:57 2015 UTC,What will happen if you feed it a regular image of a dog?
MachineLearning,3bz3yr,tushar1408,2,Fri Jul 3 09:12:33 2015 UTC,"I would suggest giving us a few examples of what a ""binarized constituency parse tree"" is. It's not clear to me, at least."
MachineLearning,3bz3yr,Coneylake,2,Sat Jul 4 00:35:30 2015 UTC,hi thanks for reply.  https://en.wikipedia.org/wiki/Branching_(linguistics)  if you could please refer to binary n-ary section of this link that will be cool.
MachineLearning,3bz3yr,autowikibot,1 point,Mon Jul 6 11:00:08 2015 UTC,"Branching (linguistics):       In linguistics, branching refers to the shape of the parse trees that represent the structure of sentences.  Assuming that the language is being written or transcribed from left to right, parse trees that grow down and to the right are right-branching, and parse trees that grow down and to the left are left-branching. The direction of branching is a reflex of the position of heads in phrases, and in this regard, right-branching structures are head-initial, whereas left-branching structures are head-final.  English has both right-branching (head-initial) and left-branching (head-final) structures, although it is more right-branching than left-branching.  Other languages such as Japanese and Turkish are strongly left-branching (head-final).    Image i     Relevant: Phonetics | Word order | Encyclopedia of Language and Linguistics   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Call Me"
MachineLearning,3bz3yr,OliverOnTheWeb,0,Mon Jul 6 11:00:40 2015 UTC,I find this can be a great resource for this kind of project.
MachineLearning,3by71k,bit_by_byte,3,Fri Jul 3 03:22:25 2015 UTC,"It is a bit of an art, although images are bit easier, as there are more examples to crib from.  I would strongly recommend starting out by trying out various ML toolkits, and reproducing results for a few well known datasets, ie MNIST, CIFAR, etc, and maybe tweaking various parameters to get a feel for their impacts.  That way, you can also see which tools you like, and get some speed estimations.  I hear Caffe is solid and fairly easy to use.  Personally, I use Theano and Keras, but I'm more on the research and theory side, so I like the math to be fairly visible.  Your dataset sounds like it's a reasonable size, so I would strongly consider using software that supports GPU acceleration, the differences in speed/price are very significant.  Out of curiosity, how many actual images do you have?  Are they all at the same resolution?"
MachineLearning,3by71k,melvinzzz,1 point,Fri Jul 3 05:19:33 2015 UTC,"There are 53,577 images in the training set and 35,127 images in the test set. I ran a script to get a histogram like output for the distribution of pixels-per-image across the data set...Here's a brief summary of the image stats:  Note : None of the images are square, but all of them are close to it.    avg pixels per image = 9,367,653 -- 28 million features = 3060x3060 images min pixels = 125,137 -- 375k features = 354x354 images max pixels = 17,915,904 -- 54 million features = 4233x4233 images     Training set information: ( *   =   majority )   21 images between 125,137 - 126,000* pixels 422 images between 1,945,600 - 2,121,728* pixels 4192 images between 4153344 - 4915200 pixels (pretty even distribution) 7603 images between 4915200 - 8000000 pixels (lots around 5 mil) 14560 images between 8 mill - 12 mill 8308 images between 12 mill - 18 mill   Most images were at least 2 million pixels, the largest grouping between 8 and 12 million, as expected given the 9.4 million average."
MachineLearning,3by71k,farticulatematter,2,Tue Jul 7 03:25:10 2015 UTC,This helped me: http://neuralnetworksanddeeplearning.com/
MachineLearning,3by71k,MusicIsLife1995,2,Fri Jul 3 04:17:56 2015 UTC,These are exactly what I want to know as well.
MachineLearning,3by71k,Bhavishya1,2,Fri Jul 3 18:25:10 2015 UTC,"This links should help. NN resources-- https://datascience.stackexchange.com/questions/2651/deep-learning-basics/6024#6024 Selecting hyperparameters and layers and number of nodes-- http://yyue.blogspot.in/2015/01/a-brief-overview-of-deep-learning.html  Use theano. It has automatic differentiation(means that you won't have to calculate the backprop gradients by hand). It is a bit hard to learn, but I found it worthy.  You can use ConvNets for images. They have proven very successful for images. You will need a Fast gpu for really doing some experimentation."
MachineLearning,3by71k,Coneylake,1 point,Sat Jul 4 10:34:34 2015 UTC,"Thanks for the links, these seem like they're exactly what I was liking for. That stackexchange link is a goldmine of resources."
MachineLearning,3by71k,syllogism_,1 point,Tue Jul 7 08:16:14 2015 UTC,"I recommend learning how the back propagation algorithm works first and try implementing it in, say, Python. This resource (that somebody else posted) is also nice to start learning back propagation: http://neuralnetworksanddeeplearning.com/"
MachineLearning,3bvi3u,Kombutini,3,Thu Jul 2 14:17:23 2015 UTC,Very nice.
MachineLearning,3bvi3u,Foxtr0t,2,Thu Jul 2 16:28:25 2015 UTC,Why does it zoom in? Is this a setting ?
MachineLearning,3bvi3u,flexiverse,3,Thu Jul 2 20:08:03 2015 UTC,"Without zooming it tends to pic out specific features and accentuate them until an equilibrium point is reached, which makes for less interesting gifs, so adding a scaling transformation keeps things interesting."
MachineLearning,3bvi3u,flexiverse,1 point,Fri Jul 3 15:21:31 2015 UTC,So how do you zoom? Just scale in a few pixels?
MachineLearning,3bvi3u,WiggleBooks,2,Fri Jul 3 16:33:14 2015 UTC,"I think this would be better recepted in the gif subreddits, or maybe even /r/woahdude"
MachineLearning,3bvi3u,sinhue,1 point,Thu Jul 2 22:04:15 2015 UTC,show me the code!
MachineLearning,3bvi3u,siblbombs,3,Thu Jul 2 14:53:33 2015 UTC,github
MachineLearning,3brpre,thirtymindin,30,Wed Jul 1 16:42:12 2015 UTC,What if google is patenting racism and plans on keeping the license fees high?
MachineLearning,3brpre,obscure_robot,13,Wed Jul 1 18:26:01 2015 UTC,Trading race warfare for class warfare. Stock up now on caviar and European car parts.
MachineLearning,3brpre,StannisBaratheon_AMA,2,Wed Jul 1 19:47:55 2015 UTC,Thinking through the details of that idea produces disturbing results.
MachineLearning,3brpre,obscure_robot,3,Wed Jul 1 20:06:43 2015 UTC,Then only the rich will be able to afford being racist.
MachineLearning,3brpre,beaverteeth92,8,Thu Jul 2 00:45:57 2015 UTC,"As I understand, the Google image identification isn't well trained on humans. If they were doing something like playing a sport, it might be able to detect the sport, like it does graduation, but not people's faces by themselves."
MachineLearning,3brpre,TheRealDJ,7,Wed Jul 1 18:38:01 2015 UTC,Graduation is my favourite sport.
MachineLearning,3brpre,HuhDude,72,Thu Jul 2 10:22:12 2015 UTC,"People, this is not racism, for fuck's sake. It's a honest mistake of the algorithm, the similarity of human faces and gorillas isn't that crazy."
MachineLearning,3brpre,2Punx2Furious,58,Wed Jul 1 21:11:53 2015 UTC,There may have been an unintentional bias toward Caucasian faces in the training data
MachineLearning,3brpre,teambob,21,Thu Jul 2 01:02:18 2015 UTC,Valid point: indirect unintentional racism.
MachineLearning,3brpre,InformationCrawler,0,Thu Jul 2 05:46:23 2015 UTC,"Even if a person said that black people look more like gorillas than white people, it wouldn't be racist. That has nothing to do with race superiority, at all."
MachineLearning,3brpre,awhaling,1 point,Fri Jul 3 21:03:10 2015 UTC,Relevant TV episode
MachineLearning,3brpre,OldZeroProg,29,Fri Jul 3 06:39:22 2015 UTC,"No one is saying Google is being deliberately racist, but it's a pretty awkward situation considering the history of eugenics comparing blacks to apes."
MachineLearning,3brpre,manbearkat,5,Wed Jul 1 23:32:00 2015 UTC,"Funnily enough all of us humans are apes, but I get what you mean."
MachineLearning,3brpre,AnOnlineHandle,7,Thu Jul 2 06:56:45 2015 UTC,Actually a lot of people -are- claiming racism.  It's soooo stupid
MachineLearning,3brpre,MyMomSaysImHot,-15,Thu Jul 2 01:17:50 2015 UTC,"It's really not stupid at all. If your service labels black people as gorillas, then that is racist. At the end of the day it doesn't matter whether its people or computers that are doing the labeling. The end result is what matters."
MachineLearning,3brpre,woohalladoobop,8,Thu Jul 2 05:25:50 2015 UTC,"I agree the algorithm isn't racist, but it does reflect the inherent problems with lack of diversity in tech. The recent transparency reports by Google show that only 2% of their employees are black. While I can't be certain, I'd guess that if they had more black employees, they would have had a greater chance of surfacing this issue when testing on their own images.   I think a fascinating study would be to measure how many times the deep mind group found an error just by playing with the system and correcting it. If this happens with any frequency, then you can make a case for systemic bias due to the workforce training the algorithms."
MachineLearning,3brpre,fjeg,3,Thu Jul 2 19:10:00 2015 UTC,Now that's a reasonable comment.
MachineLearning,3brpre,2Punx2Furious,19,Thu Jul 2 19:12:28 2015 UTC,It's a very unfortunate mistake. We will have to get used to the sincerity of machines.
MachineLearning,3brpre,psystepper,9,Wed Jul 1 23:59:37 2015 UTC,"This reminds me of the early days of facial recognition where cameras couldn't recognize African American faces and people were screaming ""racist!"" when the algorithm was just not good enough."
MachineLearning,3brpre,Pandanleaves,7,Thu Jul 2 02:32:00 2015 UTC,"Well, it was and wasn't racism. That's a clear racial bias in training/testing/marketing. That product shipped and didn't work at all for dark skinned people. Since then there has been special effort made to get black facesnin front of things such as kinect. It's also a sign of who makes these things. Bet it worked for Asian/Caucasian faces. If the first time your facial recognition software sees a black face is in the store after you shipped, that's a symptom of racial bias."
MachineLearning,3brpre,pohatu,3,Thu Jul 2 14:18:56 2015 UTC,"I think you're reading too much into it. The algorithm didn't work well when the lighting conditions weren't very good so that dark-skinned faces blended right into the background. You know how even today, it remains quite a challenge to train a computer to recognize edges in a picture? Yeah, pretty much that. Iirc, the facial recognition system worked fine in bright lighting."
MachineLearning,3brpre,Pandanleaves,2,Fri Jul 3 01:47:21 2015 UTC,"https://youtu.be/t4DT3tQqgRM  It doesn't seem that dark to me in there. Now the kinect, yes you're right, it was easily solved with lights -- and I know the kinect was especially tested with dark skinned people. But I don't know what the deal was with HP. I've heard this mentioned as an example of hidden bias in design. There are other examples related to height assumptions and gender assumptions.  But you probably have a good point about edge detection and resolution and lighting, especially that hp camera was probably low resolution as far as detection is concerned."
MachineLearning,3brpre,pohatu,4,Fri Jul 3 02:32:39 2015 UTC,Somewhere there's a google engineer getting thousands of images of black people to use as training data..
MachineLearning,3brpre,idiogeckmatic,5,Thu Jul 2 15:22:09 2015 UTC,Really? Because this looks to me like whoever trained the algorithm forgot that black people exist. Its like the damn thing where it classified a couch as a cat because the couch had leopard print. Colour is a really easy feature to use/extract. You have to be careful to make sure your training set is sufficiently broad.   Just to be clear: forgetting that black people exist in both training and testing an algorithm is racist. No one is claiming that an algorithm is racist. I will claim that omitting black people from your data set is racist.
MachineLearning,3brpre,rawr_777,-1,Thu Jul 2 16:51:37 2015 UTC,"""Because this looks to me like whoever trained the algorithm forgot that black people exist.""  The fact that you jump to this instead of ""they didn't train on enough pictures of Gorillas"" is illustrative, IMO.    "" forgetting that black people exist in both training and testing an algorithm is racist.""  Forgetting that they exist?  Useless hyperbole based on absurd presuppositions."
MachineLearning,3brpre,patrickSwayzeNU,-20,Thu Jul 2 18:28:23 2015 UTC,They could have trained it to classify black people as Gorillas though. How can we know for sure?
MachineLearning,3brpre,broconsulate,3,Wed Jul 1 23:39:34 2015 UTC,You're painfully stupid.  *edit - unless you're being facetious... in which case I detract my statement.
MachineLearning,3brpre,patrickSwayzeNU,1 point,Thu Jul 2 15:14:01 2015 UTC,I'd say innocent until proven guilty.
MachineLearning,3brpre,2Punx2Furious,-17,Thu Jul 2 09:29:07 2015 UTC,i think there may be some other implications here and it may not be a mistake.
MachineLearning,3brpre,watersign,2,Thu Jul 2 01:35:17 2015 UTC,We get it.  You post in r/coontown and are an established douche.
MachineLearning,3brpre,patrickSwayzeNU,-45,Thu Jul 2 18:31:31 2015 UTC,"racism is about feelings. if someone feels discriminated then its racist, at least to them"
MachineLearning,3brpre,torpedoshit,15,Wed Jul 1 22:30:40 2015 UTC,One is not responsible for other people's feelings.
MachineLearning,3brpre,uber_kerbonaut,18,Wed Jul 1 22:35:00 2015 UTC,"I politely disagree with both you and /u/torpedoshit   I think if you know something is rude and will hurt someone's feelings, them you shouldn't say it.   But still, feeling discriminated isn't the same as the offender having racist intentions. You can feel injured and insulted without someone intentionally doing so, as I think is the case here."
MachineLearning,3brpre,Iamnotanorange,7,Wed Jul 1 22:43:45 2015 UTC,"Still, I don't think it's right that some people want to force the algorithm to remove words like ""gorilla"" just because it might be offensive in some context. The quality of science shouldn't be ruined like this just to avoid offending some people."
MachineLearning,3brpre,2Punx2Furious,8,Wed Jul 1 23:11:51 2015 UTC,"I might argue differently.   If they haven't quite figured out how to detect gorillas within a certain degree of specificity, then I'd be happy with those analyses becoming ""invisible"" to the user. Until they're able to get it right, anyway.   Edit: they don't have to disrupt science, just get better at it in private for a while."
MachineLearning,3brpre,Iamnotanorange,2,Wed Jul 1 23:20:30 2015 UTC,"What if the algo says the skin of a person is black? Some people could find that offensive, but it's still true. What if it mistakes a person for an animal? It is true, but people could find it offensive. I don't think this kind of compromises are productive, but I guess I'm not the one that makes these decisions, so it doesn't really matter."
MachineLearning,3brpre,2Punx2Furious,2,Wed Jul 1 23:24:01 2015 UTC,"That is sort of a patch and the ultimate goal is to make it better at distinguishing people and gorillas, but I assume they're also going to have to model what it means to be polite in various cultures. Needless to say, that is a tall order."
MachineLearning,3brpre,uber_kerbonaut,1 point,Thu Jul 2 04:03:47 2015 UTC,"Actually I agree in most part, but I think we need a few policies to make our moral code both as nice as possible, but also to keep good behavior achievable, so that people don't just abandon the prospect. 1. I think if you know something is rude and will hurt the majority of the listeners' feelings, then you shouldn't say it to them. 2. If you don't know whether it will hurt them, or somethings appears to be beyond your control, you're not responsible for it."
MachineLearning,3brpre,uber_kerbonaut,2,Thu Jul 2 03:57:31 2015 UTC,How do you get food into your mouth every day being this stupid?
MachineLearning,3brpre,WaxProlix,2,Wed Jul 1 22:42:22 2015 UTC,"c'mon he can speak English and type, he is well above self feeding. there is no need to insult him."
MachineLearning,3brpre,Revrak,6,Thu Jul 2 01:22:35 2015 UTC,"There was a time when Germany's then-first lady Bettina Wulff had a even more drastic problem: Google would auto complete the search ""Bettina Wulff"" to ""Bettina Wulff prostitute"".  They said they were not able to do anything about it."
MachineLearning,3brpre,sieisteinmodel,50,Wed Jul 1 20:18:20 2015 UTC,"Gorillas are closely related to humans and have a similar facial structure.  With the exception of albinos, they're all black.    In any case, why are they running an image tagging service without a label blacklist?  I can imagine lots of labels that are more offensive, like labeling a feminine-looking man as a woman."
MachineLearning,3brpre,alexmlamb,10,Wed Jul 1 18:34:41 2015 UTC,"they wanted to show species of animals as categories (""things""). obviously this is going to go very badly sometimes. My niece as a baby was classified as a bird and queen's guards were classified as elephants."
MachineLearning,3brpre,Lamurias,41,Wed Jul 1 19:56:23 2015 UTC,So you imply that women are even lower on the social totem pole than gorillas? Rough... :p
MachineLearning,3brpre,furbyhater,9,Wed Jul 1 20:35:29 2015 UTC,"How would a label blacklist help? What are they going to do, ban the words man, woman and gorilla?"
MachineLearning,3brpre,TooSunny,8,Wed Jul 1 23:28:30 2015 UTC,"No, just use decision theory and come up with a certain cost for predicting ""gorilla"" if ""woman"" is likely as well."
MachineLearning,3brpre,sieisteinmodel,5,Thu Jul 2 06:01:37 2015 UTC,"Yes, interesting. It's getting unpredictable. Perhaps they can use ML algorithms to teach the computer to spot potentially offensive tagging?"
MachineLearning,3brpre,psystepper,7,Thu Jul 2 00:01:42 2015 UTC,Would that be an easier problem than the original classification?
MachineLearning,3brpre,thfuran,1 point,Thu Jul 2 02:08:10 2015 UTC,"Of course, if you give it the additional side information in the form of a data set."
MachineLearning,3brpre,sieisteinmodel,1 point,Fri Jul 3 07:06:28 2015 UTC,"I mean, you could feed in a misclassification offensiveness matrix trivially, but learning one seems like just massively over complicating things."
MachineLearning,3brpre,thfuran,1 point,Fri Jul 3 14:42:36 2015 UTC,"The acquisition of a data set would be quite an exercise, yes."
MachineLearning,3brpre,sieisteinmodel,2,Fri Jul 3 18:44:58 2015 UTC,So you show it a picture of what is obviously a gorilla and then what?
MachineLearning,3brpre,Jonno_FTW,-2,Thu Jul 2 02:49:16 2015 UTC,Yeah.  Replace man and woman with person and get rid of gorilla.
MachineLearning,3brpre,alexmlamb,7,Thu Jul 2 01:37:59 2015 UTC,But then you would neither be able to distinguish between men and women or identify gorillas. This is a loss of functionality that would be averted by simply training the classifier better.
MachineLearning,3brpre,thfuran,2,Thu Jul 2 02:06:32 2015 UTC,"Yeah well, it will never be perfect though. And how about transgendered people? If you cave in to PC nonsense there is no end to this. You should strive to make it better but fuck people getting ""offended"" by algorithms making classification mistakes."
MachineLearning,3brpre,muyuu,1 point,Thu Jul 2 07:51:39 2015 UTC,"The neural net doesn't care about which label it incorrectly applies.  On the other hand, different errors have different real world losses.  If a group of five girls take a picture together and one of them is accidentally tagged as a man, then it's a bad user experience.  If a 40 year old is accidentally tagged as elderly, then it's also a bad user experience.  If a dog is misclassified as the wrong breed, then the impact on the user experience is fairly small.    A blacklist just prevents the model from making the most costly mistakes.  An alternative would be to just adjust the thresholds (i.e. you need really high probability to label an image as a gorilla), but I dislike this idea.  One mislabeling is too many.  Also it looks like Google actually did go with a blacklist:   http://www.telegraph.co.uk/technology/google/11710136/Google-Photos-assigns-gorilla-tag-to-photos-of-black-people.html"
MachineLearning,3brpre,alexmlamb,1 point,Thu Jul 2 03:36:39 2015 UTC,The amount of downvotes you get just illustrates how many people decided to not read the first chapters of Bishop's PRML.
MachineLearning,3brpre,sieisteinmodel,1 point,Fri Jul 3 07:05:54 2015 UTC,"But...then...how would you search for images of men, women or gorillas?"
MachineLearning,3brpre,TooSunny,4,Thu Jul 2 03:05:28 2015 UTC,"... Why would they make a blacklist? You're acting like it's necessary that the algorithm see black people as gorillas, and this is just how it is.  This is understandable that a very early, primitive classification algorithm makes this mistake, but it's undeniably a mistake. Google, and all vision researchers, shouldn't accept this kind of failure."
MachineLearning,3brpre,omniron,1 point,Thu Jul 2 01:42:26 2015 UTC,"Adding a label blacklist isn't ""accepting failure"". It's not like they're going to stop working on the photo tagging system and consider it ""done"" at this point. But Google Photos is not a research prototype. Google Photos is a product. How users feel about your product matters for the success of that product."
MachineLearning,3brpre,dwf,0,Thu Jul 2 15:39:14 2015 UTC,"done is better than perfect, as long as you have a reasonable risk mitigation strategy like a blacklist."
MachineLearning,3brpre,webspiderus,1 point,Thu Jul 2 01:55:53 2015 UTC,of course there's a blacklist.
MachineLearning,3brpre,webspiderus,4,Thu Jul 2 01:54:00 2015 UTC,"I'm assume it is the reverse, they probably have labelled data for the training set which is more like a whitelist.  They almost certainly have an actual gorilla in the training set."
MachineLearning,3brpre,unkz,2,Thu Jul 2 03:07:32 2015 UTC,"they certainly did, and almost certainly other things that existed in the training set (e.g. hippo, pig) did not make it as a Google Photos surfaced category because people realized it might look really shitty when applied to a false positive."
MachineLearning,3brpre,webspiderus,1 point,Thu Jul 2 04:12:02 2015 UTC,"In any case, why are they running an image tagging service without a label blacklist?   That would be a very extensive blacklist. Like, how many animals exist that wouldn't be offensive when applied to some person? Dogs are an extremely common pet, so you'd want an algorithm that can identify dogs, and yet it's gonna be pretty offensive to call people that.   It's honestly better to just accept that this kind of shit is going to eventually happen with imperfect computer vision algorithms and no human oversight. That said, I think the best choice would be to extensively train it in recognizing humans / human faces, over a variety of races (totally possible their training data was heavily biased with white people). Relatively, misidentifying a human is much, much more costly than basically any other possible mistake."
MachineLearning,3brpre,fernsauce,1 point,Thu Jul 2 08:08:08 2015 UTC,"Why is being misclassified as a woman more offensive than a gorilla?  Yeah but I got your point, not that I disagree."
MachineLearning,3brpre,personanongrata,-19,Thu Jul 2 12:20:27 2015 UTC,"I don't know if this was sarcasm, but what you said is more offensive than both of those combined. Basically for you there is a bigger gap between man and woman, than between human and gorilla, as in:  value(man) - value(woman)  >  value(human) - value(gorilla)"
MachineLearning,3brpre,waterflock,8,Wed Jul 1 20:35:46 2015 UTC,Just because there's a gap doesn't mean one's better. I'd expect neither men nor women would like getting confused for the opposite sex. Although I still think the gorilla thing is more offensive.
MachineLearning,3brpre,DCarrier,92,Wed Jul 1 20:59:51 2015 UTC,"She does look like a bit like a gorilla. The computer isn't racist, we just see it as racist when that sort of thing happens. To a computer, humans aren't that different from other primates anyway, let alone pictures without clothes of a woman with a skin tone matching a gorilla. Gorillas, after all, are the mostly closely related primate after the bonobo chimp and common chimp to us humans."
MachineLearning,3brpre,c3534l,3,Wed Jul 1 17:57:56 2015 UTC,"Human adults are also FANTASTICALLY good at doing human facial recognition. We literally have a section of our brain devoted to it and it still takes years to develop.  Even with all of that, you have folks in the west saying: ""Asians all look the same"". I mean, that is pretty rough. Our facial recognition training is so intense that we notice mm scale differences and still have to focus on zones that vary more in our commonly seen races to be any good at it.  For a general purpose image recognition tool, this is really quite good. A handful of years ago it might have labelled them as ""Bushes"" or ""Box"". Shit was terrible."
MachineLearning,3brpre,Ambiwlans,-25,Fri Jul 3 04:51:12 2015 UTC,"I kind of can't believe that this is the top comment. She doesn't look like a gorilla. No human being would mistake her for a gorilla. The fact that a neural network mistook her for a gorilla is understandable, but the fact that this is part of a product that Google has made publicly available and millions of people use is pretty inexcusable in my opinion."
MachineLearning,3brpre,woohalladoobop,24,Thu Jul 2 05:23:55 2015 UTC,"Of course no one would mistake her for a gorilla. But it's not a human being categorizing the photos. You can't make an algorithm like that infallible, at least not to my knowledge. The field and people's devices simply aren't that advanced. It's always going to make mistakes. Now, if you told me it consistently made offensive categorizations and google did nothing to correct the problem, then I could see where outrage is justified. But it's probably a relatively simple neural net that's just trying to map pixels to labels.  It's essentially the same problem you get when auto-correct makes you say something embarrassing. My cell phone wasn't trying to make me say something sexual to my mother, it isn't smart enough to know what a mother is or that auto-correcting ""kn"" to ""in"" when I mean ""on"" can make for all the difference in the world. The algorithm isn't that smart, it's not ""intelligent."" If the ""search for similar images"" feature on google these days is any indication, their algorithm probably cares a lot of the general location of pixels of a certain hue more than anything else.  Getting upset that an algorithm happened to misclassify a picture of a black girl's face as a gorilla is like getting angry at your alphabet soup for swearing at you. The only way to deal with something like this is to learn there's a common problem that is culturally sensitive like this and hand-write a rule that says ""if the next ranked guess to gorilla is 'human face', then don't classify it as anything."" Even then, sometimes it might still get it wrong.  Those are your cultural preconceptions based on a very nasty human history of racism. But sometimes an algorithm is going to misclassify a fork as a knife, a calculator as a remote control, or a person as a gorilla. People will just have to learn to deal with it."
MachineLearning,3brpre,c3534l,1 point,Thu Jul 2 05:51:19 2015 UTC,"Eh, you can argue about the efficacy of using machine learning w/o any human oversight, sure. But the fact of the matter is there's no image recognition algorithm on the planet that can always correctly identify images. It's going to make mistakes, and quite frankly, mistaking a human for a gorilla (which is still a primate, much closer to on the mark than a lot of classifications) is really on the tamer side of mis-identifications.  I mean, it's particularly bad because of the super racist subtext, but in order for the computer to know that it was racist, it would have to know that this is a picture of a human being and not a gorilla, and it doesn't.  At the end of the day, though, machine learning is still a really useful tool, even if it's not perfect. It just needs to be understood that it is, above all, still a machine. It doesn't think, it's not trying to convey any messages when it labels an X a Y. And, frankly, given how many people use this, this kind of thing is probably inevitable."
MachineLearning,3brpre,fernsauce,-91,Thu Jul 2 07:50:42 2015 UTC,"There are two possibilities:   Identify a gorilla as a human. Identify a human as a gorilla.   Assuming Google set priors to what they expect a random image to be, the former is acceptable, the latter not."
MachineLearning,3brpre,NotFromMumbai,19,Wed Jul 1 18:19:27 2015 UTC,Google doesn't manually set the priors for thousands of image categories...
MachineLearning,3brpre,blindConjecture,3,Wed Jul 1 20:50:03 2015 UTC,"Probably their algorithm can check probability of gorilla and black people. If both are high, don't label as gorilla or black. But there are so many nuances of social norm, its not workable."
MachineLearning,3brpre,butthink,-16,Thu Jul 2 00:35:49 2015 UTC,It doesn't have to be manual.  They could just use the training set frequencies.
MachineLearning,3brpre,NotFromMumbai,35,Wed Jul 1 21:27:54 2015 UTC,The whole point of machine learning is that you're not going in there and programming decisions like this.
MachineLearning,3brpre,heisenbrau,-8,Wed Jul 1 21:22:27 2015 UTC,"Machine learning is exactly about those types of decisions. Every model is built on a set of hard coded assumptions, parameters that require tuning, and thresholds for labeling. We're clearly not building a giant truth table, that's the job of the model, however we are directing how the model learns and operates."
MachineLearning,3brpre,therobot24,-21,Thu Jul 2 00:20:56 2015 UTC,"They aren't ""just"" doing machine learning. They are delivering a product. Products often involve making tradeoffs between automation and manual safeguards."
MachineLearning,3brpre,dwf,19,Wed Jul 1 22:45:35 2015 UTC,"manual safeguards   Nope, you do not know what you're talking about at all, this is about machine learning."
MachineLearning,3brpre,KnownAsGiel,-26,Wed Jul 1 23:12:48 2015 UTC,It's actually really funny how wrong you are in this particular instance.
MachineLearning,3brpre,dwf,7,Wed Jul 1 23:45:48 2015 UTC,Care to enlighten us?
MachineLearning,3brpre,bajuwa,-21,Thu Jul 2 01:56:35 2015 UTC,"Not really, no. :)"
MachineLearning,3brpre,dwf,62,Thu Jul 2 02:28:32 2015 UTC,This sounds like you don't know much about machine learning and haven't even attempted to educate yourself on the topic before assuming malevolence.
MachineLearning,3brpre,TalismanButt,-4,Wed Jul 1 20:13:33 2015 UTC,"It sounds like you don't know much about product development, and haven't even attempted to educate yourself before assuming that all mistakes on the part of your fancy algorithm have equivalent public relations costs."
MachineLearning,3brpre,dwf,-35,Thu Jul 2 15:43:06 2015 UTC,"I suppose I don't know much about machine learning.  But I am at a total loss how I am assuming malevolence.  Is the perceived malevolence toward Google or the humans in the image --- believe me, I have no idea.  I am sincere; I'll be really very grateful if you (or some other expert) could concisely tell me what I'm missing in what I thought was a simple statement on Bayesian learning."
MachineLearning,3brpre,NotFromMumbai,14,Wed Jul 1 20:54:43 2015 UTC,"They aren't manually setting priors. The model is trained using a huge amount of labeled images as an input, and essentially learns to classify images based on RGB pixel values. Google ""neural networks"" and ""deep convolutional neural networks""  There is no real concise way to explain what is going in, its an incredibly complicated research area.. But here's a recent article that came out that is interesting, and might give you a better idea to how the idea of a simple prior like you are thinking of doesn't really apply  http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html  Also, there are way more than 2 possibilities... Think of all the things that the model could classify a human as ... literally anything you can think of.. When you think about it like that, the fact that it classified a picture of a human as a gorilla really isn't that far off compared to what it could have predicted"
MachineLearning,3brpre,mjs128,-12,Wed Jul 1 23:15:41 2015 UTC,"Thanks for your reply.  Rephrasing what I said, my guess is that the training set would have a lot more images of humans than of gorillas (as the case would be with the test/query images).  If so, one wouldn't be surprised if gorillas are mistaken by the classifier as humans, but not the other way round.  The choice of the training set implicitly sets the priors.  Will appreciate any further feedback."
MachineLearning,3brpre,NotFromMumbai,20,Thu Jul 2 00:21:07 2015 UTC,Neural nets do not work like Naive Bayes.
MachineLearning,3brpre,altrego99,-5,Thu Jul 2 00:55:15 2015 UTC,"Sure.  But any classifier built on a training set is implicitly setting priors.    You can disregard the statement about priors and just think in terms of the number of images in the training set of humans vs. gorillas and how that influences the parameter selection.  In other words, you will get a different classifier, be it neural networks or logistic regression, if the frequency of human images and gorilla images is reversed.  The mechanics of the layers, backpropagation algorithm, etc. specific to neural networks are not relevant here."
MachineLearning,3brpre,NotFromMumbai,3,Thu Jul 2 01:24:34 2015 UTC,"You typically correct for large class imbalances to prevent that. Also, they'd have to have used  more images of gorillas than of humans for that to apply here, which I find unlikely."
MachineLearning,3brpre,DanceOnGlass,1 point,Thu Jul 2 06:56:51 2015 UTC,"Well yeah - if you're saying it will make less error in classifying something as human, just because it's seen more humans, you're right.  The statement that the error has a preferred direction isn't true though.  If for example the classifier is only trained on humans and gorillas, even if it's seen way too many humans, it can still easily misclassify gorilla as human.  This is similar to modeling fraud or credit defaults. The even rate being small doesn't mean non-occurrence will be classified as occurrence less often in an equally weighted sample."
MachineLearning,3brpre,altrego99,-46,Thu Jul 2 14:34:01 2015 UTC,Your response sounds like you don't know product management or public relations.
MachineLearning,3brpre,zelmerszoetrop,19,Wed Jul 1 20:21:45 2015 UTC,"It's ideas like this that prevent technological innovation, for better or worse.   Ethics within medicine prevent human experimentation  Current social climate apparently should prevent research in artificial intelligence  One of those doesn't make sense."
MachineLearning,3brpre,isuckwithusernames,-11,Wed Jul 1 21:34:56 2015 UTC,"This isn't about preventing research, this is about mediating the use of an algorithm within a product."
MachineLearning,3brpre,dwf,6,Wed Jul 1 22:45:23 2015 UTC,Forcing research into the strict definition of an eventual product is preventing research.
MachineLearning,3brpre,isuckwithusernames,-2,Wed Jul 1 23:08:32 2015 UTC,"Do you honestly think that they are just going to freeze the current network in place and stop trying to improve it once they turn it into a product? When you deliver a product to be used by the general public you need to think through the scenarios where your imperfect cutting-edge algorithm could do something really embarrassing, hurtful or harmful. It's just a real shame that their quality assurance pipeline didn't catch this."
MachineLearning,3brpre,dwf,46,Thu Jul 2 15:56:06 2015 UTC,"I don't know anything about those, so you are correct. I'm not, however, in a product management or a public relations subreddit making ridiculous statements about the way they work."
MachineLearning,3brpre,TalismanButt,12,Wed Jul 1 20:27:34 2015 UTC,"I had assumed that the tags were generated by Googles algorithm as essentially ""this the search word people use to find this image"", but thinking about it I really have no idea how its generated. Still,  I find it highly unlikely that someone hard-coded racism into this feature."
MachineLearning,3brpre,c3534l,3,Wed Jul 1 18:55:27 2015 UTC,"This is not how deep neural networks work. You don't get to set anything, except some hyperparameters that are not related to what you're talking about."
MachineLearning,3brpre,Indrionas,11,Thu Jul 2 06:17:32 2015 UTC,"Despite what people like us may think, this is potentially a Public Relations disaster as far as the general public is concerned."
MachineLearning,3brpre,voodoochile78,2,Wed Jul 1 21:56:52 2015 UTC,"That's true, there's always a risk releasing unproven, beta software in the wild."
MachineLearning,3brpre,omniron,5,Thu Jul 2 01:45:07 2015 UTC,"Side note, I think they need to train more pictures of airplanes on the ground.  I looks like their algorithm thinks ""the sky"" is ""airplanes"""
MachineLearning,3brpre,tehgargoth,4,Wed Jul 1 23:17:21 2015 UTC,"This seems pertinent:  http://content.time.com/time/business/article/0,8599,1954643,00.html"
MachineLearning,3brpre,CatReallyRock,20,Wed Jul 1 20:59:12 2015 UTC,That will be a big problem. Computer that are racist just because they don't see a difference.
MachineLearning,3brpre,NitroXSC,51,Wed Jul 1 17:41:51 2015 UTC,"Check your privilege, HAL9000!"
MachineLearning,3brpre,GibbsSamplePlatter,8,Wed Jul 1 17:49:50 2015 UTC,That's a contradiction in terms.
MachineLearning,3brpre,hobbified,16,Wed Jul 1 18:54:07 2015 UTC,"It's also worrying that they put ""skyscrapers"" next to ""planes"". Nice subtle 9/11 joke, Google."
MachineLearning,3brpre,sharplikeginsu,0,Wed Jul 1 17:55:21 2015 UTC,Something something steel beams
MachineLearning,3brpre,geebr,0,Wed Jul 1 18:52:57 2015 UTC,"Oh god, I didn't even notice that. Those sky scrapers even look like the twin towers."
MachineLearning,3brpre,c3534l,3,Wed Jul 1 19:10:05 2015 UTC,"I think google photos just has some trouble with animals. As you can see here.  Ahem, William Blake's Urizen is not a lion, guys. Far from it.   edit: also, sea lions aren't really lions either."
MachineLearning,3brpre,Iamnotanorange,1 point,Wed Jul 1 22:49:23 2015 UTC,"It's very interesting to see the cases where the machine still fails. And also, to wonder how the case is likely to be solved. This time I don't have a clue, how to teach a computer the difference between a real lion and a sea lion."
MachineLearning,3brpre,psystepper,1 point,Thu Jul 2 00:15:25 2015 UTC,Does it classify that one guy who looks like a walrus as a walrus? And that woman everyone thinks looks like a horse (I don't see it) as a horse?
MachineLearning,3brpre,pohatu,1 point,Thu Jul 2 14:35:41 2015 UTC,Inb4 no outrage when it identifies Larry King as a proboscis monkey.
MachineLearning,3brpre,ClickHereForBacardi,2,Wed Jul 1 19:21:09 2015 UTC,If anyone is interested in understanding how it actually works read this.  http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html  It makes a lot of sense how this could happen.
MachineLearning,3brpre,tobyfunke,2,Wed Jul 1 21:07:41 2015 UTC,source on google trying to patent classification?
MachineLearning,3brpre,gogonzo,3,Thu Jul 2 01:49:28 2015 UTC,/r/MachineLearning/comments/3bl8f8/did_google_just_patent_classification/  http://www.freepatentsonline.com/y2015/0178383.html
MachineLearning,3brpre,GoldmanBallSachs_,2,Thu Jul 2 05:29:23 2015 UTC,"Google's definition of racism:   the belief that all members of each race possess characteristics or abilities specific to that race, especially so as to distinguish it as inferior or superior to another race or races.   Google's definition of classification:   the action or process of classifying something according to shared qualities or characteristics.   Misclassifying one species as another isn't racism, according to Google's definitions.  Some people would probably say ""Convenient, that!"" (because they want to be angry.)  If Google ever classified humans by race, that's when they'd be racist (by their definitions.)"
MachineLearning,3brpre,EmoryM,1 point,Wed Jul 1 23:24:20 2015 UTC,"Honestly, (I don't give a) fuck (about) Google's definitions."
MachineLearning,3brpre,furbyhater,1 point,Fri Jul 3 00:16:20 2015 UTC,"That's cool, is this racist by any definition?"
MachineLearning,3brpre,EmoryM,1 point,Fri Jul 3 05:02:34 2015 UTC,I want to see one of those neural network optimized paintings of a gorilla
MachineLearning,3brpre,skpkzk2,1 point,Thu Jul 2 01:51:17 2015 UTC,It takes a pretty smart gorilla to graduate from university!
MachineLearning,3brpre,green_meklar,1 point,Wed Jul 1 20:53:11 2015 UTC,If she didn't look like a gorilla google would have chosen a different category.   Ergo she looks like a gorilla.   The only solution to this is to stop identifying gorillas.
MachineLearning,3brpre,Tiramisuu2,0,Thu Jul 2 05:39:54 2015 UTC,As far as I knew this was a troll. Is there a link to a program actually producing this output?
MachineLearning,3brpre,captcha_bot,4,Thu Jul 2 06:26:29 2015 UTC,"given the context, 95% sure the user isn't actually the person posting the output. they're the guy with the fake black person twitter account being generally hella racist.  this source here verifies its legitimacy."
MachineLearning,3brpre,fernsauce,1 point,Thu Jul 2 07:59:04 2015 UTC,"Ah, thanks for the link."
MachineLearning,3brpre,captcha_bot,0,Thu Jul 2 14:47:39 2015 UTC,"ITT: Lots and lots of people who have spent too much time studying machine learning and have completely forgotten that soft skills exist, and have not bothered to spend any time learning about the problems of discrimination in our society, and have no idea what the term ""microaggression"" means."
MachineLearning,3brpre,mgmuscari,-7,Sat Jul 4 19:02:19 2015 UTC,"Question from someone not familiar with machine learning: presumably they program words to contain multiple satisfactory sets? So yeah she's a primate who is not white or albino, but that doesn't mean she's closer in recognition to a gorilla. Surely black people constitute a satisfactory set of 'person' the same way old white peoples are distinct from mole rat or shaved cats? (Even though some definitely look like them)"
MachineLearning,3brpre,kspacey,10,Wed Jul 1 21:52:24 2015 UTC,It's more complicated than that.  In machine learning a computer is given thousands/millions of pre-labeled images and it algorithmically breaks the images down into patterns that can be used to categorize pictures that it has never seen before.  They are extremely prone to error as this is a bleeding edge technology.  Look at the voice recognition on your phone for example.
MachineLearning,3brpre,tehgargoth,7,Wed Jul 1 23:15:31 2015 UTC,"Wow, that is not how machine learning works at all. Don't make assumptions about something you know nothing about."
MachineLearning,3brpre,KnownAsGiel,-1,Wed Jul 1 23:19:01 2015 UTC,No? I guess I've been doing it wrong this whole time!
MachineLearning,3brpre,tehgargoth,-11,Wed Jul 1 23:22:58 2015 UTC,"ok since you're going to be an asshat and clearly not try to understand what I'm saying let me talk down to you like the idiot you are, since I guess we're both going to engage in this behavior.  Machine learning CLEARLY engages in creating connected graphs, at the very least between images and words as seen above. This requires input and training, through which the computer uses a learning algorithm to build associations. All of this I know to be true.  Now the algorithms themselves are just a black box to me, that is why I stated RIGHT AWAY that I wasn't an expert since I was looking for helpful insight, instead of your useless asshattery. Anyhoo what doesnt depend on this intricate knowledge is that the output, the graph, connects images to words. Unforunately there is another, in between graph which embodies 'concepts' which the black box is attempting to build but can't really do, because its a machine so at best its approximating.  Let me give you a super simple example. flowers!. Now you and I have references to flowers that build off a concept which we easily recognize, however a learning black box can depend solely upon images which leaves us with a problem. For instance tulips look pretty much nothing like daisies and actually look a lot more like wine glasses  So lets say we've trained a computer to recognize a wine glasses and flowers (but used pretty much only a set of daisies) surely a BETTER programming function would accept that there are N disparate classes of flowers that should all be associated, which would correct the failure which would label tulips as wine glasses (as happened here).  But you know, fuck you and all."
MachineLearning,3brpre,kspacey,6,Thu Jul 2 01:52:03 2015 UTC,"It's on you not to make assumptions. Sorry, but knownasgiel is right, why bother engaging if you are just coming here with assumptions? ML is a very complicated subject, and the mis-classification is something that I don't think surprises many people...I'm guessing most people seriously looking at ML know about the inaccuracies and the guessing game it kind of is right now.  But coming here with a predefined idea of how these algorithms work, and trying to bring some flower/glass idea into the mix is  nonconstructive and making assumptions is plain wrong.  A) Google is making an algorithm to classify everything. Not just flowers, wine glasses and tulips, but everything you can thrown into the image box. Therefore, it has to look at very many features from very many objects, and unsurprising, if an image is closely related to another, miss-classifications happen.  B) A ""better"" programming function -- you realize google made this? I didn't look at the source, but I'm relatively certain they didn't just let some nutjob dream up their algorithms.  C) From the wine glass example you have brought, its evident you don't know enough to comment on this dept about machine learning, so it's best you keep your assumptions to yourself and perhaps read a little into machine learning. You said yourself you don't know about ML enough to comment on it well, so why comment on the works of others?"
MachineLearning,3brpre,GoldenKaiser,5,Thu Jul 2 06:02:40 2015 UTC,"ok since you're going to be an asshat and clearly not try to understand what I'm saying let me talk down to you like the idiot you are, since I guess we're both going to engage in this behavior.   What does that buy anyone?  Yeah, it's weird for KnownAsGiel to go off on you for ""assumptions"" when you were literally just asking a question.  But I don't see many scenarios where escalating the asshattery helps anything.   Machine learning CLEARLY engages in creating connected graphs, at the very least between images and words as seen above. This requires input and training, through which the computer uses a learning algorithm to build associations. All of this I know to be true.   I think part of the confusion here is that the graph you're talking about is implicit.  There's a difference between ""it sure looks like a function from A to B exists"" and ""they must have an explicit list of pairs (a,b)"".   Let me give you a super simple example. flowers!. Now you and I have references to flowers that build off a concept which we easily recognize, however a learning black box can depend solely upon images which leaves us with a problem. For instance tulips look pretty much nothing like daisies and actually look a lot more like wine glasses   It sounds like you're saying that we have some source of knowledge outside of the images we perceive that tells us that tulips are flowers and not wine glasses, and that is different from how image classification algorithms work.  But it's not different; that source of knowledge outside of the images we perceive corresponds to the training data that the algorithms work with.  And I wouldn't be surprised in the least if an algorithm trained with too-homogeneous data misclassified a tulip as a wine glass, or a black person as a gorilla.  Diverse training data is a must."
MachineLearning,3brpre,jpfed,2,Thu Jul 2 04:22:30 2015 UTC,"I'm not too familiar with the actual implementation here, but in general we aren't at the level yet of having those kind of deep hierarchies. It didn't realize the picture contained a primate, and then try to decide which primate.  Instead, there were millions of labelled images, some of people, some of airplanes, gorrillas, etc. Then some algorithm, probably a Convolutional Neural Network of some kind, was trained so that it can look at new images and try to figure out what is in them. In most cases, it will output some sort of weights that say how ""likely"" (to use the word very roughly) it is that the image contains something of each of the classes. In this case, it could be the it thought gorilla was just slightly more likely than person, so it went with gorilla.  We are still pretty far away from reasoning about abstract concepts like ""people and gorillas are both primates"" and combining that with images, although it is a possible promising direction.  The latest best results with the neural network approaches are all algorithms where we don't really understand ""why"" the algorithm chose what it did. Sometimes we can gain insight into the network and say things like ""these nodes seem to correspond to cat ears"", etc., but we can't give a clear answer for why one thing was chosen over another (at least not currently)."
MachineLearning,3brpre,bradfordmaster,-2,Wed Jul 1 23:31:02 2015 UTC,"Nobody is asking for these levels of hierarchies, clearly the algorithm is already capable of extrapolating and connecting things like a partial wing to a plane, it shouldn't be too difficult to say that there are a generalized N points that can all qualify as X word. Meaning need not apply"
MachineLearning,3brpre,kspacey,4,Thu Jul 2 01:38:56 2015 UTC,"First of all, I wouldn't call anything the algorithm does extrapolating. The data set is from Google images, I can almost guarantee there are similar pictures of airplane wings labeled as airplane. Even if there weren't, picking out a wing as part of an airplane would be interpolating, not extrapolating.  I'm also really not sure what you are proposing. Are you saying someone should manually come up with huge lists of 'these feature points go with this word?' Or are you saying it should just always classify guerilla as person to avoid this?"
MachineLearning,3brpre,bradfordmaster,2,Thu Jul 2 02:00:48 2015 UTC,"So lets say we've trained a computer to recognize a wine glasses and flowers (but used pretty much only a set of daisies) surely a BETTER programming function would accept that there are N disparate classes of flowers that should all be associated,    You appear to be suggesting with this that there should be broad category called ""flower"", and classes of flowers within that.  That constitutes a hierarchical classification."
MachineLearning,3brpre,jpfed,4,Thu Jul 2 04:25:09 2015 UTC,Now that's racist.
MachineLearning,3brpre,Jonno_FTW,0,Thu Jul 2 01:34:09 2015 UTC,"the result of the google algorithm itself is racist. IMO there should be a fine imposed on racial discrimination, even if it wasn't on purpose. That would motivate the companies to test their algorithms more thoroughly before releasing them to the public."
MachineLearning,3bw0h2,kjfries,7,Thu Jul 2 16:41:42 2015 UTC,kaggle
MachineLearning,3bw0h2,j1395010,5,Thu Jul 2 17:02:31 2015 UTC,"You could try some Kaggle competitions: https://www.kaggle.com/competitions  These are real-world problems, so they will force you to find solutions."
MachineLearning,3bw0h2,Jeff_Fohl,3,Thu Jul 2 17:04:07 2015 UTC,"Kaggle is a good approach - though you might also take inspiration from the winner and formulate a new problem on your own.  I recommend http://www.chioka.in/kaggle-competition-solutions/ - find some competitions in a field that interests you, and look at what the top people are doing/using in these competitions!"
MachineLearning,3bw0h2,kkastner,2,Thu Jul 2 17:12:55 2015 UTC,Scraping data from around the net and writing programs to model it. What's really fun is if you're lucky enough to have some personal datasets lying around. It's awesome to analyze your own data.
MachineLearning,3bw0h2,Notlambda,2,Thu Jul 2 22:26:19 2015 UTC,kaggle. coursera too...
MachineLearning,3bw0h2,watersign,1 point,Fri Jul 3 00:43:22 2015 UTC,"Find a machine learning library in the language of your choice and play around with it. I'm still learning, but that, reading scholarly articles helped me the most."
MachineLearning,3bw0h2,theirfReddit,1 point,Thu Jul 2 17:04:18 2015 UTC,"Definitely kaggle as the others said, it's the reason I discovered this subreddit. I just finished my first kaggle competition and got top 10%, just by putting in a lot of time and trying different things. I did NOT use any clever ideas or deep insights, i.e. no matter who you are, you can have good success there, if you put in some effort.  My approach was honestly to just look at which methods sklearn (a big ML python library) offers that did regression. I had no idea what they did, but just looking around, trying a lot of things made me learn loads of stuff about ML.  They even have kind of guided competitions that are only there to teach you machine learning basics, although I haven't tried those, because I wanted to be better than my colleagues who also did the competition and that competitive factor is much more motivating than just ""oh let's see what I can learn about this""."
MachineLearning,3bwqz2,SkiddyX,5,Thu Jul 2 19:59:27 2015 UTC,Check out microsoft DSSM way of hashing words into character trigrams : http://research.microsoft.com/en-us/projects/dssm/ they obtain an embedding somehow resistant to misspelled words (provided the misspelled word has a rather similar trigram hash)
MachineLearning,3bwqz2,CptZouglou,1 point,Thu Jul 2 20:13:56 2015 UTC,"Imo, best style for word vector representation. Compact yet expressive.   There seems to be some magic with trigrams (the trigram hmm was king for a while and trigram chain crfs are pretty good too). But I wonder how well a bigram or 4gram hash would work. I also wonder if there's a collision type proof on why the trigram hash is good."
MachineLearning,3bwqz2,Articulated-rage,1 point,Fri Jul 3 01:25:23 2015 UTC,What do you mean by resistance? Does this method allow for context of the sentence?
MachineLearning,3bwqz2,Articulated-rage,1 point,Fri Jul 3 04:03:52 2015 UTC,"Read into the method,  but,  it's resistant because they had a supervised signal of when two vectors should be similar.  So they use the minimization of cosine distance between these words as their objective. It's how they made sent2vec.  Edit: actually it's the Gibbs distribution over the cosine distance."
MachineLearning,3bwqz2,zmjjmz,2,Fri Jul 3 19:29:27 2015 UTC,"Do you have a context for the word? It might be worthwhile just trying to bring the skip-gram model back in and predict the word given its context, then maybe sort by edit distance."
MachineLearning,3bwqz2,zmjjmz,1 point,Thu Jul 2 20:14:37 2015 UTC,The idea was to have the complete sentence (with the misspelled word) and have the network try to predict the correct sentence
MachineLearning,3bt5f5,doomie,1 point,Wed Jul 1 22:57:08 2015 UTC,"I still wonder why they don't get the effects of the original adversarial samples, where changing a few pixels completely changed the classification.  Is it purely a better trained neural network?"
MachineLearning,3bt5f5,marijnfs,3,Thu Jul 2 08:41:09 2015 UTC,"there's usually regularization on the pixels imposed here that prevents these cases. Here, the regularization seems to be slightly funny (a random wiggle in space), instead of something like L2 pixel loss, etc."
MachineLearning,3bt5f5,badmephisto,1 point,Thu Jul 2 11:07:04 2015 UTC,"I don't suppose someone fancies putting together a how-to setup for the first steps, do they? I don't know what an IPython Notebook is or how to get those dependencies set up :/  Edit: I'm a programmer, but not a Python native."
MachineLearning,3bt5f5,BrassTeacup,1 point,Thu Jul 2 10:03:52 2015 UTC,"I went through it and it's a pain. Installing caffe comes with issues most of the time. If you're really interested and do not have the dependencies already installed (caffe mostly), you might need at least an hour of installs. Plus, I went through some issues regarding image output and had to dwell into the (short enough) code. Really nice if you want to have something working in a few hours from scratch. Definitely not a ""press button and get results""."
MachineLearning,3bt5f5,sildar44,1 point,Thu Jul 2 12:46:29 2015 UTC,"I think it's much more than a few hours if someone doesn't know how to install stuff in the python ecosystem, what IPython notebook is etc."
MachineLearning,3bt5f5,bonoboTP,1 point,Thu Jul 2 13:08:09 2015 UTC,"Continuum Anaconda or Enthought Canopy make this setup pretty easy - much easier than setting up the native OS stuff. The hardest part is Caffe - the notebook is basically ""type stuff in, hit enter, repeat"" and is installed by default in Anaconda and Canopy.  Actually getting in touch with scientific Python world takes more (for sure) but installing pre-reqs can be easy."
MachineLearning,3bt5f5,kkastner,1 point,Thu Jul 2 13:44:06 2015 UTC,"I don't know those and had some bad experience with such collections (they work for one thing, but then when you try to add other python stuff there can be compatibility problems, then you need to learn about virtualenv and similar things...).  I installed things with pip and compiling some of it myself. I know it took a lot of time to get familiar with what all these libraries do, how to install them, how to solve compilation issues, resolve dependency problems, etc.   Granted, I also had to learn a lot of Linux stuff along the way as well. Anyway, it's very hard to estimate how long something takes without knowing the background of someone. ""Programmer"" is very generic, one could be a C# ASP.NET web developer, or a Java GUI programmer and have no idea about scientific libraries in Python and how this whole thing works. It's called the ""curse of knowledge"" when we think that stuff we know is totally obvious and easy.  It may easily take up to a few days, if one gets problems along the way, needs to ask on forums or stackexchange.  Again, strongly depends on the background. While not likely on this subreddit, but some ""programmers"" can't even fizzbuzz (nothing personal), also there are many novices who are just interested in ML recreationally, so the spectrum is huge.  This is not a discouragement, maybe the Anaconda and Enthought Canopy make it all much simpler. But don't be surprised if things take a bit longer."
MachineLearning,3bt5f5,bonoboTP,0,Thu Jul 2 14:01:50 2015 UTC,ugh can't someone just put all the right crap in a folder and upload it somewhere
MachineLearning,3bt5f5,skeddles,1 point,Mon Jul 6 02:35:22 2015 UTC,"Not really. You can just look at the IPython notebook via GitHub et copy/paste the program lines in your editor. You really don't need IPython. The rest is basically the installation of caffe, which has some documentation and many answers on the mailing list regarding install problems. The hardest part will be to fill relevant fields in the Makefile if needed, sure some will find it hard, but I really think most will succeed in a few hours at most :)"
MachineLearning,3bt5f5,sildar44,1 point,Thu Jul 2 15:16:35 2015 UTC,"(caffe mostly)   Tell me about it. I've spent 3 hours trying to figure out why I can't get it to install. I have every prerequisite and it still can't find hdf5.h for some reason. I wish this used a more polished framework, or something that has an actual package ideally."
MachineLearning,3bw45c,atmb4u,3,Thu Jul 2 17:09:41 2015 UTC,One of the more general forms of this would be Learning Activation Functions to Improve Deep Neural Networks.
MachineLearning,3bw45c,siblbombs,2,Thu Jul 2 17:14:49 2015 UTC,A recent example would be Parametric Rectified Linear Units (PReLU): http://arxiv.org/abs/1502.01852
MachineLearning,3bw45c,iori42,2,Thu Jul 2 17:12:17 2015 UTC,"Maxout networks can be thought of in this way. PReLu just adds an adaptable leak to a rectifier, whereas APL units are a bit more general (than maxout or PReLu) in the class of activation functions they can learn (in practice, for two of the three tasks they looked at, they seem to basically learn to be rectifiers)."
MachineLearning,3bw45c,dwf,1 point,Thu Jul 2 18:24:01 2015 UTC,"APL units?  Haven't heard of those, do you have a good reference, google isn't turning anything up?"
MachineLearning,3bw45c,melvinzzz,2,Thu Jul 2 18:46:59 2015 UTC,"From Learning Activation Functions to Improve Deep Neural Networks, which I found mentioned in @siblbombs response.   adaptive piecewise linear (APL) activation unit"
MachineLearning,3bw45c,JesseBuesking,1 point,Thu Jul 2 18:58:29 2015 UTC,Thanks!
MachineLearning,3bwki8,mizay7,1 point,Thu Jul 2 19:11:31 2015 UTC,"I'm a bot, bleep, bloop. Someone has linked to this thread from another place on reddit:   [/r/rstats] Asked for help in r/machinelearning but there is seemingly no activity. Looking to make wordclouds out of LDA topics   If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads. (Info / Contact)"
MachineLearning,3bwki8,TotesMessenger,0,Thu Jul 2 23:54:52 2015 UTC,"Not got time to look at the example, but check out the wordcloud package you might be able to achieve what you want with that."
MachineLearning,3bta4h,samim23,1 point,Wed Jul 1 23:35:58 2015 UTC,Very cool! I'll mess with this later.
MachineLearning,3bt60n,JasonSilvermann,1 point,Wed Jul 1 23:01:49 2015 UTC,"I would both subscribe to a blog like and and work through the problems, but how would yours be different than the one here? http://deeplearning.net/tutorial/  Perhaps, your blog could be less technical and a bit more intuitive...maybe the exercises could utilize Torch or deeplearning4j instead of Theano"
MachineLearning,3bt60n,the1witt,1 point,Thu Jul 2 01:13:59 2015 UTC,"I like the idea of getting less technical, and more practical.  Why is this important to you?"
MachineLearning,3bt60n,the1witt,2,Thu Jul 2 15:20:07 2015 UTC,"It lessens the initial learning curve. When you're learning a new subject and immediately have a lot of equations and symbols thrown at you, it can be discouraging."
MachineLearning,3bt60n,devDorito,1 point,Fri Jul 3 19:36:24 2015 UTC,"I would love it if you dove straight into examples and then explained how it worked step by step. I would definitely subscribe to it if it was in a language I was interested in as well.   It would be even better if you took and author's papers and showed how to implement what they're saying. As a person who's a programmer but not a computer scientist, reading these formulas and algorithms people have implemented is more like reading another language than it is like reading something I would understand.  For example, a step by step solution to creating a NEAT network with recurrency and loopbacks from scratch would be very useful to me. (no external libraries, just straight, readable code)  If you get a blog started up, I will subscribe and regularly visit it. If it's good, i may even consider donating."
MachineLearning,3bt60n,devDorito,1 point,Thu Jul 2 01:57:57 2015 UTC,Why would learning more about Deep Learning be interesting to you?
MachineLearning,3bt60n,devDorito,1 point,Thu Jul 2 15:30:52 2015 UTC,"I have a always been interested in ai and ai research, but my main goal is to set the ai up to play games, and for that id like to catch up on some of the current ai/neural networking techniques.   My favorite part of it is taking any given game and defining what it meas to be successful in that game."
MachineLearning,3bt60n,UTD_Vagrant,1 point,Thu Jul 2 17:03:48 2015 UTC,"That's so cool, that's how i first gained my interest in AI :)  Why is your main goal to use AI for video games?  And are you sure your interested in deep learning or might you be more interested in reinforcement learning (or a lot of game AI is search algorithms, like A*, or minimax)?"
MachineLearning,3bt60n,TheSreudianFlip,1 point,Thu Jul 2 17:51:46 2015 UTC,"Thanks, and I'm positive it's neural nets, not A* or other ai algorithms/languages, as those are slightly interesting... but they don't have as much potential to surprise us on a comparatively level playing ground as much as a neural net does. Definitely interested in DL applied to VG's."
MachineLearning,3bt60n,TheSreudianFlip,1 point,Thu Jul 2 18:00:02 2015 UTC,"Yes,Yes"
MachineLearning,3bt60n,TheSreudianFlip,1 point,Thu Jul 2 03:49:47 2015 UTC,Why would you benefit from deep learning examples?
MachineLearning,3bt60n,teluthen,1 point,Thu Jul 2 15:31:17 2015 UTC,"Yes, I really would!"
MachineLearning,3bt60n,teluthen,1 point,Thu Jul 2 05:59:22 2015 UTC,Awesome! Why would that be cool to you?
MachineLearning,3bw7wz,LLCoolZ,1 point,Thu Jul 2 17:38:46 2015 UTC,The reddit effect is starting to really slow it down. Awesome though! I'm loving this.
MachineLearning,3bw7wz,Notlambda,1 point,Thu Jul 2 21:26:14 2015 UTC,The link is down.
MachineLearning,3bt5tk,SpaceEnthusiast,8,Wed Jul 1 23:00:16 2015 UTC,"There's quite a lot, for example:   http://arxiv.org/abs/1412.7149 http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning http://arxiv.org/abs/1404.0736 http://arxiv.org/abs/1412.1442 http://arxiv.org/abs/1405.3866 http://arxiv.org/abs/1412.4526 http://ttic.uchicago.edu/~haotang/speech/06638949.pdf http://arxiv.org/abs/1504.04788 http://arxiv.org/abs/1506.04449 http://arxiv.org/abs/1412.6550"
MachineLearning,3bt5tk,kjearns,1 point,Thu Jul 2 00:35:56 2015 UTC,Thanks. That's gonna be a long read!
MachineLearning,3bt5tk,davmre,6,Sun Jul 5 06:28:34 2015 UTC,This has been explored by Rich Caruana (Do Deep Nets Really Need to be Deep?) and more recently by Geoff Hinton's group at Google (Distilling the Knowledge in a Neural Network). The basic approach is to train a shallower or smaller network to imitate the output of the full network.
MachineLearning,3bt5tk,kkastner,3,Thu Jul 2 00:18:56 2015 UTC,"Usually thinner and deeper is preferable to wider - Caruana's work actually needed many more parameters to get equivalent performance, despite the wording of the abstract it seems deep nets DO need to be deep. Having one fat layer is useful if you want ultra-parallelism, but the representation power for the same number of parameters is much less.  Depth is a seems to be a pretty important component to making these approximations work. See for example FitNets, also linked above or even the original Inception paper - thin deep networks (which seem to require multi-level ""hints"" to train... even though they should train directly) are really powerful!"
MachineLearning,3bt5tk,robertsdionne,2,Thu Jul 2 13:33:06 2015 UTC,"Compressing Neural Networks with the Hashing Trick  http://jmlr.org/proceedings/papers/v37/chenc15.html  Oh, that's http://arxiv.org/abs/1504.04788 from kjearns's comment."
MachineLearning,3bt5tk,MrTwiggy,2,Thu Jul 2 01:12:30 2015 UTC,"Davmre offers some great links, but another one you might want to consider is the paper on Model Compression, which was cited by Hinton's Dark Knowledge paper. Offers some more generalizable insights on how to compress unwieldly ensembles of models into a neural network that can approximate the function it represents. Mainly focused on generating synthetic data to help in the approximation/compression."
MachineLearning,3bt5tk,mat_kelcey,1 point,Thu Jul 2 01:48:14 2015 UTC,Thanks. That's cool.
MachineLearning,3bt5tk,robertsdionne,2,Sun Jul 5 06:29:08 2015 UTC,Somewhat related is the idea of quantization; if you train using weights that are double precision how much do you lose by just converting them all to floats? not a lot it turns out. An extreme example of quantization is 1 bit SGD
MachineLearning,3buv7x,live_love_laugh,2,Thu Jul 2 09:59:10 2015 UTC,I am in
MachineLearning,3buv7x,aptgetcode,1 point,Thu Jul 2 10:54:17 2015 UTC,Cool! Shall I make an empty github-project and make you a collaborator on it? What language would you prefer to make it in?  edit: I already made an empty github repository. What's your github name? I'd like to add you as a collaborator.
MachineLearning,3buv7x,aptgetcode,1 point,Thu Jul 2 11:45:48 2015 UTC,right now I think python as the future in programming language... I am good at it.  don't get surprised that my github repo is empty :P ... cause I am too lazy to upload and my internet bandwidth is less... https://github.com/nishantrpai  .... this my account :P....I am impressed looking at your account.
MachineLearning,3buv7x,aptgetcode,1 point,Thu Jul 2 17:05:04 2015 UTC,I am so eager... can we talk on hangouts?
MachineLearning,3buv7x,teluthen,1 point,Thu Jul 2 17:08:27 2015 UTC,PM sent
MachineLearning,3buv7x,teluthen,2,Fri Jul 3 09:59:37 2015 UTC,"This is the kind of too useless but definitly too insteresting project.  Would be a pleasure to follow the project (github?) even if I won't go in.  Note that i've done with JS + D3.JS a kind of things like that where you had living being of a species which could move on a cell map. Each cell produces some resources (each turn resource += rand[min, max] depending on the cell) and each ""agent"" could move, eat, reproduce. All parameters could be set at the begining with a polution factor (overconsume lower cell production), a amount of food needed to reproduce, etc...  Such kind of experiments can show/teach you various things about evolution etc.. Really instructive. That feeling when u created something looking like life.  (Sry for kinda poor english)  EDIT: How will the dots choose what they will do? Do it random? Are they able to learn? Do you want to get datas and then improve dots algorithm?"
MachineLearning,3buv7x,teluthen,1 point,Thu Jul 2 11:20:38 2015 UTC,I'm surprised that I have two positive replies already. :-D  So basically my idea was that the dots are learners. So in the beginning they will do random stuff and then slowly they find out what actions lead to pleasant feelings and what actions lead to unpleasant feelings. And since their needs are constantly changing (for example if they spent 2 minutes dancing with other dots the need to move slower and be alone might start to become more demanding) their actions will also constantly change.
MachineLearning,3buv7x,teluthen,1 point,Thu Jul 2 11:48:32 2015 UTC,Nice! :) Starred your project on github. Same nickname.  Would check it and maybe collaborate if u dont mind and if chosen language is python or JS :p (JS kinda fine for easy visualisation)
MachineLearning,3buv7x,NasenSpray,1 point,Thu Jul 2 12:51:00 2015 UTC,"My preference would be swift, because I like the safety and cleanliness of that language and I like that I think it's a fast language. But of course I know that then it's only usable on OS X / iOS.  I'm also okay with JS (only ES6 though cause of the syntactic sugar like classes), but I'm afraid that JS would be too slow to create something that could work at a reasonable frame-rate.  I don't know python myself so that would not be my preference.  However, I guess it's up to the guy who will do the most programming. And since I suck at AI-programming that would probably not be me."
MachineLearning,3buv7x,mynameisvinn,1 point,Thu Jul 2 13:12:38 2015 UTC,"Well, did u chose? Saw that u added me to the project, nice ;)  Is it planned to use some little project management tools, like the super trello.com? So that we can easily split work/get involved and see evolution/roadmap"
MachineLearning,3buv7x,autowikibot,1 point,Thu Jul 9 09:02:11 2015 UTC,It seems it'll be python. So you'd like to join? So far we don't have any project management tools and since I myself am slightly swamped by my day-job it's moving a bit slow.  The other guy and I are communicating via google hangouts. Would you like to pm me your email address so you can join in on the conversation?
MachineLearning,3bub7m,grandzooby,2,Thu Jul 2 05:24:57 2015 UTC,I'm interested. Is this the article that spurred your interest? https://hbr.org/2015/06/inventory-management-in-the-age-of-big-data
MachineLearning,3bub7m,EngineeredEdge,2,Thu Jul 2 13:09:39 2015 UTC,I've actually worked in supply chain for quite a few years and have been studying machine learning for a couple years.  I'm trying to make an academic project of bringing the two together.
MachineLearning,3bub7m,EngineeredEdge,2,Thu Jul 2 17:17:52 2015 UTC,have you asked your company if there is any way you can use your data set? like if you obscure the data in some way? Or use that data to generate another set with the same characteristics?
MachineLearning,3bu92z,john_philip,1 point,Thu Jul 2 05:02:08 2015 UTC,"The software part is lacking differentiation by language (Python, +- Theano, JS, etc').  Could use more blogs &/or articles.  (Sorry, but the previous ""AwesomeXForY"" Gits were very good. Standards are made :))"
MachineLearning,3bu92z,ddofer,1 point,Thu Jul 2 10:18:47 2015 UTC,Nice stuff
MachineLearning,3bs7ao,pierrelux,2,Wed Jul 1 18:47:53 2015 UTC,Where's the link to download the data set? I see a footnote in the draft saying that it's forthcoming -- is it up somewhere?
MachineLearning,3bs7ao,VelveteenAmbush,3,Fri Jul 3 03:03:46 2015 UTC,"Hi! Co-author here: we are just putting the finishing touches on the dataset, and it will hopefully be made available in the next week (definitely before July 20th)."
MachineLearning,3bs7ao,Loweryder,2,Fri Jul 3 17:38:03 2015 UTC,Awesome! Thank you. I think this data set is pretty exciting and really appreciate that you're making it available.
MachineLearning,3bs7ao,VelveteenAmbush,1 point,Fri Jul 3 18:28:26 2015 UTC,"I doubt it's the same one, but here is the ubuntu IRC log for a month at the end of 2012. You can google around for more recent ones, but I had this one handy:  https://www.amazon.com/clouddrive/share/7y2LEdj0ZL701I8C54lSdHGRYv72J2W37jdaY5zFsnc?ref_=cd_share_link_copy"
MachineLearning,3bqcio,mrkulk,1 point,Wed Jul 1 08:10:57 2015 UTC,"Proper link.  I think this problem is not going to go away. Reddit needs to make an exception for arxiv. If the pdf link is posted, it should refuse and ask the submitter to post the abs url instead."
MachineLearning,3bqcio,physixer,1 point,Thu Jul 2 00:01:07 2015 UTC,reddit is not going to do that for a tiny subreddit. but mods could make a rule and delete submissions without the right url.
MachineLearning,3btmw3,YourWelcomeOrMine,9,Thu Jul 2 01:27:36 2015 UTC,was it ever?
MachineLearning,3btmw3,dfarber,2,Thu Jul 2 01:39:51 2015 UTC,Weka was never important.
MachineLearning,3btmw3,GoldmanBallSachs_,2,Thu Jul 2 05:30:33 2015 UTC,"In my opinion, the only thing Weka is good at is a unified interface to a large number of methods."
MachineLearning,3btmw3,Foxtr0t,-2,Thu Jul 2 11:45:45 2015 UTC,"It's useful when you use Java. I think it's the best ML library with in-memory ML algorithms for Java (although, like Java, it's somewhat verbose)"
MachineLearning,3btmw3,stolzen,3,Thu Jul 2 13:41:54 2015 UTC,haha
MachineLearning,3btmw3,j_lyf,0,Fri Jul 3 00:38:46 2015 UTC,whats wrong? I've been working with it for 2 years and it's good. care to suggest better ML libraries for java?
MachineLearning,3btmw3,stolzen,2,Fri Jul 3 09:25:53 2015 UTC,Python
MachineLearning,3btmw3,GoldmanBallSachs_,1 point,Fri Jul 3 22:21:00 2015 UTC,very funny. try using it for a production system written in java
MachineLearning,3btmw3,stolzen,1 point,Sat Jul 4 15:14:03 2015 UTC,use mlpack
MachineLearning,3btmw3,j_lyf,2,Sun Jul 5 19:35:49 2015 UTC,There doesn't seem to be a java binding. Or I just cant find it?
MachineLearning,3bt7d3,futrawo,2,Wed Jul 1 23:12:51 2015 UTC,"I'm not sure what is different about the learning to rank problem, but I'll assume that this is equivalent to the classically studied setting in statistics based on ordinal regression. The most common approach for ordinal regression is to assume that there exist underlying real (latent) values but you observe a binned version, e.g., everyone has an age but a data set of persons only includes age groups as the covariate. Under this setting, the metrics used are simply those induced by the standard regression case."
MachineLearning,3brme7,DSLSharedTask,3,Wed Jul 1 16:18:07 2015 UTC,"I didn't see this mentioned,  but msr also has sent2vec."
MachineLearning,3brme7,Articulated-rage,1 point,Wed Jul 1 17:29:12 2015 UTC,Here's an unconventional way based on high dimensional computing if you're interested: http://arxiv.org/abs/1412.7026
MachineLearning,3brme7,ScarShark,-2,Wed Jul 1 18:36:47 2015 UTC,"""learnt""  We humans don't deserve this. None of it."
MachineLearning,3brme7,yggdrasilly,1 point,Wed Jul 1 16:46:39 2015 UTC,http://grammarist.com/spelling/learned-learnt/
MachineLearning,3bsa7v,simonhughes22,1 point,Wed Jul 1 19:08:23 2015 UTC,"hi simon, thanks for the link. it sounds like yuri, the author, either works for you directly or somewhere in your organization. i don't suppose you could ask yuri to release his code?"
MachineLearning,3bsa7v,lfod,1 point,Wed Jul 1 20:11:22 2015 UTC,"He's actually my boss. Which code are you interested in? There are many pieces to it. There's the code I wrote to compute the similarity, then he used gephi to do the graph analysis \ clustering. And finally D3 to do the visualization. The latter code can be scraped from the page itself if you're interested, although I think it's a little messy as it was adapted from a different visualization that did a lot more. We had to trim it down and simplify for mobile useage."
MachineLearning,3brbip,solololol,3,Wed Jul 1 14:57:49 2015 UTC,"Startup: Indico, man: Alec Radford. https://indico.io/"
MachineLearning,3brbip,Foxtr0t,3,Wed Jul 1 15:24:55 2015 UTC,We only have GRU models in production :)
MachineLearning,3brbip,alecradford,2,Wed Jul 1 19:20:00 2015 UTC,Which means they are LSTMs :)
MachineLearning,3brbip,flukeskywalker,2,Wed Jul 1 19:23:11 2015 UTC,Might as well expand my original search. Who else is using GRUs in production that you know of?
MachineLearning,3brbip,kjearns,1 point,Thu Jul 2 01:04:10 2015 UTC,"Interesting, do you find they work better than LSTMs or are you using them for other reasons?  I've personally never had success with GRUs."
MachineLearning,3brbip,alecradford,5,Wed Jul 1 22:28:47 2015 UTC,"In general they are pretty interchangeable, but GRU is ~30-50% faster since it has fewer gates.  ngram BOW baselines are still very strong for many of the kinds of problems we deal with, so it can be difficult to justify models that are 10-100x slower/computationally expensive so any speed up we can get is crucial in deciding to deploy a LSTM/GRU model."
MachineLearning,3brbip,knighton_,1 point,Thu Jul 2 02:40:22 2015 UTC,"thanks a lot, very interesting to hear it straight from the source"
MachineLearning,3brbip,knighton_,1 point,Thu Jul 2 00:55:19 2015 UTC,"Perfect example, thanks a lot. I don't imagine the total number of companies being very high, but I'll update this as I find more."
MachineLearning,3bnsbv,onewugtwowugs,9,Tue Jun 30 18:26:25 2015 UTC,"Its pop science but James Gliek's the information is worth a read especially if you don't know any information theory.  I've also read a couple of Michael Lewis' books (largely since they are in every airport bookstore) and they tend to be quite readable and a couple of them deal with areas of sports and business where statistics (and ml) have application (ie moneyball, the big short, flash boys)"
MachineLearning,3bnsbv,micro_cam,3,Tue Jun 30 21:00:19 2015 UTC,"As mentioned, The Information is a great book. I'm thinking about reading Chaos as well, but that might not happen this summer.  Will look up Michael Lewis, thanks!"
MachineLearning,3bnsbv,sv0f,3,Tue Jun 30 21:18:18 2015 UTC,"Chaos is an amazing amazing book. The chapter ""The Dynamical Systems Collective"" made me want to go to graduate school!"
MachineLearning,3bnsbv,Kalessar,1 point,Wed Jul 1 00:06:48 2015 UTC,Chaos is definitely worth it
MachineLearning,3bnsbv,gabrielgoh,6,Wed Jul 1 14:53:54 2015 UTC,"This isn't the lightest reading you'll find here, but the writing style is entertaining enough that you can skip the math and ponder the deeper philosophical ideas. I recommend E.T Jayne's Probability Theory: The Logic of Science"
MachineLearning,3bnsbv,ChefLadyBoyardee,2,Tue Jun 30 22:29:23 2015 UTC,"As someone without formal mathematical training, that book made no sense to me at first. The course Introduction to Mathematical Thinking on Coursera helped me get through the first few chapters, and beginning to see the big ideas Jaynes presents. Just leaving this note here in case it helps anyone else."
MachineLearning,3bnsbv,quirm,16,Wed Jul 1 07:57:19 2015 UTC,"Andrei Karpathy's Blog is really good. E.g. ""The Unreasonable Effectiveness of Recurrent Neural Networks"""
MachineLearning,3bnsbv,steelypip,11,Tue Jun 30 21:32:03 2015 UTC,"None of these are directly related to machine learning, but are fun reads that are tangential to the subject:  The Information by James Glick - a history of Information Theory  The Signal And The Noise by Nate Silver - on what can and can't be predicted  The Theory That Would Not Die by Sharon Bertsch McGrayne - a history of the fall and rise of Bayes Theorem and Bayesian reasoning."
MachineLearning,3bnsbv,SoundOfOneHand,1 point,Tue Jun 30 21:07:18 2015 UTC,"The Information was a great read and just the type of book I would hope to get recommended! (too bad I failed to mention it)  I have had The Signal and the Noise on my to read list for a while now, but I have never heard of your last recommendation. Sounds interesting and will definitely look it up, thanks for your suggestions!"
MachineLearning,3bnsbv,InconspicuousTree,3,Tue Jun 30 21:17:21 2015 UTC,In the same vein I enjoyed The Drunkard's Walk
MachineLearning,3bnsbv,Gubrial,1 point,Tue Jun 30 22:09:51 2015 UTC,The Signal and the Noise is amazing for learning how to think statistically. I loved the part about Deep Blue
MachineLearning,3bnsbv,simonhughes22,5,Wed Jul 1 06:28:00 2015 UTC,"Vehicles by Valentino Braitenberg is a pretty fun read, even if you aren't very interested in robotics."
MachineLearning,3bnsbv,gwern,5,Wed Jul 1 01:24:51 2015 UTC,"Read the book, Talking Nets: An Oral History of Neural Networks.   Great historical read - it takes the form of long interviews with researchers in neural networks from the beginning up until the wave right before ""deep learning"" (I think the book came out in 1998). It does have an interview with Geoff Hinton (though at a time when he thought the next big thing was the Helmholtz Machine). Also Sejnowski, Lettvin, Rumelhart, and many others. If those names ring a bell or they don't then this book is worth your time :) Among just those mentioned are the (co-)inventors of backpropagation and boltzmann machines (i think).   There is not a single line of mathematics, so in that sense it's light. Mathematical ideas are discussed as well as (auto)biographical context. Really cool stuff."
MachineLearning,3bnsbv,simonhughes22,7,Wed Jul 1 06:59:42 2015 UTC,"That won't teach you anything about Machine Learning. Hoftstadter believes it's a waste of time. I've found the most useful thing to do is to watch some of the many Coursera courses, such as the one from Andrew Ng. They are reasonably approachable, although do involve some math, but it's going to be hard to get away from that. I found this book to be a very good introduction (http://www.amazon.com/s/ref=nb_sb_ss_c_0_12?url=search-alias%3Dstripbooks&field-keywords=tom+mitchell+machine+learning&sprefix=tom+mitchell+machine+learning%2Caps%2C186), although since I've read it, due to the popularity of Data Science the price has exploded. You can probably find it on a non use site cheaper and import it (even better if you're not in the US). They artificially jack up the prices of text books for Americans."
MachineLearning,3bnsbv,shimst3r,6,Tue Jun 30 19:33:01 2015 UTC,"That won't teach you anything about Machine Learning. Hoftstadter believes it's a waste of time.   One interesting thing about that is that ML has, the past few years, been making a lot of inroads into the sort of thing Hofstadter was into; the recent word-vector stuff for solving verbal analogies is something he would have appreciated, and using pairs of RNNs for natural language translation is most intriguing from a Hofstadterian perspective."
MachineLearning,3bnsbv,xamdam,4,Wed Jul 1 02:52:34 2015 UTC,"Nice point @gwern.  His PhD descendant Melanie Mitchell has done some work on integrating deep neural networks and the Hofstadter-style models (which could accurately be described as ""symbolic"").   And the rest of this mini-rant is not directed at @gwern but at the comment he replies to. I think it's off-base to characterize the Hofstadter tradition as antithetical to machine learning. The point is that there are some cognitive abilities that contemporary neural networks, while impressive,  are not even close to modelling. Word2vec and even hot off the press Question-Answering recurrent nets trained with reinforcement learning-based models of attention can't even come close to telling you that the town where I grew up is a Faulkner-esque decaying aristocracy. Or a Hofstadter example [1] - to think of a meeting as “an emperor-has-no-clothes situation”. So rather than sit around and wait until we have enough data and compute power to try to solve these paradigm open problems in language understanding using neural networks, they are looking other models of cognitive processes to try and shed light on how the brain does it.   [1] - The Man Who Could Teach Machines to Think, Wired 2013"
MachineLearning,3bnsbv,bge0,1 point,Wed Jul 1 07:28:20 2015 UTC,"I didn't say Hofstadter's views aren't valuable, just that he doesn't agree with Machine Learning, which is quite well documented. To quote Hofstadter in the article you reference  “To me, as a fledgling AI person,” he says, “it was self-evident that I did not want to get involved in that trickery. It was obvious: I don’t want to be involved in passing off some fancy program’s behavior for intelligence when I know that it has nothing to do with intelligence. And I don’t know why more people aren’t that way.”  He refers to it as just engineering and not really tackling intelligence. In that part he's talking about Deep Blue, the chess AI, which is not really doing Machine Learning, but I doubt his views on ML are any different.  I actually agree with a lot of his views on how our higher-level cognition is working and how the mind emerges from the interactions of a lot of low level processes, but I also believe that we can learn a lot from ML, by building these lower level processes. I'd love to know what he thinks about some of the recent advances in deep learning. For instance in that article it talks about how a machine can't easily recognize hand-written A's. Well now they can pretty darned well thanks to deep learning. I doubt his views have changed however."
MachineLearning,3bnsbv,GibbsSamplePlatter,4,Tue Jul 7 02:51:33 2015 UTC,Another good MOOC is the one on Neural Networks by Geoffrey Hinton also available via Coursera.
MachineLearning,3bnsbv,simonhughes22,3,Tue Jun 30 20:30:16 2015 UTC,"I think that's an unfair way to put Hofstadter's position. From what I've read and heard, his position is more like ""neural networks aren't the whole story for intelligence""."
MachineLearning,3bnsbv,walrusesarecool,1 point,Wed Jul 1 07:04:33 2015 UTC,"I will be traveling around China mostly by train this summer, so unfortunately Coursera courses is not a viable option. ML text books is not really what I was looking for, but thanks anyway."
MachineLearning,3bnsbv,Articulated-rage,5,Tue Jun 30 21:20:52 2015 UTC,https://github.com/coursera-dl/coursera
MachineLearning,3bnsbv,patrickSwayzeNU,4,Wed Jul 1 12:31:33 2015 UTC,Fyi you can download the videos to watch offline.
MachineLearning,3bnsbv,AshRolls,1 point,Wed Jul 1 01:21:37 2015 UTC,This. I watch all the videos from my dumb old Nexus 7 tablet.
MachineLearning,3bnsbv,srkiboy83,1 point,Wed Jul 1 12:55:12 2015 UTC,"You can download the videos onto your laptop from Coursera, or using something like tubemate."
MachineLearning,3bnsbv,Articulated-rage,5,Tue Jul 7 02:53:47 2015 UTC,"Books by Richard Dawkins for biological understanding of evolution. (GA's are like reinforcement learning) Selfish Gene (coining the term meme), Climbing Mount impossible, (Fitness landscapes!) The extended phenotype, the blindwatchmaker.  Also the red queen by ridley. And the meme machine by Susan Blackmoore  Others that are interesting include:  Complexity by Roger lewin, Emergence: From Chaos to Order by holland,  programming the universe by seth lloyd  Philosophy from bertrand russell and karl Popper,  or recently by dan dennett  Also books by Peter Medwar such as plutus republic"
MachineLearning,3bnsbv,rkabir,1 point,Tue Jun 30 22:10:22 2015 UTC,I second Dennett. His stuff on the intentional stance is great.
MachineLearning,3bnsbv,wildething,1 point,Wed Jul 1 16:38:36 2015 UTC,I second Selfish Gene and Red Queen.  Since we have similar taste I'm going to check out some of the others you mentioned.
MachineLearning,3bnsbv,sodermalm,8,Thu Jul 2 13:23:27 2015 UTC,"I highly recommend Superintelligence: Paths, Dangers, Strategies by Nick Bostrom. It's a very interesting read about where all this machine learning may be heading.  Nick Bostrom is a professor at Oxford. He has a background in physics, computational neuroscience, and mathematical logic as well as philosophy."
MachineLearning,3bnsbv,yaolubrain,2,Tue Jun 30 21:34:57 2015 UTC,"I recommend Bernstein's ""Against the Gods: The Remarkable Story of Risk""."
MachineLearning,3bsv15,SupportVectorMachine,4,Wed Jul 1 21:37:39 2015 UTC,Google is a private company not a a single university professor. Do you think apple thought it was a hoax to try to patent rounded rectangles?
MachineLearning,3bsv15,Goldbar7,1 point,Wed Jul 1 21:50:57 2015 UTC,"I agree that an entire company trying to ""make a point"" is quite a different thing from a professor somewhere trying to do the same thing.  And this point being made on behalf of Google would require that corporate brass be in on it.  It's a stretch ... but Google seems to like to stretch.  The Apple example seems somehow different to me.  Perhaps it's the distinct corporate philosophies of the two companies (other than the main one: make money).  One stresses design and closed-source platforms, while the other at least engages in open-source development and products that are largely cost-free (with the exception of the price of one's personal data).  I don't know.  I could be totally wrong about this.  But this just seems so ridiculous otherwise."
MachineLearning,3bsv15,GoldmanBallSachs_,1 point,Wed Jul 1 21:59:08 2015 UTC,"The Apple example seems somehow different to me. Perhaps it's the distinct corporate philosophies of the two companies (other than the main one: make money). One stresses design and closed-source platforms, while the other at least engages in open-source development and products that are largely cost-free (with the exception of the price of one's personal data).   You are giving too much credit to Google. They have a fiduciary relationship with shareholders are legally required to maximize profit (whether short or long term) as a for-profit public company. If this patent is approved you can be sure Google will act to enforce it."
MachineLearning,3bsv15,melvinzzz,3,Thu Jul 2 05:49:11 2015 UTC,"Via Betteridge's law, No.  Seriously though, are you aware that to file a patent requires the inventor to sign an oath which includes statements such as:  ""I hereby acknowledge that any willful false statement made in this declaration is punishable under 18 U.S.C. 1001 by fine or imprisonment of not more than five (5) years, or both.""?  Are you familiar with the Sarbanes–Oxley act and the requirements it makes on executives of publicly traded companies.  Are you aware the patent attorneys could be disbared for knowingly false statements?  Or the fact that such actions by Google (if supported on the executive level) could open them up to shareholder lawsuits?    Alas (and unsurprisingly), multibillion dollar market-cap companies are not structurally built to allow tomfoolery, so I think this is just a case of Google being serious.  Now, as other people have mentioned in the prior thread, this may be viewed by Google as a defensive play to prevent patent trolls, but at the end of the day the patent system is broken, and if granted, this would technically give Google the ability to sue for infringement.  Hopefully the management over at Google won't do that, but as we have seen, this is subject to change at shareholder whim."
MachineLearning,3bsv15,melvinzzz,1 point,Wed Jul 1 22:58:19 2015 UTC,"Well, I can't argue against your invocation of Betteridge's law here, so there's that.  But as for the other things you cite, is there anywhere in the patent application that a false statement is made?  If not, then none of the above apply.  Even if they are pulling a hoax, they can do so without making a single false statement or perpetrating an outright fraud.    And if they are indeed being serious, which I acknowledge they very well may be, the same strictures you mention would apply to the same patent that to me reads like it could be a hoax.  So I guess I don't see the difference."
MachineLearning,3bsv15,melvinzzz,1 point,Wed Jul 1 23:21:34 2015 UTC,"While I appreciate your optimism, I think you missed the greater point that large companies simply cannot 'pull a hoax', and I challenge you to find even a single example of a publicly traded company doing such a thing.  In fact, even individuals such as Sokal are exceedingly rare.  If Google wants to convince the government of something, they will lobby, which they spent over $15 million dollars on last year.  Creating an embarrassment for the USPTO would undermine those efforts.  I do understand the desire to believe the best, but corporations are not people, and thinking of them as such will only lead to disappointment and is potentially dangerous."
MachineLearning,3bsv15,melvinzzz,1 point,Wed Jul 1 23:50:30 2015 UTC,"Corporations aren't people?  Did Mitt Romney lie to me?  Is nothing true in this world?  Look, you're probably right.  Even in posting this I wasn't sure I believed it.  Yes, Sokal types are quite rare, but that's what would make this approach so ""special"" if that's what they were doing.  And that there aren't any ready examples of corporations doing it only reinforces that point instead of ruling out the possibility, however remote it is.  But would you at least agree that Google submitted a patent application for a process that does not appear to be novel in any substantive way?  As Google employs plenty of people who know a thing or two about ML, they are certainly more than aware of that fact.  So, if they are not trying to make a point by submitting a totally legitimate description of an old idea just to see if it gets through, then they are knowingly (and falsely) claiming ownership of an old idea.  The latter case seems like more of a potential PR nightmare than the former, at least to me.  But I'd be lying if I claimed to really understand the dank labyrinth of intellectual property and corporate law.  EDIT: I'm intentionally not citing Google's frequent, but innocuous, April Fools hoaxes as evidence of their willingness to conduct corporate tomfoolery.  But I'm going to mention it here just for the hell of it."
MachineLearning,3bsv15,rarimascarydino,1 point,Thu Jul 2 00:06:05 2015 UTC,"I will agree with you completely that it's a crummy patent, and the technical people should know better.  However my guess is that what happened (having seen this happen in other organizations) is:  1) Tech guy makes something cool 2) Management and or an incentive system system compels said tech guy to talk with in house patent lawyer. 3) Patent lawyer writes things up, advises tech guy 'Don't research into prior art' (standard advice), 'Let's make the claims as wide as possible, it's the patent offices job to narrow them' (also somewhat common). 4) Tech guy either A) Doesn't care that he know prior art cuz he's getting a bonus or B) Lawyer tells him, 'Don't worry, it's the patent offices job' 5) ...Profit?"
MachineLearning,3bru35,AintNoFortunateSon,2,Wed Jul 1 17:13:25 2015 UTC,getting a job
MachineLearning,3bru35,fdasssafsda,1 point,Wed Jul 1 17:40:50 2015 UTC,"Kind of related, there is a lot of work on machine learning and automated theorem proving https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-792.pdf"
MachineLearning,3bru35,Rickasaurus,1 point,Wed Jul 1 17:59:17 2015 UTC,There's a strong overlap between Bayes Nets and Bayesian Epistemology.
MachineLearning,3bnqdf,john_philip,3,Tue Jun 30 18:12:49 2015 UTC,"What an excellent set of videos!  I think I know what I'll be doing with my ""free time"" this summer."
MachineLearning,3bnqdf,grandzooby,1 point,Tue Jun 30 19:03:16 2015 UTC,"I admire your proactive self discipline, good sir. I tip my hat to you."
MachineLearning,3bnqdf,brouwjon,2,Wed Jul 1 12:59:23 2015 UTC,"This material is great, my lecturer used the slides for a 4th year data mining course."
MachineLearning,3bnqdf,bckygldstn,2,Tue Jun 30 19:58:02 2015 UTC,Great set of videos. That textbook is pretty good too.  A machine learning class I took a few months ago used that textbook.  Can't beat free.  We also used chapter 3 of this resource for the Perceptron part: http://ciml.info/
MachineLearning,3bnqdf,walterj89,1 point,Wed Jul 1 03:52:30 2015 UTC,My concern with these authors is that they're frequentists. Can anyone address that?
MachineLearning,3bnqdf,classicalhumanbeing,1 point,Wed Jul 1 05:25:41 2015 UTC,"I'm reading a book by Alan Downey - ""Think Bayes"".  In one of the early examples, he shows how the frequentist method of looking at a simple problem yields clearly bad results compared to the Bayesian approach.  I don't know enough to know if this is just because it's a simple case.  But if not, and globally the Bayesian approach is better, why are there frequentists at all?  Does it really only boil down to the idea that Bayesians accept that they use some subjective decisions in their models and frequentists work under the assumption that their modeling is completely objective?"
MachineLearning,3bnqdf,grandzooby,3,Wed Jul 1 08:36:45 2015 UTC,Bayesian techniques are often a lot more computationally intensive. That's why frequentist approaches were almost ubiquitous until the advent of personal computers.
MachineLearning,3bnqdf,bluecoffee,2,Wed Jul 1 10:09:28 2015 UTC,Frequentists were the orthodoxy forever except for the last decade when bayesians took over. That's a separate discussion re bayesian v frequentist - but it's my impression that Trevor Hastie and Robert Tibshirani are frequentists.
MachineLearning,3bnqdf,classicalhumanbeing,1 point,Wed Jul 1 08:44:07 2015 UTC,"There are also cases where the Bayesian method gets you the wrong answer but the frequentist method gets you the correct answer.   They are two different but not orthogonal methods of approaching problems.   There is also a lot of what I'll call ""convenient"" bayesians. Because you need some prior belief in a Bayesian approach, but most people choose a ""convenient"" prior that makes the math nicer rather than trying to come up with a prior that actually models what they really think their true belief is."
MachineLearning,3bn9oy,votadini_,33,Tue Jun 30 16:15:10 2015 UTC,"Juergen puts his finger on an important point. The community should be aware of the fact that being credited for one's scientific achievements is the main driving factor for people to do research--especially since industry pays so well for machine learning experts.  While all of the things he addresses are of a historical nature, it is quite common among currently published material to not do literature research properly.  I could point to several cases among my peers. In one case the exact ideas were published by others more than 2 years after the original paper. It was leading industry researchers and they even got the paper accepted into a major conference--even though the most stupid google scholar search directly lead to the prior work.  This is detrimental in so many ways. For one, young researchers are disouraged for not being attributed correctly. Further, research is redone.  This goes in line with Sermanet's recent G+ post here: https://plus.google.com/u/0/+PierreSermanet/posts/VngsFR3tug9."
MachineLearning,3bn9oy,sieisteinmodel,10,Tue Jun 30 18:11:37 2015 UTC,"Ironically LeCun signal boosted Sermanet's post via FB. I have a bit of a sense is that there might be a bit of a conspiracy to ignore Schmidhuber, not on his intellectual merits but due to his personality quirks.  Not saying that's fair or anything."
MachineLearning,3bn9oy,xamdam,2,Tue Jun 30 20:56:03 2015 UTC,Shots fired.
MachineLearning,3bn9oy,shaggorama,6,Tue Jun 30 20:20:40 2015 UTC,did you expect there to be?
MachineLearning,3bn9oy,j1395010,6,Tue Jun 30 16:41:17 2015 UTC,"Still, it's an important point to reflect upon. I guess the IJCNN contest omission was an important one."
MachineLearning,3bn9oy,XalosXandrez,5,Tue Jun 30 17:01:07 2015 UTC,maybe more people need to look at old science... focusing on the new leads to reimplementing the wheel.
MachineLearning,3bn9oy,j1395010,2,Tue Jun 30 17:34:47 2015 UTC,if you get a chance to do research you will probably find yourself more interested
MachineLearning,3brb5v,ArgonJargon,2,Wed Jul 1 14:55:02 2015 UTC,http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=AD0256582
MachineLearning,3brb5v,idsardi,1 point,Wed Jul 1 16:31:34 2015 UTC,Thanks a lot! What search engine do you use to find it?
MachineLearning,3brb5v,idsardi,1 point,Wed Jul 1 22:47:04 2015 UTC,Google scholar in this case.
MachineLearning,3bqgmb,Piz-dur,3,Wed Jul 1 09:10:46 2015 UTC,"Always go for numpy/scipy when you can. If you really can't, look into PyPy, which is able to execute ""loopy"" code quite a bit faster than the standard Python interpreter (but a lot of packages, including numpy and scipy iirc, are not available in PyPy)."
MachineLearning,3bqgmb,benanne,2,Wed Jul 1 09:16:31 2015 UTC,"When you must loop (when your operations can't be expressed in terms of numpy/scipy functions), scipy.weave can be helpful.  It lets you write inline c code to operate on numpy arrays.  This is often 100x faster than looping in python."
MachineLearning,3bqgmb,treebranchleaf,1 point,Wed Jul 1 13:27:19 2015 UTC,"Thanks a lot, I'll stick with numpy/scipy :)"
MachineLearning,3bqgmb,zardeh,1 point,Wed Jul 1 09:39:10 2015 UTC,Def numpy if it's that or code. tanh for example is all you need in regressive logic.
MachineLearning,3bqgmb,GibbsSamplePlatter,1 point,Wed Jul 1 10:44:15 2015 UTC,"Numpy is sometimes availible in PyPy, although you need to jupm through hoops to make it work."
MachineLearning,3bqgmb,virdvip,1 point,Wed Jul 1 14:51:31 2015 UTC,"Really depends. Usually operations can be done easily in Numpy/Scipy without loops.    If you're doing Theano, for the love of God, avoid loops. So slow."
MachineLearning,3bo9ev,D33B,2,Tue Jun 30 20:26:28 2015 UTC,Reservoir sampling is a well known interview puzzle.
MachineLearning,3bl8f8,rantana,36,Tue Jun 30 03:16:44 2015 UTC,"I came in expecting the title of this post to be misleading until I clicked the link and saw the patent is titled ""Classifying Data Objects."" Hopefully this is just a defensive patent to keep it out of the hands of patent trolls, but only time will tell."
MachineLearning,3bl8f8,terremoto,39,Tue Jun 30 04:57:44 2015 UTC,"I can't comment on the contents as I don't read patents, but I will comment on the title: always ignore the title of a patent. Patent titles are not the actual claims of the patent and are not enforceable. Google did not patent the classification of data objects. They patented a particular method for classifying data objects which is described in the body of the patent. The title is just a loose descriptor of the subject of the patent. The actual claims of the patent are contained in the body, and if you want to know what the patent covers, you have to read those claims and not just the title."
MachineLearning,3bl8f8,rkern,19,Tue Jun 30 09:34:03 2015 UTC,"Came here to say this. Having said that, Claim 1 is both independent and surprisingly vague, and probably does apply to a lot of existing classification techniques, as I understand it."
MachineLearning,3bl8f8,kylotan,8,Tue Jun 30 09:37:48 2015 UTC,That's the point of claim 1.
MachineLearning,3bl8f8,Anslab,1 point,Tue Jun 30 10:18:49 2015 UTC,Read the image classification example. It's a copy/paste for an image recognition algorithm.
MachineLearning,3bl8f8,bwaxxlo,5,Tue Jun 30 20:54:24 2015 UTC,"This is also an application, not a grant."
MachineLearning,3bl8f8,stiffitydoodah,9,Tue Jun 30 15:52:59 2015 UTC,"Hopefully this is just a defensive patent to keep it out of the hands of patent trolls, but only time will tell.   You don't need to patent something just to prevent others from patenting it - simply publish the invention to prove there is prior art."
MachineLearning,3bl8f8,sanity,2,Tue Jun 30 15:17:05 2015 UTC,Exactly. A real defensive patent is a patent that you believe has teeth but that you only intend to attempt enforcing if action is broth against you.
MachineLearning,3bl8f8,m1sta,-1,Tue Jun 30 16:40:34 2015 UTC,"simply publish the invention to prove there is prior art.   If that was enough, this patent and many other software patents would not have been granted."
MachineLearning,3bl8f8,terremoto,17,Tue Jun 30 23:54:54 2015 UTC,"FWIW, Google has a pretty strong record on this sort of thing."
MachineLearning,3bl8f8,dwf,7,Tue Jun 30 06:18:20 2015 UTC,I pray a troll never takes control of Google or Microsoft or Sun...oh wait. Shit. Well I hope it never happens to Google.  But this shouldn't be allowed for them or for trolls. Defensive patents like this shouldn't be necessary.
MachineLearning,3bl8f8,pohatu,52,Tue Jun 30 13:50:07 2015 UTC,Cosine similarity too.  This is bullshit.
MachineLearning,3bl8f8,breezehair,16,Tue Jun 30 04:21:36 2015 UTC,Why isn't this in arstechnica? They have already written similar articles when Apple does it.  This should be on the news.
MachineLearning,3bl8f8,maxToTheJ,1 point,Tue Jun 30 15:25:00 2015 UTC,"And what about this? http://www.google.com/patents/US7158961  I was noticing how vague and broad that patent is.  It seems to be describing any LSH implementation wherein the hash function family involves a dot product with a random vector.  I noticed this work out of MIT/Stanford is very similar, but does not cite the Charikar work: https://graphics.stanford.edu/courses/cs468-06-fall/Papers/12%20lsh04.pdf  I wonder if that paper falls under the google patent or not."
MachineLearning,3bl8f8,compsc,71,Fri Jul 3 23:17:35 2015 UTC,Supercharge your applications with Google's Matrix Algebra™
MachineLearning,3bl8f8,PowerLondon,7,Tue Jun 30 04:16:56 2015 UTC,In all honesty I wanted to both google that AND get results...
MachineLearning,3bl8f8,kalfasyan,1 point,Tue Jun 30 06:30:09 2015 UTC,"In this context, the original title of this PageRank paper gets a new, interesting meaning  ""The $25,000,000,000 Eigenvector: The Linear Algebra behind[sold by] Google"""
MachineLearning,3bl8f8,rasbt,41,Tue Jun 30 22:18:12 2015 UTC,"Given the gigantic amount of work and inventions in this space before this patent, I can't imagine that this patent would be enforceable... I hope."
MachineLearning,3bl8f8,blowjobtransistor,47,Tue Jun 30 03:58:42 2015 UTC,This is the most insane thing - you can't patent math.
MachineLearning,3bl8f8,spacecraycray,6,Tue Jun 30 04:47:42 2015 UTC,...not yet anyway
MachineLearning,3bl8f8,tehyosh,9,Tue Jun 30 14:33:20 2015 UTC,"I can't wait for ""Manipulation of symbols"""
MachineLearning,3bl8f8,CaptainBland,48,Tue Jun 30 18:02:51 2015 UTC,"If this patent gets enforced against competitors, I am the first person who says goodbye to all of it: google search, gmail, maps, and YouTube. This is so ridiculous that I hope no one in the patent office takes it seriously. What's next, the Latin alphabet?"
MachineLearning,3bl8f8,rasbt,6,Tue Jun 30 12:16:36 2015 UTC,+1
MachineLearning,3bl8f8,CrossfitFTW,1 point,Tue Jun 30 16:07:06 2015 UTC,"It's not bad, I did it a couple years ago and it's so refreshing.  Duckduckgo for search.  Fastmail for mail and calendar and CalDAV.   Here maps for maps with navigation...they are offline downloadable too!"
MachineLearning,3bl8f8,afunkthewmd,9,Tue Jun 30 23:36:06 2015 UTC,Kinda looks like they patented word embeddings. Heh.
MachineLearning,3bl8f8,sinjax,7,Tue Jun 30 06:10:20 2015 UTC,I could understand the patentibility of Skipgram with non-negative matrix estimation better because it is a quite specific implementation of vector space modelling. But this patent is very broad to say the least. It references Google's embedding techniques as examples. Might as well patent convolutional neural networks while they're at it.
MachineLearning,3bl8f8,MyrdinnSlothrop,2,Tue Jun 30 13:09:05 2015 UTC,"Noo, I love working with c-NNs"
MachineLearning,3bl8f8,Capn_Cook,9,Tue Jun 30 14:19:31 2015 UTC,"FYI this patent is in review, not granted. I would be surprised if it is granted based on the large amount of prior art readily available."
MachineLearning,3bl8f8,siblbombs,1 point,Tue Jun 30 13:35:07 2015 UTC,I'm a bit surprised that they even applied after the Alice decision.
MachineLearning,3bl8f8,stiffitydoodah,19,Tue Jun 30 22:52:47 2015 UTC,"The method of claim 1, wherein the high-dimensional space is a one thousand dimensional space.    So R1000 is off limits now? ^"
MachineLearning,3bl8f8,shele,17,Tue Jun 30 12:00:30 2015 UTC,"People had explored R1 to R999 before, but Google was the first company with the daring and expertise to breach four digit dimensionality.  Also, I call dibs on the technique of adding a zero to the end of a thousand dimensional vector."
MachineLearning,3bl8f8,rasbt,4,Tue Jun 30 13:21:03 2015 UTC,"And suddenly, there is a peak in the download number of packages for  PCA and manifold learning techniques for dimensionality reduction..."
MachineLearning,3bl8f8,farsass,2,Tue Jun 30 14:03:18 2015 UTC,The cure for the curse of dimensionality brought by google!!
MachineLearning,3bl8f8,MyrdinnSlothrop,7,Tue Jun 30 14:41:36 2015 UTC,"Looks like word embeddings but no mention of the Skipgram or Continuous Bag-of-Words approaches specifically. Surely they cannot patent vector space models this broadly? High dimensional VSMs are the basis of a lot of machine learning algorithms.  There's also the problem that the people in the patent office are generally simply not qualified to determine whether a particular software invention is patentable or not, leaving it up to the courts to decide whether a patent was valid when the owner tries to assert their rights to it. That means if you're a small company and you ""infringe"" on an invalid patent, you likely don't have the resources to fight the patent anyway (even if it's invalid).  So if I wish to start a company for natural language technologies, I can completely forget vector space models all together because there is nearly no way I can bring up the capital to fight Google. That is completely bonkers."
MachineLearning,3bl8f8,IR77,7,Tue Jun 30 12:58:35 2015 UTC,"Thanks for posting this. I came across one from Google last week as well, and was wondering about it.  "" OBJECT DETECTION USING DEEP NEURAL NETWORKS "" http://www.freepatentsonline.com/y2015/0170002.html   What is claimed is:    A computer-implemented method comprising: receiving an input image; generating a full object mask by providing the input image to a first deep neural network object detector that produces a full object mask for an object of a particular object type depicted in the input image, wherein the full object mask identifies regions of the input image that correspond to the object and regions of the input image that do not correspond to the object; generating a partial object mask by providing the input image to a second deep neural network object detector that produces a partial object mask for a portion of the object of the particular object type depicted in the input image; and determining a bounding box for the object in the image using the full object mask and the partial object mask."
MachineLearning,3bl8f8,voodoochile78,17,Tue Jun 30 13:58:05 2015 UTC,Relevant: http://www.theonion.com/article/microsoft-patents-ones-zeroes-599
MachineLearning,3bl8f8,Make3,1 point,Tue Jun 30 04:24:40 2015 UTC,"""java is just trillions of zeros and ones"", TIL java is in the terabyte + range"
MachineLearning,3bl8f8,johanvts,8,Wed Jul 1 02:10:06 2015 UTC,"Does anyone know how to read this? When they say:  ""What is claimed is: A method performed by one or more computers...[for classification]"". Is that to be understood as them claiming any method that does this, or a specific method that they describe elsewhere?"
MachineLearning,3bl8f8,kylotan,4,Tue Jun 30 07:59:38 2015 UTC,It means exactly the method described in that paragraph. A patent is self-contained and can't relate to external work.
MachineLearning,3bl8f8,johanvts,6,Tue Jun 30 12:53:51 2015 UTC,"Thanks, then this patent really does seem like a wide patent on classification."
MachineLearning,3bl8f8,-Ulkurz-,11,Tue Jun 30 13:19:45 2015 UTC,"Is this for real? There is no way, classification isn't something new..."
MachineLearning,3bl8f8,mycl,6,Tue Jun 30 04:46:39 2015 UTC,Surely there is prior art going back decades?
MachineLearning,3bl8f8,classicalhumanbeing,12,Tue Jun 30 10:33:43 2015 UTC,Fuck google.
MachineLearning,3bl8f8,singularai,4,Tue Jun 30 09:26:13 2015 UTC,I wonder how much they'll charge for a license.
MachineLearning,3bl8f8,piesdesparramaos,5,Tue Jun 30 03:57:17 2015 UTC,"I hope no researcher wants to work at Google anymore. Shame on you Google, and Larry and Sergey should feel specially embarrased as former Data Mining researchers..."
MachineLearning,3bl8f8,zuring,2,Tue Jun 30 09:12:58 2015 UTC,Perhaps this is an extended patent for word2vec?
MachineLearning,3bl8f8,tehgargoth,2,Tue Jun 30 13:07:34 2015 UTC,"This thing has ""prior art"" written all over it, they are trying to patent things that plenty of people have been doing for a looong time.  Probably just an overzealous lawyer."
MachineLearning,3bl8f8,say_wot_again,1 point,Tue Jun 30 14:25:04 2015 UTC,"But the system moved to first-to-file a few years ago, didn't it? Would that impact this?"
MachineLearning,3bl8f8,tehgargoth,5,Tue Jun 30 15:25:22 2015 UTC,"If something is in common use, the patent isn't enforceable.  The USPTO will pass them because it's easier to just pass unenforceable patents and let the courts correct it if someone is bold enough to actually try and sue someone with it than it is researching every patent that comes in.  Companies file all sorts of crazy patents that could never be enforced. I remember once seeing a patent for a doubly linked list."
MachineLearning,3bl8f8,say_wot_again,1 point,Tue Jun 30 15:34:12 2015 UTC,Thanks
MachineLearning,3bl8f8,tehgargoth,2,Tue Jun 30 15:37:04 2015 UTC,"on the other side of that, sometimes companies get a patent that is just relevant enough to hold up in court.  If you look at the Paltalk cases, they had a patent on ""3d avatars in an online world"" and got a bunch of money from microsoft for halo."
MachineLearning,3bl8f8,blackrat47,4,Tue Jun 30 16:09:52 2015 UTC,"Not google's fault. I mean, yeah, it's a little bit google's fault. But you should expect any company to do whatever they can to eliminate competition and further their interests.  The point is making sure they can't do it. This is entirely on the patent office for granting such a fucking stupid application."
MachineLearning,3bl8f8,decimated_napkin,10,Tue Jun 30 09:18:43 2015 UTC,"People are downvoting you because they are angry, but you are absolutely fucking right. As a company Google has no loyalty to the public and should be expected to do whatever they can to get ahead. What should also be expected is that our government doesn't set up such an assbackwards intellectual property system. It's that system that we should ultimately be criticizing, not Google. Though that being said, they're still being fucking assholes."
MachineLearning,3bl8f8,BoojumG,3,Tue Jun 30 14:48:28 2015 UTC,"I'll hold off on the ""fucking assholes"" pitchforks until they actually try to use the patent. I don't think that will ever happen. Patents are mostly defensive for large companies.  I agree that the major problem is the intellectual property environment that both makes such patents possible and punishes large companies that don't pursue them."
MachineLearning,3bl8f8,decimated_napkin,1 point,Tue Jun 30 18:34:01 2015 UTC,"Alright fair enough, I retract my fucking assholes statement in lieu of further evidence."
MachineLearning,3bl8f8,computersrneet,4,Tue Jun 30 20:01:16 2015 UTC,Well now I'm hoping a masters focus in machine learning wasn't the worst choice of my life.
MachineLearning,3bl8f8,IrishWilly,2,Tue Jun 30 06:07:33 2015 UTC,Hope you like working for Google.
MachineLearning,3bl8f8,computersrneet,1 point,Tue Jun 30 16:56:31 2015 UTC,"Let's be real here, I wouldn't turn them down."
MachineLearning,3bl8f8,ctornync,1 point,Wed Jul 1 15:10:49 2015 UTC,"So...   Best-case scenario:  wide-ranging reform of technical patents. (Unlikely.) Second-best:  over-general, technically-unjustified patents are never issued.  (Astronomically unlikely.)  More specifically, Google doesn't get this patent and no one else gets it or anything similar, ever, either.  (I'm not qualified to guess, but I'd lean toward unlikely.) Third-best:  Google gets this patent and uses it benevolently:  only to ward off patent trolls.  (Somewhat likely.)   Other scenarios include Google getting this patent and using it maliciously; Google not getting this patent, but someone more malicious than Google getting some version of it instead; proceedingly more awful things...  Speaking of unlikely, are there any technical or patent lawyers that frequent /r/machinelearning?"
MachineLearning,3bl8f8,watersign,1 point,Tue Jun 30 16:59:19 2015 UTC,Apple patented rounded corners on the iPhone..why can't classification be patented? lolol
MachineLearning,3bl8f8,ukkoylijumala,1 point,Tue Jun 30 05:08:56 2015 UTC,Maybe there is a connection to this? /r/technology post  Although there is a 4-day difference.
MachineLearning,3bl8f8,sieisteinmodel,1 point,Tue Jun 30 06:19:23 2015 UTC,It's a shame.
MachineLearning,3bl8f8,BomTomdabil,1 point,Tue Jun 30 06:42:22 2015 UTC,"http://www.google.com/patents/opnpledge/pledge/  I know it doesn't solve all of the problems, but it seems like they aren't trying to keep academics and OSS-ers out at all.  But as was said, titles and abstracts on patents are almost deliberately vague. Read the claims."
MachineLearning,3bl8f8,say_wot_again,3,Tue Jun 30 12:21:32 2015 UTC,"So now for profit companies wouldn't be allowed to use any existing classification algorithms, just researchers and FOSS? That still sounds ridiculous; I really hope we're all just badly misreading the patent."
MachineLearning,3bl8f8,BomTomdabil,1 point,Tue Jun 30 15:24:27 2015 UTC,"I can't confidently say what it means for anybody doing explicitly commercial work with it, but yeah I hope it isn't as Draconian as we are all inclined to take it."
MachineLearning,3bl8f8,bushrod,1 point,Tue Jun 30 15:44:05 2015 UTC,This says more about our patent system than anything.  Can anyone familiar with the patent system explain how something like this gets accepted?  Is the examiner incompetent or corrupt?
MachineLearning,3bl8f8,alexmlamb,0,Tue Jun 30 17:21:21 2015 UTC,It's interesting that patents seem to be influential in computer vision but don't seem to have an impact on ML.
MachineLearning,3bl8f8,computersrneet,4,Tue Jun 30 06:40:49 2015 UTC,How doesn't it affect ML? It's classification.
MachineLearning,3bl8f8,alexmlamb,1 point,Tue Jun 30 06:47:42 2015 UTC,"There are patents, but I don't know if they've ever been used successfully."
MachineLearning,3bl8f8,computersrneet,2,Tue Jun 30 06:56:42 2015 UTC,"This was published 5 days ago, though. I'm no IP expert by any means, but this patent seems extremely general. Certainly more general than other patents I've read."
MachineLearning,3boncv,vladiim,1 point,Tue Jun 30 22:08:16 2015 UTC,"Serious question. Are these good use cases (or better, business cases) on why one would want to run any machine learning algorithm in client side (inside the browser)?"
MachineLearning,3boncv,onewugtwowugs,3,Wed Jul 1 02:31:56 2015 UTC,"Given that the data size and computational cost is low enough for most computers and browsers, I can think of:   Educational purposes: It lets us interactively visualize algorithms using standard visualization toolkits such as d3.js, which is easily embeddable on web sites. Guarantee personal data integrity: We can build web apps that rely on machine learning and guarantee that no data leaves the local machine.  We won't have to care about maintaining and paying for a potentially large server infrastructure."
MachineLearning,3boncv,virdvip,1 point,Wed Jul 1 11:06:30 2015 UTC,Computer vision using WebGL shaders - that's example. It's easy develop and run one algorithm in browser at development and production. But you must deal with browser limitations.
MachineLearning,3boncv,ChefLadyBoyardee,1 point,Wed Jul 1 13:12:07 2015 UTC,"Totally, it saves you from having to run it on the server, which saves money on server fees.  I have a friend who did this for his startup. It was purely for a client-side visualization, so it made sense to run it in the browser, rather than ask a server to do it and send the result back.  For a customer-oriented website, anything you can do to simplify your stack is welcome. Having to spin up extra servers to handle some frequently run and computationally intense code is less than ideal. It's just another point of failure your engineers have to deal with.  There may be security reasons why you can't trust clients with a calculation, so you have to do it on the server. But if you can run it on the client (and it's reasonably performant), there are good reasons to do so."
MachineLearning,3bntqs,andycampbell22,2,Tue Jun 30 18:36:39 2015 UTC,"Hmm, this is an ad for the company that makes the data.  Also, I'm not surprised that modern sentiment analysis could beat the market from 5 years ago (when the state of ML was more primitive), but seriously, at this point all of the quant shops have upgraded and the efficient market hypothesis probably means this technique is now useless."
MachineLearning,3bntqs,melvinzzz,1 point,Wed Jul 1 18:49:56 2015 UTC,"Just glancing at the chart, October 2014 doesn't look so good. In a downturn the strategy might not perform so well."
MachineLearning,3blknf,dianalennon,1 point,Tue Jun 30 05:13:55 2015 UTC,#7 says fitting a linear regression model is O(1). Is that right? It seems like for k parameters that should be O(kn) at best just because of the matrix multiplication.
MachineLearning,3bpfn7,Aerospacio,1 point,Wed Jul 1 02:23:41 2015 UTC,"Maybe   Hadsell, Raia, Sumit Chopra, and Yann LeCun. ""Dimensionality reduction by learning an invariant mapping.""Computer vision and pattern recognition, 2006 IEEE computer society conference on. Vol. 2. IEEE, 2006.  I don't know about incremental auto encoders. Can you provide a reference? I am interested in learning how they differ from the paper above."
MachineLearning,3bolwd,RationalMonkey,3,Tue Jun 30 21:57:48 2015 UTC,"I'd say definitely go off the shelf as a lean proof-of-concept, but then you say it's your 'defining technology.'  Are you trying to compete on having a better recommender than your competitors?   Have you heard of PredictionIO?"
MachineLearning,3bolwd,TraptInaCommentFctry,1 point,Tue Jun 30 22:42:53 2015 UTC,Thank you. This looks like a great option. I get to have something customizable but we can also roll it out very quickly
MachineLearning,3bolwd,dfarber,-1,Wed Jul 1 04:42:48 2015 UTC,"If you use a service, what the hell is your startup doing?"
MachineLearning,3bp8kw,alexmlamb,2,Wed Jul 1 01:24:30 2015 UTC,"It's pretty flawed that the first author decided to rate the women himself rather than using multiple annotators.  Moreover, I find that when I label data myself, I sometimes bias my labels to try to make things easier for the model.  It would be better to use mechanical turk.   I wonder if the model is just picking up the preference for hair color, setting, or something like that.  I'm kind of inclined to think this given the success of logistic regression on top of VGG net.   Does this violate any research ethics by using human subjects without their consent?  As the paper admits, scoring people as attractive or not attractive is really flawed, since traits associated with attractiveness mostly lie on a spectrum.  An obvious alternative is to rank people from 1 to 10, or something like this.  I actually don't think that this is a great solution.  I would actually prefer giving the user two people, and then having the user pick which person is more attractive."
MachineLearning,3bp8kw,romulanhippie,2,Wed Jul 1 01:41:48 2015 UTC,"I was really surprised to see this was accepted for the ICML workshop.  It was an entertaining and good blog post, but I'm not so sure about a serious academic paper."
MachineLearning,3bo7u8,trianta2,1 point,Tue Jun 30 20:15:07 2015 UTC,"Your factor graph -- taking the representation of the factors that you've written on the factors literally -- doesn't represent a valid factorization of a joint probability distribution.  Factors can be derived from probability distributions, but are usually not equivalent to them.  Look at it this way, if you really had access to P(V1|X) and P(V2|X) -- with everything else marginalized out -- then you'd already be done, right?  It sounds like what you have are two models f1:X->V1 and f2:X->V2 that don't take into account V2 or V1, respectively. This is different from P(V1|X) and P(V2|X) because f1 and f2 don't marginalize over the other variable.  And different from factors fi:XxVi->R."
MachineLearning,3bo7u8,rcwll,1 point,Wed Jul 1 13:36:48 2015 UTC,"Thanks for the reply. If I understand correctly, you're saying that there's a difference between my prediction of V1 and V2 and the conditional probability of these variables given the data. How would one incorporate the prediction from a discriminative learner (SVM, ANN, RF, etc.) along with the probabilistic relationship between V1 and V2? Isn't this what CRFs accomplish? I believe what I'm after is a ""smart averaging"" of the predictions with my prior and pairwise knowledge, but I'm having trouble casting the problem in a probabilistic framework."
MachineLearning,3bo7u8,rcwll,1 point,Wed Jul 1 18:05:12 2015 UTC,"Right.  You have three variables, V1, V2, and X.  You have a joint distribution P(V1, V2, X).  To find P(V1 | X) you need to find P(V1, V2 | X) = P(V1, V2, X) / P(X) and then integrate out V2.  This does not have anything to do with something like an SVM that takes X as an input and spits out a point estimate of V1 or V2, which is what you seem to have.    All of which is moot, because you're trying to analyze it as a factor graph, which means that you don't need distributions, you need factors, which are functions that take all variables connected to the factor as inputs and produce a real number as an output.  Your discriminative model (probably) doesn't do this, at least not tractably; it takes X and an input and produces a value with the same support as V1 or V2 as an output.  If you're using a discriminative model there, you're trying to plug functions with the wrong domains and ranges into your graph at those two factors.  And then there's problems with interpreting the remaining factors as probability, since P(V1) = \int P(V1,V2)dV2, which means either you have contradictory or redundant information across those factors, but at least you can plug the connected factors in and get a number out.  But in any event, I don't think CRFs are what you really want; I think you'd be better off just using an end-to-end discriminative classifier that lets you specify a general cost function (like a neural network).  You should then fit both V1 and V2 simultaneously using that model, and add a term involving the negative log of P(V1, V2) to the cost function.  This will make it fit a discriminative learner that minimizes the cost while not deviating ""too far"" from your prior (or making the model justify a significant deviation with really high predictive performance), effectively regularizing your predictions, which is what it sounds like you actually want."
MachineLearning,3bo7u8,rcwll,1 point,Wed Jul 1 19:19:27 2015 UTC,"Thanks again for the in-depth reply. I'll explore the custom cost function route you recommended.   I suppose I'm still hung up on the CRF solution because I see implementations (PyStruct, CRFsuite, etc.) which use discriminative leaners like SVMs or logistic regression and predict in a structured manner, but only for discrete valued random variables. Naturally, I'm looking for an off-the-shelf solution and I'm wondering why continuous variables aren't supported."
MachineLearning,3blki0,john_philip,1 point,Tue Jun 30 05:12:24 2015 UTC,the interesting parts of that course were never recorded
MachineLearning,3bm53l,nexe,-2,Tue Jun 30 09:40:41 2015 UTC,"As someone who doesn't speak German, I'd imagine you could simplify your program a bit:  echo angry"
MachineLearning,3bm53l,mathsive,3,Tue Jun 30 11:50:13 2015 UTC,As someone who doesn't speak German   okay then ...
MachineLearning,3bm53l,mathsive,2,Tue Jun 30 12:08:26 2015 UTC,"Sorry, just a(n apparently bad) joke! I'm a much bigger fan of Ruby than Python et al. and wish there was a better machine learning community/set of libraries. Would offer some useful feedback if I knew German, but it looks simple and tidy which is great."
MachineLearning,3blfzh,ecobost,2,Tue Jun 30 04:25:18 2015 UTC,"I drew that diagram in Microsoft PowerPoint. It's a bit unorthodox I guess, but I found it the easiest tool to get the job done quickly :) This style doesn't really work for much deeper networks though (GoogLeNet, VGG etc.)."
MachineLearning,3blfzh,benanne,1 point,Tue Jun 30 08:32:28 2015 UTC,"i've done it in illustrator, it's a fucking pain in the ass.  but I think they're somewhat out of style now anyway."
MachineLearning,3blfzh,dfarber,1 point,Tue Jun 30 08:04:22 2015 UTC,Inkscape is good for that too
MachineLearning,3blfzh,farsass,1 point,Tue Jun 30 12:00:57 2015 UTC,Many people use Inkscape for these kinds of diagrams.
MachineLearning,3bltg7,samim23,-2,Tue Jun 30 06:56:36 2015 UTC,Spoiler: Including images generated by convolutional neural networks that were fed porn. Going deeper... for real ;-)
MachineLearning,3bo3hu,HannibalofBarca,8,Tue Jun 30 19:43:28 2015 UTC,"No, a chat bot that was trained trained on a movie script dataset outputs ""To live forever"" when the input is ""What is the purpose of living?"".  A reasonable analysis would phrase that as this: A likely answer to the question ""What is the purpose of living?"" you could see in a movie is ""To live forever"". And that is what the ""AI"" figured out. Which is much more reasonable than the headline ""AI thinks the purpose of life is to live forever.""  Please don't post more ignorant articles written by people who have very little knowledge of machine learning here."
MachineLearning,3bo3hu,jrkirby,-1,Tue Jun 30 20:01:02 2015 UTC,How is that different to how humans figure out what the purpose of living is?
MachineLearning,3bo3hu,holocauster-ride,3,Tue Jun 30 20:58:14 2015 UTC,"If you had a human that did nothing but read movie scripts, and had no life, then perhaps not so different."
MachineLearning,3bo3hu,jrkirby,-1,Tue Jun 30 22:08:15 2015 UTC,Most humans are just that (although they watch movies instead of reading the scripts).
MachineLearning,3bo3hu,holocauster-ride,3,Tue Jun 30 22:15:35 2015 UTC,"You mean they don't leave the theatre and go home after the movie ends? They never went to school, or had a job, or talked with their parents? They just sit there watching movies all day, every day? If you know of any people like this, please report it, because I'm pretty sure their basic human rights are being violated."
MachineLearning,3bo3hu,jrkirby,2,Tue Jun 30 22:20:07 2015 UTC,"This fictional person would also have to be Mary the scientist,  who knows nothing of the world.    Statistical relationships between arbitrary symbols means nothing without symbol grounding.  The academic world has such a short term memory that this debate from the 80s is basically forgotten.   For a good paper on this: Stevan Harnad. The Symbol Grounding Problem. 1990.   Also,  Devault & Stone. Societal grounding is essential to meaningful language use.  AAAI. 2006."
MachineLearning,3bo3hu,Articulated-rage,-4,Wed Jul 1 05:10:38 2015 UTC,See? Your responses are just meaningless semantic arguments that fail to spend any time understanding. I Can't tell the difference between this conversation and the chat bot.
MachineLearning,3bo3hu,holocauster-ride,1 point,Tue Jun 30 22:32:03 2015 UTC,"Humans are capable of debating the meaning of life between each other, this system essentially is not. If I was in a debate with you, repeating my point 10000 times would not make it more likely that you would agree with me, but if I sent the same training example into the RNN 10000 times it would change its output."
MachineLearning,3bo3hu,siblbombs,1 point,Tue Jun 30 23:11:40 2015 UTC,"“If you tell a lie big enough and keep repeating it, people will eventually come to believe it.""  - Joseph Goebbels"
MachineLearning,3bo3hu,holocauster-ride,1 point,Wed Jul 1 08:17:35 2015 UTC,So you see Joseph Goebbles as a leading light in philosophical debate? Good to know.
MachineLearning,3bo3hu,siblbombs,0,Wed Jul 1 13:24:08 2015 UTC,"This isn't a philosophical debate, you said humans don't believe lies after being told them thousands of times, which is objectively false.  I think you rate the bullshit detecting capacity of the average human too highly."
MachineLearning,3bo3hu,holocauster-ride,1 point,Wed Jul 1 13:53:23 2015 UTC,"If even one person is capable of detecting bullshit, it is more powerful than this RNN as constituted. A RNN has no mechanism to reject a training example, it is forced to consider it as truth."
MachineLearning,3bo3hu,siblbombs,0,Wed Jul 1 14:20:06 2015 UTC,"Sure, but my initial claim was that most humans answer questions and think based mainly on conversations they have had before."
MachineLearning,3bo3hu,holocauster-ride,-2,Wed Jul 1 14:33:28 2015 UTC,Did someone train your AI on a script of people with sticks up their asses?   The headline is dumb sure. But this project was an interesting angle on the chat bot and it was the only one that came up in my search results when looking up the script.
MachineLearning,3bo3hu,Articulated-rage,3,Wed Jul 1 05:11:47 2015 UTC,"For (a brief) set of links on actual dialogue research:   Microsoft is using this to make Cortana smarter Honda wants you to talk to its car SRI's PAL which spun off into a company that Apple bought to create Siri Oliver Lemon who has a bunch of interesting system building efforts The PARLANCE project which is Europe's conversational system effort ICT has some interesting systems such as a gun slinger and SimSensei, which can act as a virtual therapist"
MachineLearning,3bo3hu,jrkirby,4,Wed Jul 1 23:41:30 2015 UTC,"This subreddit was initially populated by researchers, data scientists, and statisticians, and programmers who work or study in the machine learning field. We'd like to keep this subreddit aligned with the same interests as it has in the past. If you'd like to learn about machine learning, statistics, and the actual techniques that are used to solve complicated problems like computer vision, feel free to stick around. You're welcome here, and many people will do their best to try to help you learn.  If you want to post inane clickbait articles, please do so somewhere else. It's fine that you think this project was an interesting angle on the chat bot, but to people who've studied machine learning, it's really not. And we'd like to keep this subreddit interesting to people who've studied machine learning, so if you haven't, please refrain from posting and voting here.  There is a subreddit that welcomes people with a layman's understanding of technology. That is /r/Futurology. Maybe they'd like the article you posted better."
MachineLearning,3bo3hu,MusicIsLife1995,1 point,Wed Jul 1 16:55:28 2015 UTC,Agreed. Topics like these belong in futurology. Who moderates this forum?!
MachineLearning,3bm2dc,jmmcd,2,Tue Jun 30 09:02:10 2015 UTC,"Interesting.  I've never used this data set before, but I think you're correct: People should not be using the seventh variable as the dependent variable.  To me, ""split"" indicates a way to divide the data into training and testing sets, not a way to ""split"" the data into their respective classes.  The latter would be very nonstandard usage.  Without doing a literature search, I don't know if a lot of people are doing this.  Generally the descriptions of the UCI data sets and the target concepts are more clearly described than in this case.  I wonder what task Forsyth originally used.  Turney's usage makes sense, if one wanted to predict the number of drinks or predict light_drinker or moderate_drinker based on the results of the blood tests.  I suppose one could also predict the likely outcome of a blood test given the other tests and drinking habits."
MachineLearning,3bi49c,hamartiated,7,Mon Jun 29 12:19:53 2015 UTC,"what courses are you looking at doing, I should probably use my summer wisely"
MachineLearning,3bi49c,fungz0r,7,Mon Jun 29 17:05:38 2015 UTC,"+1, more info would be nice."
MachineLearning,3bi49c,Xupid,1 point,Mon Jun 29 17:41:45 2015 UTC,"Interested as well, open discussion of a potential course would probably be good."
MachineLearning,3bi49c,Legumez,5,Tue Jun 30 02:26:22 2015 UTC,I'm planning on working through CMU's Convex Optimization course for the remainder of my summer. If anyone else in also interested please feel free to PM me.
MachineLearning,3bi49c,no_potion,1 point,Mon Jun 29 21:05:02 2015 UTC,+1 for the open data science masters.
MachineLearning,3bi49c,sedpugan,1 point,Tue Jun 30 00:36:19 2015 UTC,"So it looks like I have a life time worth of studying to do. Im pretty bad at maths so once I work through my kids algebra curriculum, I might join you in a few years.  Joking aside, this is a great link."
MachineLearning,3bi49c,tonylee0707,1 point,Tue Jun 30 01:26:01 2015 UTC,"+1, I am more interested in the basics of SVM, random forests, if anyone wanna form a group and do that!"
MachineLearning,3bim8a,caroljmcdonald,1 point,Mon Jun 29 15:08:09 2015 UTC,Interesting. I wonder how it can manage joins.
MachineLearning,3bhgfz,iori42,5,Mon Jun 29 06:39:48 2015 UTC,Great write-up! Bonus points for including relevant links and citations.
MachineLearning,3bme8c,nsmithn77,10,Tue Jun 30 11:45:33 2015 UTC,google
MachineLearning,3bhb4b,KeponeFactory,1 point,Mon Jun 29 05:35:43 2015 UTC,Link to abstract page?
MachineLearning,3bhb4b,naught101,3,Mon Jun 29 17:21:36 2015 UTC,http://arxiv.org/abs/1506.06579
MachineLearning,3bhb4b,cant_learn_japanese,-20,Mon Jun 29 17:28:48 2015 UTC,I like how this is posted 1 am in the morning.    edit: I didn't expect such negative/critical responses.Quality posts are still being posted at night. Thats a cool thing.
MachineLearning,3bhb4b,123A321,31,Mon Jun 29 07:08:16 2015 UTC,Because life only exists in your timezone?
MachineLearning,3bhb4b,torekoo,1 point,Mon Jun 29 09:08:30 2015 UTC,well presumably it's the author's timezone as well.
MachineLearning,3bhb4b,j1395010,1 point,Mon Jun 29 15:48:42 2015 UTC,It's 1am in the afternoon here.
MachineLearning,3bhb4b,naught101,1 point,Mon Jun 29 17:08:06 2015 UTC,"I like that you specified that it would be 1 am ""in the morning"" as opposed to other times where it would be 1 am."
MachineLearning,3bhb4b,tlalexander,1 point,Mon Jun 29 17:20:12 2015 UTC,Thanks. Some people don't appreciate 1am unless they're reminded that it happens pretty late at night/early in the morning.
MachineLearning,3bll9c,Maybenabe,1 point,Tue Jun 30 05:20:22 2015 UTC,"A parallel question would be to ask ""what's the relationship between theoretical computer science and complexity theory"" .    I'm  not sure how to answer.  One is a title for non-application centric research endeavors for an entire field while the other is both a subset and a mechanism/tool for those endeavors. Sorry this wasn't more clear.  It's late here.  Edit: I may also be wrong.  Sorry if I am."
MachineLearning,3bll9c,Articulated-rage,1 point,Tue Jun 30 07:21:54 2015 UTC,"I think (personal opinion) it's a question of emphasis.  ""Theoretical machine learning"" generally is more concerned with analyzing or improving performance bounds given some fixed complexity budget; ""Computational learning theory"" seems to be more concerned with finding the time/space complexity of solving some class of problems.  So ""fix the complexity and manage the problem/performance"" maps loosely to theoretical machine learning, and ""fix some problem/performance and manage the complexity"" maps to computational learning theory.  There's also a small-ish community that spends a lot of time thinking about identification and learning of formal languages and grammars, Chomsky and non-, and I've only ever seen that work described as ""computational learning theory"".  There is a huge overlap, though; different parts of the elephant etc."
MachineLearning,3bjphw,piscoster,2,Mon Jun 29 19:53:57 2015 UTC,http://topepo.github.io/caret/varimp.html
MachineLearning,3bjphw,Fireflite,1 point,Mon Jun 29 23:45:21 2015 UTC,"What I would do is predicting with that variable, predicting without that variable, and compare prediction error.   If you automate it, doing that may not be a pain."
MachineLearning,3bjphw,mareram,1 point,Fri Jul 3 08:25:48 2015 UTC,"chi square, correlation, Linear regression weights, etc. etc."
MachineLearning,3bjphw,xyzwonk,1 point,Mon Jun 29 23:12:18 2015 UTC,It really depends on the method but for tasks that weight the inputs (most) a larger magnitude weight means its more predictive.
MachineLearning,3bjl5m,Jonjo9,4,Mon Jun 29 19:23:46 2015 UTC,"It looks like you can find the source code for the example here. I've never used Mocha.jl, but I've worked with with HDF5.jl quite a bit (this is the library Mocha.jl use for handling HDF5 also). HDF5.jl is super easy to use, especially if you're familiar with the HDF5 format. Essentially you just need to look at their example, convert.jl, and figure out how to wrangle your data into the appropriate format.   The only other recommendation I have is to mind your matrix order, Julia is column-major like Fortran, rather than row-major like C/NumPy. For example, notice how they use the last index,       dset_data[:,:,1,idx[j]] = img[(r_idx-1)*h*w+1:r_idx*h*w]    in the 4D dataset they setup to index examples. Not sure what the extra dimension is for though..."
MachineLearning,3bjl5m,afireohno,1 point,Tue Jun 30 01:56:42 2015 UTC,"Ok thanks for this, I think my problem is that I haven't used HDF5 before and I'm trying to convert without really knowing what is exactly HDF5 is, looks like I've got some reading to do!"
MachineLearning,3bioyc,veeraman,8,Mon Jun 29 15:30:29 2015 UTC,Try this: http://www.loper-os.org/bad-at-entropy/manmach.html
MachineLearning,3bioyc,CurufinweNoldor,2,Mon Jun 29 15:36:33 2015 UTC,Analysing Humanly Generated Random Number Sequences: A Pattern-Based Approach. (PLoS ONE) does exactly that.
MachineLearning,3bioyc,mango_tree,2,Mon Jun 29 17:00:34 2015 UTC,"""Also, how many minimum numbers an algorithm would need to start predicting?""  0, but it'll perform completely randomly (which is why your question is silly... you didn't specify a required performance level)  Then again, no matter what performance level you specify the correct answer is going to be ""it depends""."
MachineLearning,3bioyc,patrickSwayzeNU,0,Mon Jun 29 17:03:21 2015 UTC,This really depends on what underlying distribution you are using for sampling your random data (if any). If you are sampling from say a Gaussian you can implement quite a  few algorithms that will learn this latent distribution (ie pick the mean) with enough samples.
MachineLearning,3bik7w,timurlenk,2,Mon Jun 29 14:52:03 2015 UTC,"Not a sexy answer, but I would start by splitting up the problem and build out the pieces separately. This process may help you understand your problem better, after you can try to implement some more advanced ML algorithms to replaces some pieces.  I see this as three main problems:  1) Collecting data to use in a model  2) Building out a model to come up some prediction or your metric of interest given input data (is your goal to increase sales? create a better user experience? create a recommendeder system?) 3) coming up with a optimization algorithm to allocate the resources given the model from #2 and the set of constraints."
MachineLearning,3bi50u,hemingwayfan,1 point,Mon Jun 29 12:27:34 2015 UTC,"If you're developing an algorithm, you may ask ""Why am I developing it using this data?"".  Do you really need such a large dataset to see if your algorithm works?  Often the answer, is that no, something like MNIST is sufficient, but it really depends on what kind of problem you're working on."
MachineLearning,3bi50u,treebranchleaf,1 point,Mon Jun 29 12:50:30 2015 UTC,"It depend on algorithm and data. But generally speaking debugging algo require small dataset. After that you should check if it really converge, but some algorithm works only on huge amount of data. Also some bugs and algorithmic problems could only be visible on big datasets. So it would be more safe include periodic verification run on full dataset in your development loop."
MachineLearning,3bi50u,serge_cell,1 point,Mon Jun 29 15:02:35 2015 UTC,"This aligns with what I was thinking, now just to start small, then run on larger and larger things. I think I can start little, iterate to test and train, then unleash on larger data as a whole without the subsets.   Thanks /u/serge_cell!"
MachineLearning,3bi50u,rasbt,1 point,Mon Jun 29 16:16:31 2015 UTC,"Here's how I'd approach this problem:   Split your big dataset into the usual training (A) and test subsets (B); don't touch the test set. Split the training set (A) into smaller chunks (C) and (D) Based on chunk (C), use hold-out cross validation or k-fold crossvalidation on different sizes of this chunk, e.g, 10 - 100% and plot it as a function of the missclassification error / SSE or whatever metric you are using and check with learning curves whether using more data would make sense. If you find that adding more training data would make sense, grow chunk (C) by adding data from chunk (D) If you are done developing your algorithm, train it on your complete training set (A), and evaluate it on the untouched test set (B);  Write your paper and report performance measure from step 5   Btw. that's a little bit paradoxical: You said that you were still in the Coursera Class level but are already developing new algorithms? That's a big leap!"
MachineLearning,3bf44r,winstonl,22,Sun Jun 28 17:37:25 2015 UTC,"How many support vectors you need depends on the complexity of the decision boundary, not the size of the data set."
MachineLearning,3bf44r,kjearns,5,Sun Jun 28 17:44:01 2015 UTC,"The is no such rule. As kjearn mentioned, it's more an indicator of the complexity of the decision boundary - but not exclusively / necessarily. There is no lower bound on the number of SVs - the whole data set could become SVs if you were exceptionally unlucky. The ForestCover dataset is a little notorious for this, as something like half the dataset becomes support vectors but it still gets ~86% accuracy."
MachineLearning,3bf44r,EdwardRaff,5,Sun Jun 28 20:57:00 2015 UTC,"Maybe there's a more sophisticated answer, but I think the general rule is relevant here: test your model using cross validation -- if the training set fit is great but fit on your test set(s) is poor, then your model is overfitted and may be too complex"
MachineLearning,3bf44r,arvi1000,3,Sun Jun 28 23:05:22 2015 UTC,"To add a point that will be obvious to many -- by changing C you effectively change the complexity of the decision boundary, which as others have said can change the number of support vectors."
MachineLearning,3bf44r,jmmcd,2,Sun Jun 28 21:12:17 2015 UTC,There is not a general metric for that. I have read some heuristic but in practice I haven't seen any of those working properly.  One intuition is that you would prefer the lower amount of SVs without compromise the quality of the model. A hight amount of SVs (close to the number of samples in your dataset) might indicate an overfitting. Your numbers seems good to me but you'd need to crossvalidate your models with distinct set of parameters to be sure.
MachineLearning,3bf44r,CephasM,1 point,Sun Jun 28 18:04:15 2015 UTC,A related question is how many support vectors to expect in a given number of dimensions. Drawing a picture is enough to convince me that in 2 dimensions we have to have three support vectors (EDIT in a hard-margin SVM). (After the implicit mapping by the kernel to a higher-dimensional space we may have many more.) What is the general case?
MachineLearning,3bf44r,jmmcd,1 point,Sun Jun 28 21:16:50 2015 UTC,You may have many more than 3 if you're using a soft-margin SVM.
MachineLearning,3bf44r,diametral,1 point,Sun Jun 28 22:02:17 2015 UTC,Yes -- thanks. I never realised that the misclassified points in a soft-margin SVM are counted as support vectors.
MachineLearning,3bf44r,jmmcd,1 point,Sun Jun 28 22:45:55 2015 UTC,"I don't know of any. There are risk bounds that are based on some norm of α (the dual variables) being bounded (see, for example, this paper), but I haven't seen anything involving the raw number of non-zero α (the ""\ell_0""-norm).  That said, 315/650 seem pretty high to me. I'm not an expert by any means, but I'd be a little worried with this number. This says that if you deleted a point at random, roughly half the time your model would change. This suggests a lack of robustness to noise.  Definitely try cross-validation like CephasM suggested. You might also want to look at the story using another kernel (e.g. if you're using a linear SVM, switch to one using a Gaussian RBF) to see what performance looks like there.   Visualizing your data, support-vectors and decision boundary via PCA/kernel PCA might also be informative."
MachineLearning,3bf44r,diametral,1 point,Sun Jun 28 21:23:32 2015 UTC,"There is no generic rule but a bunch of trade-offs:   accuracy vs prediction latency underfitting (too few support vectors) versus overfitting (too many support vectors)   The underfitting vs overfitting trade-offs is classically adjusted via cross-validated grid search to find the optimal values of the RBF kernel bandwidth parameter and the regularization parameter.  The accuracy vs prediction latency is application specific: for some applications, prediction time is very important and you cannot allow for predictions that do too much computation. More support vectors will yield slower predictions (both higher prediction latency and as well as lower prediction throughput). If this is a problem one could impose a higher bound on the number of support vectors (while accepting some underfitting at the same time)."
MachineLearning,3bj4iw,Chispy,10,Mon Jun 29 17:25:52 2015 UTC,"Well this is certainly a different type of thread than we normally see here.  Full disclosure: I'm more pessimistic about the possibilities of the future than some of my colleagues, though I would characterize it more as realistic.   Are we going to use AI as a real time mind augmentation to facilitate information retrieval and enhanced decision making?   We already use AI (I'll stick to calling it AI for now even though it's not the best term for what we're talking about here) to facilitate information retrieval and assist in decision making.  It sounds like you're thinking about something like Google Glass but better.  From an AI standpoint that's not all that difficult, at least compared to the difficulties in hardware design for something like that.  I'm not sure what you mean by mind augmentation but that sounds like a hardware problem as well.  Getting computers to recognize objects in images is one of the most fundamental problems in AI and has had a lot of work put into it already.  There's always room for improvement but we're already in a usable state as far as AI goes.   For example, walking by a plant and having a vivid 3d representation of a chloryphyll pop out and show me how it creates energy from light.   Again, much more of a hardware, interface design, and visualization problem than it is an AI problem.   Will AI know us better than we know ourselves and therefore be useful to make more informed and rational decisions for us?   Well that entirely depends on what you mean by ""know ourselves"".  We already use AI to learn more about ourselves than we could ever figure out with our own brains.  In some sense we've been doing that since before computers were even invented.  Using mathematics to discover patterns in data that humans aren't able to see has been happening for centuries.  This is a bit different from what you're asking but there are going to be huge changes in medicine as a result of AI.  In 10-20 years you'll walk into a hospital and your entire genome will be analyzed along with your medical records and current health status to help determine the best method of treatment.  Expect this Wikipedia article to get a lot bigger over the next few decades.  As I'm typing this I'm procrastinating on a project that is using real time vital sign data to monitor patient pain/stress for an entire hospital.  I've sat in on meetings in the last month that are the beginnings of using full genetic analysis as a standard method for determining treatment."
MachineLearning,3bj4iw,kevjohnson,3,Mon Jun 29 18:07:04 2015 UTC,"As I'm typing this I'm procrastinating on a project that is using real time vital sign data to monitor patient pain/stress for an entire hospital. I've sat in on meetings in the last month that are the beginnings of using full genetic analysis as a standard method for determining treatment.   How many hospitals are you aware of that are actively working towards integrating real-time vitals analysis for the whole facility? What's one of the major challenges in your project?  And second, do you think genome screening will really be useful in prescribing treatments if we can't match the ""genotype to the phenotype""? A ton of people would need their genomes sequenced, and to have extensive medical records, in order for doctors to start spotting the patterns that lead to certain diseases. I think this will take a very long time to become useful."
MachineLearning,3bj4iw,brouwjon,3,Mon Jun 29 18:17:32 2015 UTC,"I wasn't able to find any papers published on real-time vital sign analytics at this scale, and I don't personally know of any other hospitals doing it right now.  I'm sure there are some, but it's not the norm at the moment.  There are some commercial products available that claim to do it, but I don't think they're super popular (example).    The healthcare industry in general has always lagged behind in non-clinical technology.  We're just now getting up and running with every health provider using electronic medical records which is insane if you think about it.  The major challenge for my project is the sheer size of the data.  At the moment there are just under 25 billion heart beats in the dataset, and that's just one of dozens of possible vital signs.  Most hospitals are not equipped to deal with data at this scale.  From a methods standpoint, the hardest part so far has been establishing what is normal and what is abnormal for each patient, though it's going to get much harder once I try to incorporate this with previous medical records and current diagnoses to try to make predictions.  I'd like to model all of these relationships directly, but I'll probably end up using some sort of nearest neighbor method to simplify the model.  I do think that genome screening will be useful for prescribing treatments.  You're right that we're going to need a lot of full genome sequences linked with full medical records before we can make something of it, but that's exactly what people are trying to set up now.  That's all I can really say on that matter :)"
MachineLearning,3bj4iw,kevjohnson,2,Mon Jun 29 18:39:01 2015 UTC,"Thanks for your reply! I've just been using the latest tech that we've seen come out in recent months, and kind of extrapolating to see what sort of technologies we'll be seeing in 5-10 years. I'm sure google and Facebook have a huge interest in making augmented reality real, along with personal AI to sift, sort, and make sense of all the data that we're surrounded with but just lack the computation to sort through it all.   I know the medical field will be huge as well, and I'm sure Googles Calico has some neat road maps for the next several decades.   What's your field, if you don't mind me asking?"
MachineLearning,3bj4iw,kevjohnson,2,Mon Jun 29 18:17:07 2015 UTC,Officially my degree is in Computational Science and Engineering but it's pretty much just Machine Learning.  My specialization is in healthcare analytics.
MachineLearning,3bj4iw,skgoa,2,Mon Jun 29 18:40:08 2015 UTC,"What are some applications for AI in the near future that will enhance our lives in the near future?   Cars are starting to have lots of active safety features. I.e. they stop you from crashing. In the near future (5-10 years) once the necessary laws have been passed and the cars have been thoroughly tested/certified, you will be able to sit in cars that drive automatically. It's technically (in the most literal sense of the word) possible right now but only in a subset of environments and weather conditions.    Are we going to use AI as a real time mind augmentation to facilitate information retrieval and enhanced decision making? Will AI know us better than we know ourselves and therefore be useful to make more informed and rational decisions for us?   Isn't this what Google, Amazon et.al. are doing right now? Expect this kind of ""recommender"" systems to become ever more powerfull and generalised. E.g. one CVPR15 paper was about fashion recommendation based on an image of the person.    What are some useful ways we could take advantage of it to benefit the average person? How would it be integrated? How will it be used with augmented reality?   You could have an app on your phone/smartwatch/smartglasses/... that generates new recipes, new music or works of fiction based on what you like. Image recognision/scene understanding systems build into your glasses could track what interests you and recommend things based on that."
MachineLearning,3bj4iw,heptode,2,Mon Jun 29 19:14:51 2015 UTC,"I think you're too optimistic regarding autonomous cars. 80% of the problem has been solved, true, but the remaining 20% are exponentially harder, with some being General AI hard. I'd be surprised if we get anywhere close to truly self driving (in all conditions and environments humans can drive in, and without having to have the need to ever yield control to a human) within the next 30-40 years. This is not something you can solve with raw computational power either: Moore's law is petering out. And development of algorithms is a very nonlinear process. Decades can pass without any major breakthroughs."
MachineLearning,3bj4iw,skgoa,1 point,Mon Jun 29 20:29:50 2015 UTC,"I should have phrased it better but I'm talking about partially automated and highly automated driving, i.e. the car does the steering while higher level functions/decision making remain the responsibility of the driver. One active area of research is in identifying potentially dangerous situations and notifying the driver sufficiently early to have a chance to intervene. In a couple of years cars will be ready for 80% of mundane comuting in relatively good weather. All the technical building blocks for that exist already and cars have been demonstrated to do this years ago.  I agree on fully automated cars (i.e. the driver doesn't have to be attentive and able to intervene) having many additional issues to solve. We might very well not see them in practical applications before the mid-point of the century or ever."
MachineLearning,3bj4iw,watersign,1 point,Mon Jun 29 20:31:09 2015 UTC,"the way i see it, everyones identity will be ""learned"" by our browsing patterns, what we type/text/talk about, where we go, etc. what google and the NSA  are going to do with it is awhole nother story....the real questions that are interesting are based on how society will work with intelligent machines and whatnot."
MachineLearning,3bhvyi,rohanpota,1 point,Mon Jun 29 10:30:08 2015 UTC,"I wouldn't say you'd necessarily ""store"" it in main memory. Once you read data from the database and save it in a variable, array, etc. to work with it in your program, it's in main memory. And a database is usually physically located on a HDD, so you're already storing it in secondary memory.   Edit: to better answer your question, I'd say you should minimize the number of reads/writes to the database. Read it from the database, do your calculations, and write it back to the database when it's ready. It may sound like common sense, but you'd be surprised how easy it can be to make a bunch of unnecessary database calls once your program gets more complex."
MachineLearning,3bg7wt,seymour1_research,2,Sun Jun 28 23:13:29 2015 UTC,"It looks like this has been asked before: https://m.reddit.com/r/MachineLearning/comments/3a24wx/copyright_laws_and_machine_learning_algorithms/  IANAL, but I'd stay away from copyrighted material as much as possible. Even if you're in the right, you'd still have to be ready to defend yourself in court."
MachineLearning,3bg7wt,j1395010,1 point,Mon Jun 29 03:05:29 2015 UTC,"Thanks! I missed that one in my searches, and it's a really good starting point for me. Looks like I'll need to talk to some lawyers in order to get any more concrete advise.  I'll look into fair use some more, and I'll try to look into the licenses of the software I use to build my models (though that might end up taking too much time)."
MachineLearning,3bg7wt,skgoa,1 point,Mon Jun 29 04:05:27 2015 UTC,"It's a really interesting question. In a lot of ways, models seem like derivative work which is worrisome.   If you find anything else interesting about this please post it. I'm curious what else is out there."
MachineLearning,3bg7wt,j1395010,1 point,Mon Jun 29 04:29:23 2015 UTC,"no one has ever been sued, and if you're just scraping web images, you will never be sued by an individual copyright holder.  if you're google, you might have to deal with something like the author's guild lawsuit, but I think you would still prevail on fair use grounds."
MachineLearning,3bg7wt,visionjedi,1 point,Mon Jun 29 13:23:08 2015 UTC,"You can not make someone else's images publicly available without their consent. You can use them in your research and you can publish or sell your results based on them, however you will need to cite them in your paper. The way to publish your data set legally is to tell people where to get the copyrighted images. That's not uncommon."
MachineLearning,3bbyh6,AlanZucconi,6,Sat Jun 27 19:32:49 2015 UTC,"I'm curious, would constructing an internal grid like he's done there really limit the domain of what this AI can do, or is this generally a good idea?  I get the impression that the AI he represents here might not be good for general purpose game playing.  As in, you'd have to trust that the way you model the data is sufficient, correct, and optimal to help the AI learn.  (Are grids really a good idea? Would a polygonal representation actually do better? Is it even a good to try to come up with a high level abstraction, or better just to tell the AI what the ""goals"" are-- score, time, place count, etc.)  That said, it's still pretty cool but I wonder about what better approaches would be possible."
MachineLearning,3bbyh6,tjgrant,8,Sat Jun 27 22:59:49 2015 UTC,"I would think you get a suboptimal result doing that, but the dimensionality of the input is much smaller and still pretty informative. That means you need a smaller network, which in turn means you need less training data / time.  This is sort of what deep learning is telling us - in domains with infinite data and time it's hard to beat just throwing it at a big neural net top-to-bottom. With smaller data, especially when the dimensionality is large, it helps to build feature extraction systems based on our knowledge of the data structure (in a sense functioning as a regularization scheme)."
MachineLearning,3bbyh6,TTPrograms,9,Sun Jun 28 05:55:32 2015 UTC,"That grid (I assume you mean the input into the NN) isn't really 'constructed', it'll be the actual map data sitting in RAM somewhere in the SNES emulator.  Mode 7 ( https://en.wikipedia.org/wiki/Mode_7 ) basically (ab)uses the 2d tile display hardware, loads the portion of the map around the player into the tile grid, then adjusts the level offset, rotation and scaling at the end of every scanline to get a fake perspective effect.  You could argue that a different input for the NN would be better (and I'd like to see that), but he's not creating an intermediate representation, that pretty much is the internal representation (although he's probably cleaned it up a bit so it's compressed down into 3 tile types rather than all of the available visual types)."
MachineLearning,3bbyh6,Orangy_Tang,5,Sun Jun 28 15:51:29 2015 UTC,"I keep getting the feeling that this is just generating random paths until it finds a valid one. I'd be really impressed if you could train it on one course, and it can successfully navigate a course it hasn't seen before."
MachineLearning,3bbyh6,rosstrich,3,Sun Jun 28 09:28:40 2015 UTC,Well that is the idea of evolution anyway. XD The problem is that I don't think it has really learnt unless you can re-use the AI from a track to another one...
MachineLearning,3bbyh6,svantana,1 point,Sun Jun 28 11:11:23 2015 UTC,"Well to begin with, the games themselves are ""overfit"" to the capabilities of human brains. So comparing machine performance with humans in this case is not really fair. It's like pitting a human against a dsp chip in a floating point multiplication competition."
MachineLearning,3bbyh6,lickyhippy,16,Sun Jun 28 12:23:09 2015 UTC,Definitely more convincing than the original Mario due to the introduction of supposedly random variations in the course caused by the opponent players. The original marIO seemed to be a case of overfitting.
MachineLearning,3bbyh6,ButtStallionn,12,Sun Jun 28 15:04:26 2015 UTC,"Yes, the original MarI/O was definitely over-fitting a level. In fact, Seth has to train EVERY new level. An effective AI would have solved new levels once trained on the previous ones. MarI/O fails the validation test, but perhaps most of it is due to the fact the input never change."
MachineLearning,3bbyh6,Notlambda,1 point,Sat Jun 27 22:36:12 2015 UTC,How would you make it more effective?
MachineLearning,3bbyh6,dfarber,1 point,Sat Jun 27 22:45:44 2015 UTC,The original MarI/O was more like a complex search strategy that finds the solution to a particular level.
MachineLearning,3bbyh6,NuScorpii,15,Sun Jun 28 06:57:36 2015 UTC,schmidhuber did this directly from the rendered graphics decades ago.
MachineLearning,3bbyh6,marijnfs,7,Mon Jun 29 23:15:28 2015 UTC,"Decades ago is a slight exaggeration. TORCS 1.3.1 (the game used) was only released in Dec 2008, and the paper was published in 2013.  http://people.idsia.ch/~juergen/compressednetworksearch.html"
MachineLearning,3bbyh6,Polycephal_Lee,1 point,Sat Jun 27 23:13:39 2015 UTC,"The main researcher of that work is Jan Koutnik, his site has nice animations: http://people.idsia.ch/~koutnik/"
MachineLearning,3bbyh6,vincentrevelations,8,Sun Jun 28 10:37:55 2015 UTC,Wow I hadn't heard of this guy before. Really interesting stuff in his AMA here 4 months ago.
MachineLearning,3bbyh6,Noncomment,3,Tue Jun 30 18:26:37 2015 UTC,With neural evolution?
MachineLearning,3bbyh6,vincentrevelations,7,Sun Jun 28 00:43:36 2015 UTC,Yes. He evolved a giant neural network that played a racing game from just visual data. There's a video/talk here.
MachineLearning,3bbyh6,BrassTeacup,1 point,Sun Jun 28 00:23:17 2015 UTC,"Mmm, no demo to compare performance, but this network is a few hundred times more complex than SethBling's. The impressive part here is that it gives good results so quickly using only 9 hidden nodes. No doubt that's because of new insights from over the past few decades."
MachineLearning,3bbyh6,vincentrevelations,1 point,Sun Jun 28 05:48:09 2015 UTC,"Somewhat off topic, but can I ask how you'd decide how many hidden layers there are in a NN?"
MachineLearning,3bbyh6,WiggleBooks,1 point,Sun Jun 28 13:19:41 2015 UTC,"In the model SethBling uses, the amount of hidden nodes is something the evolutionary algorithm chooses. For other NNs, it's mostly based on other research. It's more of a natural science, going with what seems to work."
MachineLearning,3bbyh6,marijnfs,1 point,Sun Jun 28 17:35:48 2015 UTC,Directly from visuals alone?! Wow thats incredible
MachineLearning,3bbyh6,inarrears,2,Sun Jun 28 19:59:16 2015 UTC,"No, no no... :p You can see it has, on the bottom half, a grid version of the world. And that's what they NN actually sees... :p  (Oh sorry, you were referring to Schmidhuber! :D )"
MachineLearning,3bbyh6,squareOfTwo,1 point,Sun Jun 28 07:10:00 2015 UTC,"Yes, it has to estimate it's speed purely by analysing the images that come in!"
MachineLearning,3bbyh6,ChefLadyBoyardee,1 point,Sun Jun 28 11:13:00 2015 UTC,this is so much more interesting than yet another MNIST digit classification paper ...
MachineLearning,3bbyh6,LurkForever,4,Tue Jun 30 18:27:56 2015 UTC,"Again, he trained it only on one track/level... which leads to an useless AI which doesn't generalize"
MachineLearning,3bbyh6,Splanky222,3,Sun Jun 28 11:04:53 2015 UTC,"I guess if it increases popular interest in machine learning, that's a good thing. Maybe it'll inspire someone to start toying with these things?  I'm trying to spin this in a positive way, because I see these posts as drawing more subscribers to /r/machinelearning and decreasing the overall comment and post quality, more than anything."
MachineLearning,3bbyh6,hughperkins,2,Sun Jun 28 18:00:36 2015 UTC,I've been wondering:  Could AI be ready to be good at Starcraft II and let it play on stream? I'd love that.
MachineLearning,3bbyh6,Schlagv,2,Sun Jun 28 20:03:13 2015 UTC,"There are Brood War AI competitions, and have been for a while.  Maybe check some of those out?"
MachineLearning,3bbyh6,Molag_Balls,1 point,Sun Jun 28 00:45:29 2015 UTC,"Not yet.  The Atari neural network and so on work for tactical games, but not so much for strategy games.  They can play space invaders, but not so much rts."
MachineLearning,3bbyh6,LurkForever,0,Sun Jun 28 02:28:07 2015 UTC,"The issue is that you need to read game data to feed to the IA (unless you do computer vision). And this is hack.  Also, moving the screen and selecting units would be very non intuitive for the IA."
MachineLearning,3bbyh6,Dwood15,-2,Sun Jun 28 08:12:30 2015 UTC,"Could AI be ready to be good at Starcraft II and let it play on stream? I'd love that.   That would most certainly break the EULA, so probably not."
MachineLearning,3bbyh6,Molag_Balls,1 point,Sun Jun 28 03:56:03 2015 UTC,I think Blizzard would be OK if you pitched them this idea.  It's a nice PR stunt on the side too.
MachineLearning,3bbyh6,devDorito,0,Sun Jun 28 00:53:41 2015 UTC,"Probably would be fine in custom games. Hell, you could use blizzard's editor to create your own neural network."
MachineLearning,3bfw9d,regularized,8,Sun Jun 28 21:33:11 2015 UTC,https://groups.google.com/forum/#!forum/ml-news
MachineLearning,3bfw9d,piesdesparramaos,9,Sun Jun 28 22:14:37 2015 UTC,Welcome to academia where nothing is organized and you're on your own for the next 4-7 years!
MachineLearning,3bfw9d,uint64,2,Mon Jun 29 03:12:54 2015 UTC,If you find open-ended scholarships you can often pick your own supervisor and come up with a research project for yourself.
MachineLearning,3bfw9d,RinzeWind,1 point,Mon Jun 29 00:23:26 2015 UTC,Some announcements like the ones you are looking for are posted from time to time on the NA Digest mailing list: http://www.netlib.org/na-digest-html/faq.html
MachineLearning,3bfw9d,Maluuba_Inc,1 point,Mon Jun 29 02:32:33 2015 UTC,vikas.gosain@maluuba.com if you are interested in working at a machine learning/ NLP startup.
MachineLearning,3bfw9d,DewitJWatt,0,Mon Jun 29 18:21:38 2015 UTC,Or you could try getting a job. I know Amazon are hiring ML roles like crazy http://www.amazon.jobs/
MachineLearning,3bgp7p,asianfrommit,2,Mon Jun 29 01:53:34 2015 UTC,well explained. Beautiful presentation
MachineLearning,3bgp7p,pmontu,2,Mon Jun 29 05:24:32 2015 UTC,"Gotta say, the computer voice is pretty grating."
MachineLearning,3bgp7p,madmooseman,1 point,Mon Jun 29 05:38:28 2015 UTC,"I guess I'm not clear on how evolving connections is much different from just evolving weights. Also, the topology evolved here seems a bit unorthodox (at least from what I'm used to seeing in ANNs where one layer's neurons strictly feed neurons in the next layer) in that neurons can feed neurons in the same layer or even skip layers. Is it possible to have some feedback to earlier layers (as in an RNN)?"
MachineLearning,3bgp7p,cafedude,0,Mon Jun 29 19:19:10 2015 UTC,"Evolving connections allows for the ANN to increase it's complexity. I'm not too sure of the differences between this ""free structure"" and the orthodox structure, I guess it just gives more freedom for the network to evolve. Also, yes you can use genetic algorithms with RNN, it works similarly."
MachineLearning,3bct6c,ojaved,3,Sun Jun 28 00:19:34 2015 UTC,Preceding lecture by Alex Smola that introduces Deep Learning can be accessed at http://www.computervisiontalks.com/9-1-deep-networks-overview-machine-learning-class-10-701-alex-smola/
MachineLearning,3beijn,ibtrippindoe,8,Sun Jun 28 14:10:45 2015 UTC,"For undergrad keep yourself as general as possible, I probably wouldn't recommend any specialty cognitive science programs. That's what your masters and PhD is for.   Math and computer science would be a good combo and probably offered at just about any university."
MachineLearning,3beijn,AltusVultur,3,Sun Jun 28 14:37:15 2015 UTC,Any reason you choose math and compsci over statistics and compsci?
MachineLearning,3beijn,AltusVultur,4,Sun Jun 28 15:15:01 2015 UTC,"Because advanced linear algebra, stochastic calculus, partial differential equations, etc are all incredibly important in addition to statistics. A statistics degree or minor would likely go very in depth to a few specific applications, math minor would get you a good introduction to everything.  Any way you go you'll have to be teaching yourself a lot in terms of machine learning and other algorithm applications, though it's easier to pick up from where your professor left off than trying to learn a new topic completely from scratch.  Very strong programming skills is vital, but if you don't understand the theory you aren't going to get far either."
MachineLearning,3beijn,bagelorder,1 point,Sun Jun 28 19:26:38 2015 UTC,For what do you need partial differential equations in Machine Learning/data science?
MachineLearning,3beijn,AltusVultur,2,Sun Jun 28 20:30:09 2015 UTC,"I suppose I'm deeper into the engineering applications than most but it sounds like OP is still in high school, he shouldn't limit himself before he's had actual exposure to any of this"
MachineLearning,3beijn,bagelorder,1 point,Sun Jun 28 21:56:30 2015 UTC,"I myself haven't yet started learning machine learning because I eagerly waiting until semester break when I have time, so I don't actually know about that much in machine learning.  Said that, I study math. For your question, there are pretty cool emerging areas in machine learning / data science which have some math foundation, e.g. topological data analysis etc. But as said, I am not a good reference here, I just repeat what I have read.  But if you have studied math and had some statistics/prob. courses diving deeper into statistics won't be a problem for you. Also CS is a can-do on your own with some preknowledge. But if you studied only statistics it will be very, very hard to learn some other math on your own. It is very time consuming and the communication with others while solving the homeworks is very important.  And last but not least: Math is great!!"
MachineLearning,3beijn,ylghyb,3,Sun Jun 28 15:25:52 2015 UTC,"As a data scientist you want to have a broad knowledge of math, comp sci, statistics, but you also want to ""specialize"" in at least one field (e.g. deep learning, NLP, graphical models, high perf computing, optimization, time series).  Math/stat-wise you don't need anything fancy, but at a minimum:   Multivariable Calculus Linear Algebra Basic probability (i.e. no measure theory) Statistical Inference Linear Models   You might want to throw in real analysis, ODE, but these are not really required. So, given the above, being a math major is probably an overkill, unless you are dead set on grad school and you want to 'prove' that you are smart enough (even then it's probably better to spend time on research and publish rather than studying to get that A in an upper level abstract algebra class). One caveat is that if you truly love math then ignore my advice and major in it!   On the Comp Sci side, you want: - Machine learning (take after probability/stat inference to get more out of it) - Optimization - Algorithms/Data structures - Databases (most school will include ""big data"" stuff in their traditional databases courses these days)  As you can see, it's actually not that hard to get the foundational courses done for data science--an ambitious undergrad can finish the above in a year. This will leave you plenty of room to explore, more deeply, some of the more specialized areas:   Natural Language Processing Computer Vision Speech Recognition Deep Learning (intersects with a lot of the above) Graphical Models High performance computing (GPUs/parallel computing) Machine learning theory Bayesian Nonparametrics   To conclude, it really doesn't matter what you major in to become a data scientist, given that barrier to entry is not that high. But CS/Stat double major seems like the natural choice."
MachineLearning,3beijn,zymoclean,1 point,Sun Jun 28 18:18:21 2015 UTC,"Thanks for that in depth reply. There's actually a difference between a combined major and a double major. The combined major is a single major for those 2 subjects, whereas a double major would be both individual majors (and probably take much longer).   Based on your post, it seems that majoring in computer science and then getting a masters in statistics (as another poster suggested) would make more sense, seeing as the basic math is already programmed into the compsci major, and all the more specialized areas seem to be comp sci oriented. Would you agree with that?"
MachineLearning,3beijn,bagelorder,1 point,Sun Jun 28 18:35:38 2015 UTC,Check to see if you have access to a compsci stat hybrid. There's a computational statistics course offered for me.
MachineLearning,3beijn,winstonl,1 point,Mon Jun 29 11:16:41 2015 UTC,For what are ODE in Machine Learning required?
MachineLearning,3beijn,SufferSome,1 point,Sun Jun 28 20:33:31 2015 UTC,"I don't necessarily think machine learning requires ODE specifically, but I think it's always good to learn some things other than what you will need. ODE (and later on PDE) is very useful for many modelling related problem and is particularly useful for financial problems. I think it's always good to learn a course on that.  BTW, I think in stochastic processes you will need some ODE, and stochastic analysis is used in machine learning."
MachineLearning,3beijn,starfox863,3,Mon Jun 29 01:35:51 2015 UTC,"Depending on the particular courses offered, I would think that compsci+statistics is the best option.  Cognitive systems sounds like it would be interesting to people interested in human intelligence, but if you're really focussed on ""data science"" rather than ""artificial intelligence"", then some combination of compsci with mathematics/statistics will be most useful to you."
MachineLearning,3beijn,holo11,1 point,Sun Jun 28 15:03:34 2015 UTC,Statistics?
MachineLearning,3beijn,kotfic,1 point,Sun Jun 28 18:34:18 2015 UTC,math/physics/statistics.  and maybe a couple of programming classes and EE signals/applied math classes would give you a good foundation
MachineLearning,3beijn,kotfic,1 point,Sun Jun 28 18:50:18 2015 UTC,"Most advice here is already quite good, I would only add that practical research will aid you significantly in reaching your goals. Even as an undergraduate, go out of your way to get involved in real research projects. This can be done by learning what faculty are working on (check with your librarian to help you find recent faculty publications), then emailing them to get guidance on how how you can get involved.   Comp sci/math or comp sci/stats is good advice, but it doesn't preclude you from ""targeting"" faculty who are involved in the cog sci program. Getting involved with real research means getting involved with real scientists and real datasets. That is the best way to become a data scientist IMHO. Most faculty will be excited about an undergrad who goes out of their way to get involved!  Good luck!"
MachineLearning,3beijn,MusicIsLife1995,1 point,Sun Jun 28 19:40:09 2015 UTC,"Thanks for that advice, I'll definitely look to get involved in some research this upcoming term. Do you have any 'key words' I should look for in finding research that will be useful as a ""data scientist"". For example, 'machine learning' is obvious, but what other types of research should I look to get involved with?"
MachineLearning,3bf6g7,whatmath,6,Sun Jun 28 17:58:39 2015 UTC,I would go with applied stats and time series. I thinks it's more relevant to industy/business and greater variety ds jobs.
MachineLearning,3bf6g7,CabSauce,1 point,Sun Jun 28 18:14:34 2015 UTC,"2nd-ing this.  Computer vision is cool, but stats/time series is more generally applicable."
MachineLearning,3bf6g7,howdidiget,2,Sun Jun 28 18:34:32 2015 UTC,"First of all what two courses you take in a semester will neither make or break you.  With computer vision, you'll learn some practical image processing + some stuff about physics of image acquisition, which are largely irrelevant outside their application domain. But they are kinda cool.  You can get a data scientist job through either track."
MachineLearning,3bbijy,airalcorn2,9,Sat Jun 27 17:13:04 2015 UTC,"I have similar datasets. As your net is converging, diagnosing your problem gets more difficult. One simple way of verifying that your cost function and assessments are set up correctly is to use the logistic outputs (in probability format and not thresholded binary) as your input and try to converge to the actual class distribution. If the net is functioning properly, it will either meander aimlessly at that performance point or it will improve the results. If you have errors, the net will make performance worse and you should be able to debug it easily.   Nets also really vastly prefer normalized (zero mean unit variance) data. Have you normalized all of the data, including the binary variables? If not, the net might ""turn off"" a few useful variables quickly by making their coefficients so small that no amount of processing will ever re-establish them. That's another common problem."
MachineLearning,3bbijy,thatguydr,9,Sat Jun 27 18:17:04 2015 UTC,"Nets also really vastly prefer normalized (zero mean unit variance) data. Have you normalized all of the data, including the binary variables? If not, the net might ""turn off"" a few useful variables quickly by making their coefficients so small that no amount of processing will ever re-establish them. That's another common problem.   Oh. My. God. Looks like you cracked it. I normalized the data like you recommended and not only did the scikit-learn implementation of logistic regression shoot up to an accuracy of 74%, but the neural net achieved an accuracy of 76% after merely seconds of training. Thank you so much! Do you have any recommended reading for why normalizing the data would have such a significant effect? And have some reddit gold as a small token of my gratitude :)."
MachineLearning,3bbijy,thatguydr,5,Sun Jun 28 02:41:16 2015 UTC,"Only decades of experience in machine learning. ;-) And thanks for the gold.   I assume that the MIT book (Bengio and LeCun's) will be a trove of such experience once written (and I think it will be released soonish?), so waiting for that is my best advice."
MachineLearning,3bbijy,MrTwiggy,4,Sun Jun 28 04:50:53 2015 UTC,"I actually started reading the draft version this past week! For anyone else who is interested, it can be found here."
MachineLearning,3bbijy,thatguydr,3,Sun Jun 28 05:14:37 2015 UTC,"Have you normalized all of the data, including the binary variables?   +1 for normalizing input to neural networks, but is it necessary to do so for binary variables? I recall reading that it isn't, but I could be wrong."
MachineLearning,3bbijy,badmephisto,2,Sat Jun 27 18:25:14 2015 UTC,"It all depends on what you initialized your input weights to and whether the coefficient of every other variable has, with the input data, a smaller derivative. I've never actually seen it be a problem, but depending on the relative sparsities of your input variables, it's not inconceivable."
MachineLearning,3bbijy,badmephisto,1 point,Sat Jun 27 18:30:37 2015 UTC,"Thanks for your reply.  So you're recommending including the output from the logistic regression classifier as an additional input variable?  And the data have not been normalized, although they are on a [0, 1] scale. I'll normalize them and report back."
MachineLearning,3bbijy,badmephisto,15,Sun Jun 28 00:41:30 2015 UTC,"""I haven't even been applying any regularization to the cost functions"" Any reason why not? This can be quite critical. I would definitely mix in some dropout. Also you don't mention this, but I hope you're doing early stopping based on some validation set."
MachineLearning,3bbijy,devDorito,6,Sat Jun 27 17:28:42 2015 UTC,"Thanks for your reply.  I did initially include regularization (both L1 and L2), but the accuracy was essentially the same, so I'm trying to understand whether or not my model is underfitting. My understanding is that both dropout and early stopping are meant to combat overfitting, so I don't think I understand how they would help. I think I'm just going to try ramping up the complexity a considerable amount and see what happens."
MachineLearning,3bbijy,antiquechrono,10,Sat Jun 27 17:53:13 2015 UTC,"Applying neural nets correctly is really quite simple: 1. Make sure you have a validation set. 2. Initialize a big network. 3. Train big network with several learning rates and dropouts. 4. Repeat until you're tired. 5. Take whatever model gives best validation performance.  When done properly it is very rare that a neural net gets performance lower than a linear classifier, because a neural net has the capacity to be a linear classifier."
MachineLearning,3bbijy,changingourworld,1 point,Sat Jun 27 19:39:42 2015 UTC,"When done properly it is very rare that a neural net gets performance lower than a linear classifier, because a neural net has the capacity to be a linear classifier.   Yes, exactly. Which is why I don't understand why the neural nets aren't even really getting close to the R logistic regression classifier's accuracy in this case."
MachineLearning,3bbijy,MrTwiggy,2,Sun Jun 28 00:37:02 2015 UTC,"Oh, I think it's quite clear: it's not done properly ;)"
MachineLearning,3bbijy,MrTwiggy,1 point,Sun Jun 28 00:50:20 2015 UTC,"Ha, fair enough. Maybe I'll just have to come back to deep learning some day."
MachineLearning,3bbijy,mostly_reasonable,5,Sun Jun 28 01:02:48 2015 UTC,Question from a noob. What's a simple example I can look at to see how to apply dropout to my own neural net?
MachineLearning,3bbijy,_superior_username_,2,Sat Jun 27 19:17:01 2015 UTC,https://github.com/Newmu/Theano-Tutorials  The modern net file has a dropout function that's really simple.
MachineLearning,3bbijy,alexmlamb,1 point,Sat Jun 27 23:16:31 2015 UTC,Here's an example
MachineLearning,3bbijy,piesdesparramaos,4,Sat Jun 27 23:06:41 2015 UTC,"There's some more information required, such as what the difference between your train/validation/test accuracies are so we can see whether bias or variance is the issue.   From my (albeit limited) understanding, with neural networks a good way to find optimal neural structure is to continually add complexity. This can be done by increasing units per layer, or number of layers. The result is typically the network begins to overfit the training set, and then regularization is used to level out the bias/variance tradeoff and repeat. The most complex/deep network you can train without overfitting typically performs quite optimally. Though, might want to take this with a grain of salt, and if anyone has any corrections I'd love to hear.  Note: Regularization is a pretty broad term, but techniques like dropout and weight constraints are often useful in NN setting."
MachineLearning,3bbijy,holdie,1 point,Sat Jun 27 17:34:59 2015 UTC,"Thanks for your reply   There's some more information required, such as what the difference between your train/validation/test accuracies are so we can see whether bias or variance is the issue.   The validation/training sets also top out around ~71%, which is why I suspect the model is underfitting, or something screwy is going on.   From my (albeit limited) understanding, with neural networks a good way to find optimal neural structure is to continually add complexity. This can be done by increasing units per layer, or number of layers. The result is typically the network begins to overfit the training set, and then regularization is used to level out the bias/variance tradeoff and repeat.   This was my understanding too, but adding complexity hasn't had much of an effect at this point. Maybe I just haven't been pushing it hard enough?"
MachineLearning,3bbijy,anonDogeLover,2,Sat Jun 27 17:48:22 2015 UTC,"This was my understanding too, but adding complexity hasn't had much of an effect at this point. Maybe I just haven't been pushing it hard enough?   That would be my initial guess. I would try to see how many units per layer/how many layers you can get in your model until it begins to overfit (which it should, if the problem is solvable with the same training set using linear regression)."
MachineLearning,3bbijy,arshakn,3,Sat Jun 27 18:22:38 2015 UTC,"In some ways the difference between the two logistic regression functions is the more mysterious part, in theory logistic regression should be solving the same, convex optimization function so I would have expected the results to be similar. That plus your difficulty beating 71% through other means makes be wonder whether the logistic regression in R is doing something clever behind the scenes, for example in how it represent the categorical features or whether it is reweighting classes. If you are really determined I would dig into the source a little to see if anything jumps out.  Its a bit odd you can't get a neural network to overfit, one thing to check might be whether you have ""pure noise"" data points, that is you might have data points that have identical features but different labels. With 30k examples in a sparse dataset with categorical values this sounds possible and would explain why the training accuracy caps out.   Another likely culprit is how the networks were trained, it sounds like you experimented pretty well but since you didn't mention it I would add that it is often crucial to ""anneal"" the learning rates during training (train with a high rate until performance peaks, then drop to a lower learning rate) to get peak performance with neural networks."
MachineLearning,3bbijy,phillypoopskins,2,Sat Jun 27 18:54:43 2015 UTC,"Thanks for your reply.   If you are really determined I would dig into the source a little to see if anything jumps out.   Good idea. I'll look into that.   Its a bit odd you can't get a neural network to overfit, one thing to check might be whether you have ""pure noise"" data points, that is you might have data points that have identical features but different labels.   I meant to add something along these lines in the original post. I definitely think this is a noisy data set and that there's some hard limit on model accuracy for these particular features. But the fact that the logistic regression model in R is achieving an accuracy of ~74% means that hard limit hasn't been reached by the neural net.  Regarding learning rate annealing, I have experimented with it a bit, but it hasn't helped either."
MachineLearning,3bbijy,piesdesparramaos,2,Sat Jun 27 19:32:58 2015 UTC,"In some ways the difference between the two logistic regression functions is the more mysterious part, in theory logistic regression should be solving the same, convex optimization function so I would have expected the results to be similar. That plus your difficulty beating 71% through other means makes be wonder whether the logistic regression in R is doing something clever behind the scenes, for example in how it represent the categorical features or whether it is reweighting classes. If you are really determined I would dig into the source a little to see if anything jumps out.   Traditional logistic regression (as performed by R's glm) is a maximum likelihood convex optimization problem. Even when it doesn't have a unique solution due to a non full-rank model matrix, the solutions form a linear subspace and all of them result in the same predictive mapping with the same predictive performance.   The core R packages come from a stats culture that cares about reliably producing the maximum likelihood model fit, regardless of whether it is a good predictor. Sklearn comes from the machine learning culture that cares about predictive performance and scaling to big data, and has little interest in optimally fitting models. Their motto is ""who cares - the model is wrong anyway, it's just a tool...""  The fact that normalizing the data improved sklearn's logistic performance is a clear flag that sklearn's algorithm is not robustly solving the maximum likelihood problem, and has to be coaxed to converge properly on difficult problems. I respect sklearn very much, but this is exactly what we should expect given the culture sklearn came from."
MachineLearning,3bbijy,watersign,2,Sun Jun 28 18:55:35 2015 UTC,"It's impossible to say without knowing more about your data, because a neural network will do about as well as (or slightly worse than) a linear model if your data generating process is linear."
MachineLearning,3bbijy,syllogism_,2,Sun Jun 28 00:39:21 2015 UTC,"That is the issue with NN, it is a challenging engineering problem to find the right hyperparameters...  IIRC, scikit-learn logistic regression model is regularized by default. So, as someone already suggested, you should therefore introduce a regularization param in your network. The way you initialize you weights and the optimization algorithm you use are also very important factors.   Also, I read one comment stating that for very sparse data, linear models are usually almost as good as it gets. But I don't know if there is any truth on that statement..  EDIT:  Some interesting advice: http://yyue.blogspot.ca/2015/01/a-brief-overview-of-deep-learning.html  EDIT 2: What optimization algorithm are you using? Because the comparison is not only among NNs and LR, but among (NN + optimization algorithm) and (LR + optimization algorithm). You could perfectly beat a complex structure with a simpler one if the optimization strategy is better. For very sparse data I have had good results with AdaGrad."
MachineLearning,3bbijy,merlin0501,1 point,Sat Jun 27 17:59:12 2015 UTC,"Thanks for your reply.   IIRC, scikit-learn logistic regression model is regularized by default. So, as someone already suggested, you should therefore introduce a regularization param in your network. The way you initialize you weights and the optimization algorithm you use are also very important factors.   I've tried including L1 and L2 regularization and they haven't had any effect. The weights and optimization algorithms are something I don't really have a lot of knowledge about, so any recommended resources would be greatly appreciated."
MachineLearning,3bci3r,Articulated-rage,-1,Sat Jun 27 22:32:16 2015 UTC,"Why are chatbots interesting?  edit: since this is top comment and most of my comments are now 0.  is it because I'm not impressed?  I'm trying to be nice and point out their limitations.  it feels like people are too enamored by shiny bells and whistles that they don't realize the triviality of it.    also, so it's clear:  there's a distinction between chatbots and dialogue systems. dialogue systems typically go from understanding -> dialogue state -> generation.  This allows for task state to play into what utterance to generate.   It can and has been a statistical endeavor.  It can and should be trained end-to-end.  This is not that.  This is just seq-to-seq on movie scripts."
MachineLearning,3bci3r,ogrisel,3,Sat Jun 27 23:55:49 2015 UTC,This one is interesting because it's trained end-to-end from unlabeled data.
MachineLearning,3bci3r,Articulated-rage,-1,Sun Jun 28 18:02:16 2015 UTC,"I can appreciate that answer.   Chat bots just feel like dead ends.  There's no where to go but better statistics.  If you need to make any decisions,  you have to leave the chatbot world.    fwiw, For deep learning and dialogue,  Steve young's group out of Cambridge has great work."
MachineLearning,3bci3r,VelveteenAmbush,5,Sun Jun 28 18:20:16 2015 UTC,"Chat bots just feel like dead ends. There's no where to go but better statistics. If you need to make any decisions, you have to leave the chatbot world.   Well, first of all, all of deep learning, or all of machine intelligence, or for that matter all of intelligence, could be reductively dismissed as ""better statistics."" Your own brain is nothing more than a piece of meat that outputs electrical signals based on ""statistics."" In this context, it's a pejorative with no content.  Second of all, chatting is how humans interact. It is the medium by which humans most naturally communicate and receive information. A good chatbot is a one-size-fits-all solution for transferring information from human to machine and back to human. Do you not think that is commercially profound? Google even constructed an example of a machine tech support bot. With improvement, this sort of thing could theoretically replace much of customer support across every industry that needs it. Even that would be just scratching the surface.  There are more exotic uses as well. The chatbot could learn to use certain tokens in its conversation to cause it to take actions in the same way that it learns to use words -- thus allowing it to make decisions. Maybe in a few years, smartphones will be nothing more than an earpiece running a chatbot, which would act like a human secretary does now, summarizing your email, taking dictations for reply emails, etc., with a rich understanding of context and conversation. A chatbot will also translate the conversational context into a vector space which could be analyzed at scale by additional neural nets. Who knows what kind of information would be accessible by that method."
MachineLearning,3bci3r,simonhughes22,1 point,Sun Jun 28 21:05:47 2015 UTC,Because you can talk to them like a person. Are you a chatbot?
MachineLearning,3bci3r,Articulated-rage,-1,Sun Jun 28 02:31:46 2015 UTC,"It's so unsatisfying when much more profound research exists.  For example, a conversational hologram could be created for a living person.    Re: if I am a chatbot. If I were a machine,  I would refer to myself as a dialogue agent."
MachineLearning,3bci3r,devDorito,1 point,Sun Jun 28 04:19:10 2015 UTC,"Well, holograms will be more interesting once they get chatbots to actually begin passing Turing tests on a regular basis, because then they can begin adding things like gestures, emotion in a 3d face, et cetera."
MachineLearning,3bci3r,Articulated-rage,0,Sun Jun 28 04:54:38 2015 UTC,You missed the point.  Chatbots won't ever make purposeful descriptions.
MachineLearning,3bci3r,VelveteenAmbush,1 point,Sun Jun 28 05:14:11 2015 UTC,What does purposeful mean in this context?
MachineLearning,3bci3r,Articulated-rage,1 point,Sun Jun 28 21:08:58 2015 UTC,"Chatbots are argmax or sample from posterior.  Purposeful means sensitive to goals and intentions of the conversation. Basically,  parameterizing the flexibility of language across contexts.     There's a continuum from large scale but meaningless and small scale but meaningful---from pure data driven to pure hand crafted.  Really, the answer is in between.    Though,  historically,  chatbots were things like Alice and AIML (artificial intelligence markup language). It associated input words with response words.  Then,  people started building off of data and it became word to word mappings.  But they don't capture the ambiguity in what people say.  People can mean different things in different contexts and it's up to the hearer to pick it up."
MachineLearning,3bci3r,Articulated-rage,0,Sun Jun 28 22:18:52 2015 UTC,"Thought of a better way of answering your question on a run tonight.   Dialogue systems do understanding to a dialogue state, then choose a dialogue move that moves the state forward according to some task specification, then generate according to that dialogue move.   So: understanding -> dialogue state -> generation.   This is what I mean by purposeful.  Starting from a chatbot, this is extremely difficult.  You have to engineer your system differently to allow it to complete tasks with its conversations."
MachineLearning,3bci3r,VelveteenAmbush,1 point,Mon Jun 29 04:38:18 2015 UTC,"Starting from a chatbot, this is extremely difficult. You have to engineer your system differently to allow it to complete tasks with its conversations.   The paper provided an example of a tech support chatbot that seems to gather facts and then try to solve the user's problem. So it seems incorrect to claim that this sort of tool, or its successors, won't be suitable to complete tasks."
MachineLearning,3bci3r,Articulated-rage,0,Mon Jun 29 18:30:52 2015 UTC,"That's the thing,  it seems to gather information and try to solve tasks.  It's a parrot.  In another conversation it also asked contradictory questions after gaining evidence from the user. It won't and can't know when it's done something incorrect,  when repair strategies are needed to ground the conversational record (make sure they are on the same page),  etc.    Tldr. Parrots just say things."
MachineLearning,3bci3r,VelveteenAmbush,1 point,Mon Jun 29 18:52:37 2015 UTC,"That's the thing, it seems to gather information and try to solve tasks.    No, it actually gathers information to solve tasks. That is what its actual behavior was, based on the sample conversations in the paper. Maybe you think it's all a giant illusion, but I say, so what if it  is? -- if the Chinese Room can translate English sentences into Chinese sentences, then the Chinese Room knows Chinese.   It won't and can't know when it's done something incorrect, when repair strategies are needed to ground the conversational record (make sure they are on the same page), etc.   If such a system were deployed as a customer service tool, you could collect some signals about how well the customer thinks the conversation went (via semantic analysis or just by ""how helpful was this conversation on a scale from 1 to 5""), and continue to train the master model on the good ones.   In another conversation it also asked contradictory questions after gaining evidence from the user.   It seems like you're arguing that the approach is doomed because this iteration of the model isn't perfect.   It's a parrot.   It handled questions that weren't in the training data, and it formulated contextually appropriate responses that weren't in the training data. I challenge you to find a parrot capable of that level of contextual conversation."
MachineLearning,3bd0ba,gopher_logik,2,Sun Jun 28 01:33:29 2015 UTC,"The keras library is an excellent and surprisingly easy library to use to apply deep learning to most problems including image recognition and NLP problems. There are a number of example scripts showing how to use to solve some cool problems, such as learning captions for images and generating sample text. And Deep Learning is hot. Easiest thing to do would be to apply it to a new problem domain, something quite niche like they did with the Merk Kaggle competition."
MachineLearning,3bd0ba,simonhughes22,1 point,Sun Jun 28 02:30:45 2015 UTC,Thanks for the heads up. It's a good place to start.
MachineLearning,3bd0ba,winstonl,1 point,Sun Jun 28 03:41:05 2015 UTC,"I suggest start with something easy, and one that has been done by many people before (so you have a tonne of readings/examples to look at).  Deep learning is good, but it may not be the easiest place to start. Why not look at some simpler problems, like some binary classification problems?"
MachineLearning,3bd0ba,simonhughes22,1 point,Sun Jun 28 17:39:53 2015 UTC,I'd also agree with that.
MachineLearning,3bd0ba,simonhughes22,1 point,Mon Jun 29 19:45:28 2015 UTC,I see what you mean. I am basically looking for a very easy deep learning problem (if one even exists).
MachineLearning,3bbv9u,andrewbarto28,1 point,Sat Jun 27 19:04:41 2015 UTC,Caffe models and code for Fully Convolutional Networks for Semantic Segmentation are available in the Caffe model zoo: https://github.com/BVLC/caffe/wiki/Model-Zoo#fully-convolutional-semantic-segmentation-models-fcn-xs
MachineLearning,3bbv9u,beneuro,1 point,Sat Jun 27 19:12:35 2015 UTC,Thanks I was looking for this one too.
MachineLearning,3bc9o4,oren_a,2,Sat Jun 27 21:14:37 2015 UTC,"Yes, train a neural network with 7 inputs, as many hidden layers as you want and 7 outputs. Make sure the activation function in the outer layer gives you values in the range of the expected scores. You could treat this as a regression problem, or a classification problem if you normalize the scores to the range 0-1 and treat them as probabilities. I am assuming the features and scores are somehow related in someway. I'd recommend the keras library on github. It has a lot of different features and models and is surprisingly easy to use.  You may not need to solve this with a neural network. You could train a simple regression or classification model, by training a separate model for each output (but taking all inputs as input). I'd always start with a simple linear model before trying something powerful and complicated like a neural network. Linear models excel at a wide range of problems, whereas (right now) deep learning does better at a small subset of problems that have a specific structure and normally a lot of data."
MachineLearning,3bc9o4,simonhughes22,0,Sun Jun 28 02:37:58 2015 UTC,"Thanks, Do you have any example code for a similar problem?"
MachineLearning,3bc9o4,dwf,1 point,Sun Jun 28 07:35:40 2015 UTC,How much training data do you have?
MachineLearning,3bbze2,CrossfitFTW,2,Sat Jun 27 19:41:11 2015 UTC,"I was going to suggest that. You will likely get the best result with a linear kernel, from the problem's i've had anyways. I'd stick with the linear svm or try logistic regression with an L1 or L2 regularizer, but you will get further by spending time experimenting with different features \ transformations on the inputs rather than playing with different models. Try using the python gensim library (https://radimrehurek.com/gensim/) to compute LSA vectors for the documents to feed to the SVM or using the TfIdf vectorizer in sklearn to feed in bi and tri-gram features from the documents also.  If you want to get really fancy, try a gradient boosted tree in sklearn, or try training a recurrent neural network on the word sequences (using a GRU or LSTM). The python \ theano based 'keras' (https://github.com/fchollet/keras) and library and 'passage' (https://github.com/IndicoDataSolutions/Passage) libraries are excellent for doing this."
MachineLearning,3bbze2,simonhughes22,1 point,Sat Jun 27 21:11:36 2015 UTC,"However, for this sort of problem you will likely get more benefit from experimenting with the features than training a bunch of different models on the same set of features."
MachineLearning,3bbze2,simonhughes22,1 point,Sat Jun 27 21:12:35 2015 UTC,Thanks!
MachineLearning,3bbyus,sqrt,3,Sat Jun 27 19:36:14 2015 UTC,Knew this was going to be some primo ill-informed tech journalism as soon as I saw   a core part of online services such as Microsoft's Bing
MachineLearning,3baew7,clbam8,2,Sat Jun 27 08:50:16 2015 UTC,You can do this trick with humans to. They're called optical illusions.  You could probably do it far more effectively if you had the processing power and could look at someone's brain structure. You could generate images where one person would be convinced it's a lion and everyone else just sees noise.
MachineLearning,3baew7,Cantareus,2,Sun Jun 28 06:45:34 2015 UTC,"They're showing the DNN abstract images.  I wouldn't call that ""fooling""."
MachineLearning,3baew7,veltrop,6,Sat Jun 27 10:01:40 2015 UTC,"...and the DNN classifies them with >99% certainty. I'd say it's ""fooled""."
MachineLearning,3baew7,BadGoyWithAGun,2,Sat Jun 27 10:08:40 2015 UTC,"It's as much ""fooling"" the DNN as these stupid ""jokes"" that only work by intentionally abusing imprecise language."
MachineLearning,3baew7,skgoa,2,Sat Jun 27 18:19:54 2015 UTC,"It's squarely out of domain. Nobody really expects any discriminatively trained learner to do well on things far outside of what they were trained on (or they shouldn't). The adversarial examples papers, on the other hand, are a bit more interesting because the images look like they're in-domain but they're confidently classified as wrong."
MachineLearning,3baew7,dwf,3,Sat Jun 27 16:09:48 2015 UTC,"Actually,  since the early days of statistical decisions,  people have expected algorithms to give proper reject responses when it shouldn't give an answer because of uncertainty (prototypical example: a cancer classifier).  If anything,  this is an argument for that.  But the DNN model doesn't have the uncertainty to know it should reject.  It's an interesting point."
MachineLearning,3baew7,Articulated-rage,3,Sat Jun 27 16:22:54 2015 UTC,"Sure, you can recalibrate against held out data to set a threshold for a reject option, or you can train with a separate ""rubbish"" class. But in either setting you're dealing with the average case. Such expectations have almost never concerned an adversarial setting where somebody is crafting an input specifically to fool you, and even classical linear classifiers can be made to output high confidence nonsense for an adversarially crafted example."
MachineLearning,3baew7,dwf,1 point,Sat Jun 27 17:30:16 2015 UTC,"Oh ya,  totally.  It's a bit unfair to make specially crafted strategies to fool the dnn.  It's not ecologically valid.  Now,  if you were using a dnn for pedestrian detection in a self driving car,  you'd want to capture that uncertainty. If these cases arose in realistic settings,  it'd be dangerous."
MachineLearning,3baew7,Articulated-rage,1 point,Sat Jun 27 17:58:03 2015 UTC,"Slightly off topic question: What is a discriminatively trained model? Is it related to a discriminative model, the one that learns a conditional distribution p(y | x) than a joint distribution p(x, y)?"
MachineLearning,3baew7,dwf,1 point,Sat Jun 27 19:26:07 2015 UTC,"Some models are unambiguously discriminative models that represent p(y|x) explicitly, and have no way of reasoning about other quantities related to the joint distribution p(x, y). But there do exist discriminative training procedures for models normally thought of as generative, for example."
MachineLearning,3baew7,veltrop,1 point,Sat Jun 27 20:58:49 2015 UTC,Fair enough.  I was likening it to seeing imagery in clouds or a rorschach.
MachineLearning,3bb7f6,Oleicas,2,Sat Jun 27 15:32:49 2015 UTC,"Take a look at https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/ with practicals at - https://github.com/oxford-cs-ml-2015  What I like about these practicals is that they move from very simple things about Torch7  to implementing complex models such as LSTMs. It takes 6-8 hours to work through them and at the end of it, you become fairly proficient enough to read other Torch7 codes at https://github.com/torch/torch7/wiki/Cheatsheet#newbies  and implement something on your own.  Torch7 is a fairly easy language to get used to, and it has lots of example code for various deep learning architectures and has built in back-propagation through nngraph. It also has MPI support built, so you can run code automatically on multiple cores."
MachineLearning,3bb7f6,dexter89_kp,1 point,Sat Jun 27 22:39:04 2015 UTC,"Thank you !,I've  never heard about it"
MachineLearning,3bb7f6,jdsutton,2,Sun Jun 28 12:35:18 2015 UTC,See if this helps you: http://natureofcode.com/book/chapter-10-neural-networks/
MachineLearning,3bb7f6,Bhavishya1,1 point,Sat Jun 27 15:50:01 2015 UTC,"It looks good, I was looking for something like this with some examples of algorithms, thank you :)"
MachineLearning,3b6185,aranag,3,Fri Jun 26 06:52:47 2015 UTC,Why is this so highly voted up?
MachineLearning,3b6185,classicalhumanbeing,1 point,Fri Jun 26 22:38:19 2015 UTC,Not sure... I'm just gonna watch all of it anyways!
MachineLearning,3b6185,MusicIsLife1995,1 point,Sat Jun 27 19:18:49 2015 UTC,I'd like to know too. Is that a well known/good place to start?
MachineLearning,3b6185,bagelorder,1 point,Sun Jun 28 15:28:28 2015 UTC,"Honestly, the bank/car illustration is...silly? It certainly didn't get his point across and I'm not sure it's even a valid point."
MachineLearning,3b6185,Pandanleaves,1 point,Mon Jun 29 04:43:00 2015 UTC,Bookmarked
MachineLearning,3b6185,LurkForever,1 point,Fri Jun 26 16:30:39 2015 UTC,Fantastic.
MachineLearning,3b85f8,devries89,2,Fri Jun 26 19:10:45 2015 UTC,those pesky Unintended Consequences
MachineLearning,3b85f8,Darkmoth,3,Fri Jun 26 20:12:55 2015 UTC,"I'm a little surprised this work was published at a ML venue (even as a workshop paper).  It appears to me as basically just the vanilla application of a CNN in an interesting area, and seems more suited for a social sciences or computer vision journal.  As someone who has not yet attended the popular ICML DL workshop, does it primarily consist of applications of standard models in various settings?"
MachineLearning,3b85f8,iidealized,3,Fri Jun 26 20:46:54 2015 UTC,"Workshops are a normal place for ""interesting, but needs to be fleshed out before a proper conference"" kinds of papers. Interesting applications certainly fall in this category, and an exploration of transfer learning for real world tasks is also publishable - finding out why something doesn't work can be a key step to figuring out what does! It seems like just the right thing for a workshop in my (biased) opinion.  The paper has more depth in the experiments, and talks a fair bit about what worked and what didn't.   Ultimately, to make this model work well for attractiveness (if it is even possible!) or other prediction tasks you probably get a multi-task prediction network, which is definitely publishable in a conference."
MachineLearning,3b85f8,kkastner,1 point,Fri Jun 26 20:38:27 2015 UTC,"I complete agree with your statements.  However (not trying to be a harsh critic, just genuinely curious), what exactly did this paper investigate on why transfer from one of the domains is better than from the other.  To me, it seems there is just a report of empirical results, but no further scientific investigation regarding the why question.  Furthermore, they never even compared to just training a CNN (with less parameters) from scratch for just this dataset to see what transfer learning even gains us in this application.  Most of the application papers I have seen (outside of ICML DL workshop) introduce new models developed specifically for the application under consideration (and hence contain new ideas that are probably useful in related applications as well)."
MachineLearning,3b85f8,iidealized,2,Fri Jun 26 23:35:01 2015 UTC,"They show that the gender network features do not transfer to attractiveness, even though it is reasonable to believe they could work. Even though the dataset is not public (and can't be) this dataset gathering and discussion of how the data exists ""in the wild"" is also useful.  They show (again...) that ImageNet features are useful on a huge number of tasks, and discuss their thoughts on why the transfer of gender features failed in section 4.  All in all, I think the ""interestingness"" of the task, coupled with the discussion of the dataset collection efforts, data statistics, and the problems in real data are useful. They also prove out a working approach to something that is not typical image recognition (gender/attractiveness) as well as several things which do not work. To me a workshop is exactly the right place for this work - not every paper has to be about a new model.  Had the paper used a semi-supervised or generative model instead of a CNN would it somehow be better? If anything, I think they would get more criticism from taking a non-standard approach to image classification since they are working on an entirely new dataset - ""why didn't you try a convnet? CNNs are meant for images"" etc. etc. And if they had done all of this stuff, it is almost too much for just a workshop paper!"
MachineLearning,3b85f8,kkastner,1 point,Sat Jun 27 17:07:13 2015 UTC,"Right, but the why is left to pure speculation on their part.  There seems to be little scientific about their insights to me."
MachineLearning,3b85f8,iidealized,2,Sat Jun 27 21:55:07 2015 UTC,"The best approach would probably combine picture features with collaborative filtering.  ""You can know a man by the company that he keeps"""
MachineLearning,3b85f8,alexmlamb,2,Sun Jun 28 21:48:52 2015 UTC,Obviously the fundamental flaw is Garbage-In-Garbage-Out which is especially the case in terms of trying to (self-)identify traits of individuals.
MachineLearning,3b85f8,mantrap2,1 point,Fri Jun 26 19:41:42 2015 UTC,Would probably work fairly well if given profile information.  trends of shared interests and such would increase the chance of it working
MachineLearning,3b85f8,dogedickguy,1 point,Fri Jun 26 19:15:02 2015 UTC,I'm curious how well this method would fair against the standard Tinder selection algorithm in terms of 'successful' matching.
MachineLearning,3b85f8,MrTwiggy,-1,Sat Jun 27 01:02:38 2015 UTC,Yup! It is garbage in  garbage out. Something like 60% of online dating profiles are bogus. Psychological forms help more if they are the right type (e.g. attachment styles in adults). So the tools are there and I don't see a great need for machine learning.
MachineLearning,3b81u6,sleepicat,2,Fri Jun 26 18:44:22 2015 UTC,"Graphical models/factor graphs are the formalism of choice for probabilistically coherent reasoning about situations like this. Where you have information, you can naturally build it in, in the form of potentials/factors/observed random variables. Where you have unobserved relationships you can often model them with latent (unobserved) variables. A variety of techniques for learning and inference in the presence of latent variables exist, the most popular being Expectation-Maximization and its approximate variants."
MachineLearning,3b81u6,dwf,1 point,Fri Jun 26 22:18:02 2015 UTC,Check this: http://matthias-rolf.blogspot.com/2014/01/putting-hard-prior-knowledge-into.html Allows to impose very different constraints and can guarantee them while leaving lots of play for estimation. The example shows how to implement a physically known monotonicity relation between variables.
MachineLearning,3b8nko,elanmart,3,Fri Jun 26 21:28:34 2015 UTC,Look here and make sure you read all the comments.  https://timdettmers.wordpress.com/2015/03/09/deep-learning-hardware-guide/  PCIe-lanes are important and the 5820 only has 28. Make sure you get the right cuda compute capability if you plan to use Torch 7 with the Facebook upgrades. Look at Amazon instances for cheap use on a big system. Here is another set of recommendations: https://www.youtube.com/watch?v=EeIWzRr80W4
MachineLearning,3b8nko,mszlazak,2,Fri Jun 26 21:45:15 2015 UTC,"Do you have any sources that 28 PCI-e lanes affects the performance? I know that is it almost irrelevant for gaming applications, i would expect it to matter even less for machine learning applications.  That said, i think that the OP doesn't have enough budget to populate all 4 PCI-e sockets."
MachineLearning,3b8nko,Tulip-Stefan,1 point,Sat Jun 27 08:12:12 2015 UTC,"Ultimatively, you're limited by the speed of your system RAM. Common Dual-Channel DDR3 can push ~20-25 GB/s peak but this bandwidth is shared by all devices in the system. Now add up all the inefficiencies inbetween and one can come to the conclusion that it's almost impossible for such a system to keep even a single PCIe 3.0 x16 link running for any significant amount of time.    I know that is it almost irrelevant for gaming applications, i would expect it to matter even less for machine learning applications.   You are probably right. I guess even a x4 link provides more than enough bandwidth to feed any ML model that you'd consider to run on a GPU."
MachineLearning,3b8nko,NasenSpray,1 point,Sun Jun 28 11:03:12 2015 UTC,"Single GPU training/running a model where its computation greatly outweighs the data upload/results download costs.  Throw all of that out the window for training the model across multiple GPUs for performance or because it doesn't fit in a single GPU.  a 16x link gives you ~13.2 GB/s of 2-way bandwidth.  Divide by resulting size of 4x/8x to get the other bandwidths.  In my own experience, Maxwell GPUs are tough to scale with a model parallel decomposition at that bandwidth.  Data parallel decomposition tends to be easier both because Convolution layers have far more hidden units than weights and because you can copy gradient updates asynchronously whilst calculating the delta and weight gradient of the next layer."
MachineLearning,3b8nko,ItsAllAboutTheCNNs,2,Sun Jun 28 17:32:49 2015 UTC,"I don't want to use EC2, since spot instances are unreliable   Do you actually know this, or are you just parroting what you've read? I find them plenty reliable, and they're certainly suitable for student projects."
MachineLearning,3b8nko,dfarber,1 point,Fri Jun 26 23:42:28 2015 UTC,CPU is not super important. Get a Titan X
MachineLearning,3b8nko,ResHacker,2,Fri Jun 26 22:56:32 2015 UTC,"I think for RNN implementations, the CPU is still very important. CNN can ""easily"" be calculated on GPU's, so for CNN implementations, I would definitively go for a strong GPU. But RNN implementations tend to use vectorization extensions of the CPU. For that, a CPU that supports AVX2 would be very useful. I think that would be an Intel Haswell or newer."
MachineLearning,3b8nko,NasenSpray,1 point,Sat Jun 27 19:43:37 2015 UTC,What's your total budget?
MachineLearning,3b8nko,Taaanos,1 point,Fri Jun 26 23:08:42 2015 UTC,"If you are going to work with libraries that support multi gpu configs, like Minerva does,  and you plan to throw 3 or more cards yes you should consider a cpu with more lanes to utilize those cards. Rule of thumb is you get the single most high end card you can afford(Titan x?, 980,970).   So considering your use case you should go with a 4790k and the best card you can afford. Cuda cores and vram is your priority."
MachineLearning,3b8nko,ItsAllAboutTheCNNs,1 point,Sat Jun 27 11:30:32 2015 UTC,"Do not buy a SandyBridge CPU, they don't play nice with Maxwell GPUs and they have all sorts of issues with PCIE Gen 3 that never really got addressed.  My P9X79-E WS motherboards won't even post with a Maxwell GPU inserted if there's a Sandybridge CPU in them.    If IvyBridge, buy a Core-i7 4820 for ~$300 with its 40 PCIE Lanes so you can (if you wish) hook up to four $1000 GPUs each connected to PLX 8747 PCIE switches.  Suggested Motheboard: Asus P9X79-E WS.  If Haswell, buy a Core-i7 5930K for ~$500 (they got wise to people building cheap Ivybridge GPU servers with 4820s apparently).  Suggested Motherboard: Asus X99-E WS ($500), the same one in NVIDIA's Deep Learning Dev Box.  This is the Haswell version of the IvyBridge server I described above.  When I was in Grad School, I made ~$12,000/year.  After taxes, I reserved ~$1000 annually for computer HW.  The amount of time that HW cut from my thesis research was well worth the financial sacrifice IMO.  Looking at your situation, I'd go with Ivybridge,  Two of my quad GPU servers are based on it and the GPUs are by far the most expensive component of them (4 GTX Titan Blacks).  The Haswell server is filled with 4 TitanXs, but I think your budget limits you here.  That said, and I will get downvoted by the penny pinchers for saying it, avoid GTX 970 and its weird memory.  Buy GTX 980 or GTX 980 TI if you want to stretch.  Avoid superclocked variants of whatever GPU you buy, they frequently have unreliable FP32 reproducibility.    And spot instances are indeed the GTX 970 of Cloud Computing.  They're cheap, but the work you have to do to work around the constant threat of termination IMO tilts the scale towards buying ones own hardware.  Similarly, the obselescence of the Grid K520 GPUs in the g2 instances also tilts the scale towards buying modern HW.  That said, if you just want to dabble, g2 spot is the cheapest option.  Even if PCIE lanes are not big perf offenders now, everybody is working on parallelizing their frameworks.  At that point, you'll want all the bandwidth between the GPUs that you can get.  I've been developing multi-GPU applications for 7 years now.  Hear me now or believe me later."
MachineLearning,3b5efn,downtownslim,12,Fri Jun 26 02:51:21 2015 UTC,"Having a highly optimized OpenCL / CPU-only implementation of CNNs (this project seems to offer both) is very interesting. However this project:   Hasn't been updated since 2 months ago. Almost entirely lacks documentation of the API Lacks benchmarking numbers (comparison to CPU-caffe should've been straight forward)   In case someone else finds their naming confusing: the intel_visual_cloud_node directory is the one that contains the actual DL framework, while the visual_cloud_tools contains the CaffeNet example. Shitty names for directories. Also their CaffeNet implementation is pretty hard to decifer, IMO.  TL;DR If this project hasn't been abandoned already, it might be worth revisiting in a few months, once they actually have something worth discussing."
MachineLearning,3b5efn,BeatLeJuce,1 point,Fri Jun 26 07:13:34 2015 UTC,"I didn't find any code for training, perhaps this project doesn't support training either?"
MachineLearning,3b5efn,sunshineatnoon,2,Tue Jul 14 07:14:15 2015 UTC,"How does it implement ""Incremental learning at runtime""?"
MachineLearning,3b5efn,XalosXandrez,2,Fri Jun 26 05:55:47 2015 UTC,There is a very interesting and complete PR to caffe that brings OpenCL support. They even recently reported a ×2 speed increase of OpenCL vs cuda on a gtx980.
MachineLearning,3b5efn,pilooch,2,Sat Jun 27 14:18:59 2015 UTC,Can you post a link to the PR?
MachineLearning,3b5efn,rantana,1 point,Sat Jun 27 14:55:34 2015 UTC,"Here, https://github.com/BVLC/caffe/pull/2195#issuecomment-115215496"
MachineLearning,3b75ak,arshakn,2,Fri Jun 26 14:47:32 2015 UTC,"Not sure why the downvote?   That was interesting, straight forward, and I learned something new."
MachineLearning,3b75ak,Simusid,-2,Sat Jun 27 12:29:19 2015 UTC,"this was ""new and interesting"" a couple years ago. Now it's basically ""hello world"" for deep learning noobs."
MachineLearning,3b6zvo,pierrelux,2,Fri Jun 26 14:05:00 2015 UTC,please put it on teespring
MachineLearning,3b6zvo,fhuszar,1 point,Fri Jun 26 15:50:51 2015 UTC,I waunt that
MachineLearning,3b7qfl,chrisalbon,1 point,Fri Jun 26 17:23:02 2015 UTC,really great podcast.
MachineLearning,3b61a5,aranag,1 point,Fri Jun 26 06:53:40 2015 UTC,It went from stereo to mono... why?
MachineLearning,3b5uez,alexmlamb,2,Fri Jun 26 05:27:27 2015 UTC,I've heard rumors that the result isn't reproduceable.
MachineLearning,3b5uez,rantana,2,Fri Jun 26 07:34:16 2015 UTC,"In the domain of text understanding, definitely seen it perform better than a typical RNN, never seen it be competitive with the same # of params GRU.  My feeling is there is some small section of hyperparameters for any given problem where an IRNN is rightly set up to be able to do well, but gating RNNs by their architecture are a lot more robust/durable overall."
MachineLearning,3b5uez,alecradford,1 point,Fri Jun 26 20:23:15 2015 UTC,Were these rumors from sieisteinmodel? ;)
MachineLearning,3b5uez,Foxtr0t,1 point,Fri Jun 26 11:30:34 2015 UTC,"I know someone who has been able to use it for their problem, but it wasn't a reproduction of the paper per-se."
MachineLearning,3b5uez,kkastner,1 point,Fri Jun 26 15:16:18 2015 UTC,"I think they are reproduceable, but they seem quite sensitive to initialization."
MachineLearning,3b5uez,iamkx,2,Sun Jun 28 21:20:32 2015 UTC,"I tried to reproduce it, and did not get it to work. But I did not spend more than a day trying to."
MachineLearning,3b5uez,sieisteinmodel,2,Fri Jun 26 09:51:58 2015 UTC,"I was able to reproduce the results somewhat, and got results pretty much between RNN and LSTM/GRU. They are vastly faster to train though, so that's a plus."
MachineLearning,3b5uez,transcranial,2,Fri Jun 26 11:47:43 2015 UTC,"I played around with it since it is so easy to put together, unfortunately I was using it for next-character prediction so it is hard to quantify if it was performing better than GRU. At this point I'm inclined to stick with GRU over iRNN, however if I though I really needed a giant recurrent layer it would be an option.  Some applications are doing a single Relu layer for recurrence instead of LSTM/GRU, so iRNN might make sense in that context. It is definitely faster to train because the recurrent calculation is more lightweight."
MachineLearning,3b5uez,siblbombs,2,Fri Jun 26 14:13:25 2015 UTC,"I also spent a day or so trying to reproduce the results, and couldn't. The paper is pretty lacking in hyperparameter/implementation details."
MachineLearning,3b60jh,john_philip,1 point,Fri Jun 26 06:42:35 2015 UTC,Thank you for sharing.
MachineLearning,3b8bqe,redlikeazebra,3,Fri Jun 26 19:57:31 2015 UTC,I think it's overkill for machine learning.
MachineLearning,3b8bqe,neuromorphic101,2,Fri Jun 26 20:24:29 2015 UTC,I don't think it's even FOR machine learning.
MachineLearning,3b8bqe,XalosXandrez,2,Sat Jun 27 05:17:23 2015 UTC,"Very interesting. I could see this being useful by using machine learning to create a small network on computer, turn these into that setup, and then use them in another application. Definitely a bit overkill for machine learning settings, but if you go the medical route, if we could create a neural network using these and implant them into a creature, it could be used to bypass damaged neural pathways.  They'll need to shrink the size of these things to the nanoscale to make it more efficient, however."
MachineLearning,3b8bqe,devDorito,2,Fri Jun 26 21:14:03 2015 UTC,"Neurogrid already emulates biological nerve cell function with eight transistors per nerve cell. Anyway, this isn't related to ML."
MachineLearning,3b654k,DSLSharedTask,1 point,Fri Jun 26 07:46:36 2015 UTC,"How much a misconception is language identification a ""solved task""?. 5 years ago, there was some discussion: http://goo.gl/vB4CVb  Today, are we ""still a long way off perfect language identification of web documents, as evaluated under realistic conditions""?  The much shown 99+% for language ID is usually based on a fix set of not so related languages.  Do we really know which languages are similar? Finding similar languages from a corpora of 1000+ languages seems difficult ( http://www.aclweb.org/anthology/W/W14/W14-2211.pdf)  It also seems like different people are having ping-pong conclusions as to whether language ID for many many languages is possible: http://goo.gl/TR0MUU and http://goo.gl/RL9SNV  Most recent, Discriminating between Similar Languages (DSL) Shared Task also shows that what we know about language ID is still far from perfect: https://goo.gl/PBtXjd  Is Language Identification Really Solved?"
MachineLearning,3b6e05,validated1,6,Fri Jun 26 09:58:28 2015 UTC,Post an example of one of these questions. amueller is active on SO so I really doubt depth is the problem.
MachineLearning,3b6e05,dfarber,3,Fri Jun 26 12:13:16 2015 UTC,Try to ask in official sklearn chat.
MachineLearning,3b6e05,qwerty_nor,2,Fri Jun 26 11:13:02 2015 UTC,"http://scikit-learn.org/stable/support.html  As noted at the support page, some scikit-learn developers answer questions at SO (given they are tagged). If that doesn't help, you can ask mailing list.  sklearn's devs try to keep the library simple, so I don't think you need personal lessons. There are plenty of tutorials given by core devs that should give you everything you need to know."
MachineLearning,3b6e05,barmaley_exe,2,Fri Jun 26 11:48:16 2015 UTC,try lxmls http://lxmls.it.pt/2015/?page_id=24
MachineLearning,3b3dmn,Bhavishya1,18,Thu Jun 25 17:17:09 2015 UTC,Have a look at http://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/
MachineLearning,3b3dmn,mlberlin,1 point,Thu Jun 25 20:44:39 2015 UTC,This is a fantastic link. Thanks for sharing this!
MachineLearning,3b3dmn,nested_dreams,16,Fri Jun 26 01:56:17 2015 UTC,"This will provide you with a .bz2 file of the English version of wikipedia, so there are no images. Then this Wikipedia Extractor should clean it up in the way you are looking for.  edit:grammar"
MachineLearning,3b3dmn,dunnowhattoputhere,3,Thu Jun 25 17:58:58 2015 UTC,Thanks for the extractor link-- Really helpful to know this exists....
MachineLearning,3b3dmn,snarik,3,Thu Jun 25 18:01:30 2015 UTC,You might be able to leverage genism's code for this: https://github.com/piskvorky/gensim/blob/develop/gensim/corpora/wikicorpus.py
MachineLearning,3b3dmn,xamdam,2,Thu Jun 25 18:01:16 2015 UTC,"There's also WikiClean, which I've found to do a good job of removing unnecessary markup.  (Whether that's useful for you depends on the task you want to perform.)"
MachineLearning,3b3dmn,ndronen,1 point,Fri Jun 26 01:28:05 2015 UTC,I do want that. Can you post a sample of the data it gives as an output. Or maybe PM me.  Thanks.
MachineLearning,3b3dmn,yodaman92,2,Fri Jun 26 07:48:27 2015 UTC,"The Polyglot project has clean, preprocessed versions of Wikipedia in a bunch of languages. Extremely useful if you don't want to spend time cleaning stuff up! The English version has about 1.7 billion tokens, so it's not really small either."
MachineLearning,3b3dmn,BlueCarMathGuy,2,Fri Jun 26 14:36:34 2015 UTC,"If I were going create a dataset like that from scratch, I would use a HTML parsing tool like   BeautifulSoup  in Python. BeautifulSoup is great for web scraping but you'll still need to do some work cleaning it up.   I'm not sure how you would go about getting the URLs for all the articles. Maybe many many calls to the  random page while tracking which you have visited."
MachineLearning,3b3dmn,omgitsjo,2,Thu Jun 25 17:49:36 2015 UTC,"Wikipedia explicitly says that they would prefer if people didn't scrape their site.  They provide torrents of all their articles AND talk sections AND images.  The packages combined are around 10GB, unzipping to about 14TB.  See https://en.wikipedia.org/wiki/Wikipedia:Database_download"
MachineLearning,3b3dmn,welshej,2,Fri Jun 26 04:38:56 2015 UTC,You could also look into something like Beautiful Soup. Parsing out those tags that you don't want shouldn't be too tough.
MachineLearning,3b3dmn,r4and0muser9482,1 point,Thu Jun 25 21:19:53 2015 UTC,"Also, why bother with wikipedia only. Check out some of the datasets here, like Google ngrams and the Common Crawl. You'll find more data than you need there..."
MachineLearning,3b3dmn,r4and0muser9482,2,Fri Jun 26 07:13:33 2015 UTC,I need to use wikipedia because the word vectors that I use(Glove) are based on that
MachineLearning,3b5hhr,Dwood15,4,Fri Jun 26 03:19:02 2015 UTC,"Don't use an 8-bit code.  If you do, the net will have to figure out how to map all 256 different bit patterns to distinct entities implicitly in its internal representations. It will have to learn to deal with, and ignore, the fact that patterns 10101100 and 10101101 may have absolutely nothing to do with each other. This will require a bigger net than just using a 256-long 1-hot code.  If you want to cut down on processing you could try putting a linear bottleneck layer between the input and the first ""real"" hidden layer, or something like that."
MachineLearning,3b5hhr,dwf,2,Fri Jun 26 04:52:40 2015 UTC,"I think its usually worth adding features rather than trying to compress them, unless you're dealing with a large highly redundant set (e.g. http://people.idsia.ch/~juergen/compressednetworksearch.html ) that you think the true signal would be better represented in another space.  As an aside, you could try feeding both whether a block exists at each of the 256 unique objects, and the current distance from 'mario' to each block, or even the minimum travelling distance to each block (us A* or something, as some blocks will occlude others, a lot of options...). So 512 variables."
MachineLearning,3b5hhr,sifnt,1 point,Fri Jun 26 08:45:49 2015 UTC,"Thanks for the thoughts! Over a cursory reading of your post and some link-clicking, all I see are articles, not implementations of your link.  As far as what you're saying, it's definitely an interesting problem to have to deal with. Let's say I simplify the problem and keep it to the way it already is, which is if the sprite is within 8 units, x and y position the neuron is firing (-1 in this case). so it's a max potential of 8*8 inputs already. THEN, with the 256 sprites that would be 8 x 8 x 256 inputs, minimum. That's a lot, and I'm not sure it's necessary. I've got an implemenation already in the works that makes it 8x8x8. If you're interested, I'll post it up once I get it to run."
MachineLearning,3b5xx1,budhdub,3,Fri Jun 26 06:10:06 2015 UTC,"I really like this paper by Eigen and Fergus, Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture. The results are pretty incredible."
MachineLearning,3b5xx1,kkastner,1 point,Fri Jun 26 18:03:40 2015 UTC,looks cool
MachineLearning,3b2bl5,bhmoz,16,Thu Jun 25 12:03:25 2015 UTC,"For recurrent NN, it is not a problem to feed the net a sequence of variable length. You compute all your intermediary passes without penalizing. Just penalize the output when you have fed the whole sequence (final state).  You can use recursive NN (use syntactic or dependency tree structure of your sentence) or recurrent NN (sequentially, one at a time). There is a comparison in When Are Tree Structures Necessary for Deep Learning of Representations?, Li, Jurafsky, Hovy.   Using a tree, you also have a powerful semi-supervised method, see Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions, Socher et al..  The problem you are talking about (""double negatives"" and all) is called compositionnality."
MachineLearning,3b2bl5,rccr90,1 point,Thu Jun 25 12:31:14 2015 UTC,Thanks. I've added these to the reading list they cover exactly what I'm trying to do quite nicely.
MachineLearning,3b2bl5,wearing_theinsideout,3,Thu Jun 25 18:56:00 2015 UTC,"This look like  a pretty cool method. If this goes well post some update!  I'm currently using the old school method of sentiment (manual training for getting weights for each word). I have been able to achieve >70% accuracy on topic-specific dataset of tweets (such as only user mentions regarding 2018 election in Paraguay).   I haven't found a solid method of using 1 sentiment model on all types of text data. As soon as I apply the model onto a dataset that has to do with food, the accuracy drops to >60% for every tweet/sentence.  I forgot where I read that an average of 4 of 5 people agree on whether something is positive or negative so 80% is a natural limit when it comes to sentiment accuracy.  Good luck!"
MachineLearning,3b2bl5,dexter89_kp,4,Thu Jun 25 19:36:22 2015 UTC,"Additionally, you can find a very good class from Socher specifically teaching deep learning for NLP. Link: http://cs224d.stanford.edu/"
MachineLearning,3b2bl5,Foxtr0t,4,Thu Jun 25 14:47:40 2015 UTC,Look up Encoder - Decoder models for LSTMs and RNNs.  http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf bhmoz describes it perfectly. You compute forward passes for the entire sequence length and take the last hidden step and pass it on to softmax and your loss criterion.
MachineLearning,3b2bl5,ttt72,1 point,Thu Jun 25 17:27:26 2015 UTC,Thanks this ties in nicely with bhmoz's answer
MachineLearning,3b2bl5,jareality,5,Thu Jun 25 18:57:30 2015 UTC,"These days it's all about RNNs, but n-grams fed to a linear or naive-bayes classifier are a good baseline. N-grams take care of ""happy"" vs ""not happy"" case.  http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/  https://github.com/mesnilgr/nbsvm"
MachineLearning,3b2bl5,dexter89_kp,1 point,Thu Jun 25 19:19:36 2015 UTC,1-D convolutional NN with pooling also can accepts a sequence of  variable length data. Although I've never try it on NLP. I only tried 2-D NN on different size images.
MachineLearning,3b2bl5,jareality,2,Thu Jun 25 20:58:06 2015 UTC,is there a good paper or resource on variable length convnets in an application? i wonder how sizes are balanced out before a classification layer; different pooling schemes to yield fixed-size outputs?
MachineLearning,3b2bl5,ttt72,3,Fri Jun 26 01:35:26 2015 UTC,The original paper which kind of started the application of convnet for nlp. http://nal.co/papers/Kalchbrenner_DCNN_ACL14  A nice application oriented paper from Nando De Freitas's group at oxford.  http://arxiv.org/abs/1406.3830
MachineLearning,3b2bl5,jareality,1 point,Fri Jun 26 04:20:09 2015 UTC,"oh i see, so dynamic pooling layers..nice. thanks so much for the links! and it looks like that can easily be done with Theano even."
MachineLearning,3b2bl5,hires,3,Fri Jun 26 15:25:44 2015 UTC,"pooling is just local summary statistics of every grids. These grids can be overlapped, non-contacted and even different length to each other.  This paper is even crazier than only variable pooling : http://nal.co/papers/Kalchbrenner_DCNN_ACL14"
MachineLearning,3b2bl5,lmf4o,1 point,Fri Jun 26 04:32:59 2015 UTC,"true, i'm gonna dig deeper into this and attempt to apply to audio."
MachineLearning,3b6idm,tushar1408,1 point,Fri Jun 26 11:00:10 2015 UTC,check out these series of lectures https://www.youtube.com/watch?v=56TYLaQN4N8&index=14&list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu
MachineLearning,3b6idm,vivanov,2,Fri Jun 26 13:47:00 2015 UTC,"Specifically, the practicals for this course are online here: https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/ and have lots of intro-to-torch7 material."
MachineLearning,3b6idm,kjearns,1 point,Fri Jun 26 23:32:06 2015 UTC,i will look into it. thanks
MachineLearning,3b1vzj,HelmsmanRobertson,4,Thu Jun 25 08:20:00 2015 UTC,"Take a look at Karpathy's char-rnn and see what you can get out of it. Any time i learn a library I try to look at other people's use of it, so that's where I would start, personally."
MachineLearning,3b1vzj,devDorito,1 point,Thu Jun 25 09:24:08 2015 UTC,"I figured I'd do something similar by checking out the Linear module's source code... The problem is that I'm afraid that some of that code is ""excessively slick"" (if such a quality even exists)..."
MachineLearning,3b1vzj,devDorito,1 point,Thu Jun 25 09:55:36 2015 UTC,what do you mean by excessively slick?
MachineLearning,3b1vzj,j1395010,3,Thu Jun 25 09:56:36 2015 UTC,"Coming from an R background, there a lots of ways you could code up the linear algebra in the Linear module... Their use of an outer-product in Linear:updateOutput is really slick, but a bit unobvious, at first. I'm not saying that the slickness is bad per se, it just makes reading the code a bit of a more involved affair."
MachineLearning,3b1vzj,dwf,-9,Thu Jun 25 13:31:52 2015 UTC,"I'm using it on a Linux install with a github clone of char-rnn by Karpathy. It's very wonderful to learn from.  I'm not sure what your concern is with ""slick""ness of his code. Elegant code is good to learn from. Not sure people can help you if you want mediocre code to look at. None of us keep track of that and it doesn't really exist in Machine Learning. You need to read up if you don't understand something. It's all just algebra and code libraries like Torch and Luajit, etc. Learn. That's what you're supposed to do."
MachineLearning,3b1vzj,dfarber,5,Thu Jun 25 12:07:22 2015 UTC,None of us keep track of that and it doesn't really exist in Machine Learning   Wow
MachineLearning,3b1vzj,Noncomment,-4,Thu Jun 25 13:05:49 2015 UTC,"Bad code in machine learning is just code that processes so slowly that it's not useful.  If you disagree, say something more than ""wow"" but there is no room for poor mathematics or code in machine learning, go make mobile apps if you want processing laxity and forgiveness. That's why we even have theta calculations, to keep track of how efficient something is. First chapter of Introduction to Algorithms talks about this. There are standards and like something such as cryptography or this topic, very rarely does something get implemented uniquely and if it is, it's probably broken or bought up by Google or Palantir/your three-letter agency of choice."
MachineLearning,3b1vzj,shaggorama,3,Thu Jun 25 19:23:12 2015 UTC,"There is tons of code I would label ""bad code"" written by people doing machine learning. ""Bad"" machine learning code is a lot like any other bad code: it lacks documentation and tests, it is unnecessarily terse, it is poorly factored and less reusable than it could be, etc.  I haven't really looked at the Torch code or char-rnn code so I can't comment, but it's plenty possible to write blazing fast, terrible machine learning code."
MachineLearning,3b1vzj,changingourworld,1 point,Thu Jun 25 21:27:28 2015 UTC,"You're right. A lazy person can always find a way to make bad code but I guess I was talking about in terms of peer criticism and review, you're not going to get yelled at for failing to document properly as annoying as it is because deep down, whatever that person did has probably already been documented and if not, no wonder you don't understand it! They made something new!  Also poor factors in recursion suck too and whether or not something has a GPU library like Cuda or whatever, yeah. There are degrees of wholesomeness to a good machine learning program but I mean no one is doing anything drastic that wasn't just learned from someone else. It's a renaissance of people working hard on improving things because they need to be improved (also because we like to but this is born out of necessity instead of how slick we can get it to run just because it's impressive or something, my GTX 960 took a day to learn a book which is a pretty long time, any worse than that and this study/hobby stops making sense)  Aiming for things that are ""less slick"" sounds like you're leaving Machine Learning/Deep Learning and entering some prerequisite stuff like analysis and interpretation/reading data and creating models that fit/don't overfit. All that should come first before ML, imo."
MachineLearning,3b1vzj,kjearns,2,Fri Jun 26 13:12:36 2015 UTC,"you're an idiot. every little subfield has it's morons who love to believe they're the only group producing ""quality"".  Ask a professional software developer, a DBA, or a math professor what they think of machine learning coders, I'm sure you'll get a different opinion."
MachineLearning,3b48zp,Icko_,1 point,Thu Jun 25 21:02:50 2015 UTC,"Yeah, I was afraid of that... Does the fact that there are like 5 brands, each with 5-20 models help in any way? I mean it's a more specific case, with very few keywords."
MachineLearning,3b48zp,Foxtr0t,1 point,Thu Jun 25 21:18:19 2015 UTC,"I'm not sure you understood me. The idea is to do the whole thing fully unsupervised - a form of clustering, where two products can only be in the same cluster(or be ""matched"") if they are from different sites. I don't see how a classifier could do that, since we wouldn't know how many or what are the classes?"
MachineLearning,3b48zp,PoddyOne,1 point,Thu Jun 25 21:25:24 2015 UTC,"Thanks a ton man, I'll look into those things! I did thought about using pictures and text description, but these bring in a lot of complications, and are not as information dense. But you're right, it'll be harder with only the titles."
MachineLearning,3b48zp,mantrap2,1 point,Thu Jun 25 21:33:01 2015 UTC,"If unsupervised is tough and supervised would be easy, maybe try active learning. The idea is to learn with a minimum of human input."
MachineLearning,3b48zp,autowikibot,1 point,Thu Jun 25 21:53:57 2015 UTC,'Price' might be a pretty strong feature!
MachineLearning,3az4qj,benanne,18,Wed Jun 24 17:44:53 2015 UTC,"My colleagues made an interactive visualization of a dreaming convnet, inspired by Google's inceptionism art. You can tell it what to visualize in the stream chat :)"
MachineLearning,3az4qj,devDorito,5,Wed Jun 24 17:46:31 2015 UTC,Is this cuda or openCL? i'd love to run it on my r9's and see what I can turn out.
MachineLearning,3az4qj,devDorito,3,Wed Jun 24 20:35:27 2015 UTC,All CUDA I'm afraid! Python + Theano + Lasagne.
MachineLearning,3az4qj,rantana,3,Wed Jun 24 20:59:39 2015 UTC,"dang. oh well.  Every time i read that something is cuda, it pains me. Sure it's great in academic settings and closed circles, but if we ever find a home use for these algorithms then we should be willing to use openCL or even directcompute, despite the performance differences people have seen using nvidia's cuda."
MachineLearning,3az4qj,devDorito,3,Wed Jun 24 21:06:21 2015 UTC,Performance differences? Is there a performance advantage to ATI GPUs for neural networks?
MachineLearning,3az4qj,rantana,3,Wed Jun 24 23:11:34 2015 UTC,"If you're serious, as far as I can tell most people are using what they have on hand, and that's nvidia. Therefore, they use CUDA because nvidia is only focusing on their proprietary tech. I have yet to see a researcher or someone do benchmarks of OpenCL on AMD vs CUDA on nvidia cards, so I can't say that CUDA is in fact faster than OpenCl across the board. It's just faster on nvidia to use CUDA.   So, in essense: we don't know?"
MachineLearning,3az4qj,devDorito,2,Thu Jun 25 02:07:18 2015 UTC,"I haven't personally used OpenCL, but I've been told it's harder to use than CUDA. AMD hasn't invested nearly the same amount of resources in creating a reasonable scientific computing platform. On top of that, nvidia now has created a CUDA API specifically for deep networks, it's really a no brainer for researchers to use CUDA."
MachineLearning,3az4qj,ric2b,4,Thu Jun 25 02:17:46 2015 UTC,"My point isn't the ease of use. It's that if computing like this gets big, it should be platform neutral whether or not nvidia has put more time into making it easier for coders or so.   So far, the only arguments i've seen from the people on this forum are the performance benefits of using cuda. But they've only been using nvidia cards and most often don't even know if AMD has anything competitive to offer performance-wise. So people already using nvidia cards when they do these projects. I don't blame them for doing that, mind you, but i feel like it's short sighted for people to lock themselves and potential users into a single platform when they 'release' these projects and say they're ready for people to use, no matter what the benefits are."
MachineLearning,3az4qj,devDorito,2,Thu Jun 25 02:26:51 2015 UTC,"Well, if it gets big they can make an openCl version but in the meantime they should use whatever lets them develop faster and that seems to be CUDA"
MachineLearning,3az4qj,ogrisel,1 point,Thu Jun 25 06:55:48 2015 UTC,That's literally what i'm saying.
MachineLearning,3az4qj,albertzeyer,1 point,Thu Jun 25 06:56:44 2015 UTC,I think it could be comparatively fast but the libraries do not exist. The OpenCL support in theano is incomplete. And there is no equivalent to the proprietary cuDNN convolution library for OpenCL. cuDNN is using a lot of optimized Cuda assembly routines so it's probably not easy to reach the same speed with cross-platform OpenCL kernels anyway.
MachineLearning,3az4qj,treebranchleaf,2,Thu Jun 25 08:43:06 2015 UTC,"Theano is an abstraction. It has several backends, one is CUDA. OpenCL is currently work-in-progress, afaik.  http://deeplearning.net/software/theano/introduction.html"
MachineLearning,3az4qj,devDorito,1 point,Thu Jun 25 09:33:08 2015 UTC,"If you make an OpenCL backend to Theano, people will use it!"
MachineLearning,3az4qj,Galestar,1 point,Thu Jun 25 13:21:19 2015 UTC,"haha, you're right. I'm sure someone would use it."
MachineLearning,3az4qj,jamesj,2,Thu Jun 25 16:04:20 2015 UTC,Is it publicly accessible? =D
MachineLearning,3az4qj,jamesj,5,Thu Jun 25 00:43:39 2015 UTC,If you're talking about the code: that should be released in a few days!
MachineLearning,3az4qj,GratefulTony,1 point,Thu Jun 25 06:26:36 2015 UTC,Has it been? I'd be really interested to play around with it!
MachineLearning,3az4qj,fimari,1 point,Mon Jun 29 16:37:20 2015 UTC,Not yet. We're aiming for later this week.
MachineLearning,3az4qj,Chispy,1 point,Mon Jun 29 17:26:39 2015 UTC,cool looking forward to it!
MachineLearning,3az4qj,jamesj,2,Mon Jun 29 17:29:34 2015 UTC,Have you considered adding feedback to the image generation by doing a sentiment analysis on the comments window? Optimize to increase the sentiment vector magnitude?
MachineLearning,3az4qj,samim23,1 point,Thu Jun 25 05:18:32 2015 UTC,It's mostly just people shouting ImageNet class names (and a whole bunch of other random words). Not too much sentiment to analyze there I would say :)
MachineLearning,3az4qj,Mr-Yellow,5,Thu Jun 25 06:27:13 2015 UTC,Thats actually much better than TV  Music: https://www.youtube.com/watch?v=KqSp2L1mzIc and snacks. tarantula was creepy...
MachineLearning,3az4qj,seekoon,5,Wed Jun 24 19:40:56 2015 UTC,"I had it set to Boards of Canada - Music has the right to Children  Also watched it on my Oculus Rift DK2 using Virtual Desktop with a virtual IMAX sized screen wrapped around me. Definitely felt some trippy vibes from this thing.  I realized I was in a shared trip generated by an AI neural network in VR. It's straight from science fiction.   I could imagine this stuff being very popular in stoner lounges or even casual bars once virtual and augmented reality become as ubiquitous as the modern day smart phone. Machine learning will make this stuff far beyond the level of complexity we can imagine today.   One day we'll have AI coding endless exotic and immersive worlds, characters, and adventures for us. I've read a lot of Ray Kurzweils work, and I'm still very skeptical of his ideas. But seeing the recent progress and large investments in VR among large tech corps in only the last couple years, along with Microsofts Hololens announcement is really reducing that skeptic in me.  I had a thought about the childrens game we usually play when we look at clouds and imagine objects out of them. Now we're getting computers with the ability to play that game as well. It's amazing."
MachineLearning,3az4qj,Mr-Yellow,1 point,Thu Jun 25 05:19:23 2015 UTC,I did the same thing :)
MachineLearning,3az4qj,wotoan,2,Mon Jun 29 16:38:11 2015 UTC,"Holy crap, this is the perfect music!"
MachineLearning,3az4qj,cybrbeast,1 point,Wed Jun 24 20:22:24 2015 UTC,here is a recording of it with music: https://www.youtube.com/watch?v=FqvLc0GKN2s
MachineLearning,3az4qj,cybrbeast,6,Wed Jun 24 23:31:10 2015 UTC,"justin.tv is the domain with the video, for script blockers."
MachineLearning,3az4qj,vrld,1 point,Wed Jun 24 19:56:49 2015 UTC,"Worked fine for me with justin.tv blocked, but I had to unblock it to get the comment box."
MachineLearning,3az4qj,appliedphilosophy,1 point,Wed Jun 24 23:37:08 2015 UTC,"Strange, I enabled a few but didn't work until that one was on."
MachineLearning,3az4qj,OrionBlastar,4,Wed Jun 24 23:43:05 2015 UTC,Any details on implementation?
MachineLearning,3az4qj,quirm,5,Wed Jun 24 19:09:54 2015 UTC,There's some info in the description underneath the stream. Here's a brief blog post with some technical details: http://317070.github.io/LSD/
MachineLearning,3az4qj,dhammack,2,Wed Jun 24 19:27:45 2015 UTC,Amazing work. How much extra resources would be required to push the visuals in full HD?
MachineLearning,3az4qj,dhammack,4,Wed Jun 24 23:26:59 2015 UTC,"Probably a ton! At the moment it's generating a ~600x350 image on a single GTX 980 gpu, at about 1 frame every 4-5 seconds. The rest is interpolation. If you have a ton of GPUs you could probably spread the workload :)"
MachineLearning,3az4qj,Silverstance,2,Wed Jun 24 23:34:40 2015 UTC,"Seems perfect for a distributed computing screensaver. Very much like Electric Sheep, ever heard of that project?  http://electricsheep.org/   Electric Sheep is a collaborative abstract artwork founded by Scott Draves. It's run by thousands of people all over the world, and can be installed on any ordinary PC, Mac, Android, or iPad. When these computers ""sleep"", the Electric Sheep comes on and the computers communicate with each other by the internet to share the work of creating morphing abstract animations known as ""sheep""."
MachineLearning,3az4qj,Megatron_McLargeHuge,1 point,Wed Jun 24 23:37:48 2015 UTC,"I don't quite understand how the prior works (did not read too much into deep nets). Is this how it works?   Estimate the mean and covariance of the (concatenated) RGB values of the pixels in the patch over patches cut from natural images. For each image, for every pixel, compute the likelihood of the patch centered at the pixel. Feed the result into the NN."
MachineLearning,3az4qj,vincentrevelations,5,Wed Jun 24 20:42:26 2015 UTC,"Almost! What we are optimizing is basically the derivative of some class output of the network (chosen by the viewers) with respect to the input of the network, let's call that x. So you can easily add a prior to that optimization problem in the form of a differentiable function of x, that you add to the objective function (scaled by a regularization parameter).  In our case, this extra term is the log likelihood of a gaussian over all 8x8 patches in the image, as well as 8x8 patches in several downscaled versions of the image (to model correlations on a larger scale)."
MachineLearning,3az4qj,Noncomment,2,Wed Jun 24 21:01:15 2015 UTC,"Thank you, this should be in the text itself for more clarity for those who just want to know the algorithm.   A picture of the downscaled versions would be awesome too. It reminds me of the work of Portilla and Simoncelli. Perhaps you could also try to match the texture statistics across the downregulated images like in Portilla & Simoncelli. That way you would get more inter-related objects rather than legs and bodies but no fractal spiders in a visually compelling way."
MachineLearning,3az4qj,distortednet,3,Thu Jun 25 22:56:15 2015 UTC,"Where is the source code behind this, is it open sourced? It looks like a bunch of pictures put together in a jigsaw type puzzle. Then the color is tweaked."
MachineLearning,3az4qj,distortednet,6,Wed Jun 24 19:12:48 2015 UTC,The source code will be release in a few days :)
MachineLearning,3az4qj,EvilNLPGuy,1 point,Wed Jun 24 19:27:59 2015 UTC,"Ah nice, thanks!"
MachineLearning,3az4qj,XalosXandrez,3,Fri Jun 26 08:44:39 2015 UTC,"Love it. If you're optimizing the logit of the class, we should be able to visualize multiple classes, right? Also you should try optimizing the differences in logits of two classes which are similar, see how it distinguishes them! I.e. +cat -dog.  I really like the google images when they take natural images and then make the image activate low-level features more. Once yall release the coe"
MachineLearning,3az4qj,MysteriousArtifact,3,Wed Jun 24 22:54:22 2015 UTC,"We tried multiple logits simultaneously, but since it's already hard enough to recognize what it's dreaming about when there's only one thing, we decided against this for the stream :) It's definitely possible though. One thing the blogpost doesn't mention is that we actually have a very soft penalty on all logits to prevent them from ballooning. Optimizing the log-probability of a class would have the same effect, but then the supression of other classes was too strong."
MachineLearning,3az4qj,zepanzercommanzer,3,Wed Jun 24 23:00:32 2015 UTC,Looking forward to playing with it when the code is released!
MachineLearning,3az4qj,FatSoccerMan,2,Wed Jun 24 23:05:10 2015 UTC,"This is amazing. Alien, yet of this world. Wild, but familiar."
MachineLearning,3az4qj,LeihTexia,1 point,Wed Jun 24 22:13:08 2015 UTC,It seems to be zooming in or rotating a static image. Is it possible to animate steps in the optimization process or sample from the net's energy distribution the way Hinton used to show with DBNs and MNIST?
MachineLearning,3az4qj,EmoryM,7,Wed Jun 24 19:31:21 2015 UTC,"The zooming / rotation is just to keep it interesting, so it doesn't keep refining the same static image. It also aids the trippiness ;) Originally we did zooming only, but that sometimes created some artifacts, which were then magnified by the convnet due to the feedback loop. Adding a slight rotation fixed that issue.  Currently it's taking about 4-5 seconds to create a single frame (10 gradient steps). The animation is due to interpolation between successive frames. Visualizing the gradient steps would create a different kind of effect, that could also be interesting :)"
MachineLearning,3az4qj,EmoryM,2,Wed Jun 24 19:40:21 2015 UTC,"Transform it with the P-frames of some movie, this would go great with a data mosh."
MachineLearning,3az4qj,brandf,1 point,Thu Jun 25 04:22:54 2015 UTC,Save some video of this. This would be amazing as generic video on things like music.
MachineLearning,3az4qj,samim23,2,Thu Jun 25 09:39:51 2015 UTC,Someone made this: https://www.youtube.com/watch?v=FqvLc0GKN2s
MachineLearning,3az4qj,jrkirby,1 point,Thu Jun 25 09:52:57 2015 UTC,"i hope there are some ways to filter that, its only a matter of time before chat starts making it do lewd stuff haha"
MachineLearning,3az4qj,317070,1 point,Thu Jun 25 10:10:17 2015 UTC,"Luckily there's not too much in the way of lewd stuff in the ImageNet dataset :) They love to request 'nipple', but it's not what they think ;)"
MachineLearning,3b0r1g,LLCoolZ,1 point,Thu Jun 25 01:16:27 2015 UTC,Does anyone know if there's any code to try this for?
MachineLearning,3b0r1g,simonhughes22,0,Fri Jun 26 02:23:10 2015 UTC,"Sorry, we don't currently plan to publish the code."
MachineLearning,3b0r1g,richardsocher,3,Fri Jun 26 03:06:54 2015 UTC,"I know you're a company so I understand why, but I strongly feel that  any published paper in CS these days should also be accompanied by the code and the data the model was trained on if it's a ML paper, so that others can reproduce the work. This is particularly pertinent in light of the Baidu scandal."
MachineLearning,3b0r1g,simonhughes22,2,Sun Jun 28 02:50:42 2015 UTC,"FYI, there's a better result on the STB binary task in: http://papers.nips.cc/paper/5487-learning-with-pseudo-ensembles.pdf"
MachineLearning,3b3q6q,uncountableB,5,Thu Jun 25 18:44:39 2015 UTC,"Doubt they'd go to the trouble of compiling your LaTeX documents. If you use IPython notebooks, however, they'll auto-render on GitHub, like so."
MachineLearning,3b3q6q,dwf,1 point,Thu Jun 25 18:48:41 2015 UTC,"Ooh, cool! Yeah, I should have clarified. I could just compile them myself and put the source code and the pdf there."
MachineLearning,3b3q6q,dwf,1 point,Thu Jun 25 19:11:32 2015 UTC,"Wait, never mind. I don't even know if Github shows pdfs. (I'm fairly new-ish to it). I might go with that suggestion. Thanks!"
MachineLearning,3b3q6q,EdwardRaff,2,Thu Jun 25 19:12:49 2015 UTC,Note that the notebooks aren't just for Python. Lots of language backends to choose from.
MachineLearning,3b3q6q,paulochf,1 point,Thu Jun 25 19:44:46 2015 UTC,Thanks! So I'm guessing you think the whole theory thing is a good idea?
MachineLearning,3b3q6q,alexmlamb,4,Thu Jun 25 20:38:06 2015 UTC,"I help with the hiring process at my company, and having an example that demonstrates some more theory level understand would be a big boon compared to other resumes.   As /u/dwf mentions, I wouldn't go through the trouble of compiling your LaTeX."
MachineLearning,3b32m7,Professional_123,2,Thu Jun 25 15:55:33 2015 UTC,"Randomly generate a high-dimensional Gaussian mixture model using, e.g., Dirichlet and Normal-Inverse-Wishart priors."
MachineLearning,3b32m7,FixDeineKabel,1 point,Fri Jun 26 14:50:39 2015 UTC,I'm guessing these would be useful.  https://en.wikipedia.org/wiki/Test_functions_for_optimization
MachineLearning,3b32m7,Liz_Me,1 point,Thu Jun 25 16:30:20 2015 UTC,"hmmm.. do you know of any applications that one may encounter such target distributions? I mean, multimodel and high dimensions, not necessarily from the ones you linked."
MachineLearning,3b32m7,sieisteinmodel,1 point,Thu Jun 25 17:45:48 2015 UTC,MNIST is still a viable benchmark for this.
MachineLearning,3b32m7,sieisteinmodel,1 point,Thu Jun 25 18:43:36 2015 UTC,In the case of an NN?
MachineLearning,3b1rl6,HelmsmanRobertson,7,Thu Jun 25 07:18:34 2015 UTC,"Try Keras, it wraps Theano in a Torch-like API: http://keras.io/ Or Lasagne, which has a bit of a different API (and which is my own project): https://github.com/Lasagne/Lasagne"
MachineLearning,3b1rl6,benanne,1 point,Thu Jun 25 07:56:30 2015 UTC,"Wow! Keras looks pretty close to what I was thinking... How extensible is it? For instance, if I make a layer with some funky output, is it smart enough to use Theano's symbolic differentiation for backwards messages and parameter gradients?"
MachineLearning,3b1rl6,jfsantos,1 point,Thu Jun 25 08:24:39 2015 UTC,"Yes, it is, as long as you follow the API for a layer you should not have any problem."
MachineLearning,3b1rl6,siblbombs,1 point,Thu Jun 25 12:07:03 2015 UTC,"Keras is going to be pretty close to what you want, although I always recommend learning some pure Theano if you can."
MachineLearning,3b0huw,g4n0n,1 point,Wed Jun 24 23:57:43 2015 UTC,"at my uni we are working on this, too. ""Deep"" graphical models are shaping up to be one of the next hype things in ml."
MachineLearning,3b0huw,skgoa,1 point,Thu Jun 25 12:42:41 2015 UTC,"Note if you are one of the co-authors - Stanford is misspelled ""Standford"" in the author list."
MachineLearning,3b2m9f,davidun,2,Thu Jun 25 13:47:11 2015 UTC,"So I'm assuming your reference is with respect to Hidden Markov Models with Gaussian mixture densities, and that you probably have atleast given a first read to the Rabiner tutorial.   In the E-step, not only are likelihoods computed, but more importantly  first and second order sufficient statistics are accumulated. In the M-Step you will notice a set of summations in the numerator and denominator for the updates for each parameter.These are essentially where the accumulated first and second order stats are used.  You are right in understanding that the EM is an iterative procedure. Likelihoods in the E-Step for estimating models in iteration k of the EM are indeed calculated from the model estimated in step k-1.  Hope this helps."
MachineLearning,3b2m9f,speechMachine,1 point,Thu Jun 25 15:04:31 2015 UTC,"e right in understanding that the EM is an iterative procedure. Likelihoods in the E-Step for estimating models in iteration k of the EM are indeed calculated from the model estimated in step k-1. Hope this helps.   Hi, thanks. This does help, and yes- I am referring to HMM training. I'm trying to implement an EM with simulated annealing, and what confuses me is that for the ""annealing step"", the algorithm compares the likelihoods of the current parameters to the likelihood of some ""neighboring"" candidate parameters. However, if the likelihoods actually ""belong"" to the parameters in previous step (k-1), then I just don't get the logic behind using it to make a decision regarding the current step's (k) parameters ."
MachineLearning,3b2m9f,speechMachine,2,Thu Jun 25 17:00:59 2015 UTC,"I'm not familiar with an ""annealing"" approach to training. To me it appears to be something non-standard that a lay-person like me might not be familiar with. Is there a paper you could point to?  I don't know what you mean by 'current' parameters. Is this comparison done after the M-Step for step k of the EM? Or is this comparison done with 'neighbouring models' in the E-Step? When were these neighbouring models computed? Are these neighbouring models updated every M-Step?"
MachineLearning,3b2m9f,kjearns,1 point,Thu Jun 25 21:17:02 2015 UTC,"well, the basic idea is that you take the E-step parameters, and perturb them (say, by adding a small noise). This gets you ""neighboring parameters"", which the algorithm now decides if it prefers over the ""current"" (not-perturbed) parameters. The decision is made as follows: if the likelihood of the perturbed parameters is better than the non-perturbed, choose the perturbed. if not- the algorithm can still chose the perturbed parameters in probability = exp(dL/T). where dL = the difference in likelihoods, T=the ""temperature"" (a noise parameter). The question of in what point exactly should the likelihood be computed is what I'm trying to understand.."
MachineLearning,3b2kf8,thefunkyoctopus,2,Thu Jun 25 13:30:55 2015 UTC,"Each parameter (weight, bias or other) gets its own velocity value."
MachineLearning,3b3xnu,sixerspl,1 point,Thu Jun 25 19:39:43 2015 UTC,No offence but this article says nothing about Machine Learning and it would be better posted somewhere like /r/datascience
MachineLearning,3b3xnu,Deterministic-Chaos,1 point,Fri Jun 26 09:47:52 2015 UTC,How can I move it? Just post a new one and delete this one?
MachineLearning,3azxra,neuromorphics,1 point,Wed Jun 24 21:18:56 2015 UTC,"Thanks for this -- as if I didn't have enough to read!  BTW, I don't think I've ever seen scientific papers authored by ""anonymous""."
MachineLearning,3azxra,TheDaler,4,Wed Jun 24 21:30:08 2015 UTC,These are term papers. A student who feels their work wasn't that great may not want their name publicly associated to it forever (and once it's out on the web it's forever).
MachineLearning,3azxra,fooazma,2,Thu Jun 25 02:22:56 2015 UTC,"Yeah, one of the anonymous students admitted that his algorithm had a bug and that he couldn't get results.  It's happened to me before and he deserves some credit for his openness."
MachineLearning,3azxra,alexmlamb,1 point,Thu Jun 25 08:21:47 2015 UTC,Particularly liked Making a Manageable Email Experience with Deep Learning by Eugene and Caswell! This class looks amazing!
MachineLearning,3azxra,iamzaf,1 point,Thu Jun 25 00:09:09 2015 UTC,"I like this one, has its own website and github. website"
MachineLearning,3azxra,muktabh,1 point,Thu Jun 25 07:21:32 2015 UTC,where can i find solutions of all Pset of cs224d?  Thanks
MachineLearning,3b3rba,no_porner,2,Thu Jun 25 18:53:24 2015 UTC,You mean as in usable software libraries?  OpenCV?
MachineLearning,3b3rba,EraYaN,1 point,Thu Jun 25 19:41:25 2015 UTC,"Yes, something in  OpenCV. Although SURF is already implemented in it."
MachineLearning,3b3rba,NMEMine,2,Thu Jun 25 20:11:58 2015 UTC,A while ago I did a computer vision project for uni which was supposed to be somehow assistive to visually impaired people. My project partner and I decided to do a playing card recognizer. We used SIFT features but I'm sure this can be done with SURF as well. It should also be easy to compare the results of the two approaches.
MachineLearning,3axxlk,Barbas,7,Wed Jun 24 11:44:56 2015 UTC,Link to paper
MachineLearning,3axxlk,dive118,7,Wed Jun 24 11:45:15 2015 UTC,"Could someone explain what is going on here? Are they ""just"" taking a series of input images and outputting a seemingly continuous video from them?"
MachineLearning,3axxlk,omniron,7,Wed Jun 24 13:53:50 2015 UTC,"Assuming I'm reading it right, that trained a network to create the stereo image pair of an image, then use traditional techniques to create the stereo pair to construct a 3d scene which they then interpolate motion over.  Pretty clever approach, requires less time and resources than having the network do full 3d rectification."
MachineLearning,3axxlk,MrTwiggy,3,Wed Jun 24 14:19:39 2015 UTC,"I'd be curious to see what sort of application this have in 3Difying movies or television shows that weren't shot stereoscopically. At the moment, it doesn't look like the quality is sufficient enough, but is it possible they could reach a high enough quality that a viewer wouldn't notice?"
MachineLearning,3axxlk,shaggorama,3,Wed Jun 24 17:47:07 2015 UTC,So... this is frame imputation?
MachineLearning,3axxlk,thatguydr,5,Wed Jun 24 13:52:30 2015 UTC,"It's harder than that, because the changes in perspective can be large. Their process needs to ""auto-rotate"" imagery, calculating the entire depth map for each image in an automated fashion. They do so by training the network on the street view imagery, which has a huge amount of ""plane-swept"" data (see the paper).  It's not the best registration I've ever seen, and it's not the most advanced registration (you can make algorithms that perform as well as they have currently). It's a really big step, though, and with a little bit of work (making the contribution of the depth map more formal and dealing with the non-linearities you get with each camera, which can be extremely large), I think they'll hit it out of the park."
MachineLearning,3axxlk,thatguydr,2,Wed Jun 24 14:16:24 2015 UTC,"You didn't read the article at all, did you?"
MachineLearning,3axxlk,mimighost,1 point,Wed Jun 24 17:26:17 2015 UTC,"Really cool, almost like watching a movie..."
MachineLearning,3axxlk,cybrbeast,-1,Wed Jun 24 18:44:36 2015 UTC,"Weird, my submission of the article that links to this video in was completely overlooked, though you guys didn't like it. Maybe it's just a question of video and title."
MachineLearning,3ayowh,rantonels,3,Wed Jun 24 15:51:21 2015 UTC,Can you describe your process for creating this NN?
MachineLearning,3ayowh,DatenWissenschaftler,2,Wed Jun 24 23:41:53 2015 UTC,"I just programmed a fast (for python) implementation of the game. MultiNEAT was used for the nn classes and evolution algorithm (no point in reinventing the wheel). Inputs and output neurons were setup as in the video description. Fitness is computed as the minimum of the performance (distance traveled at death) over a certain number of sample games on random levels. The NEAT algorithm allows evolving the needed architecture so I did not need to specify hidden layers/neurons.  I ran the evolution loop on a 300 individual population, with 3 samples for the evaluation function. When the NNs win, I switch to 6 for extra precision, and continue evolving. Again, when they win, I switch to 10. This I think should perfect the network so that it loses less probably.  For visualization I used matplotlib and pygame because I did not want to spend more than 10 minutes on it."
MachineLearning,3ayowh,tabacof,1 point,Thu Jun 25 07:17:46 2015 UTC,"How many generations did it take to reach the level shown in the video? Also, what do you mean by the NN winning?"
MachineLearning,3ayowh,tfimg24,1 point,Thu Jun 25 13:47:15 2015 UTC,"Surprisingly, very little. It writhes around a shameful score of 2-3 for a very random number of gens (between 10 and 200) and then discovers how to get through the holes. As soon as it does, its score rises up to 100 (my condition for ""winning"") in just a couple of generations."
MachineLearning,3b0270,billconan,5,Wed Jun 24 21:53:32 2015 UTC,Possibly for regularization to prevent over-fitting?
MachineLearning,3b0270,yggdrasilly,2,Wed Jun 24 23:16:27 2015 UTC,"A little off the topic but, is Stanford's NLP class available now?  Where?  The only one I know about it on coursera and its not being offered right now.  Thanks!"
MachineLearning,3b0270,TheDaler,1 point,Wed Jun 24 22:13:19 2015 UTC,"You don't want your model to only see how a word appears in a context window of only N words.  Consider this example:   This relative newcomer has quickly taken the lead in the race.   If you always use a context of length 5, and consider taken as the centre word:   has quickly taken the lead   You'll never see the example of newcomer on the surrounding window of taken. A randomly sized window gives you the chance to estimate better parameters."
MachineLearning,3ayqy9,krimtheguy,5,Wed Jun 24 16:05:42 2015 UTC,You can try Keras it has a nice example of character generation with RNN or LSTM. https://github.com/fchollet/keras
MachineLearning,3ayqy9,CptZouglou,2,Wed Jun 24 16:13:43 2015 UTC,you could modify the lstm example written for theano: http://deeplearning.net/tutorial/lstm.html  You need to modify to resemble an encoder step and then do the decoder step by sampling from the last hidden layer  Also if you take a look at Karpathy's Neural Talk code: https://github.com/karpathy/neuraltalk/tree/master/imagernn  you can find python code for a lstm/rnn which takes a set of features and then learns image-words mapping based on data.
MachineLearning,3ayqy9,dexter89_kp,2,Thu Jun 25 04:39:48 2015 UTC,"Hmmm.... I wonder if it's possible to run lua from within python?  I reckon it ought to be possible somehow in fact.  That would meet your requirements right?  If you could pass data from python into the lua, and run the lstm script?"
MachineLearning,3ayqy9,hughperkins,1 point,Thu Jun 25 11:04:36 2015 UTC,"thought about that already, the thing is that I wanted to do it purely in python. Keras might be useful for my needs tho so will give it a try, otherwise I will try to do that."
MachineLearning,3azryw,rccr90,6,Wed Jun 24 20:37:01 2015 UTC,"If you work with (Deep) neural networks, a GPU is an absolute must-have nowadays. It speeds up your calculation by a factor of 10-100 (yes, I am by no means exaggerating). Thus, you will need a desktop machine (laptop GPUs are far below the power of desktop GPUs). Having a desktop is nice anyway, because you can just set it up somewhere, connect to it remotely to start your computations and then control it remotely all day long, while still using your Macbook as your main development/personal machine and be mobile. Plus you'll get much more power at the same price when buying a destkop, which is the point of this anyhow.  I'd put together my own PC rather then buy a pre-build model. Most companies have nice workstation-offers, but the prices are exorbitant, so I'd recommend against it unless you need the support contract that comes along with a professional workstation.  Note that almost all ML-GPU software available today is written for CUDA, so you need to buy an nvidia card. Since you're willing to spend 4k, you could probably splurge on an nvidia titan x, which costs ~1k and is a semi-professional card. A GTX 980/GTX 980Ti isn't much slower and offers much of the same features, but has only half the RAM (but at a far more reasonable price). Personally I'd go for a TitanX since you will eventually always run out of RAM and you seem to have a large enough budget.  Depending on what you do with your nets and how far you're willing to go, the other thing to consider is that you might want to upgrade to more GPUs later on. Make sure to get a mainboard and a case that have enough room for those upgrades. Ideally you can fit up to 4 GPUs in your machine. Chances are you're not going to need it for a long time, but maybe it's nice to have the option. Plus having a nice case allows for better airflow (GPUs tend to get pretty hot).  With your budget you could go for an ""E"" class CPU, which has more cores than normal Intel CPUs (and support for more PCIe lanes, which translates into more GPU bandwidth). Which is nice for running things on parallel (e.g. cross validation or grid search, or maybe just to speed up a single algorithm like a random forest). The current flagship is the i7-5960X, which has 8 cores (16 with hyperthreading) and can address 64 GB RAM.  Whatever is left of your budget you can pour into buying as much memory as you can."
MachineLearning,3azryw,BeatLeJuce,1 point,Wed Jun 24 22:28:29 2015 UTC,"So yea, I was thinking about utilizing a similar method to yours, writing my stuff on my little laptop and executing my programs on a desktop (I can't sit still very long and like working all over the place).   As far as the GPU I did some research (after seeing the suggestions) and it seems like Nvidia is indeed the best way to go for what I want. From previous posts on this sub it seems like 95% of machine learning people choose Nvidia.   I have my eye on the Titan X..."
MachineLearning,3azryw,BeatLeJuce,2,Thu Jun 25 13:34:03 2015 UTC,"it seems like 95% of machine learning people choose Nvidia.    Make that 99.999%.  NVidia puts a lot of money into making sure that academic researchers write their GPU code in CUDA, which can only execute on nvidia gpus. Thus most code out there uses CUDA.  Disclaimer: I'm mainly speaking about deep learning research, YMMV in other ML areas, but it would surprise me."
MachineLearning,3azryw,quirm,1 point,Thu Jun 25 13:41:51 2015 UTC,"I would go with a workstation mainboard. Your budget allows you to buy one of the recent Intel Xeons and the main advantage is that a single processor can address much more RAM and you can use ECC RAM (error correction / fault tolerance). Also DDR4 ECC isn't that expensive, i.e. it is on par with its non-ecc counter-parts. Depending on the the memory type you can have 128GB-512GB RAM with 8 banks."
MachineLearning,3azryw,Rickasaurus,2,Thu Jun 25 23:01:21 2015 UTC,"I have a Lenovo w540 with a top end mobile Core i7 (about 1/2 the speed of the top end desktop model) and 32GB of ram, and 2x 1TB Samsung 950 SSDs, was about $3K a year and a half ago. The trackpad is awful, but otherwise it's a great computer.   As far as I know Apple still doesn't offer any laptops with 32GB of ram."
MachineLearning,3azryw,Rickasaurus,1 point,Wed Jun 24 20:56:28 2015 UTC,Thanks for the input  Lenovo looks like a great option!  What OS are you using on it?
MachineLearning,3azryw,Rickasaurus,2,Wed Jun 24 21:02:31 2015 UTC,"Also, it has a Quadro 2100m GPU, fine for playing around with or testing CUDA but not really practical for sure."
MachineLearning,3azryw,ryptophan,1 point,Thu Jun 25 17:58:21 2015 UTC,Current company is a Microsoft shop and so windows. I'd Google around first if I were thinking Linux. Might want to make sure you get the Intel wifi card too.
MachineLearning,3azryw,ryptophan,2,Thu Jun 25 04:44:09 2015 UTC,"Are you using Theano/Python for your neural networks? I'd recommend grabbing a NVIDIA GPU, in that case. I have a GTX980, and it's great. I'd pick up a card with at least 3GB of RAM.  Other than that, I have a i7-4790K and 32GB ram... Wish I had purchased a larger SSD (120GB).  My rig came in at about $2000 CAD. If you want to spend more, I'm certain there's a better CPU and multi-GPU setup you could go for (if you think you could use it).  That's all assuming you want to get a desktop. If you are going for a beefier laptop, I'm not sure about that.."
MachineLearning,3azryw,Tulip-Stefan,1 point,Wed Jun 24 21:09:13 2015 UTC,"I've been using NumPy/R, I'm relatively new to this. I just looked at Theano though and its definitely a factor that will improve processing time greatly! (Thanks)  As for the laptop/desktop debate I'm not very sure. The prices are much lower for a desktop and processing is better. I really just  don't want to wait on calculations like I do with my current setup. I'm open to anything at this point (still have a few days to choose). I've used a MacBook Pro 2012 to run a similar (yet larger) calculation and it finished under 3 hours, something that I would love to be able to beat with my next setup.  So question: Is the GPU what will decide the crunch time on the data?"
MachineLearning,3azryw,XeonPhitanium,1 point,Wed Jun 24 21:27:14 2015 UTC,Is the GPU what will decide the crunch time on the data?   If you're running neural networks using a library that has CUDA support -> yes. Take note that when you run on a GPU you're limited by the available GPU RAM... That's why I mentioned getting a GPU with a minimum of 3GB RAM. My GTX 980 has 4GB.  Other than that - and it's probably more important for you - you just want to get a nice multicore (4 or 6 core) CPU. Intel CPUs are my preference.
MachineLearning,3azryw,JustFinishedBSG,1 point,Wed Jun 24 22:08:25 2015 UTC,Thanks for the explanation!  Now people's suggestions make more sense.
MachineLearning,3azryw,Stevo15025,2,Thu Jun 25 13:27:47 2015 UTC,"Getting a desktop is generally a good idea. I'm currently doing machine learning on my laptop, but if i needed something beefier I'd definitely keep my current laptop and add a desktop for larger models. I drag my laptop with my everywhere, and obviously it's not possible to run hour long simulations on battery power.  Getting a good GPU in a chassis that i would call mobile enough to move is usually not really possible, the exception that I'm aware of is the lenovo W series. I have been not very impressed with other manufacturers as their products are a lot heavier with the same class of GPU. Keep in mind that even if you can get an top of the line W540 for $1.5k, a desktop for the same price is going to net at least 4 times the GPU power. In terms of CPU power, the difference is much smaller. An other poster mentioned the i7-4790K, this thing is 'only' ~25-50% faster than a high-end notebook CPU from the same generation. Is that worth the loss of mobility to you?  Surely it is possible to get a faster CPU than the i7-4790K or equivalent i7-5xxx series, but the performance per dollar drops quite sharply above this price segment."
MachineLearning,3azryw,XeonPhitanium,1 point,Wed Jun 24 21:53:23 2015 UTC,"The only thing that I'm trying to get out of this for sure is faster computing, the rest is secondary.  After reading all the responses and similar posts, I'm set on just building a desktop with little consideration for mobility.. The GPU will definitely be the primary component.   As far as laptops go I was researching and landed on the same the Lenovo W series, they seem pretty nice.!"
MachineLearning,3azryw,Stevo15025,2,Thu Jun 25 13:52:07 2015 UTC,"My current dev box, total BOM ~$4,235:  Components  Asus X99-E WS Motherboard $500  Intel Core i7 5930K CPU $550  32 GB RAM DDR4 RAM $500  Corsair Carbide Series Air 540 Case $130  Kingston Digital 480 GB SSD $200  Cooler Master Hype 212 Fan $35  GTX TitanX times 2 $2000  EVGA SuperNova 1600WG2 $320  You might otherwise recognize this as a DIY Digits Dev Box with two GTX TitanX GPUs with room for two more when you're ready.  For my code that has hand-optimized CUDA kernels, running on a GTX 680 takes ~122 seconds per epoch.  Running on a single TitanX takes ~26 seconds.  I cannot sing the praises of TitanX any louder but of course YMMV."
MachineLearning,3azryw,joeyglasgow,1 point,Thu Jun 25 00:15:28 2015 UTC,"total BOM ~$4,235  Cooler Master Hype 212   Come on...."
MachineLearning,3avq4i,ojaved,5,Tue Jun 23 21:54:54 2015 UTC,"It suggests tweaks for getting better performance (Suggestions for initialization, optimization etc.)"
MachineLearning,3avq4i,AlcaDotS,3,Tue Jun 23 22:12:48 2015 UTC,"Yes, nice overview of tricks for performance boosts for RNNs."
MachineLearning,3avq4i,Molag_Balls,1 point,Wed Jun 24 11:06:32 2015 UTC,I watched Indico's other video on getting started with NNs in Theano and I was really pleased with both of these videos. He speaks quickly but the information is solid. Great launchpad for learning more.  Now I just wish they'd make a video on ESNs / Reservoir Computing.
MachineLearning,3axqix,ravo87,2,Wed Jun 24 10:08:10 2015 UTC,"machine comprehension, building deep semantic representations of text, question answering: for example this work and stuff referenced therein: http://arxiv.org/abs/1506.03340  deep representations of text can also be used for information retrieval: http://dl.acm.org/citation.cfm?doid=2661829.2661935 conversation modelling: see recent work: http://arxiv.org/abs/1506.06714, http://arxiv.org/pdf/1506.05869v1.pdf"
MachineLearning,3axqix,fhuszar,1 point,Wed Jun 24 12:08:21 2015 UTC,"Please post links to the abstract, not the PDF.  Thanks."
MachineLearning,3axqix,ndronen,1 point,Wed Jun 24 18:18:05 2015 UTC,"Is there a list of ""best paper"" awards for all the major conferences somewhere?"
MachineLearning,3axqix,cryptocerous,1 point,Wed Jun 24 12:52:30 2015 UTC,http://www.aclweb.org/aclwiki/index.php?title=Best_paper_awards  Not updated however :(
MachineLearning,3axqix,yodaman92,1 point,Wed Jun 24 18:34:35 2015 UTC,"Nearly all NLP involves ML, so it depends on what aspect of NLP you find interesting.  Personally I'm fascinated by vector space representations of text. I enjoy thinking about the underlying philosophical implications that such representation has on language."
MachineLearning,3auzyh,ojaved,3,Tue Jun 23 18:51:35 2015 UTC,Score! I didn't get a chance to take his course due to scheduling conflicts. Now I get a second chance!
MachineLearning,3auzyh,hammerheadquark,1 point,Tue Jun 23 23:25:33 2015 UTC,Where is Lecture 1?
MachineLearning,3auzyh,alek9,4,Wed Jun 24 16:18:50 2015 UTC,"Hi, this is Dhruv.   Lecture 1 was recorded by VT recording and is available here: http://www.dms.tlos.vt.edu/content/adhoc/q10041/  Apologies for the sound quality (at some point the mic battery died). Lecture 1 is mostly organizational and motivational.   You'll find pointers to relevant books, chapters, readings, etc on the class webpage here: https://filebox.ece.vt.edu/~s15ece5984/  As a note to anyone interested -- I am planning to teach ""Deep Learning for Perception"" in Fall 2015.   Similar to the Intro to ML class, I will post the video lectures online. Please feel free to follow along."
MachineLearning,3auzyh,dhruvbatra,1 point,Wed Jun 24 18:05:00 2015 UTC,Sounds great - is there a page or a list to watch for updates?
MachineLearning,3auzyh,xamdam,1 point,Thu Jun 25 15:18:44 2015 UTC,"Lecture 1 is just on class administration ( the number of exams, homeworks, grading policy). Machine Learning is not discussed in the lecture."
MachineLearning,3auzyh,fatboy93,5,Wed Jun 24 18:18:27 2015 UTC,"While what you say is right, but the whole purpose of listening to lectures and attending them is that, you understand something through someone's perspective, who is more educated than you're which makes it more easier to understand and often times they put it in a simpler manner."
MachineLearning,3auzyh,Articulated-rage,3,Wed Jun 24 04:26:02 2015 UTC,"Until the illustrated primer is a thing,  customized reactions to confusions and questions trumps online tutorials."
MachineLearning,3auzyh,Foxtr0t,1 point,Wed Jun 24 06:58:47 2015 UTC,"That's like saying ""courses still amuse me in a world with 24x7x365 crap"""
MachineLearning,3ayvlh,tfimg24,1 point,Wed Jun 24 16:38:20 2015 UTC,"funny that he writes this stuff while plugging an overpriced cloud ""solution""."
MachineLearning,3ayvlh,sdsfs23fs,0,Wed Jun 24 16:57:12 2015 UTC,"Hi sdsfs23fs, I wrote the article. I'm critical myself, and call out plenty of hypocrisy so I'm definitely open to having such a criticism directed at me.  Here's my thinking and maybe you can tell me where you disagree:   I have this idea for a post about a real problem of people using supercomputer-scale solutions for small problems. I write about it and get it published on Terminal.com.   Terminal provides convenient solutions, which I don't really have a problem with. I pay a premium for apple computers over PCs with comparable hardware because they tend to be more stable and I'm more productive on them. I don't think this is fundamentally wrong.   When I describe the use of GPUs instead of Hadoop, I mention that you can get a GPU instance for less than $1 per hour on Terminal. Now I could have pointed that you could get a similar instance on Amazon EC2 for $.50 per hour but:  1) This difference doesn't really affect the analysis  2) Terminal provides a convenient value added of being able to write the code when the instance costs less than 1 penny per hour, then instantly turn the instance into a GPU instance without restarting, and then turn it back as soon as its done running (also without restarting). This doesn't strike me as overpriced or hypocritical. But I'm open to further critique."
MachineLearning,3ayvlh,zackchase,1 point,Fri Jun 26 07:51:45 2015 UTC,"you can get AWS spot instance for 0.05 USD per hour, which does significantly affect the analysis."
MachineLearning,3ayvlh,sdsfs23fs,0,Sat Jun 27 19:51:47 2015 UTC,"For GPU? Unless I'm gravely mistaken, the price for a GPU spot instance is 50 cents per hour. I don't think you read the context clearly."
MachineLearning,3ayvlh,zackchase,1 point,Sat Jun 27 20:03:44 2015 UTC,so apparently you decided to write an article about a topic you don't know the first thing about...  http://ec2price.com/  g2.2xlarge typically fluctuates between about 0.05 and 0.1.
MachineLearning,3ayvlh,sdsfs23fs,0,Sat Jun 27 21:58:56 2015 UTC,1) It's not the topic of the article. Please read and think before commenting.  2) Spot prices are not apples to apples. The comparison here should be on-demand prices. I misspoke before.  3) [insert snarky reddit comment of comparable absurdity to yours here]
MachineLearning,3ayvlh,zackchase,1 point,Mon Jun 29 09:53:28 2015 UTC,"so you misspoke, and somehow I'm the one who needs to think?  have you ever even used spot instances? They're perfectly suitable for most tasks, and a hell of a lot better than $1/hour bullshit.  fuck off, shill."
MachineLearning,3ayvlh,sdsfs23fs,0,Wed Jul 1 04:33:30 2015 UTC,This is so far afield of the point of the article that I'm surprised I'm responding. Good luck with your work and happy trolling.
MachineLearning,3ayvlh,zackchase,0,Wed Jul 8 18:05:56 2015 UTC,"your 5 minutes of fame was a week ago, no one gives a shit anymore bro."
MachineLearning,3ax2x6,True-Creek,2,Wed Jun 24 04:59:18 2015 UTC,"Could hear a few specific songs there, faint but completely recreated.  I'm still learning but from what I know so far it's likely over-fit in that case because it just doesn't have much data to work on.  Perhaps having a few more artists discography to train with might significantly improve that result"
MachineLearning,3ax2x6,dogedickguy,1 point,Wed Jun 24 10:55:29 2015 UTC,"I'm wondering whether they could get rid of the noise with certain statistical constraints, similar to the correlation constraints they've mentioned in the recent Inceptionism blog post on the Google's research blog."
MachineLearning,3ax2x6,Noncomment,2,Wed Jun 24 05:09:22 2015 UTC,"Well they shouldn't need to, since the RNN is supposed to model the statistics of the sequence on it's own. Unlike the Google convnet which was only trained to classify images, not generate them.  However, usually when samples are take from RNNs, they just draw from it's predictions. If the net is very uncertain what will happen next, this doesn't work very well. Even if it's only 95% sure what frequencies will occur next, that means 5% of the frequencies are going to be totally random.  If that's the case, a better way might be adversarial networks. Where one network is trained to distinguish generated data from real data, and another is trained to fool that network. The result should ideally be data which is as indistinguishable as possible from real data. I hear this is very hard to get right though, and I'm not sure if it works for sequence data/RNNs."
MachineLearning,3ax2x6,Noncomment,1 point,Thu Jun 25 08:42:50 2015 UTC,"Very interesting, thanks for the reply.   and another is trained to fool that network   What's the use of that network?"
MachineLearning,3ax2x6,vincentrevelations,1 point,Thu Jun 25 14:22:43 2015 UTC,"One network generates and the other discriminates. The discriminating network tries to tell the difference between what the generating network has produced, and true data. The generating network tries to fool the discriminating network. You can backpropagate through the discriminating network and into the generating one.  See the paper: http://arxiv.org/abs/1406.2661"
MachineLearning,3ax2x6,utopiah,1 point,Thu Jun 25 15:31:10 2015 UTC,"Probably. Noise is easy to spot on the frequency spectrum. I'm thinking they could even take it a step further, and train the NN to recognize noise as a feature which it has to minimize."
MachineLearning,3ax2x6,phreeza,1 point,Wed Jun 24 22:25:30 2015 UTC,No draft paper or notebook or anything?
MachineLearning,3ax2x6,horsey_jumpy,2,Wed Jun 24 15:19:14 2015 UTC,Here you go: http://cs224d.stanford.edu/reports/NayebiAran.pdf
MachineLearning,3aybmh,vvlkq,2,Wed Jun 24 14:06:32 2015 UTC,"CRF don't have transition probabilities, they have unnormalized factor scores between two adjacent labels. If trained well it should effectively learn a negative transition score between labels unseen in the training set. This doesn't mean that it will be impossible for the best label sequence to have that transition. For instance, the local scores can be large enough for that label to mask the low transition scores.  There are ways to get around this limitation. The easiest is to just manually set the transition weights such that the factor scores act almost as a hard-constraint."
MachineLearning,3axozh,AfraidOfToasters,7,Wed Jun 24 09:45:09 2015 UTC,"OpenGL and CUDA are not alternatives - I think you mean OpenCL :) Anyway, if you want to train convnets, go with NVIDIA. There is simply no alternative right now. That may change at some point in the near feature though. I believe there are some OpenCL-based implementations already, but they are still much slower than their CUDA counterparts (I would be very happy to be proven wrong about this by the way!)."
MachineLearning,3axozh,benanne,2,Wed Jun 24 10:17:34 2015 UTC,"I'm currently porting Torch's cuda nn to OpenCL, https://github.com/hughperkins/clnn You can see on this page that I ran the Soumith benchmarking layers (ie AlexNet) for both the cuda and OpenCL version, and that the opencl version runs about twice as slow as the cuda version, on the same hardware.  Sooo... if you can get a non-cuda card that is about 2-5 times as fast as a cuda card, for the same price, then the non-cuda card is perhaps a good deal.  If the specs are the same, at the same price point, then the cuda card might be a better bet."
MachineLearning,3axozh,hughperkins,1 point,Wed Jun 24 14:47:12 2015 UTC,test/test-mnist2.lua on a GTX 970:   CUDA: 1.6s CL: 10.8s   Are you developing with a 940M or do you also have something more suitable?
MachineLearning,3axozh,NasenSpray,3,Wed Jun 24 20:03:29 2015 UTC,"I hear that the NVidia OpenCL implementation is purposely bad. Not sure if this is true, but AMD is on OpenCL 2.0 while NVidia is on 1.1 (maybe 1.2 soon). So a comparison between OpenCL and CUDA would have to be cross-hardware.  I use OpenCL on a R290, I am very happy with it, it is very fast (anecdotal evidence). I really hope there is some more migration to OpenCL soon, I like the interface way more than that of CUDA (I tried both). Also meta-programming is nicer in OpenCL."
MachineLearning,3axozh,CireNeikual,1 point,Wed Jun 24 23:37:06 2015 UTC,"I hear that the NVidia OpenCL implementation is purposely bad. Not sure if this is true, but AMD is on OpenCL 2.0 while NVidia is on 1.1 (maybe 1.2 soon).   It seems they got better during the last year or so (at least on Maxwell 2) and support OpenCL 1.2 since 350.05. Devs repeatedly complained on Nvidia's forum that their GPUs suck on many OpenCL benchmarks and that they were unable to achieve the same performance as CUDA. I think this may have convinced them to improve. Just compare this hilariously bad result with this.   So a comparison between OpenCL and CUDA would have to be cross-hardware.   Same hardware comparisons are useful to show the maturity of the implementation. In my case, the fact that OpenCL takes almost 7x longer than CUDA and that it scales worse compared to the benchmarks on GitHub tells me that cltorch/clnn still has much work to do."
MachineLearning,3axozh,NasenSpray,1 point,Fri Jun 26 12:45:18 2015 UTC,"If you continue porting that over to OpenCL i would love you forever, as i'd love to use Karpath's char-rnn on my r9 290, but I only have AMD cards. Vendor lockout is real."
MachineLearning,3axozh,devDorito,1 point,Wed Jun 24 20:43:33 2015 UTC,"Hmmm, good point.  You mean this right? https://github.com/karpathy/char-rnn/blob/master/model/LSTM.lua  Will take a look."
MachineLearning,3axozh,hughperkins,1 point,Wed Jun 24 22:47:56 2015 UTC,"Here you go: https://github.com/hughperkins/char-rnn  Simply add option '-opencl 1', and it should magically use opencl :-)  (You'll need to install https://github.com/hughperkins/cltorch and https://github.com/hughperkins/clnn first of course)"
MachineLearning,3axozh,hughperkins,1 point,Thu Jun 25 16:00:55 2015 UTC,That is fantastic!
MachineLearning,3axozh,devDorito,1 point,Thu Jun 25 16:01:55 2015 UTC,"Thanks.  it was a bit slow at first (2seconds, compared to 0.15 seconds on nvidia, per batch, on nvidia hardware).  But if you grab the latest version of cltorch, just published, it should be much faster now.  I get 0.52s for opencl, and 0.15s for nvidia, on nvidia hardware, which is near my target (target is about twice as slow, on nvidia, I suppose)."
MachineLearning,3axozh,hughperkins,1 point,Sat Jun 27 09:56:56 2015 UTC,"Thanks for the response! After a little bit of looking around, I think you are right about CUDA. The card most comfortable in my price range would be the gtx 970 do you think the 3.5g vram would be enough? Or should I wait for something more substantial?"
MachineLearning,3axozh,benanne,2,Wed Jun 24 10:29:06 2015 UTC,"Depends what you want to do with it. 3-4GB is usually enough for me, but if you want to train an architecture like GoogLeNet, it might not suffice. More is better, but it gets expensive quickly. The 980 Ti with its 8GB is probably a good deal, but iirc its price is about twice your budget :) looks like I'm wrong, it only has 6GB."
MachineLearning,3axozh,benanne,1 point,Wed Jun 24 10:32:10 2015 UTC,Would SLI work if I ended up needing more?
MachineLearning,3axozh,siblbombs,2,Wed Jun 24 11:59:57 2015 UTC,"SLI is not useful. You could use Multiple GPUs, if you use a library that supports it (coupling them with SLI is not useful for this kind of workload though). However, many commonly used libraries don't support this very well yet, although they are all working on it."
MachineLearning,3axozh,NasenSpray,1 point,Wed Jun 24 12:08:16 2015 UTC,"Thanks again. I guess ill go for something beefy now and, if i need to, get a friend for him later."
MachineLearning,3axozh,hughperkins,2,Wed Jun 24 12:13:55 2015 UTC,"I used a 970 for a while, and it worked fine. At ~4gb you have to make some choices around how you use the memory, usually for me this meant less preloading of training data on the card."
MachineLearning,3axozh,NasenSpray,2,Wed Jun 24 13:17:37 2015 UTC,"If you want to learn OpenCL: Go with the R9 390X. It can run circles around a Titan X/980Ti is a respectable GPU, but implementing all the algorithms is going to be frustratingly hard. Ideally, you should already be able to write high-performance code for CPUs and have a good understanding of multi-threading, caches, memory access patterns et cetera.  If you want to do ML: Get a Nvidia card. Almost everything uses CUDA.    Edit: I need to apologize, I mixed up R9 390X (5.9 TFlops) and R9 Fury X (8.6 TFlops)."
MachineLearning,3axozh,harharveryfunny,2,Wed Jun 24 13:18:20 2015 UTC,"Hmmm, seems you are right: R9 390X has about the same performance as the Titan, but 5 times cheaper: http://gpuboss.com/gpus/Radeon-R9-390X-vs-GeForce-GTX-TITAN"
MachineLearning,3axozh,NasenSpray,1 point,Wed Jun 24 15:15:16 2015 UTC,"FYI, please don't rely on gpuboss only. They only do raw number comparisons that rarely reflect reality."
MachineLearning,3axozh,Nixonite,2,Wed Jun 24 18:56:23 2015 UTC,"According to this seemingly well-informed source, the performance bottleneck for GPU accelerated machine learning is actually memory bandwidth rather than FLOPS, which would put AMD's top of the line range at a significant advantage to Nvidia.  https://timdettmers.wordpress.com/2014/08/14/which-gpu-for-deep-learning/  e.g. Nvidia GTX 980 does 224 GB/sec, and Titan X 336 GB/sec while AMD's just released R9 Fury X does 512 GB/sec and they've previewed a dual-GPU Fury card that does 1024 GB/sec !!  http://wccftech.com/amd-radeon-r9-fury-launch-reviews-roundup/"
MachineLearning,3axozh,watersign,1 point,Wed Jun 24 19:50:51 2015 UTC,It still needs to be seen how Fury X' memory bandwidth translates into real world performance. Current results are pretty mixed.
MachineLearning,3awqme,winstonl,5,Wed Jun 24 03:00:49 2015 UTC,Trees!
MachineLearning,3awqme,jtkme,4,Wed Jun 24 03:15:57 2015 UTC,"What matters is not so much the algorithmic choice here, but more how you deal with the missing values. Would you like to insert some average values in there or leave them missing and let the classifier implicitly deal with them. You may end up with less control in the latter, but that may work better than the former that presumes a model to generate the missing values."
MachineLearning,3awqme,slaxmansci,1 point,Wed Jun 24 03:33:12 2015 UTC,"That's exactly what I am asking here.  It is possible to fill in the missing values with averages, it's also possible to even throw all entries with missing values out (due to the nature of the problem - we are not looking into making a prediction for every company, just pick out the better ones). I guess what I should ask is if there are methods that can handle missing values (i.e. the classifier implicitly deals with them). I think trees is such a type, although I am not terribly sure."
MachineLearning,3awqme,slaxmansci,1 point,Wed Jun 24 12:59:35 2015 UTC,"Discarding cases with missing values is not an option if your test cases can end-up with missing values in them Imputation (filling with averages, etc) is by-far the most common way to handle missing data If the missing values are localized to only certain attributes (and/or their combinations), you could build separate classifiers for each combination of missing attributes.  When predicting, use the appropriate classifier, based on the attributes which are missing (if any) in the test case at hand If features are all binary, and if 1's are very rare, you can use 0's implicitly in models like LR, SVMs, etc. (implicit imputation based on priors) If your goal is to predict missing ""ratings"" (or if your problem can be cast as such), you can use matrix factorization techniques that construct their objective only over ratings that are known/available"
MachineLearning,3awqme,USER_PVT_DONT_READ,2,Wed Jun 24 16:02:16 2015 UTC,"It depends on the assumptions you make on the underlying missingness mechanism: Missing Completely At Random (MCAR), Missing At Random (MAR) or Informatively Missing (IM)  https://en.wikipedia.org/wiki/Missing_data#Types_of_missing_data"
MachineLearning,3awqme,ciolaamotore,1 point,Wed Jun 24 12:24:02 2015 UTC,Could you elaborate a little bit more on the task and domain (if there is a specific task and domain)? For instance it would be interesting to know whether those values are missing at random.
MachineLearning,3awqme,AlcaDotS,1 point,Wed Jun 24 07:14:36 2015 UTC,"You could look into collaborative filtering techniques. Those usually deal with sparse datasets.   A few days ago I learned about matrix factorization for collaborative filtering, which seems like a nice place to start. This article describes how it was used for the 'Netflix prize' competition. https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf"
MachineLearning,3awqme,jostmey,1 point,Wed Jun 24 11:14:05 2015 UTC,I suggest using a generative model. It will allow you stitch together the incomplete rows of data into a comprehensive model.
MachineLearning,3awqme,svantana,1 point,Wed Jun 24 19:28:13 2015 UTC,"Probabilistic methods are good at dealing with uncertainty and missing information - e.g. a GMM is easily modified to use gaussians as input rather than numbers. Then you can use whatever prior you like for missing data points (even the ""flat"" limit where the variance goes to infinity). For discrete variables (the industry one is such I would assume), you can use of 1-of-N encoding if the number of alternatives is not too large."
MachineLearning,3axir9,methodds,2,Wed Jun 24 08:14:02 2015 UTC,One thing that may help is generating new fakers from mixtures of existing ones to improve your training set. It's hard to say more without knowing about the nature and interaction of the features. Some variation on random forests might work well.
MachineLearning,3axir9,Rickasaurus,1 point,Wed Jun 24 08:28:39 2015 UTC,Try clustering. Identify a cluster which got these 5 fakers. Maybe you have more cases than 5.
MachineLearning,3axir9,Alena_hk,0,Thu Jun 25 18:09:52 2015 UTC,"how big is this data set??   SMOTE isn't generally a good idea, IMO.   And what models are you using ..?"
MachineLearning,3axir9,watersign,1 point,Wed Jun 24 14:31:27 2015 UTC,"the combined dataset for the first 2 waves is not too big, just about 700 cases. I tried RandomForests, Logistic Regression and SVM machines so far and ended up with relatively similar results. Honestly I have no other idea how to work with the data despite using synthetic sampling as there are only five fakers in the first waves :/"
MachineLearning,3axir9,watersign,0,Wed Jun 24 17:42:47 2015 UTC,so you only have 5 data points that are tagged as fraudulent?
MachineLearning,3axir9,watersign,1 point,Wed Jun 24 22:03:43 2015 UTC,"yes, this is the main problem. I don't see any other way than over/-undersampling here."
MachineLearning,3askpq,vaginitischlamydia,4,Tue Jun 23 04:15:41 2015 UTC,"Went in thinking that this would be all about his research, was pleasantly surprised that this was a much broader piece."
MachineLearning,3askpq,timClicks,6,Tue Jun 23 07:00:31 2015 UTC,"It's not an interview. It's a narrative with [scarce] Hinton quotes. Anyway, a mainstream journalist wouldn't know what questions to ask (for an interesting machine-learning-oriented interview)."
MachineLearning,3askpq,Foxtr0t,5,Tue Jun 23 20:17:27 2015 UTC,"You're right about its not really being an interview and maybe missing some richer content. I don't know U of T Magazine, but I'd expect a bit more from a publication by the University of Toronto, even if it's mainstream-oriented. For example, Hinton deserves all the praise that can be given to him, but saying that he pretty much invented the field of deep learning without dropping a few other names doesn't feel right."
MachineLearning,3au2fs,fhuszar,1 point,Tue Jun 23 14:46:59 2015 UTC,"Good post, as usual."
MachineLearning,3axa7a,isolar89,2,Wed Jun 24 06:20:24 2015 UTC,"This issue is called unbalanced data, specifically you have imbalance in your y variable. It causes a problem because the model can do very well (getting 32/33rds of points correct) by predicting a constant 0.  There are at least two easy approaches. 1. Sub-sample the over-represented class as you suggested. As /u/pgay suggested, you could run multiple times with different sub-samples. 2. Over-sample the under-represented class (basically just copy those rows).   There are also more sophisticated methods, e.g. a loss function that rewards getting the under-represented class correct more than the over-represented class. Search for ""asymmetric loss function""."
MachineLearning,3axa7a,jmmcd,1 point,Wed Jun 24 22:03:32 2015 UTC,"Thanks, this is giving me much better results!"
MachineLearning,3axa7a,jmmcd,1 point,Thu Jun 25 03:20:16 2015 UTC,No problem. Which method are you using to get the better results?
MachineLearning,3axa7a,pgay,2,Thu Jun 25 10:24:09 2015 UTC,I duplicated the ZipCodes with a Stores (to make it about 2k data points) and randomly selected 2k cities without a store.  I decided to go for a logistic regression. I want to do some CV to find the best fit but so far my equation looks something like this: Hit = B0+B1per_capita_income+B2median_age+B3White(binary)+B4per_capita_income*median_age.  Will be posting the results on a github page soon
MachineLearning,3axlhm,gitamar,1 point,Wed Jun 24 08:54:35 2015 UTC,"Has anyone seen something similar to this in real-estate? Specifically, predicting the success of a flip in terms of ROI?"
MachineLearning,3asto8,iori42,3,Tue Jun 23 05:52:03 2015 UTC,"http://i.imgur.com/8WvjXbB.jpg  Jokes aside, thanks for sharing!"
MachineLearning,3asto8,piesdesparramaos,5,Tue Jun 23 11:45:05 2015 UTC,"this made me chuckle. compared to inceptionism, these images are  soooo much smaller. Imagine my state of mind last week (when Inceptionism released) when I knew the general public perception would be exactly this :)"
MachineLearning,3asto8,r-sync,3,Tue Jun 23 14:01:50 2015 UTC,"Thats a lot of javascript code, and it looks pretty similar to torch code.  Do you have a torch-in-javascript?  Will you opensource it?"
MachineLearning,3asto8,londons_explorer,2,Tue Jun 23 17:19:18 2015 UTC,https://github.com/soumith/nnjs  A quick-and-dirty-and-incomplete converter from torch model to nn.js model  https://github.com/soumith/eyescream/blob/gh-pages/convert_to_nnjs.lua
MachineLearning,3asto8,r-sync,2,Tue Jun 23 19:19:28 2015 UTC,"I'm missing something here. He says the key to ""objectness"" was giving a class label input to the generative network, but later he says to save time they train a separate network for each class..."
MachineLearning,3asto8,j1395010,2,Tue Jun 23 12:45:54 2015 UTC,"there were two datasets. The CIFAR database, and the LSUN database.  We did the class-conditional model on the CIFAR dataset, that worked great.  On the LSUN dataset, where we were too short of time to modify our script to add the one-hot-coding, we just instead trained a separate network on each class (there are 10 classes in LSUN). That worked too."
MachineLearning,3asto8,r-sync,2,Tue Jun 23 13:59:39 2015 UTC,"thanks! I had a look at the paper, it sounded like the class-conditional was needed for CIFAR because it was a smaller training set, would you agree with that?"
MachineLearning,3asto8,j1395010,2,Tue Jun 23 14:27:51 2015 UTC,"Yes, CIFAR definitely does not have enough data to train a separate model per class."
MachineLearning,3asto8,r-sync,1 point,Tue Jun 23 14:35:03 2015 UTC,Did they suffer from the so called 'helvetica scenario' where the generator gets stuck in a single mode I wonder?
MachineLearning,3asto8,dylanbyte,1 point,Tue Jun 23 14:57:45 2015 UTC,What happens if G_0 == G_1 == G_2 etc.?
MachineLearning,3asto8,londons_explorer,1 point,Tue Jun 23 17:17:38 2015 UTC,how are people not going nuts over this? this is so much more impressive than those stupid eyeball paintings.
MachineLearning,3asto8,j1395010,5,Tue Jun 23 12:07:50 2015 UTC,"The two methods are showing different things. The images google released are showing images generated by a model that was trained to do classification. The fact that it can generate anything is remarkable. The model here was trained to be generative, so it's no surprise that it can generate more realistic images. While the results here are a substantial improvement over previous work, it still shows how far we are from generating images. As someone mentioned below, our brains are filling much of the detail for these small images. For example, can you point out exactly where the front/back of the car is? Can you tell which parts are the birds head? You can see the missing detail in the larger images. It's great that it picks up global patterns of where the sky is and where the ground is, but there's much less structure in the images than the ones that google generated."
MachineLearning,3asto8,rantana,3,Tue Jun 23 13:43:37 2015 UTC,"It's great that it picks up global patterns of where the sky is and where the ground is,   It's not clear to me that it's doing that; it seems like those are defined by the downsampled image that is fed to it in the first place.  The pipeline seems to be: take an actual picture of (say) a church; downsample it to 8x8; and then have the generative networks recursively fill in detail as it is repeatedly upscaled back to 32x32.  The large-scale features -- e.g. where in the picture the sky is and what color it is -- seem to be defined by the initial 8x8 input image."
MachineLearning,3asto8,VelveteenAmbush,3,Tue Jun 23 17:26:09 2015 UTC,"Well, I would argue that these Inceptionism images are more impressive; they're larger, clearer, to my eyes have more ""objectness,"" were seeded randomly rather than with a downsampled photograph, and seem to have been generated by a simpler and less fragile model.  So if the objective is just to make cool pictures of given objects, I think Inceptionism has the crown. And if the objective is something else (e.g. hardening image classification nets against exotic, pathological images of the type demonstrated in this paper ), then it's hard to know whether to be impressed in the absence of benchmarks or survey of the current state of the art."
MachineLearning,3asto8,VelveteenAmbush,2,Tue Jun 23 17:21:38 2015 UTC,"I think that a fundamental objective is to be able to model the probability distribution over images, possibly conditioned on something like a text description or other information that describes the image.    The Inceptionism work doesn't give a generative model, whereas the Facebook paper does."
MachineLearning,3asto8,alexmlamb,2,Tue Jun 23 20:29:50 2015 UTC,"a fundamental objective is to be able to model the probability distribution over images   Can you explain what this means? I don't understand it.   possibly conditioned on something like a text description or other information that describes the image.   Unless I'm mistaken, these images aren't in fact conditioned on a text description or really anything at all except for (a) the test data that was used to train the network in the first place, and (b) a downsampled 8x8 image to seed the generation.   The Inceptionism work doesn't give a generative model, whereas the Facebook paper does.   I also don't understand what this means. The Inceptionism work is clearly a model that generates images, so do you mean that (i) they don't fully specify their model (i.e. as far as I know they haven't released code or divulged all of the details of how it works), or (ii) there's some other necessary criterion that a model has to fulfill to be a ""generative model"" that it doesn't?  (Sorry if this sounds argumentative, it's not intended, I just genuinely don't understand.)"
MachineLearning,3asto8,VelveteenAmbush,2,Tue Jun 23 21:20:58 2015 UTC,"It's a great question.    A generative model of images should only generate images that could actually be observed in the dataset (real-looking images) and it should eventually generate every image in the dataset with some probability if you sample long enough.    The inceptionism images could never be observed in real life, even though they share elements with real images.  The color scheme and local patterns look nothing like real images.  Additionally, there is no reason to think that it would generate every image that could be drawn from the data generating process if you sampled long enough.    ""Unless I'm mistaken, these images aren't in fact conditioned on a text description or really anything at all except for (a) the test data that was used to train the network in the first place, and (b) a downsampled 8x8 image to seed the generation.""  Right, but once you have a generative model for images, it should be fairly trivial to build a generative model that's conditioned on a text description or some other side information."
MachineLearning,3asto8,alexmlamb,1 point,Thu Jun 25 06:17:58 2015 UTC,I wonder how much of this is related to how our brains 'fill in the blanks' when processing often incomplete visual information from our eyes
MachineLearning,3asqxv,g4n0n,3,Tue Jun 23 05:20:08 2015 UTC,"And this being a University of Montreal (1st author) paper, we can expect Theano code to be out soon. (Y)"
MachineLearning,3asqxv,muktabh,2,Tue Jun 23 12:05:17 2015 UTC,Always nice to get the theano code from some of these papers.
MachineLearning,3asqxv,siblbombs,1 point,Tue Jun 23 14:11:14 2015 UTC,"To our knowledge, this is the first application of a neural-network model to open-domain response generation, and we believe that the present work will lay groundwork for more complex models to come.   Didn't Google publish one four days ago? [Edit: seems like this team did the research many months ago, but the paper was only published now -- sorry for the incorrect implication.] Two other thoughts: not sure why their context is constrained to a single preceding remark; doesn't this mean that the model is theoretically incapable of being influenced by anything that happened in the conversation more than two lines ago? And it seems odd that they don't include perplexity as a benchmark."
MachineLearning,3asqxv,VelveteenAmbush,1 point,Tue Jun 23 17:07:38 2015 UTC,"This paper was written for NAACL-HLT 2015 (submitted December 4, 2014, accepted February 20, 2015, and presented June 1, 2015), so it was done long before the Google work. This work has been continued after that submission - Alessandro gave a talk at UMontreal in February/March about it. I would guess you will see more papers from him and collaborators soon...  There have been many papers in the last few days about dialog / QA - very likely all NIPS and EMNLP submissions. It is an exciting topic, and appears to be the ""caption generator"" popular thing to do for NIPS/EMNLP this year.  Perplexity (like NLL) is a pretty misleading benchmark on most real tasks. I think the inclusion of the other metrics is more than sufficient, though I agree that not including it seems incomplete.  Their context is constrained to the previous remark, but if you look at dataset construction in 5.1 this makes sense. The whole goal was to generate responses to context, message pairs - there was no long term Q/A or dialog happening in this particular experiment, but I am positive the authors have moved to that now. As such, there was no reason for long term prior context though this could easily be included by passing hidden states, conditioning on concatenated representations, or a number of other ways."
MachineLearning,3asqxv,kkastner,1 point,Tue Jun 23 17:29:04 2015 UTC,"""Perplexity (like NLL) is a pretty misleading benchmark on most real tasks.""  Why?"
MachineLearning,3asqxv,alexmlamb,1 point,Tue Jun 23 20:33:17 2015 UTC,"Better perplexity/nll in anything I have done has not translated to real gains in either sample quality or structured prediction performance (things like BLEU, etc.). Or if it has (better real score and better NLL/perplexity), there are equivalent models with equally good NLL performance but which are completely unusable for the true task. It seems like it is a good proxy when you are doing classification (not structured prediction) but it starts to fail when things become dependent over time/space/etc.  A good investigation of better metrics for structured prediction and generative models is definitely needed, though what we have works now. But I don't see it as ""necessary"" in a paper that focuses on the tasks this paper did, though I am sure a reviewer dinged them for it."
MachineLearning,3asqxv,kkastner,1 point,Tue Jun 23 21:50:08 2015 UTC,"but it starts to fail when things become dependent over time/space/etc.   Interesting. What's the intuition here? Is it that, even if it puts a decent amount of probability mass on the correct token prediction, its highest-probability incorrect predictions could lead it more or less off the mark such that the qualitative significance could diverge faster or slower? Couldn't that be solved to some extent by beam-searching a bunch of tokens ahead of the ground truth (rather than staying just one token ahead at any time) and evaluating perplexity over a number of such samples? I guess the appeal of perplexity is its objectivity, unlike BLEU or qualitative estimations... seems like it's worth refining for this kind of task if possible..."
MachineLearning,3av4pc,tehgargoth,4,Tue Jun 23 19:23:09 2015 UTC,"This is equivalent to making the first layer weight matrix block diagonal.  Sticking data from two sources together by concatenating their representations is a pretty standard thing to do; there's no reason those two sources shouldn't be networks.  I don't think the connection to averaging multiple models is correct though.  The reason averaging makes predictions better is that it reduces variance, this approach will do the opposite."
MachineLearning,3av4pc,kjearns,2,Tue Jun 23 21:13:53 2015 UTC,"There is not much difference in my mind between this and concatenating all the inputs as features and feeding through the same layers, unless you want to study the intermediate output after the first independent hidden layers and put special cost or interpretation on the output there. Otherwise, at the end of the network they are both predicting p(y | a1...an, b1...bn, c1...cn) at the end of the day, except your proposal could allow for unique costs on p(a1...an) p(b1...bn) p(c1...cn) which are each modeled through separate hiddens before concatenating the embeddings."
MachineLearning,3av4pc,kkastner,1 point,Tue Jun 23 19:38:09 2015 UTC,"I found this diagram which is what I was attempting to describe, still digging on my own...  http://www.hindawi.com/journals/bmri/2011/349490/fig2/  EDIT: Thats not exactly what I was trying to describe... but like where the above diagram has one input feeding into 3 nodes of the first hidden layer here, would have multiple inputs feeding into each of the first layer groups like individual 2 layer networks feeding into a single, larger 3rd hidden layer"
MachineLearning,3av4pc,farsass,2,Tue Jun 23 19:59:55 2015 UTC,"it's entirely possible to do what you want, but the question is why? one reason would be to impose different structures on the weight matrices of each group, perhaps via regularization or the training algorithm.  You could easily try it with theano or torch"
MachineLearning,3av4pc,MusicIsLife1995,1 point,Tue Jun 23 20:15:12 2015 UTC,"This method would work very well with Map Reduce, effectively improving computational speed."
MachineLearning,3av4pc,MusicIsLife1995,1 point,Wed Jun 24 03:31:09 2015 UTC,"I was thinking about this with noisy realtime timeseries data or maybe even voice recognition.  I tend to have to constantly retrain due to changes in the patterns of data over time.  I've seen a lot of people will change their inputs completely and retrain until something works with the newly changed data and then keep a collection of networks that work during certain scenarios, but I was thinking if you know that during certain conditions inputs 0-10 have patterns, and other conditions inputs 11-20 have patterns, Something like this might be useful to segregate sets of inputs that are collectively significant to each other in the first and second layer but in the big picture each group may or may not be significant with the current set of data.  If your third layer has one node per input group then the fourth layer is generating weights based on a single node output for each input group.  I was just curious if my line of thinking was sane before I dug in to try and build it."
MachineLearning,3av4pc,Powlerbare,1 point,Wed Jun 24 15:52:07 2015 UTC,"Dude, go for it! This sounds super interesting. I'm curious about what results you get."
MachineLearning,3av4pc,dwf,1 point,Wed Jun 24 17:36:25 2015 UTC,"The goal i was thinking about was maybe if you know that patterns emerge between different subsets of inputs based on your domain knowledge couldn't you lower the training cost by feeding each input group into a smaller number of hidden layers per group?  I was thinking of something like.. Imagine you have 1000 inputs for a binary classification and you know that there are definite patterns in say 1-100, 101-200 etc.. and this 1000 input network comes back with best responses with 1200 first hidden layer nodes and 1200 second hidden layer nodes... what if you could have each 100 input group feed into 110 hidden layer nodes feeding into 1 hidden layer node EDIT: for each subset of inputs (10) which all feed into 12 hidden layer nodes could it come back with a similar result?"
MachineLearning,3at10u,aranag,2,Tue Jun 23 07:27:10 2015 UTC,Thanks for sharing
MachineLearning,3at10u,dopadelic,1 point,Tue Jun 23 08:15:30 2015 UTC,my pleasure
MachineLearning,3ate8j,alexeyr,1 point,Tue Jun 23 10:38:49 2015 UTC,"Note that, in 8/10 cases, the validation accuracy is higher than the testing accuracy. This is to be expected because the hyperparameters are being selected based on their performance on the validation set, and not the testing set.   Doesn't this mean that you need another validation set to ensure that you're not ""overfitting at a distance"" via hyperparameter selection?"
MachineLearning,3ate8j,VelveteenAmbush,1 point,Wed Jun 24 17:07:20 2015 UTC,"Yes, AFAIK."
MachineLearning,3ate8j,gibberfish,1 point,Wed Jun 24 21:14:33 2015 UTC,"That is the exact purpose of the testing set. Both the hyperparameter and parameter optimization happens without the data in set, so it provides a more unbiased idea of real world performance."
MachineLearning,3atc4c,trtm,1 point,Tue Jun 23 10:08:55 2015 UTC,Maybe try Physionet?
MachineLearning,3atc4c,FulyenCurtz,1 point,Tue Jun 23 15:14:01 2015 UTC,"I'm not familiar with medical imaging data, but there is a huge variety of free medical data sets online.  Can you be more specific about what you what? Biomolecular data?  Clinical data?"
MachineLearning,3atc4c,gibberfish,1 point,Tue Jun 23 15:43:00 2015 UTC,"Try looking for challenge datasets. A lot of medical image processing conferences organize yearly challenges with problem-specific datasets, chances are you can find some sets from previous years online, or you could ask the group who organized the challenge."
MachineLearning,3atqys,bbcomp,4,Tue Jun 23 13:07:01 2015 UTC,"This is a strange idea. If you truly know nothing about the inside (really true black box), then the no free lunch theorem says that no optimization algorithm is better than any other. So this becomes a competition of guessing what kind of structure is usually found in such black box competitions. For example some sort of (piecewise) smoothness can be probably assumed. You can also assume that the computation of the black box function can be written in a programming language in reasonable length. But what else? Of course we can try and verify these things but we have a budget and the order in which to examine such hypotheses is based on what we think about the content of the black box. We can't avoid thinking about the internals.  Anyway, the main idea is really flawed and is based on the misconception that ""intelligence"" is a thing, and you can write code that just ""works out"" or ""realizes"" how the function behaves and then ""generalizes"". There is no such ""thing"" as generalization in general (no free lunch). What you put in is what you get out. The performance of an algorithm reflects the appropriateness of your assumptions. Nothing will automatically improve stuff by knowing a general procedure of ""improving"".  The question is reduced to: What are the problems that we usually want to solve and what is a common property of these problems? But if the competition organizers don't draw from the same pool of ""practical problems"" as we imagined, then we'll necessarily perform worse."
MachineLearning,3atqys,bonoboTP,2,Tue Jun 23 14:44:26 2015 UTC,The no free lunch theorem has nothing to do with real world problems. It assumes problems are drawn from a totally random distribution. With no structure at all. Real problems are not totally random and have structure which can be exploited.  Solomonoff induction is a resolution to the machine learning version of the problem. It suggests a prior of simple algorithms in some Turing complete language.  You can't compute Solomonoff induction of course. But it suggests that problems should have lots of simple structure and patterns. So some methods should do better than random.
MachineLearning,3atqys,Noncomment,1 point,Wed Jun 24 17:05:19 2015 UTC,"As I understand, Solomonoff induction is basically a way to choose a prior over the data, with the assumption that stuff is probably generated by a universal Turing machine running a short program (the shorter, the more probable).  Two problems with this:   If you're trying to learn a function that was designed by a human, you can assume it's basically a Turing machine. But we don't know whether natural processes are really perfectly computable. There might be cases where things aren't predictable, there is just no correlation between past and future. For example quantum weirdness, such as radioactive decay. The choice of the universal Turing machine matters. I know it's only a constant addition (adding a piece of compiler program that translates from one machine to the other), but it's just enough to make it important. Because for short programs, the compiler dominates. And you can have Turing machines arbitrarily far away: For any n, you can have Turing machines A and B for which the emulator of B written as a program for A takes up at least n bits. So complexity is still not objective. Of course we can just take a ""sensible"" and ""practically relevant"" universal Turing-equivalent language such as C, but then we just shifted the question of what is sensible one step away.   There is no way of escaping the question of what structure we consider natural and what not. The problem of subjective Bayesian priors can not be mumbled away. But the priors don't have to be ""designed"" by us, they are basically ingrained in our perceptions and ways of thinking by evolution. The fact that we have eyes for example expresses our prior that we ""expect"" stuff to exist that we can see. The sort of structure that we expect in data is also the same story. We think of things as ""simple"" if it has a brief representation in our brains, and it has such a representation if that was useful during the eons. We won't be able to derive those things from scratch by mathematics."
MachineLearning,3atqys,bonoboTP,1 point,Thu Jun 25 13:25:18 2015 UTC,"OK to start with, this is all philosophical stuff. It has nothing to do with the practical problem at hand. I'm just saying that the no free lunch theorems are only true in the hypothetical universe that has no priors, and where everything is totally random.  We aren't ideal philosophers with no priors. We don't live in a universe of pure entropy. We expect to be able to capture a great deal of the world in relatively few bits of information. If the no free lunch theorems were true, then all our predictions would be worse than random. All engineering would be impossible.  Now to philosophical stuff...  You can add randomness to Solomonoff Induction. Rather than requiring it to be perfectly deterministic. But in theory it can still model quantum mechanics and stuff, it just assumes the randomness comes from predefined random number table or something. The hidden variables are captured in the hypothesis itself.  SI is just a prior. Yes maybe assuming the universe is written in C code is wrong, but it will still get you arbitrarily close to the correct answer. Someone with a LISP prior might get slightly closer. But if you find the universe looks LISP like, you can just simulate LISP in C. Really any programming language humans can imagine, and even the human brain itself, can be simulated in any programming language. So it's not a big deal at all."
MachineLearning,3atqys,Noncomment,1 point,Thu Jun 25 13:41:46 2015 UTC,"I think we agree. Maybe it's too obvious for you. But some people imagine that we can build an intelligent machine that by virtue of it being intelligent, will predict well. When in fact what machine learning is actually about is a competition of priors. Of course we don't do this consciously, but the choice of a learning algorithm is equivalent to choosing a prior. So seen from a big picture, machine learning research is actually about formalizing our buit-in priors.  Yes, part of that prior is that we believe in induction. For example, that algorithms that we find to work well for face detection on one dataset, will also work well on another dataset.  I know it's philosophical, but it shows that a fully general intelligence, that just figures out everything, is impossible. Some people imagine building AI as if something will suddenly ""click"" and we tap into the ""intelligence"" in the universe and it will almost-magically just do smart stuff. There is no such thing to tap into. The statement ""this AI algorithm works well"" is the same as ""here are my priors over how the world works""."
MachineLearning,3atqys,bonoboTP,1 point,Thu Jun 25 14:09:49 2015 UTC,Well Solomonoff induction is a perfectly reasonable prior. The reason no one uses it is because it requires nearly infinite computing power. So I think the goal of machine learning is more about how to approximate SI.
MachineLearning,3atqys,Noncomment,1 point,Thu Jun 25 16:05:27 2015 UTC,"I wanted to emphasize that Solomonoff induction is not fully specified without specifying the universal Turing machine to be used. Of course you can say ""well just pick a reasonable one"", but then we just shifted the question ""what is a reasonable prior?"" to ""what is a reasonable universal Turing machine?"".  I think if we are allowed to pick very esoteric, pathological universal Turing machines, then no-free-lunch style problems come up again."
MachineLearning,3atqys,bonoboTP,1 point,Thu Jun 25 17:48:06 2015 UTC,Yes the necessity of priors will always upset purist philosophers. They can go assuming that the world is perfectly random and unstructured and that all priors are equally likely.  But my point is it's not a practical issue. Any reasonable programming language can run any other programming language. Or any existing machine learning algorithm for that matter.
MachineLearning,3atqys,Noncomment,1 point,Fri Jun 26 03:16:35 2015 UTC,"""As there is no well-performing universal search algorithm, the metaheuristics we are developing must be biased towards certain problem classes."" (from a recent review on NFL: http://image.diku.dk/igel/paper/NFLTLaPoM.pdf)  In BBComp, the selection of problems was biased towards certain problem classes which we consider to be ""common"" and ""of interest for practical applications"". We acknowledge that different people have different opinions about what is this ""common"" and ""practically relevant"", however, a public discussion about this would destroy the black-box character of the testbed.  The competition is not intended to result in the design of new algorithms, but rather to benchmark existing ones. The expected positive output is that users will be more satisfied by the results of the best performing rather than the worst performing algorithms because the former are apparently more suitable at least for the class of problems represented in the competition. Of course, in accordance with NFL, there may be some problem domain for which the ranking is reversed, however, the organizers would be more than surprised to see that happening for any ""practically relevant"" problem class."
MachineLearning,3atqys,5at,1 point,Tue Jun 23 16:51:19 2015 UTC,"there may be some problem domain for which the ranking is reversed, however, the organizers would be more than surprised to see that happening for any ""practically relevant"" problem class.   If you get 100 participants, the odds of their ranking reversing, purely by chance, would be 1/100! or 1e-158, so technically, you are right: it would be very surprising.  On the other hand, if you are claiming that some solution ranking would be preserved among different domains of practical interest, you need a stronger argument than ""we will be very surprised if it's not true""."
MachineLearning,3atqys,bonoboTP,1 point,Tue Jun 23 18:01:29 2015 UTC,"it seems at least optimistic to make the claim you described, i.e., that the entire ranking will be preserved.   our task is to bias the classes of competition problems towards the classes of problems that the ""average users"" will deal with in a way that the users will be more satisfied by the results if they select the best-ranked algorithms rather than the worst-ranked algorithms. We will be surprised to see that the opposite situation (best-ranked algorithms perform worse than worst-ranked algorithms) will tend happen more often."
MachineLearning,3atqys,elfion,1 point,Tue Jun 23 18:25:42 2015 UTC,"Interesting. It must be hard to come up with the functions for such a competition. I know it gets somewhat philosophical, but I'm trying to think about the source of the common properties shared by ""practical problems"". Is it because we tend to pose certain questions in certain ways and we effectively impose structure on the data by our way of picking and formulating them? I'm not sure.  Anyway, it seems to me that knowing what you are optimizing can be immensely helpful in achieving your goals. For example if you know that a parameter represents the number of hidden units in some neural network, it helps a lot. People usually don't have to solve the ""general problem"". I know there are companies who provide such plug-and-play machine learning service without using any domain-level information of what the task actually is. But it seems weird to me. Automation is good, but throwing away valuable information seems wasteful.  I think the algorithm shouldn't have to discover everything from scratch, we should find effective methods of specifying our special circumstances. For example we could have a questionnaire or a chatbot that asked us what the industry is, what the machine is, etc, and then we could have a centralized knowledge pool where we collect what works well in which applications. There is a huge amount of such experience scattered around in companies and research institutes that you could use when you do a new project."
MachineLearning,3at7uy,cast42,2,Tue Jun 23 09:09:03 2015 UTC,"I think this was really nicely done, but you may wish to consider a different evaluation metric; for example, the multi class ROC."
MachineLearning,3apgvn,muktabh,18,Mon Jun 22 13:05:36 2015 UTC,Human: what is the purpose of existence ?  Machine: to find out what happens when we get to the planet earth .  Something about that is very creepy.
MachineLearning,3apgvn,slanderman,25,Mon Jun 22 14:41:22 2015 UTC,Human: what is immoral ? Machine: the fact that you have a child .  is my favourite
MachineLearning,3apgvn,elanmart,6,Mon Jun 22 14:46:47 2015 UTC,I demand a live demo.
MachineLearning,3apgvn,cryptocerous,4,Mon Jun 22 14:53:16 2015 UTC,Human: what is the purpose of emotions ? Machine: i don ’t know .
MachineLearning,3apgvn,jcannell,3,Mon Jun 22 18:13:50 2015 UTC,They must have trained it on /r/childfree.
MachineLearning,3apgvn,SirGolan,6,Mon Jun 22 18:22:19 2015 UTC,"Actually this makes sense now.  They trained it on Battlestar Galactica!  Human: what is the definition of altruism ? Machine: if you don ’t believe in god , then you don ’t know .  Human: what is the purpose of life ? Machine: to serve the greater good .  Human: what is the purpose of living ? Machine: to live forever ."
MachineLearning,3apgvn,jcannell,1 point,Mon Jun 22 18:15:37 2015 UTC,"Ah, so it is essentially Leoben Bot in that regard!"
MachineLearning,3apgvn,slanderman,15,Tue Jun 23 00:04:48 2015 UTC,"Machine: could you please let me know what is the operating system you are using ?  Human: linux  Machine: ok this is the issue with the system performance , nasser , tools .  Human: what is that ?  Machine: is it a windows machine ?  Machine: we have linux on your machine .  Machine: is it windows xp or windows 7 ?  Human: i am using linux   The singularity is close."
MachineLearning,3apgvn,MrTwiggy,2,Mon Jun 22 15:36:11 2015 UTC,"Realistically, how could the NN possibly do that? It would have to learn a single neuron whose only purpose is just to store a single variable, whether or not the user is running linux. And you need hundreds of these neurons, for every special case like this. And then the number of parameters blows up with the square of the number of neurons.  EDIT: Actually it's worse then that, the system only gets the last line as input. ""what is that ?"" doesn't tell network what OS is being referred to at all."
MachineLearning,3apgvn,Noncomment,3,Tue Jun 23 03:03:17 2015 UTC,"Not necessarily. Look up Neural Turing Machines. They provide an augmentation to NN structure that allows it to store arbitrary data in a memory cell of arbitrary size. Getting it to work on any real datasets with formidable performance has not been seen yet, but there are avenues available. I believe Memory Networks also offer a random access memory augmentation."
MachineLearning,3apgvn,MrTwiggy,2,Tue Jun 23 03:25:39 2015 UTC,"Realistically, how could the NN possibly do that? It would have to learn a single neuron whose only purpose is just to store a single variable, whether or not the user is running linux. And you need hundreds of these neurons, for every special case like this. And then the number of parameters blows up with the square of the number of neurons.   It doesn't seem crazy that a LSTM network trained on tech support chat data should develop some neural architecture to keep track of what hardware the simulated user is running. Andrej Karpathy's LSTM network that does pure sequence prediction (which seems very similar to the model employed here, except one character at a time with no concept of words) did a great job of keeping track of open and close parentheses, open and close brackets, whether it was inside of a quotation or URL, how long its current line was, and so on, all through unsupervised character-level learning. Who knows what sort of inferences are plausible without the network scale requirements blowing up exponentially. Maybe it will go all the way up to human-level conversational skills."
MachineLearning,3apgvn,VelveteenAmbush,1 point,Tue Jun 23 07:11:38 2015 UTC,"That's true, but Karpathy's NN does have individual neurons which detect parentheses. He actually found it and visualized what it was sensitive to in the post.  The entire network only has a few hundred neurons, which doesn't seem like a lot. Not if you need an individual neuron for every low level feature. There are thousands of words in the English language.  And it does blow up. That parentheses detector requires a bare minimum of 600 floating point operations each cycle, and probably more because of LSTM stuff. If you have 1,000 neurons, that's a minimum of 1,000,000 weights! 10,000 is 100,000,000! And we are barely into anything the size of an insect brain, let alone a human.  And the vast majority of those weights are probably going to be close to zero anyway."
MachineLearning,3apgvn,Noncomment,1 point,Thu Jun 25 09:17:03 2015 UTC,Every customer support rep ever
MachineLearning,3apgvn,linuxuser86,8,Sun Jul 12 03:21:17 2015 UTC,"From the article, it appears that they are only providing the previous line as context to the network.  Why not the last N lines?  Given that it's a RNN, that wouldn't increase the complexity of the network, and should only slow down training by a factor of N.  Given that is Google, compute is unlikely to be a problem, so it's unclear why they made the decision they did.  I suspect the coherence of the the conversations would be significantly improved with additional context."
MachineLearning,3apgvn,melvinzzz,3,Mon Jun 22 16:51:34 2015 UTC,"You'd have to combine it with a method that didn't average over variation (there was a recent post on here for an arxiv paper that had a cool way of doing it). Just extending the context doesn't give you as much bang for your buck as you'd hope (at least,  I'm assuming that this statistical utterance sequence model is only different in degree from previous attempts over the last decade.  i.e. It learns the same things better,  not new things)"
MachineLearning,3apgvn,Articulated-rage,2,Mon Jun 22 19:30:33 2015 UTC,"What averaging are you referring to?  The seq2seq method uses a LSTM, so the network itself determines how to accumulate context.  Perhaps you are referring to older HMM style methods, in which case you are correct, but this network doesn't use that sort of method."
MachineLearning,3apgvn,melvinzzz,1 point,Mon Jun 22 20:36:28 2015 UTC,"Does LSTM track variations in the context?   For example:   The city council denied the city protestors a permit.  They (feared/advocated) violence   To condition on this context, I thought LSTM would treat both cases in (2) as virtually the same context, therefore losing the distinction.   In other words, I thought LSTM (in a crude description) was just a mechanism for knowing which stored embedded representation to evoke given the current observed data.    But to your original question about the last N lines as context: many cases are like the one I've described where the flexibility in interpretation and language use comes from the representation of the words themselves rather than a larger context.  The context that's needed is some sort of knowledge representation which allows you to track these types of situations.  This paper has a pretty good method for it.   And just a quick point: HMMs weren't the last batch of models for conversation.  Reinforcement learning was/is the current state of the art, and combining it with deep learning is very fruitful.  Jason Williams and Steve Young are the best right now, imo."
MachineLearning,3apgvn,Articulated-rage,1 point,Mon Jun 22 20:53:36 2015 UTC,"In theory there are almost no bounds on what an LSTM/RNN can learn to do, in practice it depends on the amount of data that the model has access to, the size of the memory, and the efficiency of the learning algorithm, which are hard to study analytically.    Somehow the model would have to have some causal understanding that advocating violence causes permit denial and that fearing violence is a reason to deny a permit."
MachineLearning,3apgvn,alexmlamb,2,Tue Jun 23 01:13:04 2015 UTC,"Thanks for the reply =).    almost no bounds.   In theory, ya.  The endemic problem that's plagued dialogue research, though, is the poverty of data.  There's many instances where the data just won't exist because conversations are the output of a lot of components coming together in a combinatoric explosion.  We're talking about a domain where most of the data is in the tail and the tail is really long and fat.   It's been thought for a while that world knowledge fills in all of these gaps, and that all you have to do is have a system that can adequately identify what knowledge is relevant, make the connections, then resolve appropriately.     Andy Kehler has some great ideas on this.  For pronoun resolution, he proposes that a mechanistic approach (ala grosz's centering theory) biases one way, a knowledge-based abduction approach (ala hobbs) biases another way, and they balance out at correct pronoun resolution.    The paper I linked earlier proposed a predicate schema method that works better than any others right now.  It does just what you're proposing.   But, in a deep learning architecture that only learns from conversation data, do you think it'll ever reach that point?  I personally don't think so (but, that doesn't mean I'm right =P).  I think that there needs to be some sort of knowledge in aggregate system that's learned in the right way then hooked up in the right way to an LSTM or something.    It's personally not currently my approach, because my current onus is purposeful linguistic action.    But, if it's going to succeed, the deep network is going to have to do more than sequence learning.  It's going to have to do sequence learning combined with some gap filling via learned knowledge.  I guess that was my point and I'm guessing you kind of agreed in that the sequence learner needs to connect the right bits to ""advocating violence causes permit denial"".   Some people are working on embedding KB triplets, maybe that'll work =)."
MachineLearning,3apgvn,Articulated-rage,5,Tue Jun 23 01:26:27 2015 UTC,"Please link to the paper's landing page instead of using a direct PDF link next time. Not everyone likes surprise-pdf links, and if the paper ever gets updated to a newer version, you won't notice that when using the PDF link."
MachineLearning,3apgvn,quirm,2,Tue Jun 23 06:35:45 2015 UTC,"Cool! I would like to try this out on this dataset. As this paper is based on ""Sequence to Sequence Learning with Neural Networks"" (2014), has anyone implemented this architecture already?"
MachineLearning,3apgvn,siblbombs,1 point,Mon Jun 22 16:07:04 2015 UTC,Blocks has an example that also has an attention mechanism.
MachineLearning,3apgvn,VelveteenAmbush,4,Mon Jun 22 16:33:44 2015 UTC,"It looks like they tried an attentional model and didn't see improvements:   Interestingly, adding the soft attention mechanism of (Bahdanau et al., 2014) did not significantly improve the perplexity on neither training or validation sets."
MachineLearning,3apgvn,siblbombs,1 point,Mon Jun 22 18:04:57 2015 UTC,"I would expect that adding an attention mechanism would be overall beneficial to it producing better outputs, but perplexity might not be a good measure to capture the improvement."
MachineLearning,3apgvn,Articulated-rage,2,Mon Jun 22 19:07:18 2015 UTC,"Hmm.  Interesting point, but to be frank, I don't understand your sentiment.  Why wouldn't perplexity be a good measure?  It's not the best, but it's a measure of surprisal.  If you expect a model to capture systematic patterns in the data, but it's still surprised by the test data, then it's not capturing the systematic patterns.   If this metric doesn't work, nearly all the log likelihood based metrics will be of the same relative change: log-likelihood itself, perplexity, AIC, BIC.    It feels to me that your post is saying ""well, I believe in this method, therefore the measure must be wrong"".   Shouldn't it be ""I believe in measurements, therefore this method must be wrong"".  Isn't that scientific progression?"
MachineLearning,3apgvn,siblbombs,2,Mon Jun 22 19:15:09 2015 UTC,"My statement is based around the following from the end of section 5.2:    An outstanding research problem is on how to objectively measure the quality of models. We did so by manual inspection and by computing the perplexity. However, there are obvious drawbacks on using either. We are actively pursuing this and other research directions.   It's not clear to me how good perplexity is at modeling consistent tone/personality across multiple dialog interactions."
MachineLearning,3apgvn,Articulated-rage,1 point,Mon Jun 22 19:27:50 2015 UTC,"Thanks for the nice reply.   I thought the point of this model was to accurately predict one-shot conversations?  In other words,  accurately predict the responses to the utterances.  Perplexity is just a measure of where your model distributes the probability mass.  I'm not sure what the authors intend by that statement."
MachineLearning,3apgvn,alexmlamb,1 point,Mon Jun 22 19:35:13 2015 UTC,"Issues with using test-set likelihood/perplexity to evaluate models:   -Not clearly interpretable. -No upper bound.  The highest achievable likelihood could grow to infinity.  (Image fitting a mixture of gaussians to a single data point). -Maximum likelihood is provably consistent, but it may not quickly converge for the properties of the distribution that we care about. -A model that maximizes likelihood may not generate the most realistic looking samples, as the likelihood criteria will cause the model to capture the variance in the distribution, whereas a lower variance distribution will probably produce cleaner and better looking samples.    Any other issues that I've left out?"
MachineLearning,3apgvn,Articulated-rage,2,Tue Jun 23 10:09:12 2015 UTC,"not clearly interpretable   computational linguists have been interpreting for 20 years (note, see edit below)   no upper bound; max likelhood >>; models that maximize ...   I'm confused.  Are you talking about an evaluation metric or an optimization metric? Those are two different functions that aren't necessarily the same.  I know the evaluation intuitions.  The optimization ones I've never thought about.  Though, it would seem to me that since perplexity is just 2 to the empirical cross entropy, which is the average log likelihood of a test point, that max likelihood should be roughly similar.   Something that I'm finding super confusing and I can't reconcile.   You are raising issues with 'perplexity' and propose 'maximum likelihood' as an alternative.  Maximum likelihood is a method, an objective.  Perplexity is just a number.    Also, for what it's worth, let's say you apply your model to a dataset and get a perplexity of 14. That means that, on average, your model is as certain about the data as a 14 sided die.    If there's any simple intuition, it's that.   edit: I linked to the wrong paper above.  still a good paper, so I'll leave it.  I can't find the exact correct paper at the moment, but this is a decent one and has the following quote:   A commonly used measure of the difficulty of a speech recognition task is the word perplexity of the task (Bahl etal. 1977). The cross-entropy we report here is just the base two logarithm of the character perplexity of a sample of text with respect to a language model.    and   We can also think of our cross-entropy as a measure of the compressibility of the data in the Brown Corpus.  only 95 of the characters are printable, it is a straightforward matter to reduce this to 7 bits per character. With a simple Huffman code, which allots bits so that common characters get short bit strings at the expense of rare characters, we can reach 4.46 bits per character. More exotic compression schemes can reach fewer bits per character. For example, the standard UNIX command compress, which employs a Lempel-Ziv scheme, compresses the Brown Corpus to 4.43 bits per character. Miller and Wegman (1984) have developed an adaptive Lempel-Ziv scheme that achieves a compression to 4.20 bits per character on the Brown Corpus. Our language model allows us to reach a compression to 1.75 bits per character."
MachineLearning,3apgvn,Noncomment,1 point,Tue Jun 23 16:40:05 2015 UTC,"I think you can fix these issues just by dividing the log probability by the number of characters or words. Since log probability is additive, this should work out.  If you take the exp of that, you should get an actual probability. The average probability of predicting the next word or character correctly. With a maximum of one and a minimum of zero. And it's clearly interpretable.   -A model that maximizes likelihood may not generate the most realistic looking samples, as the likelihood criteria will cause the model to capture the variance in the distribution, whereas a lower variance distribution will probably produce cleaner and better looking samples.   Not necessarily. Just taking the most likely sample, with no variance, produces extremely artificial looking data. Ie, the same sentence over and over again. Or in images, you get really weird colors and blurriness, because you they just take the average of all the possible RGB values.  If you have a good model, then samples drawn from it should be indistinguishable from real data. At least, to that model they should be indistinguishable, and have all the same statistical properties as real data. Where the model is uncertain, of course the samples will seem random. But so will real data, to the model."
MachineLearning,3apgvn,kkastner,3,Sat Jun 27 22:01:28 2015 UTC,There is also a simpler example here that is an open sourcing of some internal research. I find it much more readable to get the meat of the technique.
MachineLearning,3apgvn,Articulated-rage,4,Mon Jun 22 20:31:51 2015 UTC,"Sooo. At the moment, it's a sophisticated chatbot?  Just using better statistics that previous chatbot?   E.g. Things we can do with current systems that would be super hard with this system: coreference, anaphora, pronoun resolution, deictic gestures when it becomes multimodal, implicatures, and metaphor.    Fancy statistical mapping that doesn't reason to a knowledge state are not likely be able to handle these.   Microsoft is also doing the same thing and even though I think it's better given their great performance on a new twitter conversation corpus,  I still don't see it overcoming any of the latent knowledge reasoning problems.   Also,  what if you want to deploy your dialogue system to have specific intentions? The community also knows how to tweak systems to fit cultural norms by modifying a few latent parameters---no new data needed."
MachineLearning,3apgvn,iwantedthisusername,1 point,Mon Jun 22 17:22:23 2015 UTC,I'd like to see a larger selection of conversations. It almost seems like they handpicked conversations that seem creepier than others.
MachineLearning,3ar221,chaddjohnson,3,Mon Jun 22 20:36:11 2015 UTC,"Not an answer to your question, but if you just want to have the ideas working, not the specific code, you could try Nathan Sprague's reimplementation in Lasagne/Theano at https://github.com/spragunr/deep_q_rl.  It doesn't do as well as DeepMind's results in their Nature paper (although that could have changed with the code changes he's pushed today), but it does at least as well as the results published in their earlier NIPS paper."
MachineLearning,3ar221,alito,1 point,Tue Jun 23 04:15:11 2015 UTC,Nice! Someone else mentioned this too on Github. I'll have to check this out.
MachineLearning,3ar221,sieisteinmodel,1 point,Tue Jun 23 04:45:14 2015 UTC,Any idea how they compare in terms of speed?
MachineLearning,3ar221,alito,1 point,Tue Jun 23 07:05:04 2015 UTC,"I haven't tried DeepMind's version, so I don't really know, but I've heard some people say that DeepMind's version was much faster."
MachineLearning,3ar221,eleswon,2,Tue Jun 23 09:30:45 2015 UTC,Are you monitoring the loss as it trains? You may need to change some hyper-parameters in order for the network to converge properly.
MachineLearning,3ar221,quirm,1 point,Mon Jun 22 22:56:35 2015 UTC,"I'm not sure how to monitor loss. I see the following as console output:  Steps:  1210000 Weight norms: nn.Sequential: [nn.SpatialConvolution: 0.044761270681415 nn.SpatialConvolution: 0.027145430011845 nn.SpatialConvolution: 0.024015799676685 nn.Linear: 0.0090948587405862 nn.Linear: 0.049483255205006] Weight max: nn.Sequential: [nn.SpatialConvolution: 0.27835908532143 nn.SpatialConvolution: 0.30056926608086 nn.SpatialConvolution: 0.30891051888466 nn.Linear: 0.068947084248066 nn.Linear: 0.22231875360012] Weight grad norms: nn.Sequential: [nn.SpatialConvolution: 0.012153453355736 nn.SpatialConvolution: 0.0037674579847074 nn.SpatialConvolution: 0.0015192372908941 nn.Linear: 0.00013075887092303 nn.Linear: 0.0024491464258815] Weight grad max: nn.Sequential: [nn.SpatialConvolution: 0.12329712510109 nn.SpatialConvolution: 0.077036507427692 nn.SpatialConvolution: 0.066019780933857 nn.Linear: 0.015237824991345 nn.Linear: 0.042702201753855]   (there's a lot more than that)  Also not familiar with hyper-parameters or which ones I should try using. I'll have to read more about those. I downloaded the same software that everyone else downloaded, so I would expect things to work for me as it did for them...but it's not."
MachineLearning,3ascxs,ScientiaOmniaVincit,6,Tue Jun 23 03:03:48 2015 UTC,"It's important to reiterate that these titles of ""learning"" and ""evolving"" are only titles. Just as with neural networks, the naming scheme is very loosely tied to the actual workings of the methods. At the end of the day, both are simply optimization routines with different assumptions about the underlying cost function. ""Evolutionary"" algorithms are often zero-order methods which apply search heuristics, and ""learning"" algorithms are often at least first-order in which they use gradient information.  Another difference can be thought of as a similar one between MCMC and variational inference. The former guarantees finding the global optima (asymptotically, and under other ""nice"" assumptions) whereas the other can get stuck in local optima but can at least reach some solution very quickly.  It's impossible to make a fine distinction however. The meaning of terminology changes every day and I'm hard-pressed to even stick to what I made above because this field especially changes so rapidly."
MachineLearning,3ascxs,dustintran,3,Tue Jun 23 03:45:24 2015 UTC,"Exactly. Don't become a prisoner of analogies (thinking that limitations in the analogous thing necessarily translate to limitations in the actual thing). Computer programming has much less restrictions and limitations and you can easily mix ideas as you wish. People coming from the sciences or medicine often don't think in this way. They like to think there are hard boundaries between methods and they are ""things"" we observe and describe the properties of.  Unless you're talking to the general public, think in CS terms, not in analogy terms. EAs are an optimization method. You can even optimize any cost function with it, you can even train neural net weights with EA.  Thinking in terms of ""lifetimes"" and as if the neural net were an agent with intentions is probably not very fruitful. Sure we like narratives like that, but it can make you blind to some engineering improvement that wouldn't translate well into this ""lifetime""/""individual"" framework.  Do whatever you can to improve the system, and then retrofit some name to it from biology or something. I think most of the allegedly ""biologically inspired"" methods are inspired from practical stuff and the researchers look for some biological analogy afterwards, to make it seem more interesting (especially for laypeople)."
MachineLearning,3ascxs,bonoboTP,1 point,Tue Jun 23 11:10:05 2015 UTC,"Thank you so much for the explanation. I think we will all agree that it is just the naming. Though as I said in the other comment, it makes it a pain the the neck to find sources sometimes. I'm looking at one thing but someone else named it completely differently so I cannot find it.   Do you have any suggestions in order to follow (and in fact, first, catch up with) the field? Or is it just about reading the articles?"
MachineLearning,3ascxs,wompfox,5,Tue Jun 23 03:51:18 2015 UTC,"If a distinction exists, you might say that evolutionary algorithms lean on randomness to arrive at a solution, where learning algorithms may try to infer what the error was in each iteration to work towards the correct solution in a systematic way"
MachineLearning,3ascxs,wompfox,2,Tue Jun 23 03:19:39 2015 UTC,"This is the impression I'm getting too. Also, when someone asks me, I tend to an answer like this:  Learning is what happens within the ""lifetime"" of an individual robot or software. It relies on ""award vs. punishment"" as in the case of organismal learning. On the other hand, in evolutionary algorithms we set many different sets of solutions and experiment on them and use the best ones for the next set of solutions (aka ""generations"").  Would that be an appropriate answer, what do you think? Are there learning algorithms that are not evolutionary but still use a set of individuals (or solutions), as well as Darwinian principles of evolution (or at least somewhat similar ones)?"
MachineLearning,3ascxs,wompfox,2,Tue Jun 23 03:31:31 2015 UTC,"I would agree in general, but how would you distinguish between learning within a lifetime and learning between generations? Could you classify iterations/events/generations as being different? Edit: (within the context of training algorithms)"
MachineLearning,3ascxs,Vystril,2,Tue Jun 23 03:42:07 2015 UTC,"Great point! That is the core of the problem, I believe. There is no reasonable way to distinguish the two. Though not properly defining them cause problems for me while trying to find articles on the topic that I'm interested in.   I guess, we can draw a line in between them in the sense that in EAs, the individual run technically ""dies"" in each iteration and is not used again unless chosen for its fitness. But in the learning algorithms, there's nothing dying, it's just improving or not.  Though I still see there's a problem in the sense that this is just a way to interpret the same thing differently, I guess..."
MachineLearning,3ascxs,wompfox,2,Tue Jun 23 03:46:43 2015 UTC,"Exactly! In the end, any distinction made is simply for the utility of creating an easy explanation of the method's operation.  Prime example: your first reply :)"
MachineLearning,3ascxs,ciolaamotore,2,Tue Jun 23 03:50:36 2015 UTC,"If a distinction exists, you might say that evolutionary algorithms lean on randomness to arrive at a solution   Not quite correct.  In general any evolutionary algorithm will have heuristics which do two things:   exploit current best found solutions - these operators tend to be less random and more heuristic based (or even gradient/hessian based), because they want to find better solutions close to good solutions already found. explore new areas of the search space - this is where randomization has a much larger role, as you want to get away from current optima in hopes of finding some new well to explore.   Getting any evolutionary algorithm to work well is a balancing act between the two (any they typically have hyperparameters that tweak things one way or the other).  Too much exploration you never converge to a good solution, while too much exploitation and you'll converge prematurely to local optima.  And of course the right ratio of exploration vs exploitation is going to depend on the optimization problem."
MachineLearning,3ascxs,harharveryfunny,2,Tue Jun 23 14:09:44 2015 UTC,Good point.  I guess the point I was trying to make is that genetic algorithms will utilize randomness to their advantage where other learning algorithms typically do not
MachineLearning,3ascxs,ford_beeblebrox,3,Tue Jun 23 14:14:40 2015 UTC,Evolutionary algorithms are just a way to do *search * which can be done in several contexts and even in machine learning while optimizing something (a loss wrt some parameters for instance).  Have a look at Stochastic Local Search (sls) for a list of other metaeuristichs like that.
MachineLearning,3apmdd,D33B,5,Mon Jun 22 13:59:55 2015 UTC,"If bayesian optimization is so awesome, why has there not yet been a paper using it to set the SOTA on any benchmark?"
MachineLearning,3apmdd,dfarber,14,Tue Jun 23 09:00:12 2015 UTC,"Better yet, use Bayesian Optimization. Or, if your algorithm utilizes an iterative/epoch approach, use Freeze-Thaw Bayesian Optimization. I believe they generally find better hyperparameters faster than random search."
MachineLearning,3apmdd,MrTwiggy,4,Mon Jun 22 15:40:00 2015 UTC,"I am curious, have people from machine learning looked into design of expirements?  There is 100 years worth of literature out there that does a great deal of this.   And yes, optimization, which is sequential, typically beats an a priori designed experiment."
MachineLearning,3apmdd,Floydthechimp,1 point,Tue Jun 23 03:37:42 2015 UTC,"In situations that a grid search is possible (parameter space is small), perhaps is it worth paying the cost of grid search to make sure a global minimum is found?"
MachineLearning,3apmdd,PoddyOne,4,Mon Jun 22 19:41:37 2015 UTC,"The issue with grid search isn't just its cost, but it also tends to result in poorer hyperparameters given the same amount of time to run as, say, bayesian optimization. Unless, of course, the hyperparameters are discreet and can be 'brute forced' into attempting all possible combinations. But even in that situation, I still think BO would find the same optimal hyperparameters but just faster."
MachineLearning,3apmdd,MrTwiggy,1 point,Mon Jun 22 21:18:43 2015 UTC,"erm, you can't really do that with grid search. Grid search is less bad in smaller dimensions, sure."
MachineLearning,3apmdd,GibbsSamplePlatter,1 point,Mon Jun 22 21:00:17 2015 UTC,"There is no circumstances that I have ever seen, theoretical or practical, when grid search is the best option."
MachineLearning,3apmdd,Floydthechimp,1 point,Tue Jun 23 03:39:00 2015 UTC,Very good point. Also a good recent post about Bayesian Optimization for parameter sweeps is here: http://jmhessel.github.io/Bayesian-Optimization/
MachineLearning,3apmdd,anveit,6,Mon Jun 22 20:57:56 2015 UTC,"Great post! FYI the paper you linked is by ""Bergstra et al."", not ""Bengio et al."" Pedantic yes. But this is academia."
MachineLearning,3apmdd,SarcasticMetaName,1 point,Mon Jun 22 21:31:58 2015 UTC,Thanks! Will fix this.
MachineLearning,3apmdd,sledgespread,3,Mon Jun 22 23:36:54 2015 UTC,"Do people use poisson disc sampling ( https://www.jasondavies.com/poisson-disc/ ) at all? It seems like it would give the same advantages mentioned in the article for random search, but it with a more consistent spread of points (so less chance of missing an area of the parameter space, or of wasting time with very similar points)."
MachineLearning,3apmdd,EdwardRaff,3,Mon Jun 22 21:31:13 2015 UTC,"I've seen something similar applied to RandomKitchenSinks that purported to give better results. However, in this case wouldn't it come out looking more like a grid search if we did this with each hyper parameter?"
MachineLearning,3apmdd,sledgespread,1 point,Mon Jun 22 22:51:57 2015 UTC,"You would do it for the entire hyper parameter space at once. So compared to the third figure of the blog post it would look something like the random layout case but less ""clumpy"".  edit: There's a paper showing how to generate a poisson disc sampling in O(N) time for arbitrary dimensions (taken from the blog I linked in the first comment)."
MachineLearning,3apmdd,EdwardRaff,2,Tue Jun 23 15:50:02 2015 UTC,"Right, that's what I'm trying to say. If I filled in that space with points like the poison disk example, the points would be laid out similar to the grid search. So if I look at the 2d sample, I can see a number of good ""columns"" and ""rows"" where the data points appear to have about the same value along the axis. Where gridsearch we know they have the exact same value along the axis."
MachineLearning,3apmdd,sledgespread,1 point,Tue Jun 23 16:33:17 2015 UTC,"Then yes, I think we're saying the same thing. It would be something between grid search and a random search."
MachineLearning,3aujap,redgansai,3,Tue Jun 23 16:54:23 2015 UTC,Very misleading title...
MachineLearning,3aujap,Reyny,2,Tue Jun 23 19:21:47 2015 UTC,Nice troll :)
MachineLearning,3aujap,nilspin,1 point,Tue Jun 23 19:53:15 2015 UTC,"redditor for 4 hours  posts link to own blog with misleading title   hmmmmmmmmm, downvote."
MachineLearning,3astuv,isolar89,1 point,Tue Jun 23 05:54:38 2015 UTC,"Yes it is possible, but I guess not in the R tree() - This is specifically a binary tree.   To do this with a Decision Tree you need to have a splitting criterion to optimise which allows for non-binary splits. Check out for example:  http://www-01.ibm.com/support/knowledgecenter/SS3RA7_15.0.0/com.ibm.spss.modeler.help/chaidnode_general.htm  I saw this when I was reading around about Chi-Squred splitting conditions that were mentioned in the thread here:  http://www.reddit.com/r/MachineLearning/comments/3abbos/bumping_why_can_a_decision_tree_not_learn_the/  I guess the 'why not' is that binary splitting is efficient and effective."
MachineLearning,3astuv,PoddyOne,1 point,Tue Jun 23 06:36:21 2015 UTC,"You might be interested in the ""ID3"" algorithm, which makes one branch per value of the variable if the split variable is categorical."
MachineLearning,3astuv,glouppe,1 point,Tue Jun 23 09:02:19 2015 UTC,"Of course, you can do continuous and categorical outcomes. Just use the rpart package."
MachineLearning,3av9vd,tradingfirm,7,Tue Jun 23 19:58:40 2015 UTC,A few things that might make the job ad better:   -Paragraphs  -Whether you're looking for someone with a PhD  -Whether or not someone with the job can publish (I'm guessing not)  -Whether people with the job are funded to go to conferences (I'm guessing yes)
MachineLearning,3av9vd,alexmlamb,0,Wed Jun 24 04:01:16 2015 UTC,"Thank you u/alexmlamb! I've reformatted the description to help readability.    -Whether you're looking for someone with a PhD   Not necessarily, but many of our researchers have PhD's.    -Whether or not someone with the job can publish   There is a non disclosure that would prevent publishing any advances made here and related to your work with us. Anything outside that would require discussion.    -Whether people with the job are funded to go to conferences   There is a budget for the team."
MachineLearning,3av9vd,NovelTrader,1 point,Wed Jun 24 13:53:49 2015 UTC,"They're called: White Bay Group http://whitebaygroup.com  Typical scummy market-maker trying to make out they're some kind of sophisticated quantitative based concern, using the virgin islands to skirt lax US tax laws.  Associated sleazM people:  Michael Kiami, Kevin Dupigny, Ehren Henderson, Cary Krumholtz, Yasi Li, Uriel Cohen, Elli Ausubel  'nuf said"
MachineLearning,3aoo66,iori42,20,Mon Jun 22 06:26:33 2015 UTC,poor caption generator seems to have some childhood trauma related to a large clock.
MachineLearning,3aoo66,fhuszar,7,Mon Jun 22 10:39:37 2015 UTC,"Yo Clock, i heard you like clocks..."
MachineLearning,3aoo66,kioopi,3,Mon Jun 22 11:10:41 2015 UTC,This is getting pretty meta. We need to go deeper.
MachineLearning,3aoo66,CarbonAvatar,2,Mon Jun 22 11:55:28 2015 UTC,It seems a bit silly that they used some of the latex image tables instead of just cropping out the images themselves.
MachineLearning,3aoo66,zmjjmz,1 point,Mon Jun 22 12:47:59 2015 UTC,Hence the collages and windowsills.
MachineLearning,3aoo66,Transfuturist,1 point,Mon Jun 22 18:30:13 2015 UTC,"A statue of a man and a dog on .   Of course it wont talk about the thing they are sitting on. I mean, look at it! It makes me want to wash my eyes with soap"
MachineLearning,3aoo66,spr34dluv,1 point,Mon Jun 22 16:03:25 2015 UTC,"Paper Here  Words in the captions seem to be linked to some kind of filtered version of the image, but for coherent sentences they must be using some kind of language generation ('A' isn't in images). [Turns out it's a NN]  That certain captions do describe the contents of the picture isn't impressive by itself. Impressive examples can easily be cherry-picked. I suspect it's been over-fitted (with a clock image)."
MachineLearning,3aoo66,baetawolf,1 point,Mon Jun 22 11:54:59 2015 UTC,Now print it out and send it to an contemporary art exhibition.
MachineLearning,3arekv,Afritus,6,Mon Jun 22 22:10:44 2015 UTC,"In parametric modelling you use your training data to set the parameters of your model, then at test time you use the parameters (i.e. not the training data) to make predictions on test points.  In non-parametric modelling you make predictions directly as a function of your training data, so you need to keep it around at test time.  For example, kmeans is parametric because you use the training data to choose where to place the k prototypes, but then at test time you just need the prototypes to make predictions.  On the other hand, knn is non-parametric because you use the training data directly to make predictions at test time.  ""Non-parametric"" is sort of a weird name, because if you think of the training data in a non-parametric model as parameters (which is not an unreasonable thing to do) then ""non-parametric"" models are actually the models with the most parameters.  There are also situations where you have a non-parametric model that has parameters other than the data; you might have a bandwidth parameter in your kernel for a kernel density estimate, for example.  Bottom line: If you need the training data to make predictions at test time then the model is non-parametric.  If you can throw the training data away after training then the model is parametric."
MachineLearning,3arekv,kjearns,1 point,Mon Jun 22 23:50:55 2015 UTC,"That was a good and simple explanation, thank you!"
MachineLearning,3ar3ks,walling,2,Mon Jun 22 20:47:28 2015 UTC,I would suggest not trying to do anything special at the training stage (although if you're doing all unicode glyphs I've never tried to train a classifier with that many classes but that's a separate issue from confusion).  How much of a problem this will be will depend on the quality of the images you're recognizing.  In my experience (I used to work on postal address recognition) the confidences from the net output will probably be helpful but not completely reliable.  Post-processing the results with some sort of word level language model might help a lot (ie. I is much more likely between two letters than is 1).
MachineLearning,3ape67,john_philip,1 point,Mon Jun 22 12:37:22 2015 UTC,more than 1 TB of what?
MachineLearning,3ape67,ciolaamotore,1 point,Tue Jun 23 05:49:25 2015 UTC,I believe it is for click prediction
MachineLearning,3ape67,bge0,1 point,Tue Jun 23 08:33:26 2015 UTC,1 TB is not even big these days... It fits into memory
MachineLearning,3ape67,GoldmanBallSachs_,1 point,Tue Jun 23 06:40:16 2015 UTC,Why isn't this a torrent? This server can't handle everyone downloading this.
MachineLearning,3ape67,MorrisCasper,1 point,Tue Jun 23 11:06:51 2015 UTC,"""dataset on click prediction"" isn't exactly descriptive enough to be interesting."
MachineLearning,3aqkjz,ml__throwaway,1 point,Mon Jun 22 18:27:19 2015 UTC,"Isn't LDA designed for your task? It seems like default option for your task, why won't you employ this?"
MachineLearning,3aqkjz,olBaa,1 point,Mon Jun 22 18:30:17 2015 UTC,"There are a lot of options out there, I'll take a deeper look at it, thanks for the suggestion!"
MachineLearning,3aqkjz,bonoboTP,1 point,Mon Jun 22 18:33:00 2015 UTC,"A general suggestion: Don't force your users into a ""filter bubble"". Such suggestions and recommendations are really useful in moderation but when overdone they can make people have a very distorted view of the world, thinking that everything revolves around their interests."
MachineLearning,3aqkjz,bonoboTP,1 point,Tue Jun 23 14:50:44 2015 UTC,"Thanks! I was thinking 80-90% interest, remaining 10% things that could be interesting?"
MachineLearning,3aqcaa,techrat_reddit,3,Mon Jun 22 17:24:43 2015 UTC,Hinton's couraera course goes into this.
MachineLearning,3aqcaa,TheArvinInUs,1 point,Tue Jun 23 00:53:42 2015 UTC,"Specifically Lecture 2.  You need to visualize it, but it makes sense once you think about perceptrons in a geometric sense.  Hinton explains it very well."
MachineLearning,3aqcaa,madmooseman,2,Tue Jun 23 07:39:32 2015 UTC,"Intuitively:   Linearly separable data means there exists such hyperplane that gets two desired subsets of your data to also be the subsets of two halves of space that the hyperplane gives.  To find it, we start with an arbitrary hyperplane and imagine to rotate it about some ""origin"" of the data(zero element of the vector space that we estimated). Doing that, we notice that rotating continuously could help us also to go through the desired hyperplane.  Unfortunately, continuous rotation in an algorithm is impossible, and we would have to rotate in some discrete steps.  What discrete steps though? Rotating the hyperplane with arbitrary fixed angle size could help us go full cycle, but it could potentially step over the desired hyperplane that we're looking for (or at least take too long).  Instead, we decide to estimate the rotation steps from the data points. The most basic idea for one rotation step would be to take one data point and see whether its on the desired or the undesired side of the hyperplane.   If its on the undesired side, we rotate the hyperplane exactly by the minimal angle required to put it on the desired side. This way we go through all data points (that were originally linearly separable) and we know that we'll find (one of the) the hyperplane that separates them by rotation, because each point was put in the desired half by construction.  The subtlety lies, of course, in not knowing that the data is linearly separable, not knowing how to center it, having original points randomly misplaced etc etc. But that doesn't prevent one from running the algorithm."
MachineLearning,3aqcaa,laf17c0dx,2,Wed Jun 24 08:41:41 2015 UTC,"Thank you so much! That made it a lot more clearer. I knew perceptron algorithm tries to find the hyperplane that separates the data and that the hyperplane gets adjusted for each data, but never made the connection between the two. Thinking it in terms of the hyperplane ""minimally"" inching closer to the solution for each wrong data made it click for me."
MachineLearning,3aqcaa,dwf,0,Wed Jun 24 11:54:38 2015 UTC,Think about the 1-D case.
MachineLearning,3aqbp1,warriortux,1 point,Mon Jun 22 17:20:11 2015 UTC,"Edit: Ah I was wrong, the docs say cosine supports sparse input"
MachineLearning,3aqbp1,yggdrasilly,1 point,Mon Jun 22 19:53:21 2015 UTC,"Works for me  In [19]: pairwise_distances(t1_vector, t2_vector)  Out[19]: array([[ 0.81649658]])  ....  sklearn = '0.14.1'  numpy = '1.8.1'  scipy = '0.14.0'"
MachineLearning,3an007,suhrob,12,Sun Jun 21 20:39:01 2015 UTC,"""Fig 2b: An image of a lamp miss-classified by your dirty, dirty mind.""  ...guilty :("
MachineLearning,3an007,CarbonAvatar,5,Mon Jun 22 02:03:52 2015 UTC,"I just see a lamp, can you explain?"
MachineLearning,3an007,sulumits-retsambew,8,Mon Jun 22 03:43:45 2015 UTC,"Woman in a thong, legs spread."
MachineLearning,3an007,bang_equals,2,Mon Jun 22 04:16:29 2015 UTC,I didn't see the lamp. SO didn't see the legs.
MachineLearning,3an007,pherlo,11,Mon Jun 22 04:18:46 2015 UTC,"The difference is that all of the human optical illusions serve a purpose to help us deal with real-world situations better, whereas the noise that negates the machine's ability to recognize the image just shows it's not really using perceptual models yet."
MachineLearning,3an007,HINDBRAIN,11,Mon Jun 22 03:44:27 2015 UTC,"@fig1b  ""No fucking way. I'll just drag A over B and..."""
MachineLearning,3an007,sulumits-retsambew,13,Sun Jun 21 23:24:11 2015 UTC,Total brain fuck.  http://i.imgur.com/pfFGsMo.gifv
MachineLearning,3an007,PianoMastR64,3,Mon Jun 22 05:02:00 2015 UTC,"When you flick your eyes back and forth between the two, they seem to contrast more. When you focus on the space just between them, it becomes much easier to see them as the same color."
MachineLearning,3an007,InfinityCoffee,2,Mon Jun 22 06:51:50 2015 UTC,I find myself amused by the small typo in the title :) Apparently we are off to a bad start!
MachineLearning,3an007,Excaligo,1 point,Mon Jun 22 05:41:00 2015 UTC,"Can you please let me know what is the typo? I'll fix it, but don't see it..."
MachineLearning,3an007,InfinityCoffee,1 point,Mon Jun 22 07:17:16 2015 UTC,Replace 'in' with 'into'.
MachineLearning,3an007,scottlawson,1 point,Mon Jun 22 09:05:36 2015 UTC,"Yes, that. It's quite innocent, but I was left a bit confused when I first read it."
MachineLearning,3an007,chidedneck,1 point,Mon Jun 22 09:11:30 2015 UTC,"ah yes - I actually changed it in the blog title already yesterday, but can't change it on reddit unfortunately. Sorry about that."
MachineLearning,3an007,chidedneck,1 point,Mon Jun 22 11:52:43 2015 UTC,"Wow that completely changes the meaning of the title. Thanks for pointing that out, I was really confused at first."
MachineLearning,3an007,PianoMastR64,2,Mon Jun 22 09:10:40 2015 UTC,"Since the author insists the world's not discreet, I then must point out that s/he meant to use ""discrete."""
MachineLearning,3aq7ue,ghrarhg,2,Mon Jun 22 16:52:52 2015 UTC,"Try running it on your own data sets. Stoichastic gradient descent usually never reached the minimum completely, but just fluctuated around the minimum for me. However, it was still fast and provided just enough accuracy for me."
MachineLearning,3aq7ue,MusicIsLife1995,2,Mon Jun 22 17:21:17 2015 UTC,"Stochastic gradient descent usually never reached the minimum completely, but just fluctuated around the minimum for me.   This is usually the case with stochastic tools though.  I think that they are best used as a ""quick and dirty"" way to find the approximate value of a global optima, and then use a (non-stochastic) local search algorithm to find the actual minima.  This also cuts the solution time.  But that's my opinion, I've done some work in the area and that's what my research was in and showed.  For nonlinear discontinuous problems, you can really cut down your solution time by using a stochastic optimization algorithm (e.g. genetic algorithm) as a global search, then use the results from that as the inputs to another (non-stochastic) algorithm.  As long as your stochastic algorithm is configured properly (runs for long enough and has enough randomness/mutation/whatever you call it), you can be pretty sure you have the approximate value of the global optima.  Then just throw that value at a local search and boom: exact value of the global optima.  My work wasn't in machine learning, but process model optimization (Chemical Engineering). Still, hybrid approaches are not just limited to ChE."
MachineLearning,3aq7ue,robfox92,1 point,Tue Jun 23 07:36:43 2015 UTC,Wow! I never knew Chemical Engineers deal with such things. Sounds super interesting.
MachineLearning,3aq7ue,MusicIsLife1995,1 point,Tue Jun 23 16:50:26 2015 UTC,"We have models of plants (so in my case, an LNG plant), and you use those models to work out how the plant can be improved (what you can change to make it run better).  The plants/systems are highly nonlinear and often have many local optima.  Stochastic optimisation is a good tool for this sort of thing, as they're resistant to getting stuck in local optima.  It's still a growing field of research and I'm not sure if it's being used widely in industry yet."
MachineLearning,3aq7ue,robfox92,1 point,Tue Jun 23 23:22:33 2015 UTC,http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf
MachineLearning,3aq0e5,MajorDeeganz,-1,Mon Jun 22 15:55:46 2015 UTC,"Talk about a waste of precious brain cells.  We could be using AI to interpret satellite photos to find CO2/CH4 polluters that actually threaten us all but no - let's waste the limited resource on some backwards Puritanical nervous nellies and other bigots, cretin and ideologues who are so insecure and ignorant that they can't accept they are merely animals with biological parts."
MachineLearning,3aq0e5,mantrap2,5,Mon Jun 22 15:59:22 2015 UTC,I don't disagree with your feeling but given todays rampant sharing on social media these type of filters are important for children.  It is also a hard problem to approach which makes it exciting in itself.
MachineLearning,3aq0e5,gibberfish,1 point,Mon Jun 22 16:02:17 2015 UTC,"""21st century technology, meet 19th century morality""."
MachineLearning,3anzmw,CarbonAvatar,3,Mon Jun 22 02:06:26 2015 UTC,"I love how it kindof makes little choruses that it repeats  ""I'm going to make your body want to get your body with me."""
MachineLearning,3anzmw,jcannell,2,Mon Jun 22 02:48:19 2015 UTC,What synthesizer was used for this?
MachineLearning,3anzmw,kkastner,2,Mon Jun 22 03:51:24 2015 UTC,This is still better than OT Genasis
MachineLearning,3anzmw,in_the_fresh,2,Mon Jun 22 06:38:21 2015 UTC,This made me lol with great fervor.
MachineLearning,3anzmw,Silverstance,2,Mon Jun 22 09:40:12 2015 UTC,"As a Jay Z fan, this is awesome!"
MachineLearning,3anzmw,FanaHOVA,2,Mon Jun 22 10:03:27 2015 UTC,"You know what's funny? It's not that bad.  So assuming OP put this together, what tools did you use, what data, etc etc? Enlighten us"
MachineLearning,3anzmw,kidpost,1 point,Mon Jun 22 13:42:58 2015 UTC,"Sorry, I can't claim credit for it. I believe it followed Karpathy's RNN blog post (http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and was created by https://twitter.com/MrChrisJohnson."
MachineLearning,3apn7d,krimtheguy,3,Mon Jun 22 14:07:05 2015 UTC,"The costs for the cluster itself in EC2 (if you use spot bids) shouldn't be too much; you can probably run it for less than ~$10/month/node, depending on which type of instance you use.  Storage is a bit steeper, runs $0.05-$0.10 per GB per month, plus I/O and transfer fees, so you're looking at $25-50 just to hold your data in EBS with that.  You can set up billing alerts for your account (somewhere under ""billing and cost management"" from your EC2 dashboard), but I don't think it'll automatically kill anything for you, just give you an email when you're over some predefined threshold.  If you're going to use EC2, I'd highly recommend using starcluster: http://star.mit.edu/cluster/docs/latest/  It removes a lot of the hassle from spinning up instances, attaching storage, and wiring them together into a cluster.  I found this to be a really good step-by-step tutorial to getting started with starcluster:  http://zonca.github.io/machine-learning-at-scale-with-python/#/"
MachineLearning,3ape77,DyingAdonis,2,Mon Jun 22 12:37:40 2015 UTC,Sepp Hohreiter has done something like that.  paper
MachineLearning,3ape77,gergi,1 point,Mon Jun 22 13:10:19 2015 UTC,"The correct term is ""Model Compression"", and the most relevant paper (which Hinton's DK paper is based on) is this one"
MachineLearning,3ape77,BeatLeJuce,-3,Mon Jun 22 12:39:02 2015 UTC,"Careful, that's a path to self-aware networks."
MachineLearning,3alih5,alexeyr,5,Sun Jun 21 11:31:36 2015 UTC,I guess there is no video available anywhere?
MachineLearning,3alih5,defertoreptar,6,Sun Jun 21 12:10:26 2015 UTC,http://www.reddit.com/r/MachineLearning/comments/39scvl/whats_wrong_with_deep_learning_a_talk_by_yann/cs734vb
MachineLearning,3alih5,sixwings,-40,Sun Jun 21 14:44:00 2015 UTC,"Energy-based unsupervised learning? That's pathetic, IMO.  Every learning method must have a critic. We must abandon the top-down label-driven back propagation critic and adopt a bottom up, data-driven critic. There is only one viable candidate: signal timing. All of our sensors and networks must be designed to work with time.  Edit: To whoever is downvoting me, fuck you and the mule you sleep with. I don't care if Reddit bans me."
MachineLearning,3alih5,redditnemo,9,Sun Jun 21 17:47:57 2015 UTC,"As far as I understand the intuition behind energy based learning is ""Based on what I learned until today, what is the minimal effort change to accommodate the new information."", which does not sound bad to me. But maybe my understanding is lacking here. Can you elaborate what's wrong with this in your opinion?   There is only one viable candidate: signal timing   Are you sure there is only one viable candidate? Can you recommend any papers on that topic?"
MachineLearning,3alih5,noman2561,2,Sun Jun 21 17:56:50 2015 UTC,I never thought it was about minimizing the change in the model to incorporate the new data but about reconciling the model with the new data by minimizing the difference in the model's representation given the input parameters. So the input is treated as a set of parameters and the traditional input space is some unknown function of these parameters. The two are alternate representations of the same thing and the relationship between them is a statistical one. You could say it's minimizing the change in the model to incorporate the new data but that's always the case no matter what you do because of the nature of mapping an input space to an output space via optimization: the more points you map out the less your model has to change to incorporate new points.
MachineLearning,3alih5,sixwings,-22,Mon Jun 22 01:54:53 2015 UTC,"There is a reason that the retina is made of tiny center-surround photoreceptors that are sensitive to the direction of a stimulus. There is a reason that our eyes constantly move in tiny motions called microsaccades. There is a reason that we cannot see anything if nothing moves in our visual field. There is a reason that biological neurons communicate with pulses. There is a reason that every biological sensor is either a stimulus onset or stimulus offset detector.  The reason is timing. The idea that learning is a form of optimization will have to be retired soon, IMO."
MachineLearning,3alih5,redditnemo,8,Sun Jun 21 18:17:03 2015 UTC,"The idea that learning is a form of optimization will have to be retired soon, IMO.   But you said yourself that every learning method must have a critic. And then you adjust yourself to that critic in some direction. And again, if necessary. Until you find a solution that works. Isn't that optimization?  And if it is not, what should it be then?"
MachineLearning,3alih5,sixwings,-13,Sun Jun 21 18:28:10 2015 UTC,"When you make timing your critic, you immediately realize that there are only two kinds of temporal correlations: sensory signals can be either concurrent or sequential. IOW, the learning system should look for concurrent patterns and sequences of patterns. The system essentially detects repeated patterns and sequences in the sensory space. No optimization is necessary."
MachineLearning,3alih5,noman2561,3,Sun Jun 21 18:47:29 2015 UTC,"Sensory space is vast compared to the number of examples we have for mapping it. Optimization is necessary because we don't have examples of all possible inputs for a given task. If we did, we would be able to make a decision tree. Because we don't, we've got to fill in the remainder of the space and the framework for making those assumptions for states we don't have is accomplished through optimization whether analytic or not."
MachineLearning,3alih5,redditnemo,2,Mon Jun 22 01:36:29 2015 UTC,"The system essentially detects repeated patterns and sequences in the sensory space. No optimization is necessary.   How does it recognize a pattern? How does it decide what a pattern is (10Hz AM signal vs. clock ticking every second, both are patterns, how to decide the time base)?  What about noisy patterns? In most speech samples there is a lot of noise so it is not guaranteed you get a clean pattern. Say you learned a clean pattern for a word and then you compare it to what you heard but it does not match. What do you do then?  I just do not see how you can function without improving yourself (optimizing)."
MachineLearning,3alih5,poulejapon,2,Sun Jun 21 21:35:48 2015 UTC,Never heard about that. Is there papers about this?
MachineLearning,3alih5,Sampo,6,Mon Jun 22 00:20:23 2015 UTC,There is a reason that we cannot see anything if nothing moves in our visual field.   What?
MachineLearning,3alih5,noman2561,2,Mon Jun 22 00:19:48 2015 UTC,"I am having difficulty considering what you're arguing here. Would you mind elaborating on it quite a bit? I'm new to the field and have heard of a few ideas you might be phrasing differently from what I've heard.   I've heard about, and the presentation discusses, reoccurrent neural networks that capture memory. These work in time by capturing a notion of separating the past and the present. I've also read some papers on using the layers of a stacked network as steps in time so the signal washes over layers in the network at each step in time. It's like encoding a time series as input. That's another method I've heard about: directly encoding a block of time as an input to the network. The oldest use of this I can remember is using wavelets to capture the dft in audio signal segments. These days I guess you'd call that a feature derived from a markov chain with dependencies among the elements in the chain.   In all I'm really not sure what you mean and would really like if you might clarify what you mean."
MachineLearning,3alr08,fimari,4,Sun Jun 21 13:35:15 2015 UTC,this is very cool? do you know the link to the code? id like to 'see' my NNs
MachineLearning,3alr08,manueslapera,1 point,Sun Jun 21 14:41:53 2015 UTC,My question exactly!
MachineLearning,3alr08,bge0,1 point,Sun Jun 21 17:23:47 2015 UTC,"This is from :   *Object Detectors Emerge in Deep Scene CNNs * by Zhou, Khosla, Lapedriza, Oliva, & Torralba A 2nd Drawnet visualisation of an Image-net trained convnet  is available from Torralba's MIT homepage"
MachineLearning,3alr08,ford_beeblebrox,1 point,Sun Jun 21 15:00:06 2015 UTC,This is the coolest thing ever
MachineLearning,3aot90,tushar1408,1 point,Mon Jun 22 07:38:50 2015 UTC,None of this is easily convertible to python primitives and if you do it will be painfully slow. learn lua (it's really not that hard) or transcode using a python library
MachineLearning,3aot90,lmilasl,1 point,Mon Jun 22 09:00:04 2015 UTC,Thanks for the tip. I have worked quite a bit with Torch but never used python for neural nets
MachineLearning,3alddt,tareumlaneuchie,12,Sun Jun 21 09:48:47 2015 UTC,"One thing I learned is: log everything and save everything. Weights, Settings, whatever. Make a new python(?) script for every single experiment. Give it a good name. Always split test sets etc with a seed, include the seed and the split-percentage into the name. Save weights as soon as your validation error decreased.   Believe me, after a month or so, when you tried 500 things, you will thank yourself when you want to reproduce your best result so far from 2 weeks ago :-)  Also: Note precessily how you preprocessed your data for each experiment. Local contrast enhancement done BEFORE resizing or AFTER? sometimes that can make a difference.  its tedious, but thats ML."
MachineLearning,3alddt,lmf4o,5,Sun Jun 21 10:16:54 2015 UTC,"Someone should write a package to help for this, a git style lab book.   Agree with method. Without writing things down you're a tinkerer, with writing things down you're a scientist."
MachineLearning,3alddt,swilts,8,Sun Jun 21 12:13:13 2015 UTC,Someone already has: http://sacred.readthedocs.org
MachineLearning,3alddt,BeatLeJuce,2,Sun Jun 21 15:30:10 2015 UTC,"Yeah, I've been using ipython notebooks for a while now and will surely get around to implementing a Git version control sometime soon..."
MachineLearning,3alddt,FliesInHisEyes,1 point,Sun Jun 21 13:01:43 2015 UTC,"Thanks, this is indeed very valuable insights.  ML is finicky enough that you need to tinker before you get to a usable system, so you better be real good at the trial and error thingie."
MachineLearning,3alddt,bluecoffee,1 point,Sun Jun 21 11:35:12 2015 UTC,So you're basically doing revision control by hand?
MachineLearning,3alddt,lmf4o,3,Sun Jun 21 15:19:31 2015 UTC,no.
MachineLearning,3alddt,londons_explorer,5,Sun Jun 21 15:22:42 2015 UTC,"I try to run as many things in parallel as possible.  I have a simple script that fires up an amazon AWS instance, runs code, and saves the stdout and schedules the machine to shutdown.  I can easily have 5 or 6 experiments running at once.  I always run fixed-time experiments so I know I am comparing apples to apples, and I either set the threshold at 10 minutes (for interactive improvements) or 12 hours (for ""lets see how good this gets"" experiments).   I choose time rather than iteration count because usually I'm aiming for best performance per FLOP rather than best performance overall.  Oddly, some of the best performing results are often where there is a bug in my code (for example, incorrectly wiring one of the gates of an LSTM to the input rather than the previous layer).   That annoys me, because I can never seem to find a general case thats good, just a single example.  I'm still looking for something to smartly queue my experiments to try to minimize total $ cost, given that amazon charges for blocks of complete hours.  I suspect I could halve my bill if I could solve that."
MachineLearning,3alddt,dhammack,2,Sun Jun 21 14:25:03 2015 UTC,"Is this tenable when you have a lot of data, O(50gb)? Seems like you would spend too much time with data transfer."
MachineLearning,3alddt,londons_explorer,1 point,Sun Jun 21 16:07:11 2015 UTC,"I usually have ~5gig data maximum. (working mostly with text and sound - big image networks are the realm of people with more GPU resources than me...).  Also, over a 1Gbit network, it only takes 40 seconds to copy the data..."
MachineLearning,3alddt,quirm,1 point,Mon Jun 22 15:00:43 2015 UTC,Doesn't amazon also charge for data transfers?
MachineLearning,3alddt,londons_explorer,1 point,Wed Jun 24 13:33:42 2015 UTC,"Copy in once.  Then never move the data in/out of AWS.   Also a bunch of datasets are already available there.  The only ""output"" I need really need is the loss/ROC etc. numbers and maybe a few sample results.   Hardly big data.  If I had a serious use case, I would copy the model out.  Most models are pretty small (maybe a gigabyte maximum, but mine are closer to 20Mb compressed)."
MachineLearning,3alddt,fhadley,1 point,Wed Jun 24 15:44:38 2015 UTC,"I have come across similar setup on a couple of occasions, and I was wondering if this ends up being cost effective for you. I can certainly see you use 'sacred' for a couple of hours to help you run a couple of experiments, but I would be super afraid of the cost involved."
MachineLearning,3alddt,londons_explorer,1 point,Sun Jun 21 20:43:06 2015 UTC,"just re cost, have you thought about renting a machine on a monthly basis outside of aws?"
MachineLearning,3alddt,bluecoffee,1 point,Mon Jun 22 08:27:36 2015 UTC,"I usually want 10 machines to run experiments quickly on for an hour, then none again for a few weeks till I pick the project up again.  Thanks for the suggestion though - I do do that already in a way with random things I leave running on a VPS."
MachineLearning,3alddt,lmf4o,4,Mon Jun 22 14:11:22 2015 UTC,First: use source control. Learn the shortcuts for your source control frontend. Making a commit should only take a few seconds. Put the result of the current code in the commit message. Use a standard format that you can search for later. Commit after each experiment. Branch whenever you change something substantial. Merge when you're happy with the result.  Second: learn to use a hyperparameter optimization library. I like Spearmint. Experimenting by hand is fine; optimizing by hand is a bad habit.
MachineLearning,3alddt,MrTwiggy,2,Sun Jun 21 13:51:06 2015 UTC,thanks for the spearmint library. have you ever used it in a deep learning set where learning is really slow (1...n days)?
MachineLearning,3alddt,bluecoffee,2,Sun Jun 21 15:24:41 2015 UTC,"Might want to look into Freeze-Thaw Bayesian Optimization. Great method for quickly finding ideal hyperparameters with iterative/epoch based algorithms.  EDIT: To clear up some confusion from below, I do believe Spearmint offers an implementation of Freeze-Thaw, and it can handle plateaus (often faster) as normal bayesian optimization. The algorithm starts and stops training of various hyperparameter models depending on which it believes to be most optimal. It tends to reach similar areas as non-freeze-thaw (sp?), just faster for more optimistic paths."
MachineLearning,3alddt,dfarber,1 point,Sun Jun 21 18:15:42 2015 UTC,"DNNs are one of the use cases explored in the Spearmint papers. You don't run each trial for days though; because the error rate quickly approaches an asymptote, during optimization it's reasonably safe to stop training early."
MachineLearning,3alddt,farsass,3,Sun Jun 21 15:58:51 2015 UTC,Spearmint is not robust enough to trust for DNN optimization. I've seen Ryan Adams give a talk on this and his results are incomplete if not intentionally deceptive.
MachineLearning,3alddt,dfarber,2,Sun Jun 21 19:30:43 2015 UTC,what makes you think so?
MachineLearning,3alddt,dfarber,2,Sun Jun 21 20:11:31 2015 UTC,"he presented his results on a certain image dataset as SOTA, when they in fact were worse (not terrible but definitely inferior)."
MachineLearning,3alddt,qkdhfjdjdhd,2,Sun Jun 21 22:44:21 2015 UTC,"also, this:   because the error rate quickly approaches an asymptote, during optimization it's reasonably safe to stop training early.   is simply not true. training can plateau for many epochs then suddenly drop and continue improving. it's simply not possible to know early on exactly how good a run will ultimately be."
MachineLearning,3alddt,dfarber,1 point,Sun Jun 21 23:05:43 2015 UTC,You should be very careful about implying that a reseearcher is being intentionally deceptive.
MachineLearning,3ane7s,invert3r,11,Sun Jun 21 22:45:07 2015 UTC,https://xkcd.com/303/
MachineLearning,3ane7s,tyggerjai,6,Sun Jun 21 22:55:46 2015 UTC,Have you heard of reddit?
MachineLearning,3ane7s,towerofterror,3,Mon Jun 22 00:16:47 2015 UTC,"I have often thought about implementing some live-updating graph or something, showing the progress of for example the error function. But, naaah"
MachineLearning,3ane7s,mot-juste,3,Sun Jun 21 23:24:30 2015 UTC,Work on a paper. Review others' papers. Tweak the parameters and start a second job in parallel. Prepare a lecture. Do a literature search. Actually read a few of all those papers I've already collected.
MachineLearning,3ane7s,JanneJM,4,Mon Jun 22 01:08:46 2015 UTC,"No no. First you have to collect the papers from where you dumped them around the office. Then you have to catalogue them, then shelve them, then realise they're out of date and dump them again. Who had time for reading?"
MachineLearning,3ane7s,tyggerjai,1 point,Mon Jun 22 10:36:44 2015 UTC,Your models must take a LONG time to train..
MachineLearning,3ane7s,PoddyOne,3,Mon Jun 22 08:05:13 2015 UTC,"Serious answer: I've always found it good to have a few project on the go at the same time. Generally I have one 'hard' and one 'easy' project. The 'hard' one usually consists of scratching my head and scribbling notes, while the 'easy' one is basic coding and running models. I find this helps both with productivity and also stops me getting in a rut with the 'hard' project."
MachineLearning,3alzfk,nat47,2,Sun Jun 21 15:10:40 2015 UTC,Is learning XOR and squaring numbers proof that the network can learn correctly?
MachineLearning,3alzfk,JonBon13,1 point,Sun Jun 21 16:30:54 2015 UTC,"Not an expert, but I've found the following resources helpful:  http://yosinski.com/media/papers/Yosinski2012VisuallyDebuggingRestrictedBoltzmannMachine.pdf  https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf  They relate to Restricted Boltzmann Machines, but I expect it that much of the information applies to any Neural Network. (e.g. if the initial random weights aren't normally distributed the network will converge on a local minimum which takes forever to get out of)"
MachineLearning,3alzfk,mhummel,1 point,Sun Jun 21 23:01:31 2015 UTC,"So it sounds like you're feeding the words as integers. You need to either feed in a 1-hot vector, or an embedding vector.  e.g. if the word ""book"" has ID 1027 in your data, it doesn't have any particular relationship to the word at ID 1028, but you're asking the network to learn weights that do that discretisation for you.  Instead you should give it a binary vector, with a ""1"" at 1027, and 0s elsewhere. Or --- better --- use an embedded word representation. See Collobert and Weston (2011) for details."
MachineLearning,3alzfk,syllogism_,1 point,Wed Jun 24 03:23:23 2015 UTC,"For some reason I never saw your response... Sorry for being so late in commenting back.  The representations I'm using for words are directly related to their probabilities in my training set. So, its not a 1-hot, but its usually close.  I don't have IDs for each word, each word in the dictionary contains a list of what % that word was tagged as X number of POS tags. Most of the time, because of my data size, there are 1-2 tags, sometimes up to 5 occurrences. I don't have every single word represented by its own ID. If I were to do this, wouldn't I have to have an enormous number of input neurons?  Can you please explain a little bit more what an embedded word representation would look like?"
MachineLearning,3alzfk,changingourworld,1 point,Mon Jul 6 19:14:46 2015 UTC,"Not related to the question, but as someone who started learning Machine Learning in Java, because it was the only language I knew, I highly recommend learning python. It is far easier to quickly test, there are many more examples/tutorials and ML libraries.   I was a little intimidated at the thought of learning a new language, but was shocked at how easy it was. It's very similar with some minor syntax changes. Good luck!"
MachineLearning,3ailzi,yogthos,30,Sat Jun 20 15:24:48 2015 UTC,karpathy comments at HN: https://news.ycombinator.com/item?id=9749660
MachineLearning,3ailzi,gwern,1 point,Sat Jun 20 16:31:36 2015 UTC,"""This article would not come as a surprise to anyone who works with ConvNets.""   Paraphrasing Ray Mooney (from memory) during the the questions for Kwiatkowski et al. Scaling Semantic Parsers with On-the-fly Ontology Matching. EMNLP 2013: ""I really appreciate that you spent the time to discuss the lemons, as well as the cherries produced by your model.""  With this is mind, perhaps it would have been fairer to say:   ""This article would not come as a surprise to anyone who has spent enough time looking through the lemons in the ImageNet validation set. Perhaps we could have done a better job of discussing the lemons more often and in more detail.""   On the topic of cringe-worthy media:   Also, we carried out an experiment on ImageNet and the outcome was that ""One human labeler (me, incidentally) with a fixed amount of training and a slightly-above average determination reached ~5% top-5 error on a subset of ImageNet test set"". The media sees this and it immediately gets spun to ""AI now Super-Human. And we're all going to die."" It makes a lot of us cringe every time.   If we take a look at the Github blog in question, Karpathy takes the time to highlight his own take-home message in a blow-out at the bottom of the article:   ""It is clear that humans will soon only be able to outperform state of the art image classification models by use of significant effort, expertise, and time.""   Should it really come as a surprise that the media is so willing to report the super-human performance of ConvNets on object recognition? We, as a community, have to accept equal responsibility for how our research is interpreted by others. We must endeavour to do better in the future."
MachineLearning,3ailzi,votadini_,14,Tue Jun 23 10:14:18 2015 UTC,"My first thought was immediately: is there some sort of couch in the ImageNet set?  From the 1000 classes of imagenet 2014, there is in fact one couch-type object:  studio couch, day bed  So this is not just a simple case of there not being any couch like category.  However, ""studio couch, day bed"" is a pretty wierd looking couch.  In general CNNs trained on ImageNet do very well on  . . .. ImageNet, which is an artificial benchmark."
MachineLearning,3ailzi,jcannell,7,Sat Jun 20 17:28:45 2015 UTC,"I wonder if solving for a tree-like classification structure can be helpful. You'd need even more processing power because of more classifiers, but each classifiers will be choosing between a smaller set of classes. First you classify between a living object and non living object, then you find whether it's an animal, or a plant for a living object; a furniture, or a tool for a non living object. And finally you get to classify whether it's a leopard, tiger, or cat, OR a couch, a chair, a table, or a desk. If lower level classifiers work well then the next level's classifiers has waaay less classes to consider. So if the classifier determining a living object and non-living works well, there can never be a leopard classification for a sofa."
MachineLearning,3ailzi,Hexorg,13,Sat Jun 20 16:30:36 2015 UTC,Yes - this - tree-like structured prediction outputs could be a significant improvement.  Confusing a couch for any type of cat should be much more important for the score than differentiating between two breeds of dogs.
MachineLearning,3ailzi,jcannell,2,Sat Jun 20 18:13:18 2015 UTC,"This is a good hypothesis, and there's lots of research in this area, but it doesn't deal with the problem being discussed in the link, IMO.  The core problem is that convnets are inherently susceptible to these types of manipulations, because as karpathy notes, they are just texture recognizers. They are not object recognizers, or concept recognizers. A single convnet will only ever be a texture/patter recognizer.   What we need to figure out is the other support algorithms that need to be chained with a convnet to MAKE them object recognizers.  There's TONS and TONS of avenues for research in this field... it's a great time to be a Comp sci grad student."
MachineLearning,3ailzi,omniron,2,Sun Jun 21 06:34:42 2015 UTC,"The core problem is that convnets are inherently susceptible to these types of manipulations, because as karpathy notes, they are just texture recognizers. They are not object recognizers, or concept recognizers. A single convnet will only ever be a texture/patter recognizer.   This is just false - there are numerous objects in Imagenet that have no texture pattern at all, and convnets have no problem recognizing objects based on geometry.  In this particular case, the convnet learned to recognize leopards solely by texture pattern, presumably because this was the best use of it's limited neural capacity.  Don't overgeneralize from a single example."
MachineLearning,3ailzi,jcannell,1 point,Mon Jun 22 02:55:03 2015 UTC,"isn't ""mixture of experts"" the word for this? or ""Hierarchical Mixtures of Experts"" if you want a tree, and not just 1 split"
MachineLearning,3ailzi,suki907,2,Sun Jun 21 11:20:48 2015 UTC,MoEs are a nice idea but damned difficult to train in practice. Overfit like crazy.
MachineLearning,3ailzi,dwf,6,Mon Jun 22 05:40:13 2015 UTC,"Lets face it, ImageNet is a pretty bad test set nowadays.  Maybe grayscale 3d Images are more suitable for good training?"
MachineLearning,3ailzi,fimari,13,Sat Jun 20 15:50:13 2015 UTC,I am thinking about this a lot. How much information do we lose by training networks on static images? I feel like video of 3D motion must have more structure in it than the sum of structure in the individual frames.
MachineLearning,3ailzi,question99,4,Sat Jun 20 16:05:09 2015 UTC,"Seems like that data would be great for ""chunking"" of objects in images.  A similar thing could be said about text - people don't naturally speak in monotone.  As I learn a new spoken natural language, I notice that the tone of voice & meter of the syllables is incredibly useful in helping me do unsupervised learning of how to chunk the language, both at the word and sentence level. This chunking seems to be the first step whenever someone's learning their first spoken language. For the language I'm currently working on, the native speakers have an incredibly consistent and noticeable change in pitch over the duration of the sentence. Likewise, think of how we teach babies - that exaggerated frequency range we use when talking to babies is obviously conveying information that's very valuable to those early stages of training.  I think we're missing out on a lot of information, by ignoring that frequency & meter training data, when we do machine learning on natural written text."
MachineLearning,3ailzi,cryptocerous,2,Sat Jun 20 19:16:06 2015 UTC,"The invariance problem exists for prosody too. It is a source of information for sure, but you want to have a good idea of what it does first to build that into your model intelligently."
MachineLearning,3ailzi,khasiv,1 point,Sat Jun 20 22:37:28 2015 UTC,"I'll stick by my position - I think it's the exact opposite, if we want to eventually reach the goal of a human-competitive natural language understanding system, I'm almost certain it will involve incorporating in these types of valuable information.  For those super primitive systems, sure, you want to get rid of it, because those systems are dumber than a dust mite."
MachineLearning,3ailzi,cryptocerous,1 point,Sat Jun 20 22:53:41 2015 UTC,"""In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information"" - http://arxiv.org/pdf/1412.6056.pdf  This is work that aims to capture facts about the world like: ""A feather will fall to the ground when released."" and ""A stone falls faster than a feather."""
MachineLearning,3ailzi,vodkagoodmeatrotten,5,Sun Jun 21 10:47:36 2015 UTC,"Why grayscale - textures are still visible in grey.  The core problem is the training criterion - the texture hack issue is just a special case.  A more unsupervised criterion could avoid this issue.  For example - presumably something like deep reinforcement learning could avoid this problem, so long as correctly differentiating between things like jaguar vs jaguar-print couch is important."
MachineLearning,3ailzi,jcannell,5,Sat Jun 20 17:39:36 2015 UTC,"Sorry for the misunderstanding, I meant something like that: http://dnasuntech.com/wp-content/uploads/2015/01/horse-head-sample-gray.jpg"
MachineLearning,3ailzi,fimari,2,Sat Jun 20 17:56:05 2015 UTC,"People blind in one eye have very poor depth perception, yet otherwise they can recognize objects just fine."
MachineLearning,3ailzi,gustserve,3,Sat Jun 20 22:03:07 2015 UTC,They can still move fairly freely in 3D space to get an idea of depth though.
MachineLearning,3ailzi,antome,3,Sun Jun 21 00:35:58 2015 UTC,People are also able to recognize objects they have never seen before after looking at just one 2d picture. I think some folks put far too much faith on video and 3d.
MachineLearning,3ailzi,cafedude,1 point,Sun Jun 21 01:26:23 2015 UTC,"That's because they have a broad corpus of 3D object information contained in their heads, with which they can use to draw conclusions on the new, novel object.  If you supply a neural network with only 2D images of cubes, the conclusion it will probably draw is that cubes are squares with trapezoids attached."
MachineLearning,3ailzi,fimari,1 point,Sun Jun 21 06:49:15 2015 UTC,"What I said also applies to 2d shapes, such as diagrams or blueprints. To me it sounds like our vision has much stronger priors than a convnet. I don't know if Hinton's approach will be fruitful, but at least he's aware of these problems."
MachineLearning,3ailzi,manixrock,1 point,Sun Jun 21 10:48:52 2015 UTC,"When you say ""grayscale 3d images"" are you meaning depthmaps?"
MachineLearning,3ailzi,mantrap2,1 point,Sun Jun 21 05:50:57 2015 UTC,"""grayscale 3d images""    I think of the Blender menu option or what you get if you search google images for grayscale 3d images."
MachineLearning,3ailzi,dwf,5,Sun Jun 21 12:37:04 2015 UTC,"The problem seems to be that the network is being trained to classify abstract categories like ""leopard"" and ""jaguar"" with no information on what those categories mean.  A possible sollution would be to add dependent-pseudo-categories with dependent elements. For example matching a ""jaguar"" category would strongly depend on also matching a ""head"" pseudo-category, which would strongly depend on matching ""eyes"" and ""mouth"" pseudo-categories.  This way it would learn to match more like humans do. For us ""jaguar"" isn't an abstract category different from ""lion"", instead we know it has a lot of things in common with it. We know that visually a ""lion"" can be seen as a sort of ""jaguar with a mane"".  Of course the dependency graph/hierarchy would have to be provided, but this information could be necessary and not extractable from the images themselves. And if you think concepts like ""head"" should be automatically created within the network itself, as a hidden neuron node, I agree. However as the current networks are trained there simply is no need for a ""head"" concept neuron to be created when a much simpler ""round spots shape"" neuron works just as well, as the article points out.  Edit: In fact it should just train the network to predict the vector in a high-dimensional word2vec space. This way it can't predict leopard without closely predicting all the concepts related to it."
MachineLearning,3ailzi,gwern,10,Sat Jun 20 19:44:29 2015 UTC,"I remember this kind of thing from the 1980s: the US Army was testing image recognition seekers for missiles and was getting excellent results on Northern German tests with NATO tanks.  Then they tested the same systems in other environment and there results were suddenly shockingly bad.  Turns out the image recognition was keying off the trees with tank-like minor features rather than the tank itself.   Putting other vehicles in the same forests got similar high hits but tanks by themselves (in desert test ranges) didn't register.  Luckily a sceptic somewhere decided to ""do one more test to make sure""."
MachineLearning,3ailzi,ford_beeblebrox,12,Sat Jun 20 16:30:48 2015 UTC,"Variations on this story are everywhere. The other version I've heard is that all the non-tank photos were taken on a sunny day and the tank photos were taken on a cloudy day, or vice versa."
MachineLearning,3ailzi,gwern,6,Sat Jun 20 17:00:19 2015 UTC,It's a common urban legend. I and some others once tried to trace the original but was never able to get back earlier than 1998 or so.
MachineLearning,3ailzi,fimari,3,Sat Jun 20 18:01:22 2015 UTC,"Marvin Minsky relates a version at 1:57 in the video Embarassing Mistakes in Perceptron Research - US military contractor, tanks but again no named details.  The video begins with a similar case but this time an Italian using a perceptron to distinguish composers being fooled when the perceptron was differing distinguishing library stamps."
MachineLearning,3ailzi,cryptocerous,2,Sat Jun 20 22:55:27 2015 UTC,"That's still recent, Jan 2011. And while Minsky says he personally knew the Italian in question and spotted the overfitting, he doesn't make any such claim about the original tank legend (just the vague ""A similar thing happened here in the United States at one of our research institutions."" and his version differs from the forest one, indicating memetic evolution like urban legends)."
MachineLearning,3ailzi,muktabh,1 point,Sun Jun 21 19:23:27 2015 UTC,I think this https://neil.fraser.name/writing/tank/ is the most common version.
MachineLearning,3ailzi,dwf,3,Sat Jun 20 18:02:27 2015 UTC,"This whole argument makes me imagine someone saying ""that cloud looks like X"", and then another person arguing back ""no it doesn't!""  That's the question. Did we train this network on ""looks like X"" or ""is entity X""? I'm betting the former was the goal, and that if we actually put some effort here into training for the latter goal, it wouldn't be too difficult."
MachineLearning,3ailzi,log_2,4,Sat Jun 20 21:16:46 2015 UTC,"I just saw this comment where it the sofa is detected correctly on imageidentify. sofa image identified .  At least in case mentioned in article, wrong results look like the problem of ImageNet being a small dataset (it has just one image in one sofa-like category) for all objects in the world. Wolfram/Google Image search have better datasets, hence their algorithms identify correctly."
MachineLearning,3ailzi,Noncomment,4,Sat Jun 20 21:32:57 2015 UTC,"Not a surprise at all. Not only is ILSVRC a pretty closed world kind of task space, leopard print is so uncommon even in an open world setting that it's not surprising that something fairly unsophisticated as a convolutional net would be fooled. And yes, they are deeply unsophisticated; the fact that they work so well is basically just proof that the computer vision emperor had no clothes."
MachineLearning,3ailzi,NeverQuiteEnough,2,Sat Jun 20 16:58:53 2015 UTC,"I wonder what a photo of a TV showing a leopard in a nature documentary will be classified as using an ultimate recognition algorithm. Will it say ""leopard"", ""TV with a leopard"", ""Samsung UE65F9000 with the David Attenborough 2013 Africa documentary on frame 16323 played during the day""... etc. Should we accept nothing but the latter, or would we be happy with the first or second response? Where do we draw the line if the latter is too specific and saying a couch is a leopard is not specific enough? After all, the couch's surface area is mostly a leopard."
MachineLearning,3ailzi,jurniss,2,Sun Jun 21 04:26:20 2015 UTC,"There will always be weird edge cases where any algorithm fails. The more non-human the algorithm, the stranger it's failure cases will seem to us.  There is a very well known bias, where humans forgive human errors way more then they forgive algorithm errors. To the point where they prefer humans, even when they do 10x worse on a given task. See Algorithm aversion."
MachineLearning,3ailzi,fasterthanlite,1 point,Sun Jun 21 06:36:57 2015 UTC,"photography of cheetahs is not often done to demonstrate the efficacy of their camouflage.  if we examine the images, none of the cats are very obfuscated."
MachineLearning,3ailzi,treebranchleaf,1 point,Sat Jun 20 20:13:01 2015 UTC,"I thought it was pretty obvious that convolutional neutral networks are a low-level feature recognizer, not a full solution to the object recognition problem.  humans incorporate a model of world plausibility when trying to disambiguate tough real world object recognition tasks. you can catch yourself doing it next time you are looking at something far away trying to figure out what it is.  I also think there's a lot richer data we can pull from a dcnn besides the final classification response."
MachineLearning,3ailzi,bluecoffee,1 point,Sat Jun 20 20:30:35 2015 UTC,This seems like the problem that zero shot learning attempts to address... Its a shame that the zero shot attribute labels are so hard to come by. Great blog post!
MachineLearning,3ailzi,masharpe,1 point,Sun Jun 21 13:16:54 2015 UTC,"CNN cannot distinguish between a cat sitting on the floor and a cat sitting on the ceiling upside down   Not quite true - CNNs are not translation invariant (they are translation-equivariant), though the max-pooling operation does make them more robust to translation.  They are definitely different from ""bag of features"" approaches that disregard all spatial information.  It just appears that in this case, the print pattern was a better discriminative feature than the body appearance.    The real issue here has less to do with CNNs than it does with discriminative vs generative training.  If a network is being trained with classifying labels as the objective, it makes sense if a simple pattern is a good predictor, the network just learns that pattern, and does not need to learn the more abstract body-pose stuff.  In a generative model the objective is to learn the underlying distribution from which the images are coming.  It should learn a density model for categories like ""leopard"", so it should (one would hope) be less easily fooled by by the couch-trick."
MachineLearning,3ailzi,maxToTheJ,0,Sun Jun 21 22:11:24 2015 UTC,That's some very impressive conclusion-jumping.
MachineLearning,3ailzi,sixwings,0,Mon Jun 22 09:19:44 2015 UTC,"Rotating the image is cheating. It's not like human vision is rotation invariant. If you think so, try reading upside down. Not as easy as reading the normal way if you haven't practiced it."
MachineLearning,3ailzi,jcannell,3,Sat Jun 20 19:47:12 2015 UTC,"Exactly. When I see an upside down cat I think ""whats up with that bat?"""
MachineLearning,3ailzi,sixwings,-4,Sat Jun 20 22:01:45 2015 UTC,Very nice study. One of the biggest differences between the adult human brain and a DNN is that the brain can instantly model a new pattern it has never seen before. That's with ZERO training on the new pattern. The cortex is like an instantly malleable virtual reality putty.
MachineLearning,3ailzi,jcannell,5,Sun Jun 21 01:28:11 2015 UTC,"One of the biggest differences between the adult human brain and a DNN   There's also the fact that current DNN's are 1000 time smaller.  Also, nobody intended a deep CNN to be a model for the whole brain, or even for all of vision."
MachineLearning,3ama7v,Hexorg,8,Sun Jun 21 16:51:46 2015 UTC,"You may want to look into NEAT and HyperNEAT, I wrote a package for both myself a while back, it works really well. As far as I know, most existing NEAT packages do the evolution for you, the one I made just provides genotypes/phenotypes. So it depends on what you want. Anyways, here is a link the the NEAT site: http://www.cs.ucf.edu/~kstanley/neat.html  And here is a link to my NEAT/HyperNEAT package if you are interested (C++11): https://github.com/222464/HyperNEAT"
MachineLearning,3ama7l,DarkAnHell,3,Sun Jun 21 16:51:41 2015 UTC,The output of a neuron is fed in as an input in the next timestep. You may want to look into things like LSTM (long-short term memory) and BPTT (backpropagation through time) to make it work properly though (if you are using backprop to train your networks).
MachineLearning,3ama7l,CireNeikual,1 point,Sun Jun 21 17:02:32 2015 UTC,"Oh okay, thank you. I'll look up those methods, too :)"
MachineLearning,3ama7l,alex_ml,1 point,Sun Jun 21 17:35:32 2015 UTC,"The simple answer to your question is, yes, you should save the values of the intermediate values. It is possible to optimize your code to not store all of the values of the series. Eg. if your recursion equations have only one timestep (eg x_{t+1} = x_t + 1), then you can store two values and alternate writing over these. However, if you are trying to backprop through time, then you should just save all the values."
MachineLearning,3ajcwh,mattsains,5,Sat Jun 20 19:32:43 2015 UTC,Original video  Original comment thread on this subreddit
MachineLearning,3ajcwh,ford_beeblebrox,8,Sat Jun 20 19:35:58 2015 UTC,"Early results on /u/SethBling 's Twitch stream suggest MarI/O learns new levels faster as the genome begins to contain more near-solution parts, ( this test is only on subsequent levels at the same difficulty - but the effect is quite pronounced ).  Some of the best nets from 24 hours training on a single level can get most of the way through another level - each with a different solution.  Togelius suggests ( in his 2009 paper Super Mario Evolution which uses Notch's Infinite Mario as a benchmark rather than ROM emulation ) averaging the best agents performing 4000 trials ( over 4 unseen levels ( 0,3,5,10 ) - for 1000 differently seeded runs each ) is sufficient to establish generalisation ( of the evolved nets ) after each generation.  AFAIK MarI/O has not faced such an exhaustive test.  Togelius uses HyperGP which evolves the network weights compared to NEAT's evolving both topology and weights and Togelius's agents train on 4 levels for 100 generations of 100 individuals each compared with MarI/O's 36 gen.s of 200 on a single level.  MarI/O uses distance and speed for fitness Togelius 2009 uses distance alone.  MarI/O has trinary vision: platform, empty, sprite. Togelius 2009's agents perceive the sprite types.  None of the agents from Togelius's initial paper completed a level which they had not seen - this may be explicable by Infinite Mario's much greater stochasticity.  Togelius' benchmark became the basis of a yearly Mario AI competition.  Both NEAT and HyperGP allow recurrent connections so memory units can potentially evolve.  In 2014 A Neuroevolution Approach to General Atari Game Playing, by Hausknecht, Lehman, Miikulainen & Stone, compared NEAT, HyperNEAT and other algorithms and concluded :   "" [our results]... imply that many Atari games can be well-learned using a compact and well-tuned network such as the ones NEAT is capable of creating as long as the representations are low-dimensional. ""   MarI/O's experiences with NEAT so far appear to concur with Hausknecht 2014 that NEAT is good at retro-gaming.  In Neuroevolution in Games: State of the Art and Open Challenges 2014, Risi & Togarius  provide a comparison of many methods, including NEAT."
MachineLearning,3ajcwh,ford_beeblebrox,2,Sat Jun 20 20:17:34 2015 UTC,"Not mine, it's /u/SethBling's!"
MachineLearning,3ajcwh,ford_beeblebrox,3,Sat Jun 20 21:47:09 2015 UTC,http://www.twitch.tv/sethbling - Seth's MarI/O experiments  Other twitch streamers are running Modified MarI/O's performing other tasks :   http://www.twitch.tv/firzen14 - modified fitness to 'total blocks seen'. http://www.twitch.tv/matthewmatical http://www.twitch.tv/sebshs  http://www.twitch.tv/winterbunny - MarI/O plays Mario Cart BBC Click features MarI/O
MachineLearning,3ajcwh,NOTWorthless,3,Sat Jun 20 21:30:20 2015 UTC,"I managed to get MarI/O's lua working in Bizhawk on a windows vista system.  MarI/O is a great fun and accessible introduction to Ken Stanley's NEAT - IMHO watching the nets run at the top of the screen helps make some details of the paper clearer & develop intuitions about the behaviours from different topologies of network.  Seth's Lua code runs in the Bizhawk emulator only on Windows though fraustratingly won't run over RDP ( though Mario does ) which has ruled out EC2 - and Bizhawk on Linux with Mono doesn't yet support Lua.  MarI/O demonstrates TAS scriptable emulators have utility as benchmarks.  As TAS emulators readily run code across a wealth of game systems including the ATARI 2600, this makes experimentation across a vast trove of classic game ROMs feasible.  Running classic ROMs means comparison of results from different system like the ALE can be direct."
MachineLearning,3ajcwh,Arryk,3,Sat Jun 20 22:08:34 2015 UTC,"I think my initial reaction (maybe others as well) to the first one was that since the methodology was promoted in terms of buzzy AI language, it should be thought of as a success to the extent at that lived up to that language. For me, AI means generalization.   As an optimization algorithm, capable of finding new glitches and new paths in levels, maybe it is great! The optimization problem certainly isn't trivial, i.e., it isn't obvious at all how one would go about overfitting even if they wanted to. And there is a lot of scope for improvement as a learning algorithm as well. I can imagine it would be useful to randomize the structure of the levels, train on different levels within generation/between generations, and I'm sure a bunch of other things. So, nice generalization may be within reach."
MachineLearning,3aknmj,thebluehedgehog,3,Sun Jun 21 03:23:28 2015 UTC,I think deep mind mentioned it somewhere. I also have read that someone used machine learning to try to predict how a human would have played and it was pretty decent at go and not terrible at chess.   Here is deep-pink: https://github.com/erikbern/deep-pink
MachineLearning,3aknmj,heltok,3,Sun Jun 21 07:08:50 2015 UTC,Learning to play chess using reinforcement learning with database games chessprogramming - Learning
MachineLearning,3aknmj,NasenSpray,2,Sun Jun 21 15:16:32 2015 UTC,"You could build one with Machine Learning. For each board position train it to predict the change in points value for the next few moves into the future, or the winner of the game. It wouldn't beat the brute force methods though I doubt."
MachineLearning,3aknmj,simonhughes22,2,Sun Jun 21 03:30:11 2015 UTC,Would there need to be a points system? Could the bot 'recall' a game in which a move + response pair led to defeat and try an alternative option until it discovered a way to win consistently? Something akin to trial-and-error?
MachineLearning,3aknmj,Articulated-rage,2,Sun Jun 21 03:43:21 2015 UTC,"Yes.  You're talking about it associating a reward signal with game states.  I. E.  Reinforcement learning. It would propagate points to the states,  but it is not necessarily the points the other poster was talking about.  So saying ""not points""  isn't quite correct.  It's just a difference in how you're modeling  the learning problem."
MachineLearning,3aknmj,nkorslund,2,Sun Jun 21 05:35:09 2015 UTC,"I went to a lecture by a developer on one of the state-of-the-art chess computer programs a couple of years ago. I was expecting some high-level AI stuff but it was all basically alpha-beta pruning and other brute force optimizations.  Someone in the audience asked if they hadn't tried neural networks or similar ML methods. The answer was that ""yeah there were experiments with that years ago, but it didn't go anywhere.""  Which leads me to believe that the chess AI guys aren't aware of the huge strides the ML field has made in the last few years. There's probably a lot to gain from combining the two methods. Humans use heuristic pattern matching to filter out the interesting branches of thought from the uninteresting ones - and I suppose ML could do the same as a way of further optimizing the tree pruning process."
MachineLearning,3aknmj,DanceOnGlass,2,Sun Jun 21 08:34:56 2015 UTC,Aren't montecarlo rollouts sota nowadays?
MachineLearning,3aknmj,kjearns,1 point,Sun Jun 21 09:18:28 2015 UTC,"One of the challenges here is making deep nets fast enough to be helpful.  A step of MCTS is extremely quick, so you sacrifice a lot of search steps to evaluate even a single board position with a deep net.  Right now STOA performance still favours more search over stronger single position evaluation."
MachineLearning,3aknmj,ford_beeblebrox,1 point,Sun Jun 21 11:58:03 2015 UTC,"In the Go domain a step of MCTS is fast but only in vast aggregate are they useful.  Typically 10 or 100 thousand playouts are averaged with MCTS.  Both Clarke & Storkey's or Maddison, Huang, Silver & Sutskevers's deep convolutional Go agents produce softmaxes across the whole board in a single forward pass, so predictions are very quick compared to MCTS.  Of course Chess is very different, the Go tree branches much more."
MachineLearning,3aknmj,kjearns,2,Sun Jun 21 12:41:50 2015 UTC,"This is true, but notice that the pure-softmax players are much worse than the MCTS players.  You can boost the performance of an MCTS player with a good evaluation function and get much better performance / rollout that way, but if you stick a big deep net in there then your rollouts are so much slower that you still end up losing in terms of performance / second.  I'm speaking about go also, I actually don't know how well my claims translate to chess.  I suspect the pure-softmax convnet based approach will not work as well in chess as in go because a single chess piece can project power much more globally than a single go stone, but at this point I'm just speculating."
MachineLearning,3aknmj,hughperkins,2,Sun Jun 21 16:11:50 2015 UTC,Check out David Silver's Treestrap: - his lecture on learning to play games: http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/games.pdf  Very good :-) - a presentation just on treestrap: learning to play chess from self-play http://www.cse.unsw.edu.au/~cs9414/15s1/lect/1page/TreeStrap.pdf
MachineLearning,3aknmj,ford_beeblebrox,1 point,Sun Jun 21 07:33:01 2015 UTC,"and the paper:   Bootstrapping from Game Tree Search Veness, Silver, Uther, & Blair   Deepmind's David Silver is currently SOTA on Deep Nets playing Go and Reinforment Learning on Atari , this paper is great, many thanks for posting it.  Treestrap is notably the first chess agent to learn through self-play from entirely random initialisation and no encoded domain knowledge."
MachineLearning,3alt3k,robinhoode,11,Sun Jun 21 14:02:26 2015 UTC,Do money making projects.
MachineLearning,3alt3k,MusicIsLife1995,2,Sun Jun 21 14:43:11 2015 UTC,"I agree. Getting results is what nontechnical people understand, especially in business. Kaggle or something similar might work."
MachineLearning,3alt3k,binomialdistribution,4,Sun Jun 21 15:00:41 2015 UTC,So true. These projects are usually more straight forward and easier than technical ones too.
MachineLearning,3alt3k,MusicIsLife1995,1 point,Sun Jun 21 15:37:32 2015 UTC,"So basically image recognition, voice recognition, industry-specific applications etc? I'm more interested in NLP applications, but for some reason I keep thinking that your average non-tech only feels a ""wow"" factor when software is not based purely on text."
MachineLearning,3alt3k,dfarber,2,Sun Jun 21 20:35:01 2015 UTC,NLP is huge in advertising which is by far the biggest industry application of ML.
MachineLearning,3alt3k,MusicIsLife1995,-4,Sun Jun 21 23:17:58 2015 UTC,"If I were in your shoes, I'd do some analysis (for free) on start ups, giving advice on how they can make more money or what things they should optimize (or do less of). This would look really good on a resume if presented well."
MachineLearning,3alt3k,pradeepbv,1 point,Sun Jun 21 20:46:07 2015 UTC,"You may want to try out https://algorithmia.com/. Solve some ML problems, win some bounties and get work to show for itself."
MachineLearning,3alt3k,panhandelslim,0,Tue Jun 23 07:38:04 2015 UTC,"Oh, and simply showing your master's or PhD certificate doesn't count :P"
MachineLearning,3alns2,curious_scourge,2,Sun Jun 21 12:51:41 2015 UTC,"If you were to show those Googly eyed feedback images some weeks ago to someone who works in the domain he would possibly don't believe it came from a NN. Look at these comments. Truth is, there are some observations made on the blog post about a prior used for generating natural images that AFAIK is not known to the general community, so don't expect to generate anything near that with a github/code repo. But I am no expert, so maybe there's someone who could help you with it. If you want to start with NNs for images in general the most common tools are Theano (the library) and convolutional neural networks (the model)."
MachineLearning,3alns2,gsmafra,2,Sun Jun 21 16:01:18 2015 UTC,"Nasen Spray below does have good insight. However Berkleys Caffe has pretrained image net convnets... Also, google does describe in ""enough"" detail how to obtain such results. Randomly generate an image of noise. Feed the image into the network - and then if you want to have freaky looking banana for instance, the error in the softmax layer in the output should be computed as if the noisey image should have been a banana. Compute all of the gradients - but only apply the last gradient that applies to the input layer to the input image (your randomly generated noise). Keep repeating this until you see something interesting appear image of the noise (input layer). Not positive but I believe this is how it works.."
MachineLearning,3alns2,Powlerbare,2,Sun Jun 21 21:28:24 2015 UTC,"Not trying to be discouraging, but...  Training these kinds of networks can take a shitload of resources. Do you have a high-end machine (e.g. Nvidia GPU with >2 GB VRAM) and are you willing to let it run for days? IMO it's going to be considerably easier to just wait until someone else manages to replicate the results and is nice enough to release his model & code. Perhaps the guy who created this...? Please anon, that would be so awesome!   Note to anon: I reuploaded the image and checked that it can't be traced back to your reddit account. PM me if you want me to remove it anyway."
MachineLearning,3alns2,NasenSpray,2,Sun Jun 21 17:03:33 2015 UTC,"I haven't worked with images before, but the strategy I'd take:   So you'll need a large training set of pictures, like tens of thousands. I'd come up with a list of keywords and then download everything that pops up on google images for those searches (there are tools to automatically do that.)   Then try to find some image recognition models, preferably pre-trained ones, and run your entire training set through those models so each image is described by a number of ""objects"" with their relative coordinates. And then train a new predictive network based upon the relative coordinates for each type of object. Basically so if you have one know object, it's likely that there will be one of this other object 50 pixels to the right, and now knowing these two objects we're likely to have these other objects over here and etc.   Now the trickier part would be actually generating each of these objects that we want to place. Probably go through the same process, but training on individual objects and keep dropping down to smaller and smaller basic patterns.  I'll have to think on the details more.  Edit: and by objects I don't just mean the obvious but really all sorts of patterns. Yes you want to be able to recognize a house, but also sky vs water and tree vs trees vs open field.  And in generating objects, you'll want to start with some bank of ""fundamental patterns"", either create or randomly generate some 10x10 pixel mostly continuous patterns that can be plugged in to your image, and encode them somehow that can be easily trained  Edit 2: in pretty sure those images by google looked at every individual pixel, or maybe a small series of them, and went with the purely ""for what we have here, what pattern of pixels are most likely to go next to it?"" method which gives you a lot more of the randomness with recognizable images popping up in the middle. Organizing by medium sized patterns/objects could theoretically give you a lot more structure."
MachineLearning,3ajdcw,jcannell,3,Sat Jun 20 19:36:36 2015 UTC,"This is the logical conclusion the ""capsules"" idea that Hinton has been talking about for a while.  The connection is really clear if you read the capsules chapter in Tijmen Tieleman's PhD thesis.  Hinton is not the only one who is thinking about vision as inverse graphics; the probabilistic programming community really likes the idea too.  Vikash Mansinghka and Josh Tenenbaum at MIT in particular have some nice papers on this topic (e.g. http://arxiv.org/abs/1307.0060v1, http://mrkulk.github.io/www_cvpr15/).  There's also OpenDR, which implements a differentiable version of OpenGL.  In principle you could use this to train an auto-encoder where the latent code space is the parameters of a 3d scene model."
MachineLearning,3ajdcw,kjearns,2,Sat Jun 20 20:15:09 2015 UTC,"In principle you could use this to train an auto-encoder where the latent code space is the parameters of a 3d scene model.   To this: is it easy to get structured parameters, say for a 3d model,  out of an auto encoder?   Also,  isn't there literature that supposes we perceive the world via instantiation of objects which represent the world? Aka,  object oriented representations at the perceptual level.  I think ""object files""  has been thrown around."
MachineLearning,3ajdcw,Articulated-rage,2,Sat Jun 20 21:10:02 2015 UTC,"Also, isn't there literature that supposes we perceive the world via instantiation of objects which represent the world?   If you think about a simple animat navigating in something like an empty Quakeworld, the entire world is static so the video stream can be completely generated from (explained by) the dynamic camera pose parameters (position + orientation).  So you'd expect that a good sensor hierarchy would output those pose parameters at the top, and thats more or less exactly what you see in a rat's hippocampus with the place and direction cells."
MachineLearning,3ajdcw,elfion,1 point,Sat Jun 20 21:23:41 2015 UTC,"It has been done with hierarchical Slow Feature Analysis, an unsupervised learning technique http://www.ini.ruhr-uni-bochum.de/PEOPLE/wiskott/Projects/SFA-PlaceCells.html Pretty cool results, Jurgen Schmidthuber works with SFA as well http://people.idsia.ch/~luciw/incsfa.html"
MachineLearning,3ajdcw,walrusesarecool,2,Sun Jun 21 13:55:21 2015 UTC,Thanks! - this is the kind of stuff I'm looking for.  I'd read a little on Hinton's capsules idea but OpenDR in particular is news to me and your suggestion is intriguing.
