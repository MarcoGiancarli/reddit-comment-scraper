ServerPorn,3dj1zg,mitnworb,16,Thu Jul 16 17:51:03 2015 UTC,"Ohh, and direct from Cisco. I bet it was only $10K per stick!"
ServerPorn,3dj1zg,punk1984,7,Thu Jul 16 21:09:48 2015 UTC,I'm not even going to ask how much that RAM cost being that it came in a Cisco box :)  If only I could afford 3/4TB of RAM! Completely useless to me but damn it would be nice!
ServerPorn,3dj1zg,edneil,4,Thu Jul 16 20:49:57 2015 UTC,Happy unwrapping!  I always wish they could combine more so I don't have to spend 5-10 unwrapping the same thing.
ServerPorn,3dj1zg,xeon65,3,Thu Jul 16 18:37:34 2015 UTC,"lol ""adding"" 768GB. To how much initially?"
ServerPorn,3dj1zg,m4g1cm4n,8,Thu Jul 16 21:05:11 2015 UTC,This will give us a total of 2.25TB.  Should keep us going for a while.
ServerPorn,3dj1zg,m4g1cm4n,3,Thu Jul 16 21:40:45 2015 UTC,"I dunno, are you sure you're not under provisioning?"
ServerPorn,3dj1zg,Scarazer,2,Fri Jul 17 06:34:19 2015 UTC,They sent all of that in ONE BOX? What if that box got damaged? That would be a nightmare.
ServerPorn,3dj1zg,dxm765,1 point,Thu Jul 16 19:53:20 2015 UTC,Should i PM you for my shipping address? :P
ServerPorn,3dj1zg,mister_wizard,1 point,Thu Jul 16 18:33:19 2015 UTC,Blade servers?  How many gb of ram each if you don't mind me asking.
ServerPorn,3dj1zg,riddlerthc,2,Fri Jul 17 13:52:22 2015 UTC,384GB per blade.
ServerPorn,3b9k3z,brent78,11,Sat Jun 27 02:25:44 2015 UTC,"12 x HP DL380 Gen8 2xIntel E5-2680, 384 GB RAM 120 TB 3PAR, Adaptive Optimization across 3 Tier (SSD, FC & NL) 2 x Arista 7150 10GB L3 Switches 2 x Brocade 16GB FC Switches   Running about 700 virtual machines @ 70% CPU/Memory utilization, 12k IOPS.  Forget that hyper-converged noise, this environment delivers consistent high performance."
ServerPorn,3b9k3z,Vertikar,2,Sat Jun 27 02:29:22 2015 UTC,What PDUs are you using?
ServerPorn,3b9k3z,Vertikar,5,Sat Jun 27 10:26:35 2015 UTC,"Server Technology.  If you're still using APC, prepare to fall in love.  I've been using them for six years."
ServerPorn,3b9k3z,arcsine,1 point,Sat Jun 27 14:34:00 2015 UTC,They do look pretty nice!
ServerPorn,3b9k3z,arcsine,1 point,Sun Jun 28 14:23:17 2015 UTC,"Any particular type of VM? How's the latency? How do you like 3Par's tools? We're doing a 3Par PoC, but I'm working the EMC bid."
ServerPorn,3b9k3z,Kingkong29,2,Sat Jun 27 23:52:19 2015 UTC,"Latency averages sub 2ms.  EMC was the other contender, 3PAR won because of its superior mgmt tools"
ServerPorn,3b9k3z,FunnySheep,1 point,Sun Jun 28 02:31:12 2015 UTC,"Wow, very nice! I'll have to horn in on the 3Par demo on Monday."
ServerPorn,3b9k3z,FunnySheep,1 point,Sun Jun 28 02:45:31 2015 UTC,Two of those servers are rather naked with their missing face plates. That's okay... I don't mind them revealing all for me to see.
ServerPorn,3b9k3z,Kenjiee,2,Fri Jul 3 22:20:50 2015 UTC,That is fabulous. Can you disclose the total figure for this (maybe ballpark)?  What kind of applications run on this? More like regular corporate stuff or more cloud/web like stuff?
ServerPorn,3b9k3z,waldizzo,1 point,Sat Jun 27 10:01:42 2015 UTC,Around $300k
ServerPorn,3b9k3z,creamersrealm,1 point,Sun Jun 28 14:34:07 2015 UTC,Thank you.
ServerPorn,3b9k3z,creamersrealm,2,Sun Jun 28 17:26:58 2015 UTC,every time i Order a DL380 i hope it will be shipped with sexy bezel. Every time HP lets me down :(
ServerPorn,3b9k3z,hamsterpotpies,2,Sat Jun 27 12:19:48 2015 UTC,"On the G8's the ""security bezel"" is actually an add on that costs 60 bucks."
ServerPorn,3b9k3z,nerddtvg,3,Sat Jun 27 13:16:34 2015 UTC,"Not worth the extra cost, doesn't even cover the power button."
ServerPorn,3b9k3z,nerddtvg,2,Sat Jun 27 14:44:08 2015 UTC,"Nice, why are you not booting off SD cards though?"
ServerPorn,3b9k3z,arcsine,3,Sat Jun 27 13:26:13 2015 UTC,"Redundancy, we like RAID.  The power/cost savings is negligible at this small scale."
ServerPorn,3aq73u,SlipStream289,15,Mon Jun 22 16:47:10 2015 UTC,"That cabling leaves a lot to be desired.  UCS packs some serious power, do it justice!  :)  Cheers."
ServerPorn,3aq73u,dismorphic,2,Mon Jun 22 17:46:04 2015 UTC,Not the most attractive looking gear in the world but damn I love working with it! Nice setup.
ServerPorn,3aq73u,Lachiexyz,2,Mon Jun 22 18:17:54 2015 UTC,"Really speaks to what the UCS is all about, being drab and without character - but a dependable workhorse. Slap a light up bezel over it like EMC gear does, maybe?"
ServerPorn,3aq73u,fullofgreendust,4,Mon Jun 22 19:24:42 2015 UTC,We toss all the bezels on our servers. Wtf is the point of a piece of plastic that prevents me from seeing lights when it's in a locked building in a separately locked room in a locked rack? Other than adding more keys to my collection?
ServerPorn,3aq73u,linuxape,1 point,Tue Jun 23 01:17:56 2015 UTC,Mirror ball GO
ServerPorn,3aq73u,Gurumba,1 point,Mon Jun 22 21:52:59 2015 UTC,"You know damned well their designers and marketers came up with this design to imply exactly that.    That said, I do like the industrial look they have."
ServerPorn,3aq73u,skarphace,2,Tue Jun 23 17:28:20 2015 UTC,"I don't really grok the UCS lines right now, anyone have an idiots guide to UCS?"
ServerPorn,3aq73u,reiger,4,Mon Jun 22 18:17:57 2015 UTC,"If you have the ability to deploy a VM somewhere, get the UCS simulator. Then, whatever you want to try (within the confines of the simulator) you can do. Good luck, dude. I really enjoy deploying them."
ServerPorn,3aq73u,Gurumba,2,Mon Jun 22 21:54:34 2015 UTC,"Great advice, thank you!."
ServerPorn,3aq73u,reiger,1 point,Mon Jun 22 21:59:58 2015 UTC,"My pleasure. I wish I could recommend a book. Kinda would like something hardcore, myself."
ServerPorn,3aq73u,Gurumba,2,Tue Jun 23 00:15:08 2015 UTC,Cabling should be tied off better - running the power down the centre of the cabinet means if you have to do any work you have to shift/touch the cables.    Surf cableporn for good cabling examples and try to replicate it's worth the effort
ServerPorn,3aq73u,TenaciousBLT,1 point,Tue Jun 23 01:00:07 2015 UTC,Nice
ServerPorn,3aq73u,penislandbic,1 point,Mon Jun 22 17:43:33 2015 UTC,That sure is a lot of voicemail...
ServerPorn,3aq73u,brycied00d,1 point,Mon Jun 22 23:06:14 2015 UTC,my company installed one of those blade chassis with 4 nodes and it seems exceptionally quiet. Is that typical for those chassis? Was almost thinking of picking one up for my homelab it was so quiet.
ServerPorn,3aq73u,sexybeam,1 point,Thu Jun 25 18:39:52 2015 UTC,"Ye gods, that's awful. Tie the power cables off to the sides and secure that fiber at least."
ServerPorn,3a6wou,Casper042,3,Wed Jun 17 18:25:26 2015 UTC,This is only covering the Discover announcements. Most Gen9 models were either released late last year or earlier this year. You can leverage this other thread for some details of the 2014 launch: http://www.reddit.com/r/sysadmin/comments/2fvb0l/the_real_hp_gen9_launch/
ServerPorn,3a6wou,FinancialAdvicePleas,9,Wed Jun 17 18:40:26 2015 UTC,Upvote for banana for scale.
ServerPorn,3a6wou,codedit,1 point,Wed Jun 17 19:42:56 2015 UTC,/r/upvotedbecausebanana
ServerPorn,3a6wou,FinancialAdvicePleas,0,Wed Jun 17 21:20:06 2015 UTC,I was hoping that was real.
ServerPorn,3a6wou,BloodyIron,1 point,Wed Jun 17 21:30:56 2015 UTC,MXM in my blades? Madness!
ServerPorn,3a6wou,BloodyIron,1 point,Wed Jun 17 19:55:42 2015 UTC,"The double wide one has a special ""Multi GPU Carrier Board"" that only takes a single PCIe slot and has 3 full size + 1 half size MXM slot behind a PLX PCIe Bridge. So with 2 of those in the double wide GPU Expansion model, you can get 6 x K3100m today which is one of the more dense solutions for vDGA with decent horsepower."
ServerPorn,3a6wou,BloodyIron,1 point,Wed Jun 17 21:19:31 2015 UTC,Holy crap! What about PCIe bus saturation? o.O?!?!
ServerPorn,3a6wou,iaindings,1 point,Wed Jun 17 21:27:04 2015 UTC,"In Gen9 you have 16 lanes of PCIe 3.0 coming in, thats 16GB/s (over 100gbps) so you should be fine.  Keep in mind Quadro cards just 1 or 2 generations are were PCIe 2.0 which was only half the speed per lane.  For Accelerated VDI its usually not an issue to be honest."
ServerPorn,3a6wou,BloodyIron,1 point,Thu Jun 18 00:20:01 2015 UTC,"Ahh if the cards are generally PCIe v2 that makes sense, neat!"
ServerPorn,3a6wou,MDSExpro,1 point,Thu Jun 18 04:47:42 2015 UTC,http://www.amulethotkey.com/products/dxm630  If you prefer the Dell side of life
ServerPorn,3a6wou,arcsine,1 point,Thu Jun 18 04:10:59 2015 UTC,Neat!
ServerPorn,39inx9,mark73,17,Thu Jun 11 23:56:33 2015 UTC,The whole article is a solid read as well!   http://blog.serverfault.com/2015/03/05/how-we-upgrade-a-live-data-center/
ServerPorn,39inx9,ziggit,4,Fri Jun 12 00:49:24 2015 UTC,"The ""What We Learned"" section reminded me I'm not alone in all of my struggles with Dell."
ServerPorn,39inx9,Scarazer,2,Fri Jun 12 15:29:10 2015 UTC,"Thanks! That took a while to write up :) I was honestly just checking for any interesting posts from this week and stumbled upon my album here.  A heads up for the ServerPorn crew: several of our sysadmins (myself included) are headed to Denver tomorrow for another round of fun. This time we're moving our Oregon DR site to Denver. I'll do another writeup and we'll post another huge album of course, but if you want a live feed of server/cable porn as we work next week, check out #milehighops on Twitter.  Happy to answer any questions, feel free to ask here or any of our guys on Twitter (I'm @Nick_Craver there). Hope you guys and gals enjoy it!"
ServerPorn,39inx9,nickcraver,8,Sun Jun 14 02:10:05 2015 UTC,I believe this is from the Stack Overflow/Stack Exchange server migration/upgrade.
ServerPorn,39inx9,Slodominator,3,Fri Jun 12 00:46:50 2015 UTC,unf.
ServerPorn,39inx9,mal5305,2,Fri Jun 12 00:14:41 2015 UTC,Wow that looks like a fun deployment!
ServerPorn,39inx9,RedSquirrelFtw,-5,Fri Jun 12 19:51:31 2015 UTC,"Wow, all those Dell servers and not one DOA part? We've been using Dell hardware for years now and have an estimated ~30% failure rate with all dell parts we receive. Trying to go with another vendor but Dell servers are just so cheap"
ServerPorn,39inx9,Akito8,2,Fri Jun 12 00:39:06 2015 UTC,Are you buying refurbs?
ServerPorn,39inx9,regmaster,-1,Fri Jun 12 19:19:07 2015 UTC,"Nope, brand new servers. I think dells problem is that they don't soak test their hardware before shipping out the new gen stuff."
ServerPorn,39inx9,Akito8,1 point,Fri Jun 12 20:09:22 2015 UTC,"Yeah I've had the same experience.  Their servers are great when they work and support is great but sucks that their DOA rate is so high.  DOA tape drives are the best. ""is it suppose to sound like that?"" followed by the carousel jamming."
ServerPorn,38pywu,digimer,5,Fri Jun 5 21:13:03 2015 UTC,"We've got six of these being setup in HA clusters. These are our second set of RX2540 M1s and I am really happy so far. Very clean build, and having the ""onboard"" NICs as daughter boards is a nice touch.  The gap above the PSUs is for an option 4x 2.5"" HDD/SDD bay that we didn't need.  The lack of HDD stickers is a bit odd, but that's because we needed drives they don't normally stock. Doesn't bother us but figured I'd mention it as I am sure some eagle-eyed folks would notice. :)"
ServerPorn,38pywu,arcsine,1 point,Fri Jun 5 21:15:28 2015 UTC,"What kind of HA cluster, ESX?"
ServerPorn,38pywu,arcsine,2,Fri Jun 5 22:20:04 2015 UTC,A few Anvil! systems.
ServerPorn,38pywu,arcsine,2,Fri Jun 5 22:45:36 2015 UTC,"Huh, so a hypervisor with storage mirroring."
ServerPorn,38pywu,arcsine,3,Sat Jun 6 00:13:11 2015 UTC,"Pretty much, ya. There is also a WebUI, monitoring and alert system and so on that isn't covered in that tutorial (that's a ""meat and potatoes"" tutorial for people who want to understand the system in depth).  We prefer replicated storage because a SAN, no matter how good it is, is a single point of failure. There are plenty of stories of bad firmware taking SANs offline. Heck, we once had a user walk up to a node and start pulling drives out in the middle of the day. Hosed the node, but the servers on it kept running because DRBD simply flagged the local storage as ""Diskless"" and switched all reads/writes to the peer.  You can, of course, you dual SANs, but this setup has worked very well for us."
ServerPorn,38pywu,emanymton,2,Sat Jun 6 00:16:32 2015 UTC,"As a SAN guy, thanks for adding the last part."
ServerPorn,38pywu,emanymton,2,Sat Jun 6 00:20:15 2015 UTC,There are times where things like the large cache in a SAN makes sense.
ServerPorn,38pywu,Casper042,1 point,Sat Jun 6 00:28:50 2015 UTC,"There's a way to build a SAN for any purpose, from backup to OLTP to media."
ServerPorn,38pywu,moffetts9001,1 point,Sat Jun 6 00:32:37 2015 UTC,"I'm not sure why would you buy a production SAN that didn't have two separate controllers on it these days, but either way - I love your documentation and it seems like you guys do awesome work."
ServerPorn,38pywu,5mall5nail5,2,Sat Jun 6 18:41:34 2015 UTC,"Thanks for the kind words! We do our best. :)  Dual controllers help, but it's not enough to remove a single SAN as a SPoF. Unlikely though it may be, something like a cable harness short can still take a box offline.   Also, things like user error can take out an array (we had a user once pull three drives from a running node, hosing the array). If this case, DRBD marked it's local store as Diskless and seemlessly routed all further reads and writes to the peer node, so the servers on that host operated in production for another hour until we could schedule a controlled reboot (migration failed when the host lost it's OS).  Another case is the ability to do maintenance. When things are in a common chassis, even though it may survive a failure, you generally need/want to take the system offline to do repairs. With a common housing, this is notably more difficult.  So for us, if it's not two separate boxes, we won't use it. Fujitsu really wanted us to use their BX400, but we decided against it for the same reason. In the end, you need to not only have redundancy, but also the ability to totally down and remove any component without interrupting production.  You may call us paranoid now. :)"
ServerPorn,38pywu,5mall5nail5,1 point,Sat Jun 6 18:48:46 2015 UTC,"For all intents and purposes, controllers are completely separate pieces of hardware, on separate power supplies... like a 3PAR 7200 series for example.  This is how they do firmware upgrades without disruption to production storage traffic, as each controller can either serve the traffic for the other or each controller serves as one part of the redundant disk path.   Your setup would fall to the same sort of failures a dual controller (node) SAN setup would.. mainly fire/water damage or extended power failure.     All this being said I have personally experience a horrible firmware upgrade on our primary SAN, but it was cascading user error on the vendor side that caused it and not the design of the hardware itself.   If there's a way to break it, a human will figure it out."
ServerPorn,38pywu,5mall5nail5,2,Sun Jun 7 00:44:43 2015 UTC,"That is an impressive setup.  As for the fire/water damage, you are absolutely correct. In fact, part of our sales process is to inform the user that HA is on one of three parts. They still need disaster recovery and backup. Only with all three is a company protected. Our job, handling the HA component, is to make sure the HA part is as resilient as possible. Falling back to DR or, worse, BU is a complete failure on our part (barring a site disaster outside anyone's control).  With the setup below, it looks from the image as if it is, basically, two SANs in one chassis. Would I assume then that you can totally take one half off-line without impacting the other half? Of course, using two completely separate SAN units (as it appears in your image), then you've effectively replicated our setup (data in two places synchronously).   cheers"
ServerPorn,35av09,lsc,10,Fri May 8 15:49:25 2015 UTC,"http://i.imgur.com/hMeM2EY.jpg  Upped the exposure a bit, and white balanced. Still a bit out of focus/shakey. Great photo though!"
ServerPorn,35av09,ILiedAboutTheCake,4,Fri May 8 16:16:27 2015 UTC,"nice, thanks!   I never know when the out of focus is stylistic and when it's not.   hah.  it's time for me to take off for the dayjob. I'll be back this evening."
ServerPorn,35av09,PBI325,3,Fri May 8 16:22:57 2015 UTC,"Even though I have a few 1.4 lenses I rarely shoot that low. 1.4 is super hard to hit focus on unless you're in a controlled space and 2.2/5/8 looks plenty shallow in most cases :)  Look up free lensing!  Edit: Oh, you're the dude IN the photo haha We'll, if you see that dude tell again tell him that stuff or something..."
ServerPorn,35av09,ILiedAboutTheCake,2,Fri May 8 16:50:07 2015 UTC,"The Edit:  really makes the comment.     but seriously, as a amateur who has a DSLR (who doesn't, these days?)  what's the deal with the vibration reduction lenses?    would they solve this sort of thing?"
ServerPorn,35av09,yaleman,3,Sat May 9 02:56:34 2015 UTC,"VR (vibration reduction) can only help you out so far (usually 1-3 ""stops"" of light). Under 1/40th - 1/60th of a second, a tripod is recommended depending how steady your hand is."
ServerPorn,35av09,absw,10,Sat May 9 06:03:12 2015 UTC,"Someone that used to be my customer wanted to take some photographs (with his very fancy photographic equipment) of a real data center.     Like most co-lo facilities, this is pretty explicitly prohibited;   I asked my guy, though, and he said it was fine, so long as I didn't get other people's stuff in the picture.    Here, I took the doors of my four (5kw each) cabinets; so I think I'm following the rules.     Most of the servers aren't too impressive.  that's a 4 in 2u ""FatTwin"" full of e5 xeons (v2) with 128GiB ram each behind my head, with the crooked labels, (last four of the drive serial) which is the newest thing I've got, but it's a pretty nice datacenter;  It's the santa clara coresite location.  The raised floor is like 3', though it's locked down but good.   (there has got to be a half-pound of screws underneath the vents in front of my servers by now)    It's difficult to tell from the photo, but it's a cold-row isolation setup."
ServerPorn,35av09,GimmeSomeSugar,1 point,Fri May 8 15:55:52 2015 UTC,Yay cold row! :D
ServerPorn,35av09,Rothaga,3,Fri May 8 23:40:11 2015 UTC,"yeah, especially in California, we need more cold-row (or hot row) isolated datacenters.  it saves serious coin in terms of power, and allows you to go significantly higher-density.     But the cold-row there?  I mean, it's cool, as long as you don't move.  The place is mostly cooled evaporatively (I mean, they have regular a/c as backup, but the air is humid)  so if my partner comes to help and sits down using a chair, often they need a jacket in the cold row.  But I sweat like a pig when actually moving servers, because I don't handle humidity quite as well as the servers do.   (well, that and I'm kind of fat and out of shape.)    I'm so glad I don't live in the south."
ServerPorn,35av09,bobalob_wtf,1 point,Sat May 9 03:46:26 2015 UTC,nice to see my vps host on here. :)
ServerPorn,35av09,bobalob_wtf,5,Sun Jun 21 08:11:23 2015 UTC,It's a little unusual to see the OP in their picture when submitted to this sub.
ServerPorn,35av09,petulant_snowflake,5,Fri May 8 17:23:10 2015 UTC,"Oh no, not this again."
ServerPorn,35av09,Daslayah,2,Fri May 8 20:13:12 2015 UTC,Is that a Compaq Proliant G1?
ServerPorn,35av09,Borsaid,1 point,Fri May 8 17:24:52 2015 UTC,"where?  I don't think so.  If you came in today, you'd see a bunch of HP DL160s, (oh god, old hardware is so sad.  I have made some poor choices.)  but I try to avoid HP even used because of their shitty bios download policies.  the two rackables are still there, too.  (I like rackables, but these are way obsolete)     But I mostly run supermicro."
ServerPorn,35av09,skarphace,1 point,Sat May 9 02:54:43 2015 UTC,"I'm just joking, the white boxes look a little like the old DL380 G1s :)   http://www.usedcomp.de/infos/DL380a.jpg"
