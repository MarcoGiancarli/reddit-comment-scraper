GraphicsProgramming,3ducev,mreinfurt,7,Sun Jul 19 15:40:37 2015 UTC,"Hey,I've started learning graphics programming along with C++ and Direct3D 3 years ago when I started college and it was hell of a run going through every subject on my own.Since you have used SharpDX and implemented something I will assume you have no experience with native coding. First of all,if you want to learn about graphics programming in detail,picking up native approach would be the best.If you dont have enough time,I'd suggest going for UE4 or other existing engines since they do almost anything for you.If you want to learn everything in detail you will need a lot of time,patience and the will to move forward because hell it is too complicated and discourages you easily.Assuming you know C++ well,figuring out how to do things with native Direct3D code as you did with SharpDX will not be that hard.So my advice for you is pick C++ and Direct3D 11 , build a framework that you can use for numerous demos and build up your knowledge.After that you can switch to D3D 12 in the future.As a start point,Frank Luna's 3D Programming book will be the best since it covers most basic concepts as well as math and linear algebra."
GraphicsProgramming,3ducev,arkenthera,3,Sun Jul 19 18:00:00 2015 UTC,"Thanks for the response! Yeah, with SharpDX I didn't have to, for example, create the SwapChain, BackBuffer and so on. I still did have to manage my VBOs, set up the rendering pipeline etc - I also took a shader class in university and know the rendering pipeline and how to calculate the matrices (Projection, View…) in detail.   So yeah, I do want to learn everything in detail and that's also my main goal (I don't want to develop a game for example). That being said, I still want to take a path that makes sense in terms of getting a job in (game) graphics programming.  The book recommendation sounds very nice, gonna check it out!"
GraphicsProgramming,3ducev,RichieSams,5,Sun Jul 19 19:42:37 2015 UTC,"Frank Luna's book is great! One thing I would add, though, is that the code samples for the book are slightly outdated. For example, .fx files are depreciated, the D3DX library doesn't exist any more (though there are some awesome replacements made by some Microsoft employees), and you should be using the DirectX in the Windows 7/8 SDK, not the June 2010 version.  That said, the concepts in the book are still highly relavant. Just be aware that the code samples are outdated. IE, you may need to do aome Googling to figure out the new way to do some things. I'm open for questions if you get stuck or if you're just curious about something. Have fun!!!"
GraphicsProgramming,3ducev,arkenthera,3,Mon Jul 20 00:09:16 2015 UTC,"Yeah that book will start you and since you have some knowledge about pipeline,shaders and the basic math behind rendering a geometry you can easily go into more advanced stuff discussed in the book. Alongside with Frank Luna's book Practical Rendering and Computation with Direct3D 11  is like a gold mine.And topics discussed in the book are also implemented on a open source Windows only graphics engine called Hieroglyph . Studying the code after you have a basic understanding of how D3D API works will get you a lot of way.And if you get stuck somewhere feel free to PM me,I'll gladly help."
GraphicsProgramming,3ducev,r2d2rigo,1 point,Sun Jul 19 20:14:45 2015 UTC,And going the native way is better just because...?
GraphicsProgramming,3ducev,arkenthera,5,Sun Jul 19 22:12:56 2015 UTC,Because you get to know what each thing does what?I understand people dont like native coding because reinvent the tire stuff but OP wants to learn everything in detail.
GraphicsProgramming,3ducev,r2d2rigo,3,Sun Jul 19 22:23:25 2015 UTC,And what does his language of choice have to do with the ability to learn graphics programming? If it has up to date bindings that respect the vanilla API there is no reason to go back to C++. He could be learning modern OpenGL through JavaScript and WebGL if he wanted to.  Now for a lack of a better example take a look at this code (https://rastertekdx.codeplex.com/SourceControl/latest#Dev/Tutorial3/DX11Class.cs) which is a straight por of a Rastertek tutorial (http://www.rastertek.com/dx11tut04.html) to SharpDX. Is it even that different?
GraphicsProgramming,3ducev,arkenthera,3,Sun Jul 19 22:58:59 2015 UTC,"Well,yeah.It was simply my personal choice going directly into C++ but OP already knew about SharpDX and wants to learn more in depth thus I adviced C++.   there is no reason to go back to C++   There are lots of reasons to pick C++( pointers cough ) especially if you're doing graphics programming.For example large scale C# projects will operate slower compared to C++,although the work you do will be much less."
GraphicsProgramming,3ducev,r2d2rigo,2,Sun Jul 19 23:15:53 2015 UTC,"There is a difference between knowing SharpDX and knowing all of it ;) (been a user for 4 years and haven't touched 11.2 and 11.3 functions yet).  Actually SharpDX isn't entirely managed - I remember using unsafe pointers (and even wrapped COM objects, ugh) in some parts of it, but yeah, usually in C# these kind of things are learnt when you know the language/runtime really well and in C++ it's straight after basic data types.  OP also has stated that he feels comfortable in C++11, so what he chooses now is up to him!"
GraphicsProgramming,3ducev,Madsy9,2,Sun Jul 19 23:23:33 2015 UTC,"What language you use is of second or third importance in my opinion, but if one really wants to learn computer graphics, it's a good idea to learn the fundamentals. And the fundamentals is not to call some OpenGL or DirectX API function; it is writing your own line rasterizer and polygon rasterizer.  The advantage of using C++ or a language which similarly focuses on efficiency is that your code will run with more than 10 fps. You might care about interactivity while learning, or maybe you don't."
GraphicsProgramming,3ducev,r2d2rigo,1 point,Mon Jul 20 06:01:14 2015 UTC,"I'm sorry but you seem to have been using SharpDX the wrong way (maybe SharpDX.Toolkit, which is more XNA-like, than standard SharpDX?). It's only thought as a very thin wrapper of DirectX for .NET, intended for giving full access to all DirectX APIs to programmers that prefer C# instead of the native way - how you end using it is totally up to you.  Forget about DX12 and Vulcan for now, since they haven't been even released! And when they do, they will lack third party documentation; so, get a starting point with D3D11/OpenGL 4 to learn more details on the pipeline since it will help you greatly with the new approaches that those technologies will have.  Also, forget about existing engines unless you want to just focus on shader development, since they take care of all low level stuff and it isn't usually open for extension/replacement.  Going the native way is up to you, however I don't recommend it if you don't want to truly learn C++ and are already comfortable in C#. Modern wrappers of both DX and OGL are good enough that they respect the native API's structure and don't introduce much performance overhead.  I have written some post on how to initialize Direct3D with SharpDX for Metro/Modern Windows apps, maybe they will server you as a starting point to see how initialization from scratch is done:  http://english.r2d2rigo.es/2012/05/28/your-first-directx-11-metro-application-using-sharpdx/ http://english.r2d2rigo.es/2014/09/06/initializing-direct3d-in-windowswindows-phone-8-1-cxaml-universal-apps-with-sharpdx/"
GraphicsProgramming,3ducev,r2d2rigo,1 point,Sun Jul 19 22:12:08 2015 UTC,"Yeah, I was using MonoGame (which is basically XNA) prior to SharpDX - but then moved to SharpDX because XNA didn't support DirectX10+. SharpDX.Toolkit was very XNA-like and therefore I didn't have to change much.  I'm also comfortable in C++11 so when going deeper with DirectX/OpenGL it sounds pretty logical to also use C++ - of course using a C# Wrapper shouldn't be all too different anyway.  To DX12/Vulkan: It's not very long until they are released for the public, but yeah, will take a while until enough 3rd party resources are available.  To Engines: Yeah, that's what I'm afraid of - I like getting into the nitty-gritty details and how stuff works ""behind the scenes""."
GraphicsProgramming,3dpxi9,irabonus,0,Sat Jul 18 08:25:26 2015 UTC,Nice
GraphicsProgramming,3dh5eh,scswift,1 point,Thu Jul 16 06:03:19 2015 UTC,"I'm assuming you use some variant of grain merge directed by contrast/least energy paths. It's an interesting concept, photoshop uses it for different things (adaptive rescale).  If you updated your website and spiced everything up a bit you could probably make a bunch more money. Although the target market is relatively small (texture artists and the like) so there's never going to grow any kind of huge business out of it."
GraphicsProgramming,3dh5eh,pyalot,4,Thu Jul 16 06:27:40 2015 UTC,"Grain merge?  No, it's nothing like that.  This was way before they came up with content-aware scaling.  It's an algorithm inspired by wavelets (working at multiple scales) with a blending equation designed to maintain contrast in the luminosity channel.   And you're right, this will never be a huge business, but I think $1K a month is achievable and pulling in $1K a month for years with little additional effort beyond porting it to C++, adding a few features, and promoting it a bit ain't bad.  I mean it's been paying my grocery bills every month since I wrote it."
GraphicsProgramming,3dh5eh,protestor,1 point,Thu Jul 16 07:02:09 2015 UTC,Did you describe your algorithm in a paper somewhere? It sounds interesting.
GraphicsProgramming,3d12nl,willp,3,Sun Jul 12 17:16:32 2015 UTC,This looks like an awesome resource. I haven't had time to read through it fully; but it looks like it should be possible to implement in any standard real-time game engine? Is there any GitHub repo with all of the code?  Thanks for the writeup!
GraphicsProgramming,3d12nl,mattdesl,2,Sun Jul 12 19:20:14 2015 UTC,"Yasin Uludag, the author of the article about SSR in new Mirror's Edge that /u/willp based his implementation on, wrote in his conclusions that the whole algorithm takes ~0.75ms on GTX 670 in half-res (960x540) so it seems like a no brainer :)"
GraphicsProgramming,3d12nl,olej,2,Mon Jul 13 13:03:37 2015 UTC,"Hey /u/mattdesl,  The reason I left timings out of my post is that I have a very high end card, and since it's a hobby project I don't have a slew of PCs with varying specs to test on to get an accurate measurement.  /u/olej is correct, though, the implementation is based on Yasin Uludag's GPU Pro 5 article, and the timings he got for his implementation should at least be in the same ballpark as this implementation.  I have Uludag's method implemented in my engine and can toggle between the two and the timings for both are typically very similar.  Increasing stride length while decreasing the maximum number of steps is the biggest performance boost for this implementation, but you may need to add in a small binary search to the end of the trace if your stride gets really high.  I typically leave the stride from 3 to 8 and the maximum steps around 80 to 150, though most rays will never need that much (and my hardware makes this non-insane :) ).  The code is in a private repository on BitBucket for now, but mainly because I'm currently trying to add DirectX 12 support and things are a little ""interesting"" at the moment.  I went back to one of my tags and pulled out some client code that you're welcome to look at though.  It should at least give you an idea of the steps necessary on the client side to get the effect running.  I added a few comments about what certain classes are in my project, but if you have any questions, let me know and I'll be happy to answer them.  All the code is written in C++ and can be found here."
GraphicsProgramming,3d12nl,mattdesl,1 point,Thu Jul 16 22:06:16 2015 UTC,Thanks for the reply! I will keep an eye on your blog since it seems to be a pretty great resource.
GraphicsProgramming,3d12nl,orost,3,Tue Jul 21 14:09:37 2015 UTC,"Programmer art notwithstanding, that's a really good article. When I have hardware capable of doing such things I'll definitely have a go at implementing the technique based on your post."
GraphicsProgramming,3czfu8,Boojum,1 point,Sun Jul 12 04:29:55 2015 UTC,Thank you! This is a gold mine.
GraphicsProgramming,3bzmj9,vinnyvicious,1 point,Fri Jul 3 13:04:23 2015 UTC,"With animation retargeting, do you mean reusing one set of bones on several meshes?"
GraphicsProgramming,3bzmj9,Madsy9,1 point,Fri Jul 3 16:53:22 2015 UTC,"No. You have Skeleton A and Skeleton B. Two different humanoid models, but with different bones. You want to transfer animations from Skeleton A to Skeleton B, without having to reskin your model to A."
GraphicsProgramming,3bzmj9,Meristic,1 point,Fri Jul 3 17:43:58 2015 UTC,"I've never done this, and honestly working with bone transformation data is a pain. The main problems to overcome I see here are with the initial configuration of the bones (bind pose) and the difference in the number of bones in the model. If both of these were exactly the same, then a bone-to-bone mapping would be trivial, otherwise a pain.  The bind pose transformations determine the initial configuration of the skeleton. Animation transformations specify a local transform from the parent bone space to the child bone space at specific keyframes. Interpolating these matrices for the specific time required and composing them gives the final animation transform for the vertices. Since the animation transforms specify an offset from the initial bind pose configuration, if the initial configuration is different, the data would probably end up looking very weird.  For the mapping, suppose we have two humanoid models. One is very detailed and has 163 bones, another is a simplified human and has 59. How would you trivially map 163 bones to 59 bones, or vice versa? And this simplification includes modifying the animation transform data previously described to either compress or extend the parent and child bone spaces to the new mapping.  All in all, not trivial. You may be able to find some white papers on it, but I'm not sure what the term for this would be called if it did exist."
GraphicsProgramming,3bslhl,gott_modus,12,Wed Jul 1 20:28:17 2015 UTC,"Look at this image from the page you linked. It's quite clear what is happening: the shadowmap texels are aligned with the surface, but because of limited resolution, they have non-zero size so half of each ends up ""sticking below"" the surface, which causes the surface at that point to be considered shadowed.  If the shadowmap had infinite resolution, texels would be infinitely small points and the problem wouldn't exist. I'm guessing that's where your confusion comes from: acne is not inherent to the math of shadow mapping, it's simply an artifact of limited precision of implementation.  Adding bias simply moves the shadowmap ""up"" in relation to the surface, so that the texels don't dip into it any more."
GraphicsProgramming,3bslhl,orost,6,Wed Jul 1 21:10:31 2015 UTC,"Minor sticking point, but worth mentioning that the diagram has the shadows backwards. The parts of the surface that are lit in the diagram should be shadowed, and vice versa. The shadow map indicates the distance from the light source to the surface that the light is striking. Surfaces further from the light than the depth value sampled from the shadow map will be in shadow, but the diagram shows the opposite."
GraphicsProgramming,3bslhl,phort99,3,Thu Jul 2 07:53:46 2015 UTC,"There are actually 2 causes:   1) floating point number precision   2) sampling  The first one occurs when calculating the depth of a fragment which is then compared with the stored depth in the shadow map. Although the depths may correspond to the same fragment they are different due to floating point imprecision (calculation order, interpolation etc. all affect the result).  The second cause is the sampling issue and is the main cause. When you generate the shadow map, the depth values of fragments you store will not be the same fragments when you render from the camera. Thus, when you sample from the shadow map texture the value will not correspond exactly to your fragment. Resolutions as well as projection calculations also have an effect on this.  Bias fixes this problem as most of the time your invalid samples and depth values are within a very small interval of your calculated depth value which you use for comparison."
GraphicsProgramming,3bslhl,7h30n,3,Wed Jul 1 20:53:36 2015 UTC,"This is not exactly my subject of expertise, but if I'm interpreting that section correctly, it would seem that in the explanatory diagram the yellow staircase line is the depth as seen in the shadow map. Since you're basically taking an image from a different perspective to make the depth map for shadow calculations, the pixels in it won't match up to pixels as seen from the color rendering perspective when you project them out. So, when you check the surface that marks the border between light and shadow, it ends up interfering with itself -- some pixels get sampled as being in light, and some in shadow. The end result is that moire ""shadow acne"" pattern. If you just shift the threshold check slightly with the bias, it should avoid testing the surface that cast the shadow against the shadow threshold -- it obviously cannot shade itself if it's what's casting the shadow.  Someone please correct me if I am mistaken."
GraphicsProgramming,3bslhl,Figs,2,Wed Jul 1 21:25:43 2015 UTC,"If you don't want to use bias, you can render the shadow map using back faces instead of front faces (with glCullFace)  This won't work for billboards, but if your meshes are all watertight, it will look good. I did this for one project, didn't need any bias, looked great."
GraphicsProgramming,3btxsh,meith1,3,Thu Jul 2 03:09:17 2015 UTC,Why are you converting axis-angle rotations into quaternions?
GraphicsProgramming,3bcg4e,RichieSams,1 point,Sat Jun 27 22:14:11 2015 UTC,"Note that the site already made it into beta a couple of months ago, but was closed down soon after. This is a re-application because Stack Exchange has changed it's opinion on what constitutes a high quality Stack Exchange site so we might make it this time.  The last time it was simply closed because of not enough activity (even though the questions, and answers, were  highly interesting, of really high quality, and tons of professional representatives from various CG industries were on board) but SE seems to agree now that pure volume (like a certain number of questions per day) isn't necessarily representative of the quality of the site."
GraphicsProgramming,3bcg4e,IPlayEveryGame,1 point,Mon Jul 13 01:20:22 2015 UTC,"If at least 10 other users on this sub are willing to commit, I'll do the same.  I might cross post across sites though ;)"
GraphicsProgramming,3bcg4e,gott_modus,1 point,Mon Jun 29 18:03:45 2015 UTC,"Considering the upvote count and the trend of commits tracked on the site, I'd say your request is fufilled.  So go commit!!! :P"
GraphicsProgramming,3bcg4e,gott_modus,1 point,Mon Jun 29 19:46:08 2015 UTC,Maybe :). We'll see where things go.
GraphicsProgramming,3b4kxm,gott_modus,13,Thu Jun 25 22:34:44 2015 UTC,"I was in the same situation as you a few months ago. I ended up doing both!   http://dy-dx.com/softrenderer/ http://dy-dx.com/openfl-raytracer/demo/html5/   They're very basic & buggy but I only spent a couple days on each (and I don't have any kind of background in 3D graphics -- I'm just a hobbyist). It turns out you don't need very much time to get something simple up and running.  So if you've got a a little bit of time to spare, do both! And then just iterate on whichever is more fun to work on."
GraphicsProgramming,3b4kxm,dy-dx,4,Thu Jun 25 23:15:49 2015 UTC,"On this note.  Ray tracers are generally much easier to write, so you could probably get a basic one done in an afternoon.  Rasterizers are a little more complicated (not too much), so one will take you longer."
GraphicsProgramming,3b4kxm,ultrapingu,1 point,Fri Jun 26 06:55:38 2015 UTC,I like your ray tracer. Did you do the calculations on the GPU or CPU? The OP said he was looking to do something in real time but 715 ms per frame isn't exactly real time.
GraphicsProgramming,3b4kxm,nathanross49,3,Thu Jun 25 23:36:24 2015 UTC,Thanks! Everything is on the CPU. There is no webgl/gpgpu in those demos.  Smarter programmers can get much better results than me: https://cdn.rawgit.com/sschoenholz/WebGL-Raytracer/master/index.html
GraphicsProgramming,3b4kxm,dy-dx,2,Fri Jun 26 00:43:13 2015 UTC,"Not necessearily real time, at least as far as rayrracing is concerned. My thoughts are that learning raytracing would be a valuable experience which could prove useful in realtime scenarios. I'm not entirely sure how often raytracing is used in realtime rendering though."
GraphicsProgramming,3b4kxm,Madsy9,4,Thu Jun 25 23:59:20 2015 UTC,"From a knowledge perspective, learn both. Writing your own rasterizer gives you some key insights into how it all works and fits together.  From a practical perspective, learn what you think is most useful. But knowing how for example a GPU rasterizes polygon primitives gives you an intuition which makes it easier to understand how to achieve the results and effects you want. I also recommend studying computational geometry, linear algebra and topology even if just on the surface."
GraphicsProgramming,3b4kxm,nathanross49,1 point,Fri Jun 26 00:05:55 2015 UTC,"Thanks, I appreciate the insight."
GraphicsProgramming,3b4kxm,nathanross49,3,Fri Jun 26 00:13:29 2015 UTC,"Ray tracers are really hard to get running in real time without sacrificing resolution, scene complexity, or writing extremely complicated data structures. I personally started with rasterizers in OpenGL as an introduction to 3D graphics programming. It will familiarize you with concepts like textures, shaders, and anti aliasing without requiring you to cut corners for performance.  At the moment I am developing a Ray Tracing graphics engine for Android and everything I learned about shaders from working with rasterization helped me tremendously.  EDIT: You have experience with rasterizers already and you're just trying to learn more about how they work. Are you trying to write a razerizer on the CPU to see what steps are involved or are you hoping to write your first hardware accelerated graphics engine using OpenGL?"
GraphicsProgramming,3b4kxm,nathanross49,2,Thu Jun 25 23:17:09 2015 UTC,"I'm more or less looking to implement either a rasterizer or an offline raytracer using a GPGPU interface.  I've been using modern (3.x/4.x) OpenGL for 2 years, now, so I figure it'd beneficial to expand the horizons somewhat."
GraphicsProgramming,3b4kxm,fb39ca4,3,Thu Jun 25 23:48:45 2015 UTC,I've never written my own rasterizer but I've made a few ray tracers in OpenGL by drawing 2 triangles that cover the whole screen and using the fragment shader to ray trace. All of my geometry either needed to be generated on the GPU or stored in a texture some how.  I don't know much about rasterizers so I can't be much help there. I can show you the ray tracing resources I used.
GraphicsProgramming,3b4kxm,EldanRetha,1 point,Thu Jun 25 23:53:41 2015 UTC,"Sounds like a cool experience. And, yeah, I'd be happy to take a look at what you have available. Thanks!"
GraphicsProgramming,3b4kxm,__Cyber_Dildonics__,3,Fri Jun 26 00:03:13 2015 UTC,"Here's my current fragment shader I'm using for my Android application. I haven't been able to refind the sources I used.  The uniform block provides basic information like camera position and how to read the color and geometry textures. The vertex shader combines u_Forward, u_Up, and u_Right together to calculate v_Direction which is then interpolated across all the fragments on screen. I haven't experimented with calculating v_Direction in the fragment shader itself. The geometry texture stores each polygon individually with the polygon's position saved first, then vectors representing p1 - p0, then p2 - p0, and finally the normal value of the polygon to be read after the correct polygon has been found. To go to the next polygon you increment the 2nd element of the texture coordinate vector, because the first element is used to choose whether you're reading the position, offsets, or normal.  This really isn't the best but I hope it helps. I will try to find the resources I used.  #version 300 es precision highp float;  layout (location = 0) out vec3 FragColor;  smooth in vec3 v_Direction;  uniform sampler2D u_TexGeo, u_TexCol;  layout (std140) uniform RayTracingBlock {  vec3 u_Forward;  lowp float u_Start;   vec3 u_Up;  lowp float u_Quads;   vec3 u_Right;  lowp float u_End;   vec3 u_Position;  lowp float u_Inc;   vec3 u_LightPos;  lowp float u_RenderDepth;   vec2 u_tu; };  const float eps = 0.0001; //small epsilon const float EPS = 0.0002; //large epsilon  void FindIntersection(in vec3 rayo, in vec3 rayd, inout vec2 tu) {  //find closest intersection  for(float tex = u_Start; tex < u_End; tex += u_Inc) {   vec3 p = texture(u_TexGeo, vec2(0.125, tex)).xyz;   vec3 u = texture(u_TexGeo, vec2(0.375, tex)).xyz;   vec3 v = texture(u_TexGeo, vec2(0.625, tex)).xyz;   vec3 n = cross(u, v);    // get intersect point of ray with triangle plane   vec3 w = rayo - p;   float r = -dot(n, w) / dot(n, rayd);   w += r * rayd;    // get and test parametric coords-   float uu = dot(u, u);   float uv = dot(u, v);   float vv = dot(v, v);   float wu = dot(w, u);   float wv = dot(w, v);    float s = (uv * wv - vv * wu);   float t = (uv * wu - uu * wv);   float D = (uv * uv - uu * vv);    float ow=step(s,0.0)*step(t,0.0)*step(D,s+t)*            step(eps,r)*step(r,tu.x);   tu = mix(tu, vec2(r, tex), ow);  } }  void main() {  vec3 rayo = u_Position;  vec3 rayd = normalize(v_Direction);  lowp vec4 rayCol = vec4(1.0);   for(lowp float i=u_RenderDepth; i>0.0; i-=1.0) {   vec2 tu = u_tu;   FindIntersection(rayo, rayd, tu);   rayo += tu.x * rayd;   float isBackground = step(u_tu.x, tu.x);    lowp vec4 color = texture(u_TexCol, vec2(0.5, tu.y));   vec3 normal = texture(u_TexGeo, vec2(0.875, tu.y)).xyz;    vec3 lightDir = normalize(u_LightPos - rayo);   float polySide = sign(dot(rayd, normal));    //Shadow   float lightDist = distance(rayo, u_LightPos) + EPS*polySide;   tu.x = lightDist;   FindIntersection(u_LightPos, -lightDir, tu);   lowp float lit = step(lightDist, tu.x);    //Diffuse   lowp float diffuse = dot(normal, lightDir);    //Specular   vec3 halfAngle = normalize(lightDir - rayd);   float ks = dot(normal, halfAngle);   lowp float specular = pow(max(ks, 0.0), 50.0);    //combine shading   color.rgb = color.rgb * max(0.5, diffuse * lit) + specular * lit;   color.rgb = mix(color.rgb, vec3(0.53,0.81,0.92), isBackground);    rayd = reflect(rayd, normal);   rayCol.rgb = mix(rayCol.rgb, color.rgb, rayCol.a);   rayCol.a *= color.a;  }   FragColor = rayCol.rgb; }   Found one: Basic format of a GLSL ray tracer, heavily inspired my project https://gist.github.com/num3ric/4408481  Still looking for the ray-triangle intersection source I used.  EDIT: found the other  Ray-Triangle intersection http://undernones.blogspot.com/2010/12/gpu-ray-tracing-with-glsl.html"
GraphicsProgramming,3b4kxm,heyheyhey27,3,Fri Jun 26 00:19:54 2015 UTC,"Why not both? There's hybrid rasterizers/raytracers that use rasterization instead of casting primary rays because it is much more efficient, and then use raytracing for all the secondary rays (shadows, reflections, refractions) because they are more accurate."
GraphicsProgramming,3b4kxm,harveypekar84,2,Fri Jun 26 03:49:42 2015 UTC,"I have implemented both for a class. The first rasterizer was first, which I found very mundane considering I already had some experience with OpenGL. I'm not a heavy graphics programmer by any means, but I haven't found any advantage from that particular project. If it sounds interesting, go for it, but I personally don't find knowing how a rasterizer works very enlightening, and it hasn't affected my views on OpenGL at all.  The ray tracer I found much more exciting. If you implement enough to render triangles + textures and write a obj parser you can give incredibly gratifying results very quickly. To me, it was also a very different way of looking at computer graphics and gave me a much better understanding of how physically based renderers work. I would highly recommend a ray tracer over a rasterizer.  Also, I wouldn't worry about CUDA up front. It's not the worst idea in the world though."
GraphicsProgramming,3b4kxm,Rangsk,2,Fri Jun 26 00:43:55 2015 UTC,"I would start with a ray trace since it is easier.  You can always write a rasterizer after.  I also wouldn't worry about real time at first, you can always change what you have to be interactive.    I would also use a couple things like embree, an object loader library, a simple image writing library (stb image might be good).  If you need more linear algebra eigen is good too.  All of these will make things a lot easier.  You can always replace any piece if you really want to write it yourself."
GraphicsProgramming,3b4kxm,RichieSams,2,Fri Jun 26 12:47:00 2015 UTC,"I have no plans to get heavily into raytracing, because I'm more interested in realtime applications.   There are forms of ray-tracing/marching that are starting to be used in real time, if you really are interested in that. For example, Unreal Engine 4 offers some form of distance-field raymarching for shadows and ambient occlusion. And there are plenty of real-time ray-marched 3D scenes on ShaderToy.com that are fun to play with. And then there's ""Voxel Cone Tracing"", which is a form of real-time GI."
GraphicsProgramming,3b4kxm,solinent,2,Fri Jun 26 15:09:57 2015 UTC,"If you really want to pick, I would say, write a raytracer. Raytracing will teach you a lot about materials, sampling, aliasing, and other classic, important topics without being constrained by the gpu. Also, a simple raytracer is a lot easier to implement then a simple rasterizer, and then you can just pick features you like to implement.  To be fair, there's not a whole lot to rasterization apart from raw speed (that's the things that's important. If you know how to clip a polygon, rasterize a triangle, depth/stencil buffers, and do perspective correct interpolation, you're about 90% of the way of understanding the important theory of what a rasterization api does for you. Read ryg's blog for a fantastic breakdown of what gpu's actually do.  Not that implementing a rasterizer is not interesting. IMHO, there's a future where graphics will use about all three:   rasterization for the first visibility pass/simple shading by using the gpu/driver software rasterization for occlusion culling of above in many scenarios, on the cpu raytracing for secondary visibility/advanced shading (shadows, reflections, etc), implemented on the gpu   Doing all three would be a very interesting project"
GraphicsProgramming,3b4kxm,angrymonkey,2,Fri Jun 26 17:09:24 2015 UTC,"You may want to look into using NVIDIA's Optix. It's a raytracing API which helps you get where you want to go much faster. At least as someone just starting out, it's really not all that useful to try to learn the stuff it's doing for you, like complex scene graphs and such."
GraphicsProgramming,3b2utk,FaiIsnaiI,14,Thu Jun 25 14:56:47 2015 UTC,"For graphics theory I highly recommend Real Time Rendering for real time (I guess clear from the title) and Physically Based Rendering for ray tracing, but also physically based rendering in general."
GraphicsProgramming,3b2utk,TurkishSquirrel,1 point,Thu Jun 25 15:27:37 2015 UTC,"Literally what I was about to suggest - Real Time Rendering introduces nearly all of the relevant topics necessary for being involved with any sort of graphical techniques, while Physically Based Rendering covers the ground that's happened in the last ~4 years or so."
GraphicsProgramming,3b2utk,tylercamp,1 point,Thu Jun 25 19:01:26 2015 UTC,"I think I'll go with physically based rendering, thank you for the reply."
GraphicsProgramming,3b2utk,c0d3M0nk3y,5,Thu Jun 25 20:13:30 2015 UTC,I second what /u/TurkishSquirrel said and also add Fundamentals of Computer Graphics which i found very nice and readable
GraphicsProgramming,3b2utk,I_SPEAK_GEEK,2,Thu Jun 25 17:35:19 2015 UTC,"/u/TurkishSquirrel has probably the best suggestions.  Peter Shirley's Realistic Ray Tracing is quite good.  Suffern's Ray Tracing From The Ground Up treated one of my minions quite well when she was learning. (Woohoo for being a grad student, sometimes I get minions).  If you're concerned with the math, Lengyel's Mathematics for 3D Game Programmers and Computer Graphics helped me a lot. I suck at math."
GraphicsProgramming,3b2utk,brieder,1 point,Thu Jun 25 21:04:32 2015 UTC,Foley van Dam is sort of one of the defacto books.  http://www.amazon.com/Computer-Graphics-Principles-Practice-Edition/dp/0321399528/ref=dp_ob_title_bk
GraphicsProgramming,3b2utk,kernalphage,1 point,Thu Jun 25 21:33:33 2015 UTC,"I find myself going back to Real Time Collision Detection time and again. While not directed at graphics engines specifically, there are chapters on model/view transforms, architecture-aware optimization, Spatial partitioning, and of course, ray-[x] intersections. It's got a math primer in the beginning, but I only understood the proofs after beginners' Linear Algebra and some... calc...?"
GraphicsProgramming,3azafm,meith1,5,Wed Jun 24 18:25:10 2015 UTC,Where is glUseProgram call ?
GraphicsProgramming,3azafm,remiaaa,3,Wed Jun 24 18:34:28 2015 UTC,Thanks I just fixed that.
GraphicsProgramming,3azafm,remiaaa,6,Wed Jun 24 18:35:05 2015 UTC,Still white ?
GraphicsProgramming,3aro9m,vinnyvicious,6,Mon Jun 22 23:28:44 2015 UTC,Mitsuba
GraphicsProgramming,3aro9m,conjugat,1 point,Tue Jun 23 02:20:13 2015 UTC,Has anyone used this for realtime applications?
GraphicsProgramming,3aro9m,__Cyber_Dildonics__,4,Tue Jun 23 17:19:10 2015 UTC,You didn't mention real time in your post.
GraphicsProgramming,3aro9m,RichieSams,1 point,Tue Jun 23 17:51:34 2015 UTC,Sorry. :(
GraphicsProgramming,3aro9m,RichieSams,4,Tue Jun 23 19:09:50 2015 UTC,Unreal Engine 4
GraphicsProgramming,3aro9m,PrototypeNM1,4,Mon Jun 22 23:43:47 2015 UTC,"Sorry, i meant FOSS."
GraphicsProgramming,3aro9m,Allanon001,-6,Tue Jun 23 01:16:49 2015 UTC,"Still technically applies, since UE4 is free for a large number of scenarios. But I get your point.  The pbrt renderer is MIT-type licenced and supports all kinds of BRDF."
GraphicsProgramming,3aro9m,Saticmotion,6,Tue Jun 23 03:41:43 2015 UTC,"Ehh, free as in beer not as in freedom. Calling Unreal ""technically"" FOSS would be diluting the term."
GraphicsProgramming,3aro9m,Rhed0x,1 point,Tue Jun 23 05:24:34 2015 UTC,Paprika Render
GraphicsProgramming,3aro9m,bioglaze,1 point,Tue Jun 23 08:06:48 2015 UTC,Tungsten perhaps?
GraphicsProgramming,3aro9m,bioglaze,0,Tue Jun 23 15:25:11 2015 UTC,Not realtime. :(
GraphicsProgramming,3aonp0,usefulendymion,3,Mon Jun 22 06:20:44 2015 UTC,"I would highly suggest RenderDoc. It's free, open source, and in my experiences, more accurate and much faster than the Visual Studio Graphics debugger.   He has a set of tutorial videos on Youtube as well as some basic documentation. Let me know if you run into any problems; I'll be more than happy to help"
GraphicsProgramming,3aonp0,RichieSams,2,Mon Jun 22 23:50:17 2015 UTC,Enable debug info on the actual shaders. Are you using d3dcompile?
GraphicsProgramming,3aonp0,torrent7,1 point,Mon Jun 22 15:08:26 2015 UTC,Debug info is on on the shaders. Im not sure what d3dcompile is. I am using vs2013 and its HLSL compiler.  EDIT: apparently the debug info was off for release mode of the specific shader I was trying to debug. Enabled it and now I can debug! Thanks!
GraphicsProgramming,3aonp0,raydey,2,Mon Jun 22 16:05:23 2015 UTC,"What does the disassembly bring up? If it doesn't look like it's doing much, you may have highlighted the incorrect draw call in the events list.  Something else that may affect it is if you haven't included your shader files into the VS project and enabled them to be used by the HLSL compiler (even if you're using d3dcompile in your code).  Edit: You answered my latter statement."
GraphicsProgramming,3aonp0,raydey,1 point,Mon Jun 22 16:21:20 2015 UTC,Disassembly does bring up some code that resembles my shader.  I'm not very experienced with deciphering that though.
GraphicsProgramming,3aonp0,Meristic,2,Mon Jun 22 16:37:45 2015 UTC,"That's all good :) Another thing you could try is on the properties of the shader file (not the project properties), go to HLSL compiler > output files and put ""$(OutDir)%(Filename).cso"" in the Object File Name and do a recompile.  It may be the case that you already have something there, in which case you can move  the generated .cso file into the same directory of your .exe (if it's not already there).  (This is a complete guess based on the idea that the analyzer can't find the debug symbols needed to debug the shader - I think they're stored in the object file the HLSL compiler spits out)."
GraphicsProgramming,3aonp0,Meristic,2,Mon Jun 22 16:49:03 2015 UTC,How are you compiling your shaders? Make sure that you have shader optimizations turned off when building shaders for release mode. Also try passing D3D11_CREATE_DEVICE_DEBUG as a flag when calling D3D11CreateDevice.
GraphicsProgramming,3aonp0,raydey,1 point,Mon Jun 22 23:10:26 2015 UTC,"Shaders are compiled using VS2013 HLSL compiler. Also, I am using that flag already."
GraphicsProgramming,3aonp0,cleroth,2,Mon Jun 22 23:18:54 2015 UTC,"If you right click the shader file, go to Properties->HLSL Compiler->General, is Disable Optimizations set to yes for the release configuration?"
GraphicsProgramming,3aonp0,specialpatrol,1 point,Mon Jun 22 23:50:21 2015 UTC,"This option is set to no. However, I did notice that debugging information was actually turned off for release mode on this specific shader. I enabled it and am now allowed to debug the shader! However for some reason now it begins to debug ~20 lines into the shader. Any guesses on why this could be?"
GraphicsProgramming,3aonp0,specialpatrol,2,Tue Jun 23 01:48:34 2015 UTC,"You have optimization on. Compiler rearranges/replaces instructions to  reduce instruction counts/make it faster.  When stepping through the code, the debugger actually steps through the GPU assembly instructions. As the instructions have been optimized, the lines of the instructions may no longer correspond to the lines of the code. You can check this by going to the disassembly and in the window right click and select ""Show Source Code"" to view it in mixed mode.  Btw as /u/RichieSams suggested, please use RenderDoc - it's so much faster, easier and has way more features to use. The VS graphics debuggers pale in comparison."
GraphicsProgramming,3aonp0,eiffeloberon,1 point,Tue Jun 23 09:33:49 2015 UTC,Thanks so much! I have to use VS graphics debugger for this class but will absolutely look at RenderDoc for personal use! This subreddit is awesome!
GraphicsProgramming,3aonp0,specialpatrol,1 point,Tue Jun 23 12:46:01 2015 UTC,Enable debug info on the project settings.
GraphicsProgramming,3aonp0,specialpatrol,1 point,Mon Jun 22 08:53:21 2015 UTC,Done and no change
GraphicsProgramming,3ag53s,nifraicl,6,Fri Jun 19 21:37:02 2015 UTC,"Approximation of quadratic Bezier curves by arc splines D.J. Walton, D.S. Meek Journal of Computational and Applied Mathematics 54 (1994) 107-120    Bisection algorithms for approximating quadratic Bezier curves by G1 arc splines Jun-Hai Yong, Shi-Min Hu, Jia-Guang Sun Computer-Aided Design 32 (2000) 253–260    Approximation of a Cubic Bezier Curve by Circular Arcs and Vice Versa Aleksas Riškus Information Technology and Control, 2006, Vol. 35, No.4    Quadratic Bézier Curve Approximation By Circular Arcs Within A Tolerance Band Seyed Amir Hossein Siahposhha    And Glyphy"
GraphicsProgramming,3ag53s,thomac,2,Fri Jun 19 22:47:25 2015 UTC,"This video off of the Glyphy link is pretty great.  It is somewhat off topic since they are using the bezier curve to make a signed distance field, but still pretty cool, and does show some visualizations of what they are doing with bezier curves.  https://vimeo.com/83732058"
GraphicsProgramming,3ag53s,__Cyber_Dildonics__,3,Sat Jun 20 14:47:27 2015 UTC,I don't think it is off-topic: he first converts the Beziers to arcs and first then to SDF. See the picture at 7:29 and https://github.com/behdad/glyphy/blob/master/src/glyphy-arcs-bezier.hh#L99
GraphicsProgramming,3ag53s,thomac,1 point,Sat Jun 20 15:24:32 2015 UTC,"i really like this approach. Right now i'm trying to find to find a solution where non homogeneous section of a curve are converted to arcs, to minimize the number of arcs needed. Their approach is to increase the number of subdivision until a good enough approximation is found. Unfortunately is best for smooth curves, but maybe a mixed approach could work for my case, where i could subdivide a problematic bezier in more simpler subsections."
GraphicsProgramming,3ag53s,agumonkey,4,Mon Jun 22 08:58:07 2015 UTC,"I have no idea how to do but I love the question. I see using normals to find the center of a tangent (inside a certain error span) circle. ""Rolling a ball"". http://math.etsu.edu/multicalc/prealpha/Chap1/Chap1-8/PlaneCurvature.gif Is that what you were thinking ? I have troubles picturing the continuity between piecewise arcs though.  psedit: Maybe iterating 3 point search http://i.stack.imgur.com/OpDfm.png as long as the circle/curve error is below a certain threshold then defining a new circle center, getting the arc, so on and so forth."
GraphicsProgramming,3ag53s,Tywien,3,Fri Jun 19 22:44:19 2015 UTC,"This is about rendering cubic splines with the GPU, but they first transform the splines into simplier curves of the form k 3 - kl = k(k 2 - l), it might be helpful.   http://http.developer.nvidia.com/GPUGems3/gpugems3_ch25.html"
GraphicsProgramming,3aec6h,thomac,3,Fri Jun 19 12:53:18 2015 UTC,"If you want the 2-d equivalent to a  scene-graph in Skia, draw into a SkPictureRecorder with a bounding-box hierarchy.  SkRTreeFactory rTreeFactory; SkPictureRecorder pictureRecorder; SkCanvas* pictureCanvas = pictureRecorder.beginRecording(         SkRect::MakeWH(W, H), &rTreeFactory); drawEntireScene(pictureCanvas); SkAutoTUnref<SkPicture> picture(         pictureRecorder.endRecordingAsPicture());   Then drawing the picture back into the a target canvas will be accelerated by the bounding-box hierarchy.  Chrome uses this to dealing with scrolling on web page."
GraphicsProgramming,3aec6h,hwc,1 point,Sun Jun 21 03:00:20 2015 UTC,Thanks for the reply!  And what to do if an item moves? Do I have to re-record the entire scene?
GraphicsProgramming,3aec6h,hwc,2,Sun Jun 21 08:02:38 2015 UTC,"short answer: yes.  If you know ahead of time what parts will move and what parts won't, you can make a picture of the static content that is under the moving item and a picture of the static content that is over the moving item."
GraphicsProgramming,3aec6h,KeinBaum,1 point,Sun Jun 21 15:12:40 2015 UTC,"To maintain z-ordering you can use a z-buffer.  You could draw only items which have a bounding box that intersects with the screen. Alternatively you could use a quad- or k-d tree too make faster decisions what too draw. The drawback is that have a high overhead when something in your scene changes.  I'd try without the trees first. If it's too slow, implement a tree. If it has too much overhead, you could mix both techniques and use a tree with only a few levels to reduce overhead and then use bounding boxes for final decision making."
GraphicsProgramming,3aec6h,__Cyber_Dildonics__,1 point,Fri Jun 19 14:16:31 2015 UTC,"First of all, I would try just drawing everything and see how many fps you get.  You will probably be surprised at how good the performance is and you might find that you are searching for a solution to a problem you won't have (premature optimization).  If you use something like nanovg (or any open gl accelerated library) you should be able to draw a crazy amount on screen before it becomes a problem.  Even drawing points on glVertex at a time can give you millions at a decent fps.    Nanovg also has a openGL 3.0 backend which might store everything as buffers on the gpu.  That will be crazy fast without any culling on your part.    For overlap, you just want an order to draw them in.  When a node is selected, put it on the top and move everything else one down.  If this is linear in memory, it will also be very fast.    So basically, don't worry about speed yet, that is likely to be the easiest part."
GraphicsProgramming,3a9j1r,solidangle,3,Thu Jun 18 08:24:02 2015 UTC,"There's some cool new stuff in there. It also seems to contain (if I'm not mistaken) the first open source implementation of Photon Beam Diffusion, which is really cool. It's a bit hard to read through the code as there is no documentation (as the book isn't out yet), but it's cool stuff."
GraphicsProgramming,3a9j1r,thunabrain,2,Thu Jun 18 08:30:47 2015 UTC,"Very cool! I'll try to give this a go on Windows with gcc (via MinGW).  I'm very happy about the added hair support. Currently it only seems to have Kajiya-Kay implemented, but I'll try adding Marschner's/d'Eon's model. The interface seems fairly straight-forward."
GraphicsProgramming,3a9j1r,Delwin,1 point,Thu Jun 18 20:59:51 2015 UTC,"Yeah, the new stuff is sadly quite limited. There sadly is no VCM, there are only two Microfacet distributions, they seem to have replaced the Dipole, there is only one phase function and there is only Kajiya-Kay.  I guess they've reached a limit on how much stuff they can put in one book. They might have to follow Glassner's example and make two volumes for the next edition"
GraphicsProgramming,3a9j1r,Delwin,1 point,Thu Jun 18 21:36:32 2015 UTC,What compiler can you build this on?  I'm trying on GCC 4.7 with no luck.
GraphicsProgramming,3aa2sl,yvanc,5,Thu Jun 18 12:43:03 2015 UTC,"An ms paint clone sounds more like a challenge from the user interface point of view, rather than a graphics point of view and the easiest way to do this would probably be with the GDI.   Look into the Win32 libraries."
GraphicsProgramming,3aa2sl,unfeelingtable,0,Thu Jun 18 21:24:50 2015 UTC,I would even encourage using .net framework. Moving to C# shouldn't be a problem.
GraphicsProgramming,3aa2sl,pardoman,2,Fri Jun 19 01:52:54 2015 UTC,Why?
GraphicsProgramming,3aa2sl,DerDangDerDang,1 point,Fri Jun 19 11:10:20 2015 UTC,Solid GDI library. Building UIs is pretty straight forward.
GraphicsProgramming,3aa2sl,pardoman,3,Fri Jun 19 15:30:58 2015 UTC,"If you want to use c++ I'd take a look at Qt 5.x  for GUI and user input handling and use opengl (Not that this is required, but for learning) for rendering."
GraphicsProgramming,3aa2sl,Destidom,4,Fri Jun 19 02:24:28 2015 UTC,"Seems like basic knowledge is holding you back right now. There's a couple paths you can go by; I'd recommend looking at OpenGL in depth. Pick up a good tutorial book or try a site like www.learnopengl.com , and dive in.  Whatever way you go, there's a lot of overhead to learning graphics programming, so be patient at the start and really dig in to concepts like vertex and element buffers, shaders, and how a program is set up to speak to your graphics card. If you don't understand the basics you won't get very far at all. Make a few basic programs that can draw a simple triangle or something, and experiment with libraries like GL3W to make the setup a bit easier. Once you really have these concepts down you can gauge what you're capable of and start applying OOP principles to designing a paint app."
GraphicsProgramming,3aa2sl,bagoombalo,4,Thu Jun 18 15:08:43 2015 UTC,I'll add that grasping and embracing high school maths is very beneficial.
GraphicsProgramming,3aa2sl,Spiderboydk,2,Fri Jun 19 00:24:16 2015 UTC,Why OpenGL?
GraphicsProgramming,3aa2sl,DerDangDerDang,2,Fri Jun 19 11:10:44 2015 UTC,because it is everywhere even in mobiles and web.
GraphicsProgramming,3aa2sl,Kilgort,3,Sun Jul 5 06:25:00 2015 UTC,"The first step is to understand how images are represented. Raster graphics editors like MS Paint treat images as arrays of pixels where each pixel has a red, green, and blue component. 100% of all components together is white; 0% is black. Mixtures of red and green make yellows, etc. This is different from mixing paints -- see RGB color model for more info.  Here's one way to represent an image like that:  struct Pixel {     unsigned char r;     unsigned char g;     unsigned char b; };  Pixel my_image[640 * 480];   You could then address individual pixels like this (again, one of several options):  Pixel* get_pixel(Pixel* image, int x, int y, int width, int height) {     if(x < 0 || y < 0 || x >= width || y >= height)         return 0; // out of bounds      return &image[y*width + x]; }   MS Paint primarily works with BMP files. You can either look up the format and write your own save and load routines, or use an existing library. (stb_image and stb_image_write is one example that I've used for simple projects.)  After you have some way of representing images, saving, and loading them, you will probably want to write functions for various tools. Bresenham's algorithm is a common way of drawing lines. Another tool you might want to implement is flood fill.  So far, that gives you a way to work with images purely through code. If you want to make it interactive though, you will need to deal with windowing somehow, and that comes with all sorts of annoying issues -- every OS and GUI library seems to have its own ideas about how to deal with event handling, timing, menus, etc. If you want to keep things simple, SDL is a bare bones windowing library aimed at games -- that's one option you could use. Alternatively, there are a lot of libraries for general purpose GUI like wxWidgets, Qt, GTK, or native options like Win32 API on Windows and Cocoa on Mac. You'll have to read the documentation and probably a fair number of examples specific to the toolkit for whatever option you choose.  If you want to do more sophisticated real-world 2D drawing, Cairo is a good library to look into."
GraphicsProgramming,3aa2sl,Figs,1 point,Fri Jun 19 03:04:42 2015 UTC,"I would start super simple and use glfw to get a window up.  From there get an rgb image on the screen (could just be a gradient) with open gl - glDrawPixels  From there use the mouse events in glfw to change the pixels near the cursor to a different color.   The draw loop should already be drawing your image buffer every frame, so you should be able to see the results."
GraphicsProgramming,3aa2sl,__Cyber_Dildonics__,1 point,Sat Jun 20 15:05:55 2015 UTC,What about pen pressure? How can we read the values of pen pressure per stroke? Any libraries we can use?
GraphicsProgramming,3aa2sl,__Cyber_Dildonics__,1 point,Sat Jun 20 23:55:33 2015 UTC,"I'm sure that isn't a huge deal, but I would really recommend not worrying about that yet and trying to make the most simple thing that could possibly work."
GraphicsProgramming,3a4drz,bbmario,4,Wed Jun 17 03:29:03 2015 UTC,"there's a proposal to add a Cairo-inspired 2D graphics API to the C++ standard library. Seems like a weird idea to me, but it may indicate that Cairo has some kind of industry standard status."
GraphicsProgramming,3a4drz,jurniss,3,Wed Jun 17 12:38:48 2015 UTC,Which is proposed by Herb Sutter. Unexpected.
GraphicsProgramming,3a4drz,Leandros99,3,Wed Jun 17 17:26:43 2015 UTC,"There is also AntiGrain, Blend2D (formerly fog) and NanoVG."
GraphicsProgramming,3a4drz,thomac,3,Wed Jun 17 08:10:24 2015 UTC,"It seems like none only one of those libraries are able to handle Unicode text strings, unlike Cairo and Skia."
GraphicsProgramming,3a4drz,RowYourUpboat,2,Wed Jun 17 19:05:25 2015 UTC,nanovg handles unicode text strings just fine.  It uses fontstash for rendering text which can either render glyphs with stb_truetype or ft2.
GraphicsProgramming,3a4drz,jonte,2,Wed Jun 17 22:16:19 2015 UTC,"Ah, looks like NanoVG supports UTF-8. Nice.  When I checked before, what documentation I could find for NanoVG turned up nothing when I searched for ""utf"" or ""unicode""."
GraphicsProgramming,3a4drz,RowYourUpboat,1 point,Wed Jun 17 22:29:03 2015 UTC,Do you know any performance benchmarks between those popular libs?
GraphicsProgramming,3a4drz,thomac,2,Wed Jun 17 13:25:48 2015 UTC,"Unfortunately no, not really. There used to be a comparison at the wiki of fog. Qt had a comparison, too, which is now gone, too. If you look up ""Nvidia path rendering"", they are comparing it against Skia, IIRC. The paper Scanline edge-flag algorithm for antialiasing benchmarks against AntiGrain. And I think there was something at Cairo mailing list few years ago, cannot find it now..."
GraphicsProgramming,3a4drz,hwc,2,Wed Jun 17 18:18:03 2015 UTC,"Skia Engineer here. I could list off all of our advantages, but it might be easier if you explained what you need in a graphics library."
GraphicsProgramming,3a4drz,thomac,1 point,Thu Jun 18 03:14:25 2015 UTC,"Hello! I have a question, if I may. I don't want to hijack this thread though so I started new one here. Thanks in advance if you could take a look at it!"
GraphicsProgramming,3a4drz,jringstad,1 point,Fri Jun 19 12:54:41 2015 UTC,"what platforms do you want to target? NanoVG has GL, GLES and DX backends, I believe."
GraphicsProgramming,3a4drz,jringstad,1 point,Wed Jun 17 12:28:13 2015 UTC,"Linux, Mac and Windows."
GraphicsProgramming,3a4drz,ChainedProfessional,1 point,Wed Jun 17 13:25:22 2015 UTC,"Well, the operating system is not that important. The GL backend covers linux, mac and windows on desktop computers, the GLES backend covers linux on android and embedded devices. The DX API could cover integrating your UI framework with DX-based games.  But if someone wants to use your UI in a setting where no hardware acceleration at all is available (e.g. because their game is itself not hardware-accelerated) or some other API is used (GNM/GNMX etc) then it might not be so easy. I also don't know what would happen on windows phone, there only a subset of DX is available. Either way, there are things like ANGLE (GL-to-DX) and MESA llvm-pipe (Software GL) that can help."
GraphicsProgramming,3a4drz,pnpbios,1 point,Wed Jun 17 13:30:01 2015 UTC,"Some real old Windows computers have poor GL support. You can work around this with ANGLE, if you can figure out how to get it."
GraphicsProgramming,3a4drz,bnolsen,1 point,Thu Jun 18 01:58:58 2015 UTC,Mono Uses Cairo to implement System.Drawing in their .NET run-time.
GraphicsProgramming,3a2nek,meith1,2,Tue Jun 16 19:18:51 2015 UTC,"The only thing I can think of, is that the translations might be placed in the wrong locations in memory. This might help.      float trans_mat[4][4] = {             {1.0f, 0.0f, 0.0f, self->translation[0]},             {0.0f, 1.0f, 0.0f, self->translation[1]},             {0.0f, 0.0f, 1.0f, self->translation[2]},             {0.0f, 0.0f, 0.0f, 1.0f}     };   should be       float trans_mat[4][4] = {             {1.0f, 0.0f, 0.0f, 0.0f},             {0.0f, 1.0f, 0.0f, 0.0f},             {0.0f, 0.0f, 1.0f, 0.0f},             {self->translation[0], self->translation[1], self->translation[2], 1.0f}     };"
GraphicsProgramming,3a2nek,KamiKagutsuchi,1 point,Tue Jun 16 20:00:41 2015 UTC,Thank you for replying. Changing that got the translations in x and y working but now z has stopped working. Also the same problem in rotations still persists.
GraphicsProgramming,3a2nek,KamiKagutsuchi,2,Tue Jun 16 20:09:30 2015 UTC,"How about trying to create the matrix like this instead:      float trans_mat[16] = {             1.0f, 0.0f, 0.0f, 0.0f,             0.0f, 1.0f, 0.0f, 0.0f,             0.0f, 0.0f, 1.0f, 0.0f,             self-&gt;translation[0], self-&gt;translation[1], self-&gt;translation[2], 1.0f};   This should ensure it's laid out correctly in memory.  Calculate the linear index from x and y like this:  index = x+4*y   Edit: This shouldn't matter but who knows"
GraphicsProgramming,3a2nek,KamiKagutsuchi,1 point,Tue Jun 16 20:21:34 2015 UTC,"Also, the object is moving way more than it should. The displacement is not natural at all compared to when the translations are on the right.   Values of 0.5 are displacing the object across half the screen even though the camera is placed far behind."
GraphicsProgramming,3a2nek,KamiKagutsuchi,1 point,Tue Jun 16 20:28:25 2015 UTC,I cant figure out whats wrong without the rest of the code. The construction of the transformation matrices should be correct now.
GraphicsProgramming,3a2nek,KamiKagutsuchi,2,Tue Jun 16 20:51:23 2015 UTC,"The problem with your rotation matrices:       float roty_mat[4][4] = {             {c[1], 0.0f, s[1], 0.0f},             {0.0f, 1.0f, 0.0f, 0.0f},             {-s[1], 0.0f, c[1], 0.0f},             {0.0f, 0.0f, 0.0f, 1.0f}     };      float rotz_mat[4][4] = {             {c[2], s[2], 0.0f, 0.0f},             {-s[2], c[2], 0.0f, 0.0f},             {0.0f, 0.0f, 1.0f, 0.0f},             {0.0f, 0.0f, 0.0f, 1.0f}     };   Should be      float roty_mat[4][4] = {             {c[1], 0.0f, -s[1], 0.0f},             {0.0f, 1.0f, 0.0f, 0.0f},             {s[1], 0.0f, c[1], 0.0f},             {0.0f, 0.0f, 0.0f, 1.0f}     };      float rotz_mat[4][4] = {             {c[2], -s[2], 0.0f, 0.0f},             {s[2], c[2], 0.0f, 0.0f},             {0.0f, 0.0f, 1.0f, 0.0f},             {0.0f, 0.0f, 0.0f, 1.0f}     };   (The wrong sin term is negative)"
GraphicsProgramming,39q2xp,kayamon,1 point,Sat Jun 13 17:28:51 2015 UTC,Is it really efficient to divide it all the way down to the pixel?
GraphicsProgramming,39q2xp,ChainedProfessional,1 point,Sun Jun 14 00:48:54 2015 UTC,"It doesn't divide all the way down to a single pixel, as you can see by simply watching the demo.   But, it does have the capability to, if it turns out to be beneficial to do so."
GraphicsProgramming,39q2xp,ChainedProfessional,1 point,Sun Jun 14 01:04:43 2015 UTC,"I was confused by    Eventually we’re guaranteed to get down to the level of a single pixel, at which point everything will fall into case 1 or 2 anyway"
GraphicsProgramming,39q2xp,goal2004,0,Sun Jun 14 02:23:38 2015 UTC,"Yes. In deferred settings any pixel saved is a considerable gain, because lighting it would be where you'd spend mots of the time."
GraphicsProgramming,39q2xp,ChainedProfessional,1 point,Sun Jun 14 00:51:36 2015 UTC,"As I understand it, with deferred lighting you render the light volume and that render pass pulls from the G-Buffer.  So the optimization here is to do as much lighting as possible, maybe 8 or 16 lights, with a single read from the G-buffer?"
GraphicsProgramming,39q2xp,bobstevens,1 point,Sun Jun 14 00:56:03 2015 UTC,"Kind of.   On a modern GPU architecture like GCN and probably Nvidia cards as well, you're doing work in groups of 64 or 32.  So in the case of a pixel shader, if you're optimizing down to a single pixel you may be going a little too far.  The GPU will run less than 64 threads when it could compute a full 64 just as efficiently, depending on bandwidth required."
GraphicsProgramming,39q2xp,phort99,1 point,Sun Jun 14 05:39:57 2015 UTC,"I don't understand the starting premise of this optimization. My understanding is that the lighting is done by rendering quads that hold the light info in vertex attributes, and the lighting is done in the pixel shader, right? Why is it more efficient to batch overlapping lights together? You have to render two quads and perform roughly the same number of pixel shader calculations in both cases, correct?"
GraphicsProgramming,39q2xp,ChainedProfessional,1 point,Sun Jun 14 08:12:44 2015 UTC,"It's this part:   All this overhead adds up, so what you really want to do is find a way to get a list of the lights that overlap on-screen, and group those together into batches. That way you only need to read the Z-buffer and reconstruct the position once for each screen pixel, and then share that cost across multiple lighting calculations.   There's a certain setup cost for each fragment, so if you can do several lights as a batch it's more efficient.  It's just like with forward lighting, where you want to do 4 lights in one pass instead of a pass for each light."
GraphicsProgramming,39q2xp,phort99,1 point,Sun Jun 14 15:16:31 2015 UTC,"What I mean is, is one batch two quads in one draw call, or is it one quad with enough vertex attributes for multiple lights?"
GraphicsProgramming,39q2xp,ChainedProfessional,2,Sun Jun 14 15:58:20 2015 UTC,One quad that draws all the lights in that quad.
GraphicsProgramming,394cx3,dabooch,1 point,Tue Jun 9 04:16:39 2015 UTC,What is the performance like when using this algorithm?
GraphicsProgramming,394cx3,fb39ca4,1 point,Tue Jun 9 15:28:46 2015 UTC,"With the ""simple/complex"" optimization I discuss at the end of the article, it's great. But if you run the coverage estimation on every pixel, the performance isn't great at all. I expect it's mostly due to the final vertical clipping steps where we clip the edges to the pixel. So, that optimization step is really quite necessary if you want the results to be fast/interactive with large polys in HD resolution. When this algorithm was first implemented back in the 80's, it was considered efficient, but of course it wasn't being used for interactive applications like I'm using it for here."
GraphicsProgramming,394cx3,Boojum,2,Tue Jun 9 16:07:34 2015 UTC,"Are your polygons all convex?  If so, I wonder how this would compare to computing the masks by using vector intrinsics to test for which subpixels are in the intersection of the half-planes?  I've heard that that's how most hardware does it these days.  That would also let you do non-uniform sampling.  EDIT: Derp!  I'd forgotten that you mentioned bow-ties.  So not convex then.  Still would be interesting to compare on input that's been decomposed to convex."
GraphicsProgramming,394cx3,fb39ca4,0,Wed Jun 10 04:22:39 2015 UTC,"Interesting. To be honest, the mask computation wasn't particularly expensive and I did think to myself that if there was an L2 cache miss, it'd likely be more expensive to look up into the table than it would be to compute the mask directly. I really should do some timings to determine what the actual speed of mask computation is vs. the average table lookup speed. I didn't spend any time optimizing the mask calculation, but it's really just simple math with some float to integer conversions and bit manipulation. I think I'll spend a bit more time on the mask calculation and see if I can vector optimize it. Maybe it would be better to just calculate it every time."
GraphicsProgramming,394cx3,thomac,1 point,Wed Jun 10 16:50:25 2015 UTC,I see. So you are only really applying this algorithm to polygon borders. I wonder how it compares to this: http://www.josiahmanson.com/research/wavelet_rasterization/wavelet_rasterization.pdf
GraphicsProgramming,38h3x0,mrjmoak3,8,Thu Jun 4 04:13:41 2015 UTC,Neat post!  I think you need to optimize your path tracer though.
GraphicsProgramming,38h3x0,pixel_fcker,5,Thu Jun 4 06:10:29 2015 UTC,"Haha - Not only did my PathTracer at the time not have any KD-trees/subdivisions, but it wasn't even checking the bounding box of the mesh itself before hopping in and checking each tri's collision.  Soooo yeah"
GraphicsProgramming,38h3x0,pixel_fcker,2,Thu Jun 4 06:12:02 2015 UTC,Haha yeah that would explain it. I would expect a well-optimized path tracer to give you similar performance to the rasterizer for that sort of image.
GraphicsProgramming,38h3x0,nemui_one_zzz,2,Thu Jun 4 14:36:05 2015 UTC,"Not really a lost art. There is a nice recent book ""Physically Based Rendering"", that explains in details how to build a rendering engine. It focuses on building a high quality offline renderer, but most of its content applies to real-time renderers as well."
GraphicsProgramming,38h3x0,fb39ca4,2,Thu Jun 11 22:03:08 2015 UTC,Yeah it's one of my favorites - I was mainly referring to real-time though in the blog :)
GraphicsProgramming,38h3x0,fb39ca4,1 point,Fri Jun 12 00:09:28 2015 UTC,Did you take NAND to Tetris?
GraphicsProgramming,38dgq4,UsuallyQuiteQuiet,23,Wed Jun 3 14:33:27 2015 UTC,"A big part of it was that they wanted to do dynamic lighting but couldn't pull off dynamic global illumination (bounced lighting).  If you had moving, bright, colorful, dynamic lighting, it becomes painfully obvious that the GI never changes.  With everything restricted to similar, muted tones the lack of dynamic GI is much less obvious.  The counter-example is Mirror's Edge which featured bright, vibrant colors. To pull this off, the made the hard technical decision to restrict all lighting to completely static and pre-baked."
GraphicsProgramming,38dgq4,corysama,6,Wed Jun 3 15:05:16 2015 UTC,GI is when you have say a blue surface and a white surface and light from the blue surface causes the white surface to gain a slight blue hue right? Also is it synonymous with ambient lighting? I've only watched a couple John Carmack quakecon lectures on computer graphics so I'm pretty much new to this stuff.
GraphicsProgramming,38dgq4,corysama,7,Wed Jun 3 15:44:21 2015 UTC,"Yep. Go look up pictures of Mirror's Edge. Bounced lighting everywhere. Gave it a distinct look that people still complement today. But, it required restrictions on the lighting that most games don't want."
GraphicsProgramming,38dgq4,mysticreddit,1 point,Wed Jun 3 15:51:10 2015 UTC,Deleted .. replied to wrong poster ...
GraphicsProgramming,38dgq4,Ravek,2,Mon Jul 13 01:19:06 2015 UTC,"Ambient lighting usually just refers to a fixed light level that is applied to everything in the scene regardless of position, orientation, incoming light, etc. It's the simplest level of lighting and definitely not synonymous with an advanced technique like GI."
GraphicsProgramming,38dgq4,bobstevens,1 point,Wed Jun 3 21:23:06 2015 UTC,"Oh right. So global illumination (coloured light reflecting off of surfaces if I understand correctly) is what would logically occur if there is ambient lightning, but not all games use it since it is quite a taxing effect?"
GraphicsProgramming,38dgq4,mysticreddit,1 point,Wed Jun 3 22:10:13 2015 UTC,"Many/most games use static global illumination and have for ages, but dynamic global illumination is still open research with only really Tomorrow Children doing interesting stuff in the space."
GraphicsProgramming,38dgq4,qubedView,2,Thu Jun 4 23:39:15 2015 UTC,Here's a pic showing how bounce lighting works (at the high level) from:   http://renderstuff.com/vray-indirect-illumination-best-settings-cg-tutorial/
GraphicsProgramming,38dgq4,jringstad,1 point,Mon Jul 13 01:19:46 2015 UTC,I've saved the link for reading later. It looks quite interesting. Thanks
GraphicsProgramming,38dgq4,reversememe,7,Mon Jul 13 02:15:03 2015 UTC,"Real is brown. It's mostly style, but also a little to cover up imperfections."
GraphicsProgramming,38dgq4,jringstad,1 point,Wed Jun 3 14:43:28 2015 UTC,Thanks for the link + quick reply. It makes sense now that its a combination of both.
GraphicsProgramming,38dgq4,PM_ME_DOGECOINS,5,Wed Jun 3 14:51:57 2015 UTC,"I think it's mostly aesthetics. We're now entering an aesthetic era where brighter colors become much more accepted. This new mainstream aesthetic is probably mostly spearheaded by googles material design philosophy, which is pretty influential, but iOS and windows 8 have also become significantly more colorful than they've ever been before."
GraphicsProgramming,38dgq4,PM_ME_DOGECOINS,-1,Wed Jun 3 17:13:24 2015 UTC,"Lol, implying material design isn't a mediocre implementation of trends that have been re-emerging for years. Flat bold colors, overly stylized illustrations, sharp 45 degree shadows, ..."
GraphicsProgramming,38dgq4,PM_ME_DOGECOINS,3,Wed Jun 3 20:31:04 2015 UTC,I don't see how I was implying any of that.
GraphicsProgramming,38dgq4,emergent_properties,2,Thu Jun 4 09:18:27 2015 UTC,"The trends have been reemerging but I would consider Material Design to be a pretty good implementation of it tbh. I highly doubt that it, along with Windows 8, precipitated the colourful games we get nowadays though."
GraphicsProgramming,38dgq4,ChainedProfessional,2,Wed Jun 3 20:44:23 2015 UTC,CINEMATIC EXPERIENCE
GraphicsProgramming,38dgq4,chadmiral_ackbar,1 point,Wed Jun 3 22:37:23 2015 UTC,Now at a jaw-juddering 30 fps!
GraphicsProgramming,37xqzf,Akashic-Record,2,Sun May 31 07:34:02 2015 UTC,"Maya and 3DS are both very complex when it comes to these kinds of internals. I am most familiar with Maya, so here's one way I approach this kind of thing:  Maya usually outputs the script for each action you do in the editor as MEL script, so you can do an action (set a keyframe, create a bone, edit an anim curve, etc) and then open the script window to see how to do that action programmatically. You will need to know the syntax and basics of MEL to interpret the output. Google can be your friend but MEL documentation is lacking - each MEL command has a ""help"" command that shows each parameter though.  If you are not allowed to use MEL, I believe Maya's Python API closely mirrors the MEL one. I do not recommend Maya's C++ API as it doesn't really correspond to MEL and it's usually too complex to understand and use when you can use scripts and/or much more robust 3rd party plug-ins or exporters instead."
GraphicsProgramming,37xqzf,RowYourUpboat,1 point,Sun May 31 17:16:50 2015 UTC,"I was avoiding MEL because I already had a bunch of other stuff written in Python, but if I can't avoid it then I will do so."
GraphicsProgramming,37xqzf,RowYourUpboat,1 point,Mon Jun 1 12:53:38 2015 UTC,"I believe the Python API is supported pretty well (like I said, it's supposed to mirror the MEL API closely - although I have heard some functions are missing.) I am curious myself and will look into it when I am not on my phone. It may even be possible to get the script window to output Python instead of MEL.  [edit] It seems like there's a pretty exact correlation between MEL and Python functions and parameters, so looking at the MEL output and then writing Python should work fine. There are also entries for bringing up the scripting documentation in the Script Editor window's Help menu (also note the tabs in the Script Editor for inputting either MEL or Python)."
GraphicsProgramming,37xqzf,wrosecrans,2,Mon Jun 1 17:35:19 2015 UTC,"Yeah, the Python bindings are basically just a mechanical tranform of the MEL.  I don't think I've ever run into anything that didn't have a 1:1 correlation, and once you learn the patterns it's actually pretty easy to write Python by looking at the MEL documentation.  Basically, where MEL does ""someCommand -flag"" like in a CLI, in Python it's always ""someCommand(flag=True)"".  It's not terribly idiomatically Pythonic, but it's easy to get used to.  Whenever I have written stuff to import/manipulate rigs in Maya, I've always done it in Python.  I've never needed to resort to the C++ SDK for that kind of work.  If you have really massive needs, you can potentially run into performance issues that make C++ make sense.  But whenever possible, I would always recommend prototyping that sort of think in Python first and only resorting to C++ if you can prove it's necessary.  My suggestion would be not to worry too much about the details of stuff like animCurves.  You have baked data, so just build the rig that you need, set rotation keys on all your bones, advance time, set rotation keys from your baked data, advance time, etc. until you are out of baked data."
GraphicsProgramming,37xqzf,RowYourUpboat,1 point,Mon Jun 1 23:28:34 2015 UTC,Make sure you read /u/wrosecrans comment (here). It has some good advice.
GraphicsProgramming,37xqzf,QuixoticChris,1 point,Mon Jun 1 23:55:10 2015 UTC,"I can't help you, but I suggest you ask in /r/3dsmax and /r/maya as well, since your question seems specific to them. Good luck!"
GraphicsProgramming,36yx22,joebaf,1 point,Sat May 23 08:06:16 2015 UTC,"The part about tiled rendering on PowerVR made me wonder:  Is it more efficient to clear the color buffer before each frame than not?  For a desktop GPU, as they say the buffer is already in VRAM either way.  But if the PowerVR has to load it into its small on-die RAM, then clearing it could be optimized to just clearing the on-die RAM and skipping the load?  I'll try to do a test, but maybe it will be such a small difference that it won't even show up?  Edit: Doesn't seem to make any difference on my chip. Maybe a hundredth of a millisecond. Small enough that I couldn't see it over the noise in my mpf counter."
GraphicsProgramming,36yx22,lua_setglobal,2,Sun May 24 22:13:55 2015 UTC,"Clearing before drawing should implicitly skip reloading the on-chip buffer with the previous frame's contents.  That should be a bit faster.  Even better is to explicitly indicate that you will not be needing the previous frame's contents using https://www.khronos.org/registry/gles/extensions/EXT/EXT_discard_framebuffer.txt  Through that extension you are also able to explicit skip copying the depth buffer out of on-chip RAM.  It won't be displayed or reloaded, so why bother even moving it off-chip?  Some more info: http://www.seas.upenn.edu/~pcozzi/OpenGLInsights/OpenGLInsights-TileBasedArchitectures.pdf"
GraphicsProgramming,36rquo,Boojum,2,Thu May 21 16:29:51 2015 UTC,"I originally found this through a post in /r/programming, but I was successfully nerd-sniped by it, so I thought I'd x-post it here."
GraphicsProgramming,36rquo,gliph,2,Thu May 21 16:30:50 2015 UTC,"Hm.... how about this:  Step 1: edge detection.  Step 2: Find points farthest from edges to be the sample points.  Step 3: Draw the map.  Maybe it could find all candidate points, then enforce only so many candidate points per area."
GraphicsProgramming,36rquo,Themaister,1 point,Thu May 21 16:48:32 2015 UTC,"Sounds a bit like image segmentation.  I don't think you could just take the points farthest from the edges, though.  Depending on how you did step 2, you could either end up with all the points clustered in one small region around the global maxima if you just took the top N farthest (unless you enforced some kind of dispersion as you mentioned).  If you take local maxima they'd be better distributed, but now the edges are unlikely to be equidistant from the two nearest points.  There might also be issues with concavity regions.  Still, it might be possible to make something like that work.  Something like sampling isocontours of the Euclidean distance map of the edges."
GraphicsProgramming,36nne0,orost,11,Wed May 20 18:27:42 2015 UTC,"The main problem with Euler angles is that compositions of 3D rotations are ugly and problematic. Specifically, certain sequences of rotations can result in gimbal lock, or the loss of a degree of freedom.  If your camera is specified by a single set of angles which are controlled directly by your input or camera controller (i.e., you are not applying Euler rotations to each other), then there's not really a problem."
GraphicsProgramming,36nne0,angrymonkey,2,Wed May 20 20:08:29 2015 UTC,Even that's fine if you're careful about the order you apply rotation. There's not many games where it's likely you're going to both pitch down and roll 90 degrees.
GraphicsProgramming,36nne0,hahanoob,1 point,Thu May 21 00:55:38 2015 UTC,"Not many games? Most games nowadays have objects which are rotated on all three axes. It's not just the camera which benefits from quaternions. And to be fair, keeping track of the XYZ, XZY, YXZ, YZX .. orders when using Euler is a major pain in the ass :)"
GraphicsProgramming,36nne0,Madsy9,1 point,Mon May 25 10:09:56 2015 UTC,"Yeah, but I was talking only about the camera :)  Euler angles give you more flexibility in terms of how you handle interpolation, tweening, damping too. And slerp has some properties that can sometimes make camera transitions look unnatural, because it always follows the shortest path, which isn't what a human operator would tend to do (i.e. they're probably not going to roll the camera while panning diagonally).  For everything else though, yeah, Quaternions are probably a better choice."
GraphicsProgramming,36nne0,hahanoob,5,Mon May 25 14:48:56 2015 UTC,"I'd say go for it. That's the way I do it, and I don't see anything wrong with it. Just double check that the matrix creation methods work correctly and you're good to go :)"
GraphicsProgramming,36nne0,adudney,1 point,Wed May 20 18:34:09 2015 UTC,"Thanks... any thoughts on how to implement that? I'm thinking one class (call it A) that implements translation movement, interface B that defines rotation-related methods, two classes C and D implementing B with quaternions and euler angles respectively, the latter also extending it with mouse- and constraints-related stuff, and then the actual usable classes for models and cameras and stuff would inherit from both A and either C or D. That would work, right? Or am I overthinking it?  edit: no, that won't work, because there would be stuff that needs to know about both position and rotation... is it worth it to add another layer of inheritance? Ah, I know nothing!"
GraphicsProgramming,36nne0,angrymonkey,6,Wed May 20 20:06:38 2015 UTC,"You're overthinking it.   Your camera will have a matrix which transforms your scene to a canonical camera space (e.g., camera at the origin, looking down the Z- axis). This matrix is the first thing that will go on your transform stack. (You will also have a projection matrix or frustum information associated with your camera).   Something will be controlling your camera; moving it around and changing its angle of view. Write a method to take that position and set of Euler angles and produce a 4x4 matrix. Each time you update the camera, clobber its matrix with the new one."
GraphicsProgramming,36nne0,adudney,1 point,Wed May 20 20:50:57 2015 UTC,"Perhaps try composition instead of inheritance? Have there be a position object and two rotation objects implementing the same interface, then have cameras contain one position object and one Euler rotation object. Give the rotation interface a method that translates a position object using local coordinates. Finally, have a matrix class method that takes both a rotation interface and a position interface and turns them into a matrix.  Edit: also have the mouse control be part of the camera or an input controller rather than the rotation object. Rotation limits can be implemented either with mouse control or in the rotation object"
GraphicsProgramming,36nne0,jurniss,1 point,Wed May 20 20:50:36 2015 UTC,"Composition gave me an idea... why don't I make those classes able to hold both types of rotation objects? By default, they would run on quaternions, but if needed any object could be told to instantiate an euler rotation module and make available euler-derived functionality. There would be a bit of overhead in having both and keep them synched, but only for the few objects that actually need both, and it would be really flexible..."
GraphicsProgramming,36nne0,smallstepforman,7,Wed May 20 22:46:18 2015 UTC,"this is too complicated. if you really want to work with both in the same project, use quats internally and convert eulers to quats at the api surface.  I would definitely not use any oop techniques that require virtual methods in a math library like this. you want everything concrete (and inline if you're using c++) so the compiler can optimize it to the fullest extent."
GraphicsProgramming,36nne0,caedicus,3,Thu May 21 00:29:58 2015 UTC,"Your camera class API SetDirection() should handle both Euler and Quaternions as input, but internally work as Quaternions.  This allows a simpler API for people accustomed to Euler angles which is easier to think about, but also allows for slerping/interpolation to properly rotate the camera to a specific direction."
GraphicsProgramming,36nne0,Zukhramm,3,Wed May 20 23:26:55 2015 UTC,"I don't think euler angles are for noobs.  They are in fact harder to work with in a lot certain situations.  Quaternions shouldn't be a pain in the ass.  They are actually really easy to work with.  They are difficult to understand mathematically at a deep level, but that doesn't mean you can't use them."
GraphicsProgramming,36nne0,zerooneinfinity,2,Thu May 21 00:23:26 2015 UTC,"I don't think I'd ever use Euler angles if I already had or would later implement quaternions anyway, there's simply no reasons. Quaternions are easy to use in an Euler angle like way anyway, so why waste time implementing another set of separate rotation code?"
GraphicsProgramming,36nne0,Zukhramm,1 point,Wed May 20 21:35:03 2015 UTC,"There are some things that I can't figure out how to do with quaternions. For example, controlling a quaternion camera with mouse, however I go about it, causes unwanted roll to sneak in and I have no clue how to lock it to ""vertical"" (i.e. no rotation around local Z-axis). I'm also not sure how I would constrain rotation properly, or how to implement a lookAt rotation.  Those things are all easy to do with angles and difficult with quaternions (I've seen some ideas, but they all involve crazy math that makes my head swim and that seems inefficient), and they are all only really needed for the camera, so it makes sense to me to give the camera a separate implementation on Euler angles.  But maybe I'm missing something? How do you do these things in quaternions?"
GraphicsProgramming,36nne0,Zukhramm,1 point,Wed May 20 21:57:03 2015 UTC,Do what makes sense for your game and what you can get running quickly. You can always switch it later.
GraphicsProgramming,36nne0,yuriks,1 point,Wed May 20 22:37:04 2015 UTC,"I'm not entirely sure I'm correct here, but it seems to me that anything that can be expressed using Euler angles could just be replaced with a product of three quaternions (one rotating around each axis).  For rotating the camera I multiply two quaternions, one rotating around the vector pointing up, and one around the cameras side vector."
GraphicsProgramming,36nne0,ChainedProfessional,1 point,Wed May 20 23:52:41 2015 UTC,"But aren't per-axis quaternions essentially just slightly obfuscated Euler angles? You're losing the benefits of quaternions if you split them like this, so you may as well use Euler angles."
GraphicsProgramming,36nne0,ReversedGif,1 point,Wed May 20 23:58:33 2015 UTC,"I don't know what those benefits are, the benefits I see are having a single easier to understand system rather than different ones for different cases."
GraphicsProgramming,36nne0,ChainedProfessional,1 point,Thu May 21 09:00:21 2015 UTC,"Well, the benefits of quaternions are supposed to be easy composition, resistance to gimbal lock, and easy interpolation. If you split a quaternion into three per-axis ones, then none of those is true any more, they just become harder to understand and slower Euler angles. So you're essentially telling me to drop quaternions entirely in favor of Euler angles, which is an option, I suppose."
GraphicsProgramming,36nne0,ReversedGif,3,Thu May 21 11:10:58 2015 UTC,"You generate three quaternions to mirror euler angles, but after you compose those three quats together you can now treat them as a rotation like any other in the rest of your engine. You don't need to special case for euler angles in the rest of the code.  If you're worried about inefficiency: Quaternions are actually cheaper to multiply together than matrices. (However, transforming points by them is more expensive than with a matrix, so you should convert quaternions to matrices before sending them to your vertex shader in most cases.)"
GraphicsProgramming,36nne0,ChainedProfessional,1 point,Thu May 21 11:49:37 2015 UTC,"Yeah for a regular camera I do Euler angles which I then convert to a bunch of matrix rotations.  Matrix operations are cheap on desktop CPUs anyway, don't sweat it.  I only use quaternions if I need something to rotate freely like for 3D physics for an airplane or racecar. (Although apparently they use axis+angle since quats can't rotate more than 180 degrees?)"
GraphicsProgramming,36nne0,ReversedGif,1 point,Thu May 21 00:47:16 2015 UTC,(Although apparently they use axis+angle since quats can't rotate more than 180 degrees?)   You high bro?
GraphicsProgramming,36nne0,Themaister,1 point,Thu May 21 12:41:38 2015 UTC,"I've never tried to use quats for rotational velocity, so I don't know exactly what the deal is."
GraphicsProgramming,36nne0,pseudorandomess,1 point,Thu May 21 12:58:26 2015 UTC,"Ah, for angular velocity, a 3-vector ω is almost always used (ω = angular rate * axis), not a quaternion. With it, you can update a quaternion with q_new = eω t/2 q (with ω there being implicitly converted to a vector-only quaternion)."
GraphicsProgramming,36nne0,KeinBaum,1 point,Thu May 21 13:57:58 2015 UTC,"Oh, the angle is implied by the length of the axis?  I forgot that part. But I still think, in a way, I was right. It's an axis-angle representation."
GraphicsProgramming,36nne0,Meristic,1 point,Thu May 21 22:21:45 2015 UTC,"Yes, well, the angular rate, not the angle.  It is indeed very similar to axis-angle, though storing a rotation as axis*angle is called a rotation vector (according to wikipedia) and that's even more similar to this.  Interestingly, the rotation vector r = 2 log q (rotation vector = double the logarithm of the corresponding quaternion) is very useful in control systems (e.g. applying torque to an object to get it to a particular orientation), as you're usually dealing with an error quaternion that is near the identity rotation, and so the corresponding rotation vector is nominally zero and can be used pretty trivially, say by multiplying it by a scale factor and applying it as a torque - voila, singularity-less orientation control."
GraphicsProgramming,36nne0,bnolsen,2,Fri May 22 01:52:56 2015 UTC,For FPS-style cameras and free-flying cameras (for demos) I much prefer Euler angles due to the simplicity.
GraphicsProgramming,36g9ns,mrjmoak3,5,Tue May 19 03:51:57 2015 UTC,"That was enjoyable to read, I like your writing style!  Though one thing caught my eye: in some of the snippets you're dynamically allocating an array on the fly (which could probably be done on the stack), but when you call delete afterwards you're not using the correct syntax for deleting an array (delete [] arr;). So unless you just forgot when copying the snippets into the post, you're going to have a leak :p."
GraphicsProgramming,36g9ns,Aka_chan,2,Tue May 19 08:00:43 2015 UTC,"Oh my god you're right, thanks!"
GraphicsProgramming,36g9ns,Leandros99,2,Tue May 19 11:06:33 2015 UTC,"Correct me if I'm wrong, but shouldn't each color channel represented by 1-byte instead of 2?"
GraphicsProgramming,36g9ns,casey12141,1 point,Tue May 19 17:54:30 2015 UTC,"I'll correct that, a spelling error :)"
GraphicsProgramming,36g9ns,wrosecrans,1 point,Tue May 19 18:13:02 2015 UTC,"Nice! There's a really good png library for C called libpng, and I'm sure there's some type of wrapper for C++ as well, if you want to learn about something else :) I can send you sample C code if you need."
GraphicsProgramming,36g9ns,TotesMessenger,1 point,Tue May 19 14:58:40 2015 UTC,"I am sort of torn between saying, ""Well done, looks like a good learning project."" and ""Oh goodness, somebody is going to read this and think that they should hack up their own TIFF IO code and deploy it into production in real life.""  Heh."
GraphicsProgramming,36g9ns,cleroth,1 point,Thu May 28 01:55:50 2015 UTC,"This thread has been linked to from another place on reddit.   [/r/computergraphics] Encoding .TIF Files the Hard Way (x-post r/GraphicsProgramming) [/r/programming] Encoding .TIF Files the Hard Way (xpost r/GraphicsProgramming)   If you follow any of the above links, respect the rules of reddit and don't vote. (Info / Contact)"
GraphicsProgramming,3692ch,Apothiem,7,Sun May 17 10:33:51 2015 UTC,"A thing to note: Many games will clear the depth buffer before drawing the gun model. This will prevent the gun model from clipping through geometry but it can look a bit weird when standing near a wall.   If you just want to render an object sticking to the camera you should just get rid of the view matrix (Assuming you're using model-, view- and projection matrices)."
GraphicsProgramming,3692ch,Mathias9807,2,Sun May 17 10:47:31 2015 UTC,"The reason you want to clear the depth buffer before drawing the gun is because the alternative has some issues. If you were to draw the gun into the existing buffer, you would run into depth precision issues (z-fighting)."
GraphicsProgramming,3692ch,kikkurs,2,Sun May 17 20:46:52 2015 UTC,"And, of course, half the gun ending up inside the object in the front."
GraphicsProgramming,3692ch,steamruler,1 point,Tue May 19 14:02:28 2015 UTC,"Well, that would be correct, just not very pretty.   Just saying, there might be games out there where this is intended."
GraphicsProgramming,3692ch,kikkurs,0,Tue May 19 15:19:34 2015 UTC,"Actually with perspective projection most precision is towards the front. The issue isn't z fighting as such, simply you don't want objects drawn over the top of the gun."
GraphicsProgramming,3692ch,dukey,1 point,Tue Jun 9 12:00:00 2015 UTC,"If you want to draw the gun in the buffer, you'd need to place the near plane that much closer to the camera center, resulting in the same precision across a larger range. And as you say, most precision is at the front, z fighting will therefore occur further in the frustum rather than at the gun itself."
GraphicsProgramming,3692ch,kikkurs,1 point,Tue Jun 9 12:32:15 2015 UTC,"I render foreground objects in two passes. In the first pass I render every object with a pixel shader that clears the depth buffer to max-depth, which effectively wipes out all of the background world behind it. On the second pass I render the foreground objects like normal so it properly depth-culls against itself, but doesn't clip into world-geometry. This is also important for my deferred renderer so I don't have to light the foreground objects differently from the rest of the scene."
GraphicsProgramming,3692ch,Graumm,1 point,Tue May 19 23:31:54 2015 UTC,You could also just not write to the depth buffer. Don't.
GraphicsProgramming,3692ch,ietsrondsofzo,4,Sun May 17 16:37:46 2015 UTC,Then it has the potential to incorrectly render the gun  if it has any overlapping forward facing polygons.  Same if you just disabled depth reads.
GraphicsProgramming,3692ch,adudney,5,Sun May 17 16:58:39 2015 UTC,Oh yeah you're right! My bad.
GraphicsProgramming,3692ch,ietsrondsofzo,6,Sun May 17 17:11:16 2015 UTC,"Do you want it to animate?  I think your best option is to just render it normally (maybe even first to limit overdraw).  You could optimise the mesh so that it only contains polygons that would be visible.  If you don't want it to animated, you could just pre render it, and then render a single quad with the image on.  Or you could render it once, then place a stencil over it so that nothing else renders over it."
GraphicsProgramming,3692ch,ultrapingu,1 point,Sun May 17 10:42:41 2015 UTC,"Position in the world is simply represented by transformations for all objects correct? Let's say transformations are represented by matrices. If each object in the world has a matrix associated with it that is used to determine world position (such as the camera and the gun), then the camera's transformation defines a space which other transformations can be parented to by simple concatenation. The gun's transformation can then just be an offset into the camera's local space - which means if it doesn't change it will always be static relative to the parent's transformation.This is the basis for object parenting systems, which is exampled in Unity.  For column-major matrices:  /* This is the gun's final world transformation */  GunXfmWorld = CameraXfmWorld * GunXfmLocal  /* This is the gun's final homogeneous position (used for vertex transformation in shaders) */  GunXfmWVP = ViewProj * (CameraXfmWorld * GunXfmLocal)"
GraphicsProgramming,36328r,TurkishSquirrel,3,Fri May 15 18:03:37 2015 UTC,"Holy shit, the computational hydrographics example. They UV-unwrapped a real cat!"
GraphicsProgramming,36328r,mindbleach,10,Sat May 16 00:41:43 2015 UTC,Here's a video of that one in particular: https://www.youtube.com/watch?v=YlUhPrAqiY0
GraphicsProgramming,36328r,Alway2535,2,Sat May 16 04:43:14 2015 UTC,Interesting how there isn't a single VR paper there.  Is this an exhaustive list of papers?
GraphicsProgramming,36328r,Flafla2,3,Fri May 15 23:54:37 2015 UTC,"I don't think they've been posted officially but Ke-Sen maintains a site that tracks them. Once the papers are officially announced I think the list shows up here.  Edit: I don't think the trailer is an exhaustive list, comparing papers on Ke-Sen's page there look to be a lot more than were mentioned here."
GraphicsProgramming,36328r,pixel4,-2,Fri May 15 23:58:11 2015 UTC,Meh; last years preview was 1000x better: https://www.youtube.com/watch?v=u3Z1hDwGEmM
GraphicsProgramming,35yb7a,spottedsocks,2,Thu May 14 15:08:14 2015 UTC,Interesting choice of music
GraphicsProgramming,35yb7a,planetelex,1 point,Tue May 19 04:23:45 2015 UTC,Great job ichael.
GraphicsProgramming,35yb7a,Jimbo-Slice,-3,Tue Jul 7 03:00:54 2015 UTC,Man that dude is ugly.
GraphicsProgramming,35yb7a,playaspec,3,Thu May 14 23:57:59 2015 UTC,It's more than enough for a proof of concept.
GraphicsProgramming,35yb7a,gliph,1 point,Thu May 14 15:42:24 2015 UTC,Taylor Swift has some wisdom for you here.  https://www.youtube.com/watch?v=nfWlot6h_JM
GraphicsProgramming,35xxkr,lua_setglobal,1 point,Thu May 14 13:11:07 2015 UTC,"For this, I basically unwrapped normal dot-product lighting using spherical coordinates and it ended up looking suspiciously like spherical harmonics.  The sun can move north or south to simulate Earth's tilt away from the ecliptic, and just rotating the whole lightmap left or right simulates Earth rotating.  All the trig can be optimized to an interpolation between an ""Earth is flat to the ecliptic"" and an ""Earth is 90 degrees to the ecliptic"" image.  My program can't load images by itself, so I used GIMP to compose the lightmap over top of Wikipedia's map.  I haven't checked that it's correct or anything but I thought it looked cool.  Edit: Also, I am using this to learn Rust. The program is about 50 lines of Rust which writes out a greyscale PPM that I loaded into GIMP."
GraphicsProgramming,35xxkr,CodyDuncan1260,2,Thu May 14 13:14:48 2015 UTC,I'm curious to see the source. Could you post a github gist?
GraphicsProgramming,35qc0z,mercfh85,4,Tue May 12 17:14:59 2015 UTC,"Are you interested in realistic rendering of terrains or real-time rendering?  If it's the former, writing a physically based renderer (path tracer, etc.) might be more of interest to you than learning a graphics API like D3D or OpenGL like you would use for more interactive, albeit not as visually impressive, graphics."
GraphicsProgramming,35qc0z,domy94,1 point,Tue May 12 18:02:39 2015 UTC,I guess im unsure of the difference between them.
GraphicsProgramming,35qc0z,ultrapingu,3,Tue May 12 18:08:02 2015 UTC,"They're not explicitly different, but with one the intention is to render it at an interactive frame rate (30fps upwards).  Realistic implies you can take as long as you like to render it, so it's more about creating something that looks very good.  The former typically implies that you'd be using techniques typically associated with games, like rasterisation, using the graphics card, etc.  The latter doesn't really imply anything, but you might model your scene a little more physically, such as with a ray tracer.  Basically do you want to be able to 'walk around' it in real time, or do you want to create high end images or videos?"
GraphicsProgramming,35qc0z,ultrapingu,1 point,Tue May 12 18:59:22 2015 UTC,"I'd def. like to ""walk around"" in real time. more game-oriented I suppose (altho im less interested in making games)."
GraphicsProgramming,35qc0z,angrymonkey,3,Tue May 12 19:15:26 2015 UTC,"I'd suggest checking out Either DirectX, or OpenGL, with your language of choice.  If you don't want to care about all of the management and asset loading code, maybe check out procedural mesh generation in an engine like unity?  As for actually generating terrains, there's tons of algorithms, though this I'm not all that experienced with.  Maybe check out something like this."
GraphicsProgramming,35qc0z,angrymonkey,2,Tue May 12 20:26:52 2015 UTC,"Are you interested in realtime or offline rendering?  For realtime, I recommend learning OpenGL or DirectX. Personally, I prefer OGL because it is cross-platform. Once you know the basics, you could get a terrain on the screen in a day or so. For you, it will take longer because you are learning how to do stuff. When I was learning OGL, Lighthouse3d was a good resource. Just google around for tutorials.  For offline, I would recommend writing a ray tracer. The principles are very simple, and you could get images on the screen pretty quickly. In your case, you would be ray-marching a heightfield. Realistic terrain renders are often aided by a good atmospheric model-- as you progress, you may want to look up Rayleigh scattering and the Henyey-Greenstein model (both of which describe how light is scattered by a small particle or volume of air).   As I always tell people, my favorite book for realistic offline rendering is Physically Based Rendering, which is very clear, very rigorous, and very thorough."
GraphicsProgramming,35qc0z,harveypekar84,1 point,Tue May 12 18:06:25 2015 UTC,I think im sorta unsure of the difference between them.
GraphicsProgramming,35qc0z,ietsrondsofzo,2,Tue May 12 18:08:15 2015 UTC,"Realtime := using your graphics card to produce interactive graphics, like in a game. You will get game-quality graphics, which won't be as realistic, because you are limited to doing everything in 1/30th or 1/60th of a second. Realtime graphics are easily made interactive; i.e. you can move and look around the scene with your keyboard/mouse/trackpad.   Offline := Writing a program that will crunch for awhile, and then output a realistic image. This strategy is what films and television use to produce their special effects. It can take a long time to produce an image, because the software is usually doing light transport, a computationally expensive physics calculation. Images will look much more realistic, though.   IMO, the latter is easier to get started in because you don't need to learn an API to produce anything, assuming you know the principles."
GraphicsProgramming,35qc0z,Madsy9,1 point,Tue May 12 18:14:59 2015 UTC,Oh I guess realtime is what Im looking for then.
GraphicsProgramming,35qc0z,Alpha2spam,2,Tue May 12 18:19:09 2015 UTC,"That math books is really awesome, the author's a legend. Can't go wrong with it. You'll need a book about graphics as well, not sure if the Luna book has it. It's better to learn real-time graphics, and apply it to an API, then visa-versa. The Real-time Rendering book by Akenine-moller and Haines is a reference text.  Terrain rendering is pretty neat, because it covers a lot (shading, geometry, streaming), but beware, it's a huge topic. Start simple (height field), work your way up there. And enjoy!"
GraphicsProgramming,35lvjt,GaianNeuron,1 point,Mon May 11 16:00:25 2015 UTC,Thanks for posting this. I have a friend who was working on water surface rendering. He'll be intrigued. :D
GraphicsProgramming,35fr0q,night-owl0,3,Sat May 9 22:40:04 2015 UTC,"If you are getting into stuff and are having success with Three.js run with that.  You can do a lot with WebGL, it is a pretty great technology.    There are books that you will want to read, but the two that you mentioned are not for you right now (because of what you are doing).  Physically Based Rendering is a fantastic book on how to write a modern, viable, offline ray tracer, but that won't help you with your real time stuff.    Watching video on linear algebra is a great idea.  To see a concept visualized helps understand it so much better.  You can watch the MIT videos on linear algebra when you feel you are getting good at it, but they move very quickly.    There will be very compelling reasons to learn C++11 and modern C++ techniques later, but while you are learning stick with what works.  A great meta way to learn will be to write webgl stuff that visualizes the linear algebra that you are learning.  I can tell you from experience that that is a very good way to solidify knowledge and it is actually helpful to anyone else trying to learn."
GraphicsProgramming,35fr0q,__Cyber_Dildonics__,2,Sun May 10 00:28:56 2015 UTC,"Thanks for the response.  I'll keep pushing forward with Three.js for the time being, and look into linear algebra.  Thanks for the tip as well!"
GraphicsProgramming,35fr0q,fattyforhire,1 point,Sun May 10 02:07:38 2015 UTC,"If you're worried about learning openGL vs webGL, they are practically the same. The pipeline is very similar and the shaders are near identical. I'd suggest starting on webGL and JavaScript before openGL and c/c++. I've done both and webGL is easier to get started up and port. OpenGL requires a ton of dependencies and libraries to choose and install, some of which are OS specific"
GraphicsProgramming,35fr0q,bitshifternz,2,Sun May 10 02:23:16 2015 UTC,"Check out https://www.shadertoy.com, it's pretty insane what can be achieved purely in shaders and you can fiddle with the source of everything on there to get an idea of how it works."
GraphicsProgramming,35fr0q,Pik16,2,Sun May 10 04:03:11 2015 UTC,"Why not a demoscene demo for PC or some other platform? Find a party to release at and write a nice production. It doesn't need to be that great. You'll just need someone to make a music track for you. Also Shadertoy is pretty cool, you can literally see what and how can stuff be done on just a single pixel shader."
GraphicsProgramming,35fr0q,Pik16,2,Sun May 10 22:27:38 2015 UTC,"I'd love to get involved with demoscene parties, actually.  I hadn't actually thought about looking for one.  Thanks for the suggestion!  I'll make sure you get greets in my first demo :D"
GraphicsProgramming,35fr0q,codyrushing,1 point,Tue May 12 05:30:49 2015 UTC,"Haha, great! You can PM if there's any questions. Check out pouet.net and scene.org as well if you don't know them already. Also my ""scene"" name would be luutifa of Mehu, for that greet or something :D"
GraphicsProgramming,35fr0q,gott_modus,2,Tue May 12 12:22:47 2015 UTC,"I'm in the exact same boat as OP (front end dev wanting to get into graphics and ""creative coding""), so all of these comments have been super helpful.  Going to dig into three.js more, and possibly openframeworks."
GraphicsProgramming,35fr0q,Danoli3,1 point,Mon May 11 15:59:21 2015 UTC,"There's quite a few good websites on graphics these days. I learned mostly from the Arcsynthesis website. Last I checked the site was down, but there are PDFs of its text floating around.  Google ""Learning Modern 3D Graphics Programming"".  It's all C++ and OpenGL 3.3, which may or may not of be interest to you."
GraphicsProgramming,35fr0q,__Cyber_Dildonics__,1 point,Sun May 10 03:41:27 2015 UTC,C++ gives you more power.  I'd recommend http://openframeworks.cc as a nice framework to use.  0.9 is coming out soon updated with all latest platforms and architectures.
GraphicsProgramming,35fr0q,Danoli3,1 point,Sun May 10 08:24:23 2015 UTC,I think that is better for someone who wants to do something they can't with webGL.    Do you know how openframeworks compares to cinder?
GraphicsProgramming,35fr0q,akmark,1 point,Sun May 10 15:34:48 2015 UTC,"It's very similar, I'd say openframeworks has possibly a slightly harder learning curve, but the output is usually better, and for more platforms."
GraphicsProgramming,35fr0q,akmark,0,Sun May 10 17:03:51 2015 UTC,You might want to explore processing. It might let you dig into some neat effects before dealing with the infrastructure of a 3D framework. In my opinion it's not that 3D is much harder I just find myself getting bogged down in building or learning the infrastructure to make 3D interesting and it's much easier to open up mspaint and make a 2D asset rather than doing the work of 3D (or building a model loader for 3D).
GraphicsProgramming,356zgb,mattdesl,1 point,Thu May 7 16:59:55 2015 UTC,Kai's Power Tools all over again!
GraphicsProgramming,356zgb,jurniss,1 point,Thu May 7 23:56:54 2015 UTC,"Interesting little glitch: http://imgur.com/7giNMMJ  I happen to recognize this pattern as (x*y)%n < n/2, no idea why that happens though."
GraphicsProgramming,3530ld,WeAreWolves,8,Wed May 6 18:01:13 2015 UTC,checkout .obj file format. tons of reference models
GraphicsProgramming,3530ld,harveypekar84,3,Wed May 6 18:09:38 2015 UTC,"and beware that face indexing starts with 1, not 0!"
GraphicsProgramming,3530ld,lycium,3,Wed May 6 18:32:41 2015 UTC,"That is a metric bitch that one, had me going for a whole weekend one time. The trouble is, it looks almost right, so you think it's just a parsing error or the like.  Also watch out that your parser doesn't ignore duplicates, as this makes almost perfect mistakes too."
GraphicsProgramming,3530ld,PandaHammer,2,Wed May 6 19:14:37 2015 UTC,"haha, exactly! Rite of passage that one"
GraphicsProgramming,3530ld,harveypekar84,2,Wed May 6 20:24:25 2015 UTC,And the fact that the indices can be negative. I spent half an hour trying to find out why my reader would crash until I decided to open the file and scroll through hundreds of thousands of lines to notice that there were negative indices.
GraphicsProgramming,3530ld,solidangle,1 point,Wed May 6 21:53:16 2015 UTC,"Thanks, I kept this in mind when I was coding a reader this afternoon. Otherwise I definitely would have overlooked it."
GraphicsProgramming,3530ld,svdree,4,Wed May 6 20:53:46 2015 UTC,For example these models from Morgan McGuire
GraphicsProgramming,3530ld,TurkishSquirrel,1 point,Wed May 6 19:07:40 2015 UTC,I'll definitely try these out once I have an octree or BVH implemented.
GraphicsProgramming,3530ld,lycium,1 point,Wed May 6 20:43:05 2015 UTC,"Seconding .obj as a simple, convenient model format, there's also lots of easy to use loaders for these files. My preferred loader is tinyobjloader, it's very simple to use."
GraphicsProgramming,3530ld,metalmutt,1 point,Wed May 6 19:10:16 2015 UTC,"The author of that library is also a really awesome dude, and will fav/retweet almost anything ray tracing related :)"
GraphicsProgramming,3530ld,harveypekar84,1 point,Wed May 6 20:17:11 2015 UTC,"Exactly what I was looking for, thanks! Edited the op with some results"
GraphicsProgramming,3530ld,arkenthera,3,Wed May 6 20:42:32 2015 UTC,"You might be getting that spotting from your reflections because floating point rounding is placing the start of the reflection rays on the wrong side of the polygon, and the reflection ray intersects the polygon it was trying to reflect from. You can usually fix this (abit, hacky, by moving the reflection ray's origin by .001 * the normal."
GraphicsProgramming,3530ld,solidangle,2,Wed May 6 22:41:39 2015 UTC,Thanks! I never would have figured out why they were spotting. I loaded another .obj and tested. Here's the before and after. I presume the reflections are still weird because of the relatively low triangle count (~1000). The reflections when I have spheres in the scene are much more accurate.
GraphicsProgramming,34niqb,dizigma_,8,Sat May 2 19:59:47 2015 UTC,"Billboarding is much easier than all that, you don't need to do any manual rotation, translation etc.  The following does the job and is fast:  Pre-multiply your model and view matrices, so:  float[16] MVMatrix = ViewMatrix * ModelMatrix;   Then set the following values of that MVMatrix to zero:  MVMatrix[1] = MVMatrix[2] = MVMatrix[4] = MVMatrix[6] = MVMatrix[8] = MVMatrix[9] = 0.0f;   Then set the following values to be the scale. [it is possible to retain the scale from the other matrices, but it involves roots and is probably a little slow even on modern android devices]  MVMatrix[0] = MVMatrix[5] = MVMatrix[10] = scale;   Then multiply MVMatrix  with your projection matrix to get what you send to your shader.  this can also be done in the shader but it will be done for each and every vertex, so avoid this if you can.  This works by effectively deleting the rotation element of the matrix from the point of view of the camera, so you don't have to worry about manual alignment.  If you are rendering a lot of them, look into instancing or reducing your draw calls to get a better framerate.  Reply if you need any more help.  PH"
GraphicsProgramming,34niqb,PandaHammer,2,Sat May 2 20:23:12 2015 UTC,"Ha, it always takes me about 3 matrix ops to get that right!"
GraphicsProgramming,34niqb,specialpatrol,2,Sun May 3 11:54:22 2015 UTC,"But this still requires drawing every billboard individually, no?"
GraphicsProgramming,34niqb,PandaHammer,2,Sat May 2 23:59:59 2015 UTC,"Yes, but try it, it is seriously fast."
GraphicsProgramming,34niqb,Flafla2,2,Sun May 3 07:24:59 2015 UTC,You could do it with a vertex shader.
GraphicsProgramming,34niqb,Madsy9,5,Sun May 3 14:05:43 2015 UTC,"For OpenGL ES 1.x you have two options:   Use glGetFloatv and glLoadMatrix to get the current modelview matrix and reset the rotation submatrix to identity, then reload the modified matrix. This will require that every quad has its own draw call however. That's really slow. If you have support for the GL_OES_point_sprite extension you can enable point sprites with glEnable(POINT_SPRITE_OES) and enable automatic generation of texture coordinates with glTexGen(POINT_SPRITE_OES, GL_COORD_REPLACE_OES, GL_TRUE). Then when you render GL_POINT primitives, they are rendered as billboarded quads with the same width and height units as set by glPointSize.   If you use OpenGL ES 2:    OpenGL ES 2 supports point sprites out of the box. You can write to the gl_PointSize variable in the vertex shader, and read gl_PointCoord in the fragment shader to get the generated texture coordinate. Point sprites are enabled by default, and texture coord generation is enabled by default. I.e no need for the glEnable and glTexGen calls like in OpenGL ES 1.x   Also, /r/GraphicsProgramming is more tailored towards the theory, not graphics APIs. Questions such as this is probably a better fit for /r/opengl"
GraphicsProgramming,34q9b2,Hackerdeadly,10,Sun May 3 16:31:42 2015 UTC,"Physically based simulation would be where you're using physics to simulate something for the sake of getting an accurate simulation.  Physically based animation would be animating something in a way that it appears to agree with the laws of physics.  Physical simulation for computer animation would be where you use a physical simulator in order to produce an animation that looks plausible. Note that the simulation does not need to be a numerically accurate portrayal of reality; it just needs to convince a human audience.  Physically based animation for computer graphics could be something where you use physically based animation to produce a computer graphic. For example, you might simulate smoke in order to a produce a picture of a match burning.  Physically based modelling for interactive simulation and games would be simulation that is able to run in real time, so it can immediately respond to interaction from human users. Many types of simulation cannot run in real time (it might take 10 minutes of simulation to produce 1 minute of animation), but you can get simulation to run in real time by using very efficient algorithms or by cutting corners on your accuracy.  Cognitive modelling is where you use a simplified model of how a human or animal thinks in order to predict how it would respond to something. You might be able to use this to animate herd behavior, like in the stampede in Lion King."
GraphicsProgramming,34q9b2,GreyMX,7,Sun May 3 18:40:36 2015 UTC,Don't do his homework for him.
GraphicsProgramming,34q9b2,moron4hire,0,Mon May 4 15:52:37 2015 UTC,You don't know how to read Wikipedia or somethin?
GraphicsProgramming,34kbik,blackenstone,1 point,Fri May 1 22:07:35 2015 UTC,Wow looks very nice. Thanks for open sourcing it. I'll take a look at how you created the different effects.
GraphicsProgramming,34kbik,andiCR,1 point,Sat May 2 00:13:26 2015 UTC,Source: https://github.com/codygibb/animus-visualizer
GraphicsProgramming,34kbik,CodyDuncan1260,1 point,Fri May 15 13:25:21 2015 UTC,"Lovin that commit message: ""PUSH TO PUBLIC!!! WHOO"""
GraphicsProgramming,33n4mv,wangninja,6,Thu Apr 23 21:38:18 2015 UTC,That's not what I see when I do that.
GraphicsProgramming,33n4mv,pyalot,3,Thu Apr 23 22:08:52 2015 UTC,"I've definitely seen looping, moving splotches of color in my closed eye vision before. When I push on my eyes, usually in a corner, I see something more like the degaussing of a CRT monitor.  What I find amusing is that Shadertoy is actually working without crashing Firefox (due to performance hang)."
GraphicsProgramming,33n4mv,Phyllostachys,4,Mon Apr 27 16:36:44 2015 UTC,"The reason shadertoy's crashing firefox (and hanging the chrome GPU process) has to do with shader compilation.  For various reasons, shader compilation can become stuck/foreverlooped as the various compiler stages try to compile/unroll/""optimize"" it. Even though the shader is valid GLSL, and even though it hasn't actually run on the GPU yet. This is mostly a problem with drivers, sometimes a problem with the compatibility wrapper ANGLE.  Shadertoy doesn't track which shaders lead to this behavior (they could). Consequently, shader authors have no clue if their shader is crashing 99% of peoples machines or 1%. Lastly, shadertoy doesn't contain shader compilation to specific shader pages, but just blasts them at every page they show, even the overview pages where there's 10 WebGL canvases all compiling their shaders.  Shadertoys author refuses to fix his page because he's of the opinion it's the user-agents job to fix things. The user agents on the other hand refuse to fix the problem because they're of the opinion it's the GPU vendors job to fix it. The GPU vendors on the other hand are of the opinion that authors shouldn't write broken shaders...  I've now used a lot of characters to spell out industry circle jerk."
GraphicsProgramming,33n4mv,pyalot,2,Mon Apr 27 17:22:48 2015 UTC,"I've now used a lot of characters to spell out industry circle jerk.   And brilliantly so, thanks for the scoop! I've been wanting to play with Shadertoy because it seems like it gives more control (as compared to GLSL sandbox) but had problems getting the main site to load (due to all the various shader examples trying to load).  It seems better now compared to the last time I checked it out."
GraphicsProgramming,33n4mv,Phyllostachys,2,Mon Apr 27 17:59:47 2015 UTC,It seems more like what I see when I have a migraine.
GraphicsProgramming,33n4mv,RowYourUpboat,1 point,Fri Apr 24 08:24:57 2015 UTC,"It's not the only kind I've seen. However, I've had a migraine before where I saw something like this. Instead of shrinking, it grew from the center outward slowly over the course of an hour."
GraphicsProgramming,33n4mv,VeXCe,2,Fri Apr 24 11:15:39 2015 UTC,"That's pretty good, I get almost the same patterns!"
GraphicsProgramming,33n4mv,planetelex,4,Thu Apr 23 23:00:00 2015 UTC,This is... I mean it's what I see too... But I never thought to make a shader of it. Good job though
GraphicsProgramming,33n4mv,ChainedProfessional,1 point,Fri Apr 24 22:31:17 2015 UTC,"Thanks! It sounded like a fun side project, so I gave it a shot."
GraphicsProgramming,33n4mv,ChainedProfessional,3,Fri Apr 24 22:58:21 2015 UTC,Can someone post a screenshot to imgur?
GraphicsProgramming,33n4mv,WTFIsThisShitImReadi,3,Thu Apr 23 23:12:13 2015 UTC,Here you go! Gif conversion killed a little bit of the quality.
GraphicsProgramming,33d1vb,Apothiem,8,Tue Apr 21 15:20:08 2015 UTC,As long as you don't move your camera there are a few workarounds but they are rather ugly and don't solve your core problem.  150 trees  doesn't sound too much and should be doable. My guess is that you are rendering too many triangles or that you render them inefficiently. How many triangles do your trees have? Are you using different levels of detail? How are you rendering your scene? Do you use instancing? Do you copy mesh data to the GPU once or every frame? Have you tried profiling your program?
GraphicsProgramming,33d1vb,KeinBaum,5,Tue Apr 21 15:51:46 2015 UTC,I'm using a simple glutSolidCone() on top of a glutSolidTorus() to make my tree. I store each tree in a vector v_tree and traverse through the vector each time. I think what arkenthera said may be right. I call the render method in my environment class in my main method render class. The render in environment creates all 150 trees and stores it in the vector every time then renders it so i have this in my main render method:           for(int q=0;q<v_Trees.size();q++){  //Render Trees andground      v_Trees[q].Envo::render(); }
GraphicsProgramming,33d1vb,cleroth,16,Tue Apr 21 17:23:08 2015 UTC,I really don't think those glut functions are designed to be efficient. You should use models instead.
GraphicsProgramming,33d1vb,goal2004,7,Tue Apr 21 17:56:39 2015 UTC,"This here is the right answer. It's not optimized for anything. It's just a set of tools for managing the window and input, which also happens to have some primitive-rendering functionality."
GraphicsProgramming,33d1vb,KeinBaum,3,Wed Apr 22 03:27:46 2015 UTC,"Ok, I'm  not a glut expert but from what I understand glut is not exactly a high performance library. I can see two things you can try without giving up glut:   A very simple LOD: Your render functions take arguments for how many sides (subdivisions) to render. Use less sides for trees farther away. Culling: Only trees within a certain area a visible. This area is called the view frustum (assuming you use a perspective projection to draw stuff). With the distance to the camera and the radius and position of a tree you can roughly determine if that tree is visible. Only render those visible trees.   A better option might be to step away from glut and use a different library or implement your own renderer. It's going to be much more flexible and, if you do it right, much more performant. However it is also going to be more work. You'll need to learn more low-level OpenGL like how to organise your data in buffers and how to actually draw stuff.  If you are really interested in low-level OpenGL, I'd go for the second route straight away. If you just want to render a few trees as simple as possible, I'd try improving the glut version first and only if it is still too slow switch to your own OpenGL code."
GraphicsProgramming,33d1vb,arkenthera,3,Wed Apr 22 00:00:52 2015 UTC,Yeah you're not doing any culling. You could watch this to get rough understanding of scene management.
GraphicsProgramming,33d1vb,zuurr,1 point,Tue Apr 21 17:55:38 2015 UTC,"You should use models, like the other posts say (don't bother with scene graphs unless you think you'll benefit from them, they're actually not that great of an idea for a lot of games).  That said, you also should not create them every time. I'm not sure if glut can simply return the geometry of the cones instead if drawing it into the screen, but if it can, this would work too.  Note that your performance problems are from regenerating the meshes every time, and not from drawing it. Computers have been able to handle drawing 150 trees every frame for a very long time. You can probably get away with not frustum culling yet if you fix this (and if you don't fix it, culling isn't going to help you)."
GraphicsProgramming,33d1vb,arkenthera,0,Wed Apr 22 05:01:58 2015 UTC,"Adding to this,OP probably doesnt use any scene graph system and rendering whole 150 objects at the same time without culling(frustum culling for instance)  even they're not in the frustum therefore poor performance."
GraphicsProgramming,33d1vb,kikkurs,2,Tue Apr 21 16:37:34 2015 UTC,"There are a lot of places where the bottleneck can be in cases like these. Could you be more specific with your situation? Things like triangles drawn per tree, shader complexity, what specific draw commands are you calling, etc."
GraphicsProgramming,33d1vb,kikkurs,3,Tue Apr 21 15:36:20 2015 UTC,"I wasn't really sure how to word it but i realise that i can't stop rendering the world as i need to traverse it. I'm using a simple glutSolidCone() on top of a glutSolidTorus() to make my tree. Then just using lighting,no shaders or textures as such."
GraphicsProgramming,33d1vb,0Camus0,7,Tue Apr 21 17:16:30 2015 UTC,"/u/cleroth has hinted at this, these functions are used for debugging purposes, if they are even used at all nowadays. The reason for this is the geometry (vertices and triangles) doesn't reside on the gpu, so you're sending everything to the gpu per frame. There's the primary bottleneck, modern hardware can render many millions of shaded triangles per second, but only if it's set up properly.  Using models is a solution in more fully featured frameworks and engines, but basic OpenGL does not have this.  What you'll need to do is to use vertex buffer objects instead to store your geometry on the gpu, and just use glDrawElements (or similar functions) to draw them. You will need to generate your cone/torus yourself and send them to the gpu at initialization. To save yourself some headache, start from a snippet or tutorial, there are lots on this subject.  Not culling anything like /u/arkenthera said is probably less likely to be a problem with scenes this simple. Scene based culling is quite timeconsuming when written from scratch, so hold off on that unless you don't have other options.   On the other hand, if your priorities lie with making a game rather than a learning experience, I highly recommend using a framework or engine as they do these things very well already.  Edit: grammar"
GraphicsProgramming,33d1vb,VilHarvey,2,Tue Apr 21 20:09:27 2015 UTC,"You can use vertex buffers and just add or remove to the buffer the changing trees, you could also use instancing and send an array of matrices with the position of each tree doing all in one single draw call."
GraphicsProgramming,33d1vb,FrezoreR,2,Tue Apr 21 23:21:31 2015 UTC,"I guess you're using OpenGL 2.1 or lower, since you're using glutSolidCone & glutSolidTorus and you mentioned you're not using a shader? If so, you can create a display list and use that to re-render them efficiently instead of recreating them in full each time. The code in your main render function will look something like this:  if (myTrees == 0) {   myTrees = glGenLists(1);   glNewList(myTrees, GL_COMPILE_AND_EXECUTE);   // Do your normal drawing here, e.g.:   for (int q = 0; q < v_Trees.size(); ++q) {     v_Trees[q].render();   }   glEndList(); } else {   glCallList(myTrees); }   A display list stores the rendering comands you issue, along with their data, on the GPU. It's a bit like a macro: you start recording by calling glNewList, do your usual drawing, then tell it you're finished by calling glEndList. Afterwards you can tell the GPU to replay the same drawing commands using glCallList. The advantage of this is that the GPU keeps a copy of all your drawing commands, so it can re-run them very quickly; it's a lot faster than redoing all the calculations and re-issuing all the drawing commands from the CPU."
GraphicsProgramming,33d1vb,__Cyber_Dildonics__,2,Wed Apr 22 00:36:18 2015 UTC,If you want performance you should drop glut right away.   If all trees use the same mesh you could and should use instanced rendering. I.e. one draw call for N trees.
GraphicsProgramming,33d1vb,Shadowratenator,1 point,Wed Apr 22 13:17:19 2015 UTC,"Other people have gone through the most likely reasons why what you are doing is slow.    But, what you should really do is profile, then you will see the real data instead of guessing."
GraphicsProgramming,338n9k,tavianator,3,Mon Apr 20 15:04:45 2015 UTC,Are min and max really branchless?
GraphicsProgramming,338n9k,__Cyber_Dildonics__,4,Mon Apr 20 23:55:36 2015 UTC,SSE has min/max instructions.  And even without SSE there's conditional moves.
GraphicsProgramming,338n9k,RowYourUpboat,2,Tue Apr 21 00:08:13 2015 UTC,"Yep, most modern processors (and GPUs) have instructions that treat min/max as a single mathematical op (instead of a branching op).  However, many C/C++ libraries don't use the CPU-specific instructions (since the old fashioned macro usually works just fine) so you have to do a little groundwork if you're implementing optimized geometry code using that assumption.  For integers in plain C, there are also bit-twiddling hacks to do branchless min/max."
GraphicsProgramming,338n9k,__Cyber_Dildonics__,3,Tue Apr 21 02:09:15 2015 UTC,"Well GCC is smart enough to optimize the old fashioned macro to the single instruction, so it's not too bad.  And on x86 at least, it will use cmov for the integer case which is probably faster than the bit twiddling hacks."
GraphicsProgramming,338n9k,thunabrain,1 point,Tue Apr 21 02:31:44 2015 UTC,Very cool and interesting.  Have you taken a look at Embree's source? I am guessing it is does something similar at least in the ISPC code since it uses bounding volume hierarchies and packets (up to 16x in size).  I think the C++ might use SSE and AVX intrinsics too.
GraphicsProgramming,338n9k,thunabrain,3,Tue Apr 21 11:54:48 2015 UTC,"At least Embree 1 used branchless SSE intersection code (I haven't looked at Embree 2). It also contained one further optimization that eliminates one min and one max call compared to the linked article.  The article performs a min(t1, t2) call to determine the closer hitpoint (and same for the more distant hitpoint), but it turns out that which of t1 and t2 are closer only depends on the sign of the ray directions. If you intersect many boxes (which you do for a BVH), you can precompute a swizzle mask once and swap t1/t2 directly without doing a min/max."
GraphicsProgramming,32x9tl,thereisnocharacterli,2,Fri Apr 17 14:05:35 2015 UTC,Is there any info on the TOAST map anywhere? Google is not too helpful on this it seems.  EDIT : Found his github
GraphicsProgramming,32x9tl,pants75,1 point,Mon Apr 20 08:12:04 2015 UTC,This is a good resource which I used to make the first few experiments with TOAST maps:  http://www.worldwidetelescope.org/docs/worldwidetelescopeprojectionreference.html  and also this: http://drdave.co.uk/blog/tag/TOAST  It's a really interesting way of mapping quads to a sphere. I actually think it would be an easier standard for environment maps than cubemaps which often feels inconvenient. Just use repeat wrapping on both S and T on the TOAST map and you have the same uniform lerp as with a hardware accelerated cubemap :)
GraphicsProgramming,32x9tl,pants75,1 point,Tue Apr 21 11:10:00 2015 UTC,Thanks for that. I assume this is basically the same thing?  http://en.wikipedia.org/wiki/Peirce_quincuncial_projection
GraphicsProgramming,32x9tl,autowikibot,1 point,Tue Apr 21 12:33:37 2015 UTC,Peirce quincuncial projection:       The Peirce quincuncial projection     is a conformal map projection. It was developed by Charles Sanders Peirce in 1879.    Image i - Peirce quincuncial projection of the world. The red equator is a square whose corners are the only four points on the map which fail to be conformal.     Interesting: Guyou hemisphere-in-a-square projection | Adams hemisphere-in-a-square projection | 1877 in science | Charles Sanders Peirce   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words
GraphicsProgramming,32x9tl,autowikibot,1 point,Tue Apr 21 12:33:56 2015 UTC,"That's a cool one :) I'm not an expert so I'm not really familiar with it. It seems to be based on elliptic functions (like ""cn"") which makes the meridians curved rather than straight (like in the TOAST map).  Edit: Noticed that they may not be straight in the TOAST map either, at least not the images that my app spits out :P Possibly a bug  I also don't know how easy that is to map to a primitive. TOAST projections map well to octahedra or HTM subdivisions of octahedra."
GraphicsProgramming,32tre7,Suttonian,10,Thu Apr 16 17:23:45 2015 UTC,"It doesn't live up to the hype, as the hype was totally created by their wrong promises about replacing the ""triangle industry"". It seems that they learned the limitations of their technology at last. The application is exactly what they show in the online demo, not more."
GraphicsProgramming,32tre7,thedrakes,9,Thu Apr 16 19:00:49 2015 UTC,http://imgur.com/2HLfQvV  Unlimited detail my ass.
GraphicsProgramming,32tre7,jrkirby,5,Thu Apr 16 20:13:24 2015 UTC,HOLY SHIT!!! FUCK POLYGONS.
GraphicsProgramming,32tre7,cleroth,7,Sat Apr 18 11:06:16 2015 UTC,We should know better than to deal with graphics programming snake oil here.  Maybe dynamically loading voxels can be useful to someone (what is the geospatial industry?) but I can't see why they have anything special over other voxel engines.
GraphicsProgramming,32tre7,__Cyber_Dildonics__,4,Thu Apr 16 18:01:42 2015 UTC,"Exactly. However, I'm all for these guys showing their stuff off to get VCs interested and pushing the tech forward.   Can't hurt to have someone pushing this direction, as long as we know to contain our excitement"
GraphicsProgramming,32tre7,mrjmoak3,11,Thu Apr 16 18:57:03 2015 UTC,"Ha yeah my excitement contains itself when I see unlit, static, aliasing  voxel art."
GraphicsProgramming,32tre7,__Cyber_Dildonics__,3,Thu Apr 16 19:51:33 2015 UTC,Hahahahahahaha
GraphicsProgramming,32tre7,mrjmoak3,0,Thu Apr 16 20:30:27 2015 UTC,"It could easily be lit by rendering a lightmap.  Can't do shit about it being static and cubic, though."
GraphicsProgramming,32tre7,mindbleach,2,Fri Apr 17 05:48:03 2015 UTC,"""Easily"""
GraphicsProgramming,32tre7,stoopdapoop,0,Fri Apr 17 13:31:33 2015 UTC,"Yes, easily. You render from the light's perspective. It's as commonplace as texture-mapping.   edit: ah, finally noticed that late-night brain fart. Shadow map, not lightmap."
GraphicsProgramming,32tre7,mindbleach,1 point,Fri Apr 17 14:55:20 2015 UTC,I think they rushed in at first but then held back for too long. Stuff like Atomontage has them beat now.
GraphicsProgramming,32tre7,ccricers,1 point,Thu Apr 16 20:58:07 2015 UTC,"They were never really dealing with voxels anyway, except to partition their point cloud. Atomontage is actually rendering voxels, and doing cool operation on them."
GraphicsProgramming,32tre7,jrkirby,6,Sat Apr 18 16:09:09 2015 UTC,See also http://potree.org/wp/demo/ and http://webglstudio.org/impart/setviewer/index.html?scene=studio.json
GraphicsProgramming,32tre7,AlunAlun,6,Thu Apr 16 20:06:26 2015 UTC,This doesn't use WebGL. It's using Native Client (hence Chrome only) and appears to be rendering entirely using the CPU. Also looks like they are using information from the previous frame to speed up the next one.
GraphicsProgramming,32tre7,fb39ca4,1 point,Thu Apr 16 23:53:26 2015 UTC,Ah thank you - incorrect assumption on my part.
GraphicsProgramming,32tre7,FrenchHustler,4,Fri Apr 17 02:32:34 2015 UTC,"Thing is... this is not adequate technology for games. For geospatial surveying, it's probably one of the best application I've seen -- and that technology CAN succeed in that field.   All of this is based on already scanned data. I highly doubt that any sort of animation, physics simulation, or dynamic lighting can be applied to those point clouds -- something that is required in the gaming industry. The geospatial industry could care less about all that as long as researchers are able to stream and visualize the huge amount of data they've scanned and acquired. Give those people a good visualization solution and a way to easily import a wide variety of data and they'll be happy."
GraphicsProgramming,32tre7,donalmacc,1 point,Thu Apr 16 22:22:02 2015 UTC,"If the vowel data (treated as particles) is a high enough density, then it could. Nvidia flex uses particles as the basis for fluid and cloth, and for rigid bodies too."
GraphicsProgramming,32tre7,fb39ca4,0,Thu Apr 16 23:11:50 2015 UTC,vowel data
GraphicsProgramming,32tre7,jrkirby,3,Thu Apr 16 23:51:45 2015 UTC,"It's when you take 4 UTF-8 characters, all of them either 41, 45, 49, 4F, 55, 59, 61, 65, 69, 6F, 75, or 79, concatenate them together, and cast it to a floating point number."
GraphicsProgramming,32tre7,pyalot,0,Fri Apr 17 01:08:07 2015 UTC,But only when they're uttered by Bruce Dell (in his typically pedantic/nerd voice).
GraphicsProgramming,32tre7,thunabrain,3,Fri Apr 17 09:38:02 2015 UTC,"Interesting! I was always convinced that they were doing point cloud rendering, not voxels. It also looks that way in the first two scenes (e.g. surfaces start breaking up into point samples when you zoom in), but in the next two scenes it definitely looks like voxels.  In terms of technology it's an impressive piece of kit, it's just unfortunate that they pitched it with such ridiculous claims. I mean sure, it's not rendering at full resolution and way below 30FPS (at least on this machine), but it's rendering what looks like really high-res outdoor data on a single CPU core with a very reasonable memory footprint (you could probably run this on a tablet). I can see this having an impact in the geospatial industry, and I would wager that there's other fields that could benefit from interactive visualization of massive models. The only thing I find dodgy is that it only runs single-core, which from what I can gather from the videos has been the case during the entirety of development. Is there some inherently sequential part of this? That would of course hinder GPU adoption.  I don't know why the focus of their videos keeps coming back to games. They could have avoided a lot of the negative backlash if they hadn't tried to sell their technology as being capable of doing something it's not."
GraphicsProgramming,32tre7,noobgiraffe,3,Fri Apr 17 03:05:50 2015 UTC,"I have been coding voxel engine for fun and did a fair amount of research. Basically their claims are bullshit. They at one point made a video showing how their solution is better then voxels which for anyone knowing anything about voxels sounded like ""we are actually using voxels just don't want to admit it"". The showed a lot of demos but all of them were completely static. This technology in it's current state is completely useless and not revolutionary at all. There are much better engines but they simply do not have the the marketing power. Check out Atomontage for example. When it comes to visualising voxel data there are also much better solution used for years to view medical scans as 3d models."
GraphicsProgramming,32tre7,cleroth,2,Fri Apr 17 09:31:12 2015 UTC,"There are much better engines but they simply do not have the the marketing power.   No, they just don't lie about it being impractical, so nobody really cares."
GraphicsProgramming,32tre7,fr0stbyte124,1 point,Sat Apr 18 11:19:45 2015 UTC,"Oh shit, Atomontage updated! I thought for sure the project was dead."
GraphicsProgramming,32tre7,lurend,2,Fri Apr 17 15:39:42 2015 UTC,Groundbreaking stuff: https://i.imgur.com/1UmIUGR.png
GraphicsProgramming,32tre7,mindbleach,2,Thu Apr 16 22:21:28 2015 UTC,"Oh hey, this thing again. My first ""holy shit that's a lot of upvotes"" comment was about those awful videos.   The explanation I recall from discussion on their video... a year ago? two years ago? ... was that they raytrace nested voxels, but only down to the scale where they're pixel-sized. So every frame you're just looking up a million-ish mostly-cached voxels.   Now having had five years to think about it, and being the fan that I am of technological lost causes, I have to admit it could be useful in games. Certainly not the miracle promised in those early dumb videos, but useful. It's raytracing, so you can do real reflections and maybe perfect sharp shadows. Voxel data can include materials and normals, or even be texture-mapped by projection, so you can splat onto a g-buffer and use modern shaders. Most importantly, having a raytracing-centric pipeline makes it possible to abandon polygons for polyhedrons - i.e., building solid 3D shapes out of pyramids or cubes or whatever and ""texturing"" them with volumetric data.   ... but that's probably not a great idea, because data starvation leaves you with blocky garbage."
GraphicsProgramming,32tre7,Necrolis,3,Fri Apr 17 05:37:52 2015 UTC,There is actually already published research on ray-traced 'nested' voxels for sharp shadows and high-quality voxel scenes (infact this is 2 years old now): http://www.cse.chalmers.se/~kampe/highResolutionSparseVoxelDAGs.pdf
GraphicsProgramming,32tre7,ccricers,2,Fri Apr 17 18:33:43 2015 UTC,"For a cool real use of fine detail voxels in games, there is VoxelQuest. It's built off something that has been years in development and still being worked on. I don't think his engine supports reflections yet, but lighting and shadows are pretty good and engine seems really fast in updating a lot of data (look at how a sine wave affects the whole scene in one video)."
GraphicsProgramming,32tre7,Allanon001,1 point,Fri Apr 17 20:30:44 2015 UTC,There graphics seem on par with the 1992 Comanche video game which also used voxels.
GraphicsProgramming,32tre7,andiCR,1 point,Thu Apr 16 20:40:47 2015 UTC,Is there a video on how it's supposed to work on a high end machine? All i see is shitty voxels on my shitty laptop.
GraphicsProgramming,32tre7,jrkirby,1 point,Thu Apr 16 23:30:48 2015 UTC,That's all it is. It's completely shitty.
GraphicsProgramming,32tre7,tobozo,1 point,Fri Apr 17 01:08:46 2015 UTC,Yet another Chrome-Only app.  [edit] and it took them 15 years to expose non standard tech
GraphicsProgramming,32s5sd,marfr960,3,Thu Apr 16 07:32:34 2015 UTC,"Oh man! Are you Marco Fratarcangeli? First of all great video, parallel physics are awesome. Especially soft body stuff.  Second of all, nearly 2 years ago I found your paper ""A Robust Method for Real-Time Thread Simulation"" and was like ""This is awesome I'm going to implement this!"".  So I set about writing the solver and got everything working but the contact/friction constraint. I had some Razer Hydra's hooked up and the Oculus Rift DK1 and it was rather fun to sling ropes around. But I was missing the ability to tie a knot like the amazing video that goes with the paper.  I might be just misunderstanding the core concept but I think there might be a printing error in the paper. At the very least you may be able to settle a mystery that's been in the back of my mind for quite some time.  You define the contact constraint as C(p) = [p - (p_n0 + p_v)]. Which Mueller defines a constraint as C:R3 -> R, but the equation you gave doesn't seem to be R3 -> R. It looks like R3 -> R3. Unless I'm missing something really obvious (totally possible, [] isn't a vector operator is it?).   I tried a myriad of constraint equations I created myself based on the point constraint of C(p) = (p - q_c) dot n_c in Mueller's paper, most worked for simple collisions but any knots I tried tying caused the simulation to explode. Perhaps because my equation was incorrect or it's gradient was off or any number of other factors. I eventually got distracted and it's been sitting in my code untouched for a while.  I know this is sort of an internet ambush about a paper published 8 years ago, but do you know what the collision constraint is or if I'm just reading the equation wrong? Either way I enjoy your work and I hope to read your new one on this parallel computation of PBD."
GraphicsProgramming,32s5sd,LordTocs,3,Thu Apr 16 16:42:28 2015 UTC,"hey, glad you like our work. The operator [] is the euclidean norm, that is the length of the vector. Do you see now?"
GraphicsProgramming,32s5sd,LordTocs,2,Thu Apr 16 17:10:45 2015 UTC,Oh! I've never seen the [v] notation I've always seen the ||v|| notation. I feel incredibly dumb now. The contact constraints in Mueller's paper are inequality constraints. Since an inequality constraint is satisfied if C() >= 0 and the euclidean norm is always >= 0. An inequality constraint would always be satisfied. Is your contact constraint an equality constraint?
GraphicsProgramming,32s5sd,LordTocs,3,Thu Apr 16 17:43:30 2015 UTC,"You are right, there is something fishy in the formula of C. And yes, the euclidean norm is always ||, I don't know why that formula has been wrote using square brackets.  Try to implement the edge-particle collision constraint like this: if the distance between p_0 and an edge is smaller than r, then apply p_v to p_0.  Note that the collision constraints should be re-computed for each iteration. In other words, line (8) in the formulation provided by Mueller should be moved between line (9) and (10). Intuitively, this is due to the fact that p_v is not the same for each iteration of the solver."
GraphicsProgramming,32s5sd,thedrakes,2,Thu Apr 16 18:31:44 2015 UTC,"Awesome, thanks for your help! I'll have to dust off my code and give that a try."
GraphicsProgramming,32s5sd,LordTocs,1 point,Thu Apr 16 18:40:43 2015 UTC,"Assuming the formula for p_v is meant to be like that, p_v is exactly the offset p should receive as he says. Now think about how you formulate this offseting of p as an inequality constraint. One way of doing this using multiple constraints: p.x >= p_n0.x + p_v.x (as p is the new state and p_n0 the old state, and then the same for y and z) this is equal to 0 <= p.x - (p_n0.x + p_v.x)  (the same as in the paper) . Therefore the [] are actually there to hint for that the equation gives multiple constraints i guess."
GraphicsProgramming,32s5sd,thedrakes,1 point,Thu Apr 16 18:50:46 2015 UTC,I don't this works because it's dependent on the direction of p_v lining up with your inequality check. If p_v is attempting to resolve the contact in a negative direction your inequality will instantly pass and no resolution attempt will be made.
GraphicsProgramming,32s5sd,LordTocs,1 point,Thu Apr 16 19:11:42 2015 UTC,"Well if p_v would line up like that, yes but it won't. It will always go from the second edge on Figure 3 to the upper point because that's how it is defined as collinear to p_n0 - p_n1 (though I'm not certain if they just insert both points of the upper edge, as like how I discussed it, it's only vertex-edge collision)  Edit: okay, I guess I made a mistake there, it would have to be p_v = (2r - ||p_n0-p_n1||) (p_n0-p_n1)/||p_n0-p_n1|| or sth like that"
GraphicsProgramming,32p0og,lgroeni,3,Wed Apr 15 15:37:03 2015 UTC,"This is awesome, thank you!"
GraphicsProgramming,32lf82,ccricers,15,Tue Apr 14 18:58:00 2015 UTC,"I thought this was going to be about how someone made a webgl sandbox and didn't have the sense to not auto-load a dozen heavy shader demos at once on the front page and was not disappointed.   Yes shadertoy is cool, while at the same time its front page is mind blowing fuckup of web design."
GraphicsProgramming,32lf82,__Cyber_Dildonics__,1 point,Wed Apr 15 00:07:53 2015 UTC,"I think most people who frequent there (including the webpage author) have über monster graphics cards, which might explain why they don't care that much :)"
GraphicsProgramming,32lf82,Madsy9,1 point,Thu Apr 16 20:49:56 2015 UTC,"Weirdly, in my experience CPU makes more difference to the front page load/hang time. My laptop with an i7 and mobile GPU gets it together much faster than my desktop with old CPU and a GPU that benchmarks much higher than the mobile one. Maybe the lag is the javascript set up?  EDIT: that said, I'm also comparing Firefox on Windows to Safari on OSX, so who knows."
GraphicsProgramming,32lf82,DerDangDerDang,1 point,Sat Apr 18 20:32:57 2015 UTC,Are you using a laptop graphic chip or a graphics card?
GraphicsProgramming,32lf82,StickiStickman,1 point,Tue Apr 14 20:15:23 2015 UTC,This is running on a desktop computer. Pentium G3258 and GTX 750Ti. It's still enough to able to run most games on reasonable settings at 1080p.
GraphicsProgramming,32lf82,KamiKagutsuchi,1 point,Tue Apr 14 22:58:00 2015 UTC,https://get.webgl.org/
GraphicsProgramming,32lf82,fb39ca4,1 point,Tue Apr 14 21:15:52 2015 UTC,Says it's disabled on my Chrome browser. I also updated my GeForce drivers to the latest version. So the only way to enable webGL on Chrome is to force it with the command line option?  I also agree with not loading every webGL widget on the search pages. It's too demanding on the browser. Maybe the website could cache a snapshot of the render and load that image instead.
GraphicsProgramming,32lf82,pyalot,1 point,Wed Apr 15 20:02:40 2015 UTC,"Enable native OpenGL with your browser. For Chrome, start the browser with the command line option --use-gl=desktop. For Firefox, go to about:config and set webgl.disable-angle to true.  Verify that it worked by going here and making sure ANGLE (under miscellaneous) is false. http://www.browserleaks.com/webgl"
GraphicsProgramming,32lf82,pyalot,1 point,Wed Apr 15 04:21:03 2015 UTC,"Jep, having problems here on Linux/GTX 780. Very annoying.   In Chrome it first it freezes for a while, then experiences 5 context losses, and then the GPU process is dead.  Firefox straight up crashes completely  Add-ons: %7B972ce4c6-7e08-4474-a285-3208198ce6fd%7D:36.0.1 BuildID: 20150305021524 CrashTime: 1429078079 EMCheckCompatibility: true FramePoisonBase: 7ffffffff0dea000 FramePoisonSize: 4096 InstallTime: 1426541551 Notes: OpenGL: NVIDIA Corporation -- GeForce GTX 780/PCIe/SSE2 -- 4.4.0 NVIDIA 331.20 -- texture_from_pixmap WebGL? libGL.so.1? libGL.so.1+ GL Context? GL Context+ WebGL+ ProductID: {ec8030f7-c20a-464f-9b0e-13a3a9e97384} ProductName: Firefox ReleaseChannel: release SecondsSinceLastCrash: 23 StartupTime: 1429078077 Theme: classic/1.0 Throttleable: 1 URL: https://www.shadertoy.com/ Vendor: Mozilla Version: 36.0.1 useragent_locale: en-US"
GraphicsProgramming,32lf82,TweetsInCommentsBot,1 point,Wed Apr 15 06:00:17 2015 UTC,Followup by IQ/shadertoy on twitter: https://twitter.com/pyalot/status/588221375980183552
GraphicsProgramming,32lf82,pyalot,2,Wed Apr 15 07:38:33 2015 UTC,@pyalot   2015-04-15 06:04 UTC  @iquilezles @poljeremias @Shadertoy You might wanna look into that http://www.np.reddit.com/r/GraphicsProgramming/comments/32lf82/anyone_having_problems_with_shadertoy/     This message was created by a bot  [Contact creator][Source code]
GraphicsProgramming,32lf82,Winchestro,1 point,Wed Apr 15 07:38:49 2015 UTC,Everybody who encounters issues/crashes should report them there:   Chrome: https://code.google.com/p/chromium/issues/list Firefox: https://bugzilla.mozilla.org/ Safari: https://bugs.webkit.org/ IE: https://connect.microsoft.com/ie/   I've filed my crashes: https://bugzilla.mozilla.org/show_bug.cgi?id=1154688  Please file your issues/crashes with any browser/os you have it with.
GraphicsProgramming,32lf82,fb39ca4,-2,Wed Apr 15 12:23:43 2015 UTC,"You don't understand. Shadertoy is the house of wisdom, the domain of Muḥammad ibn Mūsā al-Khwārizmī reborn. You can not demand access, it gets granted. It is a gift. It may take months, or maybe years, but eventually you will be allowed to pass the gate of the thousand shaders. Just keep returning stronger and wiser until they open for you."
GraphicsProgramming,32gyfn,gott_modus,7,Mon Apr 13 18:44:24 2015 UTC,"I realise this is not /r/cscareerquestions, but I would appreciate an answer specifically related to graphics programming careers aswell."
GraphicsProgramming,32gyfn,donotworryigotthis,7,Mon Apr 13 19:38:42 2015 UTC,"The number 1 thing any employer (graphics or not) is looking for is obvious: Experience. If you can get paid experience, awesome, if not, side projects. Look at siggraph papers and implement some of the techniques explained in them. They don't have to be recent.  Make sure your linear algebra is strong.  What I've found is that outside of the DoD (to which never applied due to personal objections) there are very few ""entry level"" graphics jobs (ignoring the low-to-no-pay video game startups). You might be better off finding a stable company that has opportunities down the road and apply to that. Also, any experience as a programmer will benefit your resume, regardless of whether it's graphics related.  This was just my experience in looking for these exact kinds of jobs a year or so ago. YMMV.  Oh, and know your fundamentals. If you are looking for real-time graphics work maybe look at Computer Graphics: Principals and Practice. For ray tracing and the like Physically Based Rendering is highly recommended."
GraphicsProgramming,32gyfn,EldanRetha,5,Tue Apr 14 00:37:09 2015 UTC,"Good advice here. Let me just add to it.  Most interviews I've been through (especially at game companies) are still depressingly algorithm-heavy. Get some practice at home solving all those typical ""interview problems"" before applying. For rendering also focus on linear algebra and really understand those ray/X intersections and dot/cross product fundamentals. Write as much C++ as possible to become as comfortable as you can with the language.  PBRT is an excellent book. I strongly recommend to learn at least the basics of physically-based shading. Naty Hoffman's SIGGRAPH course slides and notes are highly recommended. As are all the other slides and course notes in each year's course."
GraphicsProgramming,32gyfn,cowpowered,0,Tue Apr 14 01:37:57 2015 UTC,"Thanks for the response. When you say ""DoD"", though, what are you referring to exactly?"
GraphicsProgramming,32gyfn,EldanRetha,5,Tue Apr 14 01:42:46 2015 UTC,Department of Defense
GraphicsProgramming,32gyfn,harveypekar84,-4,Tue Apr 14 01:55:18 2015 UTC,"Data oriented design, heavily related to #altdevblogaday and Mike Acton"
GraphicsProgramming,32gyfn,qartar,3,Wed Apr 15 15:11:23 2015 UTC,"Top three things not already on your list:   Solid C++ programming In-depth familiarity with at least one modern 3D API, whether it's Mantle, Metal, OpenGL 4, or DirectX 11. (You'd be surprised how many candidates come in with only experience in Unity or fixed function OpenGL.) Linear Algebra and more algorithms.   Hardware vendors are always looking for qualified graphics engineers. You won't be working on rendering engines but it's about as much graphics as anyone can get."
GraphicsProgramming,32gyfn,angrymonkey,2,Tue Apr 14 05:04:51 2015 UTC,For what kind of job? Realtime or offline?
GraphicsProgramming,32gyfn,ccricers,1 point,Mon Apr 13 20:24:36 2015 UTC,"Either or. If I understand correctly, offline refers to things like CG Animation in films, CAD software, Ray-traced imaging, etc. By that understanding, I only have (slight) professional experience in offline. However, most of my spare time has been spent doing realtime."
GraphicsProgramming,32gyfn,ccricers,2,Mon Apr 13 21:25:02 2015 UTC,"I have one of those niche cross-disciplinary degrees (BA in Electronic Media) My math seems to be weaker than most CS graduates and I haven't taken anything beyond Calc I or Discrete Maths in class. I can interpret pseudocode fine but come to a standstill at trying to read the big LaTeX math equations.  My professional background is in web programming but I want to make a lateral change in my career to real-time programming, preferably with graphics. I had a colleague refer me to a small firm which is working in imaging with medical equipment. Had a good interview with them, passed the basic technical test (I know basic linear algebra), and then later they told me they went with an outsourced supplier for a software release instead.  So right now I'm back to square one on finding a job. I don't necessarily want to find a job in the video games industry, but still related to graphics. These kinds of jobs seem sparse, or I'm just not looking in the right places."
GraphicsProgramming,329ajy,Sify007,4,Sat Apr 11 19:14:02 2015 UTC,"If you are only interested in graphics programming a SIGGRAPH membership is a lot more cost effective. Costs $42/year and you get access to recordings of all talks and courses from the last 10 years or so. Sure, not all of the talks are games specific, but they do have quite a lot of realtime graphics related content. Also, the courses are amazing."
GraphicsProgramming,329ajy,irabonus,7,Sun Apr 12 09:07:28 2015 UTC,"You are able to find most SIGGRAPH stuff (Course slides/notes and paper preprints) here. You only have to get a SIGGRAPH membership for the recordings, but it takes ages for the recordings to be uploaded to the Digital Library and a quarter for the recordings seems to be missing."
GraphicsProgramming,329ajy,solidangle,3,Sun Apr 12 12:30:08 2015 UTC,"Thanks for the link! I do find the recordings useful, especially for the courses, but that's probably just because I tend to retain information better when I listen to someone talking."
GraphicsProgramming,329ajy,irabonus,2,Sun Apr 12 15:37:12 2015 UTC,I had a subscription a while back and I can confirm it takes them ages to upload videos. The videos from SIGGRAPH 2012 were not there when 2013 was taking place. I also can confirm a lot of them not work. Maybe there is a point of subscribing back.  Thanks for the answers!
GraphicsProgramming,329ajy,irabonus,1 point,Sun Apr 12 22:12:20 2015 UTC,"Oh, I've only had the subscription since February, so I guess I didn't notice most of those troubles yet. Though yeah, now that you say it, especially the older videos are kinda hit and miss. I just thought that was an issue with my machine."
GraphicsProgramming,329ajy,memorystomp,3,Sun Apr 12 22:35:01 2015 UTC,"Look through the free stuff from this year first to see if this presentation format is useful to you.  Also, if a members-only presentation looks interesting search for it elsewhere.  A lot of people put slides up on their own site which will help you decide if you want to see the videos. Check out this collection of rendering/engine blogs if you just want to browse around."
GraphicsProgramming,329ajy,memorystomp,2,Sat Apr 11 19:41:05 2015 UTC,What about member only audio/video content? Do you find that worth 500 USD / year?
GraphicsProgramming,329ajy,zuurr,4,Sat Apr 11 19:47:53 2015 UTC,Don't buy it blind.  Watch some of the free videos and read through the slides to see if it is what you are expecting.  The member videos are very similar and I think you can even look through the descriptions without a membership.  I access it through a group plan at work.  I don't think I would spend that much for it if I were on my own.
GraphicsProgramming,329ajy,harveypekar84,2,Sat Apr 11 20:30:24 2015 UTC,Cool. Thanks!
GraphicsProgramming,323rwx,Madsy9,3,Fri Apr 10 10:01:23 2015 UTC,"The reason a pinhole camera is implemented isn't because you couldn't reproject a (cube or otherwise) rendering.  It's because you cannot produce anything but a rectlinear rendering from a rasterizer without gaps between T-junctions (and odd warbling geometry).  The main advent of rectlinear projection is that straight lines in the scene are straight lines on screen. This allows ""the great hack"" (tm), whereas you can cover a given edge (of a triangle for instance) with a straight edge on screen. So any triangle that lines up to that edge will also seamlessly (let's not beat on details) match that edge. This has a great advantage, in that you don't need to rasterize curved polygons, which in turn makes a rasterizer extremely fast and oblivious to the kind of projection in use (as long as it preserves straight lines from scene to screen).  It's also the traditional format used in photo and movie cameras (and so it's logical to optimize for it because that's what audiences are used to).  With reprojection to a non-rectlinear format, you get new exciting problems, too. Pinching, stretching, aliasing and generally a missmatch of the rectlinear sampling rate, and the non-rectlinear sampling rate. And of course in order to get a reprojectable (360° format) from a rectlinearly rendered scene you need to render it at least 6x (once per cubeface), which can be quite an overhead."
GraphicsProgramming,323rwx,pyalot,1 point,Sat Apr 11 06:40:22 2015 UTC,"Just a nitpick: the particular projection in your link is an equirectangular projection, not a Mercator projection.  But I love how responsive the webpage is.  Very cool."
GraphicsProgramming,323rwx,dmswart,1 point,Fri Apr 17 13:45:39 2015 UTC,"Ah, thanks for the correction. Map projections is not my expertise, so I should have figured :)"
GraphicsProgramming,324bcx,itsdenn,2,Fri Apr 10 13:50:34 2015 UTC,I learned a lot from from 3D Computer Graphics Using OpenGL series which goes a bit into the details on how OpenGL works.
GraphicsProgramming,324bcx,cottonface,0,Fri Apr 10 14:59:14 2015 UTC,Thanks for the tip!
GraphicsProgramming,324bcx,c0de517e,1 point,Fri Apr 10 21:24:38 2015 UTC,"On linux you have only openGL really and I doubt you'll get more modern APIs soon. Anyhow the good news is that the API doesn't really matter, once you learn one and how GPU works, moving to another one is not a big deal... You can even learn a higher level wrapper of sorts and then work your way down the hardware, bgfx isn't bad https://github.com/bkaradzic/bgfx some people even start nowadays from WebGL!"
GraphicsProgramming,324bcx,fb39ca4,2,Fri Apr 10 16:23:30 2015 UTC,"Seconding WebGL. If you learn raw WebGL (no libraries), it will directly translate to OpenGL later on."
GraphicsProgramming,324bcx,c0de517e,-1,Fri Apr 10 17:37:37 2015 UTC,well i was thinking about waiting for the vulkan api to be released. And I dont want to learn opengl if it because obsolete quite fast.
GraphicsProgramming,324bcx,a_bit_of_byte,0,Fri Apr 10 21:26:24 2015 UTC,"OGL won't be more obsolete than it is right now, but -REALLY- api's don't matter, once you learn the GPU you will be able to switch APIs quickly. Also, Vulkan being more low-level might not be the best one for a beginner"
GraphicsProgramming,324bcx,a_bit_of_byte,1 point,Sat Apr 11 18:55:34 2015 UTC,"OpenGL is the only one I have experience with, but if you're aiming for portability, it's your best bet. Everything else is really too specialized. Plus, if you're on Linux, it is quite literally your only choice."
GraphicsProgramming,31wmcu,zaxudih,15,Wed Apr 8 18:59:00 2015 UTC,"As someone who went to GDC, listened to both talks on DX12 and Vulkan, and am very interested in them both, and considering you're working on your own engine, I would very strongly recommend against taking them into consideration.  Both of these APIs are stripping away complexity, allowing developers to get closer to the metal while they work. However, that complexity they are stripping away is the same complexity that ensures all the multiprocessing going on underneath the hood stays in sync. Without those systems in place, you're actually left having to do the multithreading programming necessary to ensure that your data stays in sync.  Both Kronos and Microsoft have said that they do not expect indie developers to have the resources to do this work - it's much more aimed for a AAA studio with a team of engine programmers, a couple of which are experienced in multithreaded coding, to do that work. And really, you also have to be a skilled enough programmer to make something that runs faster than the system you're replacing.  All that said, if you're developing your own engine, you should limit yourself to DX11 or OGL. I love OpenGL, but Microsoft has a LOT of tools available for DX that are not available for OpenGL, so generally I would suggest DirectX 11 before OpenGL.  So.... unless your goal is to really dig deeply into graphics - so deeply that this is more of a graphics demo for your own skills than a full engine - then I would suggest sticking with DX11. If you really love digging into hex code and figuring out why your screen is black when you feel like it should have things on it, then OpenGL may be something to consider.  But all that said, remember, the complexities that are being stripped out of DX12 and Vulkan still need to exist to keep things in sync - it's just that the developer needs to program it now.  Just an FYI - DX12 will be available on Windows 8 and 10, but Microsoft has ruled Windows 7 out. Part of their push to get people to move to newer versions of Windows."
GraphicsProgramming,31wmcu,philipbuuck,3,Wed Apr 8 19:22:24 2015 UTC,I agree - the new low-level APIs are not intended for new developers or small studios.
GraphicsProgramming,31wmcu,lurend,3,Wed Apr 8 19:54:23 2015 UTC,"Without those systems in place, you're actually left having to do the multithreading programming necessary to ensure that your data stays in sync.   That's actually a bit of a relief since it makes my considerations much easier. I don't intend this engine to really be a graphics engine tech demo - graphics programming is alright but messing with it on that level sounds exactly like what I'd never want a job doing.   ...figuring out why your screen is black when you feel like it should have things on it   I've heard this sentiment from a decent number of people comparing their experience with OpenGL versus Dx11. The graphics debugger in visual studio is pretty nice to have. The thing is my brother is on a big Linux craze and I know an annoyingly large number of people with Mac OS instead of Windows. I figured it would be good to experience the joys of concocting a modular enough architecture to port to those with relative ease.   I'll probably start with a Dx11 renderer but keep it modular enough to be able to create and plug in an OpenGL one if I choose to make one at a later time. Right now I'm just dying to get started working on something actually presentable with it.  Thanks a ton for clarifying those particularly important details I missed about the upcoming Direct3D 12 and Vulkan."
GraphicsProgramming,31wmcu,arkenthera,3,Wed Apr 8 20:06:51 2015 UTC,"Hey! I think you should check this out if you decide on going with D3D 11. It is like the very basic text book you should read.It will get you started with Win32 programming,some basic math and linear algebra  too."
GraphicsProgramming,31wmcu,arkenthera,1 point,Wed Apr 8 20:11:39 2015 UTC,Thanks for the suggestion. I've already created a somewhat messy dx11 renderer but because getting it to that (honestly kind of pathetic) point was my own personal hell for ~3 months and I dread the inevitable re-write I might have to check that book out.
GraphicsProgramming,31wmcu,philipbuuck,3,Wed Apr 8 20:21:15 2015 UTC,"Hah ask me about it. I've been studying D3D 11 for 3 years. And 1 and half of it was pretty much looking at example codes/SDK documentation with blank eyes,understanding nothing etc. Now I have almost 20k lines of code modern d3d renderer which I am pretty much satisfied and intend extending to vulkan and d3d 12 as well."
GraphicsProgramming,31wmcu,bat_country,1 point,Wed Apr 8 20:25:37 2015 UTC,This is a pretty great book in my opinion
GraphicsProgramming,31wmcu,bat_country,1 point,Wed Apr 8 20:29:18 2015 UTC,"I don't intend this engine to really be a graphics engine tech demo - graphics programming is alright but messing with it on that level sounds exactly like what I'd never want a job doing.   If this is the case why write your own engine at all?  Why not pop one level higher and just use Unreal 4, Unity5, or Source2?"
GraphicsProgramming,31wmcu,coder543,2,Wed Apr 8 20:19:54 2015 UTC,Why not just use Unreal/Unity/Source? I want the experience of building an engine myself. It's fun to make systems - it's good brain exercise. It's a little bit more about the process than the result. Those do a ton of stuff for me I'd like to do myself and have control over. I've got some ideas rolling around my head I'd like to try that I wouldn't be able to do with those because so much stuff is done for you and there's nothing you can do to change it.
GraphicsProgramming,31wmcu,programmingjay,2,Wed Apr 8 21:03:57 2015 UTC,"Keep an eye on DX12, Vulkan then.  Given your stated goals you may enjoy snuggling up as close to the GPU as possible."
GraphicsProgramming,31wmcu,lurend,0,Wed Apr 8 21:07:39 2015 UTC,"it's not just about Mac and Linux (both of which are worth very serious consideration for any programmer -- developing on Linux is just so much fun, and Mac has some really nice stuff too), it's about everything including Windows. At least a limited form of OpenGL runs on everything except the Xbox and perhaps windows phone. iPhones, iPads, Android phones, PlayStation, Web Browsers (WebGL), Mac, Linux, Windows, and more all run OpenGL.  You can choose to develop for Windows and Xbox... or you can choose to develop for everything. I don't understand the appeal of limiting future options for having a little less initial boilerplate code. Once you get into the code, beyond the starting line, things in OpenGL are really not much (if any) harder than DirectX from what I understand.  I'd also recommend looking at just using an engine if you want to write a game. Irrlicht, Unity, Unreal. There are many good options."
GraphicsProgramming,31wmcu,philipbuuck,1 point,Thu Apr 9 00:19:30 2015 UTC,"It's mostly because of the debugging tools and such that people gravitate toward Dx11. Honestly try using the graphics debugger - it's pretty useful to be able to step through each stage of the graphics pipeline on whatever draw call you want to see exactly where things went wrong. That can save an immense amount of time. And I've already created a Dx11 Renderer once (kind of a shitty one but I had instanced geometry and lighting figured out which is nice) so creating a new and hopefully much more organized one would likely take less time for me to get one started than OpenGL.  I get what you're saying about how much more opportunities open with OpenGL which is a big part of why I was having difficulty on deciding which to try first for this engine I'm building.  And as far as other engines - I'd like the experience of making a whole engine myself. The merit of the project is more about learning from the obstacles present in creating the system. I know how to make a renderer (Dx11 at least,) export models/animations from Maya and load them into a program, make animation and particle systems, pathfinding algorithms and decision trees, apply linear algebra for collision and physics - I just want to put all that knowledge to use or I'll probably forget it. Use it or lose it and I think I'm starting to lose it. Gaining a platform on which to create games and programs is a plus but the journey is equal or more important than the destination.  edit: pardon the brick 'o' text"
GraphicsProgramming,31wmcu,dinosaurdynasty,2,Thu Apr 9 01:43:59 2015 UTC,"Just to add onto your point about a graphics debugger, I develop games mostly for mobile and both Xcode and the android sdk provide graphics debuggers. Xcode's seems to do what you're talking about stepping through each state change/draw call and showing bound/existing buffers and whatnot. It also allows live editing of shaders which I've found greatly useful. I haven't really used the android sdk's debugger often enough to know much of its capabilities but it seems to have the same things."
GraphicsProgramming,31wmcu,jringstad,4,Thu Apr 9 08:29:09 2015 UTC,"Mantle and Vulkan will both be operating at a very low-level. As an indie/solo developer you will not have the resources to gain any significant advantage over DirectX/OpenGL. That's not to say that you shouldn't play with them as they become available, but for any serious(-ish) work, stick with the existing frameworks. There are huge amounts of online documentation available for both DirectX and OpenGL. That won't be the case for Mantle/Vulkan any time soon.  As for choosing between OpenGL and DirectX - I'm a huge GNU/Linux fan and supporter, and I've primarily worked with OpenGL. But there's no way getting around the fact that the tools and resources available for DirectX are simply better. You are more likely to succeed and build a cool application with DirectX. That being said, there seems to be a movement towards GNU/Linux based gaming with Valves upcoming device. With DirectX you will be limiting yourself to the Windows platform and devices.  So, ask yourself the question, do you mind sacrificing some excellent tools and resources for the off-chance that your engine will be run in GNU/Linux environments? Do you see yourself ever using a non-Windows platform? Be honest.  With all that being said, my suggestion would be not to fret too much over this. Make a choice and get busy developing a cool game engine. You'll find that many DirectX constructs are also found in OpenGL and vice versa."
GraphicsProgramming,31wmcu,lurend,2,Wed Apr 8 19:49:36 2015 UTC,"I've chosen Dx11 renderer for now but I'm planning on leaving room to develop an OpenGL renderer later down the line - multi platform development is kind of a ""luxury"" feature for me at this point. If/when I get around to doing the OpenGL renderer is there a version you would suggest working with? I'd just go with the latest stable version but I'd rather not find myself working with something that's in an awkward transitional stage kinda like Dx10 was."
GraphicsProgramming,31wmcu,Spiderboydk,2,Wed Apr 8 20:13:31 2015 UTC,"In practice, unless you want to be a graphics programmer, you probably won't end up learning both APIs. Unless you intend to work in graphics, I would plan on sticking with DX11.  There are so many elements to a game engine, there's really not a lot of use in obsessing over the DX/OGL debate - again, unless you want to work in graphics.  All that said though, OpenGL is hard to 'version' the same way as DX11, but OpenGL 3.3 is extremely safe, 4.0 is almost as safe (and adds tesselation, which could potentially be a feature you'd want) and I think it gets more experimental as you move forward (4.5 is the latest version) but still, I think 4.4 is pretty regularly supported in modern cards."
GraphicsProgramming,31wmcu,ccricers,1 point,Wed Apr 8 20:37:55 2015 UTC,When my desire to get something actually presentable has been satiated (1st and maybe 2nd game or program projects) I might look into creating an OpenGL renderer. I'm trying to make a particularly modular engine that I can just swap things in or out whenever I feel inclined to add or revamp a module. This engine is going to just be a toolkit for personal projects. Kind of a hobby project for experimenting and learning. It might be good for spike solution-ish kind of things.  3.3 sounds exactly what I'd like to work with if/when I get to the OpenGL renderer. Tessellation is pretty neat but it isn't a huge interest for me.
GraphicsProgramming,31wmcu,lurend,2,Wed Apr 8 22:52:41 2015 UTC,"Be aware that on Mesa (Linux’s open source drivers) core support only goes up to 3.3 (and that’s quite recent). However, many later features are supported as standard extensions."
GraphicsProgramming,31wmcu,ultrapingu,2,Thu Apr 9 04:09:33 2015 UTC,"DX11 is roughly equivalent to GL 4.3+. If you want compute shaders, you need 4.3 or later. 4.5 is the very latest. Note that with OpenGL you can also always get features from a newer version in an older version by using extensions. So if you want, you can use 3.3 and then use  GL_ARB_compute_shader. That might allow you to run on some hardware that cannot implement GL 4.3 fully, but still supports compute shaders.  That's only really relevant though if you want to edge out the last bit of hardware compatibility."
GraphicsProgramming,31wmcu,ultrapingu,1 point,Thu Apr 9 13:12:30 2015 UTC,With OpenGL the choice of version is less important. It's mostly backward compatible and only recently have they begun removing long deprecated functionality.   My suggestion would be to go with the latest version that your hardware support. Unless you're using some really recent features you'll find your software also supported by older hardware.
GraphicsProgramming,31rkd2,gamesloth,3,Tue Apr 7 16:44:56 2015 UTC,"Damn, that's really cool. Is the source code anywhere? I'm fairly new to graphics programming, would like to give something like this a look over."
GraphicsProgramming,31rkd2,Reddit1990,3,Tue Apr 7 18:29:28 2015 UTC,"Yes, you can view the source, but it's not clean enough to really learn anything from. I've linked to a few resources that helped me out though, on the page."
GraphicsProgramming,31rkd2,jrkirby,2,Tue Apr 7 23:30:40 2015 UTC,Just view page source. It's not obfuscated or anything.
GraphicsProgramming,31rkd2,heyheyhey27,1 point,Tue Apr 7 20:25:48 2015 UTC,"Check out tutorials for Worley noise and Fractional Brownian Motion noise; they're very common and pretty simple to understand. These textures are all just combinations of those two noise functions, plus some stuff with sines/cosines."
GraphicsProgramming,31rkd2,mrjmoak3,2,Tue Apr 7 20:40:21 2015 UTC,"I love the Worley Noise and the Sine Wave modulated by Worley Noise, they look amazing!"
GraphicsProgramming,31rkd2,pnpbios,1 point,Tue Apr 7 18:22:44 2015 UTC,a number of those reminded me of reboot and babylon 5.
GraphicsProgramming,31rkd2,doublecloudhead,1 point,Tue Apr 7 19:47:25 2015 UTC,Amazing stuff!
GraphicsProgramming,31r8hr,mrjmoak3,2,Tue Apr 7 15:19:36 2015 UTC,"Very cool! If you haven't stumbled across it yet, PBRT is a must read book for implementing a physically based ray tracer.  Some notes on performance:   I notice in Renderer::Render you write the image out to file as you render it. I'd recommend instead keeping a framebuffer of the image's pixels while rendering and save results there during rendering. Then after rendering is done you write this out to an image file. This will remove serialization work/overhead from your main render loop. Definitely look into multithreading your renderer before going to packet/stream tracing with AVX. Multithreading a ray tracer isn't too difficult as there really is only a small amount that must be synchronized and it should give a big boost in performance. From there check out packet (but really stream) tracing techniques to get the most out of each thread as well."
GraphicsProgramming,31r8hr,TurkishSquirrel,1 point,Tue Apr 7 18:24:26 2015 UTC,"They're on the list :)  Funnily enough I recently bought PBRT - It's amazing, definitely one of those books I'll keep around forever."
GraphicsProgramming,31r8hr,GijsB,2,Tue Apr 7 18:27:09 2015 UTC,"this framebuffer is easy to implement :  Vec3 *framebuffer = new Vec3[w*h];//set up image raw data table  //for every pixel const unsigned int i = y*w+x; framebuffer[i] = Vec3(//color of pixel x&y)  //and then from framebuffer to file FILE *f = fopen(""result.ppm"", ""w"");         // Write image to PPM file.      fprintf(f, ""P3\n%d %d\n%d\n"", w, h, 255);      for (unsigned int i=0; i<w*h; i++){         fprintf(f,""%d %d %d "", framebuffer[i].x,framebuffer[i].y, framebuffer[i].z);      }   and for the multithreading use OpenMP and just place :  #pragma omp parallel for schedule(dynamic, 1)   right infront of the first loop that loops through the pixels, and use the build command :  -fopenmp   as easy as that"
GraphicsProgramming,31r8hr,Sify007,1 point,Tue Apr 7 21:40:13 2015 UTC,"Yeah, I've actually written a couple for separate projects :)  Thanks though!"
GraphicsProgramming,31r8hr,solidangle,2,Tue Apr 7 21:52:15 2015 UTC,"Hi - good stuff here. I also have a few comments.  If speed is one of your main concerns you should consider using SSE for matrix and vector operations. This advice you should however take with a grain of salt - profile and see if you need speed up in this area. You can also consider using a well established solution like GLM.  Use smart pointers. I definitely suggest using unique_ptr since it is a zero overhead way to make sure you delete everything and it also make you think a ownership a little. shared_ptr is more flexible, but at the same time it make reasoning about lifetime of the object it points to harder since you can't always be sure when the last reference is released.  EDIT: spelling and such..."
GraphicsProgramming,31r8hr,solidangle,3,Wed Apr 8 09:07:17 2015 UTC,"Using SSE for Matrix and Vector operations can't hurt, but I doubt it's going to give the largest performance boost. Using SSE somewhere else can cause a huge performance boost though. Currently he is using a simple binary KD-Tree with non-optimal splits (he seems to split the primitives in two equal groups, which can cause problems in a myriad different situations, such as a teapot in a stadium, the SAH heuristic works much better). If he really wants to optimize his raytracer he should replace his KD-Tree with a Spatial Splits QBVH (which uses SSE), which is current state of the art and not too difficult to implement."
GraphicsProgramming,31r8hr,NotcamelCase,1 point,Wed Apr 8 15:31:13 2015 UTC,"I'll look into that, sounds exciting"
GraphicsProgramming,31r8hr,NotcamelCase,2,Wed Apr 8 15:37:50 2015 UTC,"It's quite exciting. The following papers and chapters should contain the info you need:   Physically Based Rendering: From Theory to Implementation by Pharr and Humphreys, Chapter 4 Shallow Bounding Volume Hierarchies for Fast SIMD Ray Tracing of Incoherent Rays by Dammertz, Hanika and Keller  Spatial Splits in Bounding Volume Hierarchies by Stich, Friedrich and Dietrich   Implementing just the QBVH takes very little effort if you already have an implementation of a BVH (see PBRT on that) and should already give you a large performance boost over your KD-Tree."
GraphicsProgramming,31r8hr,0h_Lord,2,Wed Apr 8 15:48:58 2015 UTC,"Neat! I also started to play with ray tracing to have some break off the rasterization-based techniques and I've been lovin' it so far! After a basic one, I think I'll try and create a GPU-based RT.  https://github.com/jmoak3/Tracer/blob/master/RayTracer/Source/RayRenderer.h#L9 - You may have memory leak here; you need to delete the Lights' elements, too.  It's really fun to start from scratch and build it up with ray tracers. Cheers!"
GraphicsProgramming,31r8hr,0h_Lord,1 point,Tue Apr 7 17:40:48 2015 UTC,"I should rename it Memory Leaker to be honest. I also don't free the KDTree's memory, or any of the Transforms I instantiate. One of the benefits of offline is that I can let Windows handle that.  For now ;)"
GraphicsProgramming,31r8hr,lurend,1 point,Tue Apr 7 17:49:47 2015 UTC,"Haha I noticed :D Though you could just use shared pointers instead of raw pointers.  One last thing, maybe you can add your system configuration just for reference at the end."
GraphicsProgramming,31r8hr,frigge,1 point,Tue Apr 7 17:57:04 2015 UTC,"When I try to implement a real time version, I probably will :)  That's a good suggestion, I'll add my system!"
GraphicsProgramming,31r8hr,lurend,1 point,Tue Apr 7 18:09:45 2015 UTC,Nice project! Did you implement this sequentially on a CPU?
GraphicsProgramming,31r8hr,lurend,2,Tue Apr 7 16:28:35 2015 UTC,"Sorry, what do you mean by sequentially? Like multithread? If so, not yet, but soon!"
GraphicsProgramming,31q8p3,rsamrat,1 point,Tue Apr 7 08:45:09 2015 UTC,Please add quarternions in these series.
GraphicsProgramming,31q8p3,xcodeart,1 point,Tue Apr 7 09:49:28 2015 UTC,Awesome.
GraphicsProgramming,31q8p3,pnpbios,1 point,Tue Apr 7 17:44:09 2015 UTC,I just found this last night...can't wait to give it a go after exams.
GraphicsProgramming,31qirn,mrjmoak3,2,Tue Apr 7 11:17:41 2015 UTC,"As one of the ray tracing papers, I figured this should be shared here :)"
GraphicsProgramming,31pj3y,mrjmoak3,1 point,Tue Apr 7 03:34:42 2015 UTC,"Somehow found myself reading this, and thought it was interesting enough to share."
GraphicsProgramming,31pj3y,UsuallyQuiteQuiet,1 point,Tue Apr 7 03:35:07 2015 UTC,The author shared the post a while ago to this sub. It's a great read and also makes me feel inadequate just typing away in c++.
GraphicsProgramming,31pj3y,UsuallyQuiteQuiet,1 point,Tue Apr 7 11:52:25 2015 UTC,Really? My bad haha
GraphicsProgramming,31pj3y,pyalot,2,Tue Apr 7 12:16:49 2015 UTC,"No worries lol, at least this way some new guys can read it for the first time."
GraphicsProgramming,31pj3y,nnevatie,1 point,Tue Apr 7 12:25:05 2015 UTC,Yeah but I always silently judge that guy who reposts within a month or two ;)
GraphicsProgramming,31lp4k,Boojum,11,Mon Apr 6 06:16:19 2015 UTC,"One of the things I enjoy sometimes is trying to track down the earliest publications on a topic in computer graphics.  This wasn't the first paper on rendering via ray tracing (that came later), but so far as I can tell, this abstract is the first published description of ray tracing on an automatic computer.  A 1949 followup mentions that this took 115 seconds to trace a ray path through 11 surfaces.  If there's interest, I'd be happy to post links to other classical papers from time to time."
GraphicsProgramming,31lp4k,a_bit_of_byte,3,Mon Apr 6 06:29:28 2015 UTC,"This is really neat. I would like to see more, personally. One way I learn about complicated topics (like computer graphics) is to start with the history. It's easier to see why complex systems are built around something straightforward, or when something that should be simple is like pulling teeth from a still-conscious bear when there's some context.   Plus, it's just interesting and my inner geek is loving this."
GraphicsProgramming,31lp4k,Madsy9,1 point,Mon Apr 6 06:56:23 2015 UTC,"Thanks.  One of my favorite things about going back to the old foundational papers is seeing just how much foresight the authors had sometimes.  I've had cases where I've come up with what I thought was a neat new idea, but then it turns out that while it's original compared to contemporary literature, there's some overlooked passing remark in the early paper that anticipates it.  Or sometimes there's something that was too expensive to be practical back then but is well within reach today."
GraphicsProgramming,31k75m,andimusprime,3,Sun Apr 5 21:54:22 2015 UTC,"What you have looks essentially correct. Perhaps it's a problem of GIGO? Not setting one of your input parameters to the correct value, for example. My first suspicion is resolution, as often what people call ""resolution"" is actually 1/resolution so that sv_position can be multiplied by it to get to screenspace.  You could try narrowing down the issue by hard-coding your inputs, and also by using the prev position as your position in the vertex shader to make sure it ends up in the correct (previous) location on screen.  As an aside, it would be much more efficient to pre-multiply your world, view, and projection matrices together and just pass that result to the vertex shader, rather than passing all 3 to the shader and doing the multiply there for every vertex."
GraphicsProgramming,31k75m,Rangsk,2,Sun Apr 5 23:35:43 2015 UTC,"Thanks for the reply, pretty sure the inputs are good. I'm currently testing using the current matrices for both, which in theory should should give  a zero motion vector, but alas no. The only place that is zero is the top left hand corner.  Think I am not understanding what format the inputs are coming in as. Could someone verify or correct me please:  input.position: pixel position in screen space, (0, 0) on top left hand cormer going to (resolution.x, resolution.y) on the bottom right hand corner  Previous position once divided by w, is (-1,1) on top left hand corner going to (1,-1 ) on the bottom right hand corner. So then we shift the range and multiply by the resolution to put it in the same format as input.position"
GraphicsProgramming,31k75m,BiiG-Fr,2,Mon Apr 6 10:15:41 2015 UTC,Did you try to remove prevPosition.xyz /= prevPosition.w; from the PS and use output.prevPosition.xyz /= output.prevPosition.w in the VS instead
GraphicsProgramming,31la77,DraugTheWhopper,2,Mon Apr 6 03:33:04 2015 UTC,"No, there's no obstacle.   You can simulate a curved projection plane by warping your output image, e.g. in a fragment shader. This is exactly how rendering for the Oculus Rift works, for example.   Rendering onto an arbitrary complex 3D surface (e.g. a building) is called Projection Mapping and there's a good wikipedia article about it."
GraphicsProgramming,31la77,VilHarvey,2,Mon Apr 6 09:30:40 2015 UTC,"Oculus Rift    Now that you mention it, I wonder if there's a projection matrix you can use that would result in that barrel distortion look without the need for a post-process effect?"
GraphicsProgramming,31la77,heyheyhey27,1 point,Mon Apr 13 18:28:32 2015 UTC,"Non-mobile: wikipedia article  That's why I'm here, I don't judge you. PM /u/xl0 if I'm causing any trouble. WUT?"
GraphicsProgramming,31la77,LittleHelperRobot,0,Mon Apr 6 09:30:45 2015 UTC,"Projection mapping:       Projection mapping, also known as video mapping and spatial augmented reality, is a projection technology used to turn objects, often irregularly shaped, into a display surface for video projection. These objects may be complex industrial landscapes, such as buildings. By using specialized software, a two- or three-dimensional object is spatially mapped on the virtual program which mimics the real environment it is to be projected on. The software can interact with a projector to fit any desired image onto the surface of that object.  This technique is used by artists and advertisers alike who can add extra dimensions, optical illusions, and notions of movement onto previously static objects. The video is commonly combined with, or triggered by, audio to create an audio-visual narrative.    Image i - Projection mapping (San Francisco)     Interesting: Freelusion | Disney Dreams! | The Neck Is a Bridge to the Body | IllumiRoom   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
GraphicsProgramming,31la77,autowikibot,1 point,Mon Apr 6 09:30:51 2015 UTC,"Nothing prevents that at all, except usefulness, I guess. It all just depends on the math you apply in your rendering program. It would not be hard at all to project the scene onto e.g. a hemisphere (instead of a plane), giving you a ""picture hemisphere"" in your diagram. However, since your screen generally is a flat plane, you inevitably end up with a picture plane of sorts anyway.  One application I can think of is if you were projecting graphics onto a the inside of a dome in a room (think stellarium) - you would want to calculate the images with a picture hemisphere.   I have also seen examples of projection art that might use more advanced geometry than a plane when rendering. Not too about that though.  In the end you always end up with a plane because images are 2d planes. However in these examples the image files would contain a picture that's projected onto something else than a plane, if that makes sense.  Hope this sheds some light on your question :)"
GraphicsProgramming,31la77,fadefade,1 point,Mon Apr 6 04:41:00 2015 UTC,"This is trivial and commonly done for ray tracers.  Fisheye, spherical, and cylindrical seem to be the most common alternate projections.  It's all a question of how you choose to map a point in screen space to a ray in world space.  It seems to be less common with rasterization, simply because you can't directly capture the mapping with the usual homogeneous matrix transforms.  But you can still manage it.  Fisheye Quake, for example did it by rendering a cube map and then reprojecting that."
GraphicsProgramming,31la77,Boojum,1 point,Mon Apr 6 06:12:58 2015 UTC,"You can map your projection to any arbitrary UV coordinates.  This could be simple shapes like spheres, or actual UVs of geometry (which can be used to bake in lighting).   This isn't something that can be done (or done easily) with rasterization, but can be done trivially with ray tracing."
GraphicsProgramming,31la77,__Cyber_Dildonics__,1 point,Mon Apr 6 15:11:14 2015 UTC,"I'm assuming a 360 degree camera here. We have to remember that our framebuffer (screen) is a flat plane and not curved or a sphere. So what is required when you want curvature is to project from 3D euclidean space to the surface of a sphere, and then project the polar coordinates to 2D euclidean space. To make matters worse, the 2D surface of a sphere has a different topology than 2D euclidean space. For example, axioms like ""the shortest distance between two points is a straight line"" is not true for polar coordinates on a sphere. It follows that triangles projected down to the surface of a sphere becomes bended instead of straight. GPUs can render triangles with bended edges, but it would not render those triangles in the same (efficient) way as it renders triangles with straight edges. I.e you would have to approximate a bended triangle with tesselation. The fact that GPUs can only render primitives with straight edges is due to how polygon rasterization is implemented. Curved interpolation (for example with 3rd order splines) is really expensive compared to linear interpolation and requires a lot more transistors. Even if hardware implemented this, it would lead to other issues, for example making MSAA and coverage testing more complicated, and figuring out which pixels belonged to which triangle. You couldn't just check a pixel position against three flat planes anymore.  So, a TL;DR so far is that what you propose requires a map projection from a sphere to a flat plane. Since at least some information is lost in the process, you have to pick your poison. For example, do you want to preserve angles or area? Gall–Peters is an area-preserving projection, while Mercator is an angle preserving projection. There are many more to choose from, depending exactly on what you need.  The way I would implement this in OpenGL is to use a cube map, and then project the cube map sphere to the 2D viewport with a mercator projection. The map projection could be stored as a texture itself as shown with this fragment shader:  #version 450 uniform sampler2D mercatorMap; //3D uvmap for the cubemap uniform samplerCube viewportSphere; //cubemap which represents the surface of a sphere uniform int viewportWidth; uniform int viewportHeight; varying out fbColor; void main(){     vec2 coord = vec2(float(gl_FragCoord.x) / float(viewportWidth), float(gl_FragCoord.y) / float(viewportheight));     vec3 mercatorCoord = texture(mercatorMap, coord).str;     fbColor = texture(viewportSphere, mercatorCoord); }   The normals for the mercator mapping would be stored as RGB values which the mercatorMap sampler refers to. In other words, it would be very cheap. However, this cubemap method would not be 100% exact because each face in the cubemap was rendered with the normal perspective projection. But it would strike the best balance between accuracy and performance. If for some reason it wasn't accurate enough, you could split up each cube face into smaller parts which were all rendered separately to decrease the inaccuracy. Perhaps one could tesselate all the primitives in a scene, project it down to a sub-part of a sphere which covered the viewport, and then render that to a cube map face.  To my knowledge, the only way to get this 100% fragment/pixel accurate is via raytracing or raymarching."
GraphicsProgramming,31la77,Madsy9,1 point,Tue Apr 7 08:35:48 2015 UTC,"See, what I've decided to do for my games and simulations is to render the scene to an off screen buffer slightly larger than the actual screen resolution, and use that buffer as a texture object and map it to the inside of a sector of a cylinder, and do the UI elements on top of that.  If you take multiple projections, and stitch them together, you are going to have overlap issues."
GraphicsProgramming,31la77,pnpbios,1 point,Fri Apr 10 19:20:42 2015 UTC,"I expect most offline ray tracers do not use a flat plane projection. For realtime graphics, hardware rasterization algorithms do not work for geometry that is curved in screen space. Segmentation would require each segment to be rendered separately since geometry would need to be clipped at each segment boundary, which would probably produce more objectionable artifacts than the flat plane projection anyway."
GraphicsProgramming,31la77,qartar,1 point,Mon Apr 6 04:35:12 2015 UTC,"My interest primarily concerns realtime graphics in the context of videogames. When you run a videogame on an ultra-wide monitor or series of monitors, or when you simply increase the FOV beyond the norm, a large portion of your screen is effectively wasted on overly scaled imagery. The segmentation bit makes sense, as it's likely much easier to render one wide 6megapixel viewport than it is to render three separate 2megapixel viewports. But, when you say that ""hardware rasterization algorithms do not work for geometry that is curved in screen space"", is this a hardware limitation, or could someone get decent results if they wrote a completely new microcode/graphics language?"
GraphicsProgramming,31la77,qartar,2,Mon Apr 6 12:51:29 2015 UTC,This is a hardware limitation.
GraphicsProgramming,31la77,__Cyber_Dildonics__,1 point,Tue Apr 7 01:58:37 2015 UTC,"It's much more complicated to project triangles onto a sphere.    It sounds like you want to look into this: https://www.youtube.com/watch?v=jQOJ3yCK8pI  I haven't looked at how it is being done, but my guess is that it is a cube map rendered from the camera position and then the cube map is 'ray traced' with a different projection.  If you mapped a projection to a specific curved monitor based on where you thought your head would be, it could actually be pretty interesting.  You may be able to do it with the source code from the github link in that video, without writing anything yourself."
GraphicsProgramming,31la77,qartar,1 point,Mon Apr 6 15:18:05 2015 UTC,You wouldn't need to ray trace into the cube map. Calculating the correct transformation inside the fragment shader would be pretty straightforward. The video gives the names of each projection used.
GraphicsProgramming,31la77,__Cyber_Dildonics__,1 point,Tue Apr 7 01:51:19 2015 UTC,"However you want to conceptualize it, it would be the same thing, pick a direction and use it to look up from the cube map."
GraphicsProgramming,31la77,__Cyber_Dildonics__,1 point,Tue Apr 7 05:15:39 2015 UTC,Most offline ray tracers do use flat plane projections for typical circumstances.  Any lens distortion is handled in compositing.  This isn't technically necessary but is an easier workflow.   UV or spherical rendering are might be used for intermediate renders although that is actually fairly rare.
GraphicsProgramming,31a2j6,Rovmis,5,Fri Apr 3 05:08:16 2015 UTC,"Decagonal and Quasi-Crystalline Tilings in Medieval Islamic Architecture, perhaps?"
GraphicsProgramming,31a2j6,Boojum,4,Fri Apr 3 18:31:19 2015 UTC,You may be able to create those kinds of images with context-free grammars.
GraphicsProgramming,31a2j6,heyheyhey27,3,Fri Apr 3 15:33:37 2015 UTC,"Funny you should ask, as r/math had a thread related to this just a few hours ago. While the exact method used in Moorish art is unknown, they are related to quasicrystal tilings and Penrose tilings. Sir Roger Penrose holds an excellent introductory lecture on the subject here"
GraphicsProgramming,31a2j6,Madsy9,1 point,Fri Apr 3 20:02:19 2015 UTC,"Oh, fantastic! Thanks for the links. It's been awhile since I've taken a look at Penrose tiles – video should be very useful."
GraphicsProgramming,31a2j6,pATREUS,3,Fri Apr 3 20:28:39 2015 UTC,Hi there. Did you visit al Hambra in Granada? It's a wonderful place. I have been looking into this subject recently and can share a few (interrelated) links for you:  http://www.d.umn.edu/cs/thesis/cbeckerJuly2012.pdf http://www.plunk.org/~hatch/ http://www.designcoding.net/?s=tessellation (using grasshopper) http://www.gravitation3d.com/magictile/ http://bulatov.org/math/1001/index.html http://dmitrybrant.com/2007/01/24/hyperbolic-tessellations
GraphicsProgramming,31a2j6,AbouBenAdhem,3,Wed May 6 19:52:16 2015 UTC,I’m not sure where you’d draw the line between “rendering algorithms” and “computational geometry”...  Have you looked into girih tilings?
GraphicsProgramming,31a2j6,sdeetee,1 point,Fri Apr 3 06:48:25 2015 UTC,"I haven't, but the paper Boojum attached makes me think I should.  I don't know if I have a good explanation, as they are definitely interrelated here.  Maybe a better statement is that the papers I've seen so far have stressed the geometric foundations of the rendering process, opposed to the complexity/algorithm step-by-step of the rendering process. E.g., here's how this symmetry can be applied to the Euclidean plane or the hyperbolic plane vs. this process is fast O(nlogn) and uses the following rendering steps."
GraphicsProgramming,31a2j6,jurniss,2,Fri Apr 3 20:35:58 2015 UTC,"Have you checked out taprats? It's a Java app for generating these, source available.  There are papers by the author too; not sure if they fit your criteria of beginners friendly."
GraphicsProgramming,314h3o,bbmario,4,Wed Apr 1 22:56:44 2015 UTC,"When you calling bgfx::setTransform, pass number of matrices your model has:  https://github.com/bkaradzic/bgfx/blob/master/include/bgfx.h#L1203  Add bgfx::Attrib::Indices and bgfx::Attrib::Weight when creating bgfx::VertexDecl:  https://github.com/bkaradzic/bgfx/blob/master/include/bgfx.h#L81  In shader use u_model[<bone index>] to lookup bone matrix.  Also if you have problems with this just open issue on GitHub."
GraphicsProgramming,30ubu2,c0de517e,5,Mon Mar 30 19:50:49 2015 UTC,"What's wrong with being wrong if it looks good? Graphics is all about cheating. I don't think anyone's claiming these methods aren't full of errors. They're claiming that they look nice, which at the end of the day is what graphics programming is about.  All that said, it's still important to look at where our methods vary from reality, so this is a good criticism of that."
GraphicsProgramming,30ubu2,jrkirby,9,Mon Mar 30 21:38:18 2015 UTC,"Nothing wrong if it looks good. But actually many of these errors do create large visual issues, sometimes that artists do hide by simply not using them when they create problems or placing/tuning things strategically not to.  Note that so many recent improvements in realtime CG fixed similar errors that previously were overlooked and ignored. Some random examples: specular antialiasing of normalmaps (e.g. Tokvsig and so on), gamma correction, correct normalmap blending math, environment cubemap fresnel integral, correct normal skinning for bone rigs, tonemapping...  I could go on forever. So many times we just ignored large and ""basic"" errors (gamma correction is such a large, glaring example) it might seem incredible in a field with such a long history now of research"
GraphicsProgramming,30ubu2,ccricers,1 point,Mon Mar 30 23:10:19 2015 UTC,"I'm already familiar with the concept, but I thought gamma errors are mostly present in textures or images that are not made in the same color space as the final render. If you are not sampling textures in a render, is gamma correction still necessary?"
GraphicsProgramming,30ubu2,goal2004,0,Wed Apr 1 23:02:39 2015 UTC,"Every source than has gamma applied needs to be turned into linear to be used in the rendering math, and every linear results needs to be turned into gamma to be displayed as standard sRGB. If you don't sample any textures you probably still have some colors around that you use as inputs, if these are expressed in sRGB they will have to be turned linear. Also the output of the rendering will need to be turned sRGB. http://http.developer.nvidia.com/GPUGems3/gpugems3_ch24.html"
GraphicsProgramming,30ubu2,Rainfly_X,2,Thu Apr 2 07:33:17 2015 UTC,"Graphics is all about cheating.   Sometimes.  Sometimes you want to render accurately, not beautifully. There's a million different kinds of projects out there."
GraphicsProgramming,30og0j,bryanedds,12,Sun Mar 29 06:24:04 2015 UTC,Be the change you want to see in the world.
GraphicsProgramming,30og0j,fb39ca4,3,Sun Mar 29 06:59:21 2015 UTC,"Assuming  a) I win the lotto (exceedingly unlikely, since I don't play it), or  b) get fired from my presently all-consuming job (more likely - tho hopefully not likely generally), or  c) an established party is willing to fund my effort (Cinderella story)  I'd love nothing more than to work on this.  However, I'm not 100% sure people would like my approach. If anything, I'd end up using a Mono / .NET language like C# or F#. That means I'd have to first generate bindings from Vulkan to .NET. Then I could start development on the renderer. But a renderer is useless without a pipeline, so I'd have to then start on that. But a framework is useless without tooling, so then I'd have to start work on that. But all three are useless without documentation, so I'd have to then start on that. And most software is useless without a serious demo at least, so I'd have to then start on that. But a great demo is impossible without some great art assets, so I'd have to find a way to fund that - as that's simply not something I can do myself!  Were I not immediately obligated, I'd have to secure funding for the approx. one - two years needed to put all this together. Even with my experience in commercial game dev and open source game engine dev, that's an incredibly hard thing to do.  It can certainly be done, and such a task is very desirable to do, but it's massive, and whether I can personally work on such a thing is different story. However, if someone can come up with the funding to make all this work, I'm happy to listen :)"
GraphicsProgramming,30og0j,7hi4g0,1 point,Sun Mar 29 16:14:20 2015 UTC,"I agree with you on how hard things really are once you start putting it all on paper, or code for that matter.  Alternatively you could simply start working, preferably already on the open on something like github.  For this initial phase, commitment might top funding, specially if people get interest and start helping you. Also, if you already have something to show it might be easier to fund.  All said, I would definitly like to help you with that endeavour. We could create a github repo, a subreddit and start work right away. If not for anything else, at least for the fun of it :)"
GraphicsProgramming,30og0j,nub_cake,1 point,Mon Mar 30 15:13:55 2015 UTC,"To let the rest of thread know, we're in discussion about getting started on a spec. If anyone else would like to participate, please e-mail me at bryanedds@gmail.com!"
GraphicsProgramming,30og0j,__Cyber_Dildonics__,1 point,Mon Mar 30 20:39:47 2015 UTC,"A serious question, why would anyone fund you if you plan to make the engine FOSS? I thought the point of funding was to see some monetization down the line."
GraphicsProgramming,30og0j,Nonakesh,4,Sun Mar 29 22:16:59 2015 UTC,"Presumably people would fund me so they can use the end result, or they're just interested in seeing the state of free open source software move forward. And it's not like I'd be asking for a 6-figure / year salary to do FOSS.  There are many reasons people fund free open source software development."
GraphicsProgramming,30og0j,VanMeerkat,0,Sun Mar 29 23:45:45 2015 UTC,"I guess now you see why no one else is doing it, besides the fact that doing such a thing in C# would be a little ridiculous.   Also when you say physically based what do you mean exactly?  Typically that refers to energy preserving shaders and lights, which actually have nothing to do with the underlying graphics API.  You could write physically based shaders and lights for Ogre right now."
GraphicsProgramming,30og0j,bobmarely17,6,Mon Mar 30 02:39:07 2015 UTC,I thought Vulkan wasn't even out yet?
GraphicsProgramming,30og0j,angrymonkey,1 point,Sun Mar 29 07:48:54 2015 UTC,"I'm still a bit confused on that point since I've only been partly paying attention. I did look for the C header files on the tubes, but I haven't found anything.  I guess you're right. Any idea about the roadmap?"
GraphicsProgramming,30og0j,GreyMX,2,Sun Mar 29 15:15:19 2015 UTC,Initial specs and implementations expected this year   Here. Near the bottom.
GraphicsProgramming,30og0j,RoyAwesome,4,Sun Mar 29 20:50:38 2015 UTC,PBR is currently being developed in jMonkeyEngine 3 (BSD-licensed). You can find it in the PBRIsComing branch: https://github.com/jMonkeyEngine/jmonkeyengine/tree/PBRisComing
GraphicsProgramming,30og0j,xcodeart,1 point,Sun Mar 29 22:20:02 2015 UTC,"Great license and truly interesting tech!  I'm wondering how hard it would be to interop a .NET game engine with this java library, however. Hmm...  Anyways, good post!"
GraphicsProgramming,30og0j,protestor,5,Sun Mar 29 23:59:35 2015 UTC,Nobody has said Mitsuba yet? It's a reseach-grade GNU physically-based renderer. A lot new rendering algorithms are implemented there because it provides a robust/easy comparison to the state-of-the-art.
GraphicsProgramming,30og0j,phire,3,Mon Mar 30 22:00:38 2015 UTC,"I don't know exactly what you're looking for when you're asking for a ""rendering engine"". Something along the lines of OpenGL/DirectX? Or something higher level like the Unreal/Source engines?  For whatever it's worth, I'm deeply involved in DART, which stands for Dynamic Animation and Robotics Toolkit. It's definitely physics-based in the sense that it adheres very accurately to real-world physics using the Featherstone algorithm for articulated bodies.  DART is designed for research in animation and robotics, so unfortunately that means it's run by a tiny number of graduate students and the API is rather unstable (although I think it's going to start converging very soon). On the plus side, it's as free and open source as any code could be, since it uses a BSD license.  If you think you'd be interested in doing a project with DART, let me know and I could give you some pointers (pun somewhat intended)."
GraphicsProgramming,30og0j,Rhed0x,6,Mon Mar 30 04:06:25 2015 UTC,"I dunno why you deleted your last post...  Anyway, as I said before, Unreal Engine isn't open source, but the source code is available for you to view."
GraphicsProgramming,30og0j,fb39ca4,5,Sun Mar 29 06:43:00 2015 UTC,"Hi, I'm working on a renderer but using a very different approach rather than the common triangle rasterization, so it wouldn't be taking any advantage of openGL/vulkan hardware acceleration. So, I have to have an OpenCL graphics pipleline. Here is the code of my project https://github.com/ivanceras/balisong. Clearly it is a work-in progress and the goal is quit ambitious."
GraphicsProgramming,30og0j,ChainedProfessional,2,Sun Mar 29 08:08:47 2015 UTC,"Vulkan is supposed to define a shading intermediate language useful for OpenCL too, right? (SPIR-V). I'm expecting this would somehow unify GPU computations (but I'm not sure).  And hopefully NVidia will support it."
GraphicsProgramming,30og0j,7hi4g0,1 point,Sun Mar 29 16:16:06 2015 UTC,"SPIR-V can do both OpenCL and GLSL style programs, but they are separate profiles and not interchangeable.  You will however be able to use both types in the same program.    And hopefully NVidia will support it.   Nvidia already demoed a alpha driver, with good performance (on their demo workload)"
GraphicsProgramming,30og0j,ietsrondsofzo,2,Sun Mar 29 23:00:31 2015 UTC,Paradox3D is quite ambitious.
GraphicsProgramming,30p0lm,verycut,2,Sun Mar 29 12:40:53 2015 UTC,https://youtu.be/AZmJSCje5dk and there is probably also sobel edge detection like a lot of cel shaders.
GraphicsProgramming,30p0lm,doaderek,3,Sun Mar 29 16:50:32 2015 UTC,"If you're talking about the game, I read somewhere that the outlines were made by having larger models placed on top of smaller ones. The larger ones then had their normals inverted, creating the black outline."
GraphicsProgramming,30p0lm,fuzzyslippers42,1 point,Sun Mar 29 15:13:20 2015 UTC,"Hairy Brushes by Steve Strassman talks about a technique for rendering sumi-e style brush/ink strokes. Apologies, I can't find a freely available copy of the paper."
GraphicsProgramming,30p0lm,CodyDuncan1260,1 point,Mon Mar 30 01:03:27 2015 UTC,"Google Scholar is often my first resort for that.  Look for the ""All N versions"" link and you can usually find freely available copies."
GraphicsProgramming,30flgd,grepp,8,Thu Mar 26 23:33:36 2015 UTC,"The real contender would be Vulkan, since Vulkan evolved from Mantle I don't think Mantle will be around for too long."
GraphicsProgramming,30flgd,0Camus0,6,Fri Mar 27 05:48:23 2015 UTC,Mantle was already canceled from what I read. Just Vulkan and DX12 now.
GraphicsProgramming,30flgd,leetNightshade,1 point,Fri Mar 27 06:58:31 2015 UTC,"Mantle will serve as testing grounds for AMD with select partners, according to their recent announcement."
GraphicsProgramming,30flgd,Astrobastard,1 point,Fri Mar 27 12:34:52 2015 UTC,"Oh I see, some bloggers had a different take-away; that was the article they were referencing when they proclaimed Mantle dead.  Referring to this line:   if you are a developer interested in Mantle ""1.0"" functionality, we suggest that you focus your attention on DirectX® 12 or GLnext.   This is one of the places that jumped to that conclusion: PCWorld."
GraphicsProgramming,30flgd,leetNightshade,4,Fri Mar 27 16:36:43 2015 UTC,"""gracefully recover rebooting whole system"". Thats great. Next thing will be Lavender Screen, work like old Bluescreen but its subtle."
GraphicsProgramming,30flgd,moniewski,2,Fri Mar 27 09:38:10 2015 UTC,"I think from looking at it that the copy paster made an error at that point because in the mantle doc it says without rebooting. It also goes on to talk about the error code you will get in that situation, which would be a rather neat trick if they rebooted the system."
GraphicsProgramming,30flgd,ZeroPipeline,3,Fri Mar 27 10:44:54 2015 UTC,"This really looks like some bad cover-up of some copying, it's just far too much.  I think the people at /r/programming might appreciate this as well.  Edit; just noticed you already did..."
GraphicsProgramming,30flgd,FaiIsnaiI,2,Fri Mar 27 06:50:27 2015 UTC,@renderpipeline:   2015-03-26 13:32:35 UTC  I had a déjà vu while reading in the #DirectX12 specs… #mantle pic.twitter.com [Imgur]     [Mistake?] [Suggestion] [FAQ] [Code] [Issues]
GraphicsProgramming,30flgd,TweetPoster,2,Thu Mar 26 23:34:21 2015 UTC,"AMD produce XBox gpu, so it have a lot of power influencing Microsoft to shape Dx12. Anyway.. if both Vulkan and Dx12 are based on Mantle there will be less differences, so probably the porting will be easier."
GraphicsProgramming,30flgd,ilmale,-1,Fri Mar 27 12:51:35 2015 UTC,"DirectX12 is not based on Mantle. Both APIs (and the one that is based on Mantle; Vulkan) are based on the functionality of modern graphics hardware, ie. not just AMD, that's why there are similarities."
GraphicsProgramming,30flgd,qartar,3,Fri Mar 27 14:40:47 2015 UTC,"Sorry, no.  It's clearly based on Mantle.  You don't get the same wording, same content per paragraph, same bullet points by just writing a spec for modern hardware."
GraphicsProgramming,30flgd,uep,0,Fri Mar 27 18:33:32 2015 UTC,DirectX 12 was written with the API and technical documentation first. A formal specification didn't even exist until very very recently. It's pretty goddamn absurd to claim that an entire API is based off of another based on similarities (albeit jarring) in the documentation for one function.
GraphicsProgramming,30flgd,qartar,1 point,Fri Mar 27 23:43:10 2015 UTC,This must be that newfangled cross-compatibility thing I've been hearing about...
GraphicsProgramming,30flgd,fr0stbyte124,1 point,Fri Mar 27 15:30:09 2015 UTC,Relevant: https://twitter.com/fedyac/status/572821608471126018
GraphicsProgramming,30flgd,dualizer,1 point,Fri Mar 27 23:50:12 2015 UTC,@fedyac   2015-03-03 18:11 UTC  http://www.anandtech.com/show/9038/next-generation-opengl-becomes-vulkan-additional-details-released  Indeed it seems that #Vulkan was inspired by #Mantle [Attached pic] [Imgur rehost]     This message was created by a bot  [Contact creator][Source code]
GraphicsProgramming,30cj63,bkaradzic,2,Thu Mar 26 06:41:24 2015 UTC,"It's funny reading all of the ""these things are flexible and can be used like this, but these other things are not because the hardware doesn't work like that"" and wondering which vendor was the lowest common denominator this time."
GraphicsProgramming,30cj63,nexuapex,2,Thu Mar 26 15:15:41 2015 UTC,I read the first few chapters of the Mantle API when it came out and it's crazy how often a sentence from this documentation is almost word for word the same as in Mantle.
GraphicsProgramming,30cj63,mtdewhumidifier,1 point,Fri Mar 27 00:42:16 2015 UTC,"I've been reading through it.  Really interesting stuff to see how much of a departure a modern, low-overhead graphics API is from what we've been using all these years."
GraphicsProgramming,30ac4x,xcodeart,1 point,Wed Mar 25 19:33:39 2015 UTC,"Haven't looked into the code, but this looks more like a ray caster than a ray tracer."
GraphicsProgramming,30ac4x,FrezoreR,1 point,Wed Mar 25 21:43:58 2015 UTC,"For now, yeah raycasting may be the right term. This is a work in progress, it will get there eventually."
GraphicsProgramming,3041v4,fadefade,5,Tue Mar 24 08:26:56 2015 UTC,"What you are looking for is the linear algebra concept of a Basis. In your case these are the three vectors defining the surface at the intersection point : Normal, Tangent, Bitangent. If you only have normal information you may choose any perpindicular two other vectors.   I would advice you to have a look at Mitsubas excellent implementation .  In case you choose to use such basis you may transform the incoming direction into this space and all frequent shading operation will also be significantly more easy, for example check the Frame::cosTheta method :)"
GraphicsProgramming,3041v4,LPCVOID,2,Tue Mar 24 13:42:38 2015 UTC,"I can't get to the mitsuba repos for some reason (the web page works fine, but downloading the source via zip or direct access don't work).  Is there another place one can look at this code?"
GraphicsProgramming,3041v4,Delwin,2,Tue Mar 24 13:51:54 2015 UTC,"The source code download link on this page doesn't work either? I just tried it and it works fine for me.  I think pbrt has an implementation too, though it is not done in a nice OOP fashion if I remember correctly. Have a look at the BSDF::WorldToLocal and vice versa method here.  I would also advice you to have a look at pbrt in general, really helpful when starting realistic image synthesis.  In case the pbrt code is a bit obscure : You are actually only doing ths : Assume n,t,b defined as above and v a vector in the local coordinate system.  global = b * v.x + t * v.y + n * v.z       In case you are interested in the mathematical explanation : The three vectors span a vector space and the elements of your local vector are used for a linear combination of the basis vectors. Maybe this can help."
GraphicsProgramming,3041v4,LPCVOID,1 point,Tue Mar 24 14:10:20 2015 UTC,"Change of basis:       In linear algebra, a basis for a vector space of dimension n is a sequence of n vectors (α1, …, αn) with the property that every vector in the space can be expressed uniquely as a linear combination of the basis vectors. The matrix representations of operators are also determined by the chosen basis. Since it is often desirable to work with more than one basis for a vector space, it is of fundamental importance in linear algebra to be able to easily transform coordinate-wise representations of vectors and operators taken with respect to one basis to their equivalent representations with respect to another basis. Such a transformation is called a change of basis.    Image from article i     Interesting: Covariance and contravariance of vectors | Gramian matrix | Spherical basis | Matrix similarity   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
GraphicsProgramming,3041v4,autowikibot,1 point,Tue Mar 24 14:10:57 2015 UTC,"Nope, that doesn't work either.  Likely work's firewall.  The second link worked however and thank you for your information."
GraphicsProgramming,3041v4,Delwin,2,Tue Mar 24 19:01:06 2015 UTC,"we usually learn matrix-vector multiplucation by looking at each element of the output vector in isolation. this leads to the perspective of taking a dot priduct with each row of the matrix. but if you look at it from a columns perspective, the vector combines the columns according to its coordinates. so the output is x(col1) + y(col2) + z(col3). plus the translation column in the 3d homogeneous coords world.   from this perspective, the answer is obvious: the z column of the matrix should be the normal, x can be any vector orthogonal to the normal, and y is x cross z.  I know I'm not the first in the thread with this answer, but I wanted to emphasize the value of looking at matrices from the columns perspective. a lot of linear algebra teaching focuses on rows too much IMO."
GraphicsProgramming,3041v4,jurniss,1 point,Wed Mar 25 00:15:32 2015 UTC,Good explanation and from my experience your are also correct with the focus on row operations in linear algebra teaching (probably because of linear solvers).
GraphicsProgramming,3041v4,LPCVOID,1 point,Wed Mar 25 12:36:40 2015 UTC,Don't forget that it's possible to work in terms of pre-multiplied row-vectors.  In that case the rows and the columns of your matrix would be transposed and you could think of the vector as combining the rows according to its coordinates.
GraphicsProgramming,3041v4,Boojum,3,Sat Mar 28 08:07:28 2015 UTC,"I snagged some sampling math from here. But this doesn't reorient to a particular vector.  Vector3 UniformSampleHemisphere(float u1, float u2) {     const float r = Sqrt(1.0f - u1 * u1);     const float phi = 2 * kPi * u2;      return Vector3(Cos(phi) * r, Sin(phi) * r, u1); }   For that we need to 'replace' the z-axis with your orientation vector. That's easy, but then we need to find two vectors that are perpendicular to the orientation and to eachother to become our new x- and y-axes. That's a little more annoying.  Vector3 OrientedSampleHemisphere(float u1, float u2, Vector3 x, Vector3 y, Vector3 z) {     Vector3 sample = UniformSampleHemisphere(u1,u2);     return x * sample.x + y * sample.y + z * sample.z; }   This just assumes we have an 'x', 'y', 'z' vector for our new basis that is correctly oriented. It's possible you already have this! I.e. if z is your surface normal, then x and y would be to unit vectors in the surface plane. To get that in general is a little annoying.  Vector3 OrientedSampleHempshere1(float u1, float u2, Vector3 orientation) {     if (orientation.x == 0 && orientation.y == 0) {         return OrientedSampleHemisphere(u1,u2, Vector3(1,0,0), Vector3(0,1,0), orientation));     } else {         Vector3 other = Vector3(1,0,0);         Vector3 x = Vector3.cross_product(other, orientation);         Vector3 y = Vector3.cross_product(x, orientation);         return OrientedSampleHemisphere(u1, u2, x, y, orientation);     } }   That assumes that orientation is a unit-vector. Otherwise x and y won't be unit vectors either and it will be sampling on a ellipsoid not a sphere. Other than that, it works just by saying that if we want to find x and y perpendicular to z=orientation, then take any vector that's not in the same direction as z (in this case we use other = (1,0,0)) then cross product that with z. That gives us one vector perpendicular to z. Now we want a vector perpendicular to z and to that new one. So we crossproduct that with z again.  I haven't tested this code but I think it'll work and be translateable to your situation.  But is this faster than doing a matrix multiplication? I'm not at all sure."
GraphicsProgramming,3041v4,tgb33,2,Tue Mar 24 11:55:35 2015 UTC,"There is a bug in this code. If the normal points toward -z, you will still sample the ""upper"" hemisphere."
GraphicsProgramming,3041v4,angrymonkey,1 point,Tue Mar 24 17:31:03 2015 UTC,"I don't believe that is a problem (that's why I used 'orientation' instead of Vector3(0,0,1) in the second line of OrientedSampleHemisphere1. But either way, I'm pretty sure that I just re-did matrix multiplication more slowly."
GraphicsProgramming,3041v4,tgb33,1 point,Thu Mar 26 00:18:36 2015 UTC,"Be careful with this!  The cross product of two unit vectors will in general not be a unit vector itself; they must be orthogonal for it to give you back another unit vector.  For example, suppose that we have:  orientation = [sqrt(2)/2, sqrt(2)/2, 0].   Then by your recipe:  other = [1, 0, 0] x = [1, 0, 0] cross [sqrt(2)/2, sqrt(2)/2, 0] = [0, 0, sqrt(2)/2] y = [0, 0, sqrt(2)/2] cross [sqrt(2)/2, sqrt(2)/2, 0] = [-0.5, 0.5, 0]   As a result, your basis x, y, orientation will consist of vectors with magnitudes 1, sqrt(2)/2, and sqrt(1/2), respectively.  This will bias your hemisphere samples in an unexpected way.  The correct way to handle this is to normalize x before computing y.  Since x and orientation are orthogonal by construction, you don't need to normalize y as well -- just x.  I've used this method to build a basis plenty of times since it's nice and straightforward.  For something a little trickier, though, Frisvad has a nice, fast, square root-free method."
GraphicsProgramming,3041v4,Boojum,1 point,Sat Mar 28 08:33:52 2015 UTC,Good point. Another reason to ignore my code and just use a standard rotation matrix.
GraphicsProgramming,3041v4,tgb33,1 point,Tue Mar 31 12:30:42 2015 UTC,"I did this for a ray tracing simulation a while ago (not for graphics purposes), it is a vector matrix rotation basically, with the matrix being the orientation of the surface represented as 3 perpendicular unitary vectors.  Here is the snippet from my C code:  /// Vector randomly generated acording to a distribution relative to the y axis /// The VECTOR type has 3 floats as components, representing the x,y and z components of the vector VECTOR randVect; /// The surface of interest /// The SURFACE type has 3 vectors that describe its orientation, the x, y and z vectors /// those 3 vectors are unitary and perpendicular to each other, x and z are on the surface, y is the normal SURFACE surface VECTOR  OrientedVect; ///the generated vector is rotated acording to the surface orientation OrientedVect.x = (randVect.x * surface.x.x) + (randVect.z,surface.z.x) + (randVect.y * surface.y.x); OrientedVect.y = (randVect.x * surface.x.y) + (randVect.z,surface.z.y) + (randVect.y * surface.y.y); OrientedVect.z = (randVect.x * surface.x.z) + (randVect.z,surface.z.z) + (randVect.y * surface.y.z);"
GraphicsProgramming,3041v4,camilonino,1 point,Tue Mar 24 20:18:49 2015 UTC,"Section 5. Basic rotations of article  Rotation matrix:       A basic rotation (also called elemental rotation) is a rotation about one of the axes of a Coordinate system. The following three basic rotation matrices rotate vectors by an angle θ about the x, y, or z axis, in three dimensions, using the right hand rule. (The same matrices can also represent a clockwise rotation of the axes )     For column vectors, each of these basic vector rotations appears counter-clockwise when the axis about which they occur points toward the observer, the coordinate system is right-handed, and the angle θ is positive. Rz, for instance, would rotate toward the y-axis a vector aligned with the x-axis, as can easily be checked by operating with Rz on the vector (1,0,0):     Interesting: Rotation formalisms in three dimensions | Kabsch algorithm | Euler's rotation theorem | Wahba's problem   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
GraphicsProgramming,301opy,akira_fmx,6,Mon Mar 23 19:36:26 2015 UTC,Were they hungry?
GraphicsProgramming,2zzkcg,DevIceMan,22,Mon Mar 23 06:33:20 2015 UTC,"Don't just learn a tool, learn the foundations.   I'm going to assume you're talking about real-time graphics (eg, graphics for games). There's few critical things you need to learn before digging into the various engines, languages, and tools:   How the programmable graphics pipeline works. 2D and 3D Math primitives (vectors, matrices, quaternions, etc.) Transformations and screen projections. Anatomy of a single frame in a game loop. Lighting and texturing (UV coords, normal mapping, etc.) Shaders (geometry, vertex, and pixel shaders). History of GPUs and current compatibility considerations.   Best resource to start learning these concepts is ""Real-Time Rendering"" by Tomas Akenine-Möller. If you're really adventurous, you could also check out ryg's ""A Trip Through the Graphics Pipeline"" for an even more in-depth view.  Once you've picked up a copy, you should then start learning C++ (shouldn't be too tough of a jump from Java) and then try to build an extremely simple 3D game using Ogre3D or similar 3D engine. (Their samples, basic tutorials, and forum have a wealth of information on these topics.)  To start learning shaders (which is fast becoming a complex and separate discipline on its own), I would start with GLSL since it's versatile and can run in modern browsers via WebGL. The majority of interest today are in pixel/fragment shaders (a bit of code that runs massively-parallel for every pixel on screen). Start by reading The Book of Shaders (currently a WIP) and then playing around with the creations on ShaderToy. (You will also want to learn about vertex shaders and potentially geometry shaders).  Once you've mastered the basic concepts, you can then start honing your skills in a specific set of tools. Unreal Engine, CryEngine, Source, and Unity are the mainstream game engines. Direct3D is more popular than OpenGL in today's studios simply because it runs better on Windows (which is the biggest market-share of PC gamers). Keep in mind that console platforms use their own graphics API (eg, PS4 uses GNMX and PSSL) so you'll have to learn many different APIs (but all of them work in similar ways).  Good luck!"
GraphicsProgramming,2zzkcg,gazaleon,0,Mon Mar 23 08:33:13 2015 UTC,"probably want to start with OpenGL/DirectX, but I would advise you to get comfortable with the basisc C++ first. Then maybe so   how long should this take?"
GraphicsProgramming,2zzkcg,iMakeSense,1 point,Mon Mar 23 20:45:52 2015 UTC,"Depends on background and experience.  For me, probably not long.  I did C++ some 10 years ago, and have a solid Java background recently.  I even passed & received an offer after a C++ coding test 1.5 years ago, though I had to spend a lot of time looking up syntax."
GraphicsProgramming,2zzkcg,Devlin1991,1 point,Tue Mar 24 00:42:20 2015 UTC,"I'd read into SIMD intrinsics (SSE2 mainly) and basic multi-threading, I use OpenMP since it's easy to use, widely supported in compilers and cross-platform. The C++ side of OpenGL should be pretty simple for you, it's just another library. High-performance code that can scale for anything CPU side is the important bit. Using Structures of Arrays rather than Arrays of Structures is the key to letting you maximise the throughput of whatever CPU your are using. Game update loops often have for loops that need to run over large data sets so making sure that it can be split across threads and that you can SIMD the commands on each thread is important.  Someone with industry experience should be able to give you more detailed pointers on current trends."
GraphicsProgramming,2zzkcg,r3v3r,6,Tue Mar 24 23:34:54 2015 UTC,"The language is mostly C++ and functional-programming is not the best fit since graphics prog. is all about side effects. (But it can obviously be done)  You probably want to start with OpenGL/DirectX, but I would advise you to get comfortable with the basisc C++ first. Then maybe some tutorials or a book? I'm sure there are other libraries/engines you could look at if that is too much for you at the beginning."
GraphicsProgramming,2zzkcg,Dworgi,1 point,Mon Mar 23 06:53:24 2015 UTC,"Thank you for the information.  C++ does seem like the primary language based on my research, or at least based on job-listings.  Tutorials are ok, but I generally find I don't learn as well from tutorials. I've found the most effective way for me to master something is to build something 'creative' with it; start with some existing source-code that works, start hacking away it it, break things, fix things, google/stackoverflow.   OpenGL/DirectX   Which one do you think is more 'employable'?  Which is more pleasurable to work with?"
GraphicsProgramming,2zzkcg,Dworgi,3,Mon Mar 23 07:33:17 2015 UTC,"DirectX is the bigger of the two in the games industry, and I'd venture that something like 70% of graphics jobs are either in or serving the games industry. Obviously, there's a lot of overlap on the tools side between film and games as well. But DirectX has the lead and the better docs right now, which may or may not change in the next year.   With your experience, I personally wouldn't tap you for TD because you lack industry-specific programming and leadership experience, but you could likely get a technical artist position with little effort."
GraphicsProgramming,2zzkcg,Dworgi,1 point,Mon Mar 23 07:50:05 2015 UTC,"DirectX sounds like a reasonable direction.  Industries don't tend to swing that far in a year, and in two years I could always learn the other one.   I personally wouldn't tap you for TD   My intent was to describe the role of developing tools for 3D Artists. You're right, technical director would be a stretch.  Assisting a TD is something I could probably do today, but lacking C++/DirectX/etc experience would probably make selling that more difficult... meaning landing a sub-optimal underpaid job for a year, and then job-hopping again."
GraphicsProgramming,2zzkcg,Dworgi,2,Mon Mar 23 08:02:45 2015 UTC,"That's a tech artist. In which case I'd also study Python, because it seems to be the standard for TAs, probably because of plugins for Maya/MoBu being Python.   Also, there tends to be a divide between engine devs and tools programmers, so maybe look around first and see if the DirectX is even necessary.   Obviously, tinker away all you want, but just saying you might not even need all that much retraining."
GraphicsProgramming,2zzkcg,VilHarvey,1 point,Mon Mar 23 08:10:39 2015 UTC,"(I appreciate the useful info, thanks!)   That's a tech artist.   Got it.   Python   I will also look into that.  Many years ago I assisted a TD, and remember Python was a big part of his work.   Also, there tends to be a divide between engine devs and tools programmers   Interesting.  I suppose it doesn't matter to me which; except games-industry attracts desperate/passionate people often leading to under-paid and long hours. So, probably tools-programmer.   Obviously, tinker away all you want, but just saying you might not even need all that much retraining.    True, I'm confident I could do the job as I've somewhat done it before.  That said, knowing C++/Python/etc competently might make a significant difference in pay, number of offers, and interview confidence.  I also plan on sticking to my current job for another 6 months, so I've got time to plan ahead."
GraphicsProgramming,2zzkcg,Dworgi,1 point,Mon Mar 23 08:26:02 2015 UTC,"Having a working C++ renderer is definitely a huge leg up. It's tough to do, so it definitely looks good in a portfolio."
GraphicsProgramming,2zzkcg,angrymonkey,1 point,Mon Mar 23 08:59:03 2015 UTC,"Developing a renderer from scratch would be quite a lot of work. I used to have a habit of tacking gigantic projects ... and actually completing them, but in the end I spent a ridiculous amount of time for little reward.  I'd probably prefer to take the route of enhancing an existing renderer in some way.  Plenty of tough things one could do that have a tangible result within a shorter period of time.  Appreciate the suggestion, but scope would just be too huge. :P"
GraphicsProgramming,2zzkcg,__Cyber_Dildonics__,1 point,Tue Mar 24 00:33:36 2015 UTC,"I mean, not necessarily. A few libraries to load meshes and you could have something that displays flat shaded models in a few days, tops.   Not fully featured of course, but enough to prove you do know what you're talking about."
GraphicsProgramming,2zzkcg,hahanoob,1 point,Tue Mar 24 07:12:12 2015 UTC,"Just a side point: maybe this is an industry- or region-specific thing, but ""technical director"" is a common job title at least in the London VFX houses and means exactly the same as what you're calling a tech artist. In any case, you're right about python being more important than c++ for that kind of role."
GraphicsProgramming,2zzkcg,__Cyber_Dildonics__,1 point,Mon Mar 23 14:12:22 2015 UTC,"Oh, really? Didn't know that. My apologies to the OP, then.  I guess it makes more sense in the movie industry context (not that they're really directing the movie or anything).  In the games industry, technical director means the head of all the technical staff at the studio.   Those are some surprisingly different roles for the same job title, though..."
GraphicsProgramming,2zzkcg,__Cyber_Dildonics__,1 point,Mon Mar 23 15:56:53 2015 UTC,"I was used to 'TD' being a software-engineer who knows a lot about the 3D/VFX space, and creates advanced tools for artists.  Also, thank you for confirming the importance of Python; it appears I should definitely try to get a good working knowledge of both based on replies here."
GraphicsProgramming,2zypzg,Pyrolistical,3,Mon Mar 23 01:16:02 2015 UTC,"Damn, this brings me back, not just quake, but I haven't used POV-ray in about 20 years."
GraphicsProgramming,2zwm8a,asb,2,Sun Mar 22 14:34:39 2015 UTC,"lowRISC is a project to produce a complete open source System-on-Chip. We are fortunate to be taking part in Google Summer of Code. We're working with a number of our friends in the wider open source software and hardware to offer a wide range of project ideas. One of these I thought might appeal to redditors here is implementing a ray tracer optimized for the Nyuzi parallel processor (its design is inspired by Larrabee). Students get paid a stipend of $5500.   Nyuzi is an open source parallel processor architecture. This project would implement a ray tracer that takes advantage of both vector arithmetic and hardware multithreading. This would act as a benchmark for exploring the performance of this architecture and also a validation test. Part of the project may also involve proposing/implementing instruction set or architecture extensions to improve performance.    EDIT to add, deadline for student applications is on Friday!"
GraphicsProgramming,2zwm8a,ChainedProfessional,2,Sun Mar 22 14:41:25 2015 UTC,"Libre hardware, CPU or GPU, is something I can get excited about."
GraphicsProgramming,2zubli,TurkishSquirrel,1 point,Sat Mar 21 21:13:03 2015 UTC,This is neat! Is there a way of compiling it so that the intrinsics generate non-AVX code? It would be interesting to see how close to 8x you are getting.
GraphicsProgramming,2zubli,snakepants,1 point,Sat Mar 21 23:00:35 2015 UTC,"I don't think so, as Scaless mentioned I'm directly using the vector types and intrinsics so the code isn't very flexible. ISPC (or something similar) is the approach I'd recommend for a more serious program and it might be able to compile a scalar version for comparison, this is really just a hacky one-off."
GraphicsProgramming,2zubli,snakepants,1 point,Sat Mar 21 23:58:58 2015 UTC,"Yeah, too bad passing -mno-avx disables the functions instead of just replacing them with C implementations. It would be useful for debugging too."
GraphicsProgramming,2zubli,planetelex,1 point,Sun Mar 22 00:40:42 2015 UTC,Doesn't embree choose to use simd at the leaves instead of packet tracing? E.g. testing against 4 triangles at once?
GraphicsProgramming,2zubli,planetelex,1 point,Mon Mar 23 04:51:16 2015 UTC,"Embree supports single, packet and hybrid traversal in the BVH and has various options for testing triangles, eg. testing N tris vs. M rays where N & M can be 1, 4 or 8 (and maybe 16 on Xeon Phi?). It's probably best not to take my word for this, I'm only somewhat familiar with Embree. Definitely check out the paper and source for specifics."
GraphicsProgramming,2zqzyj,redstar2530,2,Fri Mar 20 22:33:51 2015 UTC,"Can you get a bit more specific as to what kind of help you're looking for?  I don't mind helping you out, and I wouldn't necessarily charge you for it. I love what I do, and I'm generally pretty good at conveying my knowledge to others, so I don't mind :)"
GraphicsProgramming,2zqzyj,goal2004,3,Sat Mar 21 01:41:31 2015 UTC,Any books you'd recommend for a beginner?
GraphicsProgramming,2zqzyj,CodeCodeCodeDurrr,2,Sat Mar 21 06:56:11 2015 UTC,PMed you
GraphicsProgramming,2zlg8n,knarkowicz,5,Thu Mar 19 15:52:57 2015 UTC,Might as well learn Vulkan at this point if you're interested in Mantle. It's supposed to replace Mantle.
GraphicsProgramming,2zlg8n,seieibob,3,Thu Mar 19 18:14:09 2015 UTC,"For me this guide has mainly educational value as it offers insight into GCN GPUs. Something that until now was available only to console developers. Additionally Mantle API is a good introduction to Vulkan/DX12, and most abstractions should be similar. So even there are no public Vulkan/DX12 docs, you can already prepare you engine for it."
GraphicsProgramming,2zlg8n,seieibob,1 point,Thu Mar 19 22:39:46 2015 UTC,True enough. This is an exciting time for computer graphics!
GraphicsProgramming,2zlg8n,dinosaurdynasty,2,Fri Mar 20 00:57:57 2015 UTC,Vulkan is supposed to be quite similar to Mantle. It might be useful to look at before it’s released.
GraphicsProgramming,2zlg8n,seieibob,7,Thu Mar 19 19:10:13 2015 UTC,"Well, that's the thing -- Vulkan is Mantle. AMD donated it to Khronos to move glNext forward, and Vulkan is built on top of it.  I'm not dismissing this by any means though!"
GraphicsProgramming,2zlg8n,dinosaurdynasty,2,Thu Mar 19 19:40:31 2015 UTC,Yeah. I wish they’d release Vulkan already :)
GraphicsProgramming,2zg54t,turol,2,Wed Mar 18 08:22:18 2015 UTC,"Not to be confused with the accumulation buffer.  I think what he means is to batch up as many 2D sprites as you can on the CPU, and submit them as one draw call."
GraphicsProgramming,2zcow5,Orangy_Tang,20,Tue Mar 17 14:18:12 2015 UTC,"There's a bit about the Bioshock Infinite shop fronts at the middle/end of this article: http://simonschreibt.de/gat/windows-ac-row-ininite/  EDIT: While digging around for that link, I also found http://solid-angle.blogspot.de/2014/03/bioshock-infinite-lighting.html."
GraphicsProgramming,2zcow5,doersino,7,Tue Mar 17 14:35:39 2015 UTC,"Well that's definitely the article I was (dimly) remembering, thanks.  However it doesn't really go into any of the maths underlying the effect. If anyone has a good reference that covers that I'd be grateful."
GraphicsProgramming,2zcow5,__Cyber_Dildonics__,3,Tue Mar 17 16:14:24 2015 UTC,"I will have to look at the article but it sounds a lot like parallax mapping, which is actually a very simple technique."
GraphicsProgramming,2zcow5,heeen,2,Tue Mar 17 16:47:17 2015 UTC,"pretty cool. Before I read about it for windows, I had the idea to use this for light fixtures, think neon tube behind frosted/diamond-cut glass."
GraphicsProgramming,2zcow5,TomorrowPlusX,2,Tue Mar 17 18:51:59 2015 UTC,"Often, when waiting for an elevator, I'll see the rich way these kinds of light fixtures illuminate and think about how I'd shade it."
GraphicsProgramming,2zcow5,herabec,3,Wed Mar 18 13:30:34 2015 UTC,Related https://udn.epicgames.com/Three/DevelopmentKitGemsInteriorMapping.html
GraphicsProgramming,2zd7do,__Cyber_Dildonics__,3,Tue Mar 17 16:43:59 2015 UTC,"The C++ API of BGFX doesnt use BGFX_SHARED_LIB_API on its functions, so the linker will not export them in a DLL on Windows (Linux will probably work). Workarounds: use the C API (bgfx::init becomes bgfx_init(BGFX_RENDERER_TYPE_OPENGL, nullptr, nullptr), see bgfx.c99.h). Or put BGFX_SHARED_LIB_API  to the funcs you need (i.e. void BGFX_SHARED_LIB_API init(...)), or stick to the static library.  And yes, BGFX is pretty cool."
GraphicsProgramming,2zd7do,ryp3gridId,2,Tue Mar 17 18:30:49 2015 UTC,This is intentional. C99 and shared lib are intented for other language bindings where static linking is not possible.
GraphicsProgramming,2zd7do,bkaradzic,1 point,Fri Mar 20 15:35:35 2015 UTC,"This makes sense now that I know about it, but it has been very confusing.  I think bgfx could benefit immensely from pre-compiled binaries for msvc and mingw (which people could use with Clion which seems to be shaping up very nicely)."
GraphicsProgramming,2zd7do,bkaradzic,1 point,Sat Mar 21 12:13:18 2015 UTC,"I don't have build system yet, nor ability to easily distribute binary files. Also limiting binary only to Windows is lame thing to do for cross-platform lib. I would need to ship binaries for: win-x86, win-x64, winrt, linux-i686, linux-x64, linux-mips, linux-arm, linux-aarch64, android-arm, android-aarch64, android-mips, nacl-x86, nacl-x64, nacl-arm, pnacl, etc. you get the point... :)"
GraphicsProgramming,2zd7do,fungosbauux,1 point,Sat Mar 21 20:30:23 2015 UTC,"Try fips! There is already support for bgfx with fips here.  To build and test (on windows, linux, macos and emscripten) it is as easy as (on windows using github shell for example):  $ git clone https://github.com/floooh/fips.git $ cd fips $ ./fips clone fips-bgfx $ cd ../fips-bgfx $ ./fips build $ ./fips run 00-helloworld   Also on windows you can do:  $ ./fips open   to launch visual studio with the project.  Note that at the moment there is no working build for building shared libraries.  But the nice thing of fips is that it manages all dependencies, so if you want to use a fipsified library in your fipsified project it is effortless. It requires only the github address in a yml file."
GraphicsProgramming,2zd7do,Colin-Gilbert,1 point,Wed Mar 25 23:20:43 2015 UTC,"Awesome thanks for the tip.  That makes sense, so I'm guessing I should be able to get it to work now that I know that.   That really seems like more cryptic bgfx pitfalls, I'm a little floored by many ways there are to get stuck just getting something to work."
GraphicsProgramming,2zd7do,bkaradzic,2,Tue Mar 17 19:43:57 2015 UTC,"I have been working on a game engine for a little while now, and after some hard work BGFX is a well-integrated component by now. Currently builds with CMake. The engine/framework comes with a basic triplanar shader and some GPU branching avoidal code I ported from GLSL. Beware: Very raw codebase, features being added/shifted/removed, etc. https://github.com/ColinGilbert/noobwerkz-engine"
GraphicsProgramming,2zd7do,bkaradzic,2,Fri Jul 3 09:50:54 2015 UTC,You should be able to build and run examples by following these steps: https://github.com/bkaradzic/bgfx#getting-source
GraphicsProgramming,2z9gw7,mflux,-3,Mon Mar 16 19:05:39 2015 UTC,Shaders are map-reduce.
GraphicsProgramming,2z9gw7,moron4hire,0,Tue Mar 17 03:21:22 2015 UTC,I know! I'll add a Hadoop backend to my engine!
GraphicsProgramming,2z9gw7,cmrx64,0,Tue Mar 17 12:46:57 2015 UTC,"As a programmer transitioning from BigData analytics, enterprise Java software stuffs to graphics programming, I find this humurous. :D"
GraphicsProgramming,2zawb7,thatisthewaz,3,Tue Mar 17 01:34:12 2015 UTC,Here's a tutorial on making a software (CPU) raytracer from absolutely scratch in a pixelpusher environment. This is made by a teacher of mine. He's great.  Have fun!
GraphicsProgramming,2zawb7,ietsrondsofzo,1 point,Tue Mar 17 10:19:47 2015 UTC,Wow thanks! This is exactly what I was looking for!
GraphicsProgramming,2zawb7,ultrapingu,2,Tue Mar 17 10:28:14 2015 UTC,"This does a really good job of explaining it.  (The top level of the site was originally posted by u/Boojum).  In short, a ray tracer is an attempt to render a scene in a pseudo physical way.  Generally speaking a scene contains some objects.  Many rays are collided with the scene, and from calculations based on these collisions, a pixels color is calculated.  The 'physical' part comes from the fact that the rays represent the paths that photons would take in a realistic scene.  The 'pseudo' part comes from the fact that object are modelled with mathematical objects such as spheres and planes (the interaction of electrons and atoms is much more complex than that of rays and spheres/planes)."
GraphicsProgramming,2zawb7,raydeen,1 point,Wed Mar 18 17:28:49 2015 UTC,"I couldn't tell you how to implement one, but the theory behind raytracing is that you're working backwards to render a scene as to how it's normally displayed in nature. In the real world, light from a source (the Sun, a lamp, etc.) casts a ray of photons to an object and that ray is then reflected off the object to your eye. Raytracing goes the other way. The color and intensity of each pixel on your screen is determined by tracing a path from that pixel to the object and then to the light source. POV-Ray is a raytracer that's been around for decades. I remember my brother-in-law playing with this on his 386. It would literally take days back then to render a scene with more than a few objects, reflections and light sources. The source code is available so maybe you can get some ideas about how it works by looking at it. All that stuff is way over my head. I just know the ELI5 bit. :)"
GraphicsProgramming,2zawb7,gegoggigog,1 point,Tue Mar 17 01:57:20 2015 UTC,"I don't know if agree. I would say that ray tracing is the general term for tracing rays in a scene. Path tracing is a subset where you generally trace one path at the time, most often from camera to light. With photon mapping you trace from the light to the scene. Bi-directional tracers trace from the camera and the light and tries to connect the rays. Each with their pros and cons. But for starters path tracing from camera to light would be the easiest. :)"
GraphicsProgramming,2z9b8t,vblanco,3,Mon Mar 16 18:26:29 2015 UTC,"I have no experience with this, but a couple of weeks ago I saw this on youtube which might help you :)  https://www.youtube.com/watch?v=HQYsFshbkYw  Edit: Description from video   In this tool-assisted education video I walk through the creation of a Duke Nukem 3D style software-rendering pseudo-3D engine from scratch. Topics such as vector rotation and portal rendering are at the core. SUBTITLES ARE AVAILABLE."
GraphicsProgramming,2z9b8t,andersevenrud,2,Mon Mar 16 19:02:45 2015 UTC,Divide by Z. It was that simple.... Thank you for the link.
GraphicsProgramming,2z9b8t,andersevenrud,1 point,Mon Mar 16 22:54:17 2015 UTC,Glad you found use for it. Happy coding! :)
GraphicsProgramming,2yxjgx,mflux,4,Fri Mar 13 17:42:56 2015 UTC,Thank you so much for sharing this. I'm surprised that the post wasn't given more upvotes. Maybe the people in /r/algorithms or /r/learnprogramming will be more interested in it.
GraphicsProgramming,2yxjgx,Oersted4,1 point,Fri Mar 13 22:52:56 2015 UTC,This was already posted about a year ago I think.
GraphicsProgramming,2yxjgx,s3vv4,2,Sat Mar 14 13:37:27 2015 UTC,"Woah holy shit, this is fascinating !"
GraphicsProgramming,2yxjgx,coyhot,1 point,Fri Mar 13 23:39:08 2015 UTC,Wow. Got a bit of a math chubby from that.
GraphicsProgramming,2yxjgx,timothyallan,1 point,Sat Mar 14 03:15:26 2015 UTC,Some of the pictures didn't load but the interactive calculator animation looks very cool.
GraphicsProgramming,2yxjgx,ChainedProfessional,1 point,Sat Mar 14 03:54:33 2015 UTC,As a guy that is interested in procedural textures this article was such an inspiration.
GraphicsProgramming,2yuhhe,kraakf,3,Thu Mar 12 22:31:20 2015 UTC,"One option is to release the source of the drivers, making it possible for motivated engine developers to do this without explicit access to AMD developers.  If they did this on Linux as well, then developers would have access to the full stack, in order to be able to learn about how the lower levels work and more easily track the problems down without simply a whole lot of trial and error on a black box."
GraphicsProgramming,2yuhhe,FrezoreR,7,Thu Mar 12 22:31:43 2015 UTC,"There is nothing to gain and all to lose by doing that for lets say Nvidia, whom I believe have the highest quality drivers among OEMs.   Also, in graphics drivers it's all about performance and nothing about readbility/structure etc.  Meaning very few people can look at them and understand what is happening.  So unless you are thrilled by looking at crazy bitshifts etc I'd advice you not to look at graphics drivers. If you still want to, there are those that are open sourced, see here."
GraphicsProgramming,2yuhhe,qartar,5,Thu Mar 12 23:45:25 2015 UTC,Even triple-A devs don't have the time or patience to wade through millions of lines of driver code. Hardware vendors have a vested interest in proactively making games run better on their own hardware because it makes their hardware more marketable.
GraphicsProgramming,2ytaw9,ngildea,1 point,Thu Mar 12 17:17:17 2015 UTC,That's awesome!
GraphicsProgramming,2ytaw9,mrjmoak3,1 point,Fri Mar 13 03:17:39 2015 UTC,Thanks :)
GraphicsProgramming,2ys7ls,joebaf,3,Thu Mar 12 11:11:05 2015 UTC,~12ms for the voxel cone tracing GI is reasonable on the PS4. It's nice to see Crassin's work making it into shipping products.  Thanks for the slides.
GraphicsProgramming,2yqjvf,akira_fmx,9,Wed Mar 11 23:53:31 2015 UTC,"Seems like a joke, poking fun at the several other low-level graphics APIs being released by other big tech companies."
GraphicsProgramming,2yqjvf,macc188,7,Thu Mar 12 08:32:04 2015 UTC,"One of Carmack's later tweets clarifies that he has no insider knowledge, but that he has heard rumours."
GraphicsProgramming,2yqjvf,DashAnimal,7,Thu Mar 12 09:39:37 2015 UTC,"@ID_AA_Carmack:   2015-03-11 16:22:04 UTC  Hey Google, could you pretty please not develop another brand new low level graphics API?     [Mistake?] [Suggestion] [FAQ] [Code] [Issues]"
GraphicsProgramming,2yqjvf,TweetPoster,11,Wed Mar 11 23:54:46 2015 UTC,Friggin Google. I won't invest in anything programming-related they create. They'll support it for 2 years and then dump it for something new.
GraphicsProgramming,2yqjvf,mahacctissoawsum,15,Thu Mar 12 01:07:49 2015 UTC,"Yeah - I'm expecting Google Code to vanish one day, and it's the only place where my projets have any organisation. =("
GraphicsProgramming,2yqjvf,SarahC,12,Thu Mar 12 06:04:29 2015 UTC,"Well, that sure was prescient..."
GraphicsProgramming,2yqjvf,person594,3,Thu Mar 12 17:38:57 2015 UTC,YEAH!  WTF!?
GraphicsProgramming,2yqjvf,SarahC,1 point,Fri Mar 13 05:28:52 2015 UTC,That's some insane prescience indeed.
GraphicsProgramming,2yqjvf,Vyleia,9,Mon Mar 16 17:51:09 2015 UTC,why not GitHub or BitBucket?
GraphicsProgramming,2yqjvf,peridox,1 point,Thu Mar 12 07:13:25 2015 UTC,I got Subversion to work with it.
GraphicsProgramming,2yqjvf,SarahC,1 point,Fri Mar 13 05:29:27 2015 UTC,Subversion is just fine.
GraphicsProgramming,2yqjvf,Vyleia,1 point,Mon Mar 16 17:51:28 2015 UTC,"I couldn't get GitHub to work, I'm not a bright person."
GraphicsProgramming,2yqjvf,SarahC,2,Fri Mar 13 06:48:41 2015 UTC,try.github.com is pretty helpful for that.
GraphicsProgramming,2yqjvf,peridox,1 point,Fri Mar 13 07:32:29 2015 UTC,Thank you..!
GraphicsProgramming,2yqjvf,SarahC,8,Tue Mar 17 05:51:10 2015 UTC,"funnily enough, google code is announced as shutting down the day after you post this comment."
GraphicsProgramming,2yqjvf,peridox,2,Thu Mar 12 19:19:00 2015 UTC,"Yeah, shit - I have things to do."
GraphicsProgramming,2yqjvf,SarahC,5,Fri Mar 13 05:29:39 2015 UTC,"Yeah - I'm expecting Google Code to vanish one day,   Seems you left the ""in"" out of that sentence."
GraphicsProgramming,2yqjvf,MarshallBanana,3,Fri Mar 13 08:45:49 2015 UTC,I'll tidy up after I finish crying.
GraphicsProgramming,2yqjvf,SarahC,1 point,Fri Mar 13 00:52:05 2015 UTC,"No need to cry, just try this."
GraphicsProgramming,2yqjvf,xPragma,1 point,Fri Mar 13 05:29:51 2015 UTC,I'll leave it a few days - 12 hours! I lose track!
GraphicsProgramming,2yqjvf,SarahC,2,Fri Mar 13 06:40:39 2015 UTC,They've already killed downloads.
GraphicsProgramming,2yqjvf,SarahC,1 point,Fri Mar 13 06:53:47 2015 UTC,What?!  I need to get my source!
GraphicsProgramming,2yqjvf,MarshallBanana,2,Thu Mar 12 09:08:55 2015 UTC,Just the downloads section. You can't upload new files. You can still get your old stuff.
GraphicsProgramming,2yqjvf,SarahC,1 point,Fri Mar 13 05:30:03 2015 UTC,Phew!
GraphicsProgramming,2yqjvf,SarahC,1 point,Fri Mar 13 08:47:18 2015 UTC,"Downloads, what downloads?  My Subversion sync is fine?!"
GraphicsProgramming,2yqjvf,SarahC,1 point,Tue Mar 17 05:51:59 2015 UTC,http://google-opensource.blogspot.com.au/2013/05/a-change-to-google-code-download-service.html
GraphicsProgramming,2yqjvf,xPragma,1 point,Fri Mar 13 06:50:36 2015 UTC,Thank you!
GraphicsProgramming,2yqjvf,SarahC,2,Fri Mar 13 07:20:33 2015 UTC,aaaand there it goes
GraphicsProgramming,2yqjvf,Rocketman7,3,Tue Mar 17 05:48:14 2015 UTC,Bastardddddddds!
GraphicsProgramming,2yqjvf,Boojum,2,Thu Mar 12 22:11:07 2015 UTC,I wonder what he's talking about. I'm guessing he has same inside information.
GraphicsProgramming,2yqjvf,Rocketman7,1 point,Fri Mar 13 05:30:13 2015 UTC,I suspect he may be referring to Freon.
GraphicsProgramming,2yqjvf,gott_modus,1 point,Thu Mar 12 07:05:23 2015 UTC,"hmm... maybe, but Freon is not a low level graphics API. As far as I know, Freon uses OpenGL ES for 3D graphics."
GraphicsProgramming,2yqjvf,Noctune,2,Thu Mar 12 09:43:24 2015 UTC,"Lmao.   In like a year we've gone from being an industry fighting over 2 cross-vendor graphics APIs - one of which is cross-platform - with low barriers to entry to now potentially 6 APIs; 4 of which are deemed as low level and controlled each by a single company.  It's like watching a movie for 30 minutes which is pretty tame in terms of its pacing, and then all of a sudden an explosion out of nowhere happens and we have a plot twist which permanently intensifies the movie."
GraphicsProgramming,2yqjvf,gott_modus,4,Thu Mar 12 10:11:54 2015 UTC,Mantle is being discontinued in favor of Vulkan at least.
GraphicsProgramming,2yqjvf,qubedView,1 point,Thu Mar 12 18:28:02 2015 UTC,AMD: the first to show and the first to leave.
GraphicsProgramming,2yqjvf,APersoner,1 point,Thu Mar 12 20:44:32 2015 UTC,"Being that they're backing Khronos, I highly doubt it."
GraphicsProgramming,2yqjvf,ccricers,1 point,Fri Mar 13 00:16:34 2015 UTC,Like they backed Mozilla?
GraphicsProgramming,2yp79b,FaiIsnaiI,12,Wed Mar 11 17:44:27 2015 UTC,"If you'd really like to understand the physics and math, I cannot recommend ""Physically Based Rendering"" by Pharr and Humphreys highly enough. That will cover just about everything you need to know about modern light transport algorithms.   If you have specific questions, I can probably answer them."
GraphicsProgramming,2yp79b,angrymonkey,1 point,Wed Mar 11 18:29:32 2015 UTC,"Not a specific question, but I was thinking about writing a simple* path tracer of sorts as a learning method. Having the freedom to get the physics/math right without having to worry about the limitations of the method I use is of huge importance to me, which lead me to think that basic ray/path tracing would be a better choice than using rasterization.  However it appears to me that tracers are usually made by people who are already verry much into the theory, and that using it as a starting project is extremely uncommon.  So, would you think that writing a basic* tracer is a good way to learn the basics? Or to start with a more common approach?  Thanks a lot for your time.  *Using only spheres with uniform material properties, camera obscura (though a lens doesn't seem too hard to pull off?), and for illumination simple point light sources with shadow rays (or mabye an 'ambient' value for materials to essentially make a material act as a light source)"
GraphicsProgramming,2yp79b,angrymonkey,3,Wed Mar 11 21:27:18 2015 UTC,"I think writing a tracer is a fantastic beginning project. Tracers are very very simple; rasterization is complex. If you can write code to intersect a ray with a sphere, a tracer like you describe should be completely doable. It would not be unreasonable to get a path tracer producing simple lit images in a few hours of coding (but if you are new and working through the concepts, it will almost certainly take you longer). In general, tracers are great because the concepts map pretty directly to physics, and you can really, really easily get some nearly photographic looking images.  Also, thin-lens cameras are pretty easy:    Choose a focal length (distance from film to lens), an aperture size, a film size, and a focal plane (region of sharp focus).  Place the aperture in front of your film, and the focal plane out in your scene somewhere in front of the camera. Foreach pixel:   Compute the location of the pixel you're shading on the film. Compute a point on the focal plane by casting a ray from the pixel through the center of the aperture.  Choose another random point on the aperture to sample from. Fire a ray from the random point on the aperture through the point on the focal plane, and shade whatever scene object it hits. Do this a bunch of times for each pixel and average the colors you get; store the result in your pixel.    Boom! Depth of field."
GraphicsProgramming,2yp79b,cbt13,1 point,Wed Mar 11 22:19:41 2015 UTC,I already had the feeling that lenses would generally be done like you described (but nothing beats the ease of a camera obscura).  I actually have the line-sphere intersection math (and most other non-graphics related stuff) worked out already.  Thank you for you time.
GraphicsProgramming,2yp79b,jrkirby,-2,Wed Mar 11 22:35:28 2015 UTC,I want to be you.
GraphicsProgramming,2yp79b,ilmale,7,Wed Mar 11 18:38:52 2015 UTC,"It's kinda like this: Given an angle of a photon hitting a surface, for all the possible angles, what's the probability the photon will exit at that angle? If you can make a function that figures this out, then that's the BRDF."
GraphicsProgramming,2yp79b,johmik,5,Wed Mar 11 20:24:08 2015 UTC,"This http://blog.selfshadow.com/publications/s2013-shading-course/ and this http://blog.selfshadow.com/publications/s2014-shading-course/ Hoffman slides should be a good introduction (second link have more updated slides, first one have course notes). Also ""Understanding the Masking-Shadowing Function"" is quite interesting.  A briefly explanation can be: A BRDF is a Distribution of probability Function that give you the probability that an incoming ray on the surface will be Reflected into a specific direction. And cause is Bidirectional the opposite is also true. Is used during shading to compute how much light energy is reflected in a certain direction. The BRDF can be measured with a Gonioreflectometer and stored in a texture, but most of the shaders use math formulas.  Most of the math formulas came from microfacet theory.  Microfacted theory said that at microscopic level everything behave like a mirror that can absorb some light and reflect the rest of light. All the formulas that you may find (Cook-torrance/Smith/Beckmann/Kelemen) is some guy that try to create a model of how these micro mirror are distributed and the probability that the normal of the mirror is oriented in the right direction (Normal distribution function). The light can also that hit the micro mirror can be shadowed by other mirror (Shadowing), of the reflected light can hit another mirror and reflected in another direction (Masking). This give you something like: Ndf(h) * Shadowing_Masking(l, v, h) * Fresnel(v, h) / (4 * dot(n, l) * dot(n, v))  We don't have a definitive model for every material but the actual model give nice results for metal and dielectric. There are also other specialized model for particular material where the generic model fail, for example clear coat (that have two layer) or fabric. Anyway, Hoffman notes should introduce the topic quite well.  edit: OMG Gold, thanks kind stranger."
GraphicsProgramming,2ynwre,ultrapingu,3,Wed Mar 11 10:13:55 2015 UTC,"Don't have any ideas for c++ and DX. But you can take one of the game engines like Unity and work only on shaders and effects. Unity uses C# and shaders (GLSL and their own SL).  Personally I used to work on games which was using C++ and DX9 before. I still like graphics but work is not related to games any more. So I found way to get fun from working on effects. Writing small androind live wallpapers using Java and OpenGL ES (with GLSL).  You can also play with WebGL. Use one of the libraries to get support of basic stuff like matrices. You will use Javascript and WebGL. Shaders are not so powerful as on desktop versions of DX or OpenGL, but you still can have fun.  Another approach would be to focus on shaders and effects for one of the 3d packages. You will build scene and then write effects for all materials. Can be interesting. I love Renderman Shading language. You can do very nice effects in it. IIRC you can use GLSL shaders in Maya."
GraphicsProgramming,2ynwre,ghisguth,1 point,Wed Mar 11 15:46:08 2015 UTC,"Hey I'm in a situation like you.I exactly want to study on what you say above(bloom,AO etc.) and I'm now working on a D3D11 renderer before implementing those techniques. I'm basically studying on open source engines like Ue4,Ogre3D,Hy3 and learn from them then implement what I learnt for my so-called graphics renderer and then I can use it for making numerous demos.Its ~8000 LOC atm and quite more to go.I plan to implement D3D12 and Vulkan too(if they release it in the near future).So if you have time I'd suggest implement a D3D11 renderer then go from there.I'd say use Unity and work on shaders only but I dont like Unity so if you dont have time you could pick Unity and try to implement the techniques."
GraphicsProgramming,2ynwre,arkenthera,1 point,Wed Mar 11 16:15:43 2015 UTC,"Well, for DirectX there's DXUT, which supplies a GLUT-style framework for initialization and update loop. Not sure I'd recommend using it though. If you can write code to create a window, initialize DirectX, and run an infinite while loop with a break statement, you can do real-time rendering. Sounds like you got that covered already anyhow.  The DirectX tool kit (https://directxtk.codeplex.com/) has a pretty handy set of utilties for DirectX programming. You can choose what subset of the tools you'd like to use, which you can read about in the link. I mostly just use the math and texture loading capabilities, as those suck to implement yourself. It also has the capability of loading some default meshes (such as the teapot) so you can get something rendering quickly.  I think the main thing to avoid writing yourself that may deter your interest would be any sort of data loading. You're not writing a graphics system, so hard-coding shader loading and object rendering is fine. Focus on the interest of learning more about the graphics pipeline and shader algorithms.  Let me know if you have anymore questions."
GraphicsProgramming,2yntm2,codelearning,10,Wed Mar 11 09:21:40 2015 UTC,"http://www.amazon.com/Real-Time-Rendering-Third-Edition-Akenine-Moller/dp/1568814240  This book is really, really good."
GraphicsProgramming,2yntm2,PvtPanda,3,Wed Mar 11 15:33:38 2015 UTC,Would this be a good starting point for a sophomore CS student with zero experience with rendering?
GraphicsProgramming,2yntm2,Rileyswims,2,Thu Mar 12 02:35:54 2015 UTC,"Yes. I used it with very little previous knowledge last year (3rd year CS student).  It can be a bit heavy at times but if you have a hard time with a concept you can always turn to google for a different perspective.  I recommend reading a sample of the book (should be a few out there) before buying it, to see if you feel you can learn from it."
GraphicsProgramming,2yntm2,PvtPanda,4,Thu Mar 12 08:06:48 2015 UTC,"Not sure if these are good, but it's a place to start. http://conceptf1.blogspot.sg/2013/11/list-of-freely-available-programming.html"
GraphicsProgramming,2yntm2,Zelarius,3,Wed Mar 11 09:47:48 2015 UTC,"That's a small explanation for shaders I wrote some weeks ago:   http://www.reddit.com/r/GraphicsProgramming/comments/2w8xhc/selfmade_glsl_shader_part_ii_technical/  Every bit helps, I hope. I learned it best myself with try & error though."
GraphicsProgramming,2yntm2,StickiStickman,2,Wed Mar 11 13:46:08 2015 UTC,"It might be worth thinking about whether you're more interested in real time graphics on GPUs (i.e., more game like) or photorealistic offline rendering (i.e., more movie like).  There's a lot of overlap between both, but if you can tell us what piques your interest we may be able to steer you a little better."
GraphicsProgramming,2yntm2,Boojum,3,Wed Mar 11 09:30:53 2015 UTC,"Not the OP, but could you recommend a good book for real time graphics? (Preferably in C++)"
GraphicsProgramming,2yntm2,CodeCodeCodeDurrr,1 point,Wed Mar 11 09:39:32 2015 UTC,that's exactly what I'm looking for.
GraphicsProgramming,2yntm2,ultrapingu,3,Wed Mar 11 09:42:06 2015 UTC,"Try Frank Luna.  I learnt DX9.c years ago with his book, it's really good at explaining the concepts behind what your doing: http://www.amazon.co.uk/Introduction-Game-Programming-DirectX-9-0c/dp/1598220160/ref=sr_1_2?ie=UTF8&qid=1426068873&sr=8-2&keywords=frank+luna  There's a DX10 and DX11 book too, but I've never read them."
GraphicsProgramming,2yntm2,RichieSams,1 point,Wed Mar 11 10:15:35 2015 UTC,"I highly recommend Frank Luna as well. His DX11 book is excellent. Some of the DirectX specific code is a bit outdated now, (for example fx files aren't a thing anymore and D3DX was killed off), but the graphics theory is still very good."
GraphicsProgramming,2yntm2,Mobile_Version,1 point,Wed Mar 11 15:53:35 2015 UTC,Here is the mobile version of your link
GraphicsProgramming,2yntm2,joebaf,1 point,Wed Mar 11 15:53:39 2015 UTC,"Hello,   Thanks for your answer, i'm interested in GPU graphics programming. I'm currently working in a company that does real time medical imaging, and I feel like I should read extra material to understand better what I do."
GraphicsProgramming,2yitxu,RichieSams,4,Tue Mar 10 03:55:56 2015 UTC,"Just from a first glance at the image, it looks like it may be correct but simply using a high FOV.  That looks like pretty standard perspective distortion at the corners to me.  See here for comparison.  EDIT: Yep, CalculateRayDirectionFromPixel(), SetProjection(), and GetDeviceCamera() look pretty reasonable to me.  I trust you're passing in the fov in radians?"
GraphicsProgramming,2yitxu,Boojum,1 point,Tue Mar 10 05:29:25 2015 UTC,"Yea, looking at it again, I realized that. I'm using a FOV_X of PI/2 aka 90 degrees. Is there something more standard?"
GraphicsProgramming,2yitxu,Boojum,2,Tue Mar 10 11:44:37 2015 UTC,"It's really a matter of taste and there's no standard, but 90 degrees is about the upper limit of what I'd use.  Much higher than that and you get the weird effect where there's this tiny picture of your scene in the center of the screen and the periphery is enormous and seems to radiate out from the center.  You can also lower the FOV a good bit and dolly out the camera if you want to flatten the perspective.  Of course, if you go too far with this it begins to resemble an orthographic projection.  But generally I generally like to start with 60 degrees as a compromise, and then adjust from there."
GraphicsProgramming,2yitxu,gidoca,2,Tue Mar 10 17:19:20 2015 UTC,"A standard lens (50mm on a full frame camera) has a horizontal FOV of about 40 degrees. The 90 degrees you are using corresponds to a focal length of 18mm on full frame, which is considered ultra-wide angle."
GraphicsProgramming,2yitxu,Madsy9,1 point,Tue Mar 10 17:19:35 2015 UTC,"If you think zoom or field-of-view is icky to work with, you could always calculate the values based on the lens focal length."
GraphicsProgramming,2yitxu,PrototypeNM1,3,Tue Mar 10 14:23:51 2015 UTC,"This is normal, correct distortion. You presumably are rendering to a viewing plane with a high FOV, pixels further from the center begin to cover increasing distant points in space as the geometry they sample is further from the viewing plane. I tried to recall the name of this effect (maybe along the lines of projection distortion) but was drawing a blank from Google."
GraphicsProgramming,2yitxu,Tomscribe,1 point,Tue Mar 10 05:40:41 2015 UTC,foreshortening?
GraphicsProgramming,2yitxu,PrototypeNM1,1 point,Tue Mar 10 09:24:38 2015 UTC,"No, there's another term but I'm still drawing a blank."
GraphicsProgramming,2yitxu,Madsy9,1 point,Tue Mar 10 16:57:17 2015 UTC,"Yea, now that I look at my other demo, I can see the distortion there as well. http://i.imgur.com/BSMXmi7.png However, it's less noticable because the shading gives your eye the impression of a sphere still.  Thank you!"
GraphicsProgramming,2yf2v2,Pyrolistical,5,Mon Mar 9 06:41:59 2015 UTC,It seems you don't like little squares or boxes. Of course a texel isn't a square and a voxel isn't a box.  That doesn't mean it's not a valid stylistic choice of representation that appeals to many people.
GraphicsProgramming,2yf2v2,pyalot,0,Tue Mar 10 09:31:57 2015 UTC,"You clearly didn't read the paper. The author of the paper didn't claim that the resulting representation from using a box filter (square pixels) isn't useful, but that people need to learn that it is not the only way to reconstruct quantized signals. Thinking that pixels have a square shape and that's it, is a gross oversimplification of digital signal processing. Quantized samples is only half of the story; filters and reconstruction is the second half."
GraphicsProgramming,2yf2v2,Madsy9,-1,Tue Mar 10 15:18:35 2015 UTC,"Clearly you didn't read my comment. Go back, yes, just a bit more, scroll up, read it again."
GraphicsProgramming,2yf2v2,pyalot,5,Tue Mar 10 15:45:18 2015 UTC,You should say that you are linking to a PDF.
GraphicsProgramming,2yf2v2,vossman77,3,Mon Mar 9 14:22:47 2015 UTC,Not sure why you are down voted.  Is common practice to put [PDF] or something in your title.  I was very confused clicking this link on mobile
GraphicsProgramming,2yf2v2,matticusrex,1 point,Tue Mar 10 04:03:09 2015 UTC,"Eh, I can agree simply because pixels can be interpreted from their source image in many ways. Look at Daniel Rozin's interactive mirrors for example."
GraphicsProgramming,2yf2v2,ccricers,0,Wed Mar 11 20:16:25 2015 UTC,"well, that's all very nice.    except that a pixel is a square.  EDIT: to clarify - pixels mean ""little squares"" to the vast majority of people.  it is impossible to change that meaning now, and silly to complain about it.  and that wasn't it even the actual intent of the article; it was a humorous ploy used by the author to get people's attention to the issue of proper handling of samples."
GraphicsProgramming,2yf2v2,qarl,6,Mon Mar 9 13:39:04 2015 UTC,"Did you actually read the paper? That is most people's reaction, but if you treat a pixel like that your image will alias.  A pixel is not a square, it is sample of the frequency clamped signal beneath it."
GraphicsProgramming,2yf2v2,__Cyber_Dildonics__,3,Mon Mar 9 14:18:29 2015 UTC,"The point is that while, from a sampling theory point of view, the little square model may not be the most useful model, it is how a pixel value is captured by cameras and displayed by screens."
GraphicsProgramming,2yf2v2,gidoca,6,Mon Mar 9 14:59:02 2015 UTC,"First, if you look at a monitor up close, maybe the pixels are close to squares.  Most screens you look at are actually not.  They use various arrays of red green blue (and even yellow) tiles of light.  Digital cameras don't capture pixels like this either, they have various patterns of sensors, usually with far more green than blue or red pixels, and they use a bayer filter to reconstruct the image.    People think pixels are squares because when you look at a zoomed image with impulse filtering, you get squares.  This is a visualization of pixels, not how they are in screens and cameras, and not how they are treated from a reconstruction point of view.  Did you read the paper?"
GraphicsProgramming,2yf2v2,__Cyber_Dildonics__,-3,Mon Mar 9 15:22:35 2015 UTC,"yes, i know.  that's all well and good.  but a pixel is a square."
GraphicsProgramming,2yf2v2,qarl,6,Mon Mar 9 15:01:49 2015 UTC,"So you downvoted me, didn't read the paper obviously, and are clinging to an ignorant stance (which I held myself until I read the paper).  Alvy Ray Smith wrote this because people at Microsoft Research held onto the same idea, and go the same results.  Why do you think there are different filters in rendering programs?  If you try a box filter with a diameter of 1 pixel, the results will almost certainly alias.    Have you written image reconstruction programs? Have you written renderers?   Seriously, read the paper and then come back and repeat the same misguided information."
GraphicsProgramming,2yf2v2,__Cyber_Dildonics__,3,Mon Mar 9 15:17:41 2015 UTC,"Have you considered that is might be because image IO is in squares? you write to the GPU as squares and you read pngs as squares. I know you'll claim they aren't, but without knowing the display technology or the exact continous->discrete mapping of the input image, squares are what you are talking about. Sure, with re-sampling, you probably are going to use gaussian or bicubic, but that is well established..."
GraphicsProgramming,2yf2v2,squashed_fly_biscuit,1 point,Mon Mar 9 15:37:54 2015 UTC,"The thing is, the situation you are describing are arrays of values, easily visualized and conceptualized by squares.  I think in the same way.  But when it comes down to anything with an integral, be it down sampling, up sampling, or displaying, you have to treat pixels as values at evenly spaced points in an integral.  That is why it is well established that gaussian and bicubic filters are used.    Most of the time people don't run into trouble with this model, especially because they just know to use better filters than 'box' from what they have read.  This explains why that is, and if you have a  camera or display that isn't perfect squares of color (I don't know of any that are, lcds have rectangles of red green and blue) you run into trouble if treating pixels conceptually as squares.    Also any kind of analytical rasterization will have unnecessary artifacts.   So, arrays of even spaced values != squares."
GraphicsProgramming,2yf2v2,__Cyber_Dildonics__,2,Mon Mar 9 15:47:20 2015 UTC,"You can represent pixels as points but they aren't points. They are squares or some arrangement of shapes/colors depending on the kind of monitor. This whole thing is just semantics to me. Just look at display technology, just because the theory and concepts are easier with points doesn't mean they are points... In the end little squares is a more physically accurate statement, in my opinion, even if it is easier to think of them as points."
GraphicsProgramming,2yf2v2,Reddit1990,2,Mon Mar 9 18:21:38 2015 UTC,"They are neither points nor squares/rectangles. They are samples of a continuous signal which has been quantized. The paper in question demonstrates this very well.  Other signals than graphics/light are quantized and used all the time, and no one describe those as ""squares"". Raw audio data for example. Why not? When reconstructing audio signals for playback, the box filter is the worst one you can use for playback. Even linear interpolation between samples is better.  The idea here is that the final representation / visualization of discrete samples is separate from the samples themselves. And the original signal is not represented exactly by the samples alone."
GraphicsProgramming,2yf2v2,Madsy9,2,Tue Mar 10 15:08:14 2015 UTC,"The idea here is that the final representation / visualization of discrete samples is separate from the samples themselves.   Yes, but the final representation is square shaped pixels on your screen... I mean nowadays these pixels can be different shapes but thats aside the point. Theres a good reason people consider them little squares... thats because they are. My screen isn't 1920x1080 pixel points of lights. They aren't points of light. They are pixels that take a certain shape which is rectangular/square."
GraphicsProgramming,2yf2v2,Reddit1990,3,Tue Mar 10 15:57:48 2015 UTC,"The paper isn't talking about monitors.  It's talking about pixels in an image.  squashed_fly_biscuit was talking about pixels in an image when he said ""you write to the GPU as squares and you read pngs as squares.""  But, if he could point to the squares inside a GPU or a PNG, I would be very impressed.  There are no squares.  There are only numbers that represent discreet samples of a continuous signal.  Step 1)  Read a 256x256 r8g8b8a8 PNG file into main memory.  The PNG was created from a downsized selfie photo.  Step 2) Decide I want to display the photograph sized to fill the full height of my 1920x1080 monitor while being rotated 45 degrees.  Step 3) Ask Reddit1990 or squashed_fly_biscuit what shape each of the 256x256 colors of my rotated photograph should be when displayed stretched and rotated on my monitor.  If you say hard-edged diamonds ♦♦♦♦ then you are implying that I have the face of a Minecraft character.  I don't appreciate that!  :p  Even then, you are still misrepresenting my blocky face because the camera was not guaranteed to be perfectly aligned with my face-cubes.  A digital image is a array of numbers that represents something.  It doesn't represent a grid of squares!  In my example, it represents a view of a scene of me standing in front of a camera.  The scene formed a continuous signal.  The camera sampled that signal into a discreet array.  I mentioned that the array had been resampled to a 256x256, but it still represents the same scene.  And, when it is resampled yet again to be rotated and stretched across my screen, it still represents a sampled view of my face --not a grid of squares.  When you rasterize an image on screen, you are resampling the image to the constraints of the monitor.  How you do that depends on what you are trying to display.  If you are trying to display a grid of colors because you are doing pixel-at-a-time image-editing, then square filter might actually be appropriate!  But, if you are trying to display my face, then a Gaussian or Lancoz filter is a better estimation of the continuous scene that the discreet array of numbers represents."
GraphicsProgramming,2yf2v2,corysama,-4,Tue Mar 10 21:05:08 2015 UTC,"actually, i upvoted you.  and i read the article.  and i knew the article's contents before reading it.  i have indeed written renderers.  you need to calm down."
GraphicsProgramming,2yf2v2,qarl,0,Mon Mar 9 15:18:54 2015 UTC,So why do you think a pixel is a square?  That part you haven't explained.
GraphicsProgramming,2yf2v2,__Cyber_Dildonics__,-2,Mon Mar 9 15:27:00 2015 UTC,"for the same reason that ""ain't"" is a word.  common usage trumps esoteric usage."
GraphicsProgramming,2yf2v2,qarl,4,Mon Mar 9 15:27:35 2015 UTC,Common usage trumps esoteric usage unless you're in a highly technical area where precise language is needed.  After all where would we be if pi=3?
GraphicsProgramming,2yf2v2,Delwin,-1,Mon Mar 9 17:12:55 2015 UTC,"in case you hadn't noticed, grandmas are now using the word ""pixel"".  this whole discussion is way past ridiculous.  let's use the word ""sample"" for what you want and the word ""pixel"" for the squares.  like is already done by everyone everywhere."
GraphicsProgramming,2yf2v2,qarl,3,Mon Mar 9 17:25:38 2015 UTC,"Except that 'sample' already has a definition and it's diferent than 'pixel'.  Pixel is in fact what it says it is - an element of a picture (pix - el).  Similarially a voxel is an element of a volume.  These are formal definitions and not subject to change.  What is going on however is that people are making assumptions about how a pixel is generated.  You are correct in that it is a sample of a dataset, and nothing in the definition says it is square.  In the simulation world we use the word 'detector' for 'the thing that samples the world to generate the information needed to create a pixel'.  That seems to be what you're aiming at.  All of this however is quite moot as a pixel is not, and honestly has never been, square.  You may know of displays where a pixel is square but I don't.  Likewise using pixel as a detector element is flat out wrong.  A camera doesn't have pixels on the CCD it has detectors.  Same problem."
GraphicsProgramming,2yf2v2,Delwin,3,Mon Mar 9 17:46:25 2015 UTC,"So instead of educating people, or even just letting people educate themselves in an area of discussion for graphics programming, you've chosen to contradict a primal and fundamental part of computer graphics with misinformation that you find convenient?  For every problem there is a solution that is simple, easy, and wrong. You could be better than that."
GraphicsProgramming,2yf2v2,__Cyber_Dildonics__,3,Mon Mar 9 15:38:43 2015 UTC,"Your disagreement isn't about the nature of graphics programming. It's about the nature of language, and about the ""real"" meaning of a word.  You seem to think that the real meaning of the word ""pixel"" is based on specialized use by graphics programmers -- that the definition that would lead to the most faithful representation of the math involved is the right one.   But most people don't use it that way. You say ""pixel,"" and they hear ""a little square."" This interpretation is almost universal. Every sentence that has the word ""pixel"" in it that you interpret according to the ""right"" definition will diverge from the speaker's intended meaning. Every sentence you assemble using the ""right"" definition of ""pixel"" will be misunderstood. This is poor use of language.  The solution is not to ""educate"" the public about the proper use of the word. This is incredibly difficult to pull off, and you don't really win anything. The solution is to give up on the specialized definition of ""pixel."" You can easily find another word to use in its place."
GraphicsProgramming,2yf2v2,redxaxder,0,Mon Mar 9 16:09:40 2015 UTC,"This isn't the public, it is a graphics programming forum.  If it was a general discussion I wouldn't care for the reasons you are outlining.  If a person of the general public actually knows what a pixel is in any form I think that's great."
GraphicsProgramming,2yf2v2,__Cyber_Dildonics__,-5,Mon Mar 9 16:15:45 2015 UTC,it's not misinformation.  a pixel is a square.  goodbye now.
GraphicsProgramming,2yf2v2,qarl,0,Mon Mar 9 15:52:00 2015 UTC,It is the very definition of misinformation.
GraphicsProgramming,2yf2v2,__Cyber_Dildonics__,2,Mon Mar 9 16:16:24 2015 UTC,"Yeah, a pixel is not a point. It has dimension that can be measured."
GraphicsProgramming,2yf2v2,qubedView,2,Mon Mar 9 15:41:50 2015 UTC,These days yes. In the past we weren't always so lucky.
GraphicsProgramming,2yf2v2,neutronium,11,Mon Mar 9 15:11:24 2015 UTC,"He's talking about TEXELS, people!  I'm not sure if the term 'texel' was even around back when this paper was written.  Regardless, Alvy Ray Smith is not a fool.  He's not talking about monitors with non-square aspect ratios, blurry screens or rgb triads vs pentiles.  He's talking about sampling theory.  If any of you don't understand why texels are not little squares, then you are exactly the target audience for this paper.  Go back and read it again.  For real this time."
GraphicsProgramming,2ydfo1,ryman2012,3,Sun Mar 8 21:16:38 2015 UTC,Give this a shot: http://www.realtimerendering.com/udacity/transforms.html  It has helped me tremendously with issues like these before.
GraphicsProgramming,2ydfo1,jringstad,1 point,Sun Mar 8 22:43:18 2015 UTC,my god that's amazing
GraphicsProgramming,2ydfo1,frenris,1 point,Fri Mar 13 06:05:14 2015 UTC,"What happens if you change your up coordinates to (0, 1, 0, 0)?"
GraphicsProgramming,2ydfo1,qartar,1 point,Sun Mar 8 21:41:41 2015 UTC,"Nothing happens, I changed the eye to be at z=50 and the near plane to be at 60, and it looks a lot more real!  But when I move the camera to x=5, the game clips off the right side.  Also, the tiles look like they are transparent slightly? Would you happen to know what that is from?  http://imgur.com/cD0WArE"
GraphicsProgramming,2ydfo1,kampasta,1 point,Sun Mar 8 22:01:47 2015 UTC,"Also, how do I make it so the pieces don't all stretch towards the camera? It doesn't look very realistic like that, the pieces should face their direction in the world, not the camera."
GraphicsProgramming,2ydfo1,phort99,1 point,Sun Mar 8 22:20:34 2015 UTC,"it's a feature  but honestly I don't know.  Are you doing fixed function or are you using shaders? Try transforming your cubes to world space on the cpu, and seeing what all the vertex coordinates are, just to make sure it's really your camera transforms, and not just a simple goof somewhere with your world transformation (which wouldn't show up in orthographic projection)  If all the vertices look to be where they should be (there are only 8 of them, so it shouldn't be a huge deal to check each one by hand) then it probably is your camera like you suspect."
GraphicsProgramming,2yah3k,ilmale,1 point,Sun Mar 8 00:20:02 2015 UTC,Presentation slides here: https://www.khronos.org/developers/library/2015-gdc
GraphicsProgramming,2yah3k,LordTocs,1 point,Sun Mar 8 00:20:39 2015 UTC,Are they actually there? I clicked on all of them and they don't seem to have the slides from the video.
GraphicsProgramming,2yah3k,LordTocs,1 point,Sun Mar 8 06:26:53 2015 UTC,Argh.. I trusted the youtube description too much. (Note: Never trust youtube anymore)
GraphicsProgramming,2yah3k,phire,1 point,Sun Mar 8 11:28:06 2015 UTC,Well I hope someone has a copy somewhere. I got locked out of the GDC talk on Vulkan.
GraphicsProgramming,2y9w93,jrkirby,16,Sat Mar 7 21:07:49 2015 UTC,"There's a bunch of things that ""Zero Driver Overhead"" could mean:  If you take all the CPU code that you put in the driver and put it in your program instead, then it won't be any faster, but it will be more controllable by you. Driver writers have to optimize for every single program, while you only have optimize for yours. If the code is in your program instead of in an opaque driver with no symbols, you have a chance of optimizing the things that your program takes advantage of. (But if you do a bad job, you can end up with the same or worse performance, which I think is what you're talking about.)  Another possible meaning is that driver overhead arises from an API that doesn't map closely to the hardware, meaning that the driver has to do a bunch of work to translate your API calls to something that the hardware understands. You can reduce the total amount of CPU work just by changing the API to be a more natural fit for the hardware (although you also have to make the appropriate changes in your program to match the new API). The total complexity is reduced, not migrated from driver to program.  And finally, there's the AZDO OpenGL model, which is largely about stripping out the driver APIs totally. Persistent mapped buffers let you write to memory without any driver code getting in the way: ""zero"" overhead instead of ""minimal"" overhead."
GraphicsProgramming,2y9w93,nexuapex,7,Sat Mar 7 21:31:06 2015 UTC,"In OpenGL for instance there's a whole bunch of state validation going on in the driver that could be avoided. ""AZDO"" techniques mostly try to amortize the cost of validation over a large number of frames and/or render ""calls"" in an effort to reduce time spent on the CPU and increase GPU utilization by preventing starvation.  Vulkan seems to ditch validation all together (it's an option now), which directly decreases time spent on the CPU. In addition, Vulkan allows multiple threads to create render state independently of one another, making it much easier to scale.  At least that is my understanding of it. Obviously I haven't been able to experiment with Vulkan yet!"
GraphicsProgramming,2y9w93,Lord_Naikon,1 point,Sat Mar 7 21:45:58 2015 UTC,"I would point out that Khronos has been very clear that Vulkan is ""stateless."" While you will have state in your engine, the driver is agnostic to what that is."
GraphicsProgramming,2y9w93,BillDStrong,4,Fri Mar 13 08:25:44 2015 UTC,"The key difference here is that shifting it into the client code allows you to optimize when it's performed - which typically means you're not doing it in the hot path.  There are still going to be costly operations such as allocating memory or validating pipeline state. What these styles of APIs do, however, is require that it be performed in advance and allow you to do all that work either asychronously or before you ever start rendering.   The end result is that actually submitting draw commands is much faster and involves less driver overhead to perform because it doesn't have to, say, re-validate the pipeline state every time you switch shaders while issuing draw commands. This, in turn, means that it's easier to saturate the GPU and minimize pipeline bubbles or stalling caused by the drivers.   As you kind of elude to - yeah, it also means that it's possible to do things less efficiently. Significantly so if you aren't careful. And for many things, it's probably not going to be any faster to do it using Vulkan or AZDO techniques. And some things simply won't be a good fit for these kinds of APIs.  But for the people who need this kind of fine-grained control over what the GPU is doing, this is a HUGE thing."
GraphicsProgramming,2y9w93,lgroeni,4,Sun Mar 8 00:44:43 2015 UTC,http://gdcvault.com/play/1020791/
GraphicsProgramming,2y9w93,romcgb,3,Sat Mar 7 21:26:45 2015 UTC,"I was initally going to say 'no' but having thought about it maybe you are right to a degree. Obvioulsy ""zero"" driver overhead is balls cos there has to be some. Yes you are doing in client code what the driver was previously doing. But it's now your responsibilty as to how to go about it whereas before the driver was a blackbox doing it's thing outside of your control."
GraphicsProgramming,2y9w93,specialpatrol,3,Sat Mar 7 21:26:04 2015 UTC,"I think you misunderstand what is happening. In old school openGL you would do a C function call for every vertex you wanted to draw.  Graphics cards got faster and eventually they were able to draw a triangle so fast that just calling 9 functions was too slow and caused too much latency.    The evolution of this is that vertex buffers were put to the card so that one function call in C could draw tens of thousands of polygons.    Now we are at the point where even function calls that do lots of work like this are not enough, with on major reason being that in openGL they must all go in one thread - the thread that created the openGL context.    What these new APIs do is batch lots of calls, so one function in C can do lots of draw calls.  This is what is meant by 0 driver overhead.  Nothing is actually zero overhead, but APIs like this reduce the cpu load to something very small."
GraphicsProgramming,2y9w93,__Cyber_Dildonics__,2,Sun Mar 8 01:50:53 2015 UTC,"So your program does the same thing, and doesn't even run much faster, it just spends more time in client code, and less time in the driver.   Even if it were exactly the same work done in a different place there are still a number of advantages.  It performs more predictably across driver revisions, venders, and platforms.  It's able to be optimized in the context of your game rather than being a black box giving you no control.  It's not encumbered by handling for features from three API revisions ago that you don't use or care about.  But aside from this, the AZDO techniques are more about handling graphics data in the way the card wishes to consume it rather than in the way an API designed 30 years ago wants you to present it.   Removing as much of this indirection as possible is a very good thing."
GraphicsProgramming,2y9w93,bobstevens,1 point,Sun Mar 8 05:20:56 2015 UTC,"The biggest problem that the big game engine developers have been complaining about has been the fact they were already doing those thing anyway. After all, most of these engines were also targeting consoles, and they didn't have DX or OpenGL, so the engines already did all that work.   And, not all of the overhead needs to be there. For example, in OpenGL, all drivers are effectively in developer mode, with costly error checking going on in most of the draw calls, and all of the compute units.  By having a layer of correctness testing that gets turned off in production, this cuts out a lot of overhead from the driver.   My speculation is this is the where part of that first big speed boost comes from, even on unoptimized drivers."
GraphicsProgramming,2y65uc,RichieSams,1 point,Fri Mar 6 20:23:27 2015 UTC,"I'm really enjoying this series so far, and looking forward to seeing where you take it.  One observation I would make is that your random number generation is clumpy as hell. That's going to cause you all sorts of noise issues further down the line. I would strongly advise you to look into QMC to try and get a better distribution."
GraphicsProgramming,2y65uc,pixel_fcker,1 point,Fri Mar 6 22:37:35 2015 UTC,curand has a sobol implementation. I'll look into it. Thanks for the suggestion.
GraphicsProgramming,2y65uc,snakepants,1 point,Fri Mar 6 22:52:16 2015 UTC,Nice tutorials! My only comment looking at the source is that the DX interop code obfuscates what you're trying to show a bit. For tutorial purposes it might be simpler to copy back to CPU and just use SetDIBitsToDevice or something. Usually path tracers aren't real-time right? Or maybe even use a small GLUT program with fixed-function drawing and GL-interop since that's a lot less code.
GraphicsProgramming,2y65uc,snakepants,1 point,Fri Mar 6 23:26:13 2015 UTC,"I re-arranged / renamed / documented the draw code to be more clear where the action is happening. There is still more work to be done, but I think it's a lot better than before. Please let me know what you think: https://github.com/RichieSams/rapt/blob/master/source/basic_path_tracer/basic_path_tracer.draw.cpp"
GraphicsProgramming,2y65uc,pyalot,1 point,Tue Mar 10 21:07:16 2015 UTC,It's much improved! Although I still think you should consider if your goal is to do a Cuda-DX interop tutorial or a path tracing tutorial. I guess just imagine how much less code you would have if your app just wrote a .pfm or something and if that would get in the way of demonstrating the path tracing concepts.
GraphicsProgramming,2xycou,jrkirby,8,Wed Mar 4 21:59:46 2015 UTC,This might be what you're looking for:  https://fgiesen.wordpress.com/2011/07/04/a-trip-through-the-graphics-pipeline-2011-part-4/  That whole series (there are 13 parts!) is great.
GraphicsProgramming,2xycou,VilHarvey,3,Wed Mar 4 22:42:06 2015 UTC,"He didn't seem to mention swizzling patterns?  I heard a rumor that GPUs don't use the usual ""y * stride + x"" for laying out a texture in memory, they follow a space-filling curve which is supposed to give better locality for the caching."
GraphicsProgramming,2xycou,ChainedProfessional,4,Thu Mar 5 01:26:20 2015 UTC,"That's exactly right. I came across this for the first time with Sega Dreamcast (PowerVR GPU) textures. It can be called ""twiddling"", ""swizzling"", ""Morton order"", or more... Here's a related Wikipedia article for one specific pattern."
GraphicsProgramming,2xycou,rokama,3,Thu Mar 5 01:36:52 2015 UTC,"Z-order curve:       In mathematical analysis and computer science, Z-order, Morton order, or Morton code is a function which maps multidimensional data to one dimension while preserving locality of the data points. It was introduced in 1966 by G. M. Morton.  The z-value of a point in multidimensions is simply calculated by interleaving the binary representations of its coordinate values. Once the data are sorted into this ordering, any one-dimensional data structure can be used such as binary search trees, B-trees, skip lists or (with low significant bits truncated) hash tables. The resulting ordering can equivalently be described as the order one would get from a depth-first traversal of a quadtree.    Image i - Four iterations of the Z-order curve.     Interesting: Z-order | Z curve | Interleave sequence   Parent commenter can toggle NSFW or delete. Will also delete on comment score of -1 or less. | FAQs | Mods | Magic Words"
GraphicsProgramming,2xycou,autowikibot,2,Thu Mar 5 01:39:17 2015 UTC,"Oh TIL Dreamcast used PowerVR.  I wish they would open the PVRTC format, someone said it's patented so you can't reverse-engineer a FOSS compressor. Not legally, anyways."
GraphicsProgramming,2xycou,ChainedProfessional,1 point,Thu Mar 5 01:53:35 2015 UTC,"Well, the formats used by the Dreamcast at least are quite well documented and understood. There was even a Photoshop plugin leaked that could read and write PVR textures, if I remember correctly."
GraphicsProgramming,2xycou,rokama,1 point,Thu Mar 5 01:59:24 2015 UTC,Thanks
GraphicsProgramming,2xycou,staticfloat,1 point,Wed Mar 4 23:29:30 2015 UTC,"Yeah, I'm on part 8 right now, been chewing on these for the last few days and they are incredible.  Super, super helpful."
GraphicsProgramming,2xycou,Madsy9,4,Thu Mar 5 00:54:57 2015 UTC,"I can't give you any concrete hardware details, but I know of some general concepts that are implemented in hardware to make texture lookups faster.   Mipmapping. By using 33% more storage space, you get better cache locality by looking up in smaller versions of a texture, called mipmaps. The mipmap level is calculated based on the relation between the area of the triangle in texture space and the area of the triangle in screen space. When looking up a smaller mipmap, texels will be closer together compared to if you looked up the largest mipmap. Texture swizzling. Another way to optimize for cache usage by changing the access pattern. There are many different swizzle patterns. Some optimize for changing depth, while others pack rows into tiles of a certain size. Texture compression. Formats like S3TC (DXT1 through DXT5) use less memory and leads to less bandwidth usage. General optimizations and hacks: IHVs do all sorts of other tricks to avoid unneccessary driver work as well. Sometimes within the bounds of the variance rules in the OpenGL specification, and sometimes they don't give a crap. For example, they might skip trilinear filtering or anisotropic filtering on primitives where anything more than bilinear filtering wouldn't lead to a noticable increase in quality, or ""cheat"" in their anisotropic filtering implementation by just grabbing more samples rather than doing the texel lookup by using a true trapezoid. And like always, some very common driver paths are more optimized than less common ones."
GraphicsProgramming,2xycou,sepht,2,Thu Mar 5 03:02:17 2015 UTC,"An idea of temporal addressing can be seen in ""rasterization patterns""  See here for a comparison of AMD+Intel+Nvidia locality in image space. Or here for a video of vertex locality on a GTX 500 series"
GraphicsProgramming,2xx7yr,thomac,2,Wed Mar 4 16:53:45 2015 UTC,"It's been a while since I've touched OpenGL, but isn't there a way to use the standard vertex group bindings (e.g. vertex fans and quads) in a 2d context? Then you would just push the vertex buffer to the GPU like you normally would."
GraphicsProgramming,2xx7yr,curtmack,3,Thu Mar 5 00:31:43 2015 UTC,"2d is easy, just setup an orthographic projection. You can also setup vertex shaders so you only need to provide x and y coordinates.  But accelerated rendering of vector paths (Bezier curves) is still an open question.    Do you convert the paths to triangles on the cpu?  Or render one triangle and calculate the curve completely in the fragment shader?  Or combine the techniques, fitting triangles roughly to the path on the cpu and doing the per-pixel stuff in the fragment shader? Maybe you can get a geometry shader to generate triangles for you? Or something else I haven't thought of? Is there even one technique which is best for all videocards?   I don't know the answers to these questions, currently I just ask OpenVG to render the paths for me."
GraphicsProgramming,2xx7yr,phire,1 point,Thu Mar 5 01:53:23 2015 UTC,"If being locked to Windows isn't a concern for you, you can always try Direct2D."
GraphicsProgramming,2xvhh5,spottedsocks,10,Wed Mar 4 04:45:30 2015 UTC,"It's been a while (18 months perhaps) since I read a graphics paper, but I really enjoyed  «Triangle Scan Conversion using 2D Homogeneous Coordinates» by Marc Olano and Trey Greer  It shows a way to do perspective-correct interpolate scalars across a triangle which is fairly cheap and works without interpolating along the edges. It only depends on the absolute screen-space X and Y components and a constant (*). Which means it's perfect for tile rasterizers. Since the main computation involves constructing a 3x3 matrix per triangle and inverting it, it is fairly easy to parallelize the work and make it use SIMD.  (*) The paper does the interpolation with normalized space coordinates, but as that gave me precision problems, I did it in screen space instead. The coordinate system doesn't really matter much in principle."
GraphicsProgramming,2xvhh5,Madsy9,8,Wed Mar 4 09:31:14 2015 UTC,A Probabilistic Model of Component-Based Shape Synthesis - Synthesis of a large variety of models from a small set of examples. Really cool for content generation.
GraphicsProgramming,2xvhh5,arkabay,1 point,Wed Mar 4 06:35:40 2015 UTC,This is really cool.  thx for the link.
GraphicsProgramming,2xvhh5,JJJams,7,Thu Mar 5 03:27:58 2015 UTC,"Sadly, i haven't read anything recently,  but Cyril Crassin's thesis is one of the coolest paper i have read.  GigaVoxels: A Voxel-Based Rendering Pipeline For Efficient Exploration Of Large And Detailed Scenes (2011)"
GraphicsProgramming,2xvhh5,romcgb,2,Wed Mar 4 08:19:20 2015 UTC,"Yep, i always wanted to implement it myself but i guess i am to dump. The rendering of the mandelbulb is amazing.  http://www.icare3d.org/news_articles/gigabroccoli_the_mandelbulb_into_gigavoxels.html"
GraphicsProgramming,2xvhh5,Gorebutcher666,5,Wed Mar 4 09:58:22 2015 UTC,"Unifying points, beams, and paths in volumetric light transport simulation is definitely a must read, although I have to admit I haven't fully read/understood it myself. They also have an implementation available on Github: smallupbp."
GraphicsProgramming,2xvhh5,TurkishSquirrel,4,Wed Mar 4 06:27:07 2015 UTC,"Haven't read them, but they seem really cool.   http://graphics.berkeley.edu/papers/Pfaff-ATC-2014-07/ – Ultra realistic tearing of various surfaces  http://www.cs.cornell.edu/Projects/Sound/modec/ – This is about compressing SOUND GENERATED FROM MODELS! I was astounded when I saw this was possible!"
GraphicsProgramming,2xvhh5,peterfsat,2,Wed Mar 4 12:00:05 2015 UTC,wow! correct sound for 3d model simulations... I was not even aware of such problem!
GraphicsProgramming,2xvhh5,joebaf,3,Wed Mar 4 15:16:59 2015 UTC,I really loved Bruneton's precomputed atmospheric scattering. If only I could understand it ;)
GraphicsProgramming,2xvhh5,Asgardur,2,Wed Mar 4 23:40:05 2015 UTC,This is just awesome. I'll definitely try to implement it in a skybox renderer.
GraphicsProgramming,2xvhh5,arkabay,2,Thu Mar 5 06:16:33 2015 UTC,"I recently had to implement this for college (not all of it) http://www.cg.tuwien.ac.at/research/publications/2007/bruckner-2007-STF/ it's quite old but hey the results are really good looking and it's cheap performance-wise compared to other alternatives.  Now something more recent I have read which I find quite cool is this https://www.cs.virginia.edu/~weimer/students/brady-mcs-paper.pdf ""Discovering New Analytic BRDFs with Genetic Programming"""
GraphicsProgramming,2xvhh5,josegv,2,Thu Mar 5 04:05:25 2015 UTC,Definitely Manifold Exploration. I really like the concept of the Newton-style manifold walks to solve for correct specular configurations. The generalized geometry factor is also super nice.
GraphicsProgramming,2xvhh5,thunabrain,1 point,Thu Mar 5 07:25:30 2015 UTC,In general Wenzel's papers usually are pretty high on my list each SIGGRAPH. They're very well and clearly written.
GraphicsProgramming,2xvhh5,betajippity,1 point,Thu Mar 12 15:00:16 2015 UTC,The stuff from Mundy et all on probabilistic volume representation:  http://vision.lems.brown.edu/project_desc/Probabilistic%20Volumetric%20Modeling
GraphicsProgramming,2xvhh5,turbod33,1 point,Wed Mar 4 12:30:39 2015 UTC,"Vertex Connection and Merging/Unified Path Space are two 2012 papers that arrive at the same rendering algorithm through two different mathematical frameworks. VCM/UPS essentially combines/subsumes bidirectional path tracing and progressive photon mapping.  Seeing that the two teams/papers arrived at the same conclusion at the same time from two very different perspectives is very cool, and on top of that, the resultant VCM algorithm is making a ton of impact in the rendering world (VCM is the core algorithm in Renderman 19/RIS and is also being adopted in Vray and Corona)."
GraphicsProgramming,2xrp6u,misdake,7,Tue Mar 3 08:33:00 2015 UTC,Ars Technica has additional information on it: http://arstechnica.com/gadgets/2015/03/khronos-unveils-vulkan-opengl-built-for-modern-systems/
GraphicsProgramming,2xrp6u,cant_lane,1 point,Tue Mar 3 14:51:12 2015 UTC,"As such, OpenGL (and Direct3D 11) will continue to have a developer audience, and Khronos intends to continue OpenGL development going forward to meet the needs of this audience.   I'm curious how they're going to effectively handle migrating developers across from OpenGL to Vulkan and how they split their resources between the development of these two APIs. Segmenting your community is generally not beneficial."
GraphicsProgramming,2xrp6u,robziehl,2,Tue Mar 3 22:41:53 2015 UTC,"Segmenting your community is generally not beneficial.   Nor is dealing with an ever growing gap of backwards compatibility provisions as modern hardware continues to diverge from the model GL was initially written to target. The community may become somewhat segmented, but both segments benefit: those on the new API don't waste time tripping over legacy, and this maintaining apps on the old API don't have the rug pulled out from under them as often by driver vendors breaking things as they implement the new hotness."
GraphicsProgramming,2xrp6u,noptastic,1 point,Wed Mar 4 03:03:35 2015 UTC,"Agreed, I'm all for keeping up with the times. I haven't used OpenGL extensively but I know my way around the legacy fixed pipeline style a bit as well as the modern style. The general impression I get from what I have read online is that OpenGL never did gradual API changes as well as they could have. I could be completely wrong on this but I have only ever dealt with 2.1 and 4.1 which is obviously a big leap in time so I never experienced the changes between minor versions."
GraphicsProgramming,2xrp6u,robziehl,2,Wed Mar 4 03:51:17 2015 UTC,Can't wait to see the API (and check the 2 tiny difference with DX12).
GraphicsProgramming,2xrp6u,ilmale,1 point,Tue Mar 3 14:44:45 2015 UTC,\o/
GraphicsProgramming,2xrp6u,gott_modus,1 point,Wed Mar 4 18:51:18 2015 UTC,"I heard both the Vulkan guys and DirectX12 guys talk at GDC this year.  I'm pretty excited about this technology. They're really opening up the API and giving developers a chance to manage, among other things, the concurrency involved in CPU/GPU processing.  This API is going to make it much easier for developers to take advantage of lots of cores. So if you have a 16 core machine, a good engine will actually be able to balance CPU/GPU with much less wait time, resulting in a better ability to spread out the work among processors.  In other words (and a bit dramatically) we may see games with tens of thousands of characters moving around on it, and the game stating that it requires 8 processors to run smoothly.  What a time to be alive."
GraphicsProgramming,2xrp6u,philipbuuck,0,Tue Mar 10 00:08:58 2015 UTC,"I am pretty new to OpenGL,  so I probably have some misconceptions. Looks like they are going the LLVM with Vulkan in that shaders will be compiled into an intermediate form which is compiled into bytecode. I guess this means that your favorite language, or even your favorite framework/engine, will be able to emit SPIR-V intermediate form instead of having to figure out a way to use OGSL or OpenGL C.  With OpenGL, I make calls to functions such as drawElements and bindTexture in my favorite programming language with OpenGL bindings. The articles seems to talk just about shaders. Will Vulkan make it so that calls such as these will be replaced by some native lanugage implementation that will emit the equivalent SPIR-V code for the equivalent call to ""drawElements"" or does SPIR-V only apply to shaders?"
GraphicsProgramming,2xrp6u,J_M_B,5,Tue Mar 3 20:50:33 2015 UTC,"SPIR-V is just for shaders (including compute shader-style shaders), the application still uses something like ""drawElements"" when it fills up the command queue to the GPU."
GraphicsProgramming,2xrxng,0ctobox,2,Tue Mar 3 10:52:46 2015 UTC,What are the rendering times for the various images you show on github?
GraphicsProgramming,2xrxng,fjolliton,3,Thu Mar 5 16:59:01 2015 UTC,"The 1080p images were rendered overnight so I can't give you an exact amount of time, but it was in the region of 6 to 17 hours depending on the polygon count and the how the scene was set up. For example, the Sponza scene by far took the longest because the whole mesh was in view and the rays bounce around a lot inside the building. The Stanford dragon, however, despite having a much higher poly count (~0.8 million tris) took the least amount of time because much less of it filled the camera view. Stanford Lucy (~28 million tris), surprisingly, took only slightly longer than the Stanford dragon despite having signifincatly more triangles..  I've been trying to improve performance and I've made a fair bit of progress but there's still plenty more to be done. I've been looking into using SSE/SIMD but that's very low level optmisation so I'm probably better off trying to reduce noise  and improve the efficiency of the algorithms first.  Any suggestions would certainly be appreciated."
GraphicsProgramming,2xrxng,FrenchHustler,1 point,Thu Mar 5 19:39:13 2015 UTC,"I see you wrote your own kd-tree... I remember when I wrote my own photon-tracer, the ANN library completely destroyed my own version of the KDTree. It'd be trivial to drop it in your path-tracer and might make huge improvements in terms of efficiency.  http://www.cs.umd.edu/~mount/ANN/"
GraphicsProgramming,2xrxng,NotcamelCase,1 point,Thu Mar 5 21:03:58 2015 UTC,Thanks! This looks really interesting.
GraphicsProgramming,2xrxng,sepht,1 point,Thu Mar 5 21:25:49 2015 UTC,"After always being fascinated by graphics and rendering, I decided to write a simple path tracer. It currently supports spheres and triangle meshes (objs), PNG textures, diffuse, specular and emissive material types. It uses a very simple KD tree to accelerate triangle intersections.  This is my first time using C++ so feel free to point out any horrible mistakes I may have made."
GraphicsProgramming,2xtpcl,Whithers,2,Tue Mar 3 20:23:04 2015 UTC,"I will assume you have good knowledge in bitshifting.  I use a simpler format to express a path traversal to access data in an octree. Consider this array of 8 bit binary:  0000 0001 0000 0001 0000 0001 0000 0001   This path is use to access a certain location at the octree. Basically this path points to xyz(0,0,0) in your 3D grid.  At level 0, look at the first element at that element look at the first element an so on..  until it traverse to the required depth (Level of Detail) 4 level in this case.  Finding the neighbor(element 2) at LOD 4 is simply:   0000 0001 0000 0001 0000 0001 0000 0010   Finding all the 8 neighbors at LOD 4 is merely progressing the bit 1 at position 0 to 7, then check if that region is occupied or not."
GraphicsProgramming,2xtpcl,xcodeart,1 point,Mon Mar 9 10:50:55 2015 UTC,"Thank you SO much!  Sorry it took me so long to reply--I got the flu really bad the day you posted and just got over it.  Yes, thanks to you and another gentleman, the last horse has finally crossed the finish line :)  I'm very excited."
GraphicsProgramming,2xtpcl,xcodeart,1 point,Mon Mar 16 17:57:21 2015 UTC,"I'm curious to what is your motivation in writing your own graphics renderer/engine. If you want make a game Unreal,Unity, Source2 engines are out for free. As for my motivation, I am trying to learn a new programming language(rust) while at the same time trying out graphics as the subject of learning the language. I did learn a lot on how to calculate this stuff without relying on already implemented solutions such as opengl. Unfortunately, I haven't had a chance in working with a company that does games while using their own in-house engine/renderer, which would be pretty exciting to work at. So, I use my spare-time to do graphics to whatever I could conceived, maybe a minimal game."
GraphicsProgramming,2xneu6,Pyrolistical,23,Mon Mar 2 07:54:52 2015 UTC,"99 lines of [horrible condensed] C++.   It's really really cool, but IMO bunching up all the lines into an unreadable mess is not cool. GO in 200 lines of pretty code would be ever better."
GraphicsProgramming,2xneu6,mcjohnalds45,5,Mon Mar 2 08:47:55 2015 UTC,"Yeah... I don't think I'd really count  if (++depth>5) if (erand48(Xi)<p) f=f*(1/p); else return obj.e;   as ""one line"". At that point, might as well just put it all on one line and call it a day."
GraphicsProgramming,2xneu6,Tasgall,4,Mon Mar 2 22:51:36 2015 UTC,"I found it pretty readable/decently commented. If you have a background in graphics, it shouldn't be too difficult to decipher."
GraphicsProgramming,2xneu6,_ArrogantAsshole_,3,Mon Mar 2 17:57:25 2015 UTC,I agree. It would be a great educational tool if expanded and commented.
GraphicsProgramming,2xneu6,jurniss,4,Mon Mar 2 15:40:11 2015 UTC,This is pretty much the exact comment I was going to make.
GraphicsProgramming,2xneu6,theLOLflashlight,1 point,Mon Mar 2 09:37:22 2015 UTC,I'm not gonna even try to see if it is multithreaded but I guess it's not. This would be a nice exercise to try to convert to C#. Some loops can easily be parallelized in C# with just changing two lines.
GraphicsProgramming,2xneu6,ccricers,2,Mon Mar 2 19:52:33 2015 UTC,"It is multithreaded with OpenMP in just one line (a pragma, line 79) and a compiler flag (-fopenmp)."
GraphicsProgramming,2xneu6,matthia,1 point,Mon Mar 2 22:03:22 2015 UTC,I see it now. I haven't used C++ in so long. So do you need both the pragma directive and the compiler flag to set it up this way?
GraphicsProgramming,2xneu6,ccricers,1 point,Mon Mar 2 22:09:57 2015 UTC,"Yes, without the compiler flag the pragma is ignored and the code runs single threaded.  For some openmp functions you have to include <omp.h>"
GraphicsProgramming,2xneu6,matthia,1 point,Mon Mar 2 22:20:17 2015 UTC,"Ahh smallpt, the start point for my university honours project, how I have (not) missed you."
GraphicsProgramming,2xneu6,DrumminOmelette,-11,Mon Mar 2 20:23:50 2015 UTC,Too condensed. And using C-style arrays in modern C++ is considered bad style. Use std::array or std::vector.
GraphicsProgramming,2xneu6,Maslo59,6,Mon Mar 2 10:49:09 2015 UTC,"std::vector is not equal as it's reserved from heap and std::array is available from C++11 onwards, only."
GraphicsProgramming,2xneu6,nnevatie,3,Mon Mar 2 14:57:19 2015 UTC,Given that it's 2015 and all major distro's have c++11 support available I don't see why it shouldn't use std::array.
GraphicsProgramming,2xneu6,Delwin,1 point,Mon Mar 2 20:01:00 2015 UTC,Much of the visual effects world is still stuck in gcc4.2.
GraphicsProgramming,2xneu6,pixel_fcker,4,Fri Mar 6 22:42:52 2015 UTC,"I would argue that for something as important and time-sensitive as graphics programming, the speedup given with c style arrays is significant."
GraphicsProgramming,2xneu6,Flafla2,3,Mon Mar 2 16:26:20 2015 UTC,Condensed is kind of the point of smallpt.
GraphicsProgramming,2xneu6,scramjam,2,Mon Mar 2 15:26:50 2015 UTC,"// smallpt, a Path Tracer by Kevin Beason, 2008"
GraphicsProgramming,2xneu6,jurniss,1 point,Mon Mar 2 15:38:51 2015 UTC,... that would be why he didn't use std::array then.  OK.
GraphicsProgramming,2xe386,corysama,2,Fri Feb 27 20:40:49 2015 UTC,"Great article, thanks. If you happen to be going to the gdc, maybe you could take a look at this session. There might be some guidance as to what techniques were implemented and how: http://schedule.gdconf.com/session/blending-cinematic-and-gameplay-animation-in-the-order-1886-presented-by-autodesk"
GraphicsProgramming,2xe386,andiCR,1 point,Fri Feb 27 21:28:14 2015 UTC,Neat but I really want to know specifics.
GraphicsProgramming,2xe386,ChainedProfessional,1 point,Fri Feb 27 23:56:35 2015 UTC,"Crafting a Next-Gen Material Pipeline for The Order: 1886 - http://blog.selfshadow.com/publications/s2013-shading-course/rad/s2013_pbs_rad_notes.pdf  Great set of course notes on how a lot of the rendering is done, lots of information on their material creation pipeline. They have great fabric rendering tech which is something a lot of games lack. Fabrics tend to look off in most microfacet based PBR engines."
GraphicsProgramming,2xe8k8,Madsy9,2,Fri Feb 27 21:20:37 2015 UTC,"For 3D to 2D, happened accross this image (scroll down one page): Essentials of Interactive Computer Graphics: Concepts and Implementation page 437-440"
GraphicsProgramming,2xe8k8,CodyDuncan1260,1 point,Fri Feb 27 23:36:56 2015 UTC,Thanks for the suggestion. Showing the 3D meshes projected down to a 2D plane is indeed a great idea. And it could also illustrate the difference between an orthographic and perspective projection :)
GraphicsProgramming,2xe8k8,CodyDuncan1260,2,Sat Feb 28 17:14:17 2015 UTC,I came accross this today: http://bost.ocks.org/mike/algorithms/  The author put up his twitter account for contact. I wonder if he could be of assistance in idea-ing visualizations of graphics concepts.
GraphicsProgramming,2xe8k8,ultrapingu,1 point,Tue Mar 10 21:05:39 2015 UTC,"Great blog, thanks a lot! :-)"
GraphicsProgramming,2x6yft,CodyDuncan1260,7,Thu Feb 26 03:24:58 2015 UTC,http://www.eecs.berkeley.edu/~lingqi/publications/paper_glints.pdf
GraphicsProgramming,2x6yft,SarahC,3,Thu Feb 26 03:25:22 2015 UTC,"This is really good, thanks!"
GraphicsProgramming,2x6yft,vanderZwan,2,Thu Feb 26 06:12:23 2015 UTC,"Thanks for sharing the paper too!  This is probably a total coincidence, but your graphs superficially resemble some of the results that I get when playing around with a particular version of a swarm algorithm called ""dancing with friends and enemies"". Or I should say my results superficially resemble your graphs, since they actually depict meaningful information.  I've never seen graphs like that before, can you tell me what they are and how they work as a data visualisation?  (the code I wrote to produce that stuff)"
GraphicsProgramming,2x6yft,TurkishSquirrel,3,Thu Feb 26 10:31:26 2015 UTC,In a similar vein (same authors also at SIGGRAPH 14) that doesn't require the high-resolution normal map and instead uses a stochastic process to generate the glinting particles: Discrete Stochastic Microfacet Models. The results of both are really great.
GraphicsProgramming,2x6yft,ultrapingu,2,Thu Feb 26 16:25:19 2015 UTC,I don't understand what's new about this?  Isn't this just normal mapping but applied only to specular?
GraphicsProgramming,2x6yft,Spifffy,11,Thu Feb 26 11:04:12 2015 UTC,"This isn't about Normal map technique or use, It' about how the normal map is rendered.  In most engines the normal map tends to come through incorrectly under strong but small specular highlights (Glinting), where sub pixel highlights aren't rendered correctly and the glinting looks awful (You can see the comparison at the beginning)  Their method just improves how glinting is rendered."
GraphicsProgramming,2x6yft,ultrapingu,1 point,Thu Feb 26 13:20:56 2015 UTC,"ah I see, thanks :)"
GraphicsProgramming,2x6yft,07dosa,1 point,Thu Feb 26 15:12:22 2015 UTC,"Simple, but extremely powerful. The last example is so impressive."
GraphicsProgramming,2x6yft,mattdesl,1 point,Thu Feb 26 08:53:16 2015 UTC,This seems focused on offline rendering. Any ideas how to simulate it in real-time?
GraphicsProgramming,2x70fo,CodyDuncan1260,3,Thu Feb 26 03:42:27 2015 UTC,"At least for offline production rendering, I believe that the baseline state of the art is now d'Eon's quantized-diffusion method.  The original landmark papers on the topic would probably be Jensen's dipole approach and its followup."
GraphicsProgramming,2x70fo,Boojum,3,Thu Feb 26 08:26:00 2015 UTC,"The current state of the art methods are  Photon Beam Diffusion and the Dual-Beam BSSRDF.  These are very hard to implement though compared to the dipole. Eugene d'Eon released a better version of the dipole which can be found here. It's also worth mentioning the multipole which supports finite slabs (instead of the infinite slabs of the dipole, so that might solve a few issues in the case of a concave hull) and layering.  Kulla and County mention that they just use a simple Gaussian or Bicubic BSSRDF in movie production, so that might work as well. This talk also introduces a nice way of sampling BSSRDFs which avoids precomputation."
GraphicsProgramming,2x70fo,solidangle,2,Thu Feb 26 18:21:25 2015 UTC,Right.  That's why I hedged and wrote baseline state of the art.  I was under the impression that most of the major commercial renderers use something at least pretty close to quantized diffusion these days.  I think photon beam diffusion is still pretty unusual; do you know of any renderer besides PRMan that comes with it?
GraphicsProgramming,2x70fo,Boojum,2,Thu Feb 26 20:28:42 2015 UTC,Do you know which renderers implemented QD? Looking at the documentation of Arnold it seem they use something based on a simple Bicubic/Gaussian profile and it seems like VRay uses something based on the dipole.  The only thing I could find was the Weta Digital used QD for a scene in Promotheus.
GraphicsProgramming,2x70fo,solidangle,2,Thu Feb 26 21:02:50 2015 UTC,"Hm.  I guess you're right.  I could have sworn that I'd heard of them using it, but I guess not.  The closest reference I can find just now is this fxguide article mentioning that some V-Ray customers had written their own QD shaders."
GraphicsProgramming,2x70fo,Boojum,2,Fri Feb 27 03:02:23 2015 UTC,"See my reply to Boojum for some sources.   I don't think however that there is a good model for subsurface scattering that supports concave shapes. You could explicitly cast a ray between two surface points and ignore that combination if the ray intersects the surface between those two points, this would mean a loss of light but it might be more visually appealing in some cases."
GraphicsProgramming,2x37nw,CodyDuncan1260,3,Wed Feb 25 07:03:17 2015 UTC,"its a paper from 2011, is it used now anywhere?"
GraphicsProgramming,2x37nw,newacco,3,Wed Feb 25 13:36:30 2015 UTC,"You know what grinds my gears? How every paper includes the date of everything it cites, but it's own date is nowhere to be found. Then you have to google to find out when it was written."
GraphicsProgramming,2x37nw,jrkirby,2,Wed Feb 25 18:59:53 2015 UTC,I wish there was a way of avoiding rendering multiple gbuffer passes with this technique. The paper mentions using MSAA but then you need to render all you other gbuffer textures as MSAA to get the position and normal at 4x or render a second non-MSAA pass for the other shading inputs.
GraphicsProgramming,2x37nw,snakepants,1 point,Wed Feb 25 07:27:53 2015 UTC,"What I don't get is how the shading time (which I assume is the lighting pass) is still the same with SRAA as without. As well as the G-buffer rendering time, even though it's clear from the same graph that you are setting 4x as much G-buffer data."
GraphicsProgramming,2x37nw,ccricers,1 point,Mon Mar 2 20:04:20 2015 UTC,"I don't buy that the G-buffer rendering time is the same either, but the shading time could be similar if you are only shading the first sample. However you either have shitty cache coherency by reading only sample 0 from a MSAA textures or you do another resolve pass to extract the 0th sample."
GraphicsProgramming,2wzqrq,vinnyvicious,10,Tue Feb 24 14:44:50 2015 UTC,What is this post about? Software Rendering? Just Software Rasterizers that mimic GPUs or also Ray Tracers and REYES renderers?
GraphicsProgramming,2wzqrq,solidangle,1 point,Tue Feb 24 16:28:36 2015 UTC,"Just curious, I have never heard of REYES renderers, can you give me some link so I can read about it ?"
GraphicsProgramming,2wzqrq,GhostNULL,2,Tue Feb 24 23:36:43 2015 UTC,The ur example would be Pixar's RenderMan implementation.  Here's the original paper on Reyes.
GraphicsProgramming,2wzqrq,Boojum,9,Wed Feb 25 02:05:29 2015 UTC,"Wow, this is like.. my favorite topic!  My tutorial code series on screen projection, clipping, bresenham line drawing, affine texture mapping and perspective-correct texture mapping (implements a scanline based rasterizer): https://github.com/Madsy/Computer-Graphics-Explained  My tilebased rasterizer with a highly optimized inner loop, based on Nick's code from the devmaster.net forums: https://github.com/Madsy/NPixel  Nick's original thread on tile-based rasterization on the devmaster.net forums: http://forum.devmaster.net/t/advanced-rasterization/6145 (Don't resurrect it though, as the thread is 12 years old)  Article I wrote on the gradient interpolation technique used in my NPixel tile rasterizer: http://codrspace.com/madsy/  And here is the original paper on the technique: «Triangle Scan Conversion using 2D Homogeneous Coordinates» by Marc Olano and Trey Greer  I have an IRC channel specifically for discussion on software-based graphics implementations and agnostic graphics algorithms. Please feel free to visit us at ##graphics on the FreeNode network!"
GraphicsProgramming,2wzqrq,Madsy9,1 point,Tue Feb 24 23:03:09 2015 UTC,Are the tutorials offline?
GraphicsProgramming,2wzqrq,Madsy9,1 point,Wed Feb 25 00:10:00 2015 UTC,"Oh.. you mean at mechcore.net? Yeah, I had to remove the wiki because mediawiki is almost impossible to keep secure. Which is why I wrote ""tutorial code series"". I'll have to make a new wiki somewhere, with a framework which has better countermeasures against spamming."
GraphicsProgramming,2wzqrq,Madsy9,1 point,Wed Feb 25 08:12:20 2015 UTC,You should try setting up a GitHub page with Jekyll. :)
GraphicsProgramming,2wzqrq,thatisthewaz,1 point,Wed Feb 25 12:02:39 2015 UTC,"Hm, that could work. I might look into that, thanks!"
GraphicsProgramming,2wzqrq,culdevu,2,Wed Feb 25 12:21:59 2015 UTC,"The scratchapixel link is broken, does anyone have a working one."
GraphicsProgramming,2wzqrq,nnevatie,1 point,Tue Feb 24 19:00:27 2015 UTC,http://www.scratchapixel.com/old/lessons/
GraphicsProgramming,2wzqrq,nnevatie,1 point,Tue Feb 24 20:35:51 2015 UTC,Here's a recent project: https://github.com/Steve132/uraster
GraphicsProgramming,2wzqrq,maratis3d,1 point,Tue Feb 24 19:06:54 2015 UTC,Very nice. Are you the author?
GraphicsProgramming,2x0yls,vinnyvicious,4,Tue Feb 24 20:12:51 2015 UTC,"The 0fps blog has a lot of useful stuff. Not exactly a tutorial, but pretty close...  Meshing  Meshing For Textures  Ambient Occulusion  Texture Sampling"
GraphicsProgramming,2x0yls,LordTocs,1 point,Tue Feb 24 21:33:23 2015 UTC,Thanks for the info! :)
GraphicsProgramming,2x0yls,ngildea,3,Wed Feb 25 17:00:07 2015 UTC,r/voxelgamedev is a good place to start.
GraphicsProgramming,2x0yls,ccricers,2,Tue Feb 24 22:14:38 2015 UTC,"Yep, it's a tad slow, but there are good questions and resources that are posted there."
GraphicsProgramming,2x0yls,fb39ca4,1 point,Tue Feb 24 22:46:42 2015 UTC,"I haven't written any tutorials, but I recently wrote a simple voxel engine, which you can try out here: http://fb39ca4.github.io/webgl-voxel-terrain/  Someday I'll get around to properly writing up an explanation, but in the meantime feel free to ask anything about how it works."
GraphicsProgramming,2wypkc,CodyDuncan1260,3,Tue Feb 24 06:20:58 2015 UTC,"Decided to graph the Smith Masking geometry term with the Beckmann distribution. This is for the G term in a BRDF Microfacet lighting function. For reference: Specular BRDF Reference - Look under heading ""Smith"" and the following ""Beckmann[4]"" Microfacet Models for Refraction through Rough Surfaces - See top right of page 6.    Move the slider for ""a"" to show changes to the function when alpha (roughness) is between 10 and 1000. Move the slider for ""b"" to show changes to the function when alpha (roughness) is between 0 and 10. The Green line represents a ""Cheaper than Free!"" approximation of G=1, which is good enough for most non-hyper-realistic purposes.   The geometry term represents the probability of microfacets occluding or shadowing other microfacet surfaces. This is fairly constant between 0 and 1.2 (view from above surface to view at 70 degrees), and then begins to fall sharply between 1.2 and pi/2 (1.57079) (70 to 90 degrees [horizontal] to the surface) as the glancing angle has more facets that occlude and shadow eachother."
GraphicsProgramming,2wuzxp,JJJams,13,Mon Feb 23 10:59:46 2015 UTC,"It's not Valve's tool, their tool is Vogl. I believe Vogl is based on apitrace, or inspired by it at least."
GraphicsProgramming,2wuzxp,bitshifternz,1 point,Mon Feb 23 13:28:33 2015 UTC,"How do they compare?  I had a quick look at Vogl, it appears to be mostly pointed at Linux.    I use Mac and PC, so I'll see if I can get both going on those platforms."
GraphicsProgramming,2wuzxp,bitshifternz,1 point,Mon Feb 23 23:39:31 2015 UTC,"I haven't used Vogl much, but I think one of there motivations was addressing the some limitations of apitrace. That is, apitrace has to capture from application start, so the traces get massive if what you want to look at is in the middle of a game level somewhere. Apitrace does allow you to trim the tracefile offline after the capture is complete, but you still have to work with these massive capture files. One of the goals of Vogl was to be able to start the capture at an arbitrary point of the application running. To do that they have to set up all the initial state correctly so the replayed capture is correct."
GraphicsProgramming,2wuzxp,Rhed0x,5,Mon Feb 23 23:57:41 2015 UTC,"Oops, not sure if that this is Valve's tool.... looks like they may have contributed to it.  It looks to be mostly the work of a José Fonseca."
GraphicsProgramming,2wuzxp,smallstepforman,5,Mon Feb 23 11:50:51 2015 UTC,"Nope, it's not. As far as I know it's a fork or a rebranding of GLSLDevil but don't quote me on this."
GraphicsProgramming,2wuzxp,lua_setglobal,-7,Mon Feb 23 13:20:22 2015 UTC,"Only helps if you want to reverse engineer someone else's code, however of little use for your own code."
GraphicsProgramming,2wuzxp,smallstepforman,3,Mon Feb 23 22:22:24 2015 UTC,"Not true (for OpenGL at least)   Each frame, you can see exactly what was sent to the GPU You can examine OGL state (very valuable for debugging) during replay View contents of buffers   Am I missing something?  I posted it here because it looks like it solves common OGL debugging issues.  Specifically, ""What's going on in there?!""."
GraphicsProgramming,2wuzxp,lua_setglobal,1 point,Mon Feb 23 22:43:46 2015 UTC,"I definitely could have used an API tracer the other day when I was hunting a crash on Pandora.  I ended up making a function that wraps ""print (glGetError)"" and putting it basically everywhere. Then when I fixed all the errors, I commented out the print, so the function does nothing."
GraphicsProgramming,2wuzxp,gott_modus,4,Tue Feb 24 00:57:18 2015 UTC,"From 4.0 onwards you can use callbacks glDebugMessageCallbackARB, no need to call glGetError().  Save you some time ..."
GraphicsProgramming,2wwn3d,thatisthewaz,2,Mon Feb 23 20:11:57 2015 UTC,?? not sure.  Quick googly search revealed this though...  http://stackoverflow.com/questions/9021244/how-to-work-with-pixels-using-direct2d
GraphicsProgramming,2wsjq1,SavvyBaby3,6,Sun Feb 22 20:23:37 2015 UTC,"Its been a while since i looked at BV code, but I would make sure that the bounding volume actually falls outside the axis. When dealing with floats, its best to give a small margin for error."
GraphicsProgramming,2wsjq1,plasticle,3,Sun Feb 22 20:37:49 2015 UTC,You nailed it. I expanded by bounding volume by a small margin and it works now!
GraphicsProgramming,2wsjq1,Boojum,5,Sun Feb 22 20:41:53 2015 UTC,"JCGT has a paper, Robust BVH Ray Traversal, that analyzes this sort of expansion in detail."
GraphicsProgramming,2wsjq1,qartar,1 point,Mon Feb 23 01:17:47 2015 UTC,When you say axis aligned do you mean the triangles in this case are co-planar?
GraphicsProgramming,2wj5uh,Boojum,9,Fri Feb 20 09:16:55 2015 UTC,"This is ""from scratch"" like how apple pies should be made, not Scratch the programming language."
GraphicsProgramming,2wj5uh,yoat,9,Fri Feb 20 16:20:26 2015 UTC,"This site has a lot of nice, basic lessons on programming computer graphics (particularly on ray tracing for offline rendering).  There's a newer version of the site, too, but it seems like it has less material than the old version so I chose link to the old one here instead.  If you're not yet ready to tackle PBRT, this may be a good stepping stone."
GraphicsProgramming,2wj5uh,dashto,1 point,Fri Feb 20 09:21:58 2015 UTC,Great link! Will the site still be updated? The real-time rendering section seems like it'd be a good addition but there have been no new videos since 2014.
GraphicsProgramming,2wj5uh,ccricers,1 point,Fri Feb 20 16:40:49 2015 UTC,Couldn't say.  I suspect the guy behind it has been pretty busy with his day job.
GraphicsProgramming,2wj5uh,solidangle,1 point,Sat Feb 21 05:21:49 2015 UTC,I'm at an intermediate level of graphics programming and would like to try PBRT but the math is too heavy for me and I learn more easily with long pseudocode examples than with long math equations.
GraphicsProgramming,2wj5uh,gott_modus,2,Fri Feb 20 23:47:58 2015 UTC,Does anyone have an idea if there is anything on those password protected pages (for example 3D Advanced Lessons -> Production BVH)? Can you read the password protected pages after signing up for an account?
GraphicsProgramming,2whjam,Gpong,3,Thu Feb 19 23:25:50 2015 UTC,"Figure out how drawing pixels works with your graphics API of choice. Figure out how to rasterize primitives (points, lines, and triangles) in 2D. For lines, there is the Bresenham and DDA algorithms, and for triangles, you can use the results of the line drawing algorithm to fill in the space between edges, scanline by scanline. Figure out how to transform points from 3D to screen space. (This is using the exact same matrix math you would use with OpenGL vertex shaders.)   At this point, you should have a simple renderer that can draw solid color primitives. You can then add in other techniques, like Z-buffering, interpolation between vertices, and shading per-fragment."
GraphicsProgramming,2whjam,fb39ca4,3,Thu Feb 19 23:58:39 2015 UTC,I took this as a challenge and did it.  Didn't compile or run it but here's a tiny rasterizer that has all those features
GraphicsProgramming,2whjam,Steve132,2,Fri Feb 20 08:42:26 2015 UTC,Neat. You should do a write up and post pictures on this subreddit once it is running.
GraphicsProgramming,2whjam,fb39ca4,1 point,Fri Feb 20 09:18:28 2015 UTC,"Wow, thanks for doing this!"
GraphicsProgramming,2whjam,uncont,3,Sun Feb 22 13:50:12 2015 UTC,"If it's for fun, I would suggest looking through this tutorial. It goes over the basics from simple point projections to full model loading and shading.  In my case, I used Java and the respective imaging functions to get a buffer set up to render to. You could probably also do the same without relying on any external C# libraries. SDL2 should also work, if you use the SDL_RenderDrawPoint(renderer, x, y) function, taken from this stackoverflow question."
GraphicsProgramming,2whjam,newaccount1236,2,Sat Feb 21 23:52:35 2015 UTC,"An alternative approach is ray-tracing. It's probably simpler to implement, but slower, in general. Probably best done as an offline renderer that outputs a static image."
GraphicsProgramming,2whjam,fb39ca4,2,Fri Feb 20 00:07:52 2015 UTC,What is a ray tracer?
GraphicsProgramming,2whjam,vincetronic,2,Fri Feb 20 01:02:24 2015 UTC,"In the most simple terms, it approaches the problem of rendering in the opposite way as rasterization.  Rasterization: for each triangle (or other primitive) figure out which pixels it covers and draw them, allowing closer primitives to cover up further away ones.  Ray tracing: for each pixel, figure out which primitives it is in, and draw the closest one.  You do ray tracing by taking a ray originating at the camera, and calculating the first object in the scene it will hit."
GraphicsProgramming,2whjam,vincetronic,2,Fri Feb 20 04:59:58 2015 UTC,"This sounds very interesting, do you have any resources that will aid in this area of study."
GraphicsProgramming,2whjam,Herbstein,1 point,Sat Feb 21 00:19:12 2015 UTC,Physically Based Rendering is a literate programming book in the style of Knuth which builds a ray tracer from first principles.
GraphicsProgramming,2whjam,vinnyvicious,1 point,Sat Feb 21 19:07:55 2015 UTC,"Note that this isn't really appropriate for realtime rendering, but you will learn a lot!"
GraphicsProgramming,2whjam,physixer,1 point,Sat Feb 21 19:10:46 2015 UTC,I've been looking for something like this for some time. Thank you for linking it!
GraphicsProgramming,2whjam,Nihilus84,1 point,Sun Feb 22 09:46:57 2015 UTC,What's the technology stack used? C++ and SDL?
GraphicsProgramming,2whjam,jurniss,1 point,Tue Feb 24 20:13:57 2015 UTC,"Ray tracer in 105 lines of C++.  Path tracer in 99 lines of C++.  Path tracer is (the starting point of) the most physically accurate form of rendering, but most computationally expensive.  I recommended reading a book along side the source code. Don't let the 99 lines get you into thinking you'll be able to understand it without knowing what equations it implements."
GraphicsProgramming,2whjam,solidangle,1 point,Mon Feb 23 20:08:15 2015 UTC,Also worth looking into is real-time ray tracing on the GPU. You can basically construct an entire ray tracer using nothing more than a quad passed into the pixel shader. Combine it with implicit modelling using distance functions and you can make remarkable stuff without any meshes.
GraphicsProgramming,2wfb9z,Qkpmle,4,Thu Feb 19 12:59:35 2015 UTC,"Also, here's a list of tools that I've used to build the app. I didn't mention WebGL, I should add it there. There's /r/polygen too if you want to see more examples.  Edit: Apparently my app crashes on some android phones. You can access the beta version if you want to get the fix sooner."
GraphicsProgramming,2wfb9z,TweetsInCommentsBot,2,Thu Feb 19 13:01:54 2015 UTC,We seriously need the source code for this. ~amaze~ work
GraphicsProgramming,2wfb9z,tutuca_,6,Thu Feb 19 14:11:46 2015 UTC,"Glad you like it! I don't have any version of this ready for publication. I want to package it in an open source library someday, but didn't do it yet.  Fortunately @hypothete made a quick JS implementation based on this article (with source), which is cool and uses uses Poisson disc sampling as a bonus.  I've also found other tools which use a similar technique and are open source, e.g. trianglify."
GraphicsProgramming,2wfb9z,Boojum,2,Thu Feb 19 14:42:30 2015 UTC,@Hypothete   2015-01-29 21:25:52 UTC  @polygenapp Nice writeup on your triangulation process! I rolled my own this morning using Poisson disk sampling: http://jsbin.com/lopuku     This message was created by a bot  [Contact creator][Source code]
GraphicsProgramming,2wfb9z,ccricers,0,Thu Feb 19 14:42:37 2015 UTC,Can't have it. He is selling his app.
GraphicsProgramming,2wfb9z,Boojum,2,Thu Feb 19 14:28:45 2015 UTC,"There's a /r/gamedev post from a while back about a related technique for vertex-colored skydomes.  It's somewhat similar to this, but instead of starting with a gradient, they use an environment map and they turn it into a polygonal sphere.  (Granted, the sphere needs to be somewhat higher poly count than this to approximate the input)."
GraphicsProgramming,2wfb9z,mattdesl,1 point,Thu Feb 19 20:57:28 2015 UTC,"I wonder how you'd even begin to do that, the meshing optimization where color variation is lower. I guess a special map where color differences are found in adjacent pixels and then scattering points more densely where difference is greater."
GraphicsProgramming,2wfb9z,DoubleOnegative,1 point,Fri Feb 20 23:41:53 2015 UTC,"Thats guy's next article links to a thread on the HW2BGBuilder tool that someone built to try to emulate that.  If you look up the Terra by Michael Garland mentioned in the thanks, you arrive at this paper:  Fast Polygonal Approximation of Terrains and Height Fields  That paper has an example on the last page of applying their heightfield algorithm to triangulating a color image."
GraphicsProgramming,2wewri,Zicore47,1 point,Thu Feb 19 09:09:28 2015 UTC,"Can you remark about the license for the algorithm itself?  I had heard that Perlin's algorithm was not available for free use - but when I looked at the wiki, it just said that ""he did not patent this algorithm"", but he did for a subsequent implementation.  Is the algorithm free to use for all purposes?  Also as it pertains specifically to your code, what is the license?"
GraphicsProgramming,2wewri,Multicorn,2,Thu Feb 19 16:49:11 2015 UTC,"I really don't remember for that implementation of perlin noise. For my code in this project, I say it's public domain."
GraphicsProgramming,2wewri,KdotJPG,1 point,Thu Feb 19 18:45:26 2015 UTC,"There are three main ""smooth"" noises as I see it: Perlin, Simplex, and OpenSimplex. Well four if you count Value noise, but Value noise is a bit of a general term that can apply to a lot of things  Perlin is the original function. Introduced in the 80s and redesigned in 2002.  Simplex is a newer redesigned function by the same dude. The unfortunate part about Simplex noise is there is a patent that applies to using the noise implemented using certain steps for textured image synthesis on a computer in the USA. The other unfortunate part is that both in the original implementation and in Gustavson's popular implementation, the 3D version has slight discontinuities because the kernel radius is slightly too large (should be sqrt(0.5) and not sqrt(0.6) to stay within the simplex). But if you reduce the kernel radius to get rid of the discontinuities, it looks a fair bit worse.  OpenSimplex is a function I designed in the process of developing a yet-to-be-released Universe-exploration-based sandbox voxel game called Uniblock. I posted the original code to /r/proceduralgeneration and it's been used by a lot of people, as well as optimized / ported to other languages. It has the advantage over Perlin much like Simplex of being more visually isotropic, but also has the advantage over Simplex of being much smoother (because the kernel radii are larger). However, it's not as fast as either. Compared to Simplex noise on a technical level, it simply makes use of the dual lattice. It uses the A_n instead of the A_n*."
GraphicsProgramming,2wewri,Multicorn,1 point,Thu Feb 19 19:48:34 2015 UTC,"Wow!  Thanks for your remarks!  I will check out your implementation.  I'm not even 100% sure exactly what I would use it for in a dynamic game setting just yet, but I use the Perlin Noise node in Blender (Cycles) all the time.  I think I can probably manage any CPU cost by caching with a load screen up front if I need to.  You've also given me some new terms to research - thanks again!"
GraphicsProgramming,2wewri,KdotJPG,2,Thu Feb 19 20:01:02 2015 UTC,"In a dynamic game setting, you can use it to:   Generate textures on the fly (My plan is is to reserve maybe 20 bits per block to ""seed"" the textures, so each block has a unique texture). I won't quite be using OpenSimplex for that, because I need to join lattice structures together at low frequencies, but I'll be using something similar. Generate maps and terrain for a 3D world! Manipulate 2D noise for a basic heightmap, 3D noise for caves, use 3D noise + an attenuation offset to get overhangs: noise(c1 * x, c1 * y, c2 * z) + c3 * z + c4. Use two or more 2D instances to determine things like temperature and humidity and generate biomes based on those. Generate wind/weather effects."
GraphicsProgramming,2wewri,KdotJPG,1 point,Thu Feb 19 20:16:51 2015 UTC,"Looks awesome!  I would also recommend looking into using OpenSimplex noise for this type of thing, in place of Perlin: https://gist.github.com/digitalshadow/134a3a02b67cecd72181  OpenSimplex noise has significantly fewer visually-significant directional artifacts.   https://dl.dropboxusercontent.com/u/58079480/OpenSimplexNoise/Perlin_vs_OpenSimplex.png   I developed OpenSimplex for a game that's yet to be released, and this code here is DigitalShadow's heavily rewritten/optimized version of my original code."
GraphicsProgramming,2wewri,Zylox,1 point,Thu Feb 19 19:01:32 2015 UTC,"I saw perlin noise and thought: ""I bet KdotJPG is gonna comment""."
GraphicsProgramming,2wewri,KdotJPG,2,Sat Feb 21 02:26:58 2015 UTC,Of course! I have http://www.reddit.com/search?q=%28perlin%7Csimplex%7Copensimplex%29+noise&sort=new in my browser's bookmarks bar.
GraphicsProgramming,2wewri,Zylox,1 point,Mon Feb 23 00:04:32 2015 UTC,"Haha, I figured you had to have something. I spend a lot of time messing around with procedural generation and noise is one thing i simply don't understand, so when researching I run into your stuff pretty much everywhere."
GraphicsProgramming,2wewri,StickiStickman,1 point,Mon Feb 23 01:51:24 2015 UTC,Great job! I wouldn't have thought about using it for planets ... makes me want to try it with elevation and diffuse noise on 3D balls :)
GraphicsProgramming,2waxbl,PandaHammer,6,Wed Feb 18 11:46:29 2015 UTC,"The light source is moving with the 3D logo? is this the ""Camera"" spinning or the logo rotating? (I don't know anything about 3D)"
GraphicsProgramming,2waxbl,ambercore1000,5,Wed Feb 18 13:52:52 2015 UTC,Camera is moving around a fixed ground plane and a fixed hovering logo.
GraphicsProgramming,2waxbl,ambercore1000,2,Wed Feb 18 14:36:11 2015 UTC,I cant stop imagining a camera flying around the logo. :D Nice work no matter what. :)
GraphicsProgramming,2waxbl,DavidZuc,2,Wed Feb 18 14:44:27 2015 UTC,Thanks!
GraphicsProgramming,2waxbl,Hard_Bodied_Man,2,Wed Feb 18 14:49:42 2015 UTC,"I think it's just the logo rotating; that would be easiest to do in opengl. Nice work, which graphics primitives did you use?"
GraphicsProgramming,2waxbl,Hard_Bodied_Man,11,Wed Feb 18 14:28:35 2015 UTC,"nope. camera is moving.  pseudocode:  campos.x = sin(time) * camdist;  campos.z = cos(time) * camdist;  camFocus = vec3(0.0f, 10.0f, 0.0f);   Then run that through the GLM lookat function to get the view matrix.  easy peasy."
GraphicsProgramming,2waxbl,Lukeme9X,3,Wed Feb 18 14:38:23 2015 UTC,didn't know you could even code in blender...
GraphicsProgramming,2waxbl,Hard_Bodied_Man,2,Wed Feb 18 16:17:22 2015 UTC,"Ah Ok.  I'm kinda new to coding (using codeacademy now) but I've been into 3D for a few years. Blender's a great little program; it's more of a multimedia suite than a standard 3d.  I primarily use Cinema 4D, which I believe uses Python as well."
GraphicsProgramming,2waxbl,Lukeme9X,6,Wed Feb 18 16:41:46 2015 UTC,Yep C4D is python.
GraphicsProgramming,2waxbl,ccricers,2,Wed Feb 18 16:43:32 2015 UTC,Nice. This sub is sweet; I look forward to learning alot here.
GraphicsProgramming,2waxbl,Hard_Bodied_Man,4,Wed Feb 18 16:56:49 2015 UTC,"Same, that was actually my first comment haha."
GraphicsProgramming,2waxbl,zlsa,2,Wed Feb 18 16:59:57 2015 UTC,Blender is my go to modeling program for workflow in game graphics. Its video editor UI is also not bad for a free program.
GraphicsProgramming,2waxbl,Hard_Bodied_Man,2,Wed Feb 18 17:04:15 2015 UTC,"it's great for a free program, but it's internal render kind of sucks.   Cinema 4D simplifies alot of things that are a pain in blender, like the node editor for materials and adding HDRI maps"
GraphicsProgramming,2waxbl,jrkirby,1 point,Wed Feb 18 19:45:17 2015 UTC,"Have you tried Cycles? It's many times slower than BI, but it's much more powerful."
GraphicsProgramming,2waxbl,AssRapeNoHomo,1 point,Wed Feb 18 19:49:48 2015 UTC,"yes, i liked it alot. I actually made this piece in cycles about a year and a half ago  http://ajamelio.com/images/graphics/chess.png  but that would've taken like 75% less time to do in Cinema 4d, with basically the same result.   The cycles lighting is awesome though."
GraphicsProgramming,2waxbl,Hard_Bodied_Man,1 point,Thu Feb 19 01:17:08 2015 UTC,"While you can code python scripts in blender, that's not how this was done. He only did modeling and baking in blender, and then made a separate application to render the model and baked textures on his desktop, as is shown in the gif."
GraphicsProgramming,2waxbl,AssRapeNoHomo,1 point,Thu Feb 19 01:23:29 2015 UTC,"Op didn't state that it was, but c++ and opengl were used. It'd make sense for the model to be exported, streamed by the program op wrote, and then build the prog as a 3d wallpaper app."
GraphicsProgramming,2waxbl,turquoiserabbit,0,Wed Feb 18 19:52:53 2015 UTC,yea i got it now. nice username lol
GraphicsProgramming,2waxbl,pnpbios,1 point,Wed Feb 18 21:09:16 2015 UTC,Thanks brah c:
GraphicsProgramming,2waxbl,fb39ca4,2,Wed Feb 18 21:18:13 2015 UTC,Neato. Any way to implement such a thing in Windows? I've actually been trying to find ways to spruce up my desktop aside from plain image slideshows. Though I must admit my programming skills are rather basic.
GraphicsProgramming,2waxbl,LukasBoersma,3,Thu Feb 19 03:41:14 2015 UTC,"Oh yes, this is very doable.  https://msdn.microsoft.com/en-us/library/windows/desktop/dd144873(v=vs.85).aspx  https://msdn.microsoft.com/en-us/library/windows/desktop/ms633504%28v=vs.85%29.aspx  But be prepared to fight with aero / what ever decorator is running."
GraphicsProgramming,2waxbl,phire,1 point,Wed Feb 18 20:33:58 2015 UTC,"I'm fairly sure it's possible, but the windows window manager is not very well documented.   Same code but with a sweet ton of win32dll calls will probably work."
GraphicsProgramming,2wari6,King_Raptorpus,7,Wed Feb 18 10:11:01 2015 UTC,"I can personally recommend that you try to code a demo. No need for game engine stuff, just take the straightest path to getting what you want on the screen. The downside is that you might need a graphics/model artist and definitely need someone to make you a music track. Also if you live in the US, there aren't that many demoparties around where you could release your new demo... Anyway, http://pouet.net is a pretty great site that lists most demos and parties that have happened and has some discussion as well. Also http://demoparty.net has some upcoming parties."
GraphicsProgramming,2wari6,Pik16,3,Wed Feb 18 10:47:47 2015 UTC,"Hurah demoscene.  There's also a bunch of handy IRC channels. IRCnet/#revision is probably the most active, but there's a few others about."
GraphicsProgramming,2wari6,HighRelevancy,1 point,Wed Feb 18 14:26:28 2015 UTC,"Yes, IRC. Getting on the social side of the demoscene is the way to learn new things and to get inspired to try new stuff."
GraphicsProgramming,2wari6,Pik16,3,Wed Feb 18 20:17:09 2015 UTC,"Can only recommend! I also got most of my initial graphics experience from demo programming, although I was lucky enough to find two other like-minded individuals at my university to make 64k demos together.  The contest aspect of demo programming definitely pushes you to learn as much as you can about fancy post processing or graphics effects, and the real-time requirement also forces you learn how to render those effects fast."
GraphicsProgramming,2wari6,thunabrain,2,Wed Feb 18 21:22:21 2015 UTC,Is small program size required to make a demo? Or shouldn't it matter much for a beginner?
GraphicsProgramming,2wari6,ccricers,1 point,Wed Feb 18 19:42:09 2015 UTC,"Demos usually have a small size or an interesting platform or very high end graphics, but you don't need to win the compo to have fun making a demo and learning. Especially the first time."
GraphicsProgramming,2wari6,Pik16,2,Wed Feb 18 20:13:20 2015 UTC,"Sounds great then. I have done some work with deferred lighting shaders, and now I am working on a game that has me using procedural generation with noise for the first time. I could use something where I can combine the two well."
GraphicsProgramming,2wari6,ccricers,3,Wed Feb 18 20:17:13 2015 UTC,"This is one of the better tutorials I have found: http://www.arcsynthesis.org/gltut/index.html  It's not perfect and far from complete, but it'll give you a solid grounding and a great base to build up from."
GraphicsProgramming,2wari6,RoyAwesome,3,Wed Feb 18 11:56:14 2015 UTC,"Processing is great for prototyping OpenGL stuff. It's not very up-to-scratch with modern standards. If you wanna try experiemental stuff, look around shadertoy."
GraphicsProgramming,2wari6,HighRelevancy,2,Wed Feb 18 14:27:40 2015 UTC,I recomend this http://diaryofagraphicsprogrammer.blogspot.com/2009/11/you-want-to-be-graphics-programmer.html
GraphicsProgramming,2wari6,danio987,1 point,Wed Feb 18 12:25:41 2015 UTC,"My old university text book ""Computer Graphics"", FS Hill Junior, Department of Electrical and Computer Engineering, University Of Massachusetts, 1990. It's a bit dated I know but the fundamentals of computer graphics programming are contained in this book. I recently worked through Chapter 5, Approaches To Infinity, to get my eye back in to Graphics Programming.   Here's a Sierpinski Cure I wrote...  https://www.youtube.com/watch?v=gwSMNtB_qQI  Here's some Julia sets and a c curve fractal that I wrote:  https://imgur.com/a/ifNYW"
GraphicsProgramming,2wari6,DavidZuc,1 point,Wed Feb 18 14:13:31 2015 UTC,"One of the best resources I found while learning graphics programming was http://www.rastertek.com/tutdx11.html. It covers a lot of topics, pretty much everything you need to get started, steps by steps. It's only DirectX, but it might be usefull to know diferrent API."
GraphicsProgramming,2wari6,oks2024,1 point,Wed Feb 18 15:55:35 2015 UTC,"As much as I like rastertek for the sheer quantity and quality of the content, I 'm not a huge fan of the amount of comments in the source."
GraphicsProgramming,2wari6,CodyDuncan1260,1 point,Sun Feb 22 22:21:40 2015 UTC,I would highly recommend three.js. You could probably build some cool things very quickly with your existing knowledge of the graphics pipeline.   http://threejs.org/docs/index.html#Manual/Introduction/Creating_a_scene
GraphicsProgramming,2wari6,cpk33,2,Wed Feb 18 15:59:30 2015 UTC,"If by quickly you mean cover an entire scene in morphing, moving spheres in 15 minutes, very yes. Super easy to use."
GraphicsProgramming,2wari6,CodyDuncan1260,1 point,Sun Feb 22 22:22:51 2015 UTC,"Yes, exactly. I liked the tutorial because you can quickly build something fun and experiment along the way."
GraphicsProgramming,2wari6,cpk33,1 point,Sun Feb 22 22:38:57 2015 UTC,"As others have said, the best tutorial for Modern OpenGL is http://www.arcsynthesis.org/gltut/ imo."
GraphicsProgramming,2wari6,superfunc,1 point,Wed Feb 18 17:43:53 2015 UTC,"There's a free 3D graphics course on udacity.com. All you have to do is make a free account and it's all short, pretty simple video lectures."
GraphicsProgramming,2wari6,LoganCplusplus,1 point,Wed Feb 18 19:08:20 2015 UTC,There are many tutorials out there where if do all the steps you end basically with a really basic graphics engine backend. I can recommend   http://www.arcsynthesis.org/gltut/index.html   http://opengl-tutorial.org/beginners-tutorials/  http://ogldev.atspace.co.uk/  You already have a programming background so understanding the source code for some of these tutorials shouldn't be a problem.  When you are pretty much done with these tutorials I recommend reading academic / research papers and try to implement some of these things you read (maybe even improve or add your own kink)
GraphicsProgramming,2wcsyj,avgudar,1 point,Wed Feb 18 21:20:03 2015 UTC,How did you get the AMD logo to show? Is it just popping a collection of particles into existence in the shape of AMD and then letting what looks like a fluid simulation whisk them out of position?
GraphicsProgramming,2wcsyj,CodyDuncan1260,1 point,Wed Feb 18 21:53:20 2015 UTC,"an image (png) is loaded into an array, then each pixel is created as a seperate object ""particle"" by looping through the array, ignoring alpha channels."
GraphicsProgramming,2wcsyj,bcfolz,1 point,Wed Feb 18 22:05:36 2015 UTC,how long did this take to render??
GraphicsProgramming,2wcsyj,FrezoreR,1 point,Thu Feb 19 19:14:54 2015 UTC,"It looks cool, but it does not really look like fire to me. Looks more like a liquid than gas. But maybe that was the intention?"
GraphicsProgramming,2wa9g2,CodyDuncan1260,6,Wed Feb 18 05:54:09 2015 UTC,"My two cents: Thank you u/_jackrogue for kicking off the discussion of ""What is Graphics Programming."" Sometimes that can be difficult to define. With Physically Based rendering being a hot topic, some parts trend more toward a physics simulation than graphics. Other synergies exist between graphics and a variety of different fields, and thus makes it hard to draw a line. Perhaps a good rule of thumb is:    ""Does the post talk about how to implement a rendered result?""    Elaborately, it means that the post communicates some idea in code, architecture, hardware features, API, mathematics, or anything else that a program must implement in order to achieve the rendered visual result.   Incomplete List of Topics (borrowed from u/pnpbios with additions): * Software rendering * Shader Programming * Ray tracing * Voxels * Lighting / shading techniques * Math and theory * OpenGL / DirectX / Mantle / other APIs * Rendering Architecture * Offline Rendering * Animation * Particles * Post Processing Effects * Hardware Support: Shader Model features, and demos) * Tools: Debuggers and OpenCV * Tutorials and Learning Materials * Physical Simulation for Visual Effects: Tearing, Breaking, Squishing, Water * Procedural Generation    Up for discussion!"
GraphicsProgramming,2wa9g2,gidoca,3,Wed Feb 18 06:04:55 2015 UTC,"All the topics you mention are related to image synthesis. Wouldn't the term ""Graphics Programming"" also encompass image manipulation?"
GraphicsProgramming,2wa9g2,gidoca,2,Wed Feb 18 08:44:46 2015 UTC,Absolutely. Got any examples of particular fields of interest in that domain?
GraphicsProgramming,2wa9g2,Boojum,6,Wed Feb 18 08:58:07 2015 UTC,"Topics that come to mind are things like colour spaces, resampling techniques, basic image enhancement, but also more advanced things like removing motion blur, image denoising, tonemapping, light field rendering, ... Admittedly, some of these topics could also be considered computer vision."
GraphicsProgramming,2wa9g2,Madsy9,3,Wed Feb 18 13:51:51 2015 UTC,"A few more ideas:   Rendering optimization Graphics file formats Plugin development (3DS, Maya, Renderers) Rasterization Stereo rendering Mesh processing Non-photorealistic rendering Vector graphics Model and material scanning/capture Scientific visualization"
GraphicsProgramming,2wa9g2,Madsy9,2,Thu Feb 19 06:44:22 2015 UTC,"Considering that there are already subreddits or OpenGL and DirectX, and physics simulation, do we want to cover those topics?  My personal opinion is that this subreddit should be about theory/algorithms and software rendering, not specific renderer-APIs. So if someone asks a question on a rendering algorithm and happens to use OpenGL for their implementation, that's fine by me. But I don't think this subreddit should become a place to ask homework questions on how to use the OpenGL or DirectX APIs themselves."
GraphicsProgramming,2wa9g2,Sify007,1 point,Sun Mar 1 12:45:42 2015 UTC,"I would argue that a good chunk of the SIGGRAPH and GDC papers speak on the usage of an API or cover physical simulation topics. The APIs are expanding new capabilities, and underneath have algorithms that are of interest to the theoretical side of graphics. Physically based rendering brings with it a lot of physical simulation topics, especially for special effects in materials and geometry.   It would be difficult to decide whether or not to keep or remove these sources if there was a rule preventing content in these domains. I do like stonefarfalle's idea of a stupid question Wednesday as a mitigation for ""homework questions""."
GraphicsProgramming,2wa9g2,Boojum,1 point,Tue Mar 10 03:44:40 2015 UTC,"Sure. I mean, it is the quality of the posted content and implicit netiquette/attitude on the subreddit which matters in general; not any hard written rule. I hope the spirit of the law here will be that general concepts/algorithms are favored over specific APIs or frameworks."
GraphicsProgramming,2wa9g2,solidangle,1 point,Tue Mar 10 03:57:06 2015 UTC,One thing that I think we should discuss are topic that are images/videos presenting someones work. On one hand I like them - you can showcase what you did and get feedback. On the other hand this us exactly why I don't like /r/ComputerGraphics since there is very little actual content buta lot of people showing off. Maybe a rule like - you can showcase but you need to link to the source code makes sense?
GraphicsProgramming,2wa9g2,stratius,2,Sun Mar 8 21:45:48 2015 UTC,"Regarding APIs, I suspect the thought is that those are already covered by /r/opengl and /r/directx.  That said, I'd lean towards being inclusive."
GraphicsProgramming,2wa9g2,Boojum,1 point,Wed Feb 18 13:08:53 2015 UTC,"/r/directx isn't really active. /r/opengl is super active, but I'd agree toward leaning inclusive."
GraphicsProgramming,2wa9g2,solidangle,2,Wed Feb 18 17:29:17 2015 UTC,"I think if someone wants to post a link to something about the APIs, the post should be about actual CG though, not just about the usage of the API."
GraphicsProgramming,2wa9g2,c0d3M0nk3y,2,Wed Feb 18 21:57:52 2015 UTC,"Never moderated before, but I'd like to apply. I'm a PhD student in computer graphics and 3D reconstruction, currently teaching a graphics course at a university.  As far as moderating rules, personally I like when subreddits have only 'self' posts to avoid terrible quality posts, ban joke comments and disable the 'downvote' button to get rid of dogma-based discussion."
GraphicsProgramming,2wa9g2,StickiStickman,1 point,Thu Feb 19 19:36:44 2015 UTC,I'd like to apply.  I think that I'd make a good complement to those here with a more real-time emphasis since my current professional area is mainly in offline rendering for film.
GraphicsProgramming,2wa9g2,StickiStickman,1 point,Mon Feb 23 01:57:31 2015 UTC,"Not sure if we're allowed to vote, but I vote for this guy. Your post quality is great and you definitely have to required knowledge. I think it would be nice to have a mod focused in offline rendering to keep some balance as most people in the sub probably are interested in real-time rendering."
GraphicsProgramming,2wa9g2,fb39ca4,1 point,Wed Feb 18 18:37:15 2015 UTC,"seconded (and upvoted before you start shouting at me that that is what the upvote is for)  As a realtime engineer myself, I also like to have some insight into what happens on the 'other side of the fence'"
GraphicsProgramming,2wa9g2,JJJams,2,Thu Feb 19 19:39:51 2015 UTC,"So, can I apply as a moderator in this post?   In any case, I'd like to do(again, duh). Are there requirements?   PS: You should add the topics that we got so far to the sidebar."
GraphicsProgramming,2wa9g2,StickiStickman,2,Tue Mar 24 15:56:09 2015 UTC,Yes. You just did! No requirements other than a reasonably good understanding of the topic. Handy if you can read academic papers; they show up a lot.
GraphicsProgramming,2wa9g2,stonefarfalle,2,Wed Feb 18 07:55:44 2015 UTC,"Well, and I'd say positive karma and friendly behavior?  But I'd say I fulfill all 3. :)"
GraphicsProgramming,2wa9g2,bytemr,2,Wed Feb 18 09:00:38 2015 UTC,"If you are in need of moderators, I'd love to help out.  I also have an idea for content. We could have weekly programming challenges like /r/dailyprogrammer does, perhaps with different categories (rasterizing vs raytracing, for example) on different days."
GraphicsProgramming,2wa9g2,arkenthera,2,Wed Feb 18 09:15:12 2015 UTC,"I'd be much more interested in gfx specific challenges, than just general challenges."
GraphicsProgramming,2waz2q,DavidZuc,3,Wed Feb 18 12:12:07 2015 UTC,How is this 4D? Is the video showing different slices of the fourth dimension over time?
GraphicsProgramming,2waz2q,LukasBoersma,2,Wed Feb 18 18:03:52 2015 UTC,"The calculation space is 4D; the video is showing the same slice (the middle) as it evolves in time according to the rule set, which in this case is:   If the pixel in question is off and it has between 1 and 2 neighbours that are turned on then turn the pixel on. If the pixel is already on and it has between 3 and 5 neighbours then it stays on. Otherwise set the pixel to off.   Note a 4D pixel will have 34 - 1 = 80 neighbours.   The automaton is seeded with a centred hypercube."
GraphicsProgramming,2waz2q,iofthebeholder,1 point,Wed Feb 18 21:35:46 2015 UTC,"looks really awesome, cool process, thx for the logic details! how did you learn about 4D calculations? strictly an integer thing or also possible interpolate between states?"
GraphicsProgramming,2waz2q,iofthebeholder,1 point,Mon Feb 23 04:52:29 2015 UTC,"how did you learn about 4D calculations?    I studied maths at uni and learnt about cellular automata through acquaintances and reading ""A New Kind Of Science"" by Stephen Wolfram.   strictly an integer thing or also possible interpolate between states?   Not sure what you mean. Cellular automata are discreet systems."
GraphicsProgramming,2waz2q,iofthebeholder,2,Mon Feb 23 12:55:40 2015 UTC,Not sure what you mean. Cellular automata are discreet systems.   that answered my question.   recently i've striven to grasp the concept of 4D space. not 3D space + time but spatial 4D. thus far concrete understanding still eludes me but i find it an interesting challenge.
GraphicsProgramming,2wasnt,raydey,1 point,Wed Feb 18 10:30:41 2015 UTC,"This seems to be a glossary of various terms about lighting, not specifically relating to graphics. Not completely irrelevant though."
GraphicsProgramming,2wasnt,jrkirby,1 point,Wed Feb 18 18:47:03 2015 UTC,Yeah you're right. Although the terms come up in a load of rendering papers and they're pretty well explained. Best I've found on the internet so far without having to resort to physical texts.
GraphicsProgramming,2wau4n,g0ld3nrati0,3,Wed Feb 18 10:55:54 2015 UTC,"I'm currently working on a kind of instrument for a flightsimulator (fsx) that is displayed on a LCD display (C-Berry) running on a raspberry PI.  I wanted to use Jogl as a Java framework but since startup time was critical I went for plain C/C++ and OpenGLES2. The Java framework took like forever to show something on screen and C++ provides the needed performance.  The instrument basically draws a number of 2D layers atop of each other, so there was no 3D engine to code, but nonetheless it took some time to get used to it. I started with this tutorial as a base and gradually added a very basic kind of layer system to transform each layer as needed.  I don't really recall finding an appropriate framework for the PI, but it could also be that I wanted to avoid large overhead since I only needed 2D capabilities (I did the research last august).  Edit: I usually do Java/Android programming and had only a short project using C++ 2 years ago. So C/C++ with its pointers was at first a bit unfamiliar. The other thing to consider, is that you should take your time to do some research on cross-compiling (I didn't). Otherwise you have to transfer your code to the PI, compile it, see that you messed up some pointers, fix the error, transfer the code AGAIN to the PI, ... and so on.  The desktop on a PI (B+) is incredibly inperformant and in my opinion not suited for doing anything productive like coding. My setup in the end was a git repo that I pushed the code into when I made some major changes, pulled the repo in the PI, compiled, and in case of some errors fixed them in the terminal with nano, then pushed again to the repo."
GraphicsProgramming,2wau4n,grebsn,1 point,Wed Feb 18 20:53:33 2015 UTC,Thanks for the information. Really useful.
GraphicsProgramming,2wau4n,grebsn,1 point,Wed Feb 18 21:27:43 2015 UTC,sorry one more question. What do you think about SDL or Cocos-2d? does it perform well in Rpi?
GraphicsProgramming,2wau4n,nmjohn,2,Sun Feb 22 09:02:14 2015 UTC,I would both give a try. Unfortunatelly I can not tell you something about the performance.   Try a minimum example and see how they run. If it works for you then go ahead :)  Also bear in mind that future versions of the PI have greatly improved hardware and might handle these frameworks even better
GraphicsProgramming,2wau4n,vade,2,Sun Feb 22 09:56:32 2015 UTC,"I'm a bare bones guy, so C++ GL ES would be my suggestion. I wouldn't say this is by any means ""an easy api"" though.  Some link I found that gives a good starter example/build file/glue code for RPI + GL ES2. http://jan.newmarch.name/LinuxSound/Diversions/RaspberryPiOpenGL/"
GraphicsProgramming,2wau4n,fb39ca4,1 point,Thu Feb 19 04:52:27 2015 UTC,"Look into OpenFrameworks, they have a Raspberry Pi port, and easy to use wrappers, plus you can deploy to Desktop as well."
GraphicsProgramming,2w9rxc,newacco,3,Wed Feb 18 03:12:42 2015 UTC,"Render monkey used to be a good one, but truth be told I've been looking for a decent shader ide for years and always wound up compiling manually.   If you want to play, shadertoy on android is good fun."
GraphicsProgramming,2w9rxc,PandaHammer,2,Wed Feb 18 07:39:14 2015 UTC,"I'm not sure if this is exactly what you are looking for, but a came across this post about two weeks ago: http://www.reddit.com/r/gamedev/comments/2umea3/nodeflex_and_shaderflex_from_logic_nodes_to/  Might be of some interest to you."
GraphicsProgramming,2w9rxc,DoctorProfessorson,1 point,Wed Feb 18 08:50:20 2015 UTC,That's for HLSL.
GraphicsProgramming,2w9rxc,josegv,1 point,Wed Feb 18 18:14:41 2015 UTC,"You could prototype GLSL in ShaderFlex using preprocessor defines in an shared include file to write code that compiles as both HLSL and GLSL. Although I won't have time to add an OpenGL renderer in this first release, I will probably add a GLSL compiler for this exact usage so users could at least get compile error/warning feedback. Also, apps like FX Composer and ShaderFlex use higher level FX style formats to wrap the actual HLSL shader language, so there's even more possible incompatibilities between shaders feom these packages than simply HlSL/GLSL, so prototyping in any package will require work to port to another system anyway. unless of course you use something like NodeFlex which can generate multiple targets if needed."
GraphicsProgramming,2w9rxc,shaderplay,2,Thu Feb 19 17:54:43 2015 UTC,Here is list of shader programming resources by FlightGear.
GraphicsProgramming,2w9rxc,AFineTapestry,1 point,Wed Feb 18 11:00:08 2015 UTC,"awesome, thanks. QShaderEdit looks right up my street."
GraphicsProgramming,2w9rxc,PandaHammer,2,Wed Feb 18 13:25:33 2015 UTC,"I have been thinking of maybe trying to write a light weight GLSL editor in my free time since most ""Shader IDEs"" are old and abandoned. I suppose these tools don't exist anymore since we can do so many things in shaders now, years ago people just made different materials and nice surface effects, but now? we can do so many things with shaders that it's hard to make a general ""IDE"" specifically aimed at shaders.  If you need basic syntax highlighting and autocomplete pretty sure Sublime, Atom or Notepad++ must have some plugins to do what you want."
GraphicsProgramming,2w9rxc,josegv,2,Wed Feb 18 18:19:52 2015 UTC,The problem is that text editors don't analyze the context of the file. E.g. they cannot spot errors if you are calling a function from a different file.   And since shader programs don't live on their own it would be good if you don't need to switch programs just bc you're writing a shader. So my suggestion is that you write a plugin for Eclipse or IntelliJ - and rake advantage of the code analysis capabilities of these tools.
GraphicsProgramming,2w9rxc,shaderplay,1 point,Wed Feb 18 19:33:41 2015 UTC,Shader IDEs are making a comeback :)  http://www.shaderplay.com/products/products.html  i'm on my phone and AlienBlue just lost my huge reply when I went to grab the link so I'll follow up in a bit about why you are very right in what you say and how I think my tools tackle these types of problems.
GraphicsProgramming,2w9rxc,FrezoreR,1 point,Thu Feb 19 18:08:45 2015 UTC,Since it hasn't been mentioned: QtCreator.
GraphicsProgramming,2w9rxc,mahacctissoawsum,1 point,Mon Feb 23 08:00:16 2015 UTC,Pretty sure I found one for Visual Studio before...can't remember what it was called. NShader perhaps?  IntelliJ has some too.
GraphicsProgramming,2wavlu,nogrogrein,3,Wed Feb 18 11:19:33 2015 UTC,Pick an existing GUI library that works for you. Very few people are able to write a GUI system that isn't trash.  Here's three of the easier libs: http://libufo.sourceforge.net/ http://anttweakbar.sourceforge.net/doc/ http://librocket.com/  Less accessible: http://libagar.org/ http://cegui.org.uk/ http://gigi.sourceforge.net/screenshots.php  There's also some dark arts voodoo you can pull in C# to host your C++ program ... very dark arts.
GraphicsProgramming,2wavlu,AcidFaucet,1 point,Wed Feb 18 14:03:34 2015 UTC,"Shhhhh!!! *casts stone* We do not talk about C++/CLI... it's the forbidden language...   I agree though, don't reinvent the wheel and use an existing GUI Library."
GraphicsProgramming,2wavlu,kayzaks,1 point,Wed Feb 18 18:48:14 2015 UTC,"C++/CLI should not be used for application level programming. It's a glue language. It's built well for gluing code betweein C++ and CLI managed languages. It gives you multifarious ways to build awful awful code because it's essentially two languages huddled against eachother.  Example:  N::Vector2f* d = new N::Vector2f(); M::Vector2f^ f = gcnew m::Vector2f();    The first is a native pointer to an object allocated uses C++ new. The second is a managed reference pointer allocated by the CLI memory managed and garbage collector. They have two totally different allocation schemes. You can use both, in the same function, with no restrictions."
GraphicsProgramming,2w8xhc,StickiStickman,2,Tue Feb 17 23:06:27 2015 UTC,"This is a very simple example where you just get the color of a higher pixel on the screen and multiply it's color with the color in the specualr texture and the diffuse texture.   This is how I'm reading this sentence, and I wanted to check that I understand: the color of a pixel at some higher location on the screen is multiplied by the specular texture value (between 0. and 1.), and the result is multiplied (or added?) to the diffuse texture value to get the overall color at that point?  How does you tell the shader which higher pixel on the screen to take the color from? I'm fairly new to shaders and I'm trying to wrap my head around how I would implement this myself, so hopefully my question makes sense.  I think the coding portion will really let me wrap my head around what's going on, looking forward to it!"
GraphicsProgramming,2w8xhc,King_Raptorpus,3,Wed Feb 18 09:35:33 2015 UTC,"First of all, sorry for the typo. ^  So, yes, multiplied. ""Mixing"" a color in shaders is always multiplying. So you mix the normal texture with the reflection.  Well, you use texture2d with the screen to get the pixel at a specific point on the screen. You can just use -0.25 to get a pixel that's 1/4th of the screens height higher.   To understand that, in texture coordinates 0.0,0.0 = the first pixel in the top left and 1.0,1.0 the last in the bottom right. To get a specific pixel you just go   1.0/screenheight * pixelpos   So let's say if you wanted to get the 80th pixel in the current pixels column you go in a 1080p screen you go:  texture2d(screenspace,pos.x,1.0/1080.0*80.0)   to get the color of the specific pixel.  ... I guess I went a bit overboard with the answer."
GraphicsProgramming,2w8xhc,Ga1apagO,1 point,Wed Feb 18 10:05:28 2015 UTC,What prerequisites/background information would you need to know to be able to imitate this?  Also I feel that your thing was very well explained and extremely interesting but I did not fully understand a lot of it because of my lack of experience in the matter/frame of reference.   Edit: I look forward to your explanation of the coding
GraphicsProgramming,2w8xhc,Ga1apagO,1 point,Wed Feb 18 00:06:02 2015 UTC,"Basic understanding of how GLSL works with colors (Basicly what the RGBA values mean) and coordinates. Not really much else, it's a extremely simple example. I'm gonna go into that in the next part, if you wish.   My goal was to explain it for people who have some knowledge of programming but must not be masters. Could you tell me what part in particular didn't you get so I can go deeper into it?"
GraphicsProgramming,2w8xhc,strncpy,1 point,Wed Feb 18 00:20:35 2015 UTC,I have effectively no coding experience so I generally just save learning tools I see and try to slowly accumulate a curriculum but the result of your shader was extremely stunning so I felt the need to comment and request further details.   As for any specific questions. I'm a newborn babe in this area. The right answer to the wrong question won't help me much.
GraphicsProgramming,2w60pv,StickiStickman,33,Tue Feb 17 06:07:35 2015 UTC,"God damned lovely shaders and city.    On a different note, from the comments:   While this is awesome as hell, how much autism does this require to make?""   It's like YouTube in there.  What a dick."
GraphicsProgramming,2w60pv,McGuirk808,10,Tue Feb 17 09:44:39 2015 UTC,"Paraphrased:   I could never do this.. must be because I don't have autism. Yeah, that's definitely it, I'm so much better than this guy."
GraphicsProgramming,2w60pv,isomorphic_horse,11,Tue Feb 17 22:04:27 2015 UTC,Thank you! I always appreciate feedback!  PS: What are you referring too?
GraphicsProgramming,2w60pv,McGuirk808,10,Tue Feb 17 10:14:49 2015 UTC,That was from a comment on the album on imgur.
GraphicsProgramming,2w60pv,SarahC,14,Tue Feb 17 10:19:52 2015 UTC,"Oh, right! I normally just ignore stuff like that, so I guess that's why I couldn't tell. At least here on reddit everything is great!"
GraphicsProgramming,2w60pv,TexasCrowbarMassacre,9,Tue Feb 17 10:22:15 2015 UTC,"That's really good!  Can you get the trees interpolating their vertices to make them look rounder, and less square?"
GraphicsProgramming,2w60pv,SarahC,5,Tue Feb 17 06:33:20 2015 UTC,"I already got a mod to make them act as ""plants"", that mean multiple planes with different rotation in the middle of the block, it looked quite good. But the thing was, it wasn't really compatible with ""Voxel Sniper"" which is the tool I use for making 4K and even 8K screenshots."
GraphicsProgramming,2w60pv,carl_pagan,2,Tue Feb 17 06:39:58 2015 UTC,Could you post a link and/or screenshots of that mod?  Sounds interesting.
GraphicsProgramming,2w60pv,SarahC,2,Tue Feb 17 08:04:07 2015 UTC,"I will, I just have to look it up in my Minecraft folder! I think it was something with ""Better Grass and Leaves"" ... I'm not sure though."
GraphicsProgramming,2w60pv,MrPf1ster,1 point,Tue Feb 17 08:33:46 2015 UTC,"This is actually now ""Better Foliage"" as ""Better Grass and Leaves"" has been discontinued.   Here is a link: http://www.minecraftforum.net/forums/mapping-and-modding/minecraft-mods/2119722-better-foliage  Note that this version is currently incompatible with OptiFine  1.8.0_HD_U_D1 which is the latest version of OptiFine for 1.8  EDIT: The mod author uploaded a hot-fix."
GraphicsProgramming,2w60pv,Athox,2,Tue Feb 17 21:37:11 2015 UTC,I see.
GraphicsProgramming,2w60pv,Athox,0,Tue Feb 17 14:29:33 2015 UTC,"haha I have you tagged as ""Poop PhD"""
GraphicsProgramming,2w60pv,Athox,0,Wed Feb 18 23:26:35 2015 UTC,"tagged as ""Poop PhD   I remember that thread. =)"
GraphicsProgramming,2w60pv,sgtmas2006,7,Thu Feb 19 07:04:49 2015 UTC,"Jesus, that is amazing. Is there anyway for you to distribute it to us? Cause I wanna play with that rendering."
GraphicsProgramming,2w60pv,OfFireAndFlame,8,Tue Feb 17 07:17:50 2015 UTC,"Like described in the album, it's still a modfication of SEUS; no matter how much I changed. The EULA forbids me to share anything, I'm really sorry."
GraphicsProgramming,2w60pv,OneDegree,14,Tue Feb 17 07:33:05 2015 UTC,"You can post the diff, just remove the content of what you deleted and keep the positions."
GraphicsProgramming,2w60pv,Zelarius,2,Tue Feb 17 13:10:08 2015 UTC,"Oh, I see what you mean. But the thing is that some files have about none of the original code in them, but are based on them so I don't know how that would behave in terms of EULA."
GraphicsProgramming,2w60pv,Zelarius,4,Tue Feb 17 22:01:27 2015 UTC,"Nobody can prevent you from shipping your own code, even if it's ""inspired by"" something else. I can make a Windows copy without it being a problem (don't work for MS, never saw the source). He can't prevent people from making shaders inspired by his. He didn't invent shaders.  The EULA only says modified versions of it (or parts of it). Even that is a gray area. When you've rewritten a part of it, and barely any of the original code is left, then he can't claim that it's a modification. Programming style or algorithms can't be copyrighted, and certainly not when the code is freely available, whatever the license. The only thing he can claim copyright of is the complete product or modifications that do not significantly alter it.  I would say that if the diff of a file is > 50%, then you've significantly altered it.  A thing that WOULD fall under minor modifications is ""tweaking the numbers"" and things like that. What you've done (I assume) is rewrite the way it works. That's slightly major..."
GraphicsProgramming,2w60pv,iObsidian,2,Wed Feb 18 01:05:55 2015 UTC,"But then again, gray area. I don't want to take any risks.   I think I'm gonna release the most important parts I changed in the future so I'm on the safe side."
GraphicsProgramming,2w60pv,sgtmas2006,2,Wed Feb 18 01:16:59 2015 UTC,"Posting a recipe for what needs to change (ie. the diff), is not gray area.  What I meant by gray area is the EULA claim that you can't do anything with it. He's barely allowed to reach into ""minor alteration"" before it no longer uniquely resembles his product and you can even start charging for it if you want (though I suspect you won't go that far)."
GraphicsProgramming,2w60pv,SteamPunk_Devil,2,Wed Feb 18 02:15:10 2015 UTC,Exactly. That's what I said. xD
GraphicsProgramming,2w60pv,longshot,1 point,Wed Feb 18 02:17:48 2015 UTC,"The Minecraft modding community is really big on pushing licensing, but have no way to enforce it really, nor is it worth their time to reinforce it. I know tons of sites that redistribute mods and whatnot, it's just something that happens in the community."
GraphicsProgramming,2w60pv,longshot,3,Wed Feb 18 15:01:20 2015 UTC,It's more a act of respecting his rules for such a great mod other than legally.
GraphicsProgramming,2w60pv,fb39ca4,12,Wed Feb 18 16:30:46 2015 UTC,"Would be a shame if an hour old reddit account popped up with a torrent link to all the files he stole off you. A damn shame, indeed."
GraphicsProgramming,2w60pv,fb39ca4,3,Tue Feb 17 15:32:56 2015 UTC,"Or if people would randomly PM me, as ""personal friends"" to let me send them the shaders. Would be a shame indeed."
GraphicsProgramming,2w60pv,fb39ca4,1 point,Tue Feb 17 22:02:23 2015 UTC,Original creator is still liable in many circumstances.  (though IANAL)
GraphicsProgramming,2w60pv,GoldenSunGod,6,Tue Feb 17 20:49:08 2015 UTC,Is the framerate playable?
GraphicsProgramming,2w60pv,Ga1apagO,12,Tue Feb 17 08:08:57 2015 UTC,I got a GTX 970 and I get about 30-50 FPS depending on the scene.
GraphicsProgramming,2w60pv,Ga1apagO,8,Tue Feb 17 08:32:55 2015 UTC,That seems acceptable for minecraft.
GraphicsProgramming,2w60pv,passwordissame,8,Tue Feb 17 09:45:05 2015 UTC,"Well, normally it's around 200-600. But it's worth the quality for sure."
GraphicsProgramming,2w6xkn,_jackrogue,17,Tue Feb 17 14:01:24 2015 UTC,Graphics programming topics not directly related to OpenGL or DirectX.  So things like   Software rendering Shader writing Ray tracing Voxels Lighting / shading techniques Math and theory
GraphicsProgramming,2w6xkn,pnpbios,1 point,Tue Feb 17 20:20:12 2015 UTC,The most proper answer at this moment.
GraphicsProgramming,2w6xkn,Diarum,1 point,Tue Feb 17 21:04:34 2015 UTC,What is Software rendering?
GraphicsProgramming,2w6xkn,PrototypeNM1,2,Wed Feb 18 05:33:00 2015 UTC,"Rendering in the CPU instead of the GPU. This might be a dated simplification since the introduction of OpenCL and Cuda, but this is the context I most often hear it used for in academic settings."
GraphicsProgramming,2w6xkn,bioglaze,1 point,Wed Feb 18 08:16:16 2015 UTC,"Software rendering has modern use cases too, like occlusion culling."
GraphicsProgramming,2w6xkn,solidangle,29,Wed Feb 18 10:52:57 2015 UTC,"There seems to be absolutely no love for offline rendering in this thread, which is really a shame. Graphics Programming isn't just sending vertices to a GPU using an API, Graphics Programming is so much more. Graphics Programming is also writing a renderer which can be used for rendering VFX or Animated movies. Graphics Programming is also writing a snow simulation which can be used to simulate the snow in a movie."
GraphicsProgramming,2w6xkn,DigitalShadow,3,Tue Feb 17 18:29:11 2015 UTC,"In the category of offline rendering, I would also like to mention procedurally generated images.  So far this sub seems more geared toward realtime rendering, which is probably what most people think of with graphics programming.  I'm wondering if people here would be interested in some of the experiments I've been doing like this: http://imgur.com/3j8HKD3 taking 4D noise functions using the extra dimensions to make a seamless animation loop."
GraphicsProgramming,2w6xkn,ccricers,2,Tue Feb 17 19:25:12 2015 UTC,"I remember reading an article in which 4D noise was also the solution to creating tiled and seamless noise patterns without resorting to more common, hackish solutions like mixing two patterns and blending away the edges which results in visible artifacts towards the center."
GraphicsProgramming,2w6xkn,DigitalShadow,2,Tue Feb 17 20:20:49 2015 UTC,"That's exactly the technique I'm using.  I realized that you can generalize it so all you need to make a given dimension seamless is an extra dimension of continuous noise.  In the example I linked, I used a 4D function with X/Y slices, using Z for time, and then W to loop Z back on to itself to create the seamless animation.  You can just as easily use X/Y and Z/W to procedurally make seamless tilable textures as long as you have a decent 4D noise function like simplex."
GraphicsProgramming,2w6xkn,ccricers,2,Tue Feb 17 21:05:45 2015 UTC,"I didn't grasp it at first, but you use the last 2 dimensions to create a loop as the ""path"" for the animation, correct? Because you can use 3D noise to animate 2D noise in a non-recurring path, but the 4th dimension is needed in order to create a loop without simply going in reverse. I recognized the simplex noise right away from the curves it makes."
GraphicsProgramming,2w6xkn,DigitalShadow,3,Tue Feb 17 21:10:12 2015 UTC,"I also had trouble imagining it at first.  The mental image that helped me most was to imagine 3D noise, or some physical equivalent, like a piece of marble or granite.  If you slice out a cylinder of it, you can look at the texture all around the cylinder it will be continuous with no obvious seam.  You could unwrap that texture at any point and you would have a 2D texture that repeats in one dimension, and you've achieved this by having an extra dimension to make the loop in.  You can also control the period of the repeating by controlling the size of the loop.  You need 4D to make a truly seamless texture though because you need to pull off the cylinder trick twice, once in 3D, and again in 4D since you need both X and Y to wrap."
GraphicsProgramming,2w6xkn,ccricers,1 point,Tue Feb 17 21:45:16 2015 UTC,The cylinder analogy is great. That's easier to understand.
GraphicsProgramming,2w6xkn,Zachamiester,1 point,Tue Feb 17 21:58:30 2015 UTC,I think you just murdered my laptop.
GraphicsProgramming,2w6xkn,slime73,6,Wed Feb 18 08:17:25 2015 UTC,"Graphics Programming is also writing a snow simulation which can be used to simulate the snow in a movie.   Graphics programming would be writing the code that renders that snow in a realistic manner, but not the code that simulates how the snow behaves – that's physics rather than graphics, another (sometimes related) subfield that requires domain-specific expertise."
GraphicsProgramming,2w6xkn,solidangle,14,Tue Feb 17 19:10:03 2015 UTC,"Simulating snow is a part of the Computer Graphics subfield Physically Based Modeling, which sure is a subfield of CG. Sure it might have a lot in common with physics, why should Light Transport Simulation (aka rendering or image synthesis) be considered as a part of Computer Graphics, but Physically Based Simulation not?  I don't think I'm the only person who thinks physically based simulation is a part of computer graphics as a lot of papers on this topic are published/presented at SIGGRAPH and Eurographics."
GraphicsProgramming,2w6xkn,slime73,2,Tue Feb 17 19:17:39 2015 UTC,"why should Light Transport Simulation (aka rendering or image synthesis) be considered as a part of Computer Graphics, but Physically Based Simulation not?   Rendering is done specifically for displaying graphics. You can easily decouple the physical simulation of how snow behaves from how it looks. Or not display any graphics at all and still have it simulate.  Is rigid-body physics simulation (e.g. Bullet Physics, Box2D, Havok, etc.) graphics programming to you?"
GraphicsProgramming,2w6xkn,solidangle,7,Tue Feb 17 19:30:07 2015 UTC,"There are a lot of things in Computer Graphics that aren't done for directly displaying graphics. Physically based simulation is a part of Computer Graphics as its primary goal is creating beautiful graphics and not precisely simulating the physics behind it. The goal of simulating snow is to simulate something that looks realistic, not to simulate something that is accurate. I do think that it's on the border of what can be considered CG, but I certainly think it's a part of CG."
GraphicsProgramming,2w6xkn,slime73,0,Tue Feb 17 19:39:03 2015 UTC,"The goal of simulating snow is to simulate something that looks realistic, not to simulate something that is accurate.   The same thing can be said for pretty much every non-rendering piece of code (and every piece of code that does have to do with rendering) in almost every video game you've ever played. Rigid-body physics certainly aren't accurate when it comes to many things, but it looks convincing enough visually most of the time and isn't too expensive, so it's used in favor of other types of simulation in many cases. That doesn't mean it's graphics programming though."
GraphicsProgramming,2w6xkn,solidangle,4,Tue Feb 17 19:54:25 2015 UTC,"I think it's a culture difference between academia and vfx/animation on one side and game programming on the other. Physically based modeling is considered to be Computer Graphics in academia and animation. I don't consider pushing triangles to the GPU using a graphics API to be Computer Graphics, but the folks that do game programming think otherwise."
GraphicsProgramming,2w6xkn,lycium,2,Tue Feb 17 20:18:53 2015 UTC,"I think it's more a question of clean definitions than a culture thing. Writing a snow simulation (your example) needn't be for graphics, whereas writing a triangle rasteriser with perspective-correct texture interpolation or something most certainly is.  So basically I'm with slime on this one, in insisting on the ""necessary"" part, not just sufficient :) Otherwise your definition includes all sorts of things like string parsing, just because you need to load an OBJ file and then eventually render it."
GraphicsProgramming,2w6xkn,TheMaddestSci,2,Wed Feb 18 02:27:17 2015 UTC,"I also consider physically based simulation a part of CG. And it's not always that easy to separate simulation from rendering like in rigid body dynamics. In fluid simulation, the level set techniques were apparently introduced to have a representation of the fluid that is convenient to visualize (i.e. raytrace or triangulate). But the coupling goes both ways: for example, level set function values on the boundary of the fluid can be used to calculate the fraction of a grid cell that is filled with fluid. It is then fed to the pressure solver and used to compute more accurate velocities.  Now, to defend the other side: certainly, realtime folks concentrate mostly on how to transform well-known (at least in academia) techniques so that they can be run at >30 FPS. But it would be too harsh to call that all not-graphics-programming. It's not like all of that is just a more complex form of ""algebraic manipulation"", so that the solution fits to the GPU pipeline. Some techniques emerge from clever insights into the very nature of a simulated process.  I admit however that state-of-the-art problems that offline and realtime people work on are usually very different."
GraphicsProgramming,2w6xkn,wrosecrans,8,Tue Feb 17 23:18:09 2015 UTC,"SIGGRAPH seems to think that snow sims are a significant part of computer graphics*, and I am inclined to agree.  The programming involved in having something to put in your graphics is absolutely a part of graphics programming.  Otherwise the only valid interpretation of graphics programming would be implementations of setPixel(x,y,c) and everything else would be mere (sometimes related) prep work.  Trying to narrow the field too much serves no purpose.  Maybe if the subreddit is eventually overflowing with content, it will make sense to make a spinoff out of a specific popular subtopic.  With regard to physics sims specifically, it matters whether you are doing an academic simulation intended to be as accurate as possible, or if you are doing it for rendering a picture.  You may be able to use various simplifying assumptions and cheats that are nonphysical if you know where the camera is pointing, and what will be visible.  Which means your choice of algorithms are driven by the goal of making pictures.  Which is pretty much the definition of doing graphics programming.   http://www.disneyanimation.com/technology/publications"
GraphicsProgramming,2w6xkn,experts_never_lie,2,Tue Feb 17 22:56:26 2015 UTC,"Go take a look at SIGGRAPH and its publication Transactions on Graphics and then show me how the simulation and physically-based modelling that has long been a major part of the field is somehow not actually ""graphics programming""."
GraphicsProgramming,2w6xkn,slime73,3,Wed Feb 18 08:37:19 2015 UTC,"SIGGRAPH is not purely about graphics programming (and CG is not either.) The name should be a hint – it's short for ""Special Interest Group on Graphics and Interactive Techniques."""
GraphicsProgramming,2w6xkn,kayzaks,7,Wed Feb 18 08:49:30 2015 UTC,I was about to add:  Programming anything that designers take for granted.
GraphicsProgramming,2w6xkn,Destidom,3,Tue Feb 17 18:30:22 2015 UTC,"I want to add  ""The Rendering Equation"" by James Kajiya siggraph 86.  It's all math in the end!"
GraphicsProgramming,2w6xkn,lycium,3,Tue Feb 17 21:34:33 2015 UTC,"Yup, and he even introduced path tracing in the same paper. What a legend :D"
GraphicsProgramming,2w6xkn,gidoca,2,Wed Feb 18 03:00:15 2015 UTC,An interesting thing is that Immel et al. introduced it independently at the same Siggraph.
GraphicsProgramming,2w6xkn,PandaHammer,5,Wed Feb 18 08:40:31 2015 UTC,"Programming computers to display graphics, so:  Examples:    Code Purpose Environment    C++ Direct3d 3D  Realtime[Games] Windows/ Xbox Gaming   C++ OpenGL 3D  Realtime[Games] Cross Platform Gaming [Linux/Mac/IOS/Android/PS/Wii]   Javascript WebGL 3d Realtime[Browser] Cross Platform 3D   Love2D 2D Game Engine Cross Platform 2D Games   GLSL/HLSL GPU Shader Programs Raytracing, Lighting Approximation    Not even close to an exhaustive list, but basically code to display pictures on a screen anywhere will fit."
GraphicsProgramming,2w6xkn,HORSEtheGAME,8,Tue Feb 17 14:21:07 2015 UTC,A lot of these are games but there are other applications as well. At my job (I work in research) I use OpenCV to manipulate images to get more data from them. Not as glamorous but still it is really cool what you can do with a single image.
GraphicsProgramming,2w6xkn,barneygale,3,Tue Feb 17 14:45:16 2015 UTC,Processing/OpenFrameworks too?
GraphicsProgramming,2w6xkn,PandaHammer,2,Tue Feb 17 15:01:55 2015 UTC,"sure, why not?"
GraphicsProgramming,2w6xkn,agmcleod,3,Tue Feb 17 15:05:50 2015 UTC,C++ Direct3d  3D Realtime[Games]  PC Gaming   Should really say Windows Gaming. PCs technically involve OS X as well.  I think it's also worth mentioning:    Code Purpose Environment    Java + OpenGL ES 2D and 3D games Android Native Games/Graphics   Objective C/Swift + OpenGL ES/Metal 2D and 3D games iOS Native Games/Graphics
GraphicsProgramming,2w6xkn,PandaHammer,2,Tue Feb 17 16:02:42 2015 UTC,"and linux, amended."
GraphicsProgramming,2w6xkn,agmcleod,3,Tue Feb 17 16:05:00 2015 UTC,"Yep, looks good :)"
GraphicsProgramming,2w6xkn,programmingd00d,1 point,Tue Feb 17 16:07:16 2015 UTC,PCs technically involve OS X as well.   I blame Apple's marketing for this one
GraphicsProgramming,2w6xkn,agmcleod,4,Tue Feb 17 17:16:06 2015 UTC,"The PC thing was around before those Mac vs PC commercials, really i blame the whole industry on that one."
GraphicsProgramming,2w6i3i,Boojum,6,Tue Feb 17 10:19:09 2015 UTC,he's done so many great articles and demos that he should be on the sidebar!  edit: or on the (non-existent) wiki!
GraphicsProgramming,2w6i3i,wongsta,3,Tue Feb 17 13:23:01 2015 UTC,I agree totally; iñigo is just plain badass
GraphicsProgramming,2w6i3i,andiCR,3,Tue Feb 17 13:39:41 2015 UTC,Just made a post to that effect.
GraphicsProgramming,2w6i3i,heyheyhey27,3,Tue Feb 17 15:58:46 2015 UTC,One man should not have such a powerful way of making me feel incompetent at graphics programming as Inigo Quilez does.
GraphicsProgramming,2w6i3i,michaelstripe,1 point,Wed Feb 18 07:31:02 2015 UTC,This is so awesome! What I have been looking to get into as a hobby for some time now. I will read them.
GraphicsProgramming,2w75w8,pnpbios,11,Tue Feb 17 15:19:50 2015 UTC,"There days, GPUs are so flexible, even hardware accelerated rendering is practically software rendering. The only part of it that isn't controlled by software is rasterization, and you could do that in CUDA or openCL if you really wanted.  I think a better term for what you're talking about is single threaded software rendering. Which, since maybe 2003 when CPU clock speeds stopped increasing, will never scale. If you can't do parallel computation, you can't do realtime graphics in a modern sense."
GraphicsProgramming,2w75w8,jrkirby,3,Tue Feb 17 21:23:52 2015 UTC,I disagree. Allthough the graphics pipeline is programmable in a lot of ways it's still fixed in other parts. For example someone mentioned Reyes rendering (can be done in parallel but not using the pipeline)  That being said SW rendering is definitely an interesting topic but albeit a bit limited in its use
GraphicsProgramming,2w75w8,r3v3r,3,Tue Feb 17 22:00:45 2015 UTC,"Or, put another way:  What runs on the GPU?  Software.  There was a period from like 1998~2008 when hardware renderers were a thing, now everything is done in software."
GraphicsProgramming,2w75w8,tormenting,5,Wed Feb 18 06:42:39 2015 UTC,"+1 this.  I'd like to get a feel for the answer to this as well.  For instance -- I was just playing a 2D game called ""The Sun and Moon"" which is pretty awesome, but it lags at dual-dvi resolutions.  If I want to learn ""write your own blitter"" style stuff, is lagginess at high-res guaranteed?  More generally, what classes of awesome graphics suit software renderers especially?"
GraphicsProgramming,2w75w8,mr_luc,1 point,Tue Feb 17 18:15:50 2015 UTC,"yeah, tri-filling is very time consuming.  There are things that can be done to speed things up. multi-core rendering, using CUDA / openCL, SSE instructions, etc.   More generally, what classes of awesome graphics suit software renderers especially?   Well, almost anything you can do with a software renderer, you can do with an video card much faster. So you target environments where you don't have access to OpenGL. So like console homebrew, single scene ray tracing, things like that.  One fun thing that I found you can do is write pixels directly to the frame buffer, letting you do all sorts of trippy things."
GraphicsProgramming,2w75w8,shiftedabsurdity,1 point,Tue Feb 17 18:30:36 2015 UTC,"One fun thing that I found you can do is write pixels directly to the frame buffer, letting you do all sorts of trippy things.   Could you elaborate on this?"
GraphicsProgramming,2w75w8,tormenting,1 point,Tue Feb 17 23:08:09 2015 UTC,"https://github.com/longjoel/gameprogrammingthewrongway/blob/master/book/Chapter%203.md  https://github.com/longjoel/gameprogrammingthewrongway/blob/master/book/resource_3_1.png  if you can get the example running, it's pretty crazy."
GraphicsProgramming,2w75w8,heyheyhey27,2,Tue Feb 17 23:31:52 2015 UTC,"'Course, you can do that in the fragment shader just as easily."
GraphicsProgramming,2w75w8,JustMakeShitUp,1 point,Wed Feb 18 06:44:22 2015 UTC,Ok. Cool.
GraphicsProgramming,2w75w8,gott_modus,3,Wed Feb 18 13:10:43 2015 UTC,"Just want to say, I love your introduction in the readme!"
GraphicsProgramming,2w75w8,AcidFaucet,3,Tue Feb 17 18:34:36 2015 UTC,"At this point, I think writing your own software renderer is basically equivalent to doing floating point in software. Back in the days when every cpu didn't have an FPU it made sense, but by now the hardware is in every machine you'll be targetting, so just use it. There may be some situations where a software renderer would make sense, but the vast majority of the time working with the GPU wll be easier and more efficient. The modern GPU graphics pipeline is so different than how the old software renderers worked that i am not sure there is even any educational value in writing a software renderer. A software renderer could I guess be a history lesson, maybe to see how things were done in the past, but its 2015 already, it's time to move on. Like my early example with floating point hardware, sure you could write a software floating point, but when every single machine has hardware that is easier, faster, and less bugprone why even bother?"
GraphicsProgramming,2w75w8,AcidFaucet,2,Tue Feb 17 21:08:40 2015 UTC,"Like my early example with floating point hardware, sure you could write a software floating point, but when every single machine has hardware that is easier, faster, and less bugprone why even bother?   I can't think of a practical one. But i can think of lots of fun ones.   The homebrew scene is alive and well for a lot of older console systems. Doing a simple 10 poly or less game on the Sega genesis could be fun. The GBA scene is still active. If you are working with an embedded Linux that doesn't have any video hardware acceleration, or are doing something directly with the framebuffer device. You are aiming for a low res look and feel, like you are trying to emulate an old school PlayStation RPG. The old ps1 didn't have a z-buffer. You want super low res paletized textures. The sheer accomplishment of doing it yourself. Did you know people are still building their own computers out of chips? http://www.vaxman.de/projects/tiny_z80/ It's just fun."
GraphicsProgramming,2w75w8,Decker87,2,Tue Feb 17 21:30:50 2015 UTC,"Even devices with hardware acceleration might have output devices that aren't connected to the pipeline. For example, there are many LCDs that connect via SPI or other serial protocols.  Having a library that could be used for that is pretty cool. Although I think that's what DirectFB is for."
GraphicsProgramming,2w75w8,solidangle,2,Tue Feb 17 22:48:35 2015 UTC,The old ps1 didn't have a z-buffer.   You really don't know how good you have it until you begin to realize what life would be like without it.
GraphicsProgramming,2w75w8,sputwiler,1 point,Thu Feb 19 03:27:00 2015 UTC,"I think you would be better off trying to get the ""old school PlayStation RPG"" look through writing GPU shaders.  For your other examples, of course if you don't have a modern GPU on your platform, you can't use it. But keep in mind those old consoles still had graphics hardware that the games used, its just that that hardware was focused more on other 2d graphics acceleration, sprites and so on.  Of course anyone is free to do what they want, but I just think that software rendering has just been thoroughly obsoleted on all fronts, and is not really worth working with anymore."
GraphicsProgramming,2w75w8,ChainedProfessional,2,Tue Feb 17 21:42:18 2015 UTC,"Software rendering will remain relevant for occlusion culling where precomputing isn't viable for quite a while. Especially in ""throw random stuff into it"" scenarios, being stupid easy to parrallelize, and working on any platform where you have the CPU time to spare.  HOQ still isn't very useful.  Edit: typed HOQ as HOC for some stupid reason"
GraphicsProgramming,2w75w8,smallstepforman,2,Wed Feb 18 13:49:48 2015 UTC,http://www.sci.utah.edu/~csilva/papers/gpg3.pdf
GraphicsProgramming,2w82tm,Boojum,1 point,Tue Feb 17 19:22:23 2015 UTC,"This is a older, but still a good alternative if you don't have a copy of Advanced Global Illumination handy."
GraphicsProgramming,2w82tm,solidangle,1 point,Tue Feb 17 19:24:07 2015 UTC,"Personally I prefer this compendium over the book. The book is too focused on the GRDF, which only seems to be used by researchers at KU Leuven, yet it almost completely skips Veach's Path Integral framework.  This compendium is really nice to have when writing a renderer, as many important and useful formulas can be found in there. The formulas aren't hidden in walls in text, so it's easy to find the ones you need."
GraphicsProgramming,2w82tm,pitforest-travis,1 point,Tue Feb 17 19:48:24 2015 UTC,"There's also a paper from 2012 called The State of the Art in Interactive Global Illumination, where interactive from what I can gather means 1 FPS upwards. Gives a good overview over work in the area."
GraphicsProgramming,2w82tm,solidangle,1 point,Tue Feb 17 20:11:56 2015 UTC,"There's a lot of research going on in real-time ray tracing, but I think it's best if we make a separate post for it, as the GI Compendium is mostly about off-line rendering, which is a bit unrelated."
GraphicsProgramming,2w6zmf,olej,2,Tue Feb 17 14:23:21 2015 UTC,"This document is nice, but it mainly focuses on the artistic side. If you're interested in the math and the programming behind Physically Based Rendering. It's mainly focused on offline rendering, but a lot of the knowledge in there can be applied to real-time rendering as well."
GraphicsProgramming,2w6zmf,solidangle,1 point,Tue Feb 17 18:31:07 2015 UTC,This just a dump of Marmoset's website?
GraphicsProgramming,2w8da1,ccricers,1 point,Tue Feb 17 20:38:20 2015 UTC,"Graphics is very much applied mathematics. It is very difficult to get much further than basic rendering without gaining a foothold in the mathematic aspects of graphics.   Most of the mathematics used is linear algebra and conceptual calculus, and by that I mean its less important to know how to produce a derivative and more important to understand what an integral or derivative means. If you want to advance to the more difficult topics, you can probably get by with what you already know, and buffing up on new topics as they arise. Definitely get comfortable converting math to code.  Can one study graphics programming via pseudocode: yes, but pseudocode + math side-by-side would be ideal."
GraphicsProgramming,2w7x16,0not,1 point,Tue Feb 17 18:42:01 2015 UTC,"Very nice, thanks for contributing this to the community!"
GraphicsProgramming,2w7x16,gnudarve,1 point,Wed Feb 18 19:25:14 2015 UTC,http://www.kevinbeason.com/smallpt/
GraphicsProgramming,2w7x16,__Cyber_Dildonics__,1 point,Thu Feb 19 04:47:53 2015 UTC,"That was one of my inspirations when I first got into graphics programming around 2008. Heck, I'm still stunned by it!"
GraphicsProgramming,2w72vr,aiothealchemist,3,Tue Feb 17 14:53:06 2015 UTC,"I can't tell you what's faster without actually measuring it, but I can give you some more information.  GLSL compilers often can and do transform the kind of small branches with few instructions like your #1 into something that can be executed without branching. You can also do this manually, for example by using some bit logic:   int smaller = a < b; //lets assume smaller is all 1s in binary if   this evals to true, and all 0 otherwise. if it isn't there is one more step necessary to make smaller to all 1s or 0s but it isn't relevant to this example. the comparison itself doesn't cause a branch. you can imagine it as a black box which, much like + - * and / just produce a number as a result of two inputs  int result = (c & smaller) | (d & ~smaller);   However the actual branching overhead for this kind of conditional assignment would be tiny either way. Even if the GPU has to execute both paths, the resulting work is just one more assignment and it would probably be hard to measure a difference here anyways. In case the reader doesn't know this: GPUs implement branches typically by running through both branches with all instances of their [warp, thread group.. whatever fancy name GPU devs give their things] and masking out the results of the threads which don't logically belong in a particular branch. Although I base this statement on many years old knowledge so much may have changed in the meantime.  Regarding the second version, once again the compiler might transform this into anything that is much better performing than what we see here, but I'm absolutely unsure about the optimization capabilities of modern shader compilers here. It might very well translate the whole thing such that both versions produce exactly the same result. If the compiler naively translates the second version it the resulting code would have 3 subs, 2 muls and at least a mov somewhere (since you use clamp() as input to another operation it forces a mov, and sign() is free here... but this is another topic).   Once again determining which is faster needs measurement, although maybe someone else can chime in here."
GraphicsProgramming,2w72vr,pitforest-travis,3,Tue Feb 17 18:37:07 2015 UTC,"The ternary operation, in general, will be faster. On pretty much all modern day hardware, that will turn into a single select instruction. The mix/clamp/sign version will very likely turn into a series of instructions."
GraphicsProgramming,2w72vr,soup_sandwich,2,Tue Feb 17 20:34:47 2015 UTC,"so, ternary operation is a select function instead of a conditional branch, and they won't be branched and negatively affect performance like if does. is that correct?"
GraphicsProgramming,2w72vr,soup_sandwich,2,Tue Feb 17 23:52:04 2015 UTC,correct :)
GraphicsProgramming,2w72vr,Overv,3,Wed Feb 18 00:50:31 2015 UTC,"For questions like this you should really just benchmark. It depends on the GPU architecture and data which one is faster and by how much, although functions like sign are generally implemented in a way that doesn't involve branches."
GraphicsProgramming,2w72vr,Acktung,1 point,Tue Feb 17 18:21:10 2015 UTC,"In general terms, a condition would just be around 2 assembler lines when compiled. A function invocation (and in the second case I'm seeing 3 of them) is more than 2 lines: the stack pointer has to be saved in order to return, push some variables from the arguments of the function and jump to the new address.  Yes, I would say first version is faster."
GraphicsProgramming,2w72vr,kaitenuous,7,Tue Feb 17 18:00:36 2015 UTC,"I think the point is that this is not really a generic case. It's known that using branching is Shaders incurs in significant performance losses, although the mechanism behind that is hard to grasp completely since GPU manufacturers don't share this kind of stuff.  For NVidia GPUs at least, GPU threads usually work in small groups (32 or so threads per group), and a branch mispredict might cause a whole lot of instruction cache flushing"
GraphicsProgramming,2w72vr,hapemask,3,Tue Feb 17 18:07:57 2015 UTC,"The ""functions"" here are part of GLSL and may correspond directly with hardware instructions if they exist. It's unlikely that they would involve any sort of stack pointer arithmetic or jumping. As was mentioned above, shader compilers may even turn the two lines into equivalent instructions. It's hard to say without benchmarking it."
GraphicsProgramming,2w72vr,fb39ca4,1 point,Tue Feb 17 19:31:41 2015 UTC,"In fact, there is no stack in shader programs."
GraphicsProgramming,2w72vr,soup_sandwich,2,Wed Feb 18 03:33:37 2015 UTC,"The GLSL intrinsics don't compile to functions. Everything is inlined on every single GPU that I am aware of. You are still correct though, the first one is faster. It turns into a single 'select' instruction."
GraphicsProgramming,2w796a,raydey,3,Tue Feb 17 15:47:20 2015 UTC,This is also the spiritual successor to the Journal of Graphics Tools (JGT).
GraphicsProgramming,2w7vwy,solidangle,1 point,Tue Feb 17 18:34:10 2015 UTC,I don't understand why this hasn't been posted here yet. This is THE book on Physically Based Rendering and describes a renderer which has most features that you expect in a modern production renderer. Although it mainly focuses on off-line rendering it is also a great source of information for people interested in real-time rendering.   This book is absolutely one of my favorite Computer Science books that I own and I don't think I'm the only one of likes it as the authors won a Sci-Tech Academy Award for the book.
GraphicsProgramming,2w6b78,Zicore47,4,Tue Feb 17 08:26:11 2015 UTC,"Neat! I love seeing new particle simulations on the web. You hit the gravity function just right. Not so strong that it sprays everywhere; not so weak that you can't play with the particles. Sometimes it takes a lot of tweaking to get it to this level of satisfaction!  Particle systems actually happens to be my area of study. If you want some info on ramping up that particle count, let me know. I've implemented a flexible system capable of 100k+ and a simple non-interactive system that went up to 16 million. Built particle systems on CPU, pixel shader, geometry shader, vertex shader, and compute shader.   You might also take a look at a WebGL fluid demo. This particular one was built using haxe, which is a web framework with some native bindings to OpenGL, I think.   This one is titled ""One Million Particles"". Unfortunately, I think the gravity function for the particles is overly-linear with a high exponentiation at the center (guess), so the particles tend to quickly flow toward the mouse cursor, but then spray outward once they get within a certain threshold of it.   Millions of HLSL Particles in XNA. I rederived the source code for this one. I like the nature of the gravity function because it creates orbital particles without too much spray. Best shown between 25-35 seconds into the video.  GPU Compute Particle Test . This one is my latest work! 100k+ compute based particle system with screen space geometry collisions. Currently working with a partner to integrate vector fields into the project, and some better particle rendering. Based on the work presented at GDC 2014 by Gareth Thomas: Advanced-Visual Effects with DirectX 11 - Compute-Based GPU Particle Systems. You can download the source for the presentation here: AMD Developer Zone"
GraphicsProgramming,2w6b78,CodyDuncan1260,1 point,Tue Feb 17 16:25:21 2015 UTC,"I know some of the XNA videos, actually they were my inspiration.  The other links are quite, interesting. I'll go through them when I have some time.  Thank you :)"
GraphicsProgramming,2w6b78,cmol,1 point,Tue Feb 17 17:36:17 2015 UTC,That was oddly satisfying!
GraphicsProgramming,2w6pb9,nobody22,1 point,Tue Feb 17 12:15:04 2015 UTC,Also relevant:   Learning Modern 3D Graphics Programming with LWJGL: https://github.com/integeruser/jgltut
GraphicsProgramming,2w6pb9,gnufreex,1 point,Tue Feb 17 12:17:14 2015 UTC,Is there any book for OpenGL 3.0? Don't have 3.3 :(
GraphicsProgramming,2w6pb9,slime73,1 point,Tue Feb 17 20:14:12 2015 UTC,"What video driver are you using? If you're in Linux using Mesa, reasonably recent versions do support GL 3.3 as long as you request a Core Profile when creating your context."
GraphicsProgramming,2w6pb9,gnufreex,1 point,Tue Feb 17 20:52:43 2015 UTC,I am running CentOS7 on IntelHD 4600. Driver is i915
GraphicsProgramming,2w6pb9,slime73,1 point,Tue Feb 17 21:16:46 2015 UTC,"http://people.freedesktop.org/~imirkin/glxinfo/glxinfo.html  On recent Mesa versions (10+), that driver supports OpenGL 3.3 (plus many OpenGL 4.x extensions) as long as you request a Core Profile context when you create it."
GraphicsProgramming,2w6pb9,gnufreex,1 point,Tue Feb 17 21:18:45 2015 UTC,OpenGL vendor string: Intel Open Source Technology Center OpenGL renderer string: Mesa DRI Intel(R) Haswell Mobile  OpenGL core profile version string: 3.1 (Core Profile) Mesa 9.2.5 OpenGL core profile shading language version string: 1.40 OpenGL core profile context flags: (none)  I currently cant use anything other than CentOS. Maybe CentOS 7.1 will have mesa 10 backport.
GraphicsProgramming,2w7ko7,woodrail,1 point,Tue Feb 17 17:13:40 2015 UTC,Also kisrhombille grid description
GraphicsProgramming,2w6xp5,Johnny_Dev,6,Tue Feb 17 14:02:55 2015 UTC,RenderDoc is my goto debugger nowadays. I do a lot of DX11 stuff at work and it's such a joy to work with. It should be said that work is also underway on OpenGL support and the current build supports a subset of GL features.
GraphicsProgramming,2w6xp5,raydey,4,Tue Feb 17 14:32:18 2015 UTC,"I like AMD's GPUPerfStudio. Works with DX11 and OpenGL at the moment, and actually has quite a few different performance debugging tools to work with."
GraphicsProgramming,2w6xp5,DougFunnie7,2,Tue Feb 17 14:20:26 2015 UTC,"The first one I tried was the now deprecated gDebugger (which seems to work on fewer and fewer programs every day). It was very limited but a lot better than nothing to a beginner getting his bearings.  Apitrace seemed a lot more powerful but a lot less user-friendly and more cumbersome.  NSight was the best mix in my opinion. Great capturing and logical division of the presentation of the data (divided in the pipeline stages). It has its problems, some deprecated OpenGL calls are forbidden and if you're not remote-debugging then breaking inside a shader will pretty much crash WDDM if you're not remote-debugging."
GraphicsProgramming,2w6xp5,bimdar,1 point,Tue Feb 17 16:38:23 2015 UTC,"Try AMD's CodeXL. They bought the developers of gDebugger, and CodeXL is like a ""spiritual successor"" with newer tech"
GraphicsProgramming,2w6xp5,DougFunnie7,1 point,Thu Feb 19 05:25:51 2015 UTC,"Depends on what api (dx11 or ogl) you are using and what you are trying to get from your debugging experience. I have personally never found a tool that gives me all the debug information that I need, but using a couple of tools in tandem gets me what I need.  For DX11, my go to tools on a day to day basis are PIX (provided by the DirectX SDK) and Intel's GPA (https://software.intel.com/en-us/gpa). When debugging or perf'ing in DX, my first resort is GPA. I think it gives the best overall idea of whats drawing where and how much time draw calls are taking.  For OGL, my go to tools are AMD's CodeXL (http://developer.amd.com/tools-and-sdks/opencl-zone/codexl/) and apitrace (https://apitrace.github.io/). CodeXL used gDebugger as its foundation and grew on that. Please use this instead of using gDebugger. gDebugger is old and deprecated. apitrace seems to be more stable with showing accurate render targets and depth and stencil textures, than CodeXL. Mainly I use apitrace to look at and debug my draw calls and their corresponding render targets. I really only go to CodeXL to check for ogl errors, look at some statistics about my draw calls, and some perf stats. Another thing to note is that apitrace supports glPush/PopDebugGroup and glObjectLabel, whereas CodeXL still does not have support for these calls.  I am personally not a big fan of MS's Visual Debugger or NSight. They do not allow me to get the information I need quickly or easily.  If you want a greater breakdown of the pros and cons of these tools, just ask."
GraphicsProgramming,2w6xp5,kingcoopa,1 point,Tue Feb 17 22:00:03 2015 UTC,"When I was using DirectX, I used to love PIX's feature that showed the effects of every draw call on render targets: it marked the region that was affected by the draw call, and if you clicked a vertex (or a primitive) in a buffer used in the call, the vertex would be highlighted in the render target view.  I could also right click one of the affected pixels and debug the pixel shader execution for this specific pixel.   Can I have something similar for OpenGL in CodeXL or apitrace?"
GraphicsProgramming,2w6xp5,TheMaddestSci,1 point,Tue Feb 17 22:33:00 2015 UTC,"Unfortunately neither CodeXL or apitrace have features like that. I've really missed those features when working with OpenGL. Possibly AMD GPU PerfStudio can do this, but I haven't explored it enough to know. Intel's GPA can do some of that pixel level debugging, but it only works with DirectX and OpenGL ES (on Android, maybe iOS).  If you find some pixel level debugging for OpenGL, I would love to know about it. I just stick with using the color buffer like printf for OpenGL :)"
GraphicsProgramming,2w7aa0,heyheyhey27,3,Tue Feb 17 15:56:31 2015 UTC,Kostas Anagnostou's readings on physically based rendering collection.
GraphicsProgramming,2w7aa0,memorystomp,2,Tue Feb 17 23:31:34 2015 UTC,http://www.lighthouse3d.com/tutorials/
GraphicsProgramming,2w7aa0,angrymonkey,2,Tue Feb 17 19:43:08 2015 UTC,Humus has a lot of great  tutorials.  http://www.humus.name/index.php?page=3D&
GraphicsProgramming,2w7aa0,FrenchHustler,0,Wed Feb 18 21:45:11 2015 UTC,Wild Magic - Geometric Tools An educational game engine library.  NeHe - OpenGL tutorials Everything you need to get started with OpenGL.
GraphicsProgramming,2w7aa0,pixaeiro,3,Tue Feb 17 18:03:44 2015 UTC,I've been told to avoid Nehe because it's outdated. What version of OpenGL is the tutorials?
GraphicsProgramming,2w7aa0,CodyDuncan1260,1 point,Wed Feb 18 06:16:55 2015 UTC,"Yes, NeHe has been around for a long time, but it's OpenGL tutorials are still valid and useful. The best thing about NeHe's tutorials is that they include project files for a lot of IDEs and even other programming languages like Java and Ruby.  I am not sure what version of OpenGL they were developed for, but I haven't had any issues compiling and building them with Qt5, which I think uses OpenGL 4.3.  I have always used NeHe as the first source of information for all my OpenGL needs, and from there I complement with StackOverflow and many other sources."
GraphicsProgramming,2w79y3,whisky_pete,3,Tue Feb 17 15:53:59 2015 UTC,"SDL2 is a very thin abstraction layer and not some hard to learn huge engine. So if you want to just have some rather thin multi-platform abstraction layer then SDL2 is one of the best.  Some people might take issue with it being C and using that in a C++ project but I don't really mind.  I guess some people might prefer C++ libs like SFML but I prefer the simplicity and wider range of platforms that SDL2 offers (which naturally means that it has fewer ""features"")."
GraphicsProgramming,2w79y3,bimdar,1 point,Tue Feb 17 16:47:59 2015 UTC,"I can understand not liking C++ libs in some situations, but why would people not like C libs?"
GraphicsProgramming,2w79y3,heyheyhey27,2,Tue Feb 17 17:25:33 2015 UTC,"Some people are kind of obsessed with OOP and they tend to wrap everything in classes, so for them a C API is just another thing to wrap awkwardly into classes."
GraphicsProgramming,2w79y3,bimdar,1 point,Tue Feb 17 18:09:02 2015 UTC,"...actually I tend to wrap small C libs in classes too, but usually I'm also attempting to simplify the interface to it at the same time, because the library often exposes stuff that I'll never need.  EDIT: and the wrappers aren't some complex OOP beast; they're usually just a class or two at the most."
GraphicsProgramming,2w79y3,heyheyhey27,1 point,Tue Feb 17 18:14:42 2015 UTC,"I still prefer SDL 1.2 for doing 2d. It's much simpler to get started with.  That being said, SDL 2.0 is far more powerful and can take advantage of modern hardware."
GraphicsProgramming,2w9tlw,nbajillionpoo,1 point,Wed Feb 18 03:26:40 2015 UTC,"Well, what does your event processing loop look like? I've recently used the SDL2 library for something and I haven't noticed any of the delays you're talking about, so I'd be interested to see where those delays appear for you."
GraphicsProgramming,2w9tlw,happens_,2,Wed Feb 18 03:54:11 2015 UTC,Found answer online. Wrap poll event in a while loop not an if statement :P
GraphicsProgramming,2w6heb,Boojum,1 point,Tue Feb 17 10:07:05 2015 UTC,This sadly lacks my favourite DreamWorks paper: ISHair: Importance Sampling for Hair Scattering
GraphicsProgramming,2w6heb,solidangle,1 point,Tue Feb 17 18:49:41 2015 UTC,"The talk abstract is there, at least.  I'm not sure why it lacks the EGSR '12 paper."
GraphicsProgramming,2w6heb,solidangle,1 point,Tue Feb 17 19:00:57 2015 UTC,"Yeah it's weird. A talk abstract isn't very useful and for some reason the actual talk is missing from their website and the ACM Digital Library (but that seems to be the case with most DreamWorks, Disney and Pixar talks). Luckily it isn't hard to find the actual paper, but it's a shame it's missing there."
GraphicsProgramming,2w5ir0,nexuapex,2,Tue Feb 17 03:18:17 2015 UTC,"Just read it.   Unfortunately, it seems that it is slower than vanilla PCF for >1k shadow map sizes.   In Matt Pettineo's A Sampling of Shadow Techniques -- the defacto reference comparison of modern shadow mapping techniques -- VSM has shown to be consistently slower than PCF. The paper adds quite some ALU overhead to VSM (which is not necessarily bad, as it can be hidden as shadow mapping is quite bandwidth intensive). So I cannot see how it can be faster than VSM -- and the paper's own benchmarks also state that. Furthermore, it seems that Ignacio Castaño's OptimizedPCF (also from the link above) has not been taken into account when comparing with PCF performance.   And PCF also has the stability advantage: No light leaking (vs. reduced light leaking with the paper's technique) and no quantization flickering (vs. some flickering in the paper).   So, from a academic point of view a pretty interesting paper (especially the comparison of different shadow map ""basis functions"", so to speak), however, VSM and its derivatives like ESM or CSM have been abandoned in favor of the optimized PCF versions for good. For example DICE implemented the PCF optimizations and never looked back to VSM, ESM, CSM, etc."
GraphicsProgramming,2w5ir0,empharr,1 point,Wed Feb 18 01:21:01 2015 UTC,"Speaking of which, MJP added moment shadow mapping to his comparison: https://mynameismjp.wordpress.com/2015/02/18/shadow-sample-update/"
GraphicsProgramming,2w5ir0,empharr,1 point,Thu Feb 19 05:10:02 2015 UTC,"Thanks for the link!  Yes, unfortunately that coincides with my own findings. It seems that the technique from the paper is neither quality-wise nor performance-wise up to par with the other techniques."
GraphicsProgramming,2w5ir0,snakepants,1 point,Fri Feb 20 00:05:17 2015 UTC,This looks cool! It looks like it has a lot of the advantages of variance shadow mapping but without the light leaking. It even has similar storage requirements since usually you would be using float32x2 to store the depth and depth squared anyway.
GraphicsProgramming,2w74in,HylianEvil,2,Tue Feb 17 15:07:38 2015 UTC,"Is this for real-time or off-line purposes? As far as I know the most popular subdivision algorithm in off-line rendering is still Catmull-Clark, which is mainly because it's pushed by Pixar and because it has some nice mathematical properties (one rings and limit surfaces)."
GraphicsProgramming,2w74in,solidangle,1 point,Tue Feb 17 18:33:03 2015 UTC,Hmm. It is going to be real time.
